2005.mtsummit-papers.4,2004.iwslt-evaluation.15,0,0.0278508,"ine Translation (EBMT) method based on Tree String Correspondence (TSC) and statistical generation. In this method, the translation examples are represented as TSC. The translation consists of three steps. The input sentence is first parsed into a tree. Then the TSC forest is searched out if it is best matched with the input tree. Lastly, the translation is generated using a statistical generation model to combine the target language strings in the TSCs. Many EBMT systems use annotated tree structures as translation examples (Watanabe, 1992; Poutsma, 2000; Al-Adhaileh et al., 2002; Way, 2003; Aramaki and Kurohashi, 2004). In these systems, it is necessary to parse both the source 25 translation fragments with a statistical model. The statistical model can improve the fluency by using n-gram co-occurrence statistics. However, the statistical model does not take into account the semantic relation between the translation example and the input sentence. In this paper, we propose a new method to select the translation fragments and generate the translation, which combines the semantic-based approach and the statistical approach. The generation model consists of three parts: the semantic similarity between the tree"
2005.mtsummit-papers.4,2001.mtsummit-papers.12,0,0.0414753,"thm based on TSC to find the TSC forest that is best matched with the input tree. For EBMT systems, there are two major approaches to select the appropriate translation fragments and generate the translation. The semantic-based approach (Aramaki et al., 2003; Armaki and Kurohashi, 2004) obtains an appropriate translation fragment for each part of the input sentence. The final translation is generated by combining the translation fragments in a pre-defined order. This approach does not take into account the fluency between the translation fragments. The statistical approach (Kaki et al., 1999; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Imamura et al., 2004) selects Abstract This paper proposes a novel Example-Based Machine Translation (EBMT) method based on Tree String Correspondence (TSC) and statistical generation. In this method, the translation examples are represented as TSC, which consists of three parts: a parse tree in the source language, a string in the target language, and the correspondences between the leaf nodes of the source language tree and the substrings of the target language string. During the translation, the input sentence is first parsed into a tree. Then the TSC forest is searche"
2005.mtsummit-papers.4,C92-2115,0,0.0608775,"stems. 1 Introduction This paper proposes a novel Example-Based Machine Translation (EBMT) method based on Tree String Correspondence (TSC) and statistical generation. In this method, the translation examples are represented as TSC. The translation consists of three steps. The input sentence is first parsed into a tree. Then the TSC forest is searched out if it is best matched with the input tree. Lastly, the translation is generated using a statistical generation model to combine the target language strings in the TSCs. Many EBMT systems use annotated tree structures as translation examples (Watanabe, 1992; Poutsma, 2000; Al-Adhaileh et al., 2002; Way, 2003; Aramaki and Kurohashi, 2004). In these systems, it is necessary to parse both the source 25 translation fragments with a statistical model. The statistical model can improve the fluency by using n-gram co-occurrence statistics. However, the statistical model does not take into account the semantic relation between the translation example and the input sentence. In this paper, we propose a new method to select the translation fragments and generate the translation, which combines the semantic-based approach and the statistical approach. The"
2005.mtsummit-papers.41,W99-0606,0,0.0770036,"Missing"
2005.mtsummit-papers.41,P98-1004,0,0.0322207,"word aligners. Experimental results indicate that the boosting method proposed in this paper performs much better than the original word aligner, achieving a large error rate reduction. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In recent years, some researchers have employed statistical word alignment models to build alignment links (Brown et al., 1993; Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Other researchers used similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). One issue about word alignment is how to improve the performance of a word aligner when the training data is fixed. One possible solution is to use boosting (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 1997 & 2000). Boosting generates the classifiers sequentially and changes the weight of the training instance that is provided as input to each inducer based on the previously built classifiers. The underlying idea of boosting is to combine simple “rules” to form an ensemble such that the 313 including reference set construction, error"
2005.mtsummit-papers.41,P03-1012,0,0.0514034,"dition, the final ensemble takes into account the weights of the alignment links produced by the individual word aligners. Experimental results indicate that the boosting method proposed in this paper performs much better than the original word aligner, achieving a large error rate reduction. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In recent years, some researchers have employed statistical word alignment models to build alignment links (Brown et al., 1993; Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Other researchers used similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). One issue about word alignment is how to improve the performance of a word aligner when the training data is fixed. One possible solution is to use boosting (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 1997 & 2000). Boosting generates the classifiers sequentially and changes the weight of the training instance that is provided as input to each inducer based on the previously built classifiers. The underlying idea of boosting is"
2005.mtsummit-papers.41,P98-1083,0,0.0760757,"Missing"
2005.mtsummit-papers.41,A00-2005,0,0.0357531,"Missing"
2005.mtsummit-papers.41,P00-1056,0,0.812121,"training set. In addition, the final ensemble takes into account the weights of the alignment links produced by the individual word aligners. Experimental results indicate that the boosting method proposed in this paper performs much better than the original word aligner, achieving a large error rate reduction. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In recent years, some researchers have employed statistical word alignment models to build alignment links (Brown et al., 1993; Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Other researchers used similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). One issue about word alignment is how to improve the performance of a word aligner when the training data is fixed. One possible solution is to use boosting (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 1997 & 2000). Boosting generates the classifiers sequentially and changes the weight of the training instance that is provided as input to each inducer based on the previously built classifiers. The underly"
2005.mtsummit-papers.41,tufis-barbu-2002-lexical,0,0.0267131,"tal results indicate that the boosting method proposed in this paper performs much better than the original word aligner, achieving a large error rate reduction. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In recent years, some researchers have employed statistical word alignment models to build alignment links (Brown et al., 1993; Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Other researchers used similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). One issue about word alignment is how to improve the performance of a word aligner when the training data is fixed. One possible solution is to use boosting (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 1997 & 2000). Boosting generates the classifiers sequentially and changes the weight of the training instance that is provided as input to each inducer based on the previously built classifiers. The underlying idea of boosting is to combine simple “rules” to form an ensemble such that the 313 including reference set construction, error rate calculation, and th"
2005.mtsummit-papers.41,N01-1003,0,0.0607072,"Missing"
2005.mtsummit-posters.7,2003.mtsummit-papers.23,0,0.0282081,"n technical document translation, product localization, etc. Now, a number of TMS tools are available at market such as Trados’ Translator’s WorkBench, SDL, IBM Translation Manager/2 and Transit. In general, the traditional TMS retrieves examples matched with the input sentence at the sentence level (Planas and Furuse, 2000). It provides good translation suggestions only when there are closely matched examples in the TM. This leads to low coverage on unseen sentences or texts. In order to solve this problem, many researchers use sub-sentential matching (Brown, 1996; Simard and Langlais, 2001; Huang et al., 2003). Brown (1996) segmented the input sentence into sequences of words and determined the translation of these sequences by performing subsentential alignment on each matched example. Simard and Langlais (2001) ranked the examples according to the length of matched sub-sequence of words. A longest available sub-sequence strategy was adopted to cover as much part of source sentences as possible. Huang et al. (2003) proposed a unified framework to generate translations with statistical confidences. Their experimental results indicated that sub-sentential matching improved translation efficiency and"
2005.mtsummit-posters.7,C92-2101,0,0.15871,"Missing"
2005.mtsummit-posters.7,macklovitch-russell-2000-whats,0,0.0705571,"Missing"
2005.mtsummit-posters.7,2001.mtsummit-ebmt.3,0,0.117687,"measurement data is fixed. 只有 在 测量 数据 被 确定 后，所 描绘 的 区域 才会 闭合。 Translation Suggestion: (9) 所有 数据 都 显示 在 测量 显示 窗 中， 只 有 在 测量 数据 被 确定 后。 where Wt i (k ) is the weight of the example k that Figure 3. An Example Showing Translation Generation containing the sub-sequence s i ; LenSub( s i ) is the length of the sub-sequence s i ; LenSrc(k ) is the length of the source part of the example k; AlignScore(k ) is the alignment score of the example k, which is calculated as shown in (6). patterns were extracted. Carl (1999) used shallow parsing methods to extract patterns. Güvenir and Cicekli (1998) and McTait (2001) used languageneutral methods to extract translation patterns. In this paper, we make use of the word alignment information and a source language parser to extract translation patterns from the examples. The extracted patterns are refined based on their occurring frequency. These patterns are then applied for sentence translation. After obtaining the best example for each subsequence, we locate the translations in the examples using word alignment information and combine the translations to generate the translation suggestion. An example is shown in Figure 3. The input sentence is segmented in"
2005.mtsummit-posters.7,P02-1040,0,0.0712064,"ficiency Evaluation Results Table 1. Statistics of Two Testing Sets 6.2 Baseline System 257 223 240 precsion = recall = |SG ∩ S R | |SG | |SG ∩ S R | |SR | (12) (13) Besides precision and recall, we also use NIST score (Doddington, 2002) for evaluation. The NIST score is calculated by using the statistics of n-gram co-occurrence. It measures both the adequacy and fluency of the translation by comparing it with the translation references. Each sentence can have one or more translation references. It is reported that the NIST score correlates better with the human judgments than the BLEU score (Papineni et al., 2002). Although our system is not an automatic translation system, the adequacy and fluency measures of the translation suggestion are also useful for TMS. For each sentence in the testing sets, we have one translation reference. The evaluation results are shown in Table 4. The results indicate that our system provides higher quality of translation suggestions than the baseline system. Group Two Our System Baseline System Our System Baseline System Our System Table 2. Testing Order Test 1 Test 1 Test 2 Test 2 When the users translate the sentences, the time that they spend on the translation is rec"
2005.mtsummit-posters.7,C00-2090,0,0.0276033,"translators for post-editing. Thus, TMS is also called a Machine Aided Human Translation System (MAHTS). Generally speaking, TMS does not conduct real translation (Macklovitch & Russell, 2000). However, it can avoid repetitive labor and improve translation efficiency. It has been widely applied in technical document translation, product localization, etc. Now, a number of TMS tools are available at market such as Trados’ Translator’s WorkBench, SDL, IBM Translation Manager/2 and Transit. In general, the traditional TMS retrieves examples matched with the input sentence at the sentence level (Planas and Furuse, 2000). It provides good translation suggestions only when there are closely matched examples in the TM. This leads to low coverage on unseen sentences or texts. In order to solve this problem, many researchers use sub-sentential matching (Brown, 1996; Simard and Langlais, 2001; Huang et al., 2003). Brown (1996) segmented the input sentence into sequences of words and determined the translation of these sequences by performing subsentential alignment on each matched example. Simard and Langlais (2001) ranked the examples according to the length of matched sub-sequence of words. A longest available s"
2005.mtsummit-posters.7,2001.mtsummit-papers.60,0,0.0848986,"t has been widely applied in technical document translation, product localization, etc. Now, a number of TMS tools are available at market such as Trados’ Translator’s WorkBench, SDL, IBM Translation Manager/2 and Transit. In general, the traditional TMS retrieves examples matched with the input sentence at the sentence level (Planas and Furuse, 2000). It provides good translation suggestions only when there are closely matched examples in the TM. This leads to low coverage on unseen sentences or texts. In order to solve this problem, many researchers use sub-sentential matching (Brown, 1996; Simard and Langlais, 2001; Huang et al., 2003). Brown (1996) segmented the input sentence into sequences of words and determined the translation of these sequences by performing subsentential alignment on each matched example. Simard and Langlais (2001) ranked the examples according to the length of matched sub-sequence of words. A longest available sub-sequence strategy was adopted to cover as much part of source sentences as possible. Huang et al. (2003) proposed a unified framework to generate translations with statistical confidences. Their experimental results indicated that sub-sentential matching improved trans"
2005.mtsummit-posters.7,C96-1030,0,0.0397118,"efficiency. It has been widely applied in technical document translation, product localization, etc. Now, a number of TMS tools are available at market such as Trados’ Translator’s WorkBench, SDL, IBM Translation Manager/2 and Transit. In general, the traditional TMS retrieves examples matched with the input sentence at the sentence level (Planas and Furuse, 2000). It provides good translation suggestions only when there are closely matched examples in the TM. This leads to low coverage on unseen sentences or texts. In order to solve this problem, many researchers use sub-sentential matching (Brown, 1996; Simard and Langlais, 2001; Huang et al., 2003). Brown (1996) segmented the input sentence into sequences of words and determined the translation of these sequences by performing subsentential alignment on each matched example. Simard and Langlais (2001) ranked the examples according to the length of matched sub-sequence of words. A longest available sub-sequence strategy was adopted to cover as much part of source sentences as possible. Huang et al. (2003) proposed a unified framework to generate translations with statistical confidences. Their experimental results indicated that sub-sentent"
2007.mtsummit-papers.52,P98-1004,0,0.0606955,"Missing"
2007.mtsummit-papers.52,W02-1601,0,0.055431,"Missing"
2007.mtsummit-papers.52,P06-1009,0,0.143339,"Missing"
2007.mtsummit-papers.52,P03-1012,0,0.0330102,"Missing"
2007.mtsummit-papers.52,2006.eamt-1.10,0,0.0148468,"emmabased method and the stem-based method outperform the word-based method when only smaller training corpus are available. Although the stem-based method and the lemma-based achieve lower AER, they do not achieve much improvement on translation quality when a larger training corpus is available. This result again confirms that large gains in alignment performance can achieve relatively small gains in translation performance (Lopez and Resnik, 2006). In conclusion, lemma-based and stem-based methods are effective to alleviate the problem of data sparseness. This result is similar to that in (Gupta and Federico 2006). However, the lemma-based method outperforms the stembased method on our corpus, which is different from that in (Gupta and Federico 2006). This may be caused by the different kind of language pairs used. 11 10 9 8 30K 90K Word Lemma 150K Stem Figure 4. Translation Results on the WA Test Set by Using Different Sizes of Training Corpus 15 This paper proposed a method to improve statistical word alignment by combining various clues. Our method first trained a baseline statistical IBM word alignment model and then improved it with different clues. The clues are mainly based on features such as l"
2007.mtsummit-papers.52,J97-2004,0,0.090719,"Missing"
2007.mtsummit-papers.52,koen-2004-pharaoh,0,0.0437797,"development set is also from the corpora distributed for the 2005 HTRDP evaluation of machine translation, which includes 278 sentences, with four reference translations for each source sentence. We use the SRILM toolkit (Stolcke, 2002) to train a language model on the Chinese Gigaword Second Edition provided by LDC (catalog number LDC2005T14). Translation Results We conduct phrase-based statistical machine translation (SMT) from English to Chinese. To perform phrase-based SMT, we need a trainer and a decoder. For training, we use Koehn's training scripts 10 . For the decoder, we use Pharaoh (Koehn, 2004). We run the decoder with its default settings (maximum phrase length 7) and then use Koehn's implementation of minimum error rate training (Och, 2003) to tune the feature weights on the development set. The translation quality was evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). The translation results are shown in Table 5. In the translation experiments, our method combines all of the clues to get the alignment results. Based on the alignment results, we extract the phrase pairs used by the Pharaoh decoder. (a) Without Chunks Baseline Our Method Figur"
2007.mtsummit-papers.52,W00-0729,0,0.0589602,"Missing"
2007.mtsummit-papers.52,P05-1057,0,0.218941,"Missing"
2007.mtsummit-papers.52,2006.amta-papers.11,0,0.0423426,"ared with the wordbased method. On smaller training corpus, lemma-based method outperforms the stem-based method. From the translation results, it can be seen that the lemmabased method and the stem-based method outperform the word-based method when only smaller training corpus are available. Although the stem-based method and the lemma-based achieve lower AER, they do not achieve much improvement on translation quality when a larger training corpus is available. This result again confirms that large gains in alignment performance can achieve relatively small gains in translation performance (Lopez and Resnik, 2006). In conclusion, lemma-based and stem-based methods are effective to alleviate the problem of data sparseness. This result is similar to that in (Gupta and Federico 2006). However, the lemma-based method outperforms the stembased method on our corpus, which is different from that in (Gupta and Federico 2006). This may be caused by the different kind of language pairs used. 11 10 9 8 30K 90K Word Lemma 150K Stem Figure 4. Translation Results on the WA Test Set by Using Different Sizes of Training Corpus 15 This paper proposed a method to improve statistical word alignment by combining various c"
2007.mtsummit-papers.52,W97-0311,0,0.0766868,"Missing"
2007.mtsummit-papers.52,W03-0435,0,0.0475076,"Missing"
2007.mtsummit-papers.52,P06-1065,0,0.0416382,"Missing"
2007.mtsummit-papers.52,P04-1066,0,0.0491909,"Missing"
2007.mtsummit-papers.52,P03-1021,0,0.0165469,"nce translations for each source sentence. We use the SRILM toolkit (Stolcke, 2002) to train a language model on the Chinese Gigaword Second Edition provided by LDC (catalog number LDC2005T14). Translation Results We conduct phrase-based statistical machine translation (SMT) from English to Chinese. To perform phrase-based SMT, we need a trainer and a decoder. For training, we use Koehn's training scripts 10 . For the decoder, we use Pharaoh (Koehn, 2004). We run the decoder with its default settings (maximum phrase length 7) and then use Koehn's implementation of minimum error rate training (Och, 2003) to tune the feature weights on the development set. The translation quality was evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). The translation results are shown in Table 5. In the translation experiments, our method combines all of the clues to get the alignment results. Based on the alignment results, we extract the phrase pairs used by the Pharaoh decoder. (a) Without Chunks Baseline Our Method Figure 2. Alignment Examples w/o Chunks Example (2) shows a sentence pair using the chunk clue. (2) the multi-field, multi-level, and multi-channel cooperat"
2007.mtsummit-papers.52,P00-1056,0,0.0699138,"our experiments, we use OAK to recognize the English chunks, and then recognize Chinese chunks based on both the word alignment results and English chunks. We also use a handcraft Chinese-English dictionary included in HowNet (Dong and Dong, 2006), which is a Chinese conceptual database 8 . This dictionary includes 55,462 entries. The handcraft English-Chinese dictionary is collected from various resources, comprising 64,234 entries. Evaluation Metrics The reference of the test data provided by CLDC includes possible links and sure links. So we use the same evaluation metrics as described in (Och and Ney, 2000). If we use A to indicate the alignments identified by the proposed methods, and S and P to denote the sure and possible links in the reference alignments, the precision, recall, and alignment error rate (AER) are calculated as described in Equations (9), (10) and (11). |A ∩S| |S| |A∩P| recall = |P| |A ∩S |+ |A ∩ P | AER = 1 − |A |+ |S| precision = (9) (10) (11) Results With the data described above, we perform bi-directional (source to target and target to source) word alignment based on IBM model 4, and obtain two alignment results on the test set. Based on these two results, we get the ""ref"
2007.mtsummit-papers.52,P02-1040,0,0.0757065,"by LDC (catalog number LDC2005T14). Translation Results We conduct phrase-based statistical machine translation (SMT) from English to Chinese. To perform phrase-based SMT, we need a trainer and a decoder. For training, we use Koehn's training scripts 10 . For the decoder, we use Pharaoh (Koehn, 2004). We run the decoder with its default settings (maximum phrase length 7) and then use Koehn's implementation of minimum error rate training (Och, 2003) to tune the feature weights on the development set. The translation quality was evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). The translation results are shown in Table 5. In the translation experiments, our method combines all of the clues to get the alignment results. Based on the alignment results, we extract the phrase pairs used by the Pharaoh decoder. (a) Without Chunks Baseline Our Method Figure 2. Alignment Examples w/o Chunks Example (2) shows a sentence pair using the chunk clue. (2) the multi-field, multi-level, and multi-channel cooperation 领域 、多 层次 、多 渠道 MT Test Set 0.1426 0.1690 Table 5. English to Chinese Translation Results (b) With Chunks 多 WA Test Set 0.1137 0.1346 From the results, it can be seen"
2007.mtsummit-papers.52,popovic-ney-2004-towards,0,0.0213012,"Missing"
2007.mtsummit-papers.52,2005.eamt-1.29,0,0.0334651,"Missing"
2007.mtsummit-papers.52,J96-1001,0,0.040066,"Missing"
2007.mtsummit-papers.52,C02-1012,0,0.0800603,"Missing"
2007.mtsummit-papers.52,H05-1010,0,0.146777,", 1998; Tiedemann, 1999). Wu and Wang (2004) used a rule-based translation system to identify and disambiguate the multi-word units and improved the multi-word alignment results. Tiedemann (2003) used chunks and n-grams. The third issue is how to make use of more linguistic information. The basic statistical word alignment method works on the word level of the plain text. In recent years, some discriminative methods are proposed to integrate various syntactic and lexical clues into the alignment models to improve alignment quality (Liu et al., 2005; Moore et al., 2006; Blunsom and Cohn, 2006; Taskar et al., 2005). In these methods, part-of-speech (POS), association measure between bilingual words, and translation dictionaries are usually used. More linguistic information, such as named entity and chunk information, may be useful for word alignment. In this paper, we propose an unified method to address all of the three problems mentioned above. The method first trains a weight for each null alignment to improve word alignment. Secondly, we use dictionaries, including human crafted translation dictionaries and automatically trained dictionaries, to improve both precision and recall of word alignment. F"
2007.mtsummit-papers.52,E03-1026,0,0.0397214,"Missing"
2007.mtsummit-papers.52,tufis-barbu-2002-lexical,0,0.0382186,"Missing"
2007.mtsummit-papers.52,J97-3002,0,0.473791,"Missing"
2007.mtsummit-papers.52,C04-1005,1,0.858069,"Missing"
2007.mtsummit-papers.52,P05-1059,0,0.0385058,"Missing"
2007.mtsummit-papers.67,P06-1002,0,0.0116763,"showed in their experiments that the heuristic methods outperform the generative models. Their analysis indicates that the performance gap stems primarily from the segmentation variable of the generative model, which increases the possibility of overfitting during training. Recently, several researches have been conducted to explore the relationship of word alignment quality measures and machine translation quality. The main points are concluded as follows. 1. It is difficult to find a direct correlation between word alignment measures (such as alignment error rate) and automated MT metrics (Ayan and Dorr, 2006; Fraser and Marcu, 2006). 2. Large gains in alignment performance under any metric are confirmed to achieve relatively small gains in translation performance (Lopez and Resnik, 2006). 3. Better feature mining can lead to substantial gain in translation quality (Lopez and Resnik, 2006). 4. It is better to generate alignments adapted to the characteristics of the translation models that will make use of this alignment information (Vilar et al., 2006). However, all of the above conclusions are made on the indomain test sets and never on the out-of-domain test sets. In addition, although Lopez an"
2007.mtsummit-papers.67,N04-4006,0,0.0153766,"d alignment information can be set as the union of the n alignments involved. n As compared with the maximum value, we can get a relative value as described in (7). We only keep those phrase pairs whose relative values are larger than a threshold. 2 G 2 ( f , e) Max( f ) (7) Model Combination Model Interpolation To combine the different phrase tables, we use linear interpolation method in this paper. For the phrase 2 n Another way to combine the phrase pairs extracted by different methods is to use the count merging method, which is widely used in language modeling (Bacchiani and Roark, 2003; Bacchiani et al., 2004). The main idea of count merging is to assign weights to the occurring count of phrase pairs, and then merging them to build translation models. The method to estimate the translation probability is shown in equation (10). e Ratio( f , e) = n ∑ α i = 1 and ∑ β i = 1 . Count Merging According to the contingency table, the log likelihood ratio for each phrase pair is defined as 2 (9) i =1 p ( f |e) = Table 1. Contingency Table for Phrase Pairs nij N (8) i =1 This threshold is determined on a development set. w( f |e) = ∑ β i counti ( f , e) i =1 n ∑ β i ∑ counti ( f , e' ) i =1 (11) e' Where cou"
2007.mtsummit-papers.67,P05-1033,0,0.0530211,"oise filtering and combination of these heuristic methods achieve larger improvement on the out-of-domain test set than on the in-domain test set. Introduction Word or phrase alignment plays a crucial role in statistical machine translation (SMT). During training, the SMT systems produce alignment between words or phrases of existing examples to estimate the statistical parameters. With these estimated parameters, the SMT systems translate source sentences into target sentences. Current state-of-the-art models in machine translation are based on alignments between phrases (Koehn et al., 2003; Chiang, 2005). Phrase-based generative models are first proposed by Marcu and Wong (2002) to extract phrase pairs. Zhao and Waibel (2005) also proposed several generative models to generate phrase pairs for machine translation. An alternative is to first generate word alignments. Phrase alignments are then inferred heuristically from these word alignments (Och et al., 1999; Koehn et al., 2003). DeNero et al. (2006) showed in their experiments that the heuristic methods outperform the generative models. Their analysis indicates that the performance gap stems primarily from the segmentation variable of the g"
2007.mtsummit-papers.67,W06-3105,0,0.0350977,"Missing"
2007.mtsummit-papers.67,J93-1003,0,0.0580581,"xical weight are important features in the phrase translation table. Lopez and Resnik (2006) found that alignment quality has little impact on the lexical weighting feature, which itself provides only a modest improvement in translation quality. Thus, we only filter the phrase pairs using phrase translation statistics. Although the phrase translation probability described in equation (2) can be used to filter the phrase table, translation probability usually overestimates the infrequently occurring pairs. In order to solve this problem, we use association measures to filter some phrase pairs. Dunning (1993) proved that log likelihood ratio performed very well on infrequently occurring data. Thus, we calculate the log likelihood ratio for each phrase pair. First we construct a contingency table as shown in Table 1. Source phrase ~Source phrase Totals Target phrase n11 n21 C1 ~Target phrase n12 n22 C2 Totals R1 R2 N n p w ( f |e, a ) = ∑ β i p w,i ( f |e, a ) Where pi ( f |e) and p w,i ( f |e, a ) ( i = 1,..., n ) are the phrase translation probability and lexical weight estimated by n different methods. α i and β i are interpolation coefficients, ensuring G ( f , e) = −2 log λ = ∑ nij log i, j (5"
2007.mtsummit-papers.67,koen-2004-pharaoh,0,0.232319,"diagonally neighboring alignment points are also included. grow-diag-final: in addition to the alignment points in grow-diag, the non-neighboring alignment points between words, of which at least one is currently unaligned, are added in a final step. grow-final: In addition to the alignment points in grow, the non-neighboring alignment points between words, of which at least one is currently unaligned, are added in a final step. m =1 Where hm (e, f) represents feature functions, and λm is the weight assigned to the corresponding feature function. In this paper, we will use the Pharaoh system (Koehn, 2004). Eight different features are used in this system. 1. a phrase translation probability 2. an inverse phrase translation probability 3. a lexical weight: measuring the quality of word alignment inside the phrase pair 4. an inverse lexical weight 5. language model 6. phrase penalty 7. word penalty 8. reordering For phrase translation probability, lexical weight, and reordering, we use the same models in (Koehn et al., 2003). We use n-grams for language modelling. For the phrase penalty and word penalty, we use the same heuristics in (Zen and Ney, 2004). Phrase Extraction With the word alignment"
2007.mtsummit-papers.67,2005.mtsummit-papers.11,0,0.202781,"he impacts of these heuristics on machine translation quality. And then we will re-evaluate the relationship of word alignment and their impacts on machine translation quality on both indomain and out-of-domain test sets. Furthermore, in order to examine the noise in the phrase pairs extracted using different alignment heuristics, we propose a method to filter the noise in the phrase tables using association measures. And we will also investigate whether combining the phrase tables extracted by different heuristics improves translation quality. We performed experiments on the Europarl corpus (Koehn, 2005; Koehn and Monz, 2006), where a multilingual in-domain training corpus, an in-domain test set, and an out-of-domain test set are available. We obtained the following results: 1. Word alignment results show that the compromise method, which makes compromise between precision and recall, performs the best on both in-domain and out-of-domain alignment test sets. 2. Translation results indicate that the heuristic methods perform differently on the in-domain and out-of-domain test sets. On the in-domain test set, the recall-oriented heuristic methods yield better translation quality. On the out-of"
2007.mtsummit-papers.67,W06-3114,0,0.459203,"these heuristics on machine translation quality. And then we will re-evaluate the relationship of word alignment and their impacts on machine translation quality on both indomain and out-of-domain test sets. Furthermore, in order to examine the noise in the phrase pairs extracted using different alignment heuristics, we propose a method to filter the noise in the phrase tables using association measures. And we will also investigate whether combining the phrase tables extracted by different heuristics improves translation quality. We performed experiments on the Europarl corpus (Koehn, 2005; Koehn and Monz, 2006), where a multilingual in-domain training corpus, an in-domain test set, and an out-of-domain test set are available. We obtained the following results: 1. Word alignment results show that the compromise method, which makes compromise between precision and recall, performs the best on both in-domain and out-of-domain alignment test sets. 2. Translation results indicate that the heuristic methods perform differently on the in-domain and out-of-domain test sets. On the in-domain test set, the recall-oriented heuristic methods yield better translation quality. On the out-of-domain test set, the p"
2007.mtsummit-papers.67,N03-1017,0,0.410847,"ation systems; (3) noise filtering and combination of these heuristic methods achieve larger improvement on the out-of-domain test set than on the in-domain test set. Introduction Word or phrase alignment plays a crucial role in statistical machine translation (SMT). During training, the SMT systems produce alignment between words or phrases of existing examples to estimate the statistical parameters. With these estimated parameters, the SMT systems translate source sentences into target sentences. Current state-of-the-art models in machine translation are based on alignments between phrases (Koehn et al., 2003; Chiang, 2005). Phrase-based generative models are first proposed by Marcu and Wong (2002) to extract phrase pairs. Zhao and Waibel (2005) also proposed several generative models to generate phrase pairs for machine translation. An alternative is to first generate word alignments. Phrase alignments are then inferred heuristically from these word alignments (Och et al., 1999; Koehn et al., 2003). DeNero et al. (2006) showed in their experiments that the heuristic methods outperform the generative models. Their analysis indicates that the performance gap stems primarily from the segmentation va"
2007.mtsummit-papers.67,2006.amta-papers.11,0,0.0240714,"ariable of the generative model, which increases the possibility of overfitting during training. Recently, several researches have been conducted to explore the relationship of word alignment quality measures and machine translation quality. The main points are concluded as follows. 1. It is difficult to find a direct correlation between word alignment measures (such as alignment error rate) and automated MT metrics (Ayan and Dorr, 2006; Fraser and Marcu, 2006). 2. Large gains in alignment performance under any metric are confirmed to achieve relatively small gains in translation performance (Lopez and Resnik, 2006). 3. Better feature mining can lead to substantial gain in translation quality (Lopez and Resnik, 2006). 4. It is better to generate alignments adapted to the characteristics of the translation models that will make use of this alignment information (Vilar et al., 2006). However, all of the above conclusions are made on the indomain test sets and never on the out-of-domain test sets. In addition, although Lopez and Resnik (2006) pointed out that it may be more useful to handle noise in phrase extraction than to improve word alignment quality, they did not provide detailed information to verify"
2007.mtsummit-papers.67,W02-1018,0,0.0223699,"rger improvement on the out-of-domain test set than on the in-domain test set. Introduction Word or phrase alignment plays a crucial role in statistical machine translation (SMT). During training, the SMT systems produce alignment between words or phrases of existing examples to estimate the statistical parameters. With these estimated parameters, the SMT systems translate source sentences into target sentences. Current state-of-the-art models in machine translation are based on alignments between phrases (Koehn et al., 2003; Chiang, 2005). Phrase-based generative models are first proposed by Marcu and Wong (2002) to extract phrase pairs. Zhao and Waibel (2005) also proposed several generative models to generate phrase pairs for machine translation. An alternative is to first generate word alignments. Phrase alignments are then inferred heuristically from these word alignments (Och et al., 1999; Koehn et al., 2003). DeNero et al. (2006) showed in their experiments that the heuristic methods outperform the generative models. Their analysis indicates that the performance gap stems primarily from the segmentation variable of the generative model, which increases the possibility of overfitting during train"
2007.mtsummit-papers.67,W99-0604,0,0.0209715,"e statistical parameters. With these estimated parameters, the SMT systems translate source sentences into target sentences. Current state-of-the-art models in machine translation are based on alignments between phrases (Koehn et al., 2003; Chiang, 2005). Phrase-based generative models are first proposed by Marcu and Wong (2002) to extract phrase pairs. Zhao and Waibel (2005) also proposed several generative models to generate phrase pairs for machine translation. An alternative is to first generate word alignments. Phrase alignments are then inferred heuristically from these word alignments (Och et al., 1999; Koehn et al., 2003). DeNero et al. (2006) showed in their experiments that the heuristic methods outperform the generative models. Their analysis indicates that the performance gap stems primarily from the segmentation variable of the generative model, which increases the possibility of overfitting during training. Recently, several researches have been conducted to explore the relationship of word alignment quality measures and machine translation quality. The main points are concluded as follows. 1. It is difficult to find a direct correlation between word alignment measures (such as align"
2007.mtsummit-papers.67,P02-1040,0,0.0919634,"l. We use the same word alignment evaluation metrics as described in (Och and Ney, 2003). If we use A to indicate the alignments identified by the proposed methods, and S and P to denote the sure and possible links in the reference alignments, the precision, recall, and alignment error rate (AER) are calculated as described in Equations (12), (13) and (14). If we take all links as sure links, then |P |= |S |. |A ∩S| |S| |A∩P| recall = |P| |A ∩S |+ |A ∩ P | AER = 1 − |A |+|S| precision = (12) (13) (14) The translation quality was evaluated using a wellestablished automatic measure: BLEU score (Papineni et al., 2002). And we also use the tool provided in the NAACL/HLT 2006 shared task on SMT to calculate the BLEU scores. We use the same method described in (Koehn and Monz, 2006) to perform the significance test. Word Alignment Results We perform bi-directional (source to target and target to source) word alignments using the GIZA++ toolkit, and obtain the symmetrized alignment results using the six word alignment heuristics described in this paper. The alignment results are shown in Table 4 for the in-domain and out-of-domain test sets. On both of the test sets, the compromise method ""growdiag"" obtains th"
2007.mtsummit-papers.67,2006.iwslt-papers.7,0,0.0819329,"follows. 1. It is difficult to find a direct correlation between word alignment measures (such as alignment error rate) and automated MT metrics (Ayan and Dorr, 2006; Fraser and Marcu, 2006). 2. Large gains in alignment performance under any metric are confirmed to achieve relatively small gains in translation performance (Lopez and Resnik, 2006). 3. Better feature mining can lead to substantial gain in translation quality (Lopez and Resnik, 2006). 4. It is better to generate alignments adapted to the characteristics of the translation models that will make use of this alignment information (Vilar et al., 2006). However, all of the above conclusions are made on the indomain test sets and never on the out-of-domain test sets. In addition, although Lopez and Resnik (2006) pointed out that it may be more useful to handle noise in phrase extraction than to improve word alignment quality, they did not provide detailed information to verify this point. In this paper, we will use different heuristics to generate word alignments, and examine the impacts of these heuristics on machine translation quality. And then we will re-evaluate the relationship of word alignment and their impacts on machine translation"
2007.mtsummit-papers.67,N04-1033,0,0.0359614,"Missing"
2007.mtsummit-papers.67,I05-3011,0,0.0217575,"than on the in-domain test set. Introduction Word or phrase alignment plays a crucial role in statistical machine translation (SMT). During training, the SMT systems produce alignment between words or phrases of existing examples to estimate the statistical parameters. With these estimated parameters, the SMT systems translate source sentences into target sentences. Current state-of-the-art models in machine translation are based on alignments between phrases (Koehn et al., 2003; Chiang, 2005). Phrase-based generative models are first proposed by Marcu and Wong (2002) to extract phrase pairs. Zhao and Waibel (2005) also proposed several generative models to generate phrase pairs for machine translation. An alternative is to first generate word alignments. Phrase alignments are then inferred heuristically from these word alignments (Och et al., 1999; Koehn et al., 2003). DeNero et al. (2006) showed in their experiments that the heuristic methods outperform the generative models. Their analysis indicates that the performance gap stems primarily from the segmentation variable of the generative model, which increases the possibility of overfitting during training. Recently, several researches have been cond"
2008.iwslt-evaluation.18,P07-2045,0,0.0140203,"ranslation. These modules include Chinese word segmentation, word alignment, named entity (NE) translation and language model. The remainder of this paper is organized as follows. Section 2 describes the core algorithms of our systems for the 5 tasks. Section 3 focuses on the specific methods adapted to spoken language translation. Sections 4 to 7 provide the details of our experiments for each task. Section 8 presents the evaluation results of our primary submissions. Section 9 concludes our work for IWSLT 2008. 2. System description 2.1. SMT system We used the phrase-based SMT system: Moses [1]. In Moses, phrase translation probabilities, reordering probabilities, and language model probabilities are combined in the log-linear model to obtain the best translation e best of the source f: e best = arg max e p(e |f ) sentence ≈ arg max e M ∑ λm hm (e, f) (1) m =1 The models or features which are employed by the decoder consist of (a) one or several phrases tables, (b) one or more language models trained with SRILM toolkit [2], (c) distancebased and lexicalized reordering models, (d) word penalty and (e) phrase penalty. The weights are set by a discriminative training method on a held-o"
2008.iwslt-evaluation.18,P03-1021,0,0.0108553,"phrase translation probabilities, reordering probabilities, and language model probabilities are combined in the log-linear model to obtain the best translation e best of the source f: e best = arg max e p(e |f ) sentence ≈ arg max e M ∑ λm hm (e, f) (1) m =1 The models or features which are employed by the decoder consist of (a) one or several phrases tables, (b) one or more language models trained with SRILM toolkit [2], (c) distancebased and lexicalized reordering models, (d) word penalty and (e) phrase penalty. The weights are set by a discriminative training method on a held-out data set [3].  Chinese-English-Spanish: PIVOT_CES (RS and CRR) • BTEC tasks  Chinese-English: BTEC_CE (RS and CRR)  Chinese-Spanish: BTEC_CS (RS and CRR) SS, RS and CRR represent different input conditions, namely spontaneous speech, read speech and correct recognition result, respectively. For different translation directions, we used different translation strategies. For Chinese-English and EnglishChinese translation, we used hybrid systems that combine rule-based machine translation (RBMT) method and statistical machine translation (SMT) method. For ChineseSpanish translation, phrase-based SMT model"
2008.iwslt-evaluation.18,D07-1030,1,0.841151,"mbination of RBMT and SMT We used two MT systems with different translation strategies for Chinese-English and English-Chinese translation. One is a RBMT software - Dr. eye 1 . The other is a phrase-based SMT system - Moses. Firstly we ran the RBMT system as a black box to translate the source texts into the target language. The translations and original source text were used as a synthetic bilingual corpus to train an SMT system. Using the bilingual corpus available for an evaluation task, we built another SMT model. Then these two translation models were combined together as a hybrid system [4]. 1 Available download.shtml - 124 - at http://www.dreye.com.cn/prod/cp-pcProceedings of IWSLT 2008, Hawaii - U.S.A. In our experiments using the development data for evaluation, we used RBMT system to translate the development data to build the synthetic bilingual corpus. In primary runs at IWSLT 2008, we built the synthetic bilingual corpus by translating the test data using RBMT system. For the pivot task, we also used RBMT system on some training sets, which will be described in Section 7. 2.3. Pivot-based SMT system For the pivot task Chinese-English-Spanish translation, we built a pivot"
2008.iwslt-evaluation.18,P07-1108,1,0.842415,"- at http://www.dreye.com.cn/prod/cp-pcProceedings of IWSLT 2008, Hawaii - U.S.A. In our experiments using the development data for evaluation, we used RBMT system to translate the development data to build the synthetic bilingual corpus. In primary runs at IWSLT 2008, we built the synthetic bilingual corpus by translating the test data using RBMT system. For the pivot task, we also used RBMT system on some training sets, which will be described in Section 7. 2.3. Pivot-based SMT system For the pivot task Chinese-English-Spanish translation, we built a pivot translation model as described in [5]. Firstly we trained two translation models on the Chinese-English corpus and English-Spanish corpus, and then built a pivot translation model for Chinese-Spanish translation using English as a pivot language. To use a phrase-based translation system such as Moses, we need to obtain a phrase-table for the ChineseSpanish translation, where two important features are needed: phrase translation probability and lexical weight. 2.3.1. Phrase translation probability the alignment information a inside ( f , e) can be obtained as shown in (4). a = {( f , e) |∃p : ( f , p) ∈ a1 & ( p, e) ∈ a 2 } We est"
2008.iwslt-evaluation.18,N03-1017,0,0.00659339,"slation dictionary extraction, word alignment, NE translation, language model, punctuation restoration, case restoration, and translation selection. Currently, most of Chinese word segmentation systems are not designed for spoken language translation. Thus, we investigated the effect of segmentation granularity and segmentation dictionary for better MT performance on spoken language. Lexical weight 3.1.1. Given a phrase pair ( f , e) and a word alignment a between the source word positions i = 1,..., n and the target word positions j = 1,..., m , the lexical weight can be estimated as follows [6]. p w ( f |e, a ) i =1 1 j |(i, j ) ∈ a ∑ w( f i |e j ) Word segmentation dictionary A Chinese dictionary is a fundamental element for word segmentation. In our segmenter, we used three kinds of dictionaries: basic dictionary, NE dictionary and in-domain dictionary. • The basic dictionary contains some commonly-used words. (3) • The NE dictionary consists of transliterated person names, Japanese first names and last names 3 , and location names. They were extracted from the Chinese-English Name Entity Lists Version 1.0 (LDC2005T34). ∀(i , j )∈a In order to estimate the lexical weight, we first"
2008.iwslt-evaluation.18,W08-0336,0,0.0137922,"in bilingual dictionaries, we added them into the final alignment set. • For the links conflicting with the links in the final alignment set, we simply deleted them. • For the remained links, we kept them in the bidirectional results. 5 For the tasks using Spanish as the target language, we only used in-domain dictionaries extracted from training data. EC Figure 1. An example of improving word alignment results Word granularity There are lots of arguments about the definition of a Chinese word. In fact, only a few researchers investigated the effect of word granularity on machine translation [8]. In this work, we followed the guidelines shown below to define what is a word. • Its translation is a word in the target language. 3.1.4. 我们 想 要 张 靠 窗 的 桌子 。 CE 6 Available at http://www.fjoch.com/GIZA++.html - 126 - Proceedings of IWSLT 2008, Hawaii - U.S.A. Training with NE categories Decoding with NE categories 玛丽 和 亨利 不 一样 大 。 mary is not so old as henry . ---------------------------------------<nr&gt;#0 和 <nr&gt;#1 不 一样 大 。 <nr&gt;#0 is not so old as <nr&gt;#1 . 苏珊 和 比尔 不 一样 大 。 ---------------------------------------<nr&gt;#0 和 <nr&gt;#1 不 一样 大 。 <nr&gt;#0 is not so old as <nr&gt;#1 . ------------------------"
2008.iwslt-evaluation.18,W07-0718,0,0.0369648,"Missing"
2008.iwslt-evaluation.18,2006.iwslt-evaluation.7,0,\N,Missing
2015.mtsummit-papers.23,J93-2003,0,0.0470688,"x, we note that the number of possible translation candidates Tx is much smaller compared to the target vocabulary size Ky . Instead of calculating the Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 303 Figure 3: Deep Output Layer of RNN Decoder translation probability for every word in the target vocabulary, we only calculate the probability for a limited set of target candidates which are relevant to the given x. The problem is how to get such a candidate set. Intuitively, we can make use of the word alignment information generated with IBM model (Brown et al., 1993). And we found that there are often too many candidates especially for the common words, which makes the improvement limited. Actually, we can get more accurate translation candidates from the phrase pairs of the phrase-based translation model (Koehn et al., 2003). Firstly, we train a phrase-based translation model with the word aligned bilingual sentence pairs. Then we segment the input sentence x into a number of continuous source phrases. Next, we search the corresponding target phrases for all source phrases from the phrase-based translation model, namely phrase table. After that, we build"
2015.mtsummit-papers.23,W14-4012,0,0.0860863,"Missing"
2015.mtsummit-papers.23,P14-1129,0,0.0728845,"Missing"
2015.mtsummit-papers.23,P07-1019,0,0.173351,"Missing"
2015.mtsummit-papers.23,koen-2004-pharaoh,0,0.21634,"existing system, Cho et al. (2014) proposed a whole new RNN Encoder-Decoder approach which generates the target translation with a neural network directly. Bahdanau et al. (2014) extended the above approach by allowing an RNN model to automatically (soft-)search for parts of a source sentence to predict a target word, and achieved a translation performance comparable to the existing state-of-the-art phrase-based systems (Koehn et al., 2003). Although NMT shows high potential, its decoding efficiency is still challenging. Cho et al. (2014) implemented a standard beam search decoding algorithm (Koehn, 2004) for an RNN Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 297 Encoder-Decoder system published as GroundHog 1 . The beam search algorithm successfully reduces the search space from exponential size to polynomial size, and it is able to translate about ten words per second. Even though the speed is acceptable for most research tasks, it is not yet efficient enough to meet the requirement of commercial systems for providing real-time translation service. Huang and Chiang (2007) proposed the forest rescoring algorithm which succeed to accelerate the dec"
2015.mtsummit-papers.23,P07-2045,0,0.0122377,"of corpus for NIST08 task, containing 67M Chinese words and 74M English words, to train our models. No monolingual corpus are used to help the model training. The NIST MT03 Chineseto-English test set is used as the development set, and NIST MT08 Chinese-to-English test set is used to evaluate our translation results. The development set is used to choose the best RNN Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 304 model in history, because the performance of RNN model will fluctuate during training. 4.2 Toolkits The open source SMT system Moses 2 (Koehn et al., 2007) is used to train a phrase-based machine translation system. The phrase table trained with Moses will be used to constrain the softmax translation. We use the open source RNN Encoder-Decoder toolkits GroundHog which is implemented with Theano (Bergstra et al., 2010; Bastien et al., 2012) to train a neural machine translation model. It is re-implemented with C++ in the GPU Environment and named NetTrans, which is well optimized and faster than GroundHog which is implemented in Python. Given the same model, there is no translation differences between GroundHog and NetTrans. We will report our ex"
2015.mtsummit-papers.23,N03-1017,0,0.112704,"models into the traditional translation decoders as additional features in a log-linear framework (Och and Ney, 2002). Rather than applying a neural network as a part of the existing system, Cho et al. (2014) proposed a whole new RNN Encoder-Decoder approach which generates the target translation with a neural network directly. Bahdanau et al. (2014) extended the above approach by allowing an RNN model to automatically (soft-)search for parts of a source sentence to predict a target word, and achieved a translation performance comparable to the existing state-of-the-art phrase-based systems (Koehn et al., 2003). Although NMT shows high potential, its decoding efficiency is still challenging. Cho et al. (2014) implemented a standard beam search decoding algorithm (Koehn, 2004) for an RNN Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 297 Encoder-Decoder system published as GroundHog 1 . The beam search algorithm successfully reduces the search space from exponential size to polynomial size, and it is able to translate about ten words per second. Even though the speed is acceptable for most research tasks, it is not yet efficient enough to meet the requiremen"
2015.mtsummit-papers.23,P02-1038,0,0.42432,"Missing"
2015.mtsummit-papers.23,D12-1089,0,0.0168434,"s. Finally, we constrain the softmax operation with the candidate word set for all the hypothesis extension of x. One disadvantage of above approach is that it is too costly to load the whole phrase table into memory. Fortunately, we can reduce the memory cost by pruning phrase table carefully. The phrase table filtering techniques have been widely used in machine translation. Instead of considering both the phrase coverage and translation features in the filtering technique, our system only considers the word coverage, which is much easier to implement. We investigated the histogram pruning (Zens et al., 2012) and length-based pruning method to reduce the memory used in our system. For histogram pruning, we preserves the X target phrases with highest probability for each source phrase. For length-based pruning, we prune the phrase pairs which contain more than X words in source or target side. Where X is the threshold. We found that even the basic length-based pruning method works well enough in our task. 4 Experimental Settings In this section, we evaluate the improved beam search algorithm on the task of Chinese-toEnglish translation. 4.1 Dataset In this paper, we use Chinese-to-English bilingual"
2020.acl-main.166,P19-1535,1,0.762994,"2016b; Zhang et al., 2018b), previous works decouple policy learning from response generation, and then use utterance-level latent variables (Zhao et al., 2019) or keywords (Yao et al., 2018) as RL actions to guide response generation. In this work, we investigate how to use prior dialog-transition information to facilitate dialog policy learning. Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Dinan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Liu et al., 2019; Bao et al., 2019; Xu et al., 2020). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn policy learning, instead of dialog informativeness improvement. Specifically, we are motivated by (Xu et al., 2020). The method in (Xu et al., 2020) has the issue of cross-domain transfer since it relies on labor-intensive knowledge graph grounded multiturn dialog datasets for model training. Compared with them, our conversational graph is automatically built from dialog datasets, which introduces very low cost for train"
2020.acl-main.166,D18-1256,0,0.0169801,", 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et al., 2018; Yao et al., 2018; Zhao et al., 2019). However, these utterance-level methods tend to produce less coherent multi-turn dialogs since it is quite challenging to learn semantic transitions in a dialog flow merely from dialog data without the help of prior information. In this paper, we propose to represent prior information about dialog transition (between a message and its response) as a graph, and optimize dialog policy based on the graph, to foster a more coherent dialog. To this end, we propose a novel conversational graph (CG) grounded policy learning frame1835 Proceedings of the 58th Annu"
2020.acl-main.166,D17-1259,0,0.0192369,"onse “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et al., 2018; Yao et al., 2018; Zhao et al., 2019). However, these utterance-level methods tend to produce less coherent multi-turn dialogs since it is quite challenging to l"
2020.acl-main.166,N16-1014,0,0.66715,"t response with two sub-steps: firstly, obtains a response representation r¯ using both M3 and a message representation (from a message-encoder); Next, produces a response “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et"
2020.acl-main.166,D16-1127,0,0.489962,"t response with two sub-steps: firstly, obtains a response representation r¯ using both M3 and a message representation (from a message-encoder); Next, produces a response “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et"
2020.acl-main.166,D16-1230,0,0.114845,"Missing"
2020.acl-main.166,D19-1187,1,0.845143,"models (Li et al., 2016b; Zhang et al., 2018b), previous works decouple policy learning from response generation, and then use utterance-level latent variables (Zhao et al., 2019) or keywords (Yao et al., 2018) as RL actions to guide response generation. In this work, we investigate how to use prior dialog-transition information to facilitate dialog policy learning. Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Dinan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Liu et al., 2019; Bao et al., 2019; Xu et al., 2020). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn policy learning, instead of dialog informativeness improvement. Specifically, we are motivated by (Xu et al., 2020). The method in (Xu et al., 2020) has the issue of cross-domain transfer since it relies on labor-intensive knowledge graph grounded multiturn dialog datasets for model training. Compared with them, our conversational graph is automatically built from dialog datasets, which introduces very"
2020.acl-main.166,W15-4640,0,0.0973246,"Missing"
2020.acl-main.166,N19-1123,0,0.0334427,"Missing"
2020.acl-main.166,D18-1255,0,0.0342656,"egeneration issue of word-level policy models (Li et al., 2016b; Zhang et al., 2018b), previous works decouple policy learning from response generation, and then use utterance-level latent variables (Zhao et al., 2019) or keywords (Yao et al., 2018) as RL actions to guide response generation. In this work, we investigate how to use prior dialog-transition information to facilitate dialog policy learning. Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Dinan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Liu et al., 2019; Bao et al., 2019; Xu et al., 2020). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn policy learning, instead of dialog informativeness improvement. Specifically, we are motivated by (Xu et al., 2020). The method in (Xu et al., 2020) has the issue of cross-domain transfer since it relies on labor-intensive knowledge graph grounded multiturn dialog datasets for model training. Compared with them, our conversational graph is automatically built from di"
2020.acl-main.166,C16-1316,0,0.253008,"rocedure, they randomly select a mechanism for response generation. As shown in Figure 3, the generator consists of a RNN based message encoder, a set of responding mechanisms, and a decoder. First, given a dialog message, the message-encoder represents it as a vector x. Second, the generator uses a responding mechanism (selected by policy) to convert x into a response representation r¯. Finally, r¯ and a keyword (selected by policy) are fed into the decoder for response generation. To ensure that the given keyword will appear in generated responses, we introduce another Seq2BF based decoder (Mou et al., 2016) to replace the original RNN decoder. Moreover, this generator is trained on a dataset with pairs of [the message, a keyword extracted from a response]-the response.3 3.2 CG Construction Given a dialog corpus D, we construct the CG with three steps: what-vertex construction, how-vertex construction, and edge construction. 3 If multiple keywords are extracted from the response, we randomly choose one; and if no keyword exists in the response, we randomly sample a word from the response to serve as “keyword”. 1837 What-vertex construction To extract content words from D as what-vertices, we use"
2020.acl-main.166,D14-1162,0,0.0831011,"Missing"
2020.acl-main.166,P15-1152,0,0.0691534,"t by grid search. The weights of the third/sixth factors are set as 0 by default because they are proposed for target-guided conversation. 1839 rameters, and the parameters of other modules stay intact during RL training. 3.8 NLG As described in Section 3.1, we use the mechanism selected by how-policy to convert x into a response representation r¯. Then we feed the keyword in the selected what-vertex and r¯ into a Seq2BF decoder (Mou et al., 2016) for response generation. Experiments and Results9 4 4.1 Datasets We conduct experiments on two widely used opendomain dialog corpora. Weibo corpus (Shang et al., 2015). This is a large micro-blogging corpora. After data cleaning, we obtain 2.6 million pairs for training, 10k pairs for validation and 10k pairs for testing. We use publicly-available lexical analysis tools10 to obtain POS tag features for this dataset and then we further use this feature to extract keywords from utterances. We use Tencent AI Lab Embedding11 for embedding initialization in models. Persona dialog corpus (Zhang et al., 2018a). This ia a crowd-sourced dialog corpora where each participant plays the part of an assigned persona. To evaluate policy controllability brought by CGPolicy"
2020.acl-main.166,P18-1205,0,0.27686,"o sub-steps: firstly, obtains a response representation r¯ using both M3 and a message representation (from a message-encoder); Next, produces a response “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et al., 2018; Yao et al."
2020.acl-main.374,W11-0705,0,0.0924445,"ed for sentiment analysis, including sentencelevel sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018; Barnes et al., 2019), aspect-level sentiment classification (Vo and Zhang, 2015), opinion extraction (Li and Lam, 2017), emotion analysis (Gui et al., 2017; Fan et al., 2019) and so on. Lexicon-based method (Turney, 2002; Taboada et al., 2011) directly utilizes polarity of sentiment words for classification. Traditional feature-based approaches encode sentiment word information in manually-designed features to improve the supervised models (Pang et al., 2008; Agarwal et al., 2011). In contrast, deep learning approaches enhance the embedding representation with the help of sentiment words (Shin et al., 2017), or absorb the sentiment knowledge through linguistic regularization (Qian et al., 2017; Fan et al., 2019). Aspect-sentiment pair knowledge is also useful for aspect-level classification and opinion extraction. Previous works often provide weak supervision by this type of knowledge, either for aspect7 Conclusion In this paper, we propose Sentiment Knowledge Enhanced Pre-training for sentiment analysis. Sentiment masking and three sentiment pre-training objectives ar"
2020.acl-main.374,W19-6119,0,0.0200401,"t al., 2019) uses WordNet super-senses to improve word-in-context tasks. A different ERNIE (Zhang et al., 2019) exploits entity knowledge for entity-linking and relation classification. 6 Related Work Sentiment Analysis with Knowledge Various types of sentiment knowledge, including sentiment words, word polarity, aspect-sentiment pairs, have been proved to be useful for a wide range of sentiment analysis tasks. Sentiment words with their polarity are widely used for sentiment analysis, including sentencelevel sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018; Barnes et al., 2019), aspect-level sentiment classification (Vo and Zhang, 2015), opinion extraction (Li and Lam, 2017), emotion analysis (Gui et al., 2017; Fan et al., 2019) and so on. Lexicon-based method (Turney, 2002; Taboada et al., 2011) directly utilizes polarity of sentiment words for classification. Traditional feature-based approaches encode sentiment word information in manually-designed features to improve the supervised models (Pang et al., 2008; Agarwal et al., 2011). In contrast, deep learning approaches enhance the embedding representation with the help of sentiment words (Shin et al., 2017), or a"
2020.acl-main.374,N19-1423,0,0.64,"timent analysis refers to the identification of sentiment and opinion contained in the input texts that are often user-generated comments. In practice, sentiment analysis involves a wide range of specific tasks (Liu, 2012), such as sentence-level sentiment classification, aspect-level sentiment classification, opinion extraction and so on. Traditional methods often study these tasks separately and design specific models for each task, based on manuallydesigned features (Liu, 2012) or deep learning (Zhang et al., 2018). Recently, pre-training methods (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019) have shown their powerfulness in learning general semantic representations, and have remarkably improved most natural language processing (NLP) tasks like sentiment analysis. These methods build unsupervised objectives at word-level, such as masking strategy (Devlin et al., 2019), next-word prediction (Radford et al., 2018) or permutation (Yang et al., 2019). Such wordprediction-based objectives have shown great abilities to capture dependency between words and syntactic structures (Jawahar et al., 2019). However, as the sentiment information of a text is seldom explicitly"
2020.acl-main.374,D19-1563,0,0.104905,"from other NLP tasks in that it deals mainly with user reviews other than news texts. There are many specific sentiment tasks, and these tasks usually depend on different types of sentiment knowledge including sentiment words, word polarity and aspect-sentiment pairs. The importance of these knowledge has been verified by tasks at different level, for instance, sentence-level sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018), aspect-level sentiment classification (Vo and Zhang, 2015; Zeng et al., 2019), opinion extraction (Li and Lam, 2017; Gui et al., 2017; Fan et al., 2019) and so on. Therefore, we assume that, by integrating these knowledge into the pre-training process, the learned representation would be more sentimentspecific and appropriate for sentiment analysis. In order to learn a unified sentiment representation for multiple sentiment analysis tasks, we propose Sentiment Knowledge Enhanced Pre-training (SKEP), where sentiment knowledge about words, polarity, and aspect-sentiment pairs are included to guide the process of pre-training. The sentiment knowledge is first automatically mined from unlabeled data (Section 3.1). With the knowledge 4067 Proceedi"
2020.acl-main.374,D17-1167,0,0.177221,"analysis differs from other NLP tasks in that it deals mainly with user reviews other than news texts. There are many specific sentiment tasks, and these tasks usually depend on different types of sentiment knowledge including sentiment words, word polarity and aspect-sentiment pairs. The importance of these knowledge has been verified by tasks at different level, for instance, sentence-level sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018), aspect-level sentiment classification (Vo and Zhang, 2015; Zeng et al., 2019), opinion extraction (Li and Lam, 2017; Gui et al., 2017; Fan et al., 2019) and so on. Therefore, we assume that, by integrating these knowledge into the pre-training process, the learned representation would be more sentimentspecific and appropriate for sentiment analysis. In order to learn a unified sentiment representation for multiple sentiment analysis tasks, we propose Sentiment Knowledge Enhanced Pre-training (SKEP), where sentiment knowledge about words, polarity, and aspect-sentiment pairs are included to guide the process of pre-training. The sentiment knowledge is first automatically mined from unlabeled data (Section 3.1). With the know"
2020.acl-main.374,D19-1355,0,0.0379178,"al., 2019), XLNet (Yang et al., 2019) and so on. Among them, BERT pre-trains a bidirectional transformer by randomly masked word prediction, and have shown strong performance gains. RoBERTa (Liu et al., 2019) further improves BERT by robust optimization, and become one of the best pre-training methods. Inspired by BERT, some works propose finegrained objectives beyond random word masking. SpanBERT (Joshi et al., 2019) masks the span of words at the same time. ERNIE (Sun et al., 2019) proposes to mask entity words. On the other hand, pre-training for specific tasks is also studied. GlossBERT (Huang et al., 2019) exploits gloss knowledge to improve word sense disambiguation. SenseBERT (Levine et al., 2019) uses WordNet super-senses to improve word-in-context tasks. A different ERNIE (Zhang et al., 2019) exploits entity knowledge for entity-linking and relation classification. 6 Related Work Sentiment Analysis with Knowledge Various types of sentiment knowledge, including sentiment words, word polarity, aspect-sentiment pairs, have been proved to be useful for a wide range of sentiment analysis tasks. Sentiment words with their polarity are widely used for sentiment analysis, including sentencelevel se"
2020.acl-main.374,2021.ccl-1.108,0,0.22006,"Missing"
2020.acl-main.374,N18-1054,0,0.0617166,"Missing"
2020.acl-main.374,N18-1202,0,0.312695,"/github.com/baidu/Senta. 1 Introduction Sentiment analysis refers to the identification of sentiment and opinion contained in the input texts that are often user-generated comments. In practice, sentiment analysis involves a wide range of specific tasks (Liu, 2012), such as sentence-level sentiment classification, aspect-level sentiment classification, opinion extraction and so on. Traditional methods often study these tasks separately and design specific models for each task, based on manuallydesigned features (Liu, 2012) or deep learning (Zhang et al., 2018). Recently, pre-training methods (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019) have shown their powerfulness in learning general semantic representations, and have remarkably improved most natural language processing (NLP) tasks like sentiment analysis. These methods build unsupervised objectives at word-level, such as masking strategy (Devlin et al., 2019), next-word prediction (Radford et al., 2018) or permutation (Yang et al., 2019). Such wordprediction-based objectives have shown great abilities to capture dependency between words and syntactic structures (Jawahar et al., 2019). However, as the sentiment"
2020.acl-main.374,S14-2004,0,0.0866188,"3 Aspect-Level Sem-L Sem-R 78.11 84.93 78.89 85.77 80.13 86.92 80.32 87.25 81.32 87.92 81.19 87.71 Opinion Role MPQA-Holder MPQA-Target 81.89/77.34 80.23/72.19 82.71/77.71 80.86/73.01 82.95/77.63 81.18/73.15 82.97/77.82 81.09/73.24 84.25/79.03 82.77/74.82 84.01/78.36 82.69/74.36 Table 4: Effectiveness of objectives. SW, WP, AP refers to pre-training objectives: Sentiment Word prediction, Word Polarity prediction and Aspect-sentiment Pair prediction. “Random Token” denotes random token masking used in RoBERTa. AP-I denotes predicting words in an Aspect-sentiment Pair Independently. 2014 Task4 (Pontiki et al., 2014). This task contains both restaurant domain and laptop domain, whose accuracy is evaluated separately. (3) For opinion role labeling, MPQA 2.0 dataset (Wiebe et al., 2005; Wilson, 2008) is used. MPQA aims to extract the targets or the holders of the opinions. Here we follow the method of evaluation in SRL4ORL (Marasovi´c and Frank, 2018), which is released and available online. 4-folder crossvalidation is performed, and the F-1 scores of both holder and target are reported. To perform sentiment pre-training of SKEP, the training part of Amazon-2 is used, which is the largest dataset among the"
2020.acl-main.374,P17-1154,0,0.0171678,"nion extraction (Li and Lam, 2017), emotion analysis (Gui et al., 2017; Fan et al., 2019) and so on. Lexicon-based method (Turney, 2002; Taboada et al., 2011) directly utilizes polarity of sentiment words for classification. Traditional feature-based approaches encode sentiment word information in manually-designed features to improve the supervised models (Pang et al., 2008; Agarwal et al., 2011). In contrast, deep learning approaches enhance the embedding representation with the help of sentiment words (Shin et al., 2017), or absorb the sentiment knowledge through linguistic regularization (Qian et al., 2017; Fan et al., 2019). Aspect-sentiment pair knowledge is also useful for aspect-level classification and opinion extraction. Previous works often provide weak supervision by this type of knowledge, either for aspect7 Conclusion In this paper, we propose Sentiment Knowledge Enhanced Pre-training for sentiment analysis. Sentiment masking and three sentiment pre-training objectives are designed to incorporate various types of knowledge for pre-training model. Thought conceptually simple, SKEP is empirically highly effective. SKEP significantly outperforms strong pre-training baseline RoBERTa, and"
2020.acl-main.374,P19-1356,0,0.0235741,"ently, pre-training methods (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019) have shown their powerfulness in learning general semantic representations, and have remarkably improved most natural language processing (NLP) tasks like sentiment analysis. These methods build unsupervised objectives at word-level, such as masking strategy (Devlin et al., 2019), next-word prediction (Radford et al., 2018) or permutation (Yang et al., 2019). Such wordprediction-based objectives have shown great abilities to capture dependency between words and syntactic structures (Jawahar et al., 2019). However, as the sentiment information of a text is seldom explicitly studied, it is hard to expect such pre-trained general representations to deliver optimal results for sentiment analysis (Tang et al., 2014). Sentiment analysis differs from other NLP tasks in that it deals mainly with user reviews other than news texts. There are many specific sentiment tasks, and these tasks usually depend on different types of sentiment knowledge including sentiment words, word polarity and aspect-sentiment pairs. The importance of these knowledge has been verified by tasks at different level, for instan"
2020.acl-main.374,W17-5220,0,0.101293,"tly studied, it is hard to expect such pre-trained general representations to deliver optimal results for sentiment analysis (Tang et al., 2014). Sentiment analysis differs from other NLP tasks in that it deals mainly with user reviews other than news texts. There are many specific sentiment tasks, and these tasks usually depend on different types of sentiment knowledge including sentiment words, word polarity and aspect-sentiment pairs. The importance of these knowledge has been verified by tasks at different level, for instance, sentence-level sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018), aspect-level sentiment classification (Vo and Zhang, 2015; Zeng et al., 2019), opinion extraction (Li and Lam, 2017; Gui et al., 2017; Fan et al., 2019) and so on. Therefore, we assume that, by integrating these knowledge into the pre-training process, the learned representation would be more sentimentspecific and appropriate for sentiment analysis. In order to learn a unified sentiment representation for multiple sentiment analysis tasks, we propose Sentiment Knowledge Enhanced Pre-training (SKEP), where sentiment knowledge about words, polarity, and aspect-sentiment pair"
2020.acl-main.374,D13-1170,0,0.0175205,"Labeling This task is to detect fine-grained opinion, such as holder and target, from input texts. Following SRL4ORL (Marasovi´c and Frank, 2018), this task is converted into sequence labeling, which uses BIOS scheme for labeling, and a CRF-layer is added to predict the labels.5 5 Experiment 5.1 Dataset and Evaluation A variety of English sentiment analysis datasets are used in this paper. Table 1 summarizes the statistics of the datasets used in the experiments. These datasets contain three types of tasks: (1) For sentence-level sentiment classification, Standford Sentiment Treebank (SST-2) (Socher et al., 2013) and Amazon-2 (Zhang et al., 2015) are used. In Amazon-2, 400k of the original training data are reserved for development. The performance is evaluated in terms of accuracy. (2) Aspect-level sentiment classification is evaluated on Semantic Eval 5 All the pretraining models, including our SKEP and baselines use CRF-Layer here, thus their performances are comparable. 4071 Model Previous SOTA RoBERTabase RoBERTabase + SKEP RoBERTalarge RoBERTalarge + SKEP Sentence-Level SST-2 Amazon-2 97.11∗ 97.372 94.9 96.61 96.7 96.94 96.5 97.33 97.0 97.56 Aspect-Level Sem-L Sem-R 81.353 87.894 78.11 84.93 81."
2020.acl-main.374,J11-2001,0,0.138897,"text is seldom explicitly studied, it is hard to expect such pre-trained general representations to deliver optimal results for sentiment analysis (Tang et al., 2014). Sentiment analysis differs from other NLP tasks in that it deals mainly with user reviews other than news texts. There are many specific sentiment tasks, and these tasks usually depend on different types of sentiment knowledge including sentiment words, word polarity and aspect-sentiment pairs. The importance of these knowledge has been verified by tasks at different level, for instance, sentence-level sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018), aspect-level sentiment classification (Vo and Zhang, 2015; Zeng et al., 2019), opinion extraction (Li and Lam, 2017; Gui et al., 2017; Fan et al., 2019) and so on. Therefore, we assume that, by integrating these knowledge into the pre-training process, the learned representation would be more sentimentspecific and appropriate for sentiment analysis. In order to learn a unified sentiment representation for multiple sentiment analysis tasks, we propose Sentiment Knowledge Enhanced Pre-training (SKEP), where sentiment knowledge about words, polarity, and as"
2020.acl-main.374,P14-1146,0,0.0354299,"ed most natural language processing (NLP) tasks like sentiment analysis. These methods build unsupervised objectives at word-level, such as masking strategy (Devlin et al., 2019), next-word prediction (Radford et al., 2018) or permutation (Yang et al., 2019). Such wordprediction-based objectives have shown great abilities to capture dependency between words and syntactic structures (Jawahar et al., 2019). However, as the sentiment information of a text is seldom explicitly studied, it is hard to expect such pre-trained general representations to deliver optimal results for sentiment analysis (Tang et al., 2014). Sentiment analysis differs from other NLP tasks in that it deals mainly with user reviews other than news texts. There are many specific sentiment tasks, and these tasks usually depend on different types of sentiment knowledge including sentiment words, word polarity and aspect-sentiment pairs. The importance of these knowledge has been verified by tasks at different level, for instance, sentence-level sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018), aspect-level sentiment classification (Vo and Zhang, 2015; Zeng et al., 2019), opinion extraction (Li and"
2020.acl-main.374,P02-1053,0,0.122795,"iment knowledge in pre-training by virtue of a relatively common mining method. We believe that a more fine-grained method would further improve the quality of knowledge, and this is something we will be exploring in the nearest future. 3.1 3.2 Unsupervised Sentiment Knowledge Mining SKEP mines the sentiment knowledge from unlabeled data. As sentiment knowledge has been the central subject of extensive research, SKEP finds a way to integrate former technique of knowledge mining with pre-training. This paper uses a simple and effective mining method based on Pointwise Mutual Information (PMI) (Turney, 2002). PMI method depends only on a small number of sentiment seed words and the word polarity WP(s) of each seed word s is given. It first builds a collection of candidate word-pairs where each word-pair contains a seed word, and meet with pre-defined part-of-speech patterns as Turney (2002). Then, the co-occurrence of a word-pair is calculated by PMI as follows: PMI(w1 , w2 ) = log p(w1 , w2 ) p(w1 )p(w2 ) Sentiment Masking Sentiment masking aims to construct a corrupted version for each input sequence where sentiment information is masked. Our sentiment masking is directed by sentiment knowledge"
2020.acl-main.374,W18-5446,0,0.0628817,"Missing"
2020.acl-main.374,N19-1036,0,0.100892,"ults for sentiment analysis (Tang et al., 2014). Sentiment analysis differs from other NLP tasks in that it deals mainly with user reviews other than news texts. There are many specific sentiment tasks, and these tasks usually depend on different types of sentiment knowledge including sentiment words, word polarity and aspect-sentiment pairs. The importance of these knowledge has been verified by tasks at different level, for instance, sentence-level sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018), aspect-level sentiment classification (Vo and Zhang, 2015; Zeng et al., 2019), opinion extraction (Li and Lam, 2017; Gui et al., 2017; Fan et al., 2019) and so on. Therefore, we assume that, by integrating these knowledge into the pre-training process, the learned representation would be more sentimentspecific and appropriate for sentiment analysis. In order to learn a unified sentiment representation for multiple sentiment analysis tasks, we propose Sentiment Knowledge Enhanced Pre-training (SKEP), where sentiment knowledge about words, polarity, and aspect-sentiment pairs are included to guide the process of pre-training. The sentiment knowledge is first automaticall"
2020.acl-main.374,P19-1139,0,0.0399134,"u et al., 2019) further improves BERT by robust optimization, and become one of the best pre-training methods. Inspired by BERT, some works propose finegrained objectives beyond random word masking. SpanBERT (Joshi et al., 2019) masks the span of words at the same time. ERNIE (Sun et al., 2019) proposes to mask entity words. On the other hand, pre-training for specific tasks is also studied. GlossBERT (Huang et al., 2019) exploits gloss knowledge to improve word sense disambiguation. SenseBERT (Levine et al., 2019) uses WordNet super-senses to improve word-in-context tasks. A different ERNIE (Zhang et al., 2019) exploits entity knowledge for entity-linking and relation classification. 6 Related Work Sentiment Analysis with Knowledge Various types of sentiment knowledge, including sentiment words, word polarity, aspect-sentiment pairs, have been proved to be useful for a wide range of sentiment analysis tasks. Sentiment words with their polarity are widely used for sentiment analysis, including sentencelevel sentiment classification (Taboada et al., 2011; Shin et al., 2017; Lei et al., 2018; Barnes et al., 2019), aspect-level sentiment classification (Vo and Zhang, 2015), opinion extraction (Li and La"
2020.acl-main.374,D17-1310,0,\N,Missing
2020.acl-main.374,P18-2120,0,\N,Missing
2020.acl-main.555,J05-3002,0,0.302043,"raph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al.,"
2020.acl-main.555,P11-1049,0,0.0446647,"Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (L"
2020.acl-main.555,P15-1153,0,0.155763,"lutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply tr"
2020.acl-main.555,N18-1150,0,0.0346674,"ed (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering th"
2020.acl-main.555,N13-1136,0,0.770568,"ecting redundancy and generating overall coherent summaries for MDS. Graphs that capture ∗ Corresponding author. relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of documents such as similarity graph based on lexical similarities (Erkan and Radev, 2004) and discourse graph based on discourse relations (Christensen et al., 2013), have been widely used in traditional graph-based extractive MDS models. However, they are not well studied by most abstractive approaches, especially the end-to-end neural approaches. Few work has studied the effectiveness of explicit graph representations on neural abstractive MDS. In this paper, we develop a neural abstractive MDS model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries. Our model augments the end-toend neural architecture with the ability to incorporate well-established grap"
2020.acl-main.555,N19-1423,0,0.474382,"ontent. Benefiting from the graph modeling, our model can extract salient information from long documents and generate coherent summaries more effectively. We experiment with three types of graph representations, including similarity graph, topic graph and discourse graph, which all significantly improve the MDS performance. 6232 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6232–6243 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Additionally, our model is complementary to most pre-trained language models (LMs), like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019b). They can be easily combined with our model to process much longer inputs. The combined model adopts the advantages of both our graph model and pre-trained LMs. Our experimental results show that our graph model significantly improves the performance of pre-trained LMs on MDS. The contributions of our paper are as follows: • Our work demonstrates the effectiveness of graph modeling in neural abstractive MDS. We show that explicit graph representations are beneficial for both document representation and summary generation. • We propose"
2020.acl-main.555,N19-1409,0,0.022001,"ument relations, thus achieves significantly better performance.We also leverage the graph structure to guide the summary decoding process, which is beneficial for long summary generation. Additionally, we combine the advantages of pretrained LMs into our model. 2.3 Summarization with Pretrained LMs Pretrained LMs (Peters et al., 2018; Radford et al.; Devlin et al., 2019; Dong et al., 2019; Sun et al., 2019) have recently emerged as a key technology for achieving impressive improvements in a wide variety of natural language tasks, including both language understanding and language generation (Edunov et al., 2019; Rothe et al., 2019). Liu and Lapata (2019b) attempt to incorporate pre-trained BERT encoder into SDS model and achieves significant improvements. Dong et al. (2019) further propose a unified LM for both language understanding and language generation tasks, which achieves state-of-the-art results on several generation tasks including SDS. In this work, we propose an effective method to combine pretrained LMs with our graph model and make them be able to process much longer inputs effectively. 3 Model Description In order to process long source documents more effectively, we follow Liu and Lap"
2020.acl-main.555,P19-1102,0,0.415565,"ansfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input documents as a concatenated flat sequence. Fan et al. (2019) further propose to construct a local knowledge graph from documents and then linearize the graph into a sequence to better sale Seq2Seq models to multidocument inputs. Fabbri et al. (2019) also introduce a middle-scale (about 50K) MDS news dataset (namely MultiNews), and propose an end-to-end model by incorporating traditional MMR-based 6233 Graph Encoding Layer Graph Decoding Layer Add & Normalize Add & Normalize Feed Forward Feed Forward Feed Forward Feed Forward Add & Normalize Add & Normalize Graph-informed Self-Attention Hierarchical Graph Attention PARAGRAPH POSITION ENCODING Add & Normalize Transformer Masked Self-Attention Transformer TOKEN POSITION ENCODING POSITIONAL ENCODING first paragraph last paragraph token1 END Figure 1: Illustration of our model, which follows"
2020.acl-main.555,D19-1428,0,0.119197,"9a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input documents as a concatenated flat sequence. Fan et al. (2019) further propose to construct a local knowledge graph from documents and then linearize the graph into a sequence to better sale Seq2Seq models to multidocument inputs. Fabbri et al. (2019) also introduce a middle-scale (about 50K) MDS news dataset (namely MultiNews), and propose an end-to-end model by incorporating traditional MMR-based 6233 Graph Encoding Layer Graph Decoding Layer Add & Normalize Add & Normalize Feed Forward Feed Forward Feed Forward Feed Forward Add & Normalize Add & Normalize Graph-informed Self-Attention Hierarchical Graph Attention PARAGRAPH POSITION ENCODING Add & Norm"
2020.acl-main.555,D08-1019,0,0.248382,"l. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan"
2020.acl-main.555,D18-1443,0,0.0774954,"de: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2S"
2020.acl-main.555,W11-1608,0,0.0265118,"The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack o"
2020.acl-main.555,D18-1446,0,0.40232,"Missing"
2020.acl-main.555,D15-1219,1,0.902847,"structed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 20"
2020.acl-main.555,D18-1205,1,0.787509,"Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input"
2020.acl-main.555,D18-1441,1,0.688871,"Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input"
2020.acl-main.555,C18-1101,0,0.0528075,"s between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu"
2020.acl-main.555,P04-1077,0,0.0143827,"l Lead LexRank FT BERT+FT XLNet+FT RoBERTa+FT T-DMCA HT GraphSum GraphSum+RoBERTa R-1 38.22 36.12 40.56 41.49 40.85 42.05 40.77 41.53 42.63 42.99 R-2 16.85 11.67 25.35 25.73 25.29 27.00 25.60 26.52 27.70 27.83 Model Lead LexRank PG-BRNN HiMAP FT RoBERTa+FT HT GraphSum R-L 26.89 22.52 34.73 35.59 35.20 36.56 34.90 35.76 36.97 37.36 G.S.(Similarity)+RoBERTa G.S.(Topic)+RoBERTa G.S.(Discourse)+RoBERTa Table 1: Evaluation results on the WikiSum test set using ROUGE F1 . R-1, R-2 and R-L are abbreviations for ROUGE-1, ROUGE-2 and ROUGE-L, respectively. rization quality is evaluated using ROUGE F1 (Lin and Och, 2004). We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) between system summaries and gold references as a means of assessing informativeness, and the longest common subsequence (ROUGE-L2 ) as a means of accessing fluency. Results on WikiSum Table 6 summarizes the evaluation results on the WikiSum dataset. Several strong extractive baselines and abstractive baselines are also evaluated and compared with our models. The first block in the table shows the results of extractive methods Lead and LexRank (Erkan and Radev, 2004). The second block shows the results of abstractive methods: (1) FT"
2020.acl-main.555,P19-1500,0,0.195307,"n Transformer TOKEN POSITION ENCODING POSITIONAL ENCODING first paragraph last paragraph token1 END Figure 1: Illustration of our model, which follows the encoder-deocder architecture. The encoder is a stack of transformer layers and graph encoding layers, while the decoder is a stack of graph decoding layers. We incorporate explicit graph representations into both the graph encoding layers and graph decoding layers. extractive model with a standard Seq2Seq model. The above Seq2Seq models haven’t study the importance of cross-document relations and graph representations in MDS. Most recently, Liu and Lapata (2019a) propose a hierarchical transformer model to utilize the hierarchical structure of documents. They propose to learn cross-document relations based on selfattention mechanism. They also propose to incorporate explicit graph representations into the model by simply replacing the attention weights with a graph matrix, however, it doesn’t achieve obvious improvement according to their experiments. Our work is partly inspired by this work, but our approach is quite different from theirs. In contrast to their approach, we incorporate explicit graph representations into the encoding process via a g"
2020.acl-main.555,D19-1387,0,0.0797698,"n Transformer TOKEN POSITION ENCODING POSITIONAL ENCODING first paragraph last paragraph token1 END Figure 1: Illustration of our model, which follows the encoder-deocder architecture. The encoder is a stack of transformer layers and graph encoding layers, while the decoder is a stack of graph decoding layers. We incorporate explicit graph representations into both the graph encoding layers and graph decoding layers. extractive model with a standard Seq2Seq model. The above Seq2Seq models haven’t study the importance of cross-document relations and graph representations in MDS. Most recently, Liu and Lapata (2019a) propose a hierarchical transformer model to utilize the hierarchical structure of documents. They propose to learn cross-document relations based on selfattention mechanism. They also propose to incorporate explicit graph representations into the model by simply replacing the attention weights with a graph matrix, however, it doesn’t achieve obvious improvement according to their experiments. Our work is partly inspired by this work, but our approach is quite different from theirs. In contrast to their approach, we incorporate explicit graph representations into the encoding process via a g"
2020.acl-main.555,2021.ccl-1.108,0,0.172335,"Missing"
2020.acl-main.555,C16-1143,0,0.164968,"Missing"
2020.acl-main.555,W04-3252,0,0.204168,"ing that graph modeling enables our model process longer inputs with better performance, and graphs with richer relations are more beneficial for MDS.1 2 Related Work 2.1 Graph-based MDS Most previous MDS approaches are extractive, which extract salient textual units from documents based on graph-based representations of sentences. Various ranking methods have been developed to rank textual units based on graphs to select most salient ones for inclusion in the final summary. Erkan and Radev (2004) propose LexRank to compute sentence importance based on a lexical similarity graph of sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been 1 Codes and results are in: https://github.com/ PaddlePaddle/Research/tree/master/NLP/ ACL2020-GraphSum further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009; Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neura"
2020.acl-main.555,P14-1084,0,0.061278,"ased on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straigh"
2020.acl-main.555,W00-1009,0,0.690742,"he summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines. 1 Introduction Multi-document summarization (MDS) brings great challenges to the widely used sequence-tosequence (Seq2Seq) neural architecture as it requires effective representation of multiple input documents and content organization of long summaries. For MDS, different documents may contain the same content, include additional information, and present complementary or contradictory information (Radev, 2000). So different from single document summarization (SDS), cross-document links are very important in extracting salient information, detecting redundancy and generating overall coherent summaries for MDS. Graphs that capture ∗ Corresponding author. relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of docu"
2020.acl-main.555,P17-1099,0,0.154693,"ss. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely Wiki"
2020.acl-main.555,D19-1323,0,0.029316,"lay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input documents as a concatenated flat sequence. Fan et al. (2019) furth"
2020.acl-main.555,D18-1206,0,0.0452166,"e, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input documents as a concatenat"
2020.acl-main.555,D18-1108,0,0.0263741,"nd the Transformer with a hierarchical graph attention mechanism to utilize explicit graph structure to guide the summary decoding process. In the following, we will focus on the graph encoding layer and graph decoding layer of our model. 3.1 Graph Encoding Layer As shown in Figure 1, based on the output of the token-level transformer encoding layers, the graph encoding layer is used to encode all documents globally. Most existing neural work only utilizes attention mechanism to learn latent graph representations of documents where the graph edges are attention weights (Liu and Lapata, 2019a; Niculae et al., 2018; Fernandes et al., 2018). However, much work in traditional MDS has shown that explicit graph representations are very beneficial to MDS. Different types of graphs capture different kinds of semantic relations (e.g. lexical relations or discourse relations), which can help the model focus on different facets of the summarization task. In this work, we propose to incorporate explicit graph representations into the neural encoding process via a graph-informed attention mechanism. It takes advantage of the explicit relations in graphs to learn better inter-paragraph relations. Each paragraph can"
2020.acl-main.555,P19-1504,0,0.0828261,"on extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input documents as a concatenated flat sequence. Fan et al. (2019) further propose to construct a local k"
2020.acl-main.555,N18-1202,0,0.0230266,"pproach is quite different from theirs. In contrast to their approach, we incorporate explicit graph representations into the encoding process via a graphinformed attention mechanism. Under the guidance of explicit relations in graphs, our model can learn better and richer cross-document relations, thus achieves significantly better performance.We also leverage the graph structure to guide the summary decoding process, which is beneficial for long summary generation. Additionally, we combine the advantages of pretrained LMs into our model. 2.3 Summarization with Pretrained LMs Pretrained LMs (Peters et al., 2018; Radford et al.; Devlin et al., 2019; Dong et al., 2019; Sun et al., 2019) have recently emerged as a key technology for achieving impressive improvements in a wide variety of natural language tasks, including both language understanding and language generation (Edunov et al., 2019; Rothe et al., 2019). Liu and Lapata (2019b) attempt to incorporate pre-trained BERT encoder into SDS model and achieves significant improvements. Dong et al. (2019) further propose a unified LM for both language understanding and language generation tasks, which achieves state-of-the-art results on several generat"
2020.acl-main.555,P13-1137,0,0.124191,"Missing"
2020.acl-main.555,K17-1045,0,0.0407222,"f sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been 1 Codes and results are in: https://github.com/ PaddlePaddle/Research/tree/master/NLP/ ACL2020-GraphSum further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009; Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova"
2020.acl-main.555,D08-1079,0,\N,Missing
2020.acl-main.9,P19-1535,1,0.785728,"e obtained through the following inference procedure: 1) Candidate Response Generation – Conditioned on each latent value z ∈ [1, K], generate corresponding candidate response r. 2) Response Selection – Calculate the probability for each response p(lr = 1|c, r) and select the one with highest coherence value as the final response. It is worth noting that the above fine-tuning and inference procedures are set up for the dialogue generation without any specific objectives. If there exists a specific objective within the conversation, such as letting both participants know more about each other (Bao et al., 2019), the fine-tuning can proceed to maximize the pre-defined rewards with reinforcement learning (RL). Under such circumstances, our latent discrete variable can be naturally treated as action within RL, and thus the response selection can be straightforwardly solved by selecting the action that results in the maximum reward. 3 3.1 3.1.2 Compared Methods The following models have been compared in the experiments. Baseline. Sequence to sequence with attention (Seq2Seq) (Vinyals and Le, 2015) is employed as the baseline for the experiments on Persona-Chat and Daily Dialog. DSTC7-AVSD has provided a"
2020.acl-main.9,W14-3346,0,0.0119552,"BERT Bi-direction Bi-direction Twitter & Reddit ✓ 15.080 12.936 12.285 Table 5: Perplexity of different pre-trained models on Persona-Chat, with best value written in bold. 3.1.3 Evaluation Metrics the response’s final score is determined via majority voting. The average Fleiss’s kappa (Fleiss and Cohen, 1973) on Persona-Chat and Daily Dialog is 0.515 and 0.480 respectively, indicating annotators have reached moderate agreement. Both automatic and human evaluations are employed to assess the performance of compared methods. In automatic evaluation, the following metrics are included: • BLEU (Chen and Cherry, 2014) measures the n-gram overlap between generated response and the target response. • Distinct-1/2 (Li et al., 2016) measures the generation diversity, which is defined as the number of distinct uni- or bi-grams divided by the total amount of generated words. • Knowledge Recall/Precision/F1 (Dinan et al., 2019b) measures the degree of informativeness w.r.t. background knowledge. • In DSTC7-AVSD, the MSCOCO platform (Chen et al., 2015) is employed for evaluation. It compares the generated response with six ground truth responses, using metrics of BLEU, METEOR, ROUGH-L and CIDEr. In human evaluatio"
2020.acl-main.9,I17-1099,0,0.0585933,"former blocks in our model L is 12 and the hidden embedding dimension D is 768. The batch size is set to 64 and K is set to 20 for the discrete latent variable. Adam optimizer (Kingma and Ba, 2015) is employed for optimization with a learning rate of 5e-5. The pretraining of dialogue generation was carried out on 8 Nvidia Telsa V100 32G GPU cards for 3.5M steps, taking about two weeks to reach convergence. 2.5 both manually annotated conversations and corresponding persona profiles (background knowledge), where two participants chat naturally and try to get to know each other. • Daily Dialog (Li et al., 2017) is a chit-chat dataset, which contains high-quality human conversations about daily life. • DSTC7-AVSD (Alamri et al., 2019), short for Audio Visual Scene-aware Dialog of the DSTC7 challenge, is a conversational question answering dataset. In DSTC7-AVSD, the system need to generate an answer given dialogue context and background knowledge. There are two available options of knowledge utilization: 1) using singlemodal information of text only, including video’s caption and summary; 2) relying on multi-modal information, including text, audio and visual features. The single-modal option is adop"
2020.acl-main.9,D19-1407,0,0.306322,"Missing"
2020.acl-main.9,D16-1230,0,0.0969151,"Missing"
2020.acl-main.9,N19-1125,0,0.0341595,"Missing"
2020.acl-main.9,P19-1534,0,0.0578899,"Missing"
2020.acl-main.9,P19-1608,0,0.188466,"straightforwardly solved by selecting the action that results in the maximum reward. 3 3.1 3.1.2 Compared Methods The following models have been compared in the experiments. Baseline. Sequence to sequence with attention (Seq2Seq) (Vinyals and Le, 2015) is employed as the baseline for the experiments on Persona-Chat and Daily Dialog. DSTC7-AVSD has provided a baseline system, which is built upon hierarchical recurrent encoders with multi-modal features. State of the art. Persona-Chat was also utilized in the ConvAI2 challenge (Dinan et al., 2019a), where the team of Lost in Conversation (LIC) (Golovanov et al., 2019) obtains the best performance. LIC is also one transformer based generation method and fine-tuned upon the pre-trained model of GPT (Radford et al., 2018). For the dataset of Daily Dialog, its best results are reported by the recently developed method – iVAEMI (Fang et al., 2019), which generates diverse responses with samplebased latent representation. In DSTC7-AVSD, the team of CMU (Sanabria et al., 2019) obtains the best performance across all the evaluation metrics. Our method. To better analyze the effects of our latent discrete variable, we also compare to the version without latent vari"
2020.acl-main.9,N18-2008,0,0.0436583,"ifference in training mode, a flexible paradigm integrating uni- and bi-directional processing is employed in this work, which is inspired by the latest unified language modeling (Dong et al., 2019). Thirdly, a discrete latent variable is introduced to model the one-to-many relationship among utterances in conversations. Each value of the latent variable corresponds to the particular conversational intent of one response, which is referred as latent speech act. Distinct with those controllable dialogue generation based on explicit labels (including emotion, keywords, domain codes, and so on) (Huang et al., 2018; Keskar et al., 2019), our latent variable gets Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one"
2020.acl-main.9,N16-1014,0,0.0833488,"dels on Persona-Chat, with best value written in bold. 3.1.3 Evaluation Metrics the response’s final score is determined via majority voting. The average Fleiss’s kappa (Fleiss and Cohen, 1973) on Persona-Chat and Daily Dialog is 0.515 and 0.480 respectively, indicating annotators have reached moderate agreement. Both automatic and human evaluations are employed to assess the performance of compared methods. In automatic evaluation, the following metrics are included: • BLEU (Chen and Cherry, 2014) measures the n-gram overlap between generated response and the target response. • Distinct-1/2 (Li et al., 2016) measures the generation diversity, which is defined as the number of distinct uni- or bi-grams divided by the total amount of generated words. • Knowledge Recall/Precision/F1 (Dinan et al., 2019b) measures the degree of informativeness w.r.t. background knowledge. • In DSTC7-AVSD, the MSCOCO platform (Chen et al., 2015) is employed for evaluation. It compares the generated response with six ground truth responses, using metrics of BLEU, METEOR, ROUGH-L and CIDEr. In human evaluation, we randomly select 100 dialogue contexts and generate responses with compared methods. Three crowd-sourcing wo"
2020.acl-main.9,P18-1205,0,0.0327722,"rted by the recently developed method – iVAEMI (Fang et al., 2019), which generates diverse responses with samplebased latent representation. In DSTC7-AVSD, the team of CMU (Sanabria et al., 2019) obtains the best performance across all the evaluation metrics. Our method. To better analyze the effects of our latent discrete variable, we also compare to the version without latent variable (Our w/o Latent).2 Experiments Settings 3.1.1 Datasets To evaluate the performance of our proposed method, comprehensive experiments have been carried out on three publicly available datasets. • Persona-Chat (Zhang et al., 2018) is a knowledge grounded conversation dataset. It provides 2 It shares the same training settings as our method with latent variables: network parameters are first initialized with BERTBASE , and the pre-training is further carried out on Reddit and Twitter. The only difference lies in the incorporation of latent variable. 89 Dataset Type Knowledge # Train # Valid # Test Persona-Chat Chit-chat with persona Persona profiles 8,939 dialogues 131,438 turns 1,000 dialogues 15,602 turns 968 dialogues 15,024 turns Daily Dialog Chit-chat N/A 11,118 dialogues 87,170 turns 1,000 dialogues 8,069 turns 1,"
2020.acl-main.9,P18-1101,0,0.0622304,"Missing"
2020.acl-main.9,P17-1061,0,0.305634,"rectional decoding flexibly via specific selfattention masks. Both response generation and latent act recognition are carried out under the unified network with shared parameters. Their detailed implementations are described as follows. Given the context c and a specific speech act z, the response generation can be estimated as Dialogue Generation Pre-training Given a piece of context, there exist multiple appropriate responses, leading to diverse conversation flows. It is widely recognized that the capability of modeling one-to-many relationship is crucial for the dialogue generation system (Zhao et al., 2017; Chen et al., 2019). To this end, we propose to encode discrete latent variables into transformer blocks for one-to-many relationship modeling, where two reciprocal tasks of response generation and latent act recognition are collaboratively carried out. 2.1 $()|&quot;, #) It is snowing outside. exempted from the restriction of human annotations and can be learned automatically from the corpus in an unsupervised way. In the pre-training of dialogue generation, response generation and latent act recognition are carried out simultaneously within a shared network. Based on the context and latent varia"
2020.acl-main.98,N16-1014,0,0.232101,"nate the goal predicted by our model, all the related knowledge and the dialog context as its input. Turn-level results Dialog-level results Methods↓ Metrics→ Fluency Appro. Infor. Proactivity Goal success rate Coherence S2S +gl. +kg. MGCG R +gl. +kg. MGCG G +gl. +kg. 1.08 1.98 1.94 0.23 0.60 0.75 0.37 1.28 1.68 0.94 1.22 1.34 0.37 0.68 0.82 0.49 0.83 0.91 Table 5: Human evaluation results at the level of turns and dialogs. 5.3 Automatic Evaluations Metrics For automatic evaluation, we use several common metrics such as BLEU (Papineni et al., 2002), F1, perplexity (PPL), and DISTINCT (DIST2) (Li et al., 2016) to measure the relevance, fluency, and diversity of generated responses. Following the setting in previous work (Wu et al., 2019; Zhang et al., 2018a), we also measure the performance of all models using Hits@1 and Hits@3.6 Here we let each model to select the best response from 10 candidates. Those 10 candidate responses consist of the ground-truth response generated by humans and nine randomly sampled ones from the training set. Moreover, we also evaluate the knowledge-selection capability of each model by calculating knowledge precision/recall/F1 scores as done in Wu et al. (2019).7 In add"
2020.acl-main.98,D18-1255,0,0.356097,"The goal-planning module can conduct dialog management to control the dialog 1037 • We identify the task of conversational recommendation over multi-type dialogs. • To facilitate the study of this task, we create a novel dialog dataset DuRecDial, with rich variability of dialog types and domains as shown in Table 1. • We propose a conversation generation framework with a novel mixed-goal driven dialog policy mechanism. Datasets↓ Metrics→ Facebook Rec (Dodge et al., 2016) REDIAL (Li et al., 2018) GoRecDial (Kang et al., 2019) OpenDialKG (Moon et al., 2019) CMU DoG (Zhou et al., 2018a) IIT DoG (Moghe et al., 2018) Wizard-of-wiki (Dinan et al., 2019) OpenDialKG (Moon et al., 2019) DuConv (Wu et al., 2019) KdConv (Zhou et al., 2020) DuRecDial #Dialogs #Utterances Dialog types Domains 1M 10k 9k 12k 4k 9k 22k 3k 29k 4.5k 10.2k 6M 163k 170k 143k 130k 90k 202k 38k 270k 86k 156k Rec. Rec., chitchat Rec. Rec. Chitchat Chitchat Chitchat Chitchat Chitchat Chitchat Rec., chitchat, QA, task Movie Movie Movie Movie, book Movie Movie 1365 Wikipedia topics Sports, music Movie Movie, music, travel Movie, music, movie star, food, restaurant, news, weather User profile No No Yes No No No No No No No Yes Table 1: Compari"
2020.acl-main.98,P19-1081,0,0.263116,"oal planning module and a goal-guided responding module. The goal-planning module can conduct dialog management to control the dialog 1037 • We identify the task of conversational recommendation over multi-type dialogs. • To facilitate the study of this task, we create a novel dialog dataset DuRecDial, with rich variability of dialog types and domains as shown in Table 1. • We propose a conversation generation framework with a novel mixed-goal driven dialog policy mechanism. Datasets↓ Metrics→ Facebook Rec (Dodge et al., 2016) REDIAL (Li et al., 2018) GoRecDial (Kang et al., 2019) OpenDialKG (Moon et al., 2019) CMU DoG (Zhou et al., 2018a) IIT DoG (Moghe et al., 2018) Wizard-of-wiki (Dinan et al., 2019) OpenDialKG (Moon et al., 2019) DuConv (Wu et al., 2019) KdConv (Zhou et al., 2020) DuRecDial #Dialogs #Utterances Dialog types Domains 1M 10k 9k 12k 4k 9k 22k 3k 29k 4.5k 10.2k 6M 163k 170k 143k 130k 90k 202k 38k 270k 86k 156k Rec. Rec., chitchat Rec. Rec. Chitchat Chitchat Chitchat Chitchat Chitchat Chitchat Rec., chitchat, QA, task Movie Movie Movie Movie, book Movie Movie 1365 Wikipedia topics Sports, music Movie Movie, music, travel Movie, music, movie star, food, restaurant, news, weather User p"
2020.acl-main.98,P02-1040,0,0.106605,"nts “with(without) knowledge”. For “S2S +gl.+kg.”, we simply concatenate the goal predicted by our model, all the related knowledge and the dialog context as its input. Turn-level results Dialog-level results Methods↓ Metrics→ Fluency Appro. Infor. Proactivity Goal success rate Coherence S2S +gl. +kg. MGCG R +gl. +kg. MGCG G +gl. +kg. 1.08 1.98 1.94 0.23 0.60 0.75 0.37 1.28 1.68 0.94 1.22 1.34 0.37 0.68 0.82 0.49 0.83 0.91 Table 5: Human evaluation results at the level of turns and dialogs. 5.3 Automatic Evaluations Metrics For automatic evaluation, we use several common metrics such as BLEU (Papineni et al., 2002), F1, perplexity (PPL), and DISTINCT (DIST2) (Li et al., 2016) to measure the relevance, fluency, and diversity of generated responses. Following the setting in previous work (Wu et al., 2019; Zhang et al., 2018a), we also measure the performance of all models using Hits@1 and Hits@3.6 Here we let each model to select the best response from 10 candidates. Those 10 candidate responses consist of the ground-truth response generated by humans and nine randomly sampled ones from the training set. Moreover, we also evaluate the knowledge-selection capability of each model by calculating knowledge p"
2020.acl-main.98,P13-2089,0,0.526547,"ender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior. This dataset allows us to systematically investigate different parts of the overall problem, e.g., how to naturally lead a dialog, how to interact with users for recommendation. Finally we establish baseline results on DuRecDial for future studies.1 1 Introduction In recent years, there has been a significant increase in the work of conversational recommendation due to the rise of voice-based bots (Christakopoulou et al., 2016; Li et al., 2018; Reschke et al., 2013; Warnestal, 2005). They focus on how to provide high-quality recommendations through dialog-based interactions with users. These work fall into two categories: (1) task-oriented dialogmodeling approaches (Christakopoulou et al., 2016; Sun and Zhang, 2018; Warnestal, 2005); (2) nontask dialog-modeling approaches with more freeform interactions (Kang et al., 2019; Li et al., 2018). ∗ This work was done at Baidu. Corresponding author: Wanxiang Che. 1 Dataset and codes are publicly available at https://github.com/PaddlePaddle/models/ tree/develop/PaddleNLP/Research/ACL2020-DuRecDial. † Almost all"
2020.acl-main.98,P19-1565,0,0.111519,"Missing"
2020.acl-main.98,D19-1203,0,0.344356,"on DuRecDial for future studies.1 1 Introduction In recent years, there has been a significant increase in the work of conversational recommendation due to the rise of voice-based bots (Christakopoulou et al., 2016; Li et al., 2018; Reschke et al., 2013; Warnestal, 2005). They focus on how to provide high-quality recommendations through dialog-based interactions with users. These work fall into two categories: (1) task-oriented dialogmodeling approaches (Christakopoulou et al., 2016; Sun and Zhang, 2018; Warnestal, 2005); (2) nontask dialog-modeling approaches with more freeform interactions (Kang et al., 2019; Li et al., 2018). ∗ This work was done at Baidu. Corresponding author: Wanxiang Che. 1 Dataset and codes are publicly available at https://github.com/PaddlePaddle/models/ tree/develop/PaddleNLP/Research/ACL2020-DuRecDial. † Almost all these work focus on a single type of dialogs, either task oriented dialogs for recommendation, or recommendation oriented open-domain conversation. Moreover, they assume that both sides in the dialog (especially the user) are aware of the conversational goal from the beginning. In many real-world applications, there are multiple dialog types in human-bot conver"
2020.acl-main.98,D14-1181,0,0.0103428,"Current goal prediction (a) Goal-planning module Figure 3: The architecture of our multi-goal driven conversation generation framework (denoted as MGCG). K. These goals will be used as answers for training of the goal-planning module, while the tuples of [context, a ground-truth goal, K, response] will be used for training of the responding module. 4.2 Goal-planning Model As shown in Figure 3(a), we divide the task of goal planning into two sub-tasks, goal completion estimation, and current goal prediction. Goal completion estimation For this subtask, we use Convolutional neural network (CNN)(Kim, 2014) to estimate the probability of goal completion by: PGC (l = 1|X, gt−1 ). (1) Current goal prediction If gt−1 is not completed (PGC &lt; 0.5), then gc = gt−1 , where gc is the goal for Y . Otherwise, we use CNN based multi-task classification to predict the current goal by maximizing the following probability: gt = arg max PGP (g ty , g tp |X, G 0 , Pi k , K), (2) gc = gt , (3) s g ty ,g tp where g ty is a candidate dialog type and g tp is a candidate dialog topic. 4.3 Retrieval-based Response Model we modify the original retrieval model to suit our task by emphasizing the use of goals. As shown"
2020.acl-main.98,D14-1007,1,0.863561,"m/PaddlePaddle/models/ tree/develop/PaddleNLP/Research/ACL2020-DuRecDial. † Almost all these work focus on a single type of dialogs, either task oriented dialogs for recommendation, or recommendation oriented open-domain conversation. Moreover, they assume that both sides in the dialog (especially the user) are aware of the conversational goal from the beginning. In many real-world applications, there are multiple dialog types in human-bot conversations (called multi-type dialogs), such as chit-chat, task oriented dialogs, recommendation dialogs, and even question answering (Ram et al., 2018; Wang et al., 2014; Zhou et al., 2018b). Therefore it is crucial to study how to proactively and naturally make conversational recommendation by the bots in the context of multi-type human-bot communication. For example, the bots could proactively make recommendations after question answering or a task dialog to improve user experience, or it could lead a dialog from chitchat to approach a given product as commercial advertisement. However, to our knowledge, there is less previous work on this problem. To address this challenge, we present a novel task, conversational recommendation over multitype dialogs, wher"
2020.acl-main.98,P19-1369,1,0.765759,"the task of conversational recommendation over multi-type dialogs. • To facilitate the study of this task, we create a novel dialog dataset DuRecDial, with rich variability of dialog types and domains as shown in Table 1. • We propose a conversation generation framework with a novel mixed-goal driven dialog policy mechanism. Datasets↓ Metrics→ Facebook Rec (Dodge et al., 2016) REDIAL (Li et al., 2018) GoRecDial (Kang et al., 2019) OpenDialKG (Moon et al., 2019) CMU DoG (Zhou et al., 2018a) IIT DoG (Moghe et al., 2018) Wizard-of-wiki (Dinan et al., 2019) OpenDialKG (Moon et al., 2019) DuConv (Wu et al., 2019) KdConv (Zhou et al., 2020) DuRecDial #Dialogs #Utterances Dialog types Domains 1M 10k 9k 12k 4k 9k 22k 3k 29k 4.5k 10.2k 6M 163k 170k 143k 130k 90k 202k 38k 270k 86k 156k Rec. Rec., chitchat Rec. Rec. Chitchat Chitchat Chitchat Chitchat Chitchat Chitchat Rec., chitchat, QA, task Movie Movie Movie Movie, book Movie Movie 1365 Wikipedia topics Sports, music Movie Movie, music, travel Movie, music, movie star, food, restaurant, news, weather User profile No No Yes No No No No No No No Yes Table 1: Comparison of our dataset DuRecDial to recommendation dialog datasets and knowledge grounded dialog"
2020.acl-main.98,D18-1076,0,\N,Missing
2020.acl-main.98,2020.acl-main.635,0,\N,Missing
2020.emnlp-main.178,D18-1337,0,0.2376,"Missing"
2020.emnlp-main.178,P19-1126,0,0.373191,"Missing"
2020.emnlp-main.178,N18-2079,0,0.0758748,"el adaptive segmentation policy for simultaneous translation. The policy learns to segment the source text by considering possible translations produced by the translation model, maintaining consistency between the segmentation and translation. Experimental results on Chinese-English and German-English translation show that our method achieves a better accuracy-latency trade-off over recently proposed state-of-the-art methods. • Fixed Policies are hard policies that follow a pre-defined schedule independent of the context. They segment the source text based on a fixed length (Ma et al., 2019; Dalvi et al., 2018). For example, the wait-k method (Ma et al., 2019) first reads k source words, and then generates one target word immediately after each subsequent word is received. Policies of this type are simple and easy to implement. However, they do not consider contextual information and usually result in a drop in translation accuracy. • Adaptive Policies learn to do segmentation according to dynamic contextual information. They either use a specific model to chunk the streaming source text (Sridhar et al., 2013; Oda et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Zheng et al., 2019a, 2020) or jo"
2020.emnlp-main.178,D14-1140,0,0.262784,"Missing"
2020.emnlp-main.178,E17-1099,0,0.488904,"ixed length (Ma et al., 2019; Dalvi et al., 2018). For example, the wait-k method (Ma et al., 2019) first reads k source words, and then generates one target word immediately after each subsequent word is received. Policies of this type are simple and easy to implement. However, they do not consider contextual information and usually result in a drop in translation accuracy. • Adaptive Policies learn to do segmentation according to dynamic contextual information. They either use a specific model to chunk the streaming source text (Sridhar et al., 2013; Oda et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Zheng et al., 2019a, 2020) or jointly learn segmentation and translation in an end-to-end framework (Arivazhagan et al., 2019; Zheng et al., 2019b; Ma et al., 2020). The adaptive methods are more flexible than the fixed ones and achieve state-of-the-art. 1 Introduction In recent years, simultaneous translation has attracted increasing interest both in research and industry community. It aims at a real-time translation that demands high translation quality and an as-short-as-possible delay between speech and translation output. A typical simultaneous translation system consists of an auto-spe"
2020.emnlp-main.178,P14-2090,0,0.329879,"They segment the source text based on a fixed length (Ma et al., 2019; Dalvi et al., 2018). For example, the wait-k method (Ma et al., 2019) first reads k source words, and then generates one target word immediately after each subsequent word is received. Policies of this type are simple and easy to implement. However, they do not consider contextual information and usually result in a drop in translation accuracy. • Adaptive Policies learn to do segmentation according to dynamic contextual information. They either use a specific model to chunk the streaming source text (Sridhar et al., 2013; Oda et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Zheng et al., 2019a, 2020) or jointly learn segmentation and translation in an end-to-end framework (Arivazhagan et al., 2019; Zheng et al., 2019b; Ma et al., 2020). The adaptive methods are more flexible than the fixed ones and achieve state-of-the-art. 1 Introduction In recent years, simultaneous translation has attracted increasing interest both in research and industry community. It aims at a real-time translation that demands high translation quality and an as-short-as-possible delay between speech and translation output. A typical simultaneous tr"
2020.emnlp-main.178,P02-1040,0,0.111276,"uture words, which is less than m. Our training follows the pre-training and fine-tuning framework (Devlin et al., 2018; Sun et al., 2019). • MILk (Arivazhagan et al., 2019): using hard attention to schedule the policy and train the policy together with the NMT model in an end-to-end framework. It uses a weight λ in the loss function to balance translation quality and latency. 9 3 Experiments We carry out experiments on two translation tasks: the NIST Chinese-English (Zh-En) translation task (2M sentences), and the WMT 2015 German-English (De-En) translation task (4.5M sentences).we use BLEU (Papineni et al., 2002) score to evaluate translation quality, and Average Lagging 5 (Ma et al., 2019) to measure latency. 3.1 Data Preprocess We use an open-source Chinese Tokenizer 6 to preprocess Chinese and apply Moses Tokenizer 7 to preprocess English and German. For Zh-En, we validate on NIST newstest 2006 and report results on newstest 2002, 2003, 2004, 2005, and 2008. We use SententcePiece 8 to implement byte-pair encoding (BPE) (Sennrich et al., 2016) for both Chinese and English by setting the vocabulary size to 20K and 18K, respectively. For De-En, we validate on newstest 2013 and then report results on n"
2020.emnlp-main.178,N13-1023,0,0.212308,"ndent of the context. They segment the source text based on a fixed length (Ma et al., 2019; Dalvi et al., 2018). For example, the wait-k method (Ma et al., 2019) first reads k source words, and then generates one target word immediately after each subsequent word is received. Policies of this type are simple and easy to implement. However, they do not consider contextual information and usually result in a drop in translation accuracy. • Adaptive Policies learn to do segmentation according to dynamic contextual information. They either use a specific model to chunk the streaming source text (Sridhar et al., 2013; Oda et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Zheng et al., 2019a, 2020) or jointly learn segmentation and translation in an end-to-end framework (Arivazhagan et al., 2019; Zheng et al., 2019b; Ma et al., 2020). The adaptive methods are more flexible than the fixed ones and achieve state-of-the-art. 1 Introduction In recent years, simultaneous translation has attracted increasing interest both in research and industry community. It aims at a real-time translation that demands high translation quality and an as-short-as-possible delay between speech and translation output. A typic"
2020.emnlp-main.178,P16-1162,0,0.0613232,"asks: the NIST Chinese-English (Zh-En) translation task (2M sentences), and the WMT 2015 German-English (De-En) translation task (4.5M sentences).we use BLEU (Papineni et al., 2002) score to evaluate translation quality, and Average Lagging 5 (Ma et al., 2019) to measure latency. 3.1 Data Preprocess We use an open-source Chinese Tokenizer 6 to preprocess Chinese and apply Moses Tokenizer 7 to preprocess English and German. For Zh-En, we validate on NIST newstest 2006 and report results on newstest 2002, 2003, 2004, 2005, and 2008. We use SententcePiece 8 to implement byte-pair encoding (BPE) (Sennrich et al., 2016) for both Chinese and English by setting the vocabulary size to 20K and 18K, respectively. For De-En, we validate on newstest 2013 and then report results on newstest 2015. We utilize a joint vocabulary, with a vocabulary size of 32K. Notably, translation quality in all experiments is measured using detokenized, cased BLEU. • MU: our proposed basic method of translating after detecting a meaning unit. • MU++: our proposed refined method to detect fine-grained meaning units. The training of segmentation models for chunk, MU and MU++ are based on the classification task of BERT 10 and ERNIE 11 ,"
2020.emnlp-main.178,2020.acl-main.254,0,0.171224,"Missing"
2020.emnlp-main.178,D19-1137,0,0.344659,"t al., 2019; Dalvi et al., 2018). For example, the wait-k method (Ma et al., 2019) first reads k source words, and then generates one target word immediately after each subsequent word is received. Policies of this type are simple and easy to implement. However, they do not consider contextual information and usually result in a drop in translation accuracy. • Adaptive Policies learn to do segmentation according to dynamic contextual information. They either use a specific model to chunk the streaming source text (Sridhar et al., 2013; Oda et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Zheng et al., 2019a, 2020) or jointly learn segmentation and translation in an end-to-end framework (Arivazhagan et al., 2019; Zheng et al., 2019b; Ma et al., 2020). The adaptive methods are more flexible than the fixed ones and achieve state-of-the-art. 1 Introduction In recent years, simultaneous translation has attracted increasing interest both in research and industry community. It aims at a real-time translation that demands high translation quality and an as-short-as-possible delay between speech and translation output. A typical simultaneous translation system consists of an auto-speech-recognition (ASR"
2020.emnlp-main.178,P19-1582,0,0.300752,"t al., 2019; Dalvi et al., 2018). For example, the wait-k method (Ma et al., 2019) first reads k source words, and then generates one target word immediately after each subsequent word is received. Policies of this type are simple and easy to implement. However, they do not consider contextual information and usually result in a drop in translation accuracy. • Adaptive Policies learn to do segmentation according to dynamic contextual information. They either use a specific model to chunk the streaming source text (Sridhar et al., 2013; Oda et al., 2014; Cho and Esipova, 2016; Gu et al., 2017; Zheng et al., 2019a, 2020) or jointly learn segmentation and translation in an end-to-end framework (Arivazhagan et al., 2019; Zheng et al., 2019b; Ma et al., 2020). The adaptive methods are more flexible than the fixed ones and achieve state-of-the-art. 1 Introduction In recent years, simultaneous translation has attracted increasing interest both in research and industry community. It aims at a real-time translation that demands high translation quality and an as-short-as-possible delay between speech and translation output. A typical simultaneous translation system consists of an auto-speech-recognition (ASR"
2021.acl-long.136,N16-1014,0,0.0183544,"ces and information inconsistency. “0” means that there are more than two incoherence errors in a session. “1” means that there are only one error. “2” means that there are no errors. Finally, we compute the average score of all the sessions. (2) Dialog engagement (Enga.) This metric measures how interesting a dialogs is. It is “1” if a dialog is interesting and the human is willing to continue the conversation, otherwise “0”. (3) Length of high-quality dialog (Length) A high-quality dialog ends if the model tends to produce dull responses or two consecutive utterances are highly overlapping (Li et al., 2016b). Single-turn Metrics. We use the following metrics: (1) Single-turn Coherence (Single.T.-Coh.) “0” if a response is inappropriate as an reply, otherwise “1”; (2) Informativeness (Info.) “0” if a response is a “safe” response, e.g. “I don’t know”, or it is highly overlapped with context, otherwise “1”; (3) Distinct (Dist.-i) It is an automatic metric for response diversity (Li et al., 2016a). 5.3 Experiment Results As shown in Table 2, GCS significantly outperforms all the baselines in terms of all the metrics except “Length-of-dialog” (sign test, p-value &lt; 0.01). It indicates that GCS can g"
2021.acl-long.136,D16-1127,0,0.0292141,"ces and information inconsistency. “0” means that there are more than two incoherence errors in a session. “1” means that there are only one error. “2” means that there are no errors. Finally, we compute the average score of all the sessions. (2) Dialog engagement (Enga.) This metric measures how interesting a dialogs is. It is “1” if a dialog is interesting and the human is willing to continue the conversation, otherwise “0”. (3) Length of high-quality dialog (Length) A high-quality dialog ends if the model tends to produce dull responses or two consecutive utterances are highly overlapping (Li et al., 2016b). Single-turn Metrics. We use the following metrics: (1) Single-turn Coherence (Single.T.-Coh.) “0” if a response is inappropriate as an reply, otherwise “1”; (2) Informativeness (Info.) “0” if a response is a “safe” response, e.g. “I don’t know”, or it is highly overlapped with context, otherwise “1”; (3) Distinct (Dist.-i) It is an automatic metric for response diversity (Li et al., 2016a). 5.3 Experiment Results As shown in Table 2, GCS significantly outperforms all the baselines in terms of all the metrics except “Length-of-dialog” (sign test, p-value &lt; 0.01). It indicates that GCS can g"
2021.acl-long.136,D19-1187,1,0.845921,"models (Chotimongkol, 2008; Ritter et al., 2010; Zhai and Williams, 2014) or variational auto-encoder (Shi et al., 2019). However, the number of their dialog states is limited to only dozens or hundreds, which cannot cover fine-grained semantics in chitchat. Moreover, our method can discover a hierarchical dialog structure, which is different from the non-hierarchical dialog structures in most previous work. 2.2 Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Moghe et al., 2018; Dinan et al., 2019; Liu et al., 2019; Xu et al., 2020c,a). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn dialog modeling. 2.3 Latent variable models for chitchat Recently, latent variables are utilized to improve diversity (Serban et al., 2017; Zhao et al., 2017; Gu et al., 2019; Gao et al., 2019; Ghandeharioun et al., 2019), control responding styles (Zhao et al., 2018; Li et al., 2020) and incorporate knowledge (Kim et al., 2020) in dialogs. Our work differs from 1727 4 ai.baidu.com/tech/nlp basic/dependency parsing Em"
2021.acl-long.136,D18-1255,0,0.0143329,"task-oriented dialogs via hidden Markov models (Chotimongkol, 2008; Ritter et al., 2010; Zhai and Williams, 2014) or variational auto-encoder (Shi et al., 2019). However, the number of their dialog states is limited to only dozens or hundreds, which cannot cover fine-grained semantics in chitchat. Moreover, our method can discover a hierarchical dialog structure, which is different from the non-hierarchical dialog structures in most previous work. 2.2 Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Moghe et al., 2018; Dinan et al., 2019; Liu et al., 2019; Xu et al., 2020c,a). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn dialog modeling. 2.3 Latent variable models for chitchat Recently, latent variables are utilized to improve diversity (Serban et al., 2017; Zhao et al., 2017; Gu et al., 2019; Gao et al., 2019; Ghandeharioun et al., 2019), control responding styles (Zhao et al., 2018; Li et al., 2020) and incorporate knowledge (Kim et al., 2020) in dialogs. Our work differs from 1727 4 ai.baidu.co"
2021.acl-long.136,P02-1040,0,0.110099,"evaluate the quality of the initialized graph (denoted as Phrase Graph) that consists of only phrases (as vertices) and initial edges (between phrases) in Section 3.2. For more details, please refer to Appendix A.1. 4.2 Evaluation Metrics We evaluate discovered dialog structure graph with both automatic evaluation and human evaluation. For automatic evaluation, we use two metrics to evaluate the performance of reconstruction: (1) NLL is the negative log likelihood of dialog utterances; (2) BLEU-1/2 measures how much that reconstructed sentences contains 1/2-gram overlaps with input sentences (Papineni et al., 2002). The two metrics indicate how well the learned dialog structure graph can capture important semantic information in dialog dataset. Further, we manually evaluate the quality of edges and vertices in the graph. For edges, (1) S-U Appr. for multi-turn dialog coherence. It measures the appropriateness of Sess-Utter edges, where these edges provide crucial prior information to ensure multi-turn dialog coherence (see results in Section 5.4). “1” if an utterance-level vertex is relevant to its session-level vertex (topic), otherwise “0”. (2) U-U Appr. for single-turn dialog coherence: It measures t"
2021.acl-long.136,N10-1020,0,0.0678486,". (2) we propose a novel model, DVAE-GNN, for hierarchical dialog struc3 Co-occurrence means that two utterance-level vertices are mapped by two adjacent utterances in a session. 7 预定了酒店 Have booked a hotel room 找到房子了么 Have you found a place to live? Speak 1: 嗯，我 提前订好了酒店。 [Yes, I have booked a hotel in advance.] Response Utterance-level semantic vertex Related Work 2.1 Utterance-level semantic vertex 6 Dialog structure learning for task-oriented dialogs There are previous work on discovering humanreadable dialog structure for task-oriented dialogs via hidden Markov models (Chotimongkol, 2008; Ritter et al., 2010; Zhai and Williams, 2014) or variational auto-encoder (Shi et al., 2019). However, the number of their dialog states is limited to only dozens or hundreds, which cannot cover fine-grained semantics in chitchat. Moreover, our method can discover a hierarchical dialog structure, which is different from the non-hierarchical dialog structures in most previous work. 2.2 Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Moghe et al., 2018; Dinan et al., 2019; Liu et al., 2019; Xu et al., 2020c,a). In this"
2021.acl-long.136,2020.emnlp-main.378,0,0.027344,"e growing interests in leveraging knowledge bases for generation of more informative responses (Moghe et al., 2018; Dinan et al., 2019; Liu et al., 2019; Xu et al., 2020c,a). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn dialog modeling. 2.3 Latent variable models for chitchat Recently, latent variables are utilized to improve diversity (Serban et al., 2017; Zhao et al., 2017; Gu et al., 2019; Gao et al., 2019; Ghandeharioun et al., 2019), control responding styles (Zhao et al., 2018; Li et al., 2020) and incorporate knowledge (Kim et al., 2020) in dialogs. Our work differs from 1727 4 ai.baidu.com/tech/nlp basic/dependency parsing Embedding space of session-level semantic vertices … Embedding space of utterance-level semantic vertices … Vectors of session-level semantic vertices Vectors of utterance-level semantic vertices Vectors of utterance phrases 放假过来找我玩啊 [Let‘s gather on holiday] 好啊，好久没见面了 [Yep, long time no see] 我明天准备去长沙上班 [I’ll go to Changsha tomorrow] 1 放假过来找我玩啊 [Let‘s gather on holiday] 1 3 2 RNN Encoder FFN 1 GNN 2 5 1 你租房子了么 [Oh, have you rent a room yet?] 2 3 4 Emb RNN Decode"
2021.acl-long.136,P15-1152,0,0.0107824,"hierarchical latent dialog states (at the level of both session and utterance) and their transitions from corpus as a dialog structure graph. Then we leverage it as background knowledge to facilitate dialog management in a RL based dialog system. Experimental results on two benchmark corpora confirm that DVAE-GNN can discover meaningful dialog structure graph, and the use of dialog structure as background knowledge can significantly improve multi-turn coherence. 1 Introduction With the aim of building a machine to converse with humans naturally, some work investigate neural generative models (Shang et al., 2015; Serban et al., 2017). While these models can generate locally relevant dialogs, they struggle to organize individual utterances into globally coherent flow (Yu et al., 2016; Xu et al., 2020b). The possible reason is that it is difficult to control the overall dialog flow without background knowledge about dialog structure.1 However, due to the complexity of opendomain conversation, it is laborious and costly to annotate dialog structure manually. Therefore, it is ∗ Equal contribution. Corresponding author: Wanxiang Che. 1 Dialog structure means dialog states and their transitions. of great i"
2021.acl-long.136,N19-1178,0,0.326662,"The possible reason is that it is difficult to control the overall dialog flow without background knowledge about dialog structure.1 However, due to the complexity of opendomain conversation, it is laborious and costly to annotate dialog structure manually. Therefore, it is ∗ Equal contribution. Corresponding author: Wanxiang Che. 1 Dialog structure means dialog states and their transitions. of great importance to discover open-domain dialog structure from corpus in an unsupervised way for coherent dialog generation. Some studies tried to discover dialog structure from task-oriented dialogs (Shi et al., 2019). However, the number of their dialog states is limited to only dozens or hundreds, which cannot cover finegrained semantics in open-domain dialogs. Furthermore, the dialog structures they discovered generally only contain utterance-level semantics (non-hierarchical), without session-level semantics (chatting topics) that are essential in open-domain dialogs (Wu et al., 2019; Kang et al., 2019; Xu et al., 2020c).2 Thus, in order to provide a full picture of open-domain dialog structure, it is desirable to discover a two-layer directed graph that contains session-level semantics in the upper-la"
2021.acl-long.136,P19-1371,0,0.0113493,"in Appendix B. 5 Experiments for Graph Grounded Dialog Generation To confirm the benefits of discovered dialog structure graph for coherent conversation generation, we conduct experiments on the graph discovered from Weibo corpus. All the systems (including baselines) are trained on Weibo corpus. 5.1 Models We carefully select the following six baselines. MMPMS It is the multi-mapping based neural open-domain conversational model with posterior mapping selection mechanism (Chen et al., 2019), which is a SOTA model on the Weibo Corpus. MemGM It is the memory-augmented opendomain dialog model (Tian et al., 2019), which learns to cluster U-R pairs for response generation. HRED It is the hierarchical recurrent encoderdecoder model (Serban et al., 2016). CVAE It is the Conditional Variational AutoEncoder based neural open-domain conversational model (Zhao et al., 2017). VHCR-EI This variational hierarchical RNN model can learn hierarchical latent variables from open-domain dialogs (Ghandeharioun et al., 2019). It is a SOTA dialog model with hierarchical VAE. DVRNN-RL It discovers dialog structure graph for task-oriented dialog modeling (Shi et al., 2019). GCS It is our proposed dialog structure graph gr"
2021.acl-long.136,P19-1369,1,0.807053,"eir transitions. of great importance to discover open-domain dialog structure from corpus in an unsupervised way for coherent dialog generation. Some studies tried to discover dialog structure from task-oriented dialogs (Shi et al., 2019). However, the number of their dialog states is limited to only dozens or hundreds, which cannot cover finegrained semantics in open-domain dialogs. Furthermore, the dialog structures they discovered generally only contain utterance-level semantics (non-hierarchical), without session-level semantics (chatting topics) that are essential in open-domain dialogs (Wu et al., 2019; Kang et al., 2019; Xu et al., 2020c).2 Thus, in order to provide a full picture of open-domain dialog structure, it is desirable to discover a two-layer directed graph that contains session-level semantics in the upper-layer vertices, utterance-level semantics in the lower-layer vertices, and edges among these vertices. In this paper, we propose a novel discrete variational auto-encoder with graph neural network (DVAE-GNN) to discover a two-layer dialog structure from chitchat corpus. Intuitively, since discrete dialog states are easier to capture transitions for dialog coherence, we use dis"
2021.acl-long.202,2020.acl-main.703,0,0.0295517,"rained on the ImageNet dataset. Recently, contrastive self-supervised learning like SimCLR (Chen et al., 2020a) and MoCo (He et al., 2020) also greatly improve the performance of visual representation learning. These pre-trained models only focus on visual tasks (e.g. image classification etc.), however, they cannot be used in textual or multimodal (i.e., with both text and image) tasks. The language pre-training methods based on the Transformer architecture are also very popular in NLP models, such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and BART (Lewis et al., 2020). However, they mainly focus on textual tasks. They cannot effectively deal with the multi-modal tasks, such as image-text retrieval, image captioning, multimodal machine translation (Lin et al., 2020a; Su et al., 2021) and visual dialog (Murahari et al., 2020). Multi-Modal Pre-training Recently, multimodal pre-training methods have been more and more popular for solving the multi-modal tasks. All of them are trained on a corpus of image-text pairs, such as ViLBERT (Lu et al., 2019), VisualBERT (Li et al., 2019b), VL-BERT (Su et al., 2019), Unicoder-VL (Li et al., 2019a) and UNITER (Chen et al"
2021.acl-long.202,2021.ccl-1.108,0,0.0544888,"Missing"
2021.acl-long.202,D16-1264,0,0.0350289,"UNIMO-base by initializing from RoBERTa-base, and UNIMO-large by initializing from RoBERTa-large. Both UNIMObase and UNIMO-large are trained for at least 500K steps. An Adam optimizer with initial learning rate 3.3 Finetuning Tasks We fine-tune our model on two categories of downstream tasks: (1) single-modal language understanding and generation tasks; (2) multimodal vision-language understanding and generation tasks. The single-modal generation tasks include: generative conversational question answering on the CoQA dataset (Reddy et al., 2019), question generation on the SQuAD 1.1 dataset (Rajpurkar et al., 2016), abstractive summarization on the CNN/DailyMail (CNNDM) dataset (Hermann et al., 2015), and sentence compression on the Gigaword dataset (Rush et al., 2015). The single-modal understanding tasks include: sentiment classification on the SST-2 dataset (Socher et al., 2013), natural language inference on the MNLI dataset (Williams et al., 2017), linguistic acceptability analysis on the CoLA dataset (Warstadt et al., 2019) and semantic similarity analysis on the STS-B dataset (Cer et al., 2017). The multi-modal tasks include: visual question answering (VQA) on the VQA v2.0 dataset (Goyal et al.,"
2021.acl-long.202,Q19-1016,0,0.0140192,"-region features are set as 512 and 100, respectively. We pre-train UNIMO-base by initializing from RoBERTa-base, and UNIMO-large by initializing from RoBERTa-large. Both UNIMObase and UNIMO-large are trained for at least 500K steps. An Adam optimizer with initial learning rate 3.3 Finetuning Tasks We fine-tune our model on two categories of downstream tasks: (1) single-modal language understanding and generation tasks; (2) multimodal vision-language understanding and generation tasks. The single-modal generation tasks include: generative conversational question answering on the CoQA dataset (Reddy et al., 2019), question generation on the SQuAD 1.1 dataset (Rajpurkar et al., 2016), abstractive summarization on the CNN/DailyMail (CNNDM) dataset (Hermann et al., 2015), and sentence compression on the Gigaword dataset (Rush et al., 2015). The single-modal understanding tasks include: sentiment classification on the SST-2 dataset (Socher et al., 2013), natural language inference on the MNLI dataset (Williams et al., 2017), linguistic acceptability analysis on the CoLA dataset (Warstadt et al., 2019) and semantic similarity analysis on the STS-B dataset (Cer et al., 2017). The multi-modal tasks include:"
2021.acl-long.202,D15-1044,0,0.04454,"eps. An Adam optimizer with initial learning rate 3.3 Finetuning Tasks We fine-tune our model on two categories of downstream tasks: (1) single-modal language understanding and generation tasks; (2) multimodal vision-language understanding and generation tasks. The single-modal generation tasks include: generative conversational question answering on the CoQA dataset (Reddy et al., 2019), question generation on the SQuAD 1.1 dataset (Rajpurkar et al., 2016), abstractive summarization on the CNN/DailyMail (CNNDM) dataset (Hermann et al., 2015), and sentence compression on the Gigaword dataset (Rush et al., 2015). The single-modal understanding tasks include: sentiment classification on the SST-2 dataset (Socher et al., 2013), natural language inference on the MNLI dataset (Williams et al., 2017), linguistic acceptability analysis on the CoLA dataset (Warstadt et al., 2019) and semantic similarity analysis on the STS-B dataset (Cer et al., 2017). The multi-modal tasks include: visual question answering (VQA) on the VQA v2.0 dataset (Goyal et al., 2017), image caption on the Microsoft COCO Captions dataset (Chen et al., 2015), visual entailment on the SNLI-VE dataset (Xie et al., 2019) and image-text r"
2021.acl-long.202,P16-1162,0,0.0126841,"learn representations that capture modality-invariant information at the semantic level. Different from previous methods, UNIMO learns from different modalities of data, including images, texts and image-text pairs, thus achieving more robust and generalizable representations for both textual and visual input. As shown in Figure 2, UNIMO employs multi-layer self-attention Transformers to learn unified semantic representations for both textual and visual data. For a textual input W, it is firstly split into a sequence of subwords W = {[CLS], w1 , ..., wn , [SEP ]} by Byte-Pair Encoding (BPE) (Sennrich et al., 2016), and then the self-attention mechanism is leveraged to learn contextual token representations {h[CLS] , hw1 , ..., hwn , h[SEP ] }. The special tokens [CLS] and [SEP ] denote the start and end of the textual sequence, respectively. Similarly, for an image V, it is firstly converted to a sequence of region features V = {[IM G], v1 , ..., vt } ([IM G] denotes the representation of the entire image), and then the self-attention mechanism is leveraged to learn contextual region representations {h[IM G] , hv1 , ..., hvt }. Similar to previous work (Chen et al., 2020b), we use Faster R-CNN (Ren et"
2021.acl-long.202,P18-1238,0,0.0530716,"Missing"
2021.acl-long.227,Q18-1021,0,0.0715666,"Missing"
2021.acl-long.227,D19-1521,0,0.0282896,"n et al., 2018) to predict an answer with the maximum sum of start and end logits across multiple segments of a sample. In addition, we use a modified cross-entropy loss (Clark and Gardner, 2017) for the TQA dataset and use a two-stage model (Groeneveld et al., 2020) with the backbone of ERNIE-D OC for the HQA dataset. Tab. 4. shows that ERNIE-D OC outperforms RoBERTa and Longformer by a considerable margin on these two datasets, and is comparable to current SOTA long-document model, i.e., BigBird on HQA in large-size model setting. Results on the Keyphrase Extraction Task. We include OpenKP (Xiong et al., 2019) dataset to evaluate ERNIE-D OC’s ability to extract keyphrases from a long document. Each document contains up to three short keyphrases and we follow the model setting of JointKPE (Sun et al., 2020a) and ETC (Ainslie et al., 2020) by applying CNNs on BERT’s output to compose n-gram embeddings for classification. We report the results of basesize models in Tab. 5 under no-visual-features setting for easy and fair comparison with baselines. ERNIE-D OC performs stably better on all metrics on the OpenKP dataset. 4.2.4 Results on Chinese Tasks We conducted extensive experiments on seven Chinese"
2021.acl-long.227,L18-1431,0,0.0219061,"short keyphrases and we follow the model setting of JointKPE (Sun et al., 2020a) and ETC (Ainslie et al., 2020) by applying CNNs on BERT’s output to compose n-gram embeddings for classification. We report the results of basesize models in Tab. 5 under no-visual-features setting for easy and fair comparison with baselines. ERNIE-D OC performs stably better on all metrics on the OpenKP dataset. 4.2.4 Results on Chinese Tasks We conducted extensive experiments on seven Chinese natural language understanding (NLU) tasks, including machine reading comprehension (CMRC2018 (Cui et al., 2018), DRCD (Shao et al., 2018), DuReader (He et al., 2017), C3 (Sun et al., 2019a)), semantic similarity (CAIL2019SCM (CAIL) (Xiao et al., 2019)), and long-text classification (IFLYTEK (IFK) (Xu et al., 2020), THUCNews (THU)5 (Sun et al., 2016)). The documents in all the aforementioned datasets are sufficiently long to be used to evaluate the effectiveness of ERNIE-D OC on long-context tasks (see detailed datasets statistics in Tab. 9). We reported the mean results with five runs for the seven Chinese tasks in Tab. 6, and summarized the hyperparameters in Tab. 16. ERNIE-D OC outperforms previous models across these Chinese"
2021.acl-long.227,N18-2074,0,0.0262244,"limited the length of the sentences in each mini-batch to 512 tokens and the length of the memory to 128. The models were trained for 500K/400K/100K steps using a batch size of 2,560/2,560/3,920 sentences for the small/base/large configurations. ERNIE-D OC was optimized with the Adam (Kingma and Ba, 2014) optimizer. The learning rate was warmed up over the first 4,000 steps to a peak value of 1e-4, and then it linearly decayed. The remaining pretraining hyperparameters were the same as those of RoBERTa (Liu et al., 2019) (see Tab. 12). Additionally, we employed relative positional embedding (Shaw et al., 2018) in our model pretraining because it is necessary for reusing hidden state without causing temporal confusion (Dai et al., 2019). Finetune. In contrast to previous models, such as BERT, RoBERTa, and XLNet, the proposed model employs the retrospective feed mechanism and the enhanced recurrence mechanism during the finetuning phase to fully utilize the advantages of these two strategies. 4.2.3 Results on English Tasks Results on Long-Text Classification Tasks. We consider two datasets: IMDB reviews (Maas et al., 2011) and Hyperpartisan News Detection (HYP) (Kiesel et al., 2019). The former is a"
2021.acl-long.472,2020.acl-main.175,0,0.0125372,"odels as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection"
2021.acl-long.472,D17-1209,0,0.0607661,"Missing"
2021.acl-long.472,2020.acl-main.461,0,0.0198281,"Missing"
2021.acl-long.472,N18-1150,0,0.0212707,"ent representation and summary generation process of the Seq2Seq architecture by leveraging the graph structure. • Automatic and human evaluation on both long-document summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 201"
2021.acl-long.472,P18-1063,0,0.0157292,"emantic relevance of the generated summaries and references, as the BERTScore improvements of BASS is obvious. Results on SDS Table 3 shows our experiment results along with other SDS baselines. Similar to WikiSUM, we also report LexRank, TransS2S, and RoBERTaS2S. Besides, we report the performance of several other baselines. ORACLE is the upper-bound of current extrative models. Seq2seq is based on LSTM encoder-decoder with attention mechanism (Bahdanau et al., 2015). Pointer and Pointer+cov are pointer-generation (See et al., 2017) with and without coverage mechanism, respectively. FastAbs (Chen and Bansal, 2018) is an abstractive method by jointly training sentence extraction and compression. TLM (Pilault et al., 2020) is a recent long-document summarization method based on language model. We also report the performances of recent pretrianing-based SOTA 6058 text generation models BART (large) and Peaguasus (base) on BIGPATENT, which both contain a parameter size of 406M . The last block shows the results of our model, which contains a parameter size of 201M . The results show that BASS consistently outperforms RoBERTaS2S, and comparable with current large SOTA models with only half of the parameter"
2021.acl-long.472,D19-5412,0,0.0207023,"(Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summariz"
2021.acl-long.472,2020.acl-main.703,0,0.0383412,"hysics Nobel Prize in 1921. The great prize was for his explanation of the photoelectric effect. The Unified Semantic Graph nomod:of Work is done during an internship at Baidu Inc. Corresponding author. cop obj won nsubj a German physicist appos nsubj published Albert Einstein dobj obl 1912 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectric eﬀect Figure 1: Ill"
2021.acl-long.472,2020.acl-main.555,1,0.901075,"representing them as nodes and their relations as edges. This greatly benefits global structure learning and longdistance relation modeling. Several previous works have attempted to leverage sentence-relation graph to improve long sequence summarization, where nodes are sentences and edges are similarity or dis6052 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6052–6067 August 1–6, 2021. ©2021 Association for Computational Linguistics course relations between sentences (Li et al., 2020). However, the sentence-relation graph is not flexible for fine-grained (such as entities) information aggregation and relation modeling. Some other works also proposed to construct local knowledge graph by OpenIE to improve Seq2Seq models (Fan et al., 2019; Huang et al., 2020). However, the OpenIE-based graph only contains sparse relations between partially extracted phrases, which cannot reflect the global structure and rich relations of the overall sequence. For better modeling the long-distance relations and global structure of a long sequence, we propose to apply a phrase-level unified se"
2021.acl-long.472,D18-1205,1,0.790714,"ls to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient cont"
2021.acl-long.472,D18-1441,1,0.763064,"ls to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient cont"
2021.acl-long.472,C18-1101,0,0.0214588,"sing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propose a discourse-aware model to extract sentences. Similarly, structures from semantic analysis also help. Liu et al. (2015) and Liao et al. (2018) propose to guide summarization with Abstract Meaning Representation (AMR) for a better comprehension of the input context. (Li and Zhuge, 2019) propose semantic link networks based MDS but without graph neural networks. Recently, the local knowledge graph by OpenIE attracts great attention. Leveraging OpenIE extracted tuples, Fan et al. (2019) compress and reduce redundancy in multi-document inputs in MDS. Their work mainly focus on the efficiency in processing long sequences. Huang et al. (2020) utilize OpenIEbased graph for boosting the faithfulness of the generated summaries. Compared with"
2021.acl-long.472,N15-1114,0,0.0238886,"rules. Dependency parsing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propose a discourse-aware model to extract sentences. Similarly, structures from semantic analysis also help. Liu et al. (2015) and Liao et al. (2018) propose to guide summarization with Abstract Meaning Representation (AMR) for a better comprehension of the input context. (Li and Zhuge, 2019) propose semantic link networks based MDS but without graph neural networks. Recently, the local knowledge graph by OpenIE attracts great attention. Leveraging OpenIE extracted tuples, Fan et al. (2019) compress and reduce redundancy in multi-document inputs in MDS. Their work mainly focus on the efficiency in processing long sequences. Huang et al. (2020) utilize OpenIEbased graph for boosting the faithfulness of the generated s"
2021.acl-long.472,P19-1500,0,0.47225,"luation on both long-document summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the r"
2021.acl-long.472,D19-1387,0,0.342053,"luation on both long-document summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the r"
2021.acl-long.472,N19-1173,0,0.398581,"et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient content based on syntax s"
2021.acl-long.472,2021.ccl-1.108,0,0.0427439,"Missing"
2021.acl-long.472,P19-1212,0,0.133693,"12 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectric eﬀect Figure 1: Illustration of a unified semantic graph and its construction procedure for a document containing three sentences. In Graph Construction, underlined tokens represent phrases., co-referent phrases are represented in the same color. In The Unified Semantic Graph, nodes of different colors indic"
2021.acl-long.472,P14-5010,0,0.00266212,", two-hop meta-path represents more complex semantic relations in graph. For example, N-V-N like [Albert Einstein]-[won]-[the physics Nobel Prize] indicates SVO (subject–verb–object) relation. It is essential to effectively model the two-hop meta-path for complex semantic relation modeling. 3.2 Graph Construction In this section, we introduce the definition and construction of the unified semantic graph. To construct the semantic graph, we extract phrases and their relations from sentences by first merging tokens into phrases and then merging co-referent phrases into nodes. We employ CoreNLP (Manning et al., 2014) to obtain coreference chains of the input sequence and the dependency parsing tree of each sentence. Based on the dependency parsing tree, we merge consecutive tokens that form a complete semantic unit into a phrase. Afterwards, we merge the same phrases from different positions and phrases in the same coreference chain to form the nodes in the semantic graph. The final statistics of the unified semantic graph on WikiSUM are illustrated in table 1, which indicates that the scale of the graph expands moderately with the inputs. This also demonstrates how the unified semantic graph compresses l"
2021.acl-long.472,2020.emnlp-main.748,0,0.0134901,"Results on SDS Table 3 shows our experiment results along with other SDS baselines. Similar to WikiSUM, we also report LexRank, TransS2S, and RoBERTaS2S. Besides, we report the performance of several other baselines. ORACLE is the upper-bound of current extrative models. Seq2seq is based on LSTM encoder-decoder with attention mechanism (Bahdanau et al., 2015). Pointer and Pointer+cov are pointer-generation (See et al., 2017) with and without coverage mechanism, respectively. FastAbs (Chen and Bansal, 2018) is an abstractive method by jointly training sentence extraction and compression. TLM (Pilault et al., 2020) is a recent long-document summarization method based on language model. We also report the performances of recent pretrianing-based SOTA 6058 text generation models BART (large) and Peaguasus (base) on BIGPATENT, which both contain a parameter size of 406M . The last block shows the results of our model, which contains a parameter size of 201M . The results show that BASS consistently outperforms RoBERTaS2S, and comparable with current large SOTA models with only half of the parameter size. This further demonstrates the effectiveness of our graph-augmented model on long-document summarization"
2021.acl-long.472,2020.findings-emnlp.217,0,0.0113767,"stractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al.,"
2021.acl-long.472,W00-1009,0,0.480591,"or his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectric eﬀect Figure 1: Illustration of a unified semantic graph and its construction procedure for a document containing three sentences. In Graph Construction, underlined tokens represent phrases., co-referent phrases are represented in the same color. In The Unified Semantic Graph, nodes of different colors indicate different types, according to section 3.1. on the long source sequence (Shao et al., 2017). Thus"
2021.acl-long.472,2020.tacl-1.18,0,0.0241394,"cument summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several larg"
2021.acl-long.472,D19-1324,0,0.0156427,"deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient content based on syntax structure and syntax rules. Dependency parsing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propo"
2021.acl-long.472,2020.acl-main.451,0,0.191789,"Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient content based on syntax structure and syntax rules. Dependency parsing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propose a discourse-aware model to extract sentences. Similarly, structures from semantic analysis also help. Liu et al. (2015) and Liao et al. (2018) propose to guide summarization with Abstract Meaning Representation (AMR) for a better comprehension of the input context. (Li and Zhuge, 2019) propose semantic link networks based MDS but without graph neural networks. Recently, the local knowledge graph by OpenIE attracts great attention. Leveraging OpenIE extracted tuples, Fan et al. (2019) compress and reduce redundancy in multi-document inputs in MDS. Their work mainly focus on the efficie"
2021.acl-long.472,2020.acl-main.640,0,0.0218044,"d by the graph. Node Initialization Similar to graph construction in section 3.2, we initialize graph representations following the two-level merging, token merging and phrase merging. The token merging compresses and abstracts local token features into higher-level phrase representations. The phrase merging aggregates co-referent phrases in a wide context, which captures long-distance and crossdocument relations. To be simple, these two merging steps are implemented by average pooling. Graph Encoding Layer Following previous works in graph-to-sequence learning (KoncelKedziorski et al., 2019; Yao et al., 2020), we apply Transformer layers for graph modeling by applying the graph adjacent matrix as self-attention mask. Graph Augmentation Following previous works (Bastings et al., 2017; Koncel-Kedziorski et al., 2019), we add reverse edges and self-loop edges in graph as the original directed edges are 6055 not enough for learning backward information. For better utilizing the properties of the united semantic graph, we further propose two novel graph augmentation methods. Supernode As the graph becomes larger, noises introduced by imperfect graph construction also increase, which may cause disconnec"
2021.acl-long.472,D15-1044,0,0.251484,"ion tasks. 1 relativity. He won the physics Nobel Prize in 1921. The great prize was for his explanation of the photoelectric effect. The Unified Semantic Graph nomod:of Work is done during an internship at Baidu Inc. Corresponding author. cop obj won nsubj a German physicist appos nsubj published Albert Einstein dobj obl 1912 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was fo"
2021.acl-long.472,P17-1099,0,0.353644,"vity. He won the physics Nobel Prize in 1921. The great prize was for his explanation of the photoelectric effect. The Unified Semantic Graph nomod:of Work is done during an internship at Baidu Inc. Corresponding author. cop obj won nsubj a German physicist appos nsubj published Albert Einstein dobj obl 1912 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectri"
2021.acl-short.120,D17-1215,0,0.109534,"ior of existing models on the challenge test set, which may provide suggestions for future model development. The dataset and codes are publicly available at https://github.com/baidu/ DuReader. 1 Introduction Machine reading comprehension (MRC) requires machines to comprehend text and answer questions about it. With the development of deep learning, the recent studies of MRC have achieved remarkable advancements (Seo et al., 2017; Wang and Jiang, 2017; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020). However, previous studies show that most of the neural models are not robust enough (Jia and Liang, 2017; Ribeiro et al., 2018b; Talmor and Berant, 2019a; Welbl et al., 2020) and fail to generalize well (Talmor and Berant, 2019b). ∗ This work was done while the first author was doing internship at Baidu Inc. † Corresponding authors To further promote the studies of robust and well generalized MRC, we construct a Chinese dataset – DuReaderrobust which comprises natural questions and documents. In this paper, we focus on evaluating the robustness and generalization from the following aspects, where robustness consists of over-sensitivity and over-stability: (1) Over-sensitivity denotes that MRC mo"
2021.acl-short.120,2021.ccl-1.108,0,0.0944696,"Missing"
2021.acl-short.120,P18-1079,0,0.124182,"s on the challenge test set, which may provide suggestions for future model development. The dataset and codes are publicly available at https://github.com/baidu/ DuReader. 1 Introduction Machine reading comprehension (MRC) requires machines to comprehend text and answer questions about it. With the development of deep learning, the recent studies of MRC have achieved remarkable advancements (Seo et al., 2017; Wang and Jiang, 2017; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020). However, previous studies show that most of the neural models are not robust enough (Jia and Liang, 2017; Ribeiro et al., 2018b; Talmor and Berant, 2019a; Welbl et al., 2020) and fail to generalize well (Talmor and Berant, 2019b). ∗ This work was done while the first author was doing internship at Baidu Inc. † Corresponding authors To further promote the studies of robust and well generalized MRC, we construct a Chinese dataset – DuReaderrobust which comprises natural questions and documents. In this paper, we focus on evaluating the robustness and generalization from the following aspects, where robustness consists of over-sensitivity and over-stability: (1) Over-sensitivity denotes that MRC models provide different"
2021.acl-short.120,P19-1485,0,0.348476,"set, which may provide suggestions for future model development. The dataset and codes are publicly available at https://github.com/baidu/ DuReader. 1 Introduction Machine reading comprehension (MRC) requires machines to comprehend text and answer questions about it. With the development of deep learning, the recent studies of MRC have achieved remarkable advancements (Seo et al., 2017; Wang and Jiang, 2017; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020). However, previous studies show that most of the neural models are not robust enough (Jia and Liang, 2017; Ribeiro et al., 2018b; Talmor and Berant, 2019a; Welbl et al., 2020) and fail to generalize well (Talmor and Berant, 2019b). ∗ This work was done while the first author was doing internship at Baidu Inc. † Corresponding authors To further promote the studies of robust and well generalized MRC, we construct a Chinese dataset – DuReaderrobust which comprises natural questions and documents. In this paper, we focus on evaluating the robustness and generalization from the following aspects, where robustness consists of over-sensitivity and over-stability: (1) Over-sensitivity denotes that MRC models provide different answers to the paraphrase"
2021.acl-short.120,2020.findings-emnlp.103,0,0.0277015,"ggestions for future model development. The dataset and codes are publicly available at https://github.com/baidu/ DuReader. 1 Introduction Machine reading comprehension (MRC) requires machines to comprehend text and answer questions about it. With the development of deep learning, the recent studies of MRC have achieved remarkable advancements (Seo et al., 2017; Wang and Jiang, 2017; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020). However, previous studies show that most of the neural models are not robust enough (Jia and Liang, 2017; Ribeiro et al., 2018b; Talmor and Berant, 2019a; Welbl et al., 2020) and fail to generalize well (Talmor and Berant, 2019b). ∗ This work was done while the first author was doing internship at Baidu Inc. † Corresponding authors To further promote the studies of robust and well generalized MRC, we construct a Chinese dataset – DuReaderrobust which comprises natural questions and documents. In this paper, we focus on evaluating the robustness and generalization from the following aspects, where robustness consists of over-sensitivity and over-stability: (1) Over-sensitivity denotes that MRC models provide different answers to the paraphrased questions. It means"
2021.autosimtrans-1.5,2021.autosimtrans-1.5,1,0.0530913,"Missing"
2021.autosimtrans-1.5,2020.acl-demos.34,0,0.0687097,"Missing"
2021.autosimtrans-1.5,P19-1126,0,0.512702,"Missing"
2021.autosimtrans-1.5,cho-etal-2014-corpus,0,0.473052,"Missing"
2021.autosimtrans-1.5,2020.lrec-1.517,0,0.114315,"Missing"
2021.autosimtrans-1.5,P14-2090,0,0.435886,"Missing"
2021.autosimtrans-1.5,W17-4608,0,0.282237,"Missing"
2021.autosimtrans-1.5,2013.iwslt-papers.14,0,0.144286,"Missing"
2021.autosimtrans-1.5,2020.emnlp-main.178,1,0.639518,"Missing"
2021.autosimtrans-1.5,P16-1162,0,0.0317362,"Missing"
2021.autosimtrans-1.5,shimizu-etal-2014-collection,0,0.278144,"Missing"
2021.autosimtrans-1.5,Q19-1020,0,0.0906152,"Missing"
2021.autosimtrans-1.5,N13-1023,0,0.444736,"Missing"
2021.autosimtrans-1.6,2021.autosimtrans-1.6,1,0.0530913,"Missing"
2021.autosimtrans-1.6,P19-1126,0,0.146797,"Missing"
2021.autosimtrans-1.6,W05-0909,0,0.103299,"Missing"
2021.autosimtrans-1.6,N19-1202,0,0.173048,"Missing"
2021.autosimtrans-1.6,E17-1099,0,0.275927,"Missing"
2021.autosimtrans-1.6,2021.autosimtrans-1.5,1,0.84463,"Missing"
2021.autosimtrans-1.6,L18-1001,0,0.276747,"Missing"
2021.autosimtrans-1.6,2020.autosimtrans-1.1,1,0.73397,"Missing"
2021.autosimtrans-1.6,2020.emnlp-main.178,1,0.859154,"Missing"
2021.autosimtrans-1.6,1983.tc-1.13,0,0.644431,"Missing"
2021.autosimtrans-1.6,2020.aacl-main.58,0,0.698322,"Missing"
2021.autosimtrans-1.6,P02-1040,0,0.119352,"Missing"
2021.autosimtrans-1.6,2020.acl-main.350,0,0.514198,"Missing"
2021.autosimtrans-1.6,P16-1009,0,0.120801,"Missing"
2021.autosimtrans-1.6,2020.lrec-1.517,0,0.288828,"Missing"
2021.emnlp-main.224,N19-1423,0,0.0315612,"ipedia articles as the collection of passages. In our experiments, we reuse the NQ version created by DPR. Evaluation Metrics Following previous work, we adopt Mean Reciprocal Rank (MRR) and Recall at top k ranks (Recall@k) to evaluate the performance of passage retrieval. MRR calculates the averaged reciprocal of the rank at which the first positive passage is retrieved. Recall@k calculates the proportion of questions to which the top k retrieved passages contain positives. Model Specifications Our retriever and reranker largely follow ERNIE-2.0 base (Sun et al., 2020), which is a BERT-like (Devlin et al., 2019) model with 12-layer transformers and introduces a continual pre-training framework on multiple pretrained tasks. As described in previous section, the retriever is initialized with the parameters of the dual-encoder in the first step of RocketQA, and the re-ranker is initialized with the parameters of the cross-encoder in the second step of RocketQA. In this section, we first describe the experimental Implementation Details We conduct experisettings, then report the main experimental results, 2829 MSMARCO Dev MRR@10 R@50 R@1000 Natural Questions Test R@5 R@20 R@100 Methods PLM BM25 (anserini)"
2021.emnlp-main.224,2020.emnlp-main.342,0,0.0288834,"(Izacard and Grave, 2020; Yang and Seo, 2020; Qu et al., 2021; Ren et al., 2021). Based on the retrieved passages from a retriever, PLM-based rerankers with the cross-encoder architecture have recently been applied on passage re-ranking to improve the retrieval results (Qiao et al., 2019; Nogueira and Cho, 2019; Wang et al., 2019; Yan et al., 2019), and yield substantial improvements over the traditional methods. Apart from separately considering the above two tasks, it has been proved that passage retrieval and passage re-ranking are actually highly related and dependent (Huang et al., 2020; Gao et al., 2020; Khattab and Zaharia, 2020). The retriever needs to capture the relevance knowledge from the re-ranker, and the re-ranker should be specially optimized according to the preceding results of the retriever. Some efforts studied the possibility of leveraging the dependency of retriever and re-ranker, and try to enhance the connection between them in an alternative way (Qu et al., 2021; Yang et al., 2020; Huang et al., 2020). Furthermore, several studies attempted to jointly train the retriever and the reader for Open-domain Question Answering (Guu et al., 2020; Sachan et al., 2021; Karpukhin et"
2021.emnlp-main.224,2021.naacl-main.241,0,0.037037,"Missing"
2021.emnlp-main.224,K19-1049,0,0.0195356,"e relevance can be measured via embedding similarity. Additionally, a subsequent procedure of passage re-ranking is widely adopted to further improve the retrieval results by incorporating a reranker (Qu et al., 2021; Luan et al., 2021). Such a two-stage procedure is particularly useful in a variety of natural language processing tasks, including question answering (Mao et al., 2021; Xiong ∗ Equal contribution. The work was done when Ruiyang Ren was doing internship at Baidu. † Corresponding authors. et al., 2020b), dialogue system (Ji et al., 2014; Henderson et al., 2017) and entity linking (Gillick et al., 2019; Wu et al., 2020). Following a retrieve-then-rerank way, the dense retriever in passage retrieval and the re-ranker in passage re-ranking jointly contribute to the final performance. Despite the fact that the two modules work as a pipeline during the inference stage, it has been found useful to train them in a correlated manner. For example, the retriever with a dual-encoder can be improved by distilling from the re-ranker with a more capable cross-encoder architecture (Qu et al., 2021; Yang et al., 2020), and the re-ranker can be improved with training instances generated from the retriever"
2021.emnlp-main.224,P19-1612,0,0.0282384,"marized as follows: • We propose a novel approach that jointly trains the dense passage retriever and passage re-ranker. It is the first time that joint training has been implemented for the two modules. • We make two major technical contributions by introducing dynamic listwise distillation and hybrid data augmentation to support the proposed joint learning approach. • Extensive experiments show the effectiveness of our proposed approach on both MSMARCO and Natural Questions datasets. 2 Related Work vided into two categories: (1) self-supervised pretraining for retrieval (Chang et al., 2020; Lee et al., 2019; Guu et al., 2020) and (2) fine-tuning pre-trained language models (PLMs) on labeled data (Lu et al., 2020; Karpukhin et al., 2020; Xiong et al., 2020a; Luan et al., 2021; Qu et al., 2021) . Our work follows the second class of approaches, which show better performance with less cost. There are two important tricks to train an effective dense retriever: (1) incorporating hard negatives during training (Karpukhin et al., 2020; Xiong et al., 2020a; Qu et al., 2021) and (2) distilling the knowledge from cross-encoder-based reranker into dual-encoder-based retriever (Izacard and Grave, 2020; Yang"
2021.emnlp-main.224,2020.emnlp-main.550,0,0.0611717,"for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other’s relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at https:// github.com/PaddlePaddle/RocketQA. 1 Introduction Recently, dense passage retrieval has become an important approach in the task of passage retrieval (Karpukhin et al., 2020) to identify relevant contents from a large corpus. The underlying idea is to represent both queries and passages as low-dimensional vectors (a.k.a., embeddings), so that the relevance can be measured via embedding similarity. Additionally, a subsequent procedure of passage re-ranking is widely adopted to further improve the retrieval results by incorporating a reranker (Qu et al., 2021; Luan et al., 2021). Such a two-stage procedure is particularly useful in a variety of natural language processing tasks, including question answering (Mao et al., 2021; Xiong ∗ Equal contribution. The work was"
2021.emnlp-main.224,2021.naacl-main.466,1,0.0736875,"uestions datasets. Our code is available at https:// github.com/PaddlePaddle/RocketQA. 1 Introduction Recently, dense passage retrieval has become an important approach in the task of passage retrieval (Karpukhin et al., 2020) to identify relevant contents from a large corpus. The underlying idea is to represent both queries and passages as low-dimensional vectors (a.k.a., embeddings), so that the relevance can be measured via embedding similarity. Additionally, a subsequent procedure of passage re-ranking is widely adopted to further improve the retrieval results by incorporating a reranker (Qu et al., 2021; Luan et al., 2021). Such a two-stage procedure is particularly useful in a variety of natural language processing tasks, including question answering (Mao et al., 2021; Xiong ∗ Equal contribution. The work was done when Ruiyang Ren was doing internship at Baidu. † Corresponding authors. et al., 2020b), dialogue system (Ji et al., 2014; Henderson et al., 2017) and entity linking (Gillick et al., 2019; Wu et al., 2020). Following a retrieve-then-rerank way, the dense retriever in passage retrieval and the re-ranker in passage re-ranking jointly contribute to the final performance. Despite the"
2021.emnlp-main.224,2021.findings-acl.191,1,0.783125,"tuning pre-trained language models (PLMs) on labeled data (Lu et al., 2020; Karpukhin et al., 2020; Xiong et al., 2020a; Luan et al., 2021; Qu et al., 2021) . Our work follows the second class of approaches, which show better performance with less cost. There are two important tricks to train an effective dense retriever: (1) incorporating hard negatives during training (Karpukhin et al., 2020; Xiong et al., 2020a; Qu et al., 2021) and (2) distilling the knowledge from cross-encoder-based reranker into dual-encoder-based retriever (Izacard and Grave, 2020; Yang and Seo, 2020; Qu et al., 2021; Ren et al., 2021). Based on the retrieved passages from a retriever, PLM-based rerankers with the cross-encoder architecture have recently been applied on passage re-ranking to improve the retrieval results (Qiao et al., 2019; Nogueira and Cho, 2019; Wang et al., 2019; Yan et al., 2019), and yield substantial improvements over the traditional methods. Apart from separately considering the above two tasks, it has been proved that passage retrieval and passage re-ranking are actually highly related and dependent (Huang et al., 2020; Gao et al., 2020; Khattab and Zaharia, 2020). The retriever needs to capture the"
2021.emnlp-main.224,2021.acl-long.519,0,0.0233286,"ang et al., 2020; Gao et al., 2020; Khattab and Zaharia, 2020). The retriever needs to capture the relevance knowledge from the re-ranker, and the re-ranker should be specially optimized according to the preceding results of the retriever. Some efforts studied the possibility of leveraging the dependency of retriever and re-ranker, and try to enhance the connection between them in an alternative way (Qu et al., 2021; Yang et al., 2020; Huang et al., 2020). Furthermore, several studies attempted to jointly train the retriever and the reader for Open-domain Question Answering (Guu et al., 2020; Sachan et al., 2021; Karpukhin et al., 2020). Different from the prior studies, our method is a joint learning architecture of the dense passage retriever and the re-ranker. 3 Methodology In this section, we describe a novel joint training approach for dense passage retrieval and passage re-ranking (called RocketQAv2) Recently, dense passage retrieval has demonstrated better performance than traditional sparse 3.1 Overview retrieval methods (e.g., TF-IDF and BM25) on In this work, we consider two tasks including the task of passage retrieval. Existing approaches dense passage retrieval and passage re-ranking, of"
2021.emnlp-main.224,D19-1599,0,0.0282163,"t. There are two important tricks to train an effective dense retriever: (1) incorporating hard negatives during training (Karpukhin et al., 2020; Xiong et al., 2020a; Qu et al., 2021) and (2) distilling the knowledge from cross-encoder-based reranker into dual-encoder-based retriever (Izacard and Grave, 2020; Yang and Seo, 2020; Qu et al., 2021; Ren et al., 2021). Based on the retrieved passages from a retriever, PLM-based rerankers with the cross-encoder architecture have recently been applied on passage re-ranking to improve the retrieval results (Qiao et al., 2019; Nogueira and Cho, 2019; Wang et al., 2019; Yan et al., 2019), and yield substantial improvements over the traditional methods. Apart from separately considering the above two tasks, it has been proved that passage retrieval and passage re-ranking are actually highly related and dependent (Huang et al., 2020; Gao et al., 2020; Khattab and Zaharia, 2020). The retriever needs to capture the relevance knowledge from the re-ranker, and the re-ranker should be specially optimized according to the preceding results of the retriever. Some efforts studied the possibility of leveraging the dependency of retriever and re-ranker, and try to enha"
2021.emnlp-main.224,2020.emnlp-main.519,0,0.0184809,"sured via embedding similarity. Additionally, a subsequent procedure of passage re-ranking is widely adopted to further improve the retrieval results by incorporating a reranker (Qu et al., 2021; Luan et al., 2021). Such a two-stage procedure is particularly useful in a variety of natural language processing tasks, including question answering (Mao et al., 2021; Xiong ∗ Equal contribution. The work was done when Ruiyang Ren was doing internship at Baidu. † Corresponding authors. et al., 2020b), dialogue system (Ji et al., 2014; Henderson et al., 2017) and entity linking (Gillick et al., 2019; Wu et al., 2020). Following a retrieve-then-rerank way, the dense retriever in passage retrieval and the re-ranker in passage re-ranking jointly contribute to the final performance. Despite the fact that the two modules work as a pipeline during the inference stage, it has been found useful to train them in a correlated manner. For example, the retriever with a dual-encoder can be improved by distilling from the re-ranker with a more capable cross-encoder architecture (Qu et al., 2021; Yang et al., 2020), and the re-ranker can be improved with training instances generated from the retriever (Qu et al., 2021;"
2021.emnlp-main.333,P19-1098,0,0.0852126,"2 for training, validation and testing and truncate each document to 768 tokens. DUC Dataset We use the benchmark datasets from the Document Understanding Conferences (DUC) containing clusters of English news articles and human reference summaries. We use DUC 2002, 2003 and 2004 datesets which contain 60, 30 and 50 clusters of nearly 10 documents respectively. Four human reference summaries have been created for each document cluster by NIST assessors. Our model is trained on DUC 2002, validated on DUC 2003, and tested on DUC 2004. We apply the similar preprocessing method with previous work (Cho et al., 2019) and truncate each document to 768 tokens Training Configuration We use the base version of RoBERTa (Liu et al., 2019b) to initialize our models in all experiments. The optimizer is Adam (Kingma and Ba, 2014) with β1=0.9 and β2=0.999, and the learning rate is 0.03 for MultiNews and 0.015 for DUC. We apply learning rate warmup over the first 10000 steps and decay as in (Kingma and Ba, 2014). Gradient clipping with maximum gradient norm 2.0 is also utilized during training. All models are trained on 4 GPUs (Tesla V100) for about 10 epochs. We apply dropout with probability 0.1 before all linear"
2021.emnlp-main.333,N13-1136,0,0.056924,"he graph structure is effective to model relations methods such as similarity graph and discourse between sentences which is an essential point to graph. Sentences are the basic information units select interrelated summary-worthy sentences in and represented as nodes in the graph. And relaextractive summarization. Erkan and Radev (2004) tions between sentences are represented as edges. utilize a similarity graph to construct an unsuper- For example, a similarity graph can be built based vised summarization methods called LexRank. G- on cosine similarities between tf-idf representations Flow (Christensen et al., 2013) and DISCOBERT of sentences. Let G denotes a graph representation (Xu et al., 2020) both use discourse graphs to gen- matrix of the input documents, where G[i][j] indierate concise and informative summaries. Li et al. cates the tf-idf weights between sentence Si and Sj . 4064 source documents. Figure 2 illustrates the overall architecture of SgSum. Sub-graph Ranking Layer Graph Pooling Graph Pooling Graph Pooling Graph Pooling Sub-graph Encoder Graph Encoder Transformer … Doc1 Transformer DocN Figure 2: Model architecture of SgSum. Graph-based multidocument encoder takes tokenized documents as"
2021.emnlp-main.333,P19-1102,0,0.103266,". Thus, our model can be viewed as a sub-graph selection framework which means selecting a proper sub-graph from a whole graph. Furthermore, the graph structure can help to reorder the sentences in the summary to obtain a more coherent summary (Christensen et al., 2013). We order the summary by placing sentences with discourse relations next to each other. 4 4.1 Experiments Experimental Setup stated, we use the similarity graph by default as it is the most widely used in previous work. MultiNews Dataset The MultiNews dataset is a large-scale multi-document summarization dataset introduced by (Fabbri et al., 2019). It contains 56,216 articles-summary pairs and each example consists of 2-10 source documents and a humanwritten summary. Following their experimental settings, we split the dataset into 44,972/5,622/5,622 for training, validation and testing and truncate each document to 768 tokens. DUC Dataset We use the benchmark datasets from the Document Understanding Conferences (DUC) containing clusters of English news articles and human reference summaries. We use DUC 2002, 2003 and 2004 datesets which contain 60, 30 and 50 clusters of nearly 10 documents respectively. Four human reference summaries h"
2021.emnlp-main.333,N10-1131,0,0.0435273,"(2013) build multi-document graphs to identify pairwise ordering constraints over the Different from above studies, some work focus sentences by accounting for discourse relation- on the summary-level selection. Wan et al. (2015) ships between sentences. More recently, Yasunaga optimize the summarization performance directly 4070 based on the characteristics of summaries and rank summaries directly during inference. Bae et al. (2019), Paulus et al. (2017) and Celikyilmaz et al. (2018) use reinforcement learning to globally optimize summary-level performance. Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019) have attempted to a build two-stage document summarization. The first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Mendes et al. (2019) follow the extract-then-compress paradigm to train an extractor for content selection. Zhong et al. (2020) propose a novel extract-then-match framework which employs a sentence extractor to prune unnecessary information, then outputs a summary by matching models. These methods consider summary as a whole rather than individual sentences. Howev"
2021.emnlp-main.333,N18-1065,0,0.0474057,"Missing"
2021.emnlp-main.333,N09-1041,0,0.0748764,"30.95 34.52 34.02 35.41 Table 2: Evaluation results on the DUC2004 test set. We report R-1, R-2 and R-L scores, and follow the ROUGE setting of Cho et al. (2019).3 as an extra training resource to improve our model. The results show that single-document data boost the performance of our unified model a further step and achieve a new SOTA result on Multinews. Results on DUC Table 2 summarizes the evaluation results on the DUC2004 dataset. The first block shows four popular unsupervised baselines, and the second block shows several strong supervised baselines. We report the results of KSLSumm (Haghighi and Vanderwende, 2009), LexRank (Erkan and Radev, 2004), DPP (Kulesza and Taskar, 2011), Sim-DPP (Cho et al., 2019) following Cho et al. (2019). Besides, we also report the results of SubModular (Lin and Bilmes, 2010), StructSVM (Sipos et al., 2012) and PG (See et al., 2017) as strong baselines. The last block shows the results of our models. The results indicate that our model SgSum consistently outperforms most baselines, which further demonstrate the effectiveness of our model on different types of corpora. Additionally, we also test the performance of SgSum-extra which add CNN/DM data as a supplement. It is com"
2021.emnlp-main.333,D18-1446,0,0.0384836,"Missing"
2021.emnlp-main.333,C16-1023,1,0.8911,"Missing"
2021.emnlp-main.333,2020.acl-main.555,1,0.918979,"blems. In this paper, we encode source documents by a Hierarchical Transformer, which consists of several sharedweight single Transformers (Vaswani et al., 2017) that process each document independently. Each Transformer takes a tokenized document as input and outputs its sentence representations. This architecture enables our model to process much longer input. Graph Encoding To effectively capture the relations between sentences in source documents, we incorporate explicit graph representations of documents into the neural encoding process via a graph-informed attention mechanism similar to Li et al. (2020). Each sentence can collect information from other related sentences to capture global information from the whole input. The graphinformed attention mechanism extends the vanilla self-attention mechanism to consider the pairwise relations in explicit graph representations as: αij = Softmax(eij + Rij ) (1) where eij denotes the origin self-attention weights between sentences Si and Sj , αij denotes the adjusted weights by graph structure. The key point of the graph-based self-attention is the additional pairwise relation bias Rij , which is computed as a Gaussian bias of the weights of graph re"
2021.emnlp-main.333,D18-1205,1,0.825796,"del SgSum consistently outperforms most baselines, which further demonstrate the effectiveness of our model on different types of corpora. Additionally, we also test the performance of SgSum-extra which add CNN/DM data as a supplement. It is comparable to Sim-DPP baseline which also uses extra CNN/DM data to train a similarity model. And the results again show that singledocument data greatly improves the performance of our model. 4.3 Transfer Performances It is commonly known that deep neural networks achieved great improvement on SDS task recently (Liu and Lapata, 2019b; Zhong et al., 2020; Li et al., 2018a,b). However, such supervised models can not work well on MDS task because parallel data for mulit-document are scarce and costly to obtain. For example, the DUC dataset only contains tens of parallel MDS data. There is a pressing need 3 -n 2 -m -w 1.2 -c 95 -r 1000 -l 250 4068 -n 2 -m -w 1.2 -c 95 -r 1000 -l 100 Models Lead LexRank BERTSUMEXT SgSum R-1 40.21 40.27 41.28 43.61 R-2 12.13 12.63 12.05 14.07 R-L 37.13 37.50 37.18 39.50 Table 3: Transfer performance on MultiNews dataset Figure 3: Results on different graph types. Models KLSumm LexRank Extract+Rewrite BERTSUMEXT PG-MMR SgSum R-1 31"
2021.emnlp-main.333,D18-1441,1,0.819301,"del SgSum consistently outperforms most baselines, which further demonstrate the effectiveness of our model on different types of corpora. Additionally, we also test the performance of SgSum-extra which add CNN/DM data as a supplement. It is comparable to Sim-DPP baseline which also uses extra CNN/DM data to train a similarity model. And the results again show that singledocument data greatly improves the performance of our model. 4.3 Transfer Performances It is commonly known that deep neural networks achieved great improvement on SDS task recently (Liu and Lapata, 2019b; Zhong et al., 2020; Li et al., 2018a,b). However, such supervised models can not work well on MDS task because parallel data for mulit-document are scarce and costly to obtain. For example, the DUC dataset only contains tens of parallel MDS data. There is a pressing need 3 -n 2 -m -w 1.2 -c 95 -r 1000 -l 250 4068 -n 2 -m -w 1.2 -c 95 -r 1000 -l 100 Models Lead LexRank BERTSUMEXT SgSum R-1 40.21 40.27 41.28 43.61 R-2 12.13 12.63 12.05 14.07 R-L 37.13 37.50 37.18 39.50 Table 3: Transfer performance on MultiNews dataset Figure 3: Results on different graph types. Models KLSumm LexRank Extract+Rewrite BERTSUMEXT PG-MMR SgSum R-1 31"
2021.emnlp-main.333,P02-1058,0,0.294435,"ified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. However, these works only consider the graph structure of source documents, but neglect the graph structures of summaries which are also important to generate coherent and informative summaries. 5.2 Sentence or Summary-level Extraction Extractive summarization methods usually produce a summary by selecting some original sentences in the document set by a sentence-level extractor. Early models employ rule-based methods to score and select sentenecs (Lin and Hovy, 2002; Lin and Bilmes, 2011; Takamura and Okumura, 2009; 5 Related Work Schilder and Kondadadi, 2008). Recently, SUM5.1 Graph-based Summarization MARUNNER (Nallapati et al., 2017) adopt an encoder based on Recurrent Neural Networks which Most previous graph extractive MDS approaches aim to extract salient textual units from docu- is the earliest neural summarization model. SUMO (Liu et al., 2019a) capitalizes on the notion of strucments based on graph structure representations tured attention to induce a multi-root dependency of sentences. Erkan and Radev (2004) introduce tree representation of the"
2021.emnlp-main.333,P11-1052,0,0.0483401,", which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. However, these works only consider the graph structure of source documents, but neglect the graph structures of summaries which are also important to generate coherent and informative summaries. 5.2 Sentence or Summary-level Extraction Extractive summarization methods usually produce a summary by selecting some original sentences in the document set by a sentence-level extractor. Early models employ rule-based methods to score and select sentenecs (Lin and Hovy, 2002; Lin and Bilmes, 2011; Takamura and Okumura, 2009; 5 Related Work Schilder and Kondadadi, 2008). Recently, SUM5.1 Graph-based Summarization MARUNNER (Nallapati et al., 2017) adopt an encoder based on Recurrent Neural Networks which Most previous graph extractive MDS approaches aim to extract salient textual units from docu- is the earliest neural summarization model. SUMO (Liu et al., 2019a) capitalizes on the notion of strucments based on graph structure representations tured attention to induce a multi-root dependency of sentences. Erkan and Radev (2004) introduce tree representation of the document. However, al"
2021.emnlp-main.333,P19-1500,0,0.0845786,"tion and a high-way layer normalization are 4065 applied after the graph-informed attention mechanism. These three components form the graph encoding layers. Graph Pooling In the MDS task, information is more massive and relations between sentences are much more complex. So it is necessary to have an overview of the central meaning of multi-document input. Zhong et al. (2020) generate a document representation with Siamese-BERT to guide the training and inference process. In this paper, based on the graph representation of documents, we apply a multi-head weighted-pooling operation similar to Liu and Lapata (2019a) to capture the global semantic information of source documents. It takes sentence representations in the source graph as input and outputs an overall representation of them (denoted as D), which provides global information of documents for both the sentence and summary selection processes. Let xi denotes the graph representation of sentence Si . For each head z ∈ {1, ..., nhead }, we first transform xi into attention scores azi and value vectors bzi , then we calculate an attention distribution a ˆzi over all sentences in the source graph based on attention scores: azi = Waz xi (3) bzi = Wb"
2021.emnlp-main.333,D19-1387,0,0.0905449,"tion and a high-way layer normalization are 4065 applied after the graph-informed attention mechanism. These three components form the graph encoding layers. Graph Pooling In the MDS task, information is more massive and relations between sentences are much more complex. So it is necessary to have an overview of the central meaning of multi-document input. Zhong et al. (2020) generate a document representation with Siamese-BERT to guide the training and inference process. In this paper, based on the graph representation of documents, we apply a multi-head weighted-pooling operation similar to Liu and Lapata (2019a) to capture the global semantic information of source documents. It takes sentence representations in the source graph as input and outputs an overall representation of them (denoted as D), which provides global information of documents for both the sentence and summary selection processes. Let xi denotes the graph representation of sentence Si . For each head z ∈ {1, ..., nhead }, we first transform xi into attention scores azi and value vectors bzi , then we calculate an attention distribution a ˆzi over all sentences in the source graph based on attention scores: azi = Waz xi (3) bzi = Wb"
2021.emnlp-main.333,P08-2052,0,0.0316995,"range of context and conveys rich relations between phrases. However, these works only consider the graph structure of source documents, but neglect the graph structures of summaries which are also important to generate coherent and informative summaries. 5.2 Sentence or Summary-level Extraction Extractive summarization methods usually produce a summary by selecting some original sentences in the document set by a sentence-level extractor. Early models employ rule-based methods to score and select sentenecs (Lin and Hovy, 2002; Lin and Bilmes, 2011; Takamura and Okumura, 2009; 5 Related Work Schilder and Kondadadi, 2008). Recently, SUM5.1 Graph-based Summarization MARUNNER (Nallapati et al., 2017) adopt an encoder based on Recurrent Neural Networks which Most previous graph extractive MDS approaches aim to extract salient textual units from docu- is the earliest neural summarization model. SUMO (Liu et al., 2019a) capitalizes on the notion of strucments based on graph structure representations tured attention to induce a multi-root dependency of sentences. Erkan and Radev (2004) introduce tree representation of the document. However, all LexRank to compute sentence importance based these models belong to sent"
2021.emnlp-main.333,P17-1099,0,0.03234,"he performance of our unified model a further step and achieve a new SOTA result on Multinews. Results on DUC Table 2 summarizes the evaluation results on the DUC2004 dataset. The first block shows four popular unsupervised baselines, and the second block shows several strong supervised baselines. We report the results of KSLSumm (Haghighi and Vanderwende, 2009), LexRank (Erkan and Radev, 2004), DPP (Kulesza and Taskar, 2011), Sim-DPP (Cho et al., 2019) following Cho et al. (2019). Besides, we also report the results of SubModular (Lin and Bilmes, 2010), StructSVM (Sipos et al., 2012) and PG (See et al., 2017) as strong baselines. The last block shows the results of our models. The results indicate that our model SgSum consistently outperforms most baselines, which further demonstrate the effectiveness of our model on different types of corpora. Additionally, we also test the performance of SgSum-extra which add CNN/DM data as a supplement. It is comparable to Sim-DPP baseline which also uses extra CNN/DM data to train a similarity model. And the results again show that singledocument data greatly improves the performance of our model. 4.3 Transfer Performances It is commonly known that deep neural"
2021.emnlp-main.333,N19-1173,0,0.112281,"sets from the Document Understanding Conferences (DUC) containing clusters of English news articles and human reference summaries. We use DUC 2002, 2003 and 2004 datesets which contain 60, 30 and 50 clusters of nearly 10 documents respectively. Four human reference summaries have been created for each document cluster by NIST assessors. Our model is trained on DUC 2002, validated on DUC 2003, and tested on DUC 2004. We apply the similar preprocessing method with previous work (Cho et al., 2019) and truncate each document to 768 tokens Training Configuration We use the base version of RoBERTa (Liu et al., 2019b) to initialize our models in all experiments. The optimizer is Adam (Kingma and Ba, 2014) with β1=0.9 and β2=0.999, and the learning rate is 0.03 for MultiNews and 0.015 for DUC. We apply learning rate warmup over the first 10000 steps and decay as in (Kingma and Ba, 2014). Gradient clipping with maximum gradient norm 2.0 is also utilized during training. All models are trained on 4 GPUs (Tesla V100) for about 10 epochs. We apply dropout with probability 0.1 before all linear layers. The number of hidden units in our models is set as 256, the feedforward hidden size is 1,024, and the number"
2021.emnlp-main.333,E12-1023,0,0.0352659,"single-document data boost the performance of our unified model a further step and achieve a new SOTA result on Multinews. Results on DUC Table 2 summarizes the evaluation results on the DUC2004 dataset. The first block shows four popular unsupervised baselines, and the second block shows several strong supervised baselines. We report the results of KSLSumm (Haghighi and Vanderwende, 2009), LexRank (Erkan and Radev, 2004), DPP (Kulesza and Taskar, 2011), Sim-DPP (Cho et al., 2019) following Cho et al. (2019). Besides, we also report the results of SubModular (Lin and Bilmes, 2010), StructSVM (Sipos et al., 2012) and PG (See et al., 2017) as strong baselines. The last block shows the results of our models. The results indicate that our model SgSum consistently outperforms most baselines, which further demonstrate the effectiveness of our model on different types of corpora. Additionally, we also test the performance of SgSum-extra which add CNN/DM data as a supplement. It is comparable to Sim-DPP baseline which also uses extra CNN/DM data to train a similarity model. And the results again show that singledocument data greatly improves the performance of our model. 4.3 Transfer Performances It is commo"
2021.emnlp-main.333,2021.ccl-1.108,0,0.045059,"Missing"
2021.emnlp-main.333,C18-1146,0,0.0222071,"e an end-to-end model which is trained on single-document data but can work well with multiple-document input. In this section we do further experiments to verify the transfer ability of our model from single to multi-document task. We follow the experiment setups of Lebanoff et al. (2018), and compare with several strong baseline models: (1) BERTSUMEXT (Liu and Lapata, 2019b), an extractive method with pre-trained LM model; (2) PG-MMR (Lebanoff et al., 2018), an encoder-decoder model which exploits the maximal marginal relevance method to select representative sentences; (3) Extract+Rewrite (Song et al., 2018), is a recent approach that scores sentences using LexRank and generates a title-like summary for each sentence using an encoder-decoder model. We follow the results of Lebanoff et al. (2018). Table 3 and Table 4 demonstrate the results on MultiNews and DUC2004 respectively. Models SgSum w/o s.g. enc w/o s.g. rank w/o s.g. enc&rank w/o graph enc w/o all R-1 47.36 46.87 46.91 46.69 46.21 45.43 R-2 18.61 17.93 17.97 17.64 17.12 16.62 R-L 43.13 42.67 42.80 42.48 42.11 41.32 Table 5: Ablation study on the MultiNews test set. s.g. is the abbreviation for sub-graph. fer ability which can reduce the"
2021.emnlp-main.333,N19-1397,0,0.0189253,"ly, Yasunaga optimize the summarization performance directly 4070 based on the characteristics of summaries and rank summaries directly during inference. Bae et al. (2019), Paulus et al. (2017) and Celikyilmaz et al. (2018) use reinforcement learning to globally optimize summary-level performance. Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019) have attempted to a build two-stage document summarization. The first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Mendes et al. (2019) follow the extract-then-compress paradigm to train an extractor for content selection. Zhong et al. (2020) propose a novel extract-then-match framework which employs a sentence extractor to prune unnecessary information, then outputs a summary by matching models. These methods consider summary as a whole rather than individual sentences. However, they neglect the relations between sentences during both scoring and selecting. 5.3 Conclusion We propose a novel framework SgSum which transforms the MDS task into the problem of sub-graph selection. SgSum captures the relations between sentences by"
2021.emnlp-main.333,N18-1158,0,0.0213222,"salient textual units from docu- is the earliest neural summarization model. SUMO (Liu et al., 2019a) capitalizes on the notion of strucments based on graph structure representations tured attention to induce a multi-root dependency of sentences. Erkan and Radev (2004) introduce tree representation of the document. However, all LexRank to compute sentence importance based these models belong to sentence-level extractors on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Chris- which select high score sentences individually and might raise redundancy (Narayan et al., 2018). tensen et al. (2013) build multi-document graphs to identify pairwise ordering constraints over the Different from above studies, some work focus sentences by accounting for discourse relation- on the summary-level selection. Wan et al. (2015) ships between sentences. More recently, Yasunaga optimize the summarization performance directly 4070 based on the characteristics of summaries and rank summaries directly during inference. Bae et al. (2019), Paulus et al. (2017) and Celikyilmaz et al. (2018) use reinforcement learning to globally optimize summary-level performance. Recent studies (Aly"
2021.emnlp-main.333,2020.acl-main.553,0,0.0666884,"sub-graph view is more appropriate to generate a coherent and concise summary. This is also the key point of our framework. Additionally, important sentences usually build up crucial sub-graphs. So it is a simple but efficient way to generate candidate sub-graphs based on those salient sentences. 3 3.1 Methodology Graph-based Multi-document Encoder Hierarchical Transformer Most previous works (Cao et al., 2017; Jin et al., 2020; Wang et al., 2017) did not consider the multi-document structure. They simply concatenate all documents together and treat the MDS as a special SDS with longer input. Wang et al. (2020) preprocess the multi-document input by truncating lead sentences averagely from each document, then concatenating them together as the MDS input. These preprocessing methods are simple ways to help the model encode multi-document inputs. But they do not make full use of the source document structures. Lead sentences extracted from each document might be similar with each other and result in redundant and incoherent problems. In this paper, we encode source documents by a Hierarchical Transformer, which consists of several sharedweight single Transformers (Vaswani et al., 2017) that process ea"
2021.emnlp-main.333,D17-1020,0,0.0195624,"he sub-graph structures, we can distinguish the quality of different candidate summaries and finally select the best one. Compared with the whole document graph view, sub-graph view is more appropriate to generate a coherent and concise summary. This is also the key point of our framework. Additionally, important sentences usually build up crucial sub-graphs. So it is a simple but efficient way to generate candidate sub-graphs based on those salient sentences. 3 3.1 Methodology Graph-based Multi-document Encoder Hierarchical Transformer Most previous works (Cao et al., 2017; Jin et al., 2020; Wang et al., 2017) did not consider the multi-document structure. They simply concatenate all documents together and treat the MDS as a special SDS with longer input. Wang et al. (2020) preprocess the multi-document input by truncating lead sentences averagely from each document, then concatenating them together as the MDS input. These preprocessing methods are simple ways to help the model encode multi-document inputs. But they do not make full use of the source document structures. Lead sentences extracted from each document might be similar with each other and result in redundant and incoherent problems. In"
2021.emnlp-main.333,2021.acl-long.472,1,0.782914,"our proposed sub-graph selection framework. et al. (2017) build on the approximate discourse graph model and account for macro-level features in sentences to improve sentence salience prediction. Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes an entity linking graph to capture the global dependencies between sentences. Li et al. (2020) incorporate explicit graph representations to the neural architecture based on a novel graph-informed selfattention mechanism. It is the first work to effectively combine graph structures with abstractive MDS model. Wu et al. (2021) present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. However, these works only consider the graph structure of source documents, but neglect the graph structures of summaries which are also important to generate coherent and informative summaries. 5.2 Sentence or Summary-level Extraction Extractive summarization methods usually produce a summary by selecting some original sentences in the document set by a sentence-le"
2021.emnlp-main.333,2020.acl-main.451,0,0.0429933,"ourse between sentences which is an essential point to graph. Sentences are the basic information units select interrelated summary-worthy sentences in and represented as nodes in the graph. And relaextractive summarization. Erkan and Radev (2004) tions between sentences are represented as edges. utilize a similarity graph to construct an unsuper- For example, a similarity graph can be built based vised summarization methods called LexRank. G- on cosine similarities between tf-idf representations Flow (Christensen et al., 2013) and DISCOBERT of sentences. Let G denotes a graph representation (Xu et al., 2020) both use discourse graphs to gen- matrix of the input documents, where G[i][j] indierate concise and informative summaries. Li et al. cates the tf-idf weights between sentence Si and Sj . 4064 source documents. Figure 2 illustrates the overall architecture of SgSum. Sub-graph Ranking Layer Graph Pooling Graph Pooling Graph Pooling Graph Pooling Sub-graph Encoder Graph Encoder Transformer … Doc1 Transformer DocN Figure 2: Model architecture of SgSum. Graph-based multidocument encoder takes tokenized documents as input and outputs sentence representations after graph encoding layers. Candidate"
2021.emnlp-main.333,K17-1045,0,0.0334523,"Missing"
2021.emnlp-main.333,K19-1074,0,0.0180512,"to identify pairwise ordering constraints over the Different from above studies, some work focus sentences by accounting for discourse relation- on the summary-level selection. Wan et al. (2015) ships between sentences. More recently, Yasunaga optimize the summarization performance directly 4070 based on the characteristics of summaries and rank summaries directly during inference. Bae et al. (2019), Paulus et al. (2017) and Celikyilmaz et al. (2018) use reinforcement learning to globally optimize summary-level performance. Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019) have attempted to a build two-stage document summarization. The first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Mendes et al. (2019) follow the extract-then-compress paradigm to train an extractor for content selection. Zhong et al. (2020) propose a novel extract-then-match framework which employs a sentence extractor to prune unnecessary information, then outputs a summary by matching models. These methods consider summary as a whole rather than individual sentences. However, they neglect the"
2021.emnlp-main.333,2020.acl-main.552,0,0.0746797,"hen,xiaoxinyan wu_hua,wanghaifeng}@baidu.com Abstract These models (called sentence-level extractors) do not consider summary as a whole but a combinaMost of existing extractive multi-document tion of independent sentences. This may cause summarization (MDS) methods score each incoherent and redundant problem, and result in sentence individually and extract salient sena poor summary even if the summary consists of tences one by one to compose a summary, high score sentences. Some works (Wan et al., which have two main drawbacks: (1) neglecting both the intra and cross-document relations 2015; Zhong et al., 2020) treat summary as a whole between sentences; (2) neglecting the coherunit and try to solve the weakness of sentenceence and conciseness of the whole summary. level extractors by using a summary-level extracIn this paper, we propose a novel MDS frametor. However, these models neglect the intra and work (SgSum) to formulate the MDS task as a cross-document relations between sentences which sub-graph selection problem, in which source also have benefits for extracting salient sentences, documents are regarded as a relation graph of detecting redundancy and generating overall cohersentences (e.g.,"
2021.emnlp-main.356,2020.tacl-1.30,0,0.0230087,"lingual corpora can gual, and cross-lingual conversational recombring performance improvement in comparison mendation baselines on DuRecDial 2.0. Experwith monolingual task setting, such as for the tasks iment results show that the use of additional English data can bring performance improveof task-oriented dialog (Schuster et al., 2019b), ment for Chinese conversational recommensemantic parsing (Li et al., 2021), QA and readdation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role labelling In recent years, there has been a significant in- (Akbik et al., 2015) and NLI (Conneau et al., 2018). crease in the research topic of conversational rec- Therefore it is necessary to create multilingual conversational recommendation dataset that might enommendation due"
2021.emnlp-main.356,2020.acl-main.747,0,0.0765699,"Missing"
2021.emnlp-main.356,D18-1269,0,0.0242468,"ddation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role labelling In recent years, there has been a significant in- (Akbik et al., 2015) and NLI (Conneau et al., 2018). crease in the research topic of conversational rec- Therefore it is necessary to create multilingual conversational recommendation dataset that might enommendation due to the rise of voice-based bots (Kang et al., 2019; Li et al., 2018; Sun and Zhang, hance model performance when compared with 2018; Christakopoulou et al., 2016; Warnestal, monolingual training setting, and it could provide 2005). These works focus on how to provide recom- a new benchmark dataset for the study of multilingual modeling techniques. mendation service in a more user-friendly manner To facilitate the study of this"
2021.emnlp-main.356,2020.emnlp-main.438,0,0.0859544,"Missing"
2021.emnlp-main.356,2020.emnlp-main.654,0,0.0324503,"ntents and slots (Li et al., dataset (DuRecDial 2.0) to enable researchers 2018; Kang et al., 2019). Recently more and more to explore a challenging task of multilingual and cross-lingual conversational recommendaefforts are devoted to the research line of the section. The difference between DuRecDial 2.0 ond category and many datasets have been created, and existing conversational recommendation including English dialog datasets (Dodge et al., datasets is that the data item (Profile, Goal, 2016; Li et al., 2018; Kang et al., 2019; Moon Knowledge, Context, Response) in DuRecDial et al., 2019; Hayati et al., 2020) and Chinese dialog 2.0 is annotated in two languages, both Endatasets (Liu et al., 2020b; Zhou et al., 2020). glish and Chinese, while other datasets are built with the setting of a single language. We However, to the best of our knowledge, almost collect 8.2k dialogs aligned across English and all these datasets are constructed in the setting Chinese languages (16.5k dialogs and 255k utof a single language, and there is no publicly terances in total) that are annotated by crowdavailable multilingual dataset for conversational sourced workers with strict quality control prorecommendation. Pre"
2021.emnlp-main.356,D19-1249,0,0.0164608,"ndation. Previous work on other NLP cedure. We then build monolingual, multilintasks have proved that multilingual corpora can gual, and cross-lingual conversational recombring performance improvement in comparison mendation baselines on DuRecDial 2.0. Experwith monolingual task setting, such as for the tasks iment results show that the use of additional English data can bring performance improveof task-oriented dialog (Schuster et al., 2019b), ment for Chinese conversational recommensemantic parsing (Li et al., 2021), QA and readdation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role labelling In recent years, there has been a significant in- (Akbik et al., 2015) and NLI (Conneau et al., 2018). crease in the research topic of conversational rec- Therefo"
2021.emnlp-main.356,D19-1203,0,0.210984,"ng2 , Zheng-Yu Niu2 , Hua Wu2 , Wanxiang Che1† 1 Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, Harbin, China 2 Baidu Inc., Beijing, China {zmliu, car}@ir.hit.edu.cn {wanghaifeng, niuzhengyu, wu_hua}@baidu.com Abstract et al., 2016; Sun and Zhang, 2018); (2) non-task dialog-modeling approaches that can conduct more In this paper, we provide a bilingual paralfree-form interactions for recommendation, withlel human-to-human recommendation dialog out pre-defined user intents and slots (Li et al., dataset (DuRecDial 2.0) to enable researchers 2018; Kang et al., 2019). Recently more and more to explore a challenging task of multilingual and cross-lingual conversational recommendaefforts are devoted to the research line of the section. The difference between DuRecDial 2.0 ond category and many datasets have been created, and existing conversational recommendation including English dialog datasets (Dodge et al., datasets is that the data item (Profile, Goal, 2016; Li et al., 2018; Kang et al., 2019; Moon Knowledge, Context, Response) in DuRecDial et al., 2019; Hayati et al., 2020) and Chinese dialog 2.0 is annotated in two languages, both Endatasets (Liu et"
2021.emnlp-main.356,C12-1089,0,0.0384622,"a can bring performance improveof task-oriented dialog (Schuster et al., 2019b), ment for Chinese conversational recommensemantic parsing (Li et al., 2021), QA and readdation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role labelling In recent years, there has been a significant in- (Akbik et al., 2015) and NLI (Conneau et al., 2018). crease in the research topic of conversational rec- Therefore it is necessary to create multilingual conversational recommendation dataset that might enommendation due to the rise of voice-based bots (Kang et al., 2019; Li et al., 2018; Sun and Zhang, hance model performance when compared with 2018; Christakopoulou et al., 2016; Warnestal, monolingual training setting, and it could provide 2005). These works focus on how to provi"
2021.emnlp-main.356,2020.acl-main.653,0,0.0690059,"Missing"
2021.emnlp-main.356,2021.eacl-main.257,0,0.0464825,"Missing"
2021.emnlp-main.356,N16-1014,0,0.0884497,"Missing"
2021.emnlp-main.356,2020.tacl-1.47,0,0.0591359,", 2019). Recently more and more to explore a challenging task of multilingual and cross-lingual conversational recommendaefforts are devoted to the research line of the section. The difference between DuRecDial 2.0 ond category and many datasets have been created, and existing conversational recommendation including English dialog datasets (Dodge et al., datasets is that the data item (Profile, Goal, 2016; Li et al., 2018; Kang et al., 2019; Moon Knowledge, Context, Response) in DuRecDial et al., 2019; Hayati et al., 2020) and Chinese dialog 2.0 is annotated in two languages, both Endatasets (Liu et al., 2020b; Zhou et al., 2020). glish and Chinese, while other datasets are built with the setting of a single language. We However, to the best of our knowledge, almost collect 8.2k dialogs aligned across English and all these datasets are constructed in the setting Chinese languages (16.5k dialogs and 255k utof a single language, and there is no publicly terances in total) that are annotated by crowdavailable multilingual dataset for conversational sourced workers with strict quality control prorecommendation. Previous work on other NLP cedure. We then build monolingual, multilintasks have proved tha"
2021.emnlp-main.356,L18-1560,0,0.012717,"riented dialog (Schuster et al., 2019b), ment for Chinese conversational recommensemantic parsing (Li et al., 2021), QA and readdation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role labelling In recent years, there has been a significant in- (Akbik et al., 2015) and NLI (Conneau et al., 2018). crease in the research topic of conversational rec- Therefore it is necessary to create multilingual conversational recommendation dataset that might enommendation due to the rise of voice-based bots (Kang et al., 2019; Li et al., 2018; Sun and Zhang, hance model performance when compared with 2018; Christakopoulou et al., 2016; Warnestal, monolingual training setting, and it could provide 2005). These works focus on how to provide recom- a new benchmark dataset for"
2021.emnlp-main.356,2020.acl-main.98,1,0.911879,", 2019). Recently more and more to explore a challenging task of multilingual and cross-lingual conversational recommendaefforts are devoted to the research line of the section. The difference between DuRecDial 2.0 ond category and many datasets have been created, and existing conversational recommendation including English dialog datasets (Dodge et al., datasets is that the data item (Profile, Goal, 2016; Li et al., 2018; Kang et al., 2019; Moon Knowledge, Context, Response) in DuRecDial et al., 2019; Hayati et al., 2020) and Chinese dialog 2.0 is annotated in two languages, both Endatasets (Liu et al., 2020b; Zhou et al., 2020). glish and Chinese, while other datasets are built with the setting of a single language. We However, to the best of our knowledge, almost collect 8.2k dialogs aligned across English and all these datasets are constructed in the setting Chinese languages (16.5k dialogs and 255k utof a single language, and there is no publicly terances in total) that are annotated by crowdavailable multilingual dataset for conversational sourced workers with strict quality control prorecommendation. Previous work on other NLP cedure. We then build monolingual, multilintasks have proved tha"
2021.emnlp-main.356,2020.lrec-1.494,0,0.0246919,"Missing"
2021.emnlp-main.356,P19-1081,0,0.0642011,"Missing"
2021.emnlp-main.356,P19-1369,1,0.858284,"Missing"
2021.emnlp-main.356,P17-1163,0,0.0377515,"Missing"
2021.emnlp-main.356,2020.coling-main.365,0,0.0251136,"ore and more to explore a challenging task of multilingual and cross-lingual conversational recommendaefforts are devoted to the research line of the section. The difference between DuRecDial 2.0 ond category and many datasets have been created, and existing conversational recommendation including English dialog datasets (Dodge et al., datasets is that the data item (Profile, Goal, 2016; Li et al., 2018; Kang et al., 2019; Moon Knowledge, Context, Response) in DuRecDial et al., 2019; Hayati et al., 2020) and Chinese dialog 2.0 is annotated in two languages, both Endatasets (Liu et al., 2020b; Zhou et al., 2020). glish and Chinese, while other datasets are built with the setting of a single language. We However, to the best of our knowledge, almost collect 8.2k dialogs aligned across English and all these datasets are constructed in the setting Chinese languages (16.5k dialogs and 255k utof a single language, and there is no publicly terances in total) that are annotated by crowdavailable multilingual dataset for conversational sourced workers with strict quality control prorecommendation. Previous work on other NLP cedure. We then build monolingual, multilintasks have proved that multilingual corpor"
2021.emnlp-main.356,Q17-1022,0,0.0479985,"Missing"
2021.emnlp-main.356,P02-1040,0,0.113776,"Missing"
2021.emnlp-main.356,N19-1380,0,0.108235,"and there is no publicly terances in total) that are annotated by crowdavailable multilingual dataset for conversational sourced workers with strict quality control prorecommendation. Previous work on other NLP cedure. We then build monolingual, multilintasks have proved that multilingual corpora can gual, and cross-lingual conversational recombring performance improvement in comparison mendation baselines on DuRecDial 2.0. Experwith monolingual task setting, such as for the tasks iment results show that the use of additional English data can bring performance improveof task-oriented dialog (Schuster et al., 2019b), ment for Chinese conversational recommensemantic parsing (Li et al., 2021), QA and readdation, indicating the benefits of DuRecDial ing comprehension (Jing et al., 2019; Lewis et al., 2.0. Finally, this dataset provides a challeng2020; Artetxe et al., 2020; Clark et al., 2020; Hu ing testbed for future studies of monolingual, et al., 2020; Hardalov et al., 2020), machine transmultilingual, and cross-lingual conversational 1 lation(Johnson et al., 2017), document classificarecommendation. tion (Lewis et al., 2004; Klementiev et al., 2012; 1 Introduction Schwenk and Li, 2018), semantic role"
2021.emnlp-main.707,P01-1008,0,0.0422567,"e., ciency with augmented data. In summary, we make data augmentation, which addresses both chal- the following contributions. lenges discussed above in a resource-cheap way. • We present a simple and resource-cheap data augThe idea of data augmentation is automatically mentation framework for cross-domain text-togenerating noisy labeled data using some delibSQL parsing with no human intervention.1 erately designed method, and the technique has • As the key component for our framework, we probeen successfully applied to a wide range of NLP pose a hierarchical SQL-to-question generation tasks (Barzilay and McKeown, 2001; Jia and Liang, model to obtain more reliable NL questions. 2016). In our cross-domain text-to-SQL task, we can directly generate labeled data over unseen DBs • In order to improve training efficiency, we proas extra training data. The key of data augmenpose a simple sampling strategy to utilize genertation is how to improve the quality of generated ated data, which is of relatively larger scale than data. As two prior works, Yu et al. (2018a) manuoriginal training data. ally align question tokens and DB elements in the 1 We release the code at https://github.com/ corresponding SQL query, in"
2021.emnlp-main.707,N19-1423,0,0.017965,"r each dataset, the first major row shows previously reported results, and the second major row gives results of our base parsers without and with data augmentation. To compare previous data augmentation methHyper-parameter settings. For each parser, we ods, we also re-implement the flat one-stage genuse default parameter settings in their released code. eration approach (FLAT) proposed by Guo et al. All these parsers are enhanced with vanilla (in (2018). We do not implement the pattern-based contrast to task-specific) pretraining models, i.e., data augmentation approach (PATTERN) of Yu BERT (Devlin et al., 2019), including IRNet-Ext. et al. (2018a) due to its requirement of human inIn order to avoid the effect of performance vitervention. Moreover, their large performance imbrations9 , we run each model for 5 times with provement is obtained over a very weak baseline. 8 The comparison of V2 and V3 is discussed at https: Performance of our baseline parsers. On Wik//github.com/microsoft/rat-sql/issues/12. iSQL, the averaged performance of our SQLova 9 Please see issues proposed at the github of RATSQL parser is lower than their reported performance by model, such as https://github.com/microsoft/ rat-sq"
2021.emnlp-main.707,P17-2090,0,0.0298472,"ta of parsing models. We conduct this experiment augmentation has been widely and successfully on the Spider dataset using IRNet model based on adopted in the computer vision field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge base, semantic parsing aims to generate a"
2021.emnlp-main.707,P16-1154,0,0.0353237,"mainly considers as illustrated by the last two examples in Figure 3. the informativeness aspect of generated NL questions. We leave such evaluation and analysis as From the perspective of SQL syntax, HAVING and GROUP_BY, are naturally bundled together, future work, which will certainly help us better understand our proposed approach. and thus are put into one clause, as shown in the third example of Figure 3. LIMIT and ORDER_BY 2.3 Clause-to-subquestion Translation Model are similarly handled. We adopt the standard Seq2Seq model with From the second perspective, some keywords copy mechanism (Gu et al., 2016) for clause-toare not explicitly expressed in NL questions. In subquestion translation, which is also used in our other words, there is a mismatch between intents expressed in NL questions and the implementa- baseline, i.e., flat SQL-to-question translation, with tion details in SQL queries. To better align them, the same hyper-parameter settings. In the input layer, we represent every SQL towe follow IRNet (Guo et al., 2019) and combine GROUP_BY with either SELECT or ORDER_BY. ken by concatenating two embeddings, i.e., word (token as string) embedding, and token type (colFor a nested SQL quer"
2021.emnlp-main.707,D18-1188,0,0.105257,"generated questions, since very complex questions are rare in the training data. Please kindly note that our simple ASTG-based generation procedure can produce a lot of patterns unseen in the original data, because our generation is at production rule level. This is advantageous from the data variety perspective. Moreover, given a DB, we only keep executable SQL queries for correctness check. 2.2 Hierarchical SQL-to-Question Generation Given an SQL query, especially a complex one, it is difficult to generate an NL question that represents exactly same meaning. In their data augmentation work, Guo et al. (2018) use a vanilla Seq2Seq model to translate SQL queries into NL questions and obtain performance boost on WikiSQL consisting of simple queries. However, as shown in Table 2, we find performance consistently drops on all datasets over our strong baselines, which is largely due to the quality issue of generated NL questions, as illustrated in Table 3. This work proposes a hierarchical SQL-toBeing a program language, all SQL queries can be represented as nested tree structures, as depicted in Figure 2-B according to some context-free grammar. In fact, most text-to-SQL parsers proposed recently adop"
2021.emnlp-main.707,P19-1444,0,0.336098,"ikiSQL consisting of simple queries. However, as shown in Table 2, we find performance consistently drops on all datasets over our strong baselines, which is largely due to the quality issue of generated NL questions, as illustrated in Table 3. This work proposes a hierarchical SQL-toBeing a program language, all SQL queries can be represented as nested tree structures, as depicted in Figure 2-B according to some context-free grammar. In fact, most text-to-SQL parsers proposed recently adopt the abstract syntax tree representation at the decoding stage (Yin and Neubig, 2018; Yu et al., 2018a; Guo et al., 2019; Wang et al., 2020a). Following those works, we design a general ASTG that can cover all SQL patterns in our adopted benchmark datasets. Due to space limita2 As discussed in the logic form-based semantic parsing tion, Figure 2 shows a fraction of the production work of Herzig and Berant (2019), distribution mismatch is rules. mainly caused by insufficient coverage of logical form temAccording to our ASTG, the SQL query in Fig- plates. 8976 question generation model to produce higher- much easier to translate clauses to subquestions quality NL questions. The idea is motivated by compared with"
2021.emnlp-main.707,D19-1394,0,0.0189092,"ing a program language, all SQL queries can be represented as nested tree structures, as depicted in Figure 2-B according to some context-free grammar. In fact, most text-to-SQL parsers proposed recently adopt the abstract syntax tree representation at the decoding stage (Yin and Neubig, 2018; Yu et al., 2018a; Guo et al., 2019; Wang et al., 2020a). Following those works, we design a general ASTG that can cover all SQL patterns in our adopted benchmark datasets. Due to space limita2 As discussed in the logic form-based semantic parsing tion, Figure 2 shows a fraction of the production work of Herzig and Berant (2019), distribution mismatch is rules. mainly caused by insufficient coverage of logical form temAccording to our ASTG, the SQL query in Fig- plates. 8976 question generation model to produce higher- much easier to translate clauses to subquestions quality NL questions. The idea is motivated by compared with direct SQL-to-question translation. our observation that there is a strong segment-level We use a standard copy-based Seq2Seq model (Gu mapping between SQL queries and corresponding et al., 2016) for clause-to-subquestion generation. questions, as shown in Figure 3. For example, the The details"
2021.emnlp-main.707,2020.acl-main.677,0,0.257833,"uSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement. Figure 1: An example of the text-to-SQL parsing task. where all question/SQL pairs of train/dev/test sets are generated against the same DB. In order to deal with the more realistic setting where DBs in the evaluation phase are unseen in the training data, researchers propose several cross-domain datasets, such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b) in English, and DuSQL (Wang et al., 2020b) in Chinese. All three datasets adopt the DB-level data splitting, meaning that a DB and all its corresponding question/SQL pairs can appear in only one of the train/dev/test sets. Cross-domain text-to-SQL parsing has two major challenges. First, unseen DBs usually introduce 1 Introduction new schemas, such as new table/column names and unknown semantics of inter-table relationships. Given a natural language (NL) question and a relaTherefore, it is crucial for a parsing model to have tional database (DB), the text-to-SQL parsing task strong generalization ability. The second challenge aims t"
2021.emnlp-main.707,2020.emnlp-main.562,1,0.725224,"uSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement. Figure 1: An example of the text-to-SQL parsing task. where all question/SQL pairs of train/dev/test sets are generated against the same DB. In order to deal with the more realistic setting where DBs in the evaluation phase are unseen in the training data, researchers propose several cross-domain datasets, such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b) in English, and DuSQL (Wang et al., 2020b) in Chinese. All three datasets adopt the DB-level data splitting, meaning that a DB and all its corresponding question/SQL pairs can appear in only one of the train/dev/test sets. Cross-domain text-to-SQL parsing has two major challenges. First, unseen DBs usually introduce 1 Introduction new schemas, such as new table/column names and unknown semantics of inter-table relationships. Given a natural language (NL) question and a relaTherefore, it is crucial for a parsing model to have tional database (DB), the text-to-SQL parsing task strong generalization ability. The second challenge aims t"
2021.emnlp-main.707,2020.acl-main.398,0,0.0547679,"Missing"
2021.emnlp-main.707,D15-1306,0,0.0153771,"sion field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge base, semantic parsing aims to generate a semantically equivalent formal representation, such as SQL query, logic form (LF), or task-oriented dialogue slots. Based on LF-based representation, Jia and Liang (2016"
2021.emnlp-main.707,C18-1105,0,0.102154,"ne the corresponding subquestion as the shortest question segment that contains all DB elements in the clause. Finally, we discard low-confidence clause/subquestion pairs to reduce noises, such as subquestions having large overlap with others. We keep overlapping subquestions, unless one subquestion fully contains another. In that case, we only keep the shorter subquestion. We find that a portion of collected clauses have multiple subquestion translations. For example, the clause “ORDER_BY age ASC” are translated as both “in ascending order of the age” and “from youngest to oldest”. We follow Hou et al. (2018) and use them as two independent clause/subquestion pairs for training. 2.4 Three Strategies for Utilizing Generated Data Given a set of DBs, the generated question/SQL pairs are usually of larger scale than the original training data (see Table 1), which may greatly increase training time. In this work, we compare the following three strategies for parser training. • The pre-training strategy first pre-trains the model with only generated data, and then finetunes the model with labeled training data. • The directly merging strategy trains the model with all generated data and labeled training"
2021.emnlp-main.707,D19-1670,0,0.0162761,"ve way the number of augmented pairs affects the accuracy to address the sparseness of labeled data, data of parsing models. We conduct this experiment augmentation has been widely and successfully on the Spider dataset using IRNet model based on adopted in the computer vision field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentat"
2021.emnlp-main.707,P17-1089,0,0.156978,"ion ability. The second challenge aims to produce a legal and executable SQL query is that the scale of labeled data is quite small for to get the correct answer (Date and Darwen, 1997), such a complex task, since it is extremely diffias depicted in Figure 1. A DB usually consists of cult to construct DBs and manually annotate cormultiple tables interconnected via foreign keys. responding question/SQL pairs. For example, the Early research on text-to-SQL parsing mainly Spider dataset has only 200 DBs and 10K quesfocuses on the in-domain setting (Li and Jagadish, tion/SQL pairs in total. 2014; Iyer et al., 2017; Yaghmazadeh et al., 2017), To deal with the first challenge, many previous * Work done during an internship at Baidu Inc. works focus on how to better encode the matching 8974 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8974–8983 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 2: An overview of our approach containing 3 stages: SQL query generation based on ASTG (Section §2.1), question hierarchical generation according to SQL structure (Section §2.2), model training via data augmentation (Section §2.4). among que"
2021.emnlp-main.707,P16-1002,0,0.0206393,"t al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge base, semantic parsing aims to generate a semantically equivalent formal representation, such as SQL query, logic form (LF), or task-oriented dialogue slots. Based on LF-based representation, Jia and Liang (2016) train a synchronous"
2021.emnlp-main.707,D18-1545,0,0.0126,"ess the sparseness of labeled data, data of parsing models. We conduct this experiment augmentation has been widely and successfully on the Spider dataset using IRNet model based on adopted in the computer vision field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge"
2021.emnlp-main.707,2020.acl-main.745,0,0.0314504,"Missing"
2021.emnlp-main.707,D18-1193,0,0.351191,".e., WikiSQL and Spider in English, and DuSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement. Figure 1: An example of the text-to-SQL parsing task. where all question/SQL pairs of train/dev/test sets are generated against the same DB. In order to deal with the more realistic setting where DBs in the evaluation phase are unseen in the training data, researchers propose several cross-domain datasets, such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b) in English, and DuSQL (Wang et al., 2020b) in Chinese. All three datasets adopt the DB-level data splitting, meaning that a DB and all its corresponding question/SQL pairs can appear in only one of the train/dev/test sets. Cross-domain text-to-SQL parsing has two major challenges. First, unseen DBs usually introduce 1 Introduction new schemas, such as new table/column names and unknown semantics of inter-table relationships. Given a natural language (NL) question and a relaTherefore, it is crucial for a parsing model to have tional database (DB), the text-to-SQL parsing task strong generali"
2021.emnlp-main.707,D18-1425,0,0.0470955,"Missing"
2021.findings-acl.191,E17-1005,0,0.0224974,"Missing"
2021.findings-acl.191,P19-1595,0,0.0356742,"Missing"
2021.findings-acl.191,N19-1423,0,0.0102182,"ct about 1.8 million unlabeled queries from Yahoo! Answers4 , ORCAS (Craswell et al., 2020), SQuAD (Rajpurkar et al.), TriviaQA (Joshi et al., 2017) and HotpotQA (Yang et al., 2018). In the pre-training stage, we reuse the passage collections from the labeled corpus (MSMARCO and NQ). 4.2 Implementation Details We conduct experiments with the deep learning framework PaddlePaddle (Ma et al., 2019) on up to eight NVIDIA Tesla V100 GPUs (with 32G RAM). Pre-trained LMs The dual-encoder is initialized with the parameters of ERNIE-2.0 base (Sun et al., 2020). ERNIE-2.0 has the same networks as BERT (Devlin et al., 2019), and it introduces a continual pre-training framework on multiple pretrained tasks. The cross-encoder setting follows the cross-encoder in RocketQA (Qu et al., 2020) Hyper-parameters (a) batch size: Our dualencoder is trained with a batch size of 512 × 1 in fine-tuning stage on NQ and 512 × 8 in other settings. We use the in-batch negative setting (Karpukhin et al., 2020) on NQ and crossbatch negative setting (Qu et al., 2020) on MSMARCO. (b) training epochs: The number of training epochs is set up to 10 for both pre-training and fine-tuning for dual-encoder. (c) warm-up and learning rate: Th"
2021.findings-acl.191,K19-1049,0,0.119052,") (b) Figure 1: An illustrative case of a query q, its positive passage p+ and negative passage p− : (a) Query-centric similarity relation enforces s(q, p+ ) &gt; s(q, p− ); (b) Passage-centric similarity relation further enforces s(p+ , q) &gt; s(p+ , p− ), where s(p+ , q) = s(q, p+ ). We use the distance (i.e., dissimilarity) for visualization: the longer the distance is, the less similar it is. ing question answering (Lee et al., 2019; Xiong et al., 2020b), information retrieval (Luan et al., 2021; Khattab and Zaharia, 2020), dialogue (Ji et al., 2014; Henderson et al., 2017) and entity linking (Gillick et al., 2019; Wu et al., 2020). With the recent advances of pre-trained language models, dense passage retrieval techniques (representing queries and passages in low-dimensional semantic space) have significantly outperformed traditional term-based techniques (Guu et al., 2020; Karpukhin et al., 2020). As the key step of finding the relevant information, it has been shown that dense passage retrieval can effectively improve the performance in a variety of tasks, includEqual contribution. The work was done when Ruiyang Ren was doing internship at Baidu. ‡ Corresponding authors. 1 Our code is available at h"
2021.findings-acl.191,P17-1147,0,0.0201338,"BERTbase ERNIEbase RoBERTabase BERTlarge BERTbase BERTbase ERNIEbase 32.5 33.0 34.3 31.1 36.0 37.0 82.2 82.9 85.5 97.3 95.9 97.7 96.8 97.9 68.4 73.3 74.0 78.4 80.7 81.9 82.8 82.7 85.4 87.3 87.5 88.4 88.5 PAIR (Ours) ERNIEbase 37.9 86.4 98.2 74.9 83.5 89.1 Table 2: Experimental results on MSMARCO and Natural Questions datasets. Note that we copy the results from original papers and we leave it blank if the original paper does not report the result. tion data, we collect about 1.8 million unlabeled queries from Yahoo! Answers4 , ORCAS (Craswell et al., 2020), SQuAD (Rajpurkar et al.), TriviaQA (Joshi et al., 2017) and HotpotQA (Yang et al., 2018). In the pre-training stage, we reuse the passage collections from the labeled corpus (MSMARCO and NQ). 4.2 Implementation Details We conduct experiments with the deep learning framework PaddlePaddle (Ma et al., 2019) on up to eight NVIDIA Tesla V100 GPUs (with 32G RAM). Pre-trained LMs The dual-encoder is initialized with the parameters of ERNIE-2.0 base (Sun et al., 2020). ERNIE-2.0 has the same networks as BERT (Devlin et al., 2019), and it introduces a continual pre-training framework on multiple pretrained tasks. The cross-encoder setting follows the cross"
2021.findings-acl.191,2020.emnlp-main.550,0,0.0584937,"e the distance (i.e., dissimilarity) for visualization: the longer the distance is, the less similar it is. ing question answering (Lee et al., 2019; Xiong et al., 2020b), information retrieval (Luan et al., 2021; Khattab and Zaharia, 2020), dialogue (Ji et al., 2014; Henderson et al., 2017) and entity linking (Gillick et al., 2019; Wu et al., 2020). With the recent advances of pre-trained language models, dense passage retrieval techniques (representing queries and passages in low-dimensional semantic space) have significantly outperformed traditional term-based techniques (Guu et al., 2020; Karpukhin et al., 2020). As the key step of finding the relevant information, it has been shown that dense passage retrieval can effectively improve the performance in a variety of tasks, includEqual contribution. The work was done when Ruiyang Ren was doing internship at Baidu. ‡ Corresponding authors. 1 Our code is available at https://github.com/ PaddlePaddle/Research/tree/master/NLP/ ACL2021-PAIR † p+ q pIntroduction ∗ p+ q Typically, the dual-encoder architecture is used to learn the dense representations of queries and passages, and the dot-product similarity between the representations of queries and passages"
2021.findings-acl.191,Q19-1026,0,0.0137076,"etter retrieval performance. In this stage, we use both ground-truth labels and pseudo labels derived from the labeled corpus for training. 4 Pre-training with learning both query-centric similarity relation (QSR) and passage-centric similarity relation (PSR) Experiments In this section, we first describe the experimental settings, then report the main experimental results, ablation study and detailed analysis. 4.1 Experimental Settings Datasets This paper focuses on the passage retrieval task. We conduct experiments on two public datasets: MSMARCO (Nguyen et al., 2016) and Natural Questions (Kwiatkowski et al., 2019). The statistics of the datasets are listed in Table 1. MSMARCO was originally designed for multiple passage machine reading comprehension, and its queries were sampled from Bing search logs. Based on the queries and passages in MSMARCO Question Answering, a dataset for passage retrieval and ranking was created, namely MSMARCO Passage Ranking. Natural Questions (NQ) was originally introduced as a dataset for open-domain QA. The queries were collected from Google search logs. DPR (Karpukhin et al., 2020) selected the queries that had short answers, and processed all the Wikipedia articles as th"
2021.findings-acl.191,P19-1612,0,0.189578,"relation constraint. Extensive experiments show that our approach significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions datasets1 . 1 p(a) (b) Figure 1: An illustrative case of a query q, its positive passage p+ and negative passage p− : (a) Query-centric similarity relation enforces s(q, p+ ) &gt; s(q, p− ); (b) Passage-centric similarity relation further enforces s(p+ , q) &gt; s(p+ , p− ), where s(p+ , q) = s(q, p+ ). We use the distance (i.e., dissimilarity) for visualization: the longer the distance is, the less similar it is. ing question answering (Lee et al., 2019; Xiong et al., 2020b), information retrieval (Luan et al., 2021; Khattab and Zaharia, 2020), dialogue (Ji et al., 2014; Henderson et al., 2017) and entity linking (Gillick et al., 2019; Wu et al., 2020). With the recent advances of pre-trained language models, dense passage retrieval techniques (representing queries and passages in low-dimensional semantic space) have significantly outperformed traditional term-based techniques (Guu et al., 2020; Karpukhin et al., 2020). As the key step of finding the relevant information, it has been shown that dense passage retrieval can effectively improve"
2021.findings-acl.191,2020.emnlp-main.519,0,0.291894,"ustrative case of a query q, its positive passage p+ and negative passage p− : (a) Query-centric similarity relation enforces s(q, p+ ) &gt; s(q, p− ); (b) Passage-centric similarity relation further enforces s(p+ , q) &gt; s(p+ , p− ), where s(p+ , q) = s(q, p+ ). We use the distance (i.e., dissimilarity) for visualization: the longer the distance is, the less similar it is. ing question answering (Lee et al., 2019; Xiong et al., 2020b), information retrieval (Luan et al., 2021; Khattab and Zaharia, 2020), dialogue (Ji et al., 2014; Henderson et al., 2017) and entity linking (Gillick et al., 2019; Wu et al., 2020). With the recent advances of pre-trained language models, dense passage retrieval techniques (representing queries and passages in low-dimensional semantic space) have significantly outperformed traditional term-based techniques (Guu et al., 2020; Karpukhin et al., 2020). As the key step of finding the relevant information, it has been shown that dense passage retrieval can effectively improve the performance in a variety of tasks, includEqual contribution. The work was done when Ruiyang Ren was doing internship at Baidu. ‡ Corresponding authors. 1 Our code is available at https://github.com/"
2021.findings-acl.191,D18-1259,0,0.019867,"Tlarge BERTbase BERTbase ERNIEbase 32.5 33.0 34.3 31.1 36.0 37.0 82.2 82.9 85.5 97.3 95.9 97.7 96.8 97.9 68.4 73.3 74.0 78.4 80.7 81.9 82.8 82.7 85.4 87.3 87.5 88.4 88.5 PAIR (Ours) ERNIEbase 37.9 86.4 98.2 74.9 83.5 89.1 Table 2: Experimental results on MSMARCO and Natural Questions datasets. Note that we copy the results from original papers and we leave it blank if the original paper does not report the result. tion data, we collect about 1.8 million unlabeled queries from Yahoo! Answers4 , ORCAS (Craswell et al., 2020), SQuAD (Rajpurkar et al.), TriviaQA (Joshi et al., 2017) and HotpotQA (Yang et al., 2018). In the pre-training stage, we reuse the passage collections from the labeled corpus (MSMARCO and NQ). 4.2 Implementation Details We conduct experiments with the deep learning framework PaddlePaddle (Ma et al., 2019) on up to eight NVIDIA Tesla V100 GPUs (with 32G RAM). Pre-trained LMs The dual-encoder is initialized with the parameters of ERNIE-2.0 base (Sun et al., 2020). ERNIE-2.0 has the same networks as BERT (Devlin et al., 2019), and it introduces a continual pre-training framework on multiple pretrained tasks. The cross-encoder setting follows the cross-encoder in RocketQA (Qu et al.,"
2021.findings-acl.198,C10-1041,0,0.115298,"Missing"
2021.findings-acl.198,D19-5522,0,0.724457,"tains an incorrect word “的(de, of)”. The CSC model produces a fluent but incorrect sentence by replacing “的(de, 1 In this paper, we ignore the tone of Pinyin, and use homophone to refer to characters with the same pinyin spellings. 2250 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2250–2261 August 1–6, 2021. ©2021 Association for Computational Linguistics of)” with “英(ying, English)”. However, the pronunciations of these two words are totally different, because the model ignores phonetic features. Recent studies tackle the issue using deep neural networks. Hong et al. (2019) used a pre-trained language model BERT (Devlin et al., 2019) to generate candidates and train a classifier with phonetic features to select the final correction. Wang et al. (2019) considered CSC as a sequence-to-sequence task and generated candidates from a confusion set 2 instead of the entire vocabulary. These methods take phonetic information as external knowledge but the discrete candidate selection obstructs the language model from learning directly via backpropagation. Zhang et al. (2020) proposed an endto-end CSC model by modifying the mask mechanism of BERT. However, they did not use"
2021.findings-acl.198,W13-4416,0,0.167613,"ically similar characters. As illustrated in Figure 1, the character “德(de, German)” is incorrectly typed as one of its homophone1 “的(de, of)”. Traditional methods of CSC firstly detect misspelled characters and generate candidates via a language model, and then use a phonetic model or rules to filter wrong candidates (Chang, 1995; Chen et al., 2013; Dong et al., 2016). To improve CSC performance, studies mainly focus on two issues: 1) how to improve the language model (Wu et al., 2010; Dong et al., 2016; Zhang et al., 2020) and 2) how to utilize external knowledge of phonological similarity (Jia et al., 2013; Yu and Li, 2014; Wang et al., 2018; Cheng et al., 2020). The language model is used to generate fluent sentences and the phonetic features can prevent the model from producing predictions whose pronunciation deviates from that of the original word. As illustrated in Fig. 1, the original Wrong sentence contains an incorrect word “的(de, of)”. The CSC model produces a fluent but incorrect sentence by replacing “的(de, 1 In this paper, we ignore the tone of Pinyin, and use homophone to refer to characters with the same pinyin spellings. 2250 Findings of the Association for Computational Linguisti"
2021.findings-acl.198,C10-2085,0,0.725247,"Missing"
2021.findings-acl.198,W13-4409,0,0.0258715,"l that incorporates phonetic features into language representation. The model encodes the Chinese characters and Pinyin tokens in a shared space. • The integration of phonological information greatly facilitates CSC. Experimental results on the benchmark SIGHAN datasets show that 2 Confusion set is a set of similar characters. our method significantly outperforms the previous state-of-the-art methods. 2 Related work Earier work on CSC follows the pipeline of error detection, candidate generation, and candidate selection (Wu et al., 2010; Jia et al., 2013; Chen et al., 2013; Chiu et al., 2013; Liu et al., 2013; Xin et al., 2014; Yu and Li, 2014; Dong et al., 2016; Wang et al., 2018). These methods mainly employ unsupervised language models and rules to select candidates. With the development of end-to-end networks, some work proposed to optimize the error correction performance directly as a sequence-labeling task with conditional random fields (CRF) (Wu et al., 2018) and recurrent neural networks (RNN) (Zheng et al., 2016; Yang et al., 2017). Wang et al. (2019) used a sequence-to-sequence framework with copy mechanism to copy the correction results directly from a prepared confusion set for the er"
2021.findings-acl.198,2020.acl-main.81,0,0.600889,"the character “德(de, German)” is incorrectly typed as one of its homophone1 “的(de, of)”. Traditional methods of CSC firstly detect misspelled characters and generate candidates via a language model, and then use a phonetic model or rules to filter wrong candidates (Chang, 1995; Chen et al., 2013; Dong et al., 2016). To improve CSC performance, studies mainly focus on two issues: 1) how to improve the language model (Wu et al., 2010; Dong et al., 2016; Zhang et al., 2020) and 2) how to utilize external knowledge of phonological similarity (Jia et al., 2013; Yu and Li, 2014; Wang et al., 2018; Cheng et al., 2020). The language model is used to generate fluent sentences and the phonetic features can prevent the model from producing predictions whose pronunciation deviates from that of the original word. As illustrated in Fig. 1, the original Wrong sentence contains an incorrect word “的(de, of)”. The CSC model produces a fluent but incorrect sentence by replacing “的(de, 1 In this paper, we ignore the tone of Pinyin, and use homophone to refer to characters with the same pinyin spellings. 2250 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2250–2261 August 1–6, 2021. ©2"
2021.findings-acl.198,W13-4408,0,0.0240902,"end-to-end CSC model that incorporates phonetic features into language representation. The model encodes the Chinese characters and Pinyin tokens in a shared space. • The integration of phonological information greatly facilitates CSC. Experimental results on the benchmark SIGHAN datasets show that 2 Confusion set is a set of similar characters. our method significantly outperforms the previous state-of-the-art methods. 2 Related work Earier work on CSC follows the pipeline of error detection, candidate generation, and candidate selection (Wu et al., 2010; Jia et al., 2013; Chen et al., 2013; Chiu et al., 2013; Liu et al., 2013; Xin et al., 2014; Yu and Li, 2014; Dong et al., 2016; Wang et al., 2018). These methods mainly employ unsupervised language models and rules to select candidates. With the development of end-to-end networks, some work proposed to optimize the error correction performance directly as a sequence-labeling task with conditional random fields (CRF) (Wu et al., 2018) and recurrent neural networks (RNN) (Zheng et al., 2016; Yang et al., 2017). Wang et al. (2019) used a sequence-to-sequence framework with copy mechanism to copy the correction results directly from a prepared confus"
2021.findings-acl.198,C12-1144,0,0.073701,"Missing"
2021.findings-acl.198,W10-4107,0,0.198678,"e spelling errors on the Internet results from Corresponding author. de He German language speak 1 Introduction ∗ ta phonologically similar characters. As illustrated in Figure 1, the character “德(de, German)” is incorrectly typed as one of its homophone1 “的(de, of)”. Traditional methods of CSC firstly detect misspelled characters and generate candidates via a language model, and then use a phonetic model or rules to filter wrong candidates (Chang, 1995; Chen et al., 2013; Dong et al., 2016). To improve CSC performance, studies mainly focus on two issues: 1) how to improve the language model (Wu et al., 2010; Dong et al., 2016; Zhang et al., 2020) and 2) how to utilize external knowledge of phonological similarity (Jia et al., 2013; Yu and Li, 2014; Wang et al., 2018; Cheng et al., 2020). The language model is used to generate fluent sentences and the phonetic features can prevent the model from producing predictions whose pronunciation deviates from that of the original word. As illustrated in Fig. 1, the original Wrong sentence contains an incorrect word “的(de, of)”. The CSC model produces a fluent but incorrect sentence by replacing “的(de, 1 In this paper, we ignore the tone of Pinyin, and use"
2021.findings-acl.198,W14-6835,0,0.712709,"racters. As illustrated in Figure 1, the character “德(de, German)” is incorrectly typed as one of its homophone1 “的(de, of)”. Traditional methods of CSC firstly detect misspelled characters and generate candidates via a language model, and then use a phonetic model or rules to filter wrong candidates (Chang, 1995; Chen et al., 2013; Dong et al., 2016). To improve CSC performance, studies mainly focus on two issues: 1) how to improve the language model (Wu et al., 2010; Dong et al., 2016; Zhang et al., 2020) and 2) how to utilize external knowledge of phonological similarity (Jia et al., 2013; Yu and Li, 2014; Wang et al., 2018; Cheng et al., 2020). The language model is used to generate fluent sentences and the phonetic features can prevent the model from producing predictions whose pronunciation deviates from that of the original word. As illustrated in Fig. 1, the original Wrong sentence contains an incorrect word “的(de, of)”. The CSC model produces a fluent but incorrect sentence by replacing “的(de, 1 In this paper, we ignore the tone of Pinyin, and use homophone to refer to characters with the same pinyin spellings. 2250 Findings of the Association for Computational Linguistics: ACL-IJCNLP 20"
2021.findings-acl.198,2020.acl-main.82,0,0.197201,"sults from Corresponding author. de He German language speak 1 Introduction ∗ ta phonologically similar characters. As illustrated in Figure 1, the character “德(de, German)” is incorrectly typed as one of its homophone1 “的(de, of)”. Traditional methods of CSC firstly detect misspelled characters and generate candidates via a language model, and then use a phonetic model or rules to filter wrong candidates (Chang, 1995; Chen et al., 2013; Dong et al., 2016). To improve CSC performance, studies mainly focus on two issues: 1) how to improve the language model (Wu et al., 2010; Dong et al., 2016; Zhang et al., 2020) and 2) how to utilize external knowledge of phonological similarity (Jia et al., 2013; Yu and Li, 2014; Wang et al., 2018; Cheng et al., 2020). The language model is used to generate fluent sentences and the phonetic features can prevent the model from producing predictions whose pronunciation deviates from that of the original word. As illustrated in Fig. 1, the original Wrong sentence contains an incorrect word “的(de, of)”. The CSC model produces a fluent but incorrect sentence by replacing “的(de, 1 In this paper, we ignore the tone of Pinyin, and use homophone to refer to characters with t"
2021.findings-acl.198,W16-4907,0,0.0267098,"Earier work on CSC follows the pipeline of error detection, candidate generation, and candidate selection (Wu et al., 2010; Jia et al., 2013; Chen et al., 2013; Chiu et al., 2013; Liu et al., 2013; Xin et al., 2014; Yu and Li, 2014; Dong et al., 2016; Wang et al., 2018). These methods mainly employ unsupervised language models and rules to select candidates. With the development of end-to-end networks, some work proposed to optimize the error correction performance directly as a sequence-labeling task with conditional random fields (CRF) (Wu et al., 2018) and recurrent neural networks (RNN) (Zheng et al., 2016; Yang et al., 2017). Wang et al. (2019) used a sequence-to-sequence framework with copy mechanism to copy the correction results directly from a prepared confusion set for the erroneous words. Cheng et al. (2020) built a graph convolution network (GCN) on top of BERT (Devlin et al., 2019) and the graph was constructed from a confusion set. Zhang et al. (2020) proposed a soft-masked BERT model that first predicts the probability of spelling error for each word, and then uses the probabilities to perform a soft-masked word embedding for correction. However, they did not use any phonetic informa"
2021.findings-acl.198,W13-4406,0,0.498195,"Missing"
2021.findings-acl.222,N16-1014,0,0.272785,"mpared to DialoGPT, as they have similar model scales. • PLATO-2 93M parameter model is a tiny version in English, which is trained with Reddit comments. As it is difficult to scale up PLATO, we use this version to compare with PLATO. • PLATO-2 336M parameter Chinese model2 will be compared to XiaoIce in the experiments. 2 This model has 24 transformer blocks and 16 attention 3.3.2 Evaluation Metrics We carry out both automatic and human evaluations in the experiments. In automatic evaluation, to assess the model’s capacity on lexical diversity, we use the corpus-level metric of distinct-1/2 (Li et al., 2016a), which is defined as the number of distinct uni- or bi-grams divided by the total number of generated words. In human evaluation, we employ four utterancelevel and dialogue-level metrics, including coherence, informativeness, engagingness and humanness. Three crowd-sourcing workers are asked to score the response/dialogue quality on a scale of [0, 1, 2], with the final score determined through majority voting. The higher score, the better. These criteria are discussed as follows, with scoring details provided in the Appendix. • Coherence is an utterance-level metric, measuring whether the r"
2021.findings-acl.222,D16-1127,0,0.126928,"mpared to DialoGPT, as they have similar model scales. • PLATO-2 93M parameter model is a tiny version in English, which is trained with Reddit comments. As it is difficult to scale up PLATO, we use this version to compare with PLATO. • PLATO-2 336M parameter Chinese model2 will be compared to XiaoIce in the experiments. 2 This model has 24 transformer blocks and 16 attention 3.3.2 Evaluation Metrics We carry out both automatic and human evaluations in the experiments. In automatic evaluation, to assess the model’s capacity on lexical diversity, we use the corpus-level metric of distinct-1/2 (Li et al., 2016a), which is defined as the number of distinct uni- or bi-grams divided by the total number of generated words. In human evaluation, we employ four utterancelevel and dialogue-level metrics, including coherence, informativeness, engagingness and humanness. Three crowd-sourcing workers are asked to score the response/dialogue quality on a scale of [0, 1, 2], with the final score determined through majority voting. The higher score, the better. These criteria are discussed as follows, with scoring details provided in the Appendix. • Coherence is an utterance-level metric, measuring whether the r"
2021.findings-acl.222,I17-1099,0,0.0192107,"ents over XiaoIce across all the human evaluation metrics. Model Static Evaluation Besides the interactive evaluation, we also employ static evaluation to analyze the model’s performance. In static evaluation, each model will produce a response towards the given multi-turn context. Those powerful models are involved in the evaluation: Meena, Blender, DialoGPT and PLATO-2 1.6B. To compare with Meena, we include their provided 60 static samples in the Appendix of the paper and generate corresponding responses with other models. We also include 60 test samples about daily life from Daily Dialog (Li et al., 2017) and 60 test samples about in-depth discussion from Reddit. Given that the measurement of humanness usually needs multi-turn interaction, this metric is excluded from static evaluation. The evaluation results are summarized in Table 3. It can be observed that PLATO-2 is able to produce coherent, informative and engaging responses across different chat scenarios. The average Fleiss’s kappa (Fleiss, 1971) of human evaluation is 0.466, indicating annotators have reached moderate agreement. Informativeness Engagingness Meena 1.750 1.617 1.583 DialoGPT 1.233 1.067 1.017 Blender 1.800 1.767 1.683 PL"
2021.findings-acl.222,2021.eacl-main.24,0,0.577455,"eat success in natural language processing (Devlin et al., 2019), especially open-domain dialogue generation. For instance, based on the general language model GPT-2 (Radford et al., 2019), DialoGPT (Zhang et al., 2020) is further trained for response generation using Reddit comments. To obtain a human-like open-domain chatbot, Meena (Adiwardana et al., 2020) scales up the network parameters to 2.6B and employs more social media conversations in the training process, leading to significant improvement on response quality. To mitigate undesirable toxic or bias traits of large corpora, Blender (Roller et al., 2021) fine-tunes the pretrained model with human annotated datasets and emphasizes desirable conversational skills of engagingness, knowledge, empathy and personality. In addition to the attempts from model scale and data selection, PLATO (Bao et al., 2020) aims Equal contribution. Stage 2.2 Evaluation Response Coherence Estimation Introduction ∗ One-to-Many Mapping Diverse Response Generation to tackle the inherent one-to-many mapping problem to improve response quality. The one-to-many mapping refers to that one dialogue context might correspond to multiple appropriate responses. It is widely rec"
2021.findings-acl.222,P16-1162,0,0.0583211,"The English training data is extracted from Reddit comments, which are collected by a third party and made publicly available on pushshift.io (Baumgartner et al., 2020). To improve the generation quality, we carry out elaborate data cleaning, as discussed in the Appendix. After filtering, the data is split into training and validation sets in chronological order. The training set contains 684M (context, response) samples, ranging from December 2005 to July 2019. For the validation set, 0.2M samples are selected from the rest data after July 2019. The English vocabulary contains 8K BPE tokens (Sennrich et al., 2016), constructed with the SentencePiece library. The Chinese training data is collected from public domain social medias. After filtering, there are 1.2B (context, response) samples in the training set, 0.1M samples in the validation set, and 0.1M samples in the test set. As for the Chinese vocabulary, it contains 30K BPE tokens. 3.2 Training Details PLATO-2 has three model sizes: a standard version of 1.6B parameters, a small version of 314M parameters, and a tiny version of 93M parameters. Detailed network and training configurations are summarized in the Appendix. The main hyperparameters used"
2021.findings-acl.222,2020.acl-main.183,0,0.02038,"020) is trained on the basis of BERTBASE using 8.3M Twitter and Reddit conversations (Cho et al., 2014; Zhou et al., 2018; Galley et al., 2019). There are 132M network parameters in this model. • DialoGPT (Zhang et al., 2020) is trained on the basis of GPT-2 (Radford et al., 2019) using Reddit comments. There are three model sizes: 117M, 345M and 762M. Since the 345M parameter model obtains the best performance in their evaluations, we compare with this version. • Blender (Roller et al., 2021) is first trained using Reddit comments and then fine-tuned with human annotated conversations – BST (Smith et al., 2020), to help emphasize desirable conversational skills of engagingness, knowledge, empathy and personality. Blender has three model sizes: 90M, 2.7B and 9.4B. Since the 2.7B parameter model obtains the best performance in their evaluations, we compare with this version. • Meena (Adiwardana et al., 2020) is an opendomain chatbot trained with social media conversations. There are 2.6B network parameters in Meena. Since Meena has not released the model or provided a service interface, it is difficult to perform comprehensive comparison. In the experiments, we include the provided samples in their pa"
2021.findings-acl.222,2020.acl-demos.30,0,0.484041,"ct the best response, respectively. PLATO-2 was trained on both Chinese and English data, whose effectiveness and superiority are verified through comprehensive evaluations, achieving new state-of-the-art results. 1 One-to-One Mapping General Response Generation Figure 1: Curriculum learning process in PLATO-2. Recently, task agnostic pre-training with largescale transformer models has achieved great success in natural language processing (Devlin et al., 2019), especially open-domain dialogue generation. For instance, based on the general language model GPT-2 (Radford et al., 2019), DialoGPT (Zhang et al., 2020) is further trained for response generation using Reddit comments. To obtain a human-like open-domain chatbot, Meena (Adiwardana et al., 2020) scales up the network parameters to 2.6B and employs more social media conversations in the training process, leading to significant improvement on response quality. To mitigate undesirable toxic or bias traits of large corpora, Blender (Roller et al., 2021) fine-tunes the pretrained model with human annotated datasets and emphasizes desirable conversational skills of engagingness, knowledge, empathy and personality. In addition to the attempts from mod"
2021.findings-acl.222,P17-1061,0,0.116345,"engagingness, knowledge, empathy and personality. In addition to the attempts from model scale and data selection, PLATO (Bao et al., 2020) aims Equal contribution. Stage 2.2 Evaluation Response Coherence Estimation Introduction ∗ One-to-Many Mapping Diverse Response Generation to tackle the inherent one-to-many mapping problem to improve response quality. The one-to-many mapping refers to that one dialogue context might correspond to multiple appropriate responses. It is widely recognized that the capability of modeling one-to-many relationship is crucial for opendomain dialogue generation (Zhao et al., 2017; Chen et al., 2019). PLATO explicitly models this one-to-many relationship via discrete latent variables, aiming to boost the quality of dialogue generation. PLATO has a modest scale of 132M network parameters and trained with 8M samples, achieving relatively good performance among conversation models on a similar scale. However, scaling up PLATO directly encounters training instability and efficiency issues, which might result from the difficulty to capture the one-to-many semantic relationship from scratch. In this work, we try to scale up PLATO to PLATO-2 and introduce an effective trainin"
2021.findings-acl.35,D19-1522,0,0.15692,"3,454 951 2,582 1,519 3,455 951 2,582 1,519 3 4 3 4 Table 1: Dataset statistics, where the columns respectively indicate the number of all facts, n-ary facts with n &gt; 2, entities, relations, facts in train/dev/test sets, and all possible arities. sor X ∈ {0, 1}|R|×|E|×···×|E |, where x = 1 means the corresponding fact is true and x = 0 otherwise. X is then decomposed and approximated by a lowrank tensor Xˆ that estimates the validity of all facts. Different tensor decomposition strategies can be applied, e.g., n-CP generalizes CP decomposition (Kruskal, 1977) and n-TuckER is built on TuckER (Balazevic et al., 2019). As the tensor representation inherently requires all facts to have the same arity, these methods are not applicable to datasets of mixed arities, e.g., JF17K and WikiPeople. 4.3 4 attention heads, hidden size d = 256, batch size b = 1024, and learning rate η = 5e−4, fixed across all the datasets. Besides, on each dataset, we tune entity/relation label smoothing rate (e) /(r) , dropout rate ρ, and training epochs τ in their respective ranges. The optimal configuration is determined by dev MRR. We leave the tuning ranges and optimal values of these hyperparameters to Appendix B. After determ"
2021.findings-acl.35,N19-1423,0,0.00523838,"s, and generalizes graph convolutional networks to n-ary relational KGs to learn entity and relation embeddings. These embeddings are then fed into a Transformer decoder to score n-ary facts. Nevertheless, during the decoding process STARE takes into account solely global dependencies and ignores the specific n-ary structure of a given fact. Transformer and its extensions Transformer (Vaswani et al., 2017) was initially devised as an encoder-decoder architecture for machine translation, and quickly received broad attention across all areas of natural language processing (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). Transformer uses neither convolution nor recurrence, but instead is built entirely with (self-) attention layers. Recently, there has been a lot of interest in modifying this attention to further meet various desired specifications, e.g., to encode syntax trees (Strubell et al., 2018; Wang et al., 2019c), character-word lattice structures (Li et al., 2020), as well as relative positions between words (Shaw et al., 2018; Wang et al., 2019a). There are also a few recent attempts that apply vanilla Transformer (Wang et al., 2019b) or hierarchical Transformer (Chen et al., 20"
2021.findings-acl.35,2020.acl-main.611,0,0.0149389,"mer (Vaswani et al., 2017) was initially devised as an encoder-decoder architecture for machine translation, and quickly received broad attention across all areas of natural language processing (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). Transformer uses neither convolution nor recurrence, but instead is built entirely with (self-) attention layers. Recently, there has been a lot of interest in modifying this attention to further meet various desired specifications, e.g., to encode syntax trees (Strubell et al., 2018; Wang et al., 2019c), character-word lattice structures (Li et al., 2020), as well as relative positions between words (Shaw et al., 2018; Wang et al., 2019a). There are also a few recent attempts that apply vanilla Transformer (Wang et al., 2019b) or hierarchical Transformer (Chen et al., 2020) to KGs, but mainly restricted to binary relations and deployed with conventional attention. This work, in contrast, deals with higher-arity relational data represented as heterogeneous graphs, and employs modified attention to encode graph structure and heterogeneity. 6 Conclusion This paper studies link prediction on higher-arity relational facts and presents a graph-based"
2021.findings-acl.35,2020.emnlp-main.596,0,0.167867,"ill be retained in the graph. Then, based on this graph representation, we employ a fully-connected attention module to characterize inter-vertex interactions, while further introducing edge-aware attentive biases to particularly handle the graph structure and heterogeneity. This enables us to capture not only local but also global dependencies within the fact. Our approach directly encodes each n-ary fact as a whole graph so as to better capture rich associations therein. In this sense, we call it GRAph-based N-ary relational learning (GRAN). The most similar prior art to this work is STARE (Galkin et al., 2020), which uses a message passing based graph encoder to obtain relation (attribute) and entity (value) embeddings, and feeds these embeddings into a Transformer (Vaswani et al., 2017) decoder to score n-ary facts. Our approach is more neatly designed by (1) excluding the computationalheavy graph encoder which, according to a contemporaneous study (Yu and Yang, 2021), may not be necessary given an expressive enough decoder, and (2) modeling the full n-ary structure of a fact during decoding which enables to capture not only global but also local dependencies therein. We evaluate our approach on a"
2021.findings-acl.35,2020.acl-main.546,0,0.182859,"ues (entities), e.g., {person: Marie Curie, award: Nobel Prize in Physics, point-intime: 1903, together-with: Pierre Curie, togetherwith: Antoine Henri Becquerel}. Link prediction then is achieved by learning the relatedness either between the values (Zhang et al., 2018; Liu et al., 2020; Fatemi et al., 2020) or between the attributevalue pairs (Guan et al., 2019; Liu et al., 2021). This representation inherently assumes that attributes of a same n-ary fact are equally important, which is usually not the case. To further discriminate importance of different attributes, Rosso et al. (2020) and Guan et al. (2020) later proposed to represent an n-ary fact as a primary triple coupled with auxiliary attribute-value descriptions, e.g., in the above 5-ary 396 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 396–407 August 1–6, 2021. ©2021 Association for Computational Linguistics fact, (Marie Curie, award-received, Nobel Prize in Physics) is the primary triple and point-in-time: 1903, together-with: Pierre Curie, together-with: Antoine Henri Becquerel are auxiliary descriptions. Link prediction then is achieved by measuring the validity of the primary triple and its compati"
2021.findings-acl.35,N18-2074,0,0.180077,"ated by typed edges. Introducing eVij further propagates edge information to the attention output. If there is V no edge linking i and j we set eK ij = eij = 0, which, at this time, degenerates to the conventional fullyconnected attention used in Transformer (Vaswani V et al., 2017). As the attentive biases eK ij , eij can be designed freely to meet any desired specifications, this attention is in essence quite flexible, capable of modeling arbitrary relationships between the input elements. This idea has actually been applied, e.g., to model relative positions between words within sentences (Shaw et al., 2018; Wang et al., 2019a), or to model various kinds of mention dependencies for relation extraction (Xu et al., 2021). Edge-aware attentive biases We now elaborate V how eK ij and eij are specifically designed for n-ary facts. Recall that given an n-ary fact represented as a heterogeneous graph G = (V, L), there are 4 distinct types of edges in the graph: subject-relation, object-relation, relation-attribute, attribute-value. To each we assign a pair of key and value biases. The attentive biases between vertices i and j are then defined as the biases associated with the type of the edge linking i"
2021.findings-acl.35,D18-1548,0,0.031971,"Missing"
2021.findings-acl.35,P19-1132,0,0.0956148,". Introducing eVij further propagates edge information to the attention output. If there is V no edge linking i and j we set eK ij = eij = 0, which, at this time, degenerates to the conventional fullyconnected attention used in Transformer (Vaswani V et al., 2017). As the attentive biases eK ij , eij can be designed freely to meet any desired specifications, this attention is in essence quite flexible, capable of modeling arbitrary relationships between the input elements. This idea has actually been applied, e.g., to model relative positions between words within sentences (Shaw et al., 2018; Wang et al., 2019a), or to model various kinds of mention dependencies for relation extraction (Xu et al., 2021). Edge-aware attentive biases We now elaborate V how eK ij and eij are specifically designed for n-ary facts. Recall that given an n-ary fact represented as a heterogeneous graph G = (V, L), there are 4 distinct types of edges in the graph: subject-relation, object-relation, relation-attribute, attribute-value. To each we assign a pair of key and value biases. The attentive biases between vertices i and j are then defined as the biases associated with the type of the edge linking i and j:   (0, 0),"
2021.findings-acl.35,D19-1098,0,0.0692493,". Introducing eVij further propagates edge information to the attention output. If there is V no edge linking i and j we set eK ij = eij = 0, which, at this time, degenerates to the conventional fullyconnected attention used in Transformer (Vaswani V et al., 2017). As the attentive biases eK ij , eij can be designed freely to meet any desired specifications, this attention is in essence quite flexible, capable of modeling arbitrary relationships between the input elements. This idea has actually been applied, e.g., to model relative positions between words within sentences (Shaw et al., 2018; Wang et al., 2019a), or to model various kinds of mention dependencies for relation extraction (Xu et al., 2021). Edge-aware attentive biases We now elaborate V how eK ij and eij are specifically designed for n-ary facts. Recall that given an n-ary fact represented as a heterogeneous graph G = (V, L), there are 4 distinct types of edges in the graph: subject-relation, object-relation, relation-attribute, attribute-value. To each we assign a pair of key and value biases. The attentive biases between vertices i and j are then defined as the biases associated with the type of the edge linking i and j:   (0, 0),"
2021.findings-emnlp.29,D18-1149,0,0.0658785,"Missing"
2021.findings-emnlp.29,P19-1177,0,0.039526,"Missing"
2021.findings-emnlp.29,W19-6622,0,0.063172,"Missing"
2021.findings-emnlp.29,W18-6301,0,0.0672122,"Missing"
2021.findings-emnlp.29,W16-2323,0,0.0803376,"Missing"
2021.findings-emnlp.29,2020.emnlp-main.82,1,0.798934,"Missing"
2021.naacl-main.136,S17-2001,0,0.0162971,"ined models. 4.3 Results on GLUE Benchmark The General Language Understanding Evaluation (GLUE; Wang et al., 2018) is a multi-task benchmark consisting of various NLU tasks, which contains 1) pairwise classification tasks like language inference (MNLI; Williams et al., 2018, RTE; Dagan et al., 2006), question answering (QNLI; Rajpurkar et al., 2016) and paraphrase detection (QQP, MRPC; Dolan and Brockett, 2005), 2) singlesentence classification tasks like linguistic acceptability (CoLA; Warstadt et al., 2019), sentiment analysis (SST-2; Socher et al., 2013) and 3) text similarity task (STS-B; Cer et al., 2017). The fine-tuning results on GLUE of ERNIEGram and various strong baselines are presented in Table 1. For fair comparison, the listed models are all in base size and fine-tuned without any data augmentation. Pre-trained with base-scale text corpora, ERNIE-Gram outperforms recent models such as TUPE and F-TFM by 1.7 and 1.3 points on average. As for large-scale text corpora, ERNIEGram achieves average score increase of 1.7 and 0.6 over RoBERTa and ELECTRA, demonstrating the effectiveness of ERNIE-Gram. 4.4 Results on Question Answering (SQuAD) The Stanford Question Answering (SQuAD) tasks are d"
2021.naacl-main.136,D18-1269,0,0.020724,"020). passages. We also evaluate ERNIE-Gram on two large scaled text classification tasks that involve long text and reasoning, including sentiment analysis datasets IMDb (Maas et al., 2011) and topic classification dataset AG’s News (Zhang et al., 2015). The results are reported in Table 3. It can be seen that ERNIE-Gram consistently outperforms previous models, showing the advantage of ERNIEGram on tasks involving long text and reasoning. 4.6 Results on Chinese NLU Tasks We execute extensive experiments on six Chinese language understanding tasks, including natural language inference (XNLI; Conneau et al., 2018), 1707 Models XNLI LCQMC Acc Acc Dev Test Dev Test DRCD EM / F1 Dev Test CMRC2018 DuReader M-NER EM / F1 EM / F1 F1 Dev Dev Dev Test RoBERTa-wwn-ext∗LARGE 82.1 81.2 90.4 87.0 89.6 / 94.8 89.6 / 94.5 68.5 / 88.4 NEZHALARGE (Wei et al., 2019) 82.2 81.2 90.9 87.9 -/-/-/MacBERTLARGE (Cui et al., 2020) 82.4 81.3 90.6 87.6 91.2 / 95.6 91.7 / 95.6 70.7 / 88.9 -/-/-/- - - BERT-wwn-ext∗BASE RoBERTa-wwn-ext∗BASE Z ENBASE (Diao et al., 2020) NEZHABASE (Wei et al., 2019) MacBERTBASE (Cui et al., 2020) ERNIE1.0BASE (Sun et al., 2019b) ERNIE2.0BASE (Sun et al., 2020) 79.4 80.0 80.5 81.4 80.3 79.9 81.2 ERNIE"
2021.naacl-main.136,2020.findings-emnlp.58,0,0.0474497,"asking and train the standard model θ to predict the original n-grams from fake ones in coarse-grained and fine-grained manners, as shown in Figure 3(a), which is efficient to model the pair relationships between similar n-grams. The generator model θ0 will not be used during fine-tuning, where the hidden size Hθ0 of θ0 has Hθ0 = Hθ /3 empirically. As shown in Figure 3(b), n-grams of different length can be sampled to mask original n-grams according to the prediction distributions of θ0 , which is more flexible and sufficient for constructing ngram pairs than previous synonym masking methods (Cui et al., 2020) that require synonyms and original words to be of the same length. Note that our method needs a large embedding layer E ∈ R|hVF ,VN i|×h to obtain n-gram vectors in pretraining. To keep the number of parameters consisMoreover, we incorporate the replaced token detection objective (RTD) to further distinguish 0 fake n-grams from the mix-grained context z¯M for interactions among explicit n-grams and finegrained contextual tokens, as shown in the right part of Figure 3(a). Formally, we donate zˆM to be the sequence after replacing masked n-grams with target n-gram identities yM , the RTD obje"
2021.naacl-main.136,L18-1431,0,0.0273221,".2 86.6 / 92.5 -/-/89.4 / 94.3 84.6 / 90.9 88.5 / 93.8 83.6 / 90.4 85.6 / 92.0 -/-/89.5 / 93.8 84.0 / 90.5 88.0 / 93.4 67.1 / 85.7 -/67.4 / 87.2 -/-/-/-/-/68.5 / 87.9 -/65.1 / 85.1 57.9 / 72.1 95.0 93.8 69.1 / 88.6 61.3 / 74.9 95.2 93.8 Table 4: Results on six Chinese NLU tasks for base-size pre-trained models. Results of models with asterisks “∗ ” are from Cui et al., 2019. M-NER is in short for MSRA-NER dataset. “BASE” and “LARGE” donate different sizes of pre-training models. Large size models have L = 24, H = 1024, A = 16 and total Parameters=340M. machine reading comprehension (CMRC2018; Cui et al., 2018, DRCD; Shao et al., 2018 and DuReader; He et al., 2018), named entity recognition (MSRA-NER; Gao et al., 2005) and semantic similarity (LCQMC; Liu et al., 2018). Results on six Chinese tasks are presented in Table 4. It is observed that ERNIE-Gram significantly outperforms previous models across tasks by a large margin and achieves new state-of-theart results on these Chinese NLU tasks in basesize model group. Besides, ERNIE-GramBASE are also better than various large-size models on XNLI, LCQMC and CMRC2018 datasets. 4.7 Ablation Studies We further conduct ablation experiments to analyze the"
2021.naacl-main.136,J05-4005,0,0.108501,"0 / 93.4 67.1 / 85.7 -/67.4 / 87.2 -/-/-/-/-/68.5 / 87.9 -/65.1 / 85.1 57.9 / 72.1 95.0 93.8 69.1 / 88.6 61.3 / 74.9 95.2 93.8 Table 4: Results on six Chinese NLU tasks for base-size pre-trained models. Results of models with asterisks “∗ ” are from Cui et al., 2019. M-NER is in short for MSRA-NER dataset. “BASE” and “LARGE” donate different sizes of pre-training models. Large size models have L = 24, H = 1024, A = 16 and total Parameters=340M. machine reading comprehension (CMRC2018; Cui et al., 2018, DRCD; Shao et al., 2018 and DuReader; He et al., 2018), named entity recognition (MSRA-NER; Gao et al., 2005) and semantic similarity (LCQMC; Liu et al., 2018). Results on six Chinese tasks are presented in Table 4. It is observed that ERNIE-Gram significantly outperforms previous models across tasks by a large margin and achieves new state-of-theart results on these Chinese NLU tasks in basesize model group. Besides, ERNIE-GramBASE are also better than various large-size models on XNLI, LCQMC and CMRC2018 datasets. 4.7 Ablation Studies We further conduct ablation experiments to analyze the major components of ERNIE-Gram. Effect of Explicitly N-gram MLM. We compare two models pre-trained with contigu"
2021.naacl-main.136,W18-2605,1,0.825381,"3.6 / 90.4 85.6 / 92.0 -/-/89.5 / 93.8 84.0 / 90.5 88.0 / 93.4 67.1 / 85.7 -/67.4 / 87.2 -/-/-/-/-/68.5 / 87.9 -/65.1 / 85.1 57.9 / 72.1 95.0 93.8 69.1 / 88.6 61.3 / 74.9 95.2 93.8 Table 4: Results on six Chinese NLU tasks for base-size pre-trained models. Results of models with asterisks “∗ ” are from Cui et al., 2019. M-NER is in short for MSRA-NER dataset. “BASE” and “LARGE” donate different sizes of pre-training models. Large size models have L = 24, H = 1024, A = 16 and total Parameters=340M. machine reading comprehension (CMRC2018; Cui et al., 2018, DRCD; Shao et al., 2018 and DuReader; He et al., 2018), named entity recognition (MSRA-NER; Gao et al., 2005) and semantic similarity (LCQMC; Liu et al., 2018). Results on six Chinese tasks are presented in Table 4. It is observed that ERNIE-Gram significantly outperforms previous models across tasks by a large margin and achieves new state-of-theart results on these Chinese NLU tasks in basesize model group. Besides, ERNIE-GramBASE are also better than various large-size models on XNLI, LCQMC and CMRC2018 datasets. 4.7 Ablation Studies We further conduct ablation experiments to analyze the major components of ERNIE-Gram. Effect of Explicitly N-g"
2021.naacl-main.136,2020.tacl-1.5,0,0.10719,"al context. However, BERT’s MLM focuses on the representations of fine-grained text units (e.g. words or subwords in English and characters in Chinese), rarely considering the coarse-grained linguistic information (e.g. named entities or phrases in English and words in Chinese) thus incurring inadequate representation learning. Many efforts have been devoted to integrate coarse-grained semantic information by independently masking and predicting contiguous sequences of n tokens, namely n-grams, such as named entities, phrases (Sun et al., 2019b), whole words (Cui et al., 2019) and text spans (Joshi et al., 2020). We argue that such contiguously masking strategies are less effective and reliable since the prediction of tokens in masked n-grams are independent of each other, which neglects the intradependencies of n-grams. Specifically, given a masked n-gramQw = {x1 , ..., xn }, x ∈ VF , we maximize p(w) = ni=1 p(xi |c) for n-gram learning, where models learn to recover w in a huge and n sparse prediction space F ∈ R|VF |. Note that VF is the fine-grained vocabulary1 and c is the context. We propose ERNIE-Gram, an explicitly ngram masked language modeling method in which n-grams are masked with single"
2021.naacl-main.136,D17-1082,0,0.049529,"Missing"
2021.naacl-main.136,P19-1314,0,0.0138094,"phrases to enhance contextual representations, BERT-wwm (Cui et al., 2019) masks whole Chinese words to achieve better Chinese representations, SpanBERT (Joshi et al., 2020) masks contiguous spans to improve the performance on span selection tasks. A few studies attempt to inject the coarsegrained n-gram representations into fine-grained contextualized representations explicitly, such as Z EN (Diao et al., 2020) and AMBERT (Zhang and Li, 2020), in which additional transformer encoders and computations for explicit n-gram representations are incorporated into both pre-training and fine-tuning. Li et al., 2019 demonstrate that explicit n-gram representations are not sufficiently reliable for NLP tasks because of n-gram data sparsity and the ubiquity of out-of-vocabulary n-grams. Differently, we only incorporate n-gram information by leveraging auxiliary n-gram classifier and embedding weights in pre-training, which will be completely removed during fine-tuning, so our method maintains the same parameters and computations as BERT. 3 Proposed Method In this section, we present the detailed implementation of ERNIE-Gram, including n-gram lexicon 1703 (a) VN extraction in Section 3.5, explicitly n-gram"
2021.naacl-main.136,C18-1166,0,0.0214755,"Missing"
2021.naacl-main.136,2021.ccl-1.108,0,0.094593,"Missing"
2021.naacl-main.136,P11-1015,0,0.0681582,"questions and 72.0 77.7 70.3 75.6 76.3 78.8 4.4 3.9 4.9 Table 3: Comparison on the test sets of RACE, IMDb and AG. The listed models are all in base-size. In the results of RACE, “High” and “Middle” represent the training and evaluation sets for high schools and middle schools respectively, “Total” is the full training and evaluation set. a (Devlin et al., 2019); b (Yang et al., 2019); c (Song et al., 2020); d (Dai et al., 2020). passages. We also evaluate ERNIE-Gram on two large scaled text classification tasks that involve long text and reasoning, including sentiment analysis datasets IMDb (Maas et al., 2011) and topic classification dataset AG’s News (Zhang et al., 2015). The results are reported in Table 3. It can be seen that ERNIE-Gram consistently outperforms previous models, showing the advantage of ERNIEGram on tasks involving long text and reasoning. 4.6 Results on Chinese NLU Tasks We execute extensive experiments on six Chinese language understanding tasks, including natural language inference (XNLI; Conneau et al., 2018), 1707 Models XNLI LCQMC Acc Acc Dev Test Dev Test DRCD EM / F1 Dev Test CMRC2018 DuReader M-NER EM / F1 EM / F1 F1 Dev Dev Dev Test RoBERTa-wwn-ext∗LARGE 82.1 81.2 90.4"
2021.naacl-main.466,W02-1033,0,0.356347,"Missing"
2021.naacl-main.466,P17-1171,0,0.154647,"ategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever 1 . 1 Introduction Open-domain question answering (QA) aims to find the answers to natural language questions from a large collection of documents. Early QA systems (Brill et al., 2002; Dang et al., 2007; Ferrucci et al., 2010) constructed complicated pipelines consisting of multiple components, including question understanding, document retrieval, passage ranking and answer extraction. Recently, inspired by the advancements of machine reading comprehension (MRC), Chen et al. (2017) proposed a simplified two-stage approach, where a traditional IR sim(q, p) = Eq(q) Ep(p) Eq(q) Ep(p) ... ... ... [CLS] q(1) ... q(k) ... [CLS] question p(1) ... p(l) passage (a) A dual-encoder based on pre-trained LMs. sim(q, p) [CLS] q(1) ... ... ... ... ... question q(k) [SEP] p(1) ... p(l) passage (b) A cross-encoder based on pre-trained LMs. Figure 1: The comparison of dual-encoder and crossencoder architectures. retriever (e.g., TF-IDF or BM25) first selects a few relevant passages as contexts, and then a neural reader reads the contexts and extracts the answers. As the recall component,"
2021.naacl-main.466,D19-5801,0,0.0177207,"ely. The dualencoders are trained on NQ for 30 epochs in all steps of RocketQA. The cross-encoders are trained for 2 epochs on both MSMARCO and NQ. Optimizers We use ADAM optimizer. Warmup and learning rate The learning rate of the dual-encoder is set to 3e-5 and the rate of linear scheduling warm-up is set to 0.1, while the learning rate of the cross-encoder is set to 1e-5. Maximal length We set the maximal length of questions and passages as 32 and 128, respectively. Unlabeled questions We collect 1.7 million unlabeled questions from Yahoo! Answers5 , ORCAS (Craswell et al., 2020) and MRQA (Fisch et al., 2019). We use the questions from Yahoo! Answers, ORCAS and NQ as new questions in the experiments of MSMARCO. We only use the questions from MRQA as the new questions in the experiments of NQ. Since both NQ and MRQA mainly contain factoid-questions, while other datasets contain both factoid and non-factoid questions. 4.2 Experimental Results In our experiments, we first examine the effectiveness of our retriever on MSMARCO and NQ datasets. Then, we conduct extensive experiments to examine the effects of the three proposed training strategies. We also show the performance of endto-end QA based on ou"
2021.naacl-main.466,2020.emnlp-main.342,0,0.255114,"re-ranking for open-domain QA Based on the retrieved passages from a first-stage retriever, BERT-based rerankers have recently been applied to retrieval-based question answering and search-related tasks (Wang et al., 2019; Nogueira and Cho, 2019; Nogueira et al., 2019b; Yan et al., 2019), and yield substantial improvements over the traditional methods. Although effective to some extent, these rankers employ the cross-encoder architecture (as shown in Figure 1b) that is impractical to be applied to all passages in a corpus with respect to a question. The re-rankers (Khattab and Zaharia, 2020; Gao et al., 2020) with light weight interaction based on the representations of dense retrievers have been studied. However, these techniques still rely on a separate retriever which provides candidates and representations. As a comparison, we focus on developing dual-encoder based retrievers. 3 Approach 3.1 Task Description The task of open-domain QA is described as follows. Given a natural language question, a system is required to answer it based on a large collection of documents. Let C denote the corpus, consisting of N documents. We split the N documents into M passages, denoted by p1 , p2 , ..., pM , wh"
2021.naacl-main.466,K19-1049,0,0.458038,"exists the discrepancy between training and inference for the dual-encoder retriever. During inference, the retriever needs to identify positive (or relevant) passages for each question from a large collection containing millions of candidates. However, during training, the model is learned to estimate the probabilities of positive passages in a small candidate set for each question, due to the limited memory of a single GPU (or other device). To reduce such a discrepancy, previous work tried to design specific mechanisms for selecting a few hard negatives from the top-k retrieved candidates (Gillick et al., 2019; Wu et al., 2020; Karpukhin et al., 2020; Luan et al., 2020; Xiong et al., 2020). However, it suffers from the false negative issue due to the following challenge. Second, there might be a large number of unlabeled positives. Usually, it is infeasible to completely annotate all the candidate passages for one question. By only examining the the top-K passages retrieved by a specific retrieval approach (e.g. BM25), the annotators are likely to miss relevant passages to a question. Taking the MSMARCO dataset (Nguyen et al., 2016) as an example, each question has only 1.1 annotated positive passa"
2021.naacl-main.466,2020.emnlp-main.550,0,0.0498813,"retriever (e.g., TF-IDF or BM25) first selects a few relevant passages as contexts, and then a neural reader reads the contexts and extracts the answers. As the recall component, the first-stage retriever significantly affects the final QA performance. Though efficient with an inverted index, traditional IR retrievers with term-based sparse representations have limited capabilities in matching questions and passages, e.g., term mismatch. To deal with the issue of term mismatch, the dual-encoder architecture (as shown in Figure 1a) has been widely explored (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020; Luan et al., 2020; Xiong et al., 2020) to learn dense representations of questions and passages in an end-to-end manner, which provides better representations for semantic matching. These studies first separately encode questions and passages to obtain their dense ∗ representations, and then compute the similarity Corresponding authors. † The work was done when Ruiyang Ren was doing inbetween the dense representations using similarity ternship at Baidu. functions such as cosine or dot product. Typically, 1 Our code is available at https://github.com/ the dual-encoder is trained by using in-b"
2021.naacl-main.466,Q19-1026,0,0.0329179,".58 100.0 Table 1: The statistics of datasets MSMARCO and Natural Questions. Here, “p” and “q” are the abbreviations of questions and passages, respectively. The length is in tokens. encoder. The cross-encoder is used both STEP 3 and STEP 4 with different purposes to promote the performance of the dual encoder. The implementation details of denoising hard negatives and data augmentation can be found in Section 4. 4 Experiments 4.1 4.1.1 Experimental Setup Datasets We conduct the experiments on two popular QA benchmarks: MSMARCO Passage Ranking (Nguyen et al., 2016) and Natural Questions (NQ) (Kwiatkowski et al., 2019). The statistics of the datasets are listed in Table 1. MSMARCO Passage Ranking MSMARCO is originally designed for multiple passage MRC, and its questions were sampled from Bing search logs. Based on the questions and passages in MSMARCO Question Answering, a dataset for passage ranking was created, namely MSMARCO Passage Ranking, consisting of about 8.8 million passages. The goal is to find positive passages that answer the questions. Natural Question (NQ) Kwiatkowski et al. (2019) introduces a large dataset for open-domain QA. The original dataset contains more than 300, 000 questions collec"
2021.naacl-main.466,P19-1612,0,0.475888,"oder and crossencoder architectures. retriever (e.g., TF-IDF or BM25) first selects a few relevant passages as contexts, and then a neural reader reads the contexts and extracts the answers. As the recall component, the first-stage retriever significantly affects the final QA performance. Though efficient with an inverted index, traditional IR retrievers with term-based sparse representations have limited capabilities in matching questions and passages, e.g., term mismatch. To deal with the issue of term mismatch, the dual-encoder architecture (as shown in Figure 1a) has been widely explored (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020; Luan et al., 2020; Xiong et al., 2020) to learn dense representations of questions and passages in an end-to-end manner, which provides better representations for semantic matching. These studies first separately encode questions and passages to obtain their dense ∗ representations, and then compute the similarity Corresponding authors. † The work was done when Ruiyang Ren was doing inbetween the dense representations using similarity ternship at Baidu. functions such as cosine or dot product. Typically, 1 Our code is available at https://github.com/"
2021.naacl-main.466,D19-1284,0,0.093921,"Missing"
2021.naacl-main.466,D19-1599,0,0.0992005,"Missing"
2021.naacl-main.466,2020.emnlp-main.519,0,0.731378,"between training and inference for the dual-encoder retriever. During inference, the retriever needs to identify positive (or relevant) passages for each question from a large collection containing millions of candidates. However, during training, the model is learned to estimate the probabilities of positive passages in a small candidate set for each question, due to the limited memory of a single GPU (or other device). To reduce such a discrepancy, previous work tried to design specific mechanisms for selecting a few hard negatives from the top-k retrieved candidates (Gillick et al., 2019; Wu et al., 2020; Karpukhin et al., 2020; Luan et al., 2020; Xiong et al., 2020). However, it suffers from the false negative issue due to the following challenge. Second, there might be a large number of unlabeled positives. Usually, it is infeasible to completely annotate all the candidate passages for one question. By only examining the the top-K passages retrieved by a specific retrieval approach (e.g. BM25), the annotators are likely to miss relevant passages to a question. Taking the MSMARCO dataset (Nguyen et al., 2016) as an example, each question has only 1.1 annotated positive passages on average, w"
2021.nlp4convai-1.14,2021.tacl-1.6,0,0.033141,"ese approaches has difficulties to hit the knowledge contained in the target response, and deteriorates the learning of knowledge utilization. Our top-k selection improves the robustness of prior knowledge selection. Some other works (Lian et al., 2019; Zhao et al., 2020; Ren et al., 2020) employ the target response to identify the grounded knowledge. Since the posterior knowledge selection is involved, it will inevitably cause discrepancy between the training and inference stages (Zhao et al., 2019). With end-to-end modeling and optimization, PLATO-KAG gets exempt from this discrepancy. KIF (Fan et al., 2021) explicitly selects external knowledge through a retrieval module, and fuses into one integrated representation to assist dialogue generation. While some knowledge details might be obscured with this fusion. As comparison, the knowledge keeps its independence and integrity in our response generation, which helps reduce the hallucination. More recently, Shuster et al. (2021) attempts to utilize the pre-trained retriever DPR (Karpukhin et al., 2020). DPR has been trained on Wikipedia which includes the knowledge sets of WoW and Holl-E. Due to the concern of potential data contamination, we choos"
2021.nlp4convai-1.14,2020.emnlp-main.550,0,0.0167985,"etween the training and inference stages (Zhao et al., 2019). With end-to-end modeling and optimization, PLATO-KAG gets exempt from this discrepancy. KIF (Fan et al., 2021) explicitly selects external knowledge through a retrieval module, and fuses into one integrated representation to assist dialogue generation. While some knowledge details might be obscured with this fusion. As comparison, the knowledge keeps its independence and integrity in our response generation, which helps reduce the hallucination. More recently, Shuster et al. (2021) attempts to utilize the pre-trained retriever DPR (Karpukhin et al., 2020). DPR has been trained on Wikipedia which includes the knowledge sets of WoW and Holl-E. Due to the concern of potential data contamination, we choosed to initialize our knowledge selection module with a general dialogue model which is pre-trained on Reddit. Thus, we facilitated an unbiased setting for our experiments and the analysis of framework generalization. Knowledge-grounded conversation is becoming a more important and popular topic, with several datasets (Zhang et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Dinan et al., 2019; Gopalakrishnan et al., 2019; Komeili et al., 2021) c"
2021.nlp4convai-1.14,N16-1014,0,0.0399779,"the pre-training models used for initialization6 . The average Fleiss’s kappa (Fleiss, 1971) in human evaluation is 0.502, indicating that annotators have reached moderate agreement. The evaluation results on the Holl-E test set are summarized in Table 2. In the evaluation on the multiple reference test set, we took the best score over multiple reference responses for each dialogue context. The results demonstrate that PLATO-KAG also achieves competitive results in Holl-E. PostKS obtains a slightly higher value on Unigram F1 than PLATO-KAG and supervised TMN. While the values on Distinct-1/2 (Li et al., 2016) indicate the PLATO-KAG and supervised TMN might have better capacity on lexical diversity. 3.3 3.3.1 Discussions Case Analysis For further qualitative analysis, two examples of generated responses from the WoW test set are provided in Table 3. It can be observed that unsupervised TMN suffers from low-quality response generation, such as generic replies with little information or statements with factual errors. In comparison, PostKS and KnowledGPT are able to generate much more informative responses, depicting contents from the selected knowledge. However, the responses fail to be coherent wit"
2021.nlp4convai-1.14,P19-1002,0,0.0159839,"ed learning difficulty of knowledge-grounded conver- for end-to-end knowledge grounded conversation sation. However, given that manual annotation is modeling. There are two main components in our expensive and time-consuming, it is not feasible to method: knowledge selection and response gencarry out the knowledge labelling on a large scale. eration. Given a dialogue context, top-k relevant Unsupervised approaches have been introduced knowledge elements are selected and utilized for to model knowledge-grounded conversation. Some response generation. The generation probability of these such as Li et al. (2019); Yavuz et al. can in turn provide training signal for the prece(2019); Lin et al. (2020) perform implicit soft fu- dent knowledge selection. Joint balanced training sion over provided knowledge elements and do not is further introduced for the effective optimization select knowledge explicitly. Some attempts have of these two components. Comprehensive experibeen made to learn the unsupervised selection of ments have been carried out on WoW and Holl-E, external knowledge based on semantic similarity verifying the effectiveness and superiority of the (Ghazvininejad et al., 2018; Dinan et al., 2"
2021.nlp4convai-1.14,2020.acl-main.6,0,0.0312241,"onversation sation. However, given that manual annotation is modeling. There are two main components in our expensive and time-consuming, it is not feasible to method: knowledge selection and response gencarry out the knowledge labelling on a large scale. eration. Given a dialogue context, top-k relevant Unsupervised approaches have been introduced knowledge elements are selected and utilized for to model knowledge-grounded conversation. Some response generation. The generation probability of these such as Li et al. (2019); Yavuz et al. can in turn provide training signal for the prece(2019); Lin et al. (2020) perform implicit soft fu- dent knowledge selection. Joint balanced training sion over provided knowledge elements and do not is further introduced for the effective optimization select knowledge explicitly. Some attempts have of these two components. Comprehensive experibeen made to learn the unsupervised selection of ments have been carried out on WoW and Holl-E, external knowledge based on semantic similarity verifying the effectiveness and superiority of the (Ghazvininejad et al., 2018; Dinan et al., 2019). proposed method. 150 Acknowledgements We would like to thank the anonymous reviewer"
2021.nlp4convai-1.14,D18-1255,0,0.162148,"aptive normalization on the second term, our method successfully maintains the balance between knowledge selection and knowledge-grounded response generation. More analyses on the component weight are included in the experiments. 3 3.1 Experiments Settings 3.1.1 Datasets In the sequence form of Equation (5a), it relies We conducted experiments on two knowledgeon one knowledge element to predict the whole grounded conversation datasets: Wizard of sequence of the target response. In the token form Wikipedia (WoW) (Dinan et al., 2019) and Holl-E of Equation (5b), the generative process can rely (Moghe et al., 2018). on different knowledge elements independently for In Wizard of Wikipedia, two participants conduct each token. in-depth discussion on a chosen beginning topic. With the sequence form, the selection of knowl- One of the participants has access to relevant knowledge just weight like the generation of one response edge and plays the role of an expert (wizard). The token. Given the long responses in knowledge- other one acts as a curious learner (apprentice). grounded conversation2 , the module of knowledge There are 18,430/1,948/1,933 dialogues in the trainselection is at a distinct disadvantag"
2021.nlp4convai-1.14,2021.eacl-main.24,0,0.0346446,"and tal results on two publicly available datasets KnowledGPT (Zhao et al., 2020) rely on the tarvalidate the superiority of PLATO-KAG. get response to identify the grounded knowledge. 1 Introduction However, involving the posterior knowledge selection will inevitably cause discrepancy between the Recently, the capability of large-scale pre-trained training and inference stages (Zhao et al., 2019). models has been verified in open-domain dialogue In this paper, we propose an unsupervised generation, including Meena (Adiwardana et al., approach for end-to-end knowledge-grounded 2020), Blender (Roller et al., 2021), and PLATO-2 conversation modeling, namely PLATO-KAG (Bao et al., 2020). Without introducing explicit (Knowledge-Augmented Generation). As shown knowledge in learning process, substantive knowlin Figure 1, given each dialogue context, the edge is implicitly embedded into parameters from top-k relevant knowledge elements are selected the training corpus. However, these models are for the subsequent response generation. Then, found to suffer from knowledge hallucinations (Roller et al., 2021; Marcus, 2020), producing plau- the model learns to generate the target response grounded on each of the"
2021.nlp4convai-1.14,2021.findings-emnlp.320,0,0.023837,"r knowledge selection is involved, it will inevitably cause discrepancy between the training and inference stages (Zhao et al., 2019). With end-to-end modeling and optimization, PLATO-KAG gets exempt from this discrepancy. KIF (Fan et al., 2021) explicitly selects external knowledge through a retrieval module, and fuses into one integrated representation to assist dialogue generation. While some knowledge details might be obscured with this fusion. As comparison, the knowledge keeps its independence and integrity in our response generation, which helps reduce the hallucination. More recently, Shuster et al. (2021) attempts to utilize the pre-trained retriever DPR (Karpukhin et al., 2020). DPR has been trained on Wikipedia which includes the knowledge sets of WoW and Holl-E. Due to the concern of potential data contamination, we choosed to initialize our knowledge selection module with a general dialogue model which is pre-trained on Reddit. Thus, we facilitated an unbiased setting for our experiments and the analysis of framework generalization. Knowledge-grounded conversation is becoming a more important and popular topic, with several datasets (Zhang et al., 2018; Moghe et al., 2018; Zhou et al., 201"
2021.nlp4convai-1.14,W19-5917,0,0.0308804,"Missing"
2021.nlp4convai-1.14,P18-1205,0,0.0251644,"the hallucination. More recently, Shuster et al. (2021) attempts to utilize the pre-trained retriever DPR (Karpukhin et al., 2020). DPR has been trained on Wikipedia which includes the knowledge sets of WoW and Holl-E. Due to the concern of potential data contamination, we choosed to initialize our knowledge selection module with a general dialogue model which is pre-trained on Reddit. Thus, we facilitated an unbiased setting for our experiments and the analysis of framework generalization. Knowledge-grounded conversation is becoming a more important and popular topic, with several datasets (Zhang et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Dinan et al., 2019; Gopalakrishnan et al., 2019; Komeili et al., 2021) collected to 5 Conclusion study it. Besides interactive dialogues, some of these datasets have annotated the corresponding knowledge for each response, aiming to ease the In this paper, an unsupervised approach is proposed learning difficulty of knowledge-grounded conver- for end-to-end knowledge grounded conversation sation. However, given that manual annotation is modeling. There are two main components in our expensive and time-consuming, it is not feasible to method: knowledge se"
2021.nlp4convai-1.14,N19-1123,0,0.150515,"ontained in the target response, deterigeneration are optimized jointly and effecorating the learning of knowledge utilization. As tively under a balanced objective. Experimenan improvement, PostKS (Lian et al., 2019) and tal results on two publicly available datasets KnowledGPT (Zhao et al., 2020) rely on the tarvalidate the superiority of PLATO-KAG. get response to identify the grounded knowledge. 1 Introduction However, involving the posterior knowledge selection will inevitably cause discrepancy between the Recently, the capability of large-scale pre-trained training and inference stages (Zhao et al., 2019). models has been verified in open-domain dialogue In this paper, we propose an unsupervised generation, including Meena (Adiwardana et al., approach for end-to-end knowledge-grounded 2020), Blender (Roller et al., 2021), and PLATO-2 conversation modeling, namely PLATO-KAG (Bao et al., 2020). Without introducing explicit (Knowledge-Augmented Generation). As shown knowledge in learning process, substantive knowlin Figure 1, given each dialogue context, the edge is implicitly embedded into parameters from top-k relevant knowledge elements are selected the training corpus. However, these models a"
2021.nlp4convai-1.14,2020.emnlp-main.272,0,0.213758,"relevant knowledge elements context. The prior top-1 knowledge selection emare selected and then employed in knowledgeployed by these approaches (Ghazvininejad et al., grounded response generation. The two com2018; Dinan et al., 2019) has difficulties to hit the ponents of knowledge selection and response knowledge contained in the target response, deterigeneration are optimized jointly and effecorating the learning of knowledge utilization. As tively under a balanced objective. Experimenan improvement, PostKS (Lian et al., 2019) and tal results on two publicly available datasets KnowledGPT (Zhao et al., 2020) rely on the tarvalidate the superiority of PLATO-KAG. get response to identify the grounded knowledge. 1 Introduction However, involving the posterior knowledge selection will inevitably cause discrepancy between the Recently, the capability of large-scale pre-trained training and inference stages (Zhao et al., 2019). models has been verified in open-domain dialogue In this paper, we propose an unsupervised generation, including Meena (Adiwardana et al., approach for end-to-end knowledge-grounded 2020), Blender (Roller et al., 2021), and PLATO-2 conversation modeling, namely PLATO-KAG (Bao et"
2021.nlp4convai-1.14,D18-1076,0,0.0195368,"er et al. (2021) attempts to utilize the pre-trained retriever DPR (Karpukhin et al., 2020). DPR has been trained on Wikipedia which includes the knowledge sets of WoW and Holl-E. Due to the concern of potential data contamination, we choosed to initialize our knowledge selection module with a general dialogue model which is pre-trained on Reddit. Thus, we facilitated an unbiased setting for our experiments and the analysis of framework generalization. Knowledge-grounded conversation is becoming a more important and popular topic, with several datasets (Zhang et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Dinan et al., 2019; Gopalakrishnan et al., 2019; Komeili et al., 2021) collected to 5 Conclusion study it. Besides interactive dialogues, some of these datasets have annotated the corresponding knowledge for each response, aiming to ease the In this paper, an unsupervised approach is proposed learning difficulty of knowledge-grounded conver- for end-to-end knowledge grounded conversation sation. However, given that manual annotation is modeling. There are two main components in our expensive and time-consuming, it is not feasible to method: knowledge selection and response gencarry out the k"
C04-1005,P98-1004,0,0.354377,"termediate result in statistical machine translation (SMT) (Brown et al. 1993). Besides being used in SMT, it is also used in translation lexicon building (Melamed 1996), transfer rule learning (Menezes and Richardson 2001), example-based machine translation (Somers 1999), etc. In previous alignment methods, some researches modeled the alignments as hidden parameters in a statistical translation model (Brown et al. 1993; Och and Ney 2000) or directly modeled them given the sentence pairs (Cherry and Lin 2003). Some researchers used similarity and association measures to build alignment links (Ahrenberg et al. 1998; Tufis and Barbu 2002). In addition, Wu (1997) used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Generally speaking, there are four cases in word alignment: word to word alignment, word to multi-word alignment, multi-word to word alignment, and multi-word to multi-word alignment. One of the most difficult tasks in word alignment is to find out the alignments that include multi-word units. For example, the statistical word alignment in IBM translation models (Brown et al. 1993) can only handle word to word and mult"
C04-1005,ahrenberg-etal-2000-evaluation,0,0.0355324,"Missing"
C04-1005,P03-1012,0,0.18936,"ecision and recall of word alignment. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al. 1993). Besides being used in SMT, it is also used in translation lexicon building (Melamed 1996), transfer rule learning (Menezes and Richardson 2001), example-based machine translation (Somers 1999), etc. In previous alignment methods, some researches modeled the alignments as hidden parameters in a statistical translation model (Brown et al. 1993; Och and Ney 2000) or directly modeled them given the sentence pairs (Cherry and Lin 2003). Some researchers used similarity and association measures to build alignment links (Ahrenberg et al. 1998; Tufis and Barbu 2002). In addition, Wu (1997) used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Generally speaking, there are four cases in word alignment: word to word alignment, word to multi-word alignment, multi-word to word alignment, and multi-word to multi-word alignment. One of the most difficult tasks in word alignment is to find out the alignments that include multi-word units. For example, the sta"
C04-1005,1996.amta-1.13,0,0.0369898,"get and target to source), and then uses the translation information in the rule-based machine translation system to improve the statistical word alignment. The improved alignments allow the word(s) in the source language to be aligned to one or more words in the target language. Experimental results show a significant improvement in precision and recall of word alignment. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al. 1993). Besides being used in SMT, it is also used in translation lexicon building (Melamed 1996), transfer rule learning (Menezes and Richardson 2001), example-based machine translation (Somers 1999), etc. In previous alignment methods, some researches modeled the alignments as hidden parameters in a statistical translation model (Brown et al. 1993; Och and Ney 2000) or directly modeled them given the sentence pairs (Cherry and Lin 2003). Some researchers used similarity and association measures to build alignment links (Ahrenberg et al. 1998; Tufis and Barbu 2002). In addition, Wu (1997) used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get t"
C04-1005,J00-2004,0,0.103013,"can only handle word to word and multi-word to word alignments. Some studies have been made to tackle this problem. Och and Ney (2000) performed translation in both directions (source to target and target to source) to extend word alignments. Their results showed that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Some researchers used preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al. 1998; Tiedemann 1999; Melamed 2000). The methods obtained multi-word candidates based on continuous N-gram statistics. The main limitation of these methods is that they cannot handle separated phrases and multi-word units in low frequencies. In order to handle all of the four cases in word alignment, our approach uses both the alignment information in statistical translation models and translation information in a rule-based machine translation system. It includes three steps. (1) A statistical translation model is employed to perform word alignment in two directions1 (English to Chinese, Chinese to English). (2) A rule-based E"
C04-1005,W01-1406,0,0.0244656,"s the translation information in the rule-based machine translation system to improve the statistical word alignment. The improved alignments allow the word(s) in the source language to be aligned to one or more words in the target language. Experimental results show a significant improvement in precision and recall of word alignment. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al. 1993). Besides being used in SMT, it is also used in translation lexicon building (Melamed 1996), transfer rule learning (Menezes and Richardson 2001), example-based machine translation (Somers 1999), etc. In previous alignment methods, some researches modeled the alignments as hidden parameters in a statistical translation model (Brown et al. 1993; Och and Ney 2000) or directly modeled them given the sentence pairs (Cherry and Lin 2003). Some researchers used similarity and association measures to build alignment links (Ahrenberg et al. 1998; Tufis and Barbu 2002). In addition, Wu (1997) used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Generally speaking, ther"
C04-1005,P00-1056,0,0.735936,"t language. Experimental results show a significant improvement in precision and recall of word alignment. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al. 1993). Besides being used in SMT, it is also used in translation lexicon building (Melamed 1996), transfer rule learning (Menezes and Richardson 2001), example-based machine translation (Somers 1999), etc. In previous alignment methods, some researches modeled the alignments as hidden parameters in a statistical translation model (Brown et al. 1993; Och and Ney 2000) or directly modeled them given the sentence pairs (Cherry and Lin 2003). Some researchers used similarity and association measures to build alignment links (Ahrenberg et al. 1998; Tufis and Barbu 2002). In addition, Wu (1997) used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Generally speaking, there are four cases in word alignment: word to word alignment, word to multi-word alignment, multi-word to word alignment, and multi-word to multi-word alignment. One of the most difficult tasks in word alignment is to fin"
C04-1005,tufis-barbu-2002-lexical,0,0.622409,"atistical machine translation (SMT) (Brown et al. 1993). Besides being used in SMT, it is also used in translation lexicon building (Melamed 1996), transfer rule learning (Menezes and Richardson 2001), example-based machine translation (Somers 1999), etc. In previous alignment methods, some researches modeled the alignments as hidden parameters in a statistical translation model (Brown et al. 1993; Och and Ney 2000) or directly modeled them given the sentence pairs (Cherry and Lin 2003). Some researchers used similarity and association measures to build alignment links (Ahrenberg et al. 1998; Tufis and Barbu 2002). In addition, Wu (1997) used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Generally speaking, there are four cases in word alignment: word to word alignment, word to multi-word alignment, multi-word to word alignment, and multi-word to multi-word alignment. One of the most difficult tasks in word alignment is to find out the alignments that include multi-word units. For example, the statistical word alignment in IBM translation models (Brown et al. 1993) can only handle word to word and multi-word to word alignmen"
C04-1005,J97-3002,0,0.109237,"(Brown et al. 1993). Besides being used in SMT, it is also used in translation lexicon building (Melamed 1996), transfer rule learning (Menezes and Richardson 2001), example-based machine translation (Somers 1999), etc. In previous alignment methods, some researches modeled the alignments as hidden parameters in a statistical translation model (Brown et al. 1993; Och and Ney 2000) or directly modeled them given the sentence pairs (Cherry and Lin 2003). Some researchers used similarity and association measures to build alignment links (Ahrenberg et al. 1998; Tufis and Barbu 2002). In addition, Wu (1997) used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Generally speaking, there are four cases in word alignment: word to word alignment, word to multi-word alignment, multi-word to word alignment, and multi-word to multi-word alignment. One of the most difficult tasks in word alignment is to find out the alignments that include multi-word units. For example, the statistical word alignment in IBM translation models (Brown et al. 1993) can only handle word to word and multi-word to word alignments. Some studies have be"
C04-1005,W03-1610,1,0.813124,"d to the Chinese word “习惯”, and “is” and “to” have null links in S . But in the translation set S 3 , “is used to&quot; is a phrase. Thus, we combine the three alignment links into a new link. The words “is”, “used” and “ to” are all aligned to the Chinese word “习惯”, denoted as (is used to, 习惯). Figure 2 describes the algorithm employed to improve the word alignment in the intersection set S . Word Similarity Calculation This section describes the method for monolingual word similarity calculation. This method calculates word similarity by using a bilingual dictionary, which is first introduced by Wu and Zhou (2003). The basic assumptions of this method are that the translations of a word can express its meanings and that two words are similar in meanings if they have mutual translations. Given a Chinese word, we get its translations with a Chinese-English bilingual dictionary. The translations of a word are used to construct its feature vector. The similarity of two words is estimated through their feature vectors with the cosine measure as shown in (Wu and Zhou 2003). If there are a Chinese word or phrase w and a Chinese word set Z , the word similarity between them is calculated as shown in Equation ("
C04-1005,1987.mtsummit-1.11,0,\N,Missing
C04-1005,J93-2003,0,\N,Missing
C04-1005,2001.mtsummit-ebmt.4,0,\N,Missing
C04-1005,C98-1004,0,\N,Missing
C08-1038,C00-1007,0,0.431987,"dels have become widely used in the field of natural language generation (NLG), often in the form of a realisation ranker in a two-stage generation architecture. The two-stage methodology is characterised by a separation between generation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realisation from the space. By and large, two statistical models are used in the rankers to choose output strings: • N-gram language models over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al., 2007). • Log-linear models with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). To date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct a grammar, have rarely been explored. An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribu"
C08-1038,N07-1021,0,0.0277415,"egorial Grammar (CCG), Tree Adjoining Grammar (TAG) etc. These grammar rules are either carefully handcrafted, as those used in FUF/SURGE (Elhadad, 1991), LKB (Carroll et al., c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 In this paper, the term “generation” is used generally for what is more strictly referred to by the term “tactical generation” or “surface realisation”. 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or created semi-automatically (Belz, 2007), or fully automatically extracted from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank (PTB) (Marcus et al., 1993). Over the last decade, probabilistic models have become widely used in the field of natural language generation (NLG), often in the form of a realisation ranker in a two-stage generation architecture. The two-stage methodology is characterised by a separation between generation and selection, in which rule-based methods are used to generate a"
C08-1038,P04-1041,1,0.894373,"Missing"
C08-1038,A00-2023,0,0.0601371,"in the field of natural language generation (NLG), often in the form of a realisation ranker in a two-stage generation architecture. The two-stage methodology is characterised by a separation between generation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realisation from the space. By and large, two statistical models are used in the rankers to choose output strings: • N-gram language models over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al., 2007). • Log-linear models with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). To date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct a grammar, have rarely been explored. An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, re"
C08-1038,P06-1130,1,0.92336,"Missing"
C08-1038,W02-2103,0,0.289707,"input to our generator are unordered fstructures automatically derived from the development and test set trees of our treebanks, which do not contain any string position information. But, due to the particulars of the automatic f-structure annotation algorithm, the order of sub-f-structures in set-valued GFs, such as ADJ, COORD, happens to correspond to their surface order. To avoid unfairly inflating evaluation results, we lexically reorder the GFs in each sub-f-structure of the development and test input before the generation process. This resembles the “permute, no dir” type experiment in (Langkilde, 2002). 5.2 Experimental Results Following (Langkilde, 2002) and other work on general-purpose generators, BLEU score (Papineni et al., 2002), average NIST simple string accuracy (SSA) and percentage of exactly matched sentences are adopted as evaluation metrics. As our system guarantees that all input fstructures can generate a complete sentence, special coverage-dependent evaluation (as has been Lexicalisation plays an important role in both English and Chinese, boosting the BLEU score without features from 0.5074 to 0.6741 for English, and from 0.5752 to 0.6639 for Chinese. Atomic-valued features"
C08-1038,W07-2303,0,0.0254705,"ct), OBJ(ect) and ADJ(unct). F-structures are hierarchical attribute2.2 Generation from F-Structures Work on generation in LFG generally assumes that the generation task is to determine the set of strings of the language that corresponds to a specified fstructure, given a particular grammar (Kaplan and Wedekind, 2000). Previous work on generation 2 F-structures can be also interpreted as quasi-logical forms (van Genabith and Crouch, 1996), which more closely resemble inputs used by some other generators. 298 within LFG includes the XLE,3 Cahill and van Genabith (2006), Hogan et al. (2007) and Cahill et al. (2007). The XLE generates sentences from fstructures according to parallel handcrafted grammars for English, French, German, Norwegian, Japanese, and Urdu. Based on the German XLE resources, Cahill et al. (2007) describe a two-stage, log-linear generation model. Cahill and van Genabith (2006) and Hogan et al. (2007) present a chart generator using wide-coverage PCFG-based LFG approximations automatically acquired from treebanks (Cahill et al., 2004). 3 Dependency-Based Generation: the Basic Idea Traditional LFG generation models can be regarded as the reverse process of parsing, and use bi-direction"
C08-1038,J93-2004,0,0.0297399,"Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 In this paper, the term “generation” is used generally for what is more strictly referred to by the term “tactical generation” or “surface realisation”. 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or created semi-automatically (Belz, 2007), or fully automatically extracted from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank (PTB) (Marcus et al., 1993). Over the last decade, probabilistic models have become widely used in the field of natural language generation (NLG), often in the form of a realisation ranker in a two-stage generation architecture. The two-stage methodology is characterised by a separation between generation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realisation from the space. By and large, two statistical models are used in the rankers to choose output strings: • N-gram language models over different units, su"
C08-1038,W05-1510,0,0.495799,"er carefully handcrafted, as those used in FUF/SURGE (Elhadad, 1991), LKB (Carroll et al., c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 In this paper, the term “generation” is used generally for what is more strictly referred to by the term “tactical generation” or “surface realisation”. 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or created semi-automatically (Belz, 2007), or fully automatically extracted from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank (PTB) (Marcus et al., 1993). Over the last decade, probabilistic models have become widely used in the field of natural language generation (NLG), often in the form of a realisation ranker in a two-stage generation architecture. The two-stage methodology is characterised by a separation between generation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realis"
C08-1038,P02-1040,0,0.0784176,"which do not contain any string position information. But, due to the particulars of the automatic f-structure annotation algorithm, the order of sub-f-structures in set-valued GFs, such as ADJ, COORD, happens to correspond to their surface order. To avoid unfairly inflating evaluation results, we lexically reorder the GFs in each sub-f-structure of the development and test input before the generation process. This resembles the “permute, no dir” type experiment in (Langkilde, 2002). 5.2 Experimental Results Following (Langkilde, 2002) and other work on general-purpose generators, BLEU score (Papineni et al., 2002), average NIST simple string accuracy (SSA) and percentage of exactly matched sentences are adopted as evaluation metrics. As our system guarantees that all input fstructures can generate a complete sentence, special coverage-dependent evaluation (as has been Lexicalisation plays an important role in both English and Chinese, boosting the BLEU score without features from 0.5074 to 0.6741 for English, and from 0.5752 to 0.6639 for Chinese. Atomic-valued features play an important role in English, and boost the BLEU score from 0.5074 in the baseline model to 0.6842 when feature names are integra"
C08-1038,A00-2026,0,0.111311,"ls are used in the rankers to choose output strings: • N-gram language models over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al., 2007). • Log-linear models with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). To date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct a grammar, have rarely been explored. An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. 297 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 297–304 Manchester, August 2008  S NP VP PRP VBP PP We believe IN in NP NP PP DT NN IN NP the law of NNS averages  ‘believe’ pres  PRED  TENSE      SUBJ           f1       OBL            PRED  f2 PERS  NUM ‘pro’  1  pl ‘in’  PFORM           f"
C08-1038,C96-1045,1,0.742104,"Missing"
C08-1038,2005.mtsummit-papers.15,0,0.171931,"acterised by a separation between generation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realisation from the space. By and large, two statistical models are used in the rankers to choose output strings: • N-gram language models over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al., 2007). • Log-linear models with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). To date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct a grammar, have rarely been explored. An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. 297 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 297–304 Manchester, August 2008  S N"
C08-1038,2007.mtsummit-ucnlg.4,0,0.496422,"c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 In this paper, the term “generation” is used generally for what is more strictly referred to by the term “tactical generation” or “surface realisation”. 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or created semi-automatically (Belz, 2007), or fully automatically extracted from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank (PTB) (Marcus et al., 1993). Over the last decade, probabilistic models have become widely used in the field of natural language generation (NLG), often in the form of a realisation ranker in a two-stage generation architecture. The two-stage methodology is characterised by a separation between generation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realisation from the space. By and large, two statistical models are used in the rankers to"
C08-1038,C00-1062,0,0.0175863,"postulates (minimally) two levels of representation: c(onstituent)-structure and f(unctional)-structure. As illustrated in Figure 1, a c-structure is a conventional phrase structure tree and captures surface grammatical configurations. The f-structure encodes more abstract functional relations like SUBJ(ect), OBJ(ect) and ADJ(unct). F-structures are hierarchical attribute2.2 Generation from F-Structures Work on generation in LFG generally assumes that the generation task is to determine the set of strings of the language that corresponds to a specified fstructure, given a particular grammar (Kaplan and Wedekind, 2000). Previous work on generation 2 F-structures can be also interpreted as quasi-logical forms (van Genabith and Crouch, 1996), which more closely resemble inputs used by some other generators. 298 within LFG includes the XLE,3 Cahill and van Genabith (2006), Hogan et al. (2007) and Cahill et al. (2007). The XLE generates sentences from fstructures according to parallel handcrafted grammars for English, French, German, Norwegian, Japanese, and Urdu. Based on the German XLE resources, Cahill et al. (2007) describe a two-stage, log-linear generation model. Cahill and van Genabith (2006) and Hogan e"
C08-1038,D07-1028,1,\N,Missing
C08-1038,W96-0501,0,\N,Missing
C08-1038,N09-2057,0,\N,Missing
C08-1038,W08-2121,0,\N,Missing
C08-1038,C04-1097,0,\N,Missing
C08-1038,N03-1031,0,\N,Missing
C08-1038,N03-2002,0,\N,Missing
C08-1038,C02-1036,0,\N,Missing
C08-1038,D09-1043,0,\N,Missing
C08-1038,C00-2126,0,\N,Missing
C08-1038,W08-1111,0,\N,Missing
C08-1038,P07-1041,0,\N,Missing
C08-1038,N09-2041,0,\N,Missing
C08-1038,P05-1012,0,\N,Missing
C08-1038,P06-1085,0,\N,Missing
C08-1038,W07-2416,0,\N,Missing
C08-1038,D09-1000,0,\N,Missing
C08-1038,W02-2105,0,\N,Missing
C08-1038,A97-1039,0,\N,Missing
C08-1038,P03-1021,0,\N,Missing
C08-1038,W15-4700,0,\N,Missing
C08-1105,W05-0620,0,0.0428654,"Missing"
C08-1105,J02-3001,0,0.25207,"may cause labeling errors such as constituents outside active region of arguments may be falsely recognized as roles. Introduction Semantic Role Labeling (SRL) has gained the interest of many researchers in the last few years. SRL consists of recognizing arguments involved by predicates of a given sentence and labeling their semantic types. As a well defined task of shallow semantic parsing, SRL has a variety of applications in many kinds of NLP tasks. A variety of approaches has been proposed for the different characteristics of SRL. More recent approaches have involved calibrating features (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; ∗ This work was partial completed while this author was at Toshiba (China) R&D Center. ∗ c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. This paper uses insights from generative linguistics to guide the solution of locality of arguments. In particular, Maximal Projection (MP) which dominates1 active region of arguments according to the projection principle of principle and parameters. Two methods, the anchor group approach and the single anch"
C08-1105,P07-1025,0,0.0332107,"Missing"
C08-1105,W05-0625,0,0.0288869,"Missing"
C08-1105,P04-1043,0,0.0708338,"Missing"
C08-1105,P05-1073,0,0.0397734,"Missing"
C08-1105,W04-3212,0,0.0123727,"D836 structure plus movement. NP-movement principle in principle and parameters indicates that noun phrases only move from A-positions (argument position) which have been assigned roles to A-positions which have not, leaving an NPtrace. On account of θ-theory and government, Apositions are nodes m-commanded 3 by predicates in D-structure. In NP-movement, arguments move to positions which are C-commanded 4 by target predicate and m-commanded by other predicates. Broadly speaking, A-positions are C-commanded by predicates after NP-movement. The key of the well-known pruning algorithm raised in (Xue and Palmer, 2004) is extracting sisters of ancestors as role candidates. Those candidate nodes are all Ccommanders of a predicate. NP-movement can give an explanation why the algorithm works. 4.1.2 Definition of Argument Anchor To capture the characteristics of A-positions, we make definition of A-anchor as following. For every predicate p in the syntax tree T , denote A the set of C-commanders of p: • a left-A-anchor satisfies: 1. left-A-anchor belongs to A; 2. left-A-anchor is a noun phrase (including NNS, NNP, etc.) or simple declarative clause (S); 3. left-A-anchor is on the left hand of p. • a right-A-anc"
C08-1105,D07-1062,0,\N,Missing
C08-1125,J93-2003,0,0.0149626,"main monolingual corpora to improve the indomain performance. We propose an algorithm to combine these different resources in a unified framework. Experimental results indicate that our method achieves absolute improvements of 8.16 and 3.36 BLEU scores on Chinese to English translation and English to French translation respectively, as compared with the baselines using only out-ofdomain corpora. 1 Introduction In statistical machine translation (SMT), the translation process is modeled to obtain the translation e best of the source sentence f by maximizing the following posterior probability (Brown et al., 1993). e best = arg max e p (e |f ) = arg max e p (f |e ) pLM (e ) (1) State-of-the-art SMT systems are trained on large collections of bilingual corpora for the C 2008. Licensed under the Creative Commons Attri○ bution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. Chengqing Zong NLPR, Institute of Automation Chinese Academy of Sciences Beijing 100080, China cqzong@nlpr.ia.ac.cn translation model p (f |e ) and monolingual target language corpora for the language model (LM) pLM (e ) . The trained SMT systems are suitable for"
C08-1125,W07-0718,0,0.0184073,"Missing"
C08-1125,W07-0722,0,0.0694584,"adaptation has also been studied for SMT (Bulyko et al., 2007). They explored discriminative estimation of language model weights by directly optimizing machine translation performances such as BLEU score (Papineni et al., 2002). Their experiments indicated about 0.4 BLEU score improvement. A shared task is organized as part of the Second Workshop on Statistical Machine Translation. A part of this shared task focused on domain adaptation for machine translation among European languages. Several studies investigated mixture model adaptation for both translation model and language model in SMT (Civera and Juan, 2007; Foster and Kuhn, 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT. Their experiments indicate an absolute improvement of more than 1 BLEU score. To enlarge the in-domain bilingual corpus, Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from comparable corpora. Adding the extracted bilingual corpus to the training data improved the performance of the MT system. In addition, Ueffing et al. (2007) explored transductive learning for SMT, where source language corpora are used to train the models. They repeatedly translated s"
C08-1125,W07-0717,0,0.0622145,"n studied for SMT (Bulyko et al., 2007). They explored discriminative estimation of language model weights by directly optimizing machine translation performances such as BLEU score (Papineni et al., 2002). Their experiments indicated about 0.4 BLEU score improvement. A shared task is organized as part of the Second Workshop on Statistical Machine Translation. A part of this shared task focused on domain adaptation for machine translation among European languages. Several studies investigated mixture model adaptation for both translation model and language model in SMT (Civera and Juan, 2007; Foster and Kuhn, 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT. Their experiments indicate an absolute improvement of more than 1 BLEU score. To enlarge the in-domain bilingual corpus, Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from comparable corpora. Adding the extracted bilingual corpus to the training data improved the performance of the MT system. In addition, Ueffing et al. (2007) explored transductive learning for SMT, where source language corpora are used to train the models. They repeatedly translated source sentences from the"
C08-1125,W06-3114,0,0.015664,"Missing"
C08-1125,P07-2045,0,0.00675131,"rmance of the SMT system. This kind of transductive learning can be seen as a means to adapt the SMT system to a new type of texts. In this paper, we use an in-domain translation dictionary and/or in-domain monolingual corpora (in both source language and target language) to improve the performance of a SMT system trained on the out-of-domain corpora. Thus, our method uses these resources, instead of an indomain bilingual corpus, to adapt a baseline system trained on the out-of-domain corpora to indomain texts. 3 Baseline MT System The phrase-based SMT system used in our experiments is Moses (Koehn et al., 2007). In Moses, phrase translation probabilities, reordering probabilities, and language model probabilities are combined in the log-linear model to obtain the best translation e best of the source sentence f : e best = arg max e p (e |f ) ≈ arg max e M ∑ λm hm (e, f) (2) m =1 The weights are set by a discriminative training method using a held-out data set as described in (Och, 2003). The models or features which are employed by the decoder are (a) one or several phrases tables, (b) one or more language models trained with SRILM toolkit (Stolcke, 2002), (c) distance-based and lexicalized distorti"
C08-1125,W07-0733,0,0.126911,"o et al., 2007). They explored discriminative estimation of language model weights by directly optimizing machine translation performances such as BLEU score (Papineni et al., 2002). Their experiments indicated about 0.4 BLEU score improvement. A shared task is organized as part of the Second Workshop on Statistical Machine Translation. A part of this shared task focused on domain adaptation for machine translation among European languages. Several studies investigated mixture model adaptation for both translation model and language model in SMT (Civera and Juan, 2007; Foster and Kuhn, 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT. Their experiments indicate an absolute improvement of more than 1 BLEU score. To enlarge the in-domain bilingual corpus, Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from comparable corpora. Adding the extracted bilingual corpus to the training data improved the performance of the MT system. In addition, Ueffing et al. (2007) explored transductive learning for SMT, where source language corpora are used to train the models. They repeatedly translated source sentences from the development set and test se"
C08-1125,J05-4003,0,0.0482352,"s indicated about 0.4 BLEU score improvement. A shared task is organized as part of the Second Workshop on Statistical Machine Translation. A part of this shared task focused on domain adaptation for machine translation among European languages. Several studies investigated mixture model adaptation for both translation model and language model in SMT (Civera and Juan, 2007; Foster and Kuhn, 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT. Their experiments indicate an absolute improvement of more than 1 BLEU score. To enlarge the in-domain bilingual corpus, Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from comparable corpora. Adding the extracted bilingual corpus to the training data improved the performance of the MT system. In addition, Ueffing et al. (2007) explored transductive learning for SMT, where source language corpora are used to train the models. They repeatedly translated source sentences from the development set and test set. Then the generated translations are used to improve the performance of the SMT system. This kind of transductive learning can be seen as a means to adapt the SMT system to a new type of texts. In"
C08-1125,P03-1021,0,0.00513523,"instead of an indomain bilingual corpus, to adapt a baseline system trained on the out-of-domain corpora to indomain texts. 3 Baseline MT System The phrase-based SMT system used in our experiments is Moses (Koehn et al., 2007). In Moses, phrase translation probabilities, reordering probabilities, and language model probabilities are combined in the log-linear model to obtain the best translation e best of the source sentence f : e best = arg max e p (e |f ) ≈ arg max e M ∑ λm hm (e, f) (2) m =1 The weights are set by a discriminative training method using a held-out data set as described in (Och, 2003). The models or features which are employed by the decoder are (a) one or several phrases tables, (b) one or more language models trained with SRILM toolkit (Stolcke, 2002), (c) distance-based and lexicalized distortion models, (d) word penalty, (e) phrase penalty. 994 Input Out-of-domain training data LO In-domain translation dictionary DI In-domain target language corpus TI (optional) In-domain source language corpus S I (optional) Begin Assign translation probabilities to D I If TI is available Training step: π = Estimate ( LO , DI , TI ) , where π represents the general model. Else Trainin"
C08-1125,P02-1040,0,0.105146,"ing indomain dictionary and monolingual corpora. And then we present the experimental results in sections 5. In the last section, we conclude this paper. 2 Related Work Translation model and language model adaptation are usually used in domain adaptation for SMT. Language model adaptation has been widely used in speech recognition (Bacchiani and Roark, 2003). In recent years, language model adaptation has also been studied for SMT (Bulyko et al., 2007). They explored discriminative estimation of language model weights by directly optimizing machine translation performances such as BLEU score (Papineni et al., 2002). Their experiments indicated about 0.4 BLEU score improvement. A shared task is organized as part of the Second Workshop on Statistical Machine Translation. A part of this shared task focused on domain adaptation for machine translation among European languages. Several studies investigated mixture model adaptation for both translation model and language model in SMT (Civera and Juan, 2007; Foster and Kuhn, 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT. Their experiments indicate an absolute improvement of more than 1 BLEU score. To enlarge the in-domain"
C08-1125,P07-1004,0,0.228266,"y. Moreover, if an indomain source language corpus (SLC) is available, we automatically translate it and obtain a synthetic in-domain bilingual corpus. By adding this synthetic bilingual corpus to the training data, we rebuild the translation model to improve 993 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 993–1000 Manchester, August 2008 translation quality. We can repeatedly translate the in-domain source language corpus with the improved model until no more improvement can be made. This is similar to transductive learning described in (Ueffing et al., 2007). We perform domain adaptation experiments on two tasks: one is the Chinese to English translation, using the test set released by the International Workshop on Spoken Language Translation 2006 (IWSLT 2006), and the other is the English to French translation, using the data released by the Second Workshop on Statistical Machine Translation (WMT 2007) (CallisonBurch et al., 2007). Experimental results indicate that our method achieves absolute improvements of 8.16 and 3.36 BLEU scores on Chinese to English translation and English to French translation respectively, as compared with the baseline"
C08-1125,2007.mtsummit-papers.67,1,0.744835,"8.16 BLEU score (Model 7 vs. Model 1). Comparison of Different Dictionaries We compare the effects of different dictionaries with concern to the translation quality. Besides the manually-made in-domain dictionary, we use other two dictionaries: the LDC dictionary and an automatically built dictionary, which is extracted from the BTEC corpus. This extracted dictionary only contains Chinese words and their translations. The extraction method is as follows: • • Build a phrase table with the in-domain bilingual corpus. Filter those phrase pairs whose values are below a threshold as described in (Wu and Wang, 2007). • • From the filtered phrase table, extract the Chinese words and their translations. Assign constant translation probabilities to the entries of the extracted dictionary. Table 6 shows the translation results. All of the methods use the out-of-domain corpus, the in-domain target language corpus, and the corresponding translation dictionaries with constant translation probabilities. The results indicate that using the general-domain dictionary also improves translation quality, achieving an improvement of about 2 BLEU score as compared with Model 2 in Table 5. It can also be seen that the in"
C08-1125,H93-1040,0,\N,Missing
C08-1125,1993.mtsummit-1.24,0,\N,Missing
C10-1148,H05-1120,0,0.0317472,"rases. If a query q hits titles t1 and t2 , then t1 and t2 are likely to be paraphrases. 1. FOR any q ∈ Q and t ∈ T 2. IF q hits t 3. IF IsP araphrase(q, t) 4. Add ⟨q, t⟩ to Pqt 5. END IF 6. END IF 7. END FOR Table 1: Hypotheses for extracting paraphrases. expansion terms for new queries. Note that the expansion terms are merely related terms of the queries, not necessarily paraphrases. There are other studies that use query logs for constructing ontologies (Sekine and Suzuki, 2007), learning named entities (Pas¸ca, 2007), building user profiles (Richardson, 2008), correcting spelling errors (Ahmad and Kondrak, 2005), and so forth. 3 8. FOR any q1 , q2 ∈ Q and t ∈ T 9. IF ⟨q1 , t⟩ ∈ Pqt and ⟨q2 , t⟩ ∈ Pqt 10. IF IsP araphrase(q1 , q2 ) 11. Add ⟨q1 , q2 ⟩ to Pqq 12. END IF 13. END IF 14. END FOR The Proposed Method 15. FOR any t1 , t2 ∈ T and q ∈ Q 16. IF ⟨q, t1 ⟩ ∈ Pqt and ⟨q, t2 ⟩ ∈ Pqt 17. IF IsP araphrase(t1 , t2 ) 18. Add ⟨t1 , t2 ⟩ to Ptt 19. END IF 20. END IF 21. END FOR 3.1 Basic Idea Nowadays, more and more users tend to search long queries with search engines. Many users even directly search questions to get exact answers. By analyzing our query log that records rich information including user qu"
C10-1148,P05-1074,0,0.0613276,"and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al., 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypot"
C10-1148,N03-1003,0,0.0965788,"ection 5 concludes the paper and discusses future directions. 2 Related Work In this section, we briefly review previous studies on paraphrase extraction and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al., 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bh"
C10-1148,P01-1008,0,0.099256,"10 paraphrase Q-Q extraction paraphrase Q-T extraction query title both query and title paraphrase T-T extraction paraphrase relation Figure 1: Illustration of the proposed method. sults. Section 5 concludes the paper and discusses future directions. 2 Related Work In this section, we briefly review previous studies on paraphrase extraction and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al., 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained"
C10-1148,P08-1077,0,0.181494,"03; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypothesis, namely, if two phrases often occur in similar contexts, their meanings tend to be similar. The disadvantage of these methods is that the underlying assumption does not always hold. Phrases with opposite meanings can also occur in similar contexts, such as “X solves Y” and “X worsens Y” (Lin and Pantel, 2001). In addition, the extracted paraphrases are generally short fragments with two slots (variables) at both ends. 2.2 Query Log Mining in IR Query logs"
C10-1148,N06-1003,0,0.119041,"Missing"
C10-1148,D08-1021,0,0.0863245,"on retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al., 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypothesis, namely, if two"
C10-1148,C04-1051,0,0.0764975,"aper and discusses future directions. 2 Related Work In this section, we briefly review previous studies on paraphrase extraction and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al., 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bhagat and Ravichandran"
C10-1148,N06-2009,0,0.299558,"Missing"
C10-1148,W03-1608,0,0.0259217,"paraphrase Q-T extraction query title both query and title paraphrase T-T extraction paraphrase relation Figure 1: Illustration of the proposed method. sults. Section 5 concludes the paper and discusses future directions. 2 Related Work In this section, we briefly review previous studies on paraphrase extraction and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al., 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and d"
C10-1148,I05-1011,0,0.147229,"Missing"
C10-1148,P02-1006,0,0.426591,"ual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypothesis, namely, if two phrases often occur in similar contexts, their meanings tend to be similar. The disadvantage of these methods is that the underlying assumption does not always hold. Phrases with opposite meanings can also occur in similar contexts, such as “X solves Y” and “X worsens Y” (Lin and Pantel, 2001). In addition, the extracted paraphrases are generally short fragments with two slots (variables) at both ends. 2.2 Query Log Mining in IR Query logs are widely used in the IR community, especially for mining si"
C10-1148,P08-1089,1,0.86364,"Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al., 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypothesis, namely, if two phrases often occur"
C10-1148,N06-1058,0,0.111969,"Missing"
C10-1148,P07-1059,0,\N,Missing
C10-1149,P05-1074,0,0.367449,"provide paraphrase options in decoding. Hence one has to first extract fine-grained paraphrases from various corpora with different methods (Zhao et al., 2008a; Zhao et al., 2009), which is difficult and timeconsuming. In addition to the MT-based method, researchers have also investigated other methods for paraphrase generation, such as the pattern-based methods (Barzilay and Lee, 2003; Pang et al., 2003), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005). 2.2 Pivot Approach for Paraphrasing Bannard and Callison-Burch (2005) introduced the pivot approach to extracting paraphrase phrases from bilingual parallel corpora. Their basic assumption is that two English phrases aligned with the same phrase in a foreign language (also called a pivot language) are potential paraphrases. Zhao et al. (2008b) extended the approach and used it to extract paraphrase patterns. Both of the above works have proved the effectiveness of the pivot approach in paraphrase extraction. Pivot approach can also be used in paraphrase generation. It generates paraphrases by translating sentences from a source language to one (singlepivot) or"
C10-1149,W03-1601,0,0.180993,"Missing"
C10-1149,N03-1003,0,0.245626,"del so as to generate varied paraphrases for different applications. The main disadvantage of the MT-based method is that its performance heavily depends on the fine-grained paraphrases, such as paraphrase phrases and patterns, which provide paraphrase options in decoding. Hence one has to first extract fine-grained paraphrases from various corpora with different methods (Zhao et al., 2008a; Zhao et al., 2009), which is difficult and timeconsuming. In addition to the MT-based method, researchers have also investigated other methods for paraphrase generation, such as the pattern-based methods (Barzilay and Lee, 2003; Pang et al., 2003), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005). 2.2 Pivot Approach for Paraphrasing Bannard and Callison-Burch (2005) introduced the pivot approach to extracting paraphrase phrases from bilingual parallel corpora. Their basic assumption is that two English phrases aligned with the same phrase in a foreign language (also called a pivot language) are potential paraphrases. Zhao et al. (2008b) extended the approach and used it to extract paraphrase patterns. Both of the"
C10-1149,W09-2503,0,0.114168,"hrases. Zhao et al. (2008b) extended the approach and used it to extract paraphrase patterns. Both of the above works have proved the effectiveness of the pivot approach in paraphrase extraction. Pivot approach can also be used in paraphrase generation. It generates paraphrases by translating sentences from a source language to one (singlepivot) or more (multi-pivot) pivot languages and then translating them back to the source language. Duboue et al. (2006) first proposed the multipivot approach for paraphrase generation, which was specially designed for question expansion in QA. In addition, Max (2009) presented a singlepivot approach for generating sub-sentential paraphrases. A clear difference between our method and the above works is that we propose selectionbased and decoding-based techniques to generate high-quality paraphrases using the candidates yielded from the pivot approach. 3 Multi-pivot Approach for Acquiring Candidate Paraphrases A single-pivot PG approach paraphrases a sentence S by translating it into a pivot language P L with a MT engine M T1 and then translating it back into the source language with M T2 . In this paper, a single-pivot PG system is represented as a triple"
C10-1149,P00-1056,0,0.0233439,"paraphrase from the 54 candidates for each source sentence S. 4.2 Decoding-based Technique The selection-based technique introduced above has an inherent limitation that it can only select a paraphrase from the candidates. That is to say, it can never produce a perfect paraphrase if all the candidates have some tiny flaws. To solve this problem, we propose the decoding-based technique, which trains a MT model using the candidate paraphrases of each source sentence S and generates a new paraphrase T for S with a MT decoder. In this work, we implement the decoding-based technique using Giza++ (Och and Ney, 2000) and Moses (Hoang and Koehn, 2008), both of which are commonly used SMT tools. For a sentence S, we first construct a set of parallel sentences by pairing S with each of its candidate paraphrases: {(S,T1 ),(S,T2 ),...,(S,TN )} (N = 54). We then run word alignment on the set using Giza++ and extract aligned phrase pairs as described in (Koehn, 2004). Here we only keep the phrase pairs that are aligned ≥3 times on the set, so as to filter errors brought by the noisy sentence pairs. The extracted phrase pairs are stored in a phrase table. Table 4 shows some extracted phrase pairs. Note that Giza+"
C10-1149,N03-1024,0,0.0905158,"ried paraphrases for different applications. The main disadvantage of the MT-based method is that its performance heavily depends on the fine-grained paraphrases, such as paraphrase phrases and patterns, which provide paraphrase options in decoding. Hence one has to first extract fine-grained paraphrases from various corpora with different methods (Zhao et al., 2008a; Zhao et al., 2009), which is difficult and timeconsuming. In addition to the MT-based method, researchers have also investigated other methods for paraphrase generation, such as the pattern-based methods (Barzilay and Lee, 2003; Pang et al., 2003), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005). 2.2 Pivot Approach for Paraphrasing Bannard and Callison-Burch (2005) introduced the pivot approach to extracting paraphrase phrases from bilingual parallel corpora. Their basic assumption is that two English phrases aligned with the same phrase in a foreign language (also called a pivot language) are potential paraphrases. Zhao et al. (2008b) extended the approach and used it to extract paraphrase patterns. Both of the above works have pro"
C10-1149,H93-1040,0,0.0370119,"Missing"
C10-1149,N06-1003,0,0.0513578,"oth the selectionbased and decoding-based techniques can make good use of the candidates and produce high-quality paraphrases. Moreover, these two techniques are complementary. (3) The proposed method outperforms a state-of-the-art paraphrase generation approach. 1 Introduction This paper addresses the problem of paraphrase generation (PG), which seeks to generate paraphrases for sentences. PG is important in many natural language processing (NLP) applications. For example, in machine translation (MT), a sentence can be paraphrased so as to make it more translatable (Zhang and Yamamoto, 2002; Callison-Burch et al., 2006). In question answering (QA), a question can be paraphrased to improve the coverage of answer extraction (Duboue and Chu-Carroll, 2006; Riezler et al., 2007). In natural language generation (NLG), paraphrasing can help to increase the expressive power of the NLG systems (Iordanskaja et al., 1991). In this paper, we propose a novel PG method. For an English sentence S, the method first acquires a set of candidate paraphrases with a multipivot approach, which uses MT engines to automatically translate S into multiple pivot languages and then translate them back into English. Furthermore, the met"
C10-1149,N06-2009,0,0.132255,"r, these two techniques are complementary. (3) The proposed method outperforms a state-of-the-art paraphrase generation approach. 1 Introduction This paper addresses the problem of paraphrase generation (PG), which seeks to generate paraphrases for sentences. PG is important in many natural language processing (NLP) applications. For example, in machine translation (MT), a sentence can be paraphrased so as to make it more translatable (Zhang and Yamamoto, 2002; Callison-Burch et al., 2006). In question answering (QA), a question can be paraphrased to improve the coverage of answer extraction (Duboue and Chu-Carroll, 2006; Riezler et al., 2007). In natural language generation (NLG), paraphrasing can help to increase the expressive power of the NLG systems (Iordanskaja et al., 1991). In this paper, we propose a novel PG method. For an English sentence S, the method first acquires a set of candidate paraphrases with a multipivot approach, which uses MT engines to automatically translate S into multiple pivot languages and then translate them back into English. Furthermore, the method employs two kinds of techniques to produce a best paraphrase T for S using the candidates, i.e., the selection-based and decoding-"
C10-1149,W08-0510,0,0.015504,"ates for each source sentence S. 4.2 Decoding-based Technique The selection-based technique introduced above has an inherent limitation that it can only select a paraphrase from the candidates. That is to say, it can never produce a perfect paraphrase if all the candidates have some tiny flaws. To solve this problem, we propose the decoding-based technique, which trains a MT model using the candidate paraphrases of each source sentence S and generates a new paraphrase T for S with a MT decoder. In this work, we implement the decoding-based technique using Giza++ (Och and Ney, 2000) and Moses (Hoang and Koehn, 2008), both of which are commonly used SMT tools. For a sentence S, we first construct a set of parallel sentences by pairing S with each of its candidate paraphrases: {(S,T1 ),(S,T2 ),...,(S,TN )} (N = 54). We then run word alignment on the set using Giza++ and extract aligned phrase pairs as described in (Koehn, 2004). Here we only keep the phrase pairs that are aligned ≥3 times on the set, so as to filter errors brought by the noisy sentence pairs. The extracted phrase pairs are stored in a phrase table. Table 4 shows some extracted phrase pairs. Note that Giza++ is sensitive to the data size. H"
C10-1149,N06-1058,0,0.0601524,"MT-based method is that its performance heavily depends on the fine-grained paraphrases, such as paraphrase phrases and patterns, which provide paraphrase options in decoding. Hence one has to first extract fine-grained paraphrases from various corpora with different methods (Zhao et al., 2008a; Zhao et al., 2009), which is difficult and timeconsuming. In addition to the MT-based method, researchers have also investigated other methods for paraphrase generation, such as the pattern-based methods (Barzilay and Lee, 2003; Pang et al., 2003), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005). 2.2 Pivot Approach for Paraphrasing Bannard and Callison-Burch (2005) introduced the pivot approach to extracting paraphrase phrases from bilingual parallel corpora. Their basic assumption is that two English phrases aligned with the same phrase in a foreign language (also called a pivot language) are potential paraphrases. Zhao et al. (2008b) extended the approach and used it to extract paraphrase patterns. Both of the above works have proved the effectiveness of the pivot approach in paraphrase extraction. Pivot approac"
C10-1149,koen-2004-pharaoh,0,0.102502,"ose the decoding-based technique, which trains a MT model using the candidate paraphrases of each source sentence S and generates a new paraphrase T for S with a MT decoder. In this work, we implement the decoding-based technique using Giza++ (Och and Ney, 2000) and Moses (Hoang and Koehn, 2008), both of which are commonly used SMT tools. For a sentence S, we first construct a set of parallel sentences by pairing S with each of its candidate paraphrases: {(S,T1 ),(S,T2 ),...,(S,TN )} (N = 54). We then run word alignment on the set using Giza++ and extract aligned phrase pairs as described in (Koehn, 2004). Here we only keep the phrase pairs that are aligned ≥3 times on the set, so as to filter errors brought by the noisy sentence pairs. The extracted phrase pairs are stored in a phrase table. Table 4 shows some extracted phrase pairs. Note that Giza++ is sensitive to the data size. Hence it is interesting to examine if the alignment can be improved by augmenting the parallel sentence pairs. To this end, we have tried augmenting the parallel set for each sentence S by pairing any 2 two candidate paraphrases. In this manner, CN sentence pairs are augmented for each S. We con2 ) sentence duct wor"
C10-1149,P02-1040,0,0.105062,"Missing"
C10-1149,I05-5010,0,0.113084,"paraphrases, such as paraphrase phrases and patterns, which provide paraphrase options in decoding. Hence one has to first extract fine-grained paraphrases from various corpora with different methods (Zhao et al., 2008a; Zhao et al., 2009), which is difficult and timeconsuming. In addition to the MT-based method, researchers have also investigated other methods for paraphrase generation, such as the pattern-based methods (Barzilay and Lee, 2003; Pang et al., 2003), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005). 2.2 Pivot Approach for Paraphrasing Bannard and Callison-Burch (2005) introduced the pivot approach to extracting paraphrase phrases from bilingual parallel corpora. Their basic assumption is that two English phrases aligned with the same phrase in a foreign language (also called a pivot language) are potential paraphrases. Zhao et al. (2008b) extended the approach and used it to extract paraphrase patterns. Both of the above works have proved the effectiveness of the pivot approach in paraphrase extraction. Pivot approach can also be used in paraphrase generation. It generates paraphrases b"
C10-1149,W04-3219,0,0.877056,"based technique is promising, which can generate paraphrases that are different from the candidates; (4) both the selection-based and decoding-based techniques outperform a state-of-the-art approach SPG (Zhao et al., 2009). 2 2.1 Related Work Methods for Paraphrase Generation MT-based method is the mainstream method on PG. It regards PG as a monolingual machine translation problem, i.e., “translating” a sentence S into another sentence T in the same language. 1326 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1326–1334, Beijing, August 2010 Quirk et al. (2004) first presented the MT-based method. They trained a statistical MT (SMT) model on a monolingual parallel corpus extracted from comparable news articles and applied the model to generate paraphrases. Their work shows that SMT techniques can be extended to PG. However, its usefulness is limited by the scarcity of monolingual parallel data. To overcome the data sparseness problem, Zhao et al. (2008a) improved the MT-based PG method by training the paraphrase model using multiple resources, including monolingual parallel corpora, monolingual comparable corpora, bilingual parallel corpora, etc. Th"
C10-1149,C02-1056,0,0.127176,"ndidate paraphrases. (2) Both the selectionbased and decoding-based techniques can make good use of the candidates and produce high-quality paraphrases. Moreover, these two techniques are complementary. (3) The proposed method outperforms a state-of-the-art paraphrase generation approach. 1 Introduction This paper addresses the problem of paraphrase generation (PG), which seeks to generate paraphrases for sentences. PG is important in many natural language processing (NLP) applications. For example, in machine translation (MT), a sentence can be paraphrased so as to make it more translatable (Zhang and Yamamoto, 2002; Callison-Burch et al., 2006). In question answering (QA), a question can be paraphrased to improve the coverage of answer extraction (Duboue and Chu-Carroll, 2006; Riezler et al., 2007). In natural language generation (NLG), paraphrasing can help to increase the expressive power of the NLG systems (Iordanskaja et al., 1991). In this paper, we propose a novel PG method. For an English sentence S, the method first acquires a set of candidate paraphrases with a multipivot approach, which uses MT engines to automatically translate S into multiple pivot languages and then translate them back into"
C10-1149,P09-1094,1,0.816899,"extracted from comparable news articles and applied the model to generate paraphrases. Their work shows that SMT techniques can be extended to PG. However, its usefulness is limited by the scarcity of monolingual parallel data. To overcome the data sparseness problem, Zhao et al. (2008a) improved the MT-based PG method by training the paraphrase model using multiple resources, including monolingual parallel corpora, monolingual comparable corpora, bilingual parallel corpora, etc. Their results show that bilingual parallel corpora are the most useful among the exploited resources. Zhao et al. (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. The main disadvantage of the MT-based method is that its performance heavily depends on the fine-grained paraphrases, such as paraphrase phrases and patterns, which provide paraphrase options in decoding. Hence one has to first extract fine-grained paraphrases from various corpora with different methods (Zhao et al., 2008a; Zhao et al., 2009), which is difficult and timeconsuming. In addition to the MT-based method, researchers have also i"
C10-1149,P08-1116,1,0.903826,"ng” a sentence S into another sentence T in the same language. 1326 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1326–1334, Beijing, August 2010 Quirk et al. (2004) first presented the MT-based method. They trained a statistical MT (SMT) model on a monolingual parallel corpus extracted from comparable news articles and applied the model to generate paraphrases. Their work shows that SMT techniques can be extended to PG. However, its usefulness is limited by the scarcity of monolingual parallel data. To overcome the data sparseness problem, Zhao et al. (2008a) improved the MT-based PG method by training the paraphrase model using multiple resources, including monolingual parallel corpora, monolingual comparable corpora, bilingual parallel corpora, etc. Their results show that bilingual parallel corpora are the most useful among the exploited resources. Zhao et al. (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. The main disadvantage of the MT-based method is that its performance heavily depends on the fine-grained paraphrases, such a"
C10-1149,P08-1089,1,0.875364,"ng” a sentence S into another sentence T in the same language. 1326 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1326–1334, Beijing, August 2010 Quirk et al. (2004) first presented the MT-based method. They trained a statistical MT (SMT) model on a monolingual parallel corpus extracted from comparable news articles and applied the model to generate paraphrases. Their work shows that SMT techniques can be extended to PG. However, its usefulness is limited by the scarcity of monolingual parallel data. To overcome the data sparseness problem, Zhao et al. (2008a) improved the MT-based PG method by training the paraphrase model using multiple resources, including monolingual parallel corpora, monolingual comparable corpora, bilingual parallel corpora, etc. Their results show that bilingual parallel corpora are the most useful among the exploited resources. Zhao et al. (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. The main disadvantage of the MT-based method is that its performance heavily depends on the fine-grained paraphrases, such a"
C10-1149,1993.mtsummit-1.24,0,\N,Missing
C10-1149,P07-1059,0,\N,Missing
C10-1149,W07-0718,0,\N,Missing
C10-1149,N03-1017,0,\N,Missing
C10-4001,W05-1205,0,0.0284196,"s , s ) and vec2(s , s ): 1 1 1 2 1 2 2 2 1 2 1 – Classifier • SVM classifier 9 2 1 2 Classification based Methods (cont’) • Malakasiotis, 2009 – Combining multiple classification features • String similarity (various levels) – Tokens, stems, POS tags, nouns only, verbs only, … • Different measures – Edit distance, Jaro-Winkler distance, Manhattan distance… • Synonym similarity – Treat synonyms in two sentences as identical words • Syntax similarity – Dependency parsing of two sentences and compute the overlap of dependencies – Classifier • Maximum Entropy classifier Alignment based Methods • Wu, 2005 – Conduct alignment based on Inversion Transduction Grammars (ITG) • Sensitive to the differences in sentence structures • Without using any thesaurus to deal with lexical variation – Performance is comparable to the classification based methods – Also Al performs f wellll iin recognizing i i ttextual t l entailment t il t 10 Alignment based Methods (cont’) • Das and Smith, 2009 – Conduct alignment based on Quasi Quasi-Synchronous Synchronous Dependency Grammar (QG) • Alignment between two dependency trees • Assumption: the dependency trees of two paraphrase sentences should be aligned closel"
C10-4001,P09-3004,0,0.0246145,"on pairs vendors |suppliers – Automatically learned synonym pairs – Classifier • SVM classifier Classification based Methods (cont’) • Finch et al., 2005 – Using MT evaluation techniques to compute sentence similarities, which are then used as classification features • WER, PER, BLEU, NIST • Feature vector vec(s , s ) 1 2 – vec1(s , s ): s as reference, s as MT system output; – vec2(s ( , s )): s as reference,, s as MT system y output; p ; – vec(s , s ): average of vec1(s , s ) and vec2(s , s ): 1 1 1 2 1 2 2 2 1 2 1 – Classifier • SVM classifier 9 2 1 2 Classification based Methods (cont’) • Malakasiotis, 2009 – Combining multiple classification features • String similarity (various levels) – Tokens, stems, POS tags, nouns only, verbs only, … • Different measures – Edit distance, Jaro-Winkler distance, Manhattan distance… • Synonym similarity – Treat synonyms in two sentences as identical words • Syntax similarity – Dependency parsing of two sentences and compute the overlap of dependencies – Classifier • Maximum Entropy classifier Alignment based Methods • Wu, 2005 – Conduct alignment based on Inversion Transduction Grammars (ITG) • Sensitive to the differences in sentence structures • Without usi"
C10-4001,N04-1031,0,\N,Missing
C10-4001,I05-1079,0,\N,Missing
C10-4001,2007.tmi-papers.28,0,\N,Missing
C10-4001,W04-3219,0,\N,Missing
C10-4001,C02-1161,0,\N,Missing
C10-4001,W03-1601,0,\N,Missing
C10-4001,C02-2008,0,\N,Missing
C10-4001,S07-1050,0,\N,Missing
C10-4001,W09-2503,0,\N,Missing
C10-4001,C08-1107,0,\N,Missing
C10-4001,S07-1029,0,\N,Missing
C10-4001,C02-1163,0,\N,Missing
C10-4001,S07-1044,0,\N,Missing
C10-4001,C02-1056,0,\N,Missing
C10-4001,S07-1091,0,\N,Missing
C10-4001,W05-0207,0,\N,Missing
C10-4001,N06-2009,0,\N,Missing
C10-4001,D09-1040,0,\N,Missing
C10-4001,P09-1053,0,\N,Missing
C10-4001,P09-1094,1,\N,Missing
C10-4001,D08-1021,0,\N,Missing
C10-4001,N06-1057,0,\N,Missing
C10-4001,W06-1610,0,\N,Missing
C10-4001,P02-1028,0,\N,Missing
C10-4001,C10-1148,1,\N,Missing
C10-4001,C10-1149,1,\N,Missing
C10-4001,P08-1077,0,\N,Missing
C10-4001,N03-1003,0,\N,Missing
C10-4001,N06-1058,0,\N,Missing
C10-4001,P09-1089,0,\N,Missing
C10-4001,I05-5010,0,\N,Missing
C10-4001,P10-2001,0,\N,Missing
C10-4001,P08-1089,1,\N,Missing
C10-4001,N06-1003,0,\N,Missing
C10-4001,2008.iwslt-papers.2,0,\N,Missing
C10-4001,P79-1016,0,\N,Missing
C10-4001,P11-1020,0,\N,Missing
C10-4001,P07-1058,0,\N,Missing
C10-4001,P98-2127,0,\N,Missing
C10-4001,C98-2122,0,\N,Missing
C10-4001,C08-1013,0,\N,Missing
C10-4001,P08-1116,1,\N,Missing
C10-4001,P02-1006,0,\N,Missing
C10-4001,E99-1042,0,\N,Missing
C10-4001,shimohata-etal-2004-building,0,\N,Missing
C10-4001,takao-etal-2002-comparing,0,\N,Missing
C10-4001,N10-1017,0,\N,Missing
C10-4001,I05-5003,0,\N,Missing
C10-4001,W07-0716,0,\N,Missing
C10-4001,I05-5001,0,\N,Missing
C12-1192,P05-1074,0,0.0235464,"In (Barzilay and Lee, 2003; Dolan et al., 2004), researchers collected comparable news articles reporting on the same event, and further extracted parallel sentences for learning paraphrase phrases and patterns. There are also studies focusing on extracting paraphrases from large-scale monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). The basic idea is that phrases or patterns appearing in similar contexts tend to have the same meaning. Besides monolingual corpora, bilingual corpora have also been exploited for paraphrase extraction. Bannard and Callison-Burch (2005) first presented the method to learn paraphrase phrases from a bilingual phrase table. The key idea is that phrases aligned with the same foreign phrase could be paraphrases. Callison-Burch (2008) then improved the method by imposing syntax constraints to filter paraphrases with different syntactic structures. In addition, Zhao et al. (2008) extended this method to paraphrase pattern extraction. To our knowledge, few studies have been conducted on learning paraphrases from query logs. Zhao et al. (2010)’s study might be the closest to our work. Their method is motivated by the assumption that"
C12-1192,N03-1003,0,0.0448077,"n introduce the pattern pair induction method in Section 3. The paraphrase pattern recognition method is proposed in Section 4, in which the user behavior features are described in detail. We present the experiment results in Section 5 and conclude the paper in Section 6. 2 2.1 Related Work Paraphrase Learning Plenty of methods have been proposed to extract paraphrases from various data sources. In (Barzilay and McKeown, 2001), the authors viewed multiple translation versions of the same literary works as monolingual parallel corpora and extracted paraphrases with a co-training algorithm. In (Barzilay and Lee, 2003; Dolan et al., 2004), researchers collected comparable news articles reporting on the same event, and further extracted parallel sentences for learning paraphrase phrases and patterns. There are also studies focusing on extracting paraphrases from large-scale monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). The basic idea is that phrases or patterns appearing in similar contexts tend to have the same meaning. Besides monolingual corpora, bilingual corpora have also been exploited for paraphrase extraction. Bannard and Callison-Burch"
C12-1192,P01-1008,0,0.0568895,"aphrases from query-click pairs of query logs, which inspires us to integrate them in our future research. 1 www.baidu.com 3138 In what follows, we first review related studies in Section 2. We then introduce the pattern pair induction method in Section 3. The paraphrase pattern recognition method is proposed in Section 4, in which the user behavior features are described in detail. We present the experiment results in Section 5 and conclude the paper in Section 6. 2 2.1 Related Work Paraphrase Learning Plenty of methods have been proposed to extract paraphrases from various data sources. In (Barzilay and McKeown, 2001), the authors viewed multiple translation versions of the same literary works as monolingual parallel corpora and extracted paraphrases with a co-training algorithm. In (Barzilay and Lee, 2003; Dolan et al., 2004), researchers collected comparable news articles reporting on the same event, and further extracted parallel sentences for learning paraphrase phrases and patterns. There are also studies focusing on extracting paraphrases from large-scale monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). The basic idea is that phrases or pat"
C12-1192,P08-1077,0,0.0171147,"araphrases from various data sources. In (Barzilay and McKeown, 2001), the authors viewed multiple translation versions of the same literary works as monolingual parallel corpora and extracted paraphrases with a co-training algorithm. In (Barzilay and Lee, 2003; Dolan et al., 2004), researchers collected comparable news articles reporting on the same event, and further extracted parallel sentences for learning paraphrase phrases and patterns. There are also studies focusing on extracting paraphrases from large-scale monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). The basic idea is that phrases or patterns appearing in similar contexts tend to have the same meaning. Besides monolingual corpora, bilingual corpora have also been exploited for paraphrase extraction. Bannard and Callison-Burch (2005) first presented the method to learn paraphrase phrases from a bilingual phrase table. The key idea is that phrases aligned with the same foreign phrase could be paraphrases. Callison-Burch (2008) then improved the method by imposing syntax constraints to filter paraphrases with different syntactic structures. In addition, Zhao et al. (2008) extended this meth"
C12-1192,I05-5001,0,0.0241781,"rn pairs induced from all sessions in the search log and sum up the frequencies for each pair. Pattern pairs satisfying the following requirement are retained: (1) the frequency of the pattern pair exceeds a threshold T1 , (2) the number of unique fillers for each slot exceeds a threshold T2 . 3140 qi 大象 的 重量 weight of an elephant qj 大象 有 多重 How much does an elephant weigh pi [n-1] 的 重量 weight of an [n-1] pj [n-1] 有 多重 How much does an [n-1] weigh slot filler: 大象(elephant) slot: [n-1] Figure 1: Example of pattern pair induction. 4 Paraphrase Pattern Recognition Following the previous studies (Brockett and Dolan, 2005; Finch et al., 2005; Malakasiotis, 2009), we recast paraphrase pattern recognition as a classification problem. Each induced pattern pair is classified into one of the two classes, i.e., paraphrase and non-paraphrase. A Support Vector Machines (SVM) classifier is used in our experiments, since it has proven effective in this task (Brockett and Dolan, 2005; Finch et al., 2005). Our classification features can be divided into two groups: the baseline features examined in previous studies (Section 4.1) and user behavior based features proposed in this work (Section 4.2). 4.1 Baseline Features (F"
C12-1192,D08-1021,0,0.0174319,"ns. There are also studies focusing on extracting paraphrases from large-scale monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). The basic idea is that phrases or patterns appearing in similar contexts tend to have the same meaning. Besides monolingual corpora, bilingual corpora have also been exploited for paraphrase extraction. Bannard and Callison-Burch (2005) first presented the method to learn paraphrase phrases from a bilingual phrase table. The key idea is that phrases aligned with the same foreign phrase could be paraphrases. Callison-Burch (2008) then improved the method by imposing syntax constraints to filter paraphrases with different syntactic structures. In addition, Zhao et al. (2008) extended this method to paraphrase pattern extraction. To our knowledge, few studies have been conducted on learning paraphrases from query logs. Zhao et al. (2010)’s study might be the closest to our work. Their method is motivated by the assumption that user queries and the clicked titles are potential paraphrases. Accordingly, they train a classifier to recognize paraphrases from query-title pairs. They further extract query-query and title-titl"
C12-1192,N06-1003,0,0.0700054,"Missing"
C12-1192,C04-1051,0,0.0382211,"pair induction method in Section 3. The paraphrase pattern recognition method is proposed in Section 4, in which the user behavior features are described in detail. We present the experiment results in Section 5 and conclude the paper in Section 6. 2 2.1 Related Work Paraphrase Learning Plenty of methods have been proposed to extract paraphrases from various data sources. In (Barzilay and McKeown, 2001), the authors viewed multiple translation versions of the same literary works as monolingual parallel corpora and extracted paraphrases with a co-training algorithm. In (Barzilay and Lee, 2003; Dolan et al., 2004), researchers collected comparable news articles reporting on the same event, and further extracted parallel sentences for learning paraphrase phrases and patterns. There are also studies focusing on extracting paraphrases from large-scale monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). The basic idea is that phrases or patterns appearing in similar contexts tend to have the same meaning. Besides monolingual corpora, bilingual corpora have also been exploited for paraphrase extraction. Bannard and Callison-Burch (2005) first present"
C12-1192,N06-2009,0,0.0675003,"Missing"
C12-1192,I05-5003,0,0.0357257,"sessions in the search log and sum up the frequencies for each pair. Pattern pairs satisfying the following requirement are retained: (1) the frequency of the pattern pair exceeds a threshold T1 , (2) the number of unique fillers for each slot exceeds a threshold T2 . 3140 qi 大象 的 重量 weight of an elephant qj 大象 有 多重 How much does an elephant weigh pi [n-1] 的 重量 weight of an [n-1] pj [n-1] 有 多重 How much does an [n-1] weigh slot filler: 大象(elephant) slot: [n-1] Figure 1: Example of pattern pair induction. 4 Paraphrase Pattern Recognition Following the previous studies (Brockett and Dolan, 2005; Finch et al., 2005; Malakasiotis, 2009), we recast paraphrase pattern recognition as a classification problem. Each induced pattern pair is classified into one of the two classes, i.e., paraphrase and non-paraphrase. A Support Vector Machines (SVM) classifier is used in our experiments, since it has proven effective in this task (Brockett and Dolan, 2005; Finch et al., 2005). Our classification features can be divided into two groups: the baseline features examined in previous studies (Section 4.1) and user behavior based features proposed in this work (Section 4.2). 4.1 Baseline Features (FBL ) Conventional fe"
C12-1192,N03-1017,0,0.01133,"entage of paraphrases decreases as the rank gets lower. We therefore design the frequency based feature as: F f r (p1 , p2 ) = P f r eq(p1 , p2 ) p f r eq(p1 , p) + C1 , (1) where f r eq(p1 , p2 ) is the frequency of 〈p1 , p2 〉 on the whole set of pattern pairs, C1 is a constant parameter used to avoid overestimating the feature value when p1 is too infrequent. We also compute the frequency based feature in the other direction, i.e., F f r (p2 , p1 ) in the same way. Lexical Score Features (Fl s ). Inspired by lexical weight features used to measure phrase pair quality in machine translation (Koehn et al., 2003), we introduce lexical score features to measure the lexical level paraphrase likelihood of each pattern pair. We design a lexical scoring approach based on the observation that many words keep unchanged when users rewrite their queries within sessions. It is reasonable to assume that those unchanged words across queries should exclusively align with themselves, while the changed words may likely form paraphrase word pairs. Accordingly, given a pair of related queries q1 = w11 ...w1m and q2 = w21 ...w2n extracted from the same session, we compute two scores for any word pair 〈w1i , w2 j 〉 (1 ≤"
C12-1192,W07-0716,0,0.0460809,"Missing"
C12-1192,P09-3004,0,0.0141166,"ch log and sum up the frequencies for each pair. Pattern pairs satisfying the following requirement are retained: (1) the frequency of the pattern pair exceeds a threshold T1 , (2) the number of unique fillers for each slot exceeds a threshold T2 . 3140 qi 大象 的 重量 weight of an elephant qj 大象 有 多重 How much does an elephant weigh pi [n-1] 的 重量 weight of an [n-1] pj [n-1] 有 多重 How much does an [n-1] weigh slot filler: 大象(elephant) slot: [n-1] Figure 1: Example of pattern pair induction. 4 Paraphrase Pattern Recognition Following the previous studies (Brockett and Dolan, 2005; Finch et al., 2005; Malakasiotis, 2009), we recast paraphrase pattern recognition as a classification problem. Each induced pattern pair is classified into one of the two classes, i.e., paraphrase and non-paraphrase. A Support Vector Machines (SVM) classifier is used in our experiments, since it has proven effective in this task (Brockett and Dolan, 2005; Finch et al., 2005). Our classification features can be divided into two groups: the baseline features examined in previous studies (Section 4.1) and user behavior based features proposed in this work (Section 4.2). 4.1 Baseline Features (FBL ) Conventional features for paraphrase"
C12-1192,D09-1040,0,0.0272263,"Missing"
C12-1192,P02-1006,0,0.216653,"Missing"
C12-1192,C08-1093,0,0.0241134,"ogs and applying them in query rewriting and recommendation. Such research can be classified into three groups. The first group of methods utilizes user clicks when computing query similarity. The underlying assumption is that if users tend to click on similar documents for two queries, then the meanings of the queries should be similar (Wen et al., 2002; Baeza-Yates and Tiberi, 2007). In the second group of methods, researchers mine query rewriting terms directly from user clicked documents. Their basic idea is that terms from queries and user clicked documents are related (Cui et al., 2002; Riezler et al., 2008). The third group of methods learns related queries from query sessions. The assumption is that queries submitted by the same user within a short time might be related in meaning (Fonseca et al., 2005; Jones et al., 2006; Zhang and Nasraoui, 2006; Szpektor et al., 2011). Our work is close to the third group. However, what we learn are 3139 paraphrase patterns rather than related queries or patterns. 3 3.1 Pattern Pair Induction Concepts Query. In this work, we collect user queries and other useful information from the used search logs and represent a query q as a triplet: q = 〈qc, qt, cn〉, whe"
C12-1192,P07-1059,0,0.0582929,"Missing"
C12-1192,P07-1058,0,0.0202016,"differences, such as inserting or deleting a stop word. The libsvm toolkit4 was used as the classifier, with its default parameter settings. Some other parameters used in our method were set empirically: T1 = 5, T2 = 3, C1 = 20, C2 = 10. 5.1 Evaluation of the Classifier We randomly sampled 5115 candidate pattern pairs to form the experimental data set. Two Chinese native speakers were asked to annotate the pattern pairs separately. A pattern pair should be annotated as positive (correct paraphrase patterns) or negative (otherwise). We follow the instance-based evaluation approach proposed by Szpektor et al. (2007). Particularly, we provide pattern slot fillers to the annotators along with the pattern pairs. A pattern pair is judged as paraphrase only when most of the instances generated by filling the slots with the provided fillers are paraphrases. We calculated the annotation agreement between two annotators. The result shows that the observed agreement is 0.96 and the Kappa value is 0.90. We believe that the high annotation agreement is due to the careful training of the annotators and the instance-based evaluation approach. A third annotator was asked to decide the final annotation for the disagree"
C12-1192,C10-1148,1,0.720372,"rpora, bilingual corpora have also been exploited for paraphrase extraction. Bannard and Callison-Burch (2005) first presented the method to learn paraphrase phrases from a bilingual phrase table. The key idea is that phrases aligned with the same foreign phrase could be paraphrases. Callison-Burch (2008) then improved the method by imposing syntax constraints to filter paraphrases with different syntactic structures. In addition, Zhao et al. (2008) extended this method to paraphrase pattern extraction. To our knowledge, few studies have been conducted on learning paraphrases from query logs. Zhao et al. (2010)’s study might be the closest to our work. Their method is motivated by the assumption that user queries and the clicked titles are potential paraphrases. Accordingly, they train a classifier to recognize paraphrases from query-title pairs. They further extract query-query and title-title paraphrases from the query-title paraphrases based on the assumption that queries clicking on the same title and titles clicked on for the same query are also likely to be paraphrases. Additionally, they induce paraphrase patterns from the mined paraphrases. Our work differs from Zhao et al.’s mainly in that"
C12-1192,P08-1089,1,0.844859,"2001; Bhagat and Ravichandran, 2008). The basic idea is that phrases or patterns appearing in similar contexts tend to have the same meaning. Besides monolingual corpora, bilingual corpora have also been exploited for paraphrase extraction. Bannard and Callison-Burch (2005) first presented the method to learn paraphrase phrases from a bilingual phrase table. The key idea is that phrases aligned with the same foreign phrase could be paraphrases. Callison-Burch (2008) then improved the method by imposing syntax constraints to filter paraphrases with different syntactic structures. In addition, Zhao et al. (2008) extended this method to paraphrase pattern extraction. To our knowledge, few studies have been conducted on learning paraphrases from query logs. Zhao et al. (2010)’s study might be the closest to our work. Their method is motivated by the assumption that user queries and the clicked titles are potential paraphrases. Accordingly, they train a classifier to recognize paraphrases from query-title pairs. They further extract query-query and title-title paraphrases from the query-title paraphrases based on the assumption that queries clicking on the same title and titles clicked on for the same q"
C12-1192,C02-1161,0,0.0978369,"Missing"
C14-1048,apidianaki-2008-translation,0,0.144502,"(we |wc ) exceed some threshold 0 < δ < 1. The second column of Table 1 presents the extraction results on a sample of source language words with the corresponding translation words. 3.2 Clustering of Translation Words For each source language word, its translation words are then clustered so as to separate different senses. At the clustering time, we first represent each translation word with a feature vector (point), so that we can measure the similarities between points. Then we perform clustering on these feature vectors, representing different senses in different clusters. Different from Apidianaki (2008) who represents all occurrences of the translation words with their contexts in the foreign language for clustering, we adopt the embeddings of the translation words as the representations and directly perform clustering on the translation words,3 rather than the contexts of occurrences. The embedding representation is chosen for two reasons: (1) Word embeddings encode rich lexical semantics. They can be directly used to measure word similarities. (2) Embedding representation of the translation words leads to extremely high-efficiency clustering, because the number of translation words is orde"
C14-1048,J93-2003,0,0.0375018,"different senses in different clusters (). Once word senses are effectively induced for each word, we are able to form the sense-labeled training data of RNNLMs by tagging each word occurrence in the source language text with its associated sense cluster (®). Finally, the sense-tagged corpus is used to train the sense-specific word embeddings in a standard manner (¯). 3.1 Translation Words Extraction Given bilingual data after word alignment, we present a way of extracting translation words for source language words by exploiting the translation probability produced by word alignment models (Brown et al., 1993; Och and Ney, 2003; Liang et al., 2006). More formally, we notate the Chinese sentence as c = (c1 , ..., cI ) and English sentence as e = (e1 , ..., eJ ). The alignment models can be generally factored as: P p(c|e) = a p(a, c|e) Q p(a, c|e) = Jj=1 pd (aj |aj− , j)pt (cj |eaj ) (3) (4) where a is the alignment specifying the position of an English word aligned to each Chinese word, pd (aj |aj− , j) is the distortion probability, and pt (cj |eaj ) is the translation probability which we use. 499 SL Word 制服 花 法 领导 Translation Words Translation Word Clusters Nearest Neighbours investment, overpow"
C14-1048,N13-1006,1,0.799372,"Missing"
C14-1048,J81-4005,0,0.804567,"Missing"
C14-1048,P14-1113,1,0.133577,"word embeddings 9 10 www.icl.pku.edu.cn/icl groups/corpus/dwldform1.asp Person, Location, Organization, Date, Time, Number and Miscellany 504 0.90 +SingleEmb Polysemous(2) Polysemous(3) +SenseEmb 0.80 0.75 0.70 0.60 0.65 per−token accuracy 0.85 Baseline Monosemous Figure 4: Per-token accuracy on the polysemous and monosemous words in the NER test data. Polysemous(k) represents the set of words that have more than or equal to k senses defined in HowNet. are shown to capture many relational similarities, which can be recovered by vector arithmetic in the embedding space (Mikolov et al., 2013b; Fu et al., 2014). Klementiev et al. (2012) and Zou et al. (2013) learned cross-lingual word embeddings by utilizing MT word alignments in bilingual parallel data to constrain translational equivalence. Most previous NNLMs induce single embedding for each word, ignoring the polysemous property of languages. In an attempt to capture the different senses or usage of a word, Reisinger and Mooney (2010) and Huang et al. (2012) proposed multi-prototype models for inducing multiple embeddings for each word. They did this by clustering the contexts of words. These multi-prototype models simply induced a fixed number"
C14-1048,1992.tmi-1.9,0,0.332596,"ng stochastic gradient descent (SGD), in which back propagation through time (BPTT) is used to efficiently compute the gradients. In the RNNLM, U is the embedding matrix, where each column vector represents a word. As discussed in Section 1, the RNNLM and even most NNLMs ignore the polysemy phenomenon in natural languages and induce a single embedding for each word. We address this issue and introduce an effective approach for capturing polysemy in the next section. 3 Sense-specific Word Embedding Learning In our approach, WSI is performed prior to the training of word embeddings. Inspired by Gale et al. (1992) and Chan and Ng (2005), who used bilingual data for automatically generating training examples of WSD, we present a bilingual approach for unsupervised WSI, as shown in Figure 1. First, we extract the translations of the source language words from bilingual data (¬). Since there may be multiple translations for the same sense of a source language word, it is straightforward to cluster the translation words, exhibiting different senses in different clusters (). Once word senses are effectively induced for each word, we are able to form the sense-labeled training data of RNNLMs by tagging each"
C14-1048,P12-1092,0,0.872734,"). They are also shown to be effective as input to NLP systems (Collobert et al., 2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013). In recent years, neural network language models (NNLMs) have become popular architectures for learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an attempt to better capture the multiple senses or usages of a word, several multi-prototype models have been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply induce K prototypes (embeddings) for every word in the vocabulary, where K is predefined as a fixed value. These models still may not capture the real senses of words, because different words may have different number of senses. We present a novel and simple method of learning sense-specific word embeddings by using bilingual parallel data. In this method, word sense induction (WSI) is performed prior to the training of NNLMs. We exploit bilingual parallel data for WSI, which is motivated by the intuition that the same word in the source language with diff"
C14-1048,S12-1049,0,0.0285058,"ich benefits many practical applications. Therefore, we first evaluate our embeddings using a similarity measurement. Word similarities are calculated using the MaxSim and AvgSim metric (Reisinger and Mooney, 2010): M axSim(u, v) = max1≤i≤ku ,1≤j≤kv s(ui , v j ) Pku Pkv 1 i j AvgSim(u, v) = ku ×k i=1 j=1 s(u , v ) v (5) (6) where ku and kv are the number of the induced senses for words u and v, respectively. s(·, ·) can be any standard similarity measure. In this study, we use the cosine similarity. Previous works used the WordSim-353 dataset (Finkelstein et al., 2002) or the Chinese version (Jin and Wu, 2012) for the evaluation of general word similarity. These datasets rarely contain polysemous words, and thus is unsuitable for our evaluation. To the best of our knowledge, no datasets for polysemous word similarity evaluation have been published yet, either in English or Chinese. In order to fill this gap in the research community, we manually construct a Chinese polysemous word similarity dataset. 5.2.1 Chinese Polysemous Word Similarity Dataset Construction We adopt the HowNet database (Dong and Dong, 2006) in constructing the dataset. HowNet is a Chinese knowledge database that maintains compr"
C14-1048,W13-3513,0,0.0127418,"language words is varied, the commonlyused k-means algorithm becomes inappropriate for this situation. Instead, we employ affinity propagation (AP) algorithm (Frey and Dueck, 2007) for clustering. In AP, each cluster is represented by one of the samples of it, which we call an exemplar. AP finds the exemplars iteratively based on the concept of “message passing”. AP has the major advantage that the number of the resulting clusters is dynamic, which mainly depends on the distribution of the data. Compared with other possible clustering approaches, such as hierarchical agglomerative clustering (Kartsaklis et al., 2013), AP determines the number of resulting clusters automatically without using any partition criterions. The third column of Table 1 lists the resulting clusters of the translation words for the sampled polysemous words. We can see that the resulting clusters are meaningful: senses are well represented by clusters of translation words. 3.3 Cross-lingual Word Sense Projection The produced clusters are then projected back into the source language to identify word senses. 3 The publicly available word embeddings proposed by Collobert et al. (2011) are used. 500 For each occurrence wo of the word w"
C14-1048,C12-1089,0,0.0290816,"10 www.icl.pku.edu.cn/icl groups/corpus/dwldform1.asp Person, Location, Organization, Date, Time, Number and Miscellany 504 0.90 +SingleEmb Polysemous(2) Polysemous(3) +SenseEmb 0.80 0.75 0.70 0.60 0.65 per−token accuracy 0.85 Baseline Monosemous Figure 4: Per-token accuracy on the polysemous and monosemous words in the NER test data. Polysemous(k) represents the set of words that have more than or equal to k senses defined in HowNet. are shown to capture many relational similarities, which can be recovered by vector arithmetic in the embedding space (Mikolov et al., 2013b; Fu et al., 2014). Klementiev et al. (2012) and Zou et al. (2013) learned cross-lingual word embeddings by utilizing MT word alignments in bilingual parallel data to constrain translational equivalence. Most previous NNLMs induce single embedding for each word, ignoring the polysemous property of languages. In an attempt to capture the different senses or usage of a word, Reisinger and Mooney (2010) and Huang et al. (2012) proposed multi-prototype models for inducing multiple embeddings for each word. They did this by clustering the contexts of words. These multi-prototype models simply induced a fixed number of embeddings for every wo"
C14-1048,N06-1014,0,0.488068,"(). Once word senses are effectively induced for each word, we are able to form the sense-labeled training data of RNNLMs by tagging each word occurrence in the source language text with its associated sense cluster (®). Finally, the sense-tagged corpus is used to train the sense-specific word embeddings in a standard manner (¯). 3.1 Translation Words Extraction Given bilingual data after word alignment, we present a way of extracting translation words for source language words by exploiting the translation probability produced by word alignment models (Brown et al., 1993; Och and Ney, 2003; Liang et al., 2006). More formally, we notate the Chinese sentence as c = (c1 , ..., cI ) and English sentence as e = (e1 , ..., eJ ). The alignment models can be generally factored as: P p(c|e) = a p(a, c|e) Q p(a, c|e) = Jj=1 pd (aj |aj− , j)pt (cj |eaj ) (3) (4) where a is the alignment specifying the position of an English word aligned to each Chinese word, pd (aj |aj− , j) is the distortion probability, and pt (cj |eaj ) is the translation probability which we use. 499 SL Word 制服 花 法 领导 Translation Words Translation Word Clusters Nearest Neighbours investment, overpower, investment, uniform 穿着dress , 警服poli"
C14-1048,N13-1090,0,0.281323,"g the sense-level word similarities. We further feed our embeddings as features in Chinese named entity recognition and obtain noticeable improvements against single embeddings. 1 Introduction Word embeddings are conventionally defined as compact, real-valued, and low-dimensional vector representations for words. Each dimension of word embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic characteristics. Word embeddings can be used straightforwardly for computing word similarities, which benefits many practical applications (Socher et al., 2011; Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al., 2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013). In recent years, neural network language models (NNLMs) have become popular architectures for learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an attempt to better capture the multiple senses or usages of a word, several multi-prototype models have been proposed (Reisinger and Mooney, 2010;"
C14-1048,J03-1002,0,0.00534514,"different clusters (). Once word senses are effectively induced for each word, we are able to form the sense-labeled training data of RNNLMs by tagging each word occurrence in the source language text with its associated sense cluster (®). Finally, the sense-tagged corpus is used to train the sense-specific word embeddings in a standard manner (¯). 3.1 Translation Words Extraction Given bilingual data after word alignment, we present a way of extracting translation words for source language words by exploiting the translation probability produced by word alignment models (Brown et al., 1993; Och and Ney, 2003; Liang et al., 2006). More formally, we notate the Chinese sentence as c = (c1 , ..., cI ) and English sentence as e = (e1 , ..., eJ ). The alignment models can be generally factored as: P p(c|e) = a p(a, c|e) Q p(a, c|e) = Jj=1 pd (aj |aj− , j)pt (cj |eaj ) (3) (4) where a is the alignment specifying the position of an English word aligned to each Chinese word, pd (aj |aj− , j) is the distortion probability, and pt (cj |eaj ) is the translation probability which we use. 499 SL Word 制服 花 法 领导 Translation Words Translation Word Clusters Nearest Neighbours investment, overpower, investment, uni"
C14-1048,N10-1013,0,0.776045,"2011; Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al., 2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013). In recent years, neural network language models (NNLMs) have become popular architectures for learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an attempt to better capture the multiple senses or usages of a word, several multi-prototype models have been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply induce K prototypes (embeddings) for every word in the vocabulary, where K is predefined as a fixed value. These models still may not capture the real senses of words, because different words may have different number of senses. We present a novel and simple method of learning sense-specific word embeddings by using bilingual parallel data. In this method, word sense induction (WSI) is performed prior to the training of NNLMs. We exploit bilingual parallel data for WSI, which is motivated by the intuition that the same word in the sour"
C14-1048,J98-1004,0,0.56021,"Missing"
C14-1048,P10-1040,0,0.632179,"ents against single embeddings. 1 Introduction Word embeddings are conventionally defined as compact, real-valued, and low-dimensional vector representations for words. Each dimension of word embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic characteristics. Word embeddings can be used straightforwardly for computing word similarities, which benefits many practical applications (Socher et al., 2011; Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al., 2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013). In recent years, neural network language models (NNLMs) have become popular architectures for learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an attempt to better capture the multiple senses or usages of a word, several multi-prototype models have been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply induce K prototypes (embeddings) for every word in the vocabulary, where K is predefine"
C14-1048,N13-1063,0,0.015,"mbeddings. 1 Introduction Word embeddings are conventionally defined as compact, real-valued, and low-dimensional vector representations for words. Each dimension of word embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic characteristics. Word embeddings can be used straightforwardly for computing word similarities, which benefits many practical applications (Socher et al., 2011; Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al., 2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013). In recent years, neural network language models (NNLMs) have become popular architectures for learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an attempt to better capture the multiple senses or usages of a word, several multi-prototype models have been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply induce K prototypes (embeddings) for every word in the vocabulary, where K is predefined as a fixed value"
C14-1048,D13-1141,0,0.169902,"d similarities from the dataset. The whole evaluation dataset will be publicly available for the research community.8 Word 制服 出 花 面 Paired word Category Mean.Sim Std.Dev 征服conquer synonym 8.60 0.29 重点key point unrelated 0.12 0.19 进enter autonym 7.90 0.97 发表publish near-synonym 7.86 0.76 茎plant stem sibling 7.80 0.12 费用cost topic-related 5.86 0.90 食物f ood hypernym 6.50 0.71 Table 2: Sample word pairs of our dataset. The unrelated words are randomly sampled. M ean.Sim represents the mean similarity of the annotations, Std.Dev represents the standard deviation. 5.2.2 Evaluation Results Following Zou et al. (2013), we use Spearman’s ρ correlation and Kendall’s τ correlation for evaluation. The results are shown in Table 3. By utilizing sense-specific embeddings, our approach significantly outperforms the single-version using either MaxSim or AvgSim measurement. For comparison with multi-prototype methods, we borrow the context-clustering idea from Huang et al. (2012), which was first presented by Sch¨utze (1998). The occurrences of a word are represented by the average embeddings of its context words. Following Huang et al.’s settings, we use a context window of size 10 and all occurrences of a word ar"
C16-1002,Q16-1031,0,0.148922,"008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target language has a small treebank of ∼3K tokens. Their models may sacrifice accuracy on target"
C16-1002,P16-1231,0,0.0254544,"stack, a buffer and a set of dependency arcs. Traditional parsing models deal with features extracted from manually defined feature templates in a discrete feature space, which suffers from the problems of Sparsity, Incompleteness and Expensive feature computation. The neural network model proposed by Chen and Manning (2014) instead represents features as continuous, low-dimensional vectors and use a cube activation function for implicit feature composition. More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016). Here, we employ the LSTM-based architecture enhanced with character bidirectional LSTMs (Ballesteros et al., 2015) for the following major reasons: 3 Duong et al. (2015b) used L2 regularizers to tie the lexical embeddings with a bilingual dictionary. 14 Char-BiLSTM … ? ?? &lt;w&gt; ?? ?? Stack LSTM RecNN ?? ?? v e &lt;/w&gt; … ?? ??+1 … ?1 o Buffer LSTM Action LSTM … l ?? ?? ? ? ?1 ? ? (a) (b) Figure 2: The LSTM-based neural parser (a) and the Char-BiLSTM for modeling words (b). • Compared with Chen & Manning’s architecture, it makes full use of the non-local features by modeling the full history inform"
C16-1002,D15-1041,0,0.052963,". (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. To the best of our knowledge, we present the first work that successfully integrate both monolingual and multilingual treebanks for parsing, with or without consistent annotation schemes. 3 Approach This section describes the deep multi-task learning architecture, using a formalism that extends on the transition-based dependency parsing model with LSTM networks (Dyer et al., 2015) which is further enhanced by modeling characters (Ballesteros et al., 2015). We first revisit the parsing approach of Ballesteros et al. (2015), then present our framework for learning with multi-typed source treebanks. 3.1 Transition-based Neural Parsing Neural models for parsing have gained a lot of interests in recent years, particularly boosted by Chen and Manning (2014). The heart of transition-based parsing is the challenge of representing the state (configuration) of a transition system, based on which the most likely transition action is determined. Typically, a state includes three primary components, a stack, a buffer and a set of dependency arcs. Tradition"
C16-1002,D12-1133,0,0.0622979,"r approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010). More recently, the idea of neural multi-task learning was applied to sequence-to-"
C16-1002,W06-2920,0,0.0880729,"us referred to as deep multi-task learning. We find that different parameter sharing strategies should be applied for different typed source treebanks adaptively, due to the different types of consistencies and inconsistencies (Figure 1). We investigate the effect of multilingual transfer parsing using the Universal Dependency Treebanks (UDT) (McDonald et al., 2013). We show that our approach improves significantly over strong supervised baseline systems in six languages. We further study the effect of monolingual heterogeneous transfer parsing using UDT and the C O NLL-X shared task dataset (Buchholz and Marsi, 2006). We consider using UDT and CoNLL-X as source treebanks respectively, to investigate their mutual benefits. Experiment results show significant improvements under both settings. Moreover, indirect comparisons on the Chinese Penn Treebank 5.1 (CTB5) using the Chinese Dependency Treebank (CDT)1 as source treebank show the merits of our approach over previous work.2 2 Related Work The present work is related to several strands of previous studies. Monolingual resources for parsing Exploiting heterogeneous treebanks for parsing has been explored in various ways. Niu et al. (2009) automatically con"
C16-1002,D08-1092,0,0.0308853,"ank of ∼3K tokens. Their models may sacrifice accuracy on target languages with a large treebank. Ammar et al. (2016) and Vilares et al. (2016) instead train a single parser on a multilingual set of rich-resource treebanks, which is a more similar setting to ours. We refer to their approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharin"
C16-1002,D14-1082,0,0.0307899,"g, with or without consistent annotation schemes. 3 Approach This section describes the deep multi-task learning architecture, using a formalism that extends on the transition-based dependency parsing model with LSTM networks (Dyer et al., 2015) which is further enhanced by modeling characters (Ballesteros et al., 2015). We first revisit the parsing approach of Ballesteros et al. (2015), then present our framework for learning with multi-typed source treebanks. 3.1 Transition-based Neural Parsing Neural models for parsing have gained a lot of interests in recent years, particularly boosted by Chen and Manning (2014). The heart of transition-based parsing is the challenge of representing the state (configuration) of a transition system, based on which the most likely transition action is determined. Typically, a state includes three primary components, a stack, a buffer and a set of dependency arcs. Traditional parsing models deal with features extracted from manually defined feature templates in a discrete feature space, which suffers from the problems of Sparsity, Incompleteness and Expensive feature computation. The neural network model proposed by Chen and Manning (2014) instead represents features as"
C16-1002,P10-1003,0,0.0239738,"e the target language has a small treebank of ∼3K tokens. Their models may sacrifice accuracy on target languages with a large treebank. Ammar et al. (2016) and Vilares et al. (2016) instead train a single parser on a multilingual set of rich-resource treebanks, which is a more similar setting to ours. We refer to their approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving mul"
C16-1002,P15-1166,1,0.812783,"works can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010). More recently, the idea of neural multi-task learning was applied to sequence-to-sequence problems with recurrent neural networks. Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. To the best of our knowledge, we present the first work that successfully integrate both monolingual and multilingual treebanks for parsing, with or without consistent annotation schemes. 3 Approach This section describes the deep multi-task learning architecture, using a formalism that exte"
C16-1002,P15-2139,0,0.461231,"(Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target language has a small treebank of ∼3K tokens. Their models may sa"
C16-1002,D15-1040,0,0.405925,"(Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target language has a small treebank of ∼3K tokens. Their models may sa"
C16-1002,P15-1033,0,0.12877,"translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. To the best of our knowledge, we present the first work that successfully integrate both monolingual and multilingual treebanks for parsing, with or without consistent annotation schemes. 3 Approach This section describes the deep multi-task learning architecture, using a formalism that extends on the transition-based dependency parsing model with LSTM networks (Dyer et al., 2015) which is further enhanced by modeling characters (Ballesteros et al., 2015). We first revisit the parsing approach of Ballesteros et al. (2015), then present our framework for learning with multi-typed source treebanks. 3.1 Transition-based Neural Parsing Neural models for parsing have gained a lot of interests in recent years, particularly boosted by Chen and Manning (2014). The heart of transition-based parsing is the challenge of representing the state (configuration) of a transition system, based on which the most likely transition action is determined. Typically, a state includes three p"
C16-1002,P15-1119,1,0.866599,"en made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target"
C16-1002,P12-1110,0,0.0139446,"ilar setting to ours. We refer to their approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010). More recently, the idea of neural multi"
C16-1002,J13-4006,0,0.0462287,"ial of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010). More recently, the idea of neural multi-task learning was applied to sequence-to-sequence problems with recurrent neural networks. Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence m"
C16-1002,D09-1127,0,0.0242498,"ge has a small treebank of ∼3K tokens. Their models may sacrifice accuracy on target languages with a large treebank. Ammar et al. (2016) and Vilares et al. (2016) instead train a single parser on a multilingual set of rich-resource treebanks, which is a more similar setting to ours. We refer to their approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks with"
C16-1002,N13-1013,0,0.666564,".e., treebanks). Numerous efforts have been made towards the construction of treebanks which established the benchmark research on dependency parsing, such as the widely-used Penn Treebank (Marcus et al., 1993). However, the heavy cost of treebanking typically limits the existing treebanks in both scale and coverage of languages. To address the problem, a variety of authors have proposed to exploit existing heterogeneous treebanks with different annotation schemes via grammar conversion (Niu et al., 2009), quasi-synchronous grammar features (Li et al., 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models. Despite their effectiveness in specific datasets, these methods typically lack the scalability of exploiting richer source treebanks, such as cross-lingual treebanks. In this paper, we aim at developing a universal framework for transfer parsing that can exploit multityped source treebanks to improve parsing of a target treebank. Specifically, we will consider two kinds of source treebanks, that are multilingual universal treebanks and monolingual heterogeneous treebanks. Cross-lingual supervision has proven highly beneficial for parsing low-resource lan"
C16-1002,D11-1109,1,0.821766,"We refer to their approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010). More recently, the idea of neural multi-task learning wa"
C16-1002,P12-1071,1,0.935143,"availability and scale of annotated training data (i.e., treebanks). Numerous efforts have been made towards the construction of treebanks which established the benchmark research on dependency parsing, such as the widely-used Penn Treebank (Marcus et al., 1993). However, the heavy cost of treebanking typically limits the existing treebanks in both scale and coverage of languages. To address the problem, a variety of authors have proposed to exploit existing heterogeneous treebanks with different annotation schemes via grammar conversion (Niu et al., 2009), quasi-synchronous grammar features (Li et al., 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models. Despite their effectiveness in specific datasets, these methods typically lack the scalability of exploiting richer source treebanks, such as cross-lingual treebanks. In this paper, we aim at developing a universal framework for transfer parsing that can exploit multityped source treebanks to improve parsing of a target treebank. Specifically, we will consider two kinds of source treebanks, that are multilingual universal treebanks and monolingual heterogeneous treebanks. Cross-lingual supervision has p"
C16-1002,J93-2004,0,0.0535212,"datasets in various languages demonstrate that our approach can make effective use of arbitrary source treebanks to improve target parsing models. 1 Introduction As a long-standing central problem in natural language processing (NLP), dependency parsing has been dominated by data-driven approaches for decades. The foundation of data-driven parsing is the availability and scale of annotated training data (i.e., treebanks). Numerous efforts have been made towards the construction of treebanks which established the benchmark research on dependency parsing, such as the widely-used Penn Treebank (Marcus et al., 1993). However, the heavy cost of treebanking typically limits the existing treebanks in both scale and coverage of languages. To address the problem, a variety of authors have proposed to exploit existing heterogeneous treebanks with different annotation schemes via grammar conversion (Niu et al., 2009), quasi-synchronous grammar features (Li et al., 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models. Despite their effectiveness in specific datasets, these methods typically lack the scalability of exploiting richer source treebanks, such as cross-lingua"
C16-1002,D11-1006,0,0.0824227,"rsing models. Despite their effectiveness in specific datasets, these methods typically lack the scalability of exploiting richer source treebanks, such as cross-lingual treebanks. In this paper, we aim at developing a universal framework for transfer parsing that can exploit multityped source treebanks to improve parsing of a target treebank. Specifically, we will consider two kinds of source treebanks, that are multilingual universal treebanks and monolingual heterogeneous treebanks. Cross-lingual supervision has proven highly beneficial for parsing low-resource languages (Hwa et al., 2005; McDonald et al., 2011), implying that different languages have a great deal of common ground in grammars. But unfortunately, linguistic inconsistencies also exist in both typologies and lexical representations across languages. Figure 1(a) illustrates two sentences in German and English with universal dependency annotations. The typological differences (subject-verb-object order) results in the opposite directions of the dobj arcs, while the rest arcs remain consistent. Similar problems also come with monolingual heterogeneous treebanks. Figure 1(b) shows an English sentence annotated with respectively the universa"
C16-1002,P09-1006,1,0.95269,"decades. The foundation of data-driven parsing is the availability and scale of annotated training data (i.e., treebanks). Numerous efforts have been made towards the construction of treebanks which established the benchmark research on dependency parsing, such as the widely-used Penn Treebank (Marcus et al., 1993). However, the heavy cost of treebanking typically limits the existing treebanks in both scale and coverage of languages. To address the problem, a variety of authors have proposed to exploit existing heterogeneous treebanks with different annotation schemes via grammar conversion (Niu et al., 2009), quasi-synchronous grammar features (Li et al., 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models. Despite their effectiveness in specific datasets, these methods typically lack the scalability of exploiting richer source treebanks, such as cross-lingual treebanks. In this paper, we aim at developing a universal framework for transfer parsing that can exploit multityped source treebanks to improve parsing of a target treebank. Specifically, we will consider two kinds of source treebanks, that are multilingual universal treebanks and monolingual he"
C16-1002,P08-1108,0,0.0257172,"t feature-level with discrete representations, which limits its scalability to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach is capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multi-typed treebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt"
C16-1002,P09-1040,0,0.0260673,"hin the stack and buffer are modeled with a recursive neural network (RecNN) as described in Dyer et al. (2015). Next, a linear mapping (W) is applied to the concatenation of st , bt and at , and passed through a component-wise ReLU: pt = ReLU(W[st ; bt ; at ] + d) (2) Finally, the probability of next action z ∈ A(S, B) is estimated using a softmax function: p(z∣pt ) = exp(gz⊺ pt + qz ) Σz ′ ∈A(S,B) exp(gz⊺′ pt + qz ′ ) (3) where A(S, B) represents the set of valid actions given the current content in the stack and buffer. We apply the non-projective transition system originally introduced by Nivre (2009) since most of the treebanks we consider in this study has a noticeable proportion of non-projective trees. In the S WAPbased system, both the stack and buffer may contain tree fragments, so RecNN is applied both in S and B to obtain representations of each position. 3.2 Deep Multi-task Learning Multi-task learning (MTL) is the procedure of inductive transfer that improves learning for one task by using the information contained in the training signals of other related tasks. It does this by learning tasks in parallel while using a shared representation. A good overview, especially focusing on"
C16-1002,petrov-etal-2012-universal,0,0.248618,"cific task with hierarchical abstractions, which gives us the flexibility to control parameter sharing in different levels accordingly. In this study, different parameter sharing strategies are applied according to the source and target treebanks being used. We consider two different scenarios: MTL with multilingual universal treebanks as source (M ULTI -U NIV) and MTL with monolingual heterogeneous treebanks as source (M ONO H ETERO). Table 1 presents our parameter sharing strategies for each setting. M ULTI -U NIV Multilingual universal treebanks are annotated with the same set of POS tags (Petrov et al., 2012), dependency relations, and share the same set of transition actions. However, the vocabularies (word, characters) are language-specific. Therefore, it makes sense to share the lookup tables (embeddings) of POS tags (Epos ), relations (Erel ) and actions (Eact ), but separate the character embeddings (Echar ) as well as the Char-BiLSTMs (BiLSTM(chars)). Additionally, linguistic typologies such as the order of subject-verb-object and adjective-noun (Figure 1(a)) varies across languages, which result in the divergence of inherent grammars of transition action sequences. So we set the action LSTM"
C16-1002,D15-1039,0,0.0201532,"ebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition"
C16-1002,D09-1086,0,0.0258726,"sent work is related to several strands of previous studies. Monolingual resources for parsing Exploiting heterogeneous treebanks for parsing has been explored in various ways. Niu et al. (2009) automatically convert the dependency-structure CDT into the phrasestructure style of CTB5 using a trained constituency parser on CTB5, and then combine the converted treebanks for constituency parsing. Li et al. (2012) capture the annotation inconsistencies among different treebanks by designing several types of transformation patterns, based on which they introduce quasi-synchronous grammar features (Smith and Eisner, 2009) to augment the baseline parsing models. Johansson (2013) also adopts the idea of parameter sharing to incorporate multiple treebanks. They focuse on parameter sharing at feature-level with discrete representations, which limits its scalability to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach is capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multi-typed treebanks and thus mor"
C16-1002,N12-1052,0,0.0751652,"Missing"
C16-1002,C14-1175,0,0.0149779,"o multi-typed treebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks tha"
C16-1002,D07-1099,0,0.0297611,"… ?1 o Buffer LSTM Action LSTM … l ?? ?? ? ? ?1 ? ? (a) (b) Figure 2: The LSTM-based neural parser (a) and the Char-BiLSTM for modeling words (b). • Compared with Chen & Manning’s architecture, it makes full use of the non-local features by modeling the full history information of a state with stack LSTMs. • By modeling words, stack, buffer and action sequence separately which indicate hierarchical abstractions of representations, we can control the information flow across tasks via parameter sharing with more flexibility (Section 3.2). Besides, we did not use the earlier ISBN parsing model (Titov and Henderson, 2007) due to its lack of scalability to large vocabulary. Figure 2(a) illustrates the transition-based parsing architecture using LSTMs. Bidirectional LSTMs are used for modeling the word representations (Figure 2(b)), which we refer to as Char-BiLSTMs henceforth. Char-BiLSTMs learn features for each word, and then the representation of each token can be calculated as: → ← Ð t] + b) x = ReLU(V[Ð w; w; (1) where t is the POS tag embedding. The token embeddings are then fed into subsequent LSTM layers to obtain representations of the stack, buffer and action sequence respectively referred to as st ,"
C16-1002,D08-1017,0,0.0311419,"on parameter sharing at feature-level with discrete representations, which limits its scalability to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach is capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multi-typed treebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Amm"
C16-1002,P16-2069,0,0.0406272,"Missing"
C16-1002,P15-1032,0,0.011917,"te includes three primary components, a stack, a buffer and a set of dependency arcs. Traditional parsing models deal with features extracted from manually defined feature templates in a discrete feature space, which suffers from the problems of Sparsity, Incompleteness and Expensive feature computation. The neural network model proposed by Chen and Manning (2014) instead represents features as continuous, low-dimensional vectors and use a cube activation function for implicit feature composition. More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016). Here, we employ the LSTM-based architecture enhanced with character bidirectional LSTMs (Ballesteros et al., 2015) for the following major reasons: 3 Duong et al. (2015b) used L2 regularizers to tie the lexical embeddings with a bilingual dictionary. 14 Char-BiLSTM … ? ?? &lt;w&gt; ?? ?? Stack LSTM RecNN ?? ?? v e &lt;/w&gt; … ?? ??+1 … ?1 o Buffer LSTM Action LSTM … l ?? ?? ? ? ?1 ? ? (a) (b) Figure 2: The LSTM-based neural parser (a) and the Char-BiLSTM for modeling words (b). • Compared with Chen & Manning’s architecture, it makes full use of the non-local feat"
C16-1002,D15-1213,0,0.0299657,"te different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target language has a small tree"
C16-1002,D08-1059,0,0.0374319,"which limits its scalability to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach is capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multi-typed treebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual"
C16-1002,C14-1051,1,0.858613,"bility to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach is capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multi-typed treebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing"
C16-1002,P15-1117,0,0.055438,"imary components, a stack, a buffer and a set of dependency arcs. Traditional parsing models deal with features extracted from manually defined feature templates in a discrete feature space, which suffers from the problems of Sparsity, Incompleteness and Expensive feature computation. The neural network model proposed by Chen and Manning (2014) instead represents features as continuous, low-dimensional vectors and use a cube activation function for implicit feature composition. More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016). Here, we employ the LSTM-based architecture enhanced with character bidirectional LSTMs (Ballesteros et al., 2015) for the following major reasons: 3 Duong et al. (2015b) used L2 regularizers to tie the lexical embeddings with a bilingual dictionary. 14 Char-BiLSTM … ? ?? &lt;w&gt; ?? ?? Stack LSTM RecNN ?? ?? v e &lt;/w&gt; … ?? ??+1 … ?1 o Buffer LSTM Action LSTM … l ?? ?? ? ? ?1 ? ? (a) (b) Figure 2: The LSTM-based neural parser (a) and the Char-BiLSTM for modeling words (b). • Compared with Chen & Manning’s architecture, it makes full use of the non-local features by modeling th"
C16-1100,D10-1051,0,0.018129,"scribes some previous work on poetry generation and compares our work with previous methods. Section 3 describes our planning based poetry generation framework. We introduce the datasets and experimental results in Section 4. Section 5 concludes the paper. 2 Related Work Poetry generation is a challenging task in NLP. Oliveira (2009; 2012) proposed a Spanish poem generation method based on semantic and grammar templates. Netzer et al. (2009) employed a method based on word association measures. Tosa et al. (2008) and Wu et al. (2009) used a phrase search approach for Japanese poem generation. Greene et al. (2010) applied statistical methods to analyze, generate and translate rhythmic poetry. Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints. Manurung (2004; 2012) and Zhou et al. (2010) used genetic algorithms for generating poems. An important approach to poem generation is based on statistical machine translation (SMT). Jiang and Zhou (2008) used an SMT-based model in gen"
C16-1100,C08-1048,0,0.154329,"ward tone); the last character of the second and last line in a quatrain must belong to the same rhyme category (Wang, 2002). With such strict restrictions, the well-written quatrain is full of rhythmic beauty. In recent years, the research of automatic poetry generation has received great attention. Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al., 2010; Manurung et al., 2012), summarization methods (Yan et al., 2013) and statistical machine translation methods (Jiang and Zhou, 2008; He et al., 2012) to generate poems. More recently, deep learning methods have emerged as a promising discipline, which considers the poetry generation as a sequence-to-sequence generation problem (Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016). These methods usually generate the first line by selecting one line from the dataset of poems according to the user’s writing intents (usually a set of keywords), and the other three lines are generated based on the first line and the previous lines. The user’s writing intent can only affect the first line, and the rest three lines may ha"
C16-1100,D16-1230,0,0.00880819,"Missing"
C16-1100,D16-1126,0,0.139197,"Missing"
C16-1100,W04-3252,0,0.0142967,"according to one sub-topic and all the preceding lines. 3.2 Poem Planning 3.2.1 Keyword Extraction The user’s input writing intent can be represented as a sequence of words. There is an assumption in the Poem Planning stage that the number of keywords extracted from the input query Q must be equal to the number of lines N in the poem, which can ensure each line takes just one keyword as the sub-topic. If the user’s input query Q is too long, we need to extract the most important N words and keep the original order as the keywords sequence to satisfy the requirement. We use TextRank algorithm (Mihalcea and Tarau, 2004) to evaluate the importance of words. It is a graph-based ranking algorithm based on PageRank (Brin and Page, 1998). Each candidate word is represented by a vertex in the graph and edges are added between two words according to their cooccurrence; the edge weight is set according to the total count of co-occurrence strength of the two words. The TextRank score S(Vi ) is initialized to a default value (e.g. 1.0) and computed iteratively until convergence according to the following equation: S(Vi ) = (1 − d) + d X Vj ∈E(Vi ) P wji Vk ∈E(Vj ) wjk S(Vj ), (1) where wij is the weight of the edge be"
C16-1100,C16-1316,0,0.0286406,"ent hidden layers of the decoder and two encoders contained 512 hidden units. Parameters of our model were randomly initialized over a uniform distribution with support [-0.08,0.08]. The model was trained with the AdaDelta algorithm (Zeiler, 2012), where the minibatch was set to be 128. The final model is selected according to the perplexity on the validation set. 4.3 Evaluation 4.3.1 Evaluation Metrics It is well known that accurate evaluation of text generation system is difficult, such as the poetry generation and dialog response generation (Zhang and Lapata, 2014; Schatzmann et al., 2005; Mou et al., 2016). There are thousands of ways to generate an appropriate and relative poem or dialog response given a specific topic, the limited references are impossible to cover all the correct results. Liu et al. (2016) has recently shown that the overlap-based automatic evaluation metrics adapted for dialog responses, such 1 A collaborative online encyclopedia provided by Chinese search engine Baidu: http://baike.baidu.com. 1056 Poeticness Fluency Coherence Meaning Does the poem follow the rhyme and tone requirements ? Does the poem read smoothly and fluently? Is the poem coherent across lines? Does the"
C16-1100,W09-2005,0,0.0455963,"r genres of poetry in China. The principles of a quatrain include: The poem consists of four lines and each line has five or seven characters; every character has a particular tone, Ping (the level tone) or Ze (the downward tone); the last character of the second and last line in a quatrain must belong to the same rhyme category (Wang, 2002). With such strict restrictions, the well-written quatrain is full of rhythmic beauty. In recent years, the research of automatic poetry generation has received great attention. Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al., 2010; Manurung et al., 2012), summarization methods (Yan et al., 2013) and statistical machine translation methods (Jiang and Zhou, 2008; He et al., 2012) to generate poems. More recently, deep learning methods have emerged as a promising discipline, which considers the poetry generation as a sequence-to-sequence generation problem (Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016). These methods usually generate the first line by selecting one line from the dataset of poems according to the user’s wr"
C16-1100,2005.sigdial-1.6,0,0.0113308,"et al., 2013). The recurrent hidden layers of the decoder and two encoders contained 512 hidden units. Parameters of our model were randomly initialized over a uniform distribution with support [-0.08,0.08]. The model was trained with the AdaDelta algorithm (Zeiler, 2012), where the minibatch was set to be 128. The final model is selected according to the perplexity on the validation set. 4.3 Evaluation 4.3.1 Evaluation Metrics It is well known that accurate evaluation of text generation system is difficult, such as the poetry generation and dialog response generation (Zhang and Lapata, 2014; Schatzmann et al., 2005; Mou et al., 2016). There are thousands of ways to generate an appropriate and relative poem or dialog response given a specific topic, the limited references are impossible to cover all the correct results. Liu et al. (2016) has recently shown that the overlap-based automatic evaluation metrics adapted for dialog responses, such 1 A collaborative online encyclopedia provided by Chinese search engine Baidu: http://baike.baidu.com. 1056 Poeticness Fluency Coherence Meaning Does the poem follow the rhyme and tone requirements ? Does the poem read smoothly and fluently? Is the poem coherent acro"
C16-1100,D14-1074,0,0.669329,"t years, the research of automatic poetry generation has received great attention. Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al., 2010; Manurung et al., 2012), summarization methods (Yan et al., 2013) and statistical machine translation methods (Jiang and Zhou, 2008; He et al., 2012) to generate poems. More recently, deep learning methods have emerged as a promising discipline, which considers the poetry generation as a sequence-to-sequence generation problem (Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016). These methods usually generate the first line by selecting one line from the dataset of poems according to the user’s writing intents (usually a set of keywords), and the other three lines are generated based on the first line and the previous lines. The user’s writing intent can only affect the first line, and the rest three lines may have no association with the main topic of the poem, which may lead to semantic inconsistency when generating poems. In addition, topics of poems are usually represented by the words from the collected poems in the training"
C16-1120,C10-3009,0,0.119928,"ctic path. Besides, they still need a relatively small amount of feature engineering to make use of the local contexts. Another line of research focuses on neural models (Collobert et al., 2011; Zhou and Xu, 2015; FitzGerald et al., 2015), which have shown great effectiveness in automatic feature learning on a variety of NLP tasks. Most recently, Roth and Lapata (2016) employ LSTM-based recurrent neural networks to obtain the representations of syntactic path features, which is similar to our work. Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010). Recent years have seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) us"
C16-1120,D12-1133,0,0.0489228,"(2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and semantic role labeling (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Llu´ıs et al., 2013). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural modeling for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. This work also inspires us in this study to develop a unified architecture for SRL and RC in prior to joint training. Recently, the idea of neural multi-task learning was applied to sequence-to-sequence pro"
C16-1120,H05-1091,0,0.100275,"generated by our model to get the global optimization. We use the three constraints defined in Che et al. (2008): • C1: Each word should be labeled with one and only one label (including NULL). • C2: Roles with a small probability (smaller than 0.3) should never be labeled (except for NULL). • C3: Some roles (except for NULL) usually appear once for a predicate in a sentence. Hence a non-duplicate-roles list is utilized for each language. 5 Multi-task Learning The commonalities between SRL and RC inspire us to explore their potential mutual benefits. According to the Shortest Path Hypothesis (Bunescu and Mooney, 2005), if e1 and e2 are two entities mentioned in the same sentence such that they are observed to be in a certain relationship R, they often indicate two arguments of the same predicate or a sequence of predicates. To gain more insights, let’s look at the following example in RC: 1268 Instrument-Agency(e2 , e1 ) “The author[e1 ] of a keygen uses a disassembler[e2 ] to look at the raw assembly code.” Here, the “Instrument-Agency” relation provides significant evidences that author and disassembler are two arguments of a certain predicate, most likely with semantic roles A0 (agent) and A1 (patient)."
C16-1120,W08-2134,1,0.792996,"¶ ´                           ¸                            ¶ (4) p(c∣p) = softmax(gc⊺ p + qc ) (5) Global Context Syntactic Path Our model is trained by minimizing the cross-entropy loss: L(θ) = − ∑N i=0 log p(ci ∣pi ), where N is number of training instances. 4.4 Post-Inference with Integer Linear Programming for SRL SRL is a structure prediction problem and the predicted results should satisfy some structural constraints. For instance, some roles only appear once for a predicate in a sentence. Following Punyakanok et al. (2004) and Che et al. (2008), we apply ILP on the probability distributions at each token generated by our model to get the global optimization. We use the three constraints defined in Che et al. (2008): • C1: Each word should be labeled with one and only one label (including NULL). • C2: Roles with a small probability (smaller than 0.3) should never be labeled (except for NULL). • C3: Some roles (except for NULL) usually appear once for a predicate in a sentence. Hence a non-duplicate-roles list is utilized for each language. 5 Multi-task Learning The commonalities between SRL and RC inspire us to explore their potentia"
C16-1120,W09-1207,1,0.825439,"C, which effectively captures global contextual features, syntactical features and lexical semantic features. • We show that SRL can be significantly improved by jointly training with RC, reaching new stateof-the-art performance. 2 Related Work The present work ties together several strands of previous studies. Semantic Role Labeling A great deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introdu"
C16-1120,C10-3004,1,0.762732,"rix-Vector Recursive Neural Network (MV-RNN) model of Socher et al. (2012), the CNN model of Zeng et al. (2014), the tensor-based model of Yu et al. (2014), the CNN model using ranking loss (dos Santos et al., 2015), and the dependency-based neural network models (Liu et al., 2015; Xu et al., 2015b). Word embeddings are pretrained using word2vec on large-scale unlabeled data. For English, Catalan, German and Spanish, we use the latest Wikipedia data. For Chinese, we obtain the raw text from Xinhua news section (2000–2010) of the fifth edition of Chinese Gigaword (LDC2011T13). The LTP toolkit (Che et al., 2010) is applied to segment Chinese text into words. We adopt predicate-wise training for SRL and sentence-wise training for RC, and use stochastic gradient descent for optimization. Initial learning rate is set to η0 = 0.1 and updated as ηt = η0 /(1 + 0.1t) on each epoch t. Our hyperparameters for the unified model are listed in Table 1. When training RC-only models, the LSTM input/hidden dimension is set to 200, and the dimension of hidden layer is 400. Dimension of embeddings word POS NE WordNet 200 25 25 25 Dimension of layers LSTM input LSTM hidden hidden 100 100 200 Table 1: Hyperparameters s"
C16-1120,J81-4005,0,0.764154,"Missing"
C16-1120,P15-1166,1,0.725108,"., 2013). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural modeling for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. This work also inspires us in this study to develop a unified architecture for SRL and RC in prior to joint training. Recently, the idea of neural multi-task learning was applied to sequence-to-sequence problems with recurrent neural networks. Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. Liu et al. (2016) incorporate different kinds of corpus for implicit discourse relation classification using multi-task neural networks. More recently, multi-task learning has also been applied to sentence compression (Klerke et al., 2016) and machine translation quality estimation (Shah and"
C16-1120,P15-1061,0,0.10748,"midhuber (1997). The LSTMs take as input the token representation xi in each position. The hidden state vectors of the two directions’ LSTM units corresponding to each target word are then concatenated as its global context representation: Ð → ← Ð Ð → ← Ð Rgc Rgc (2) e1 = [ h e1 ; h e1 ]; e2 = [ h e2 ; h e2 ] Note that an important difference between our model and previous neural models is that we utilize the hidden state vectors of e1 and e2 instead of the representation of the whole sentence, which frees us from using position-related features (Zeng et al., 2014; Collobert et al., 2011; dos Santos et al., 2015). 4.3 Syntactic Path Representation We define the nearest common ancestor token of e1 and e2 as nca(e1 , e2 ). Then the path from e1 , e2 to nca(e1 , e2 ), i.e., e1 → . . . → nca(e1 , e2 ) and nca(e1 , e2 ) ← . . . ← e2 , are also modeled with bidirectional LSTMs, as shown in Figure 2 (right panel). We use two kinds of syntactic paths, including a generic path that takes the token representation xi as input, and a relation path that takes the dependency relations along the path as input (Figure 2). These two paths are modeled with BiLSTMgen and BiLSTMrel respectively. The hidden state vectors"
C16-1120,D15-1112,0,0.036994,"Missing"
C16-1120,J02-3001,0,0.823726,"owards the understanding of natural language sentences. Multi-typed semantic relations have been defined between two terms in a sentence in natural language processing (NLP) to promote various applications. For instance, the task of Semantic Role Labeling (SRL) defines shallow semantic dependencies between arguments and predicates, identifying the semantic roles, e.g., who did what to whom, where, when, and how. SRL has been a long-standing and challenging problem in NLP, primarily because it is strongly dependent on rich contextual and syntactical features used by the underlying classifiers (Gildea and Jurafsky, 2002). Another instance is Relation Classification (RC) which assigns sentences with two marked entities (or nominals) to a predefined set of relations (Hendrickx et al., 2010). Compared with SRL, relations defined in RC express much deeper semantics. Figure 1 shows example annotations of SRL and RC respectively. These two problems are typically studied separately in different communities. Hence the connections between them are neglected, both in data resources and approaches. In this paper, we show that SRL and RC have a lot of common ground and can be modeled with a unified model. We start by loo"
C16-1120,P12-1110,0,0.0555057,"16). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and semantic role labeling (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Llu´ıs et al., 2013). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural modeling for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. This work also inspires us in this study to develop a unified architecture for SRL and RC in prior to joint training. Recently, the idea of neural multi-task learnin"
C16-1120,J13-4006,0,0.0450169,"or-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and semantic role labeling (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Llu´ıs et al., 2013). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural modeling for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. This work also inspires us in this study to develop a unified architecture for SRL and RC in prior to joint training. Recently, the idea of neural multi-task learning was applied to sequence-to-sequence problems with recurrent neu"
C16-1120,S10-1006,0,0.134728,"Missing"
C16-1120,N16-1179,0,0.0329688,"e-to-sequence problems with recurrent neural networks. Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. Liu et al. (2016) incorporate different kinds of corpus for implicit discourse relation classification using multi-task neural networks. More recently, multi-task learning has also been applied to sentence compression (Klerke et al., 2016) and machine translation quality estimation (Shah and Specia, 2016). 3 Problem Definition This section gives formal definitions of the two tasks to be investigated: SRL and RC. 3.1 Semantic Role Labeling We follow the setup of the CoNLL-2009 shared task. Given a sentence s, each token is annotated with a predicated POS tag and predicted word lemma. Some tokens are also marked as predicates. Besides, 1266 UNESCO ?1 Lexical Semantic Features ?2 is ?1 ?2 holding ?1 ?2 its ?1 meetings ?1 ?2 ?2 in ?1 Paris ?1 ?2 ROOT ?2 SBJ VC UNESCO is is holding nearest common ancestor ???? ???????? ????????? ???"
C16-1120,N15-1121,0,0.0244141,"Missing"
C16-1120,D11-1109,1,0.853687,"et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and semantic role labeling (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Llu´ıs et al., 2013). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural modeling for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. This work also inspires us in this study to develop a unified architecture for SRL and RC in prior to joint training. Recently, the idea of neural multi-task learning was applied to"
C16-1120,P15-2047,0,0.130443,"ave seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS taggin"
C16-1120,Q13-1018,0,0.135887,"Missing"
C16-1120,P14-5010,0,0.00386489,"rve that SRL converges about 4 times slower than RC by running them separately, hence we sample from SRL 4 times often than RC during training. Despite the lack of theoretical guarantee, we found it working well in practice. Second, the key for multi-task learning to work is parameter sharing. Given the unified architecture, we can share most of the network parameters for knowledge transfer. Note that different dependency parses might be used for SRL and RC in practice. In this work, we use the officially provided predicted parses from CoNLL-2009 shared task in SRL, but adopt Stanford parser (Manning et al., 2014) to obtain parses for sentences in RC. These kinds of parses are quite different in terms of both the head-finding rules and the dependency relations. Therefore, we set the parameters involving dependency path modeling as task-specific, i.e., BiLSTMgen , BiLSTMrel and Wsp (Figure 2). The output weights (g) are task-specific as standard of multi-task learning, in order to handle different set of relations to be classified in SRL and RC. 6 Experiment In this section, we first describe data and our experimental settings, then the results and analysis. 6.1 Data and Settings For SRL, we evaluate on"
C16-1120,S14-2082,0,0.0236389,"reat deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets. Howev"
C16-1120,P16-1105,0,0.0255862,"ng neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and semantic role labeling (Hat"
C16-1120,J08-2003,0,0.0372843,"re templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets. However, tensor-based approaches cannot well generalize the high-sparsity structural features like syntactic path. Besides, they still need a relatively small amount of feature engineering to make use of the local contexts. Another line of research f"
C16-1120,C04-1197,0,0.144135,"Missing"
C16-1120,S10-1057,0,0.218415,"ffectiveness in automatic feature learning on a variety of NLP tasks. Most recently, Roth and Lapata (2016) employ LSTM-based recurrent neural networks to obtain the representations of syntactic path features, which is similar to our work. Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010). Recent years have seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs"
C16-1120,P16-1113,0,0.0848515,"gh computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets. However, tensor-based approaches cannot well generalize the high-sparsity structural features like syntactic path. Besides, they still need a relatively small amount of feature engineering to make use of the local contexts. Another line of research focuses on neural models (Collobert et al., 2011; Zhou and Xu, 2015; FitzGerald et al., 2015), which have shown great effectiveness in automatic feature learning on a variety of NLP tasks. Most recently, Roth and Lapata (2016) employ LSTM-based recurrent neural networks to obtain the representations of syntactic path features, which is similar to our work. Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010). Recent years have seen a great deal of work on using neural networks to alleviat"
C16-1120,D14-1045,0,0.0554788,"Missing"
C16-1120,N16-1069,0,0.0260159,". (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. Liu et al. (2016) incorporate different kinds of corpus for implicit discourse relation classification using multi-task neural networks. More recently, multi-task learning has also been applied to sentence compression (Klerke et al., 2016) and machine translation quality estimation (Shah and Specia, 2016). 3 Problem Definition This section gives formal definitions of the two tasks to be investigated: SRL and RC. 3.1 Semantic Role Labeling We follow the setup of the CoNLL-2009 shared task. Given a sentence s, each token is annotated with a predicated POS tag and predicted word lemma. Some tokens are also marked as predicates. Besides, 1266 UNESCO ?1 Lexical Semantic Features ?2 is ?1 ?2 holding ?1 ?2 its ?1 meetings ?1 ?2 ?2 in ?1 Paris ?1 ?2 ROOT ?2 SBJ VC UNESCO is is holding nearest common ancestor ???? ???????? ????????? ????????? SBJ Global Context Representation ??? ??? ROOT VC Syntactic"
C16-1120,D12-1110,0,0.304758,"yntactic path features, which is similar to our work. Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010). Recent years have seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination"
C16-1120,P03-1002,0,0.0441816,"ificantly improved by jointly training with RC, reaching new stateof-the-art performance. 2 Related Work The present work ties together several strands of previous studies. Semantic Role Labeling A great deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, t"
C16-1120,J08-2002,0,0.0306309,"mantic Role Labeling A great deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds o"
C16-1120,N16-1065,0,0.0356887,"Missing"
C16-1120,D15-1062,0,0.151139,"eal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and se"
C16-1120,D15-1206,0,0.0976347,"eal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and se"
C16-1120,W04-3212,0,0.0766239,"ointly training with RC, reaching new stateof-the-art performance. 2 Related Work The present work ties together several strands of previous studies. Semantic Role Labeling A great deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approa"
C16-1120,D14-1041,0,0.0363967,"esearch has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets. However, tensor-based appro"
C16-1120,N15-1155,0,0.0531329,"Missing"
C16-1120,C14-1220,0,0.564155,"Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010). Recent years have seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL"
C16-1120,P15-1109,0,0.0258044,"pturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets. However, tensor-based approaches cannot well generalize the high-sparsity structural features like syntactic path. Besides, they still need a relatively small amount of feature engineering to make use of the local contexts. Another line of research focuses on neural models (Collobert et al., 2011; Zhou and Xu, 2015; FitzGerald et al., 2015), which have shown great effectiveness in automatic feature learning on a variety of NLP tasks. Most recently, Roth and Lapata (2016) employ LSTM-based recurrent neural networks to obtain the representations of syntactic path features, which is similar to our work. Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also"
D07-1027,P04-1041,1,0.927642,"Missing"
D07-1027,W03-1005,0,0.0180297,"ost-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006) to recover empty nodes and identify their antecedents;1 (ii) integrating non-local dependency recovery into the parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to CFG-oriented approaches, a number of richer treebank-based grammar acquisition and parsing methods based on HPSG (Miyao et al., 2003), CCG (Clark and Hockenmaier, 2002), LFG (Riezler et al., 2002; Cahill et al., 2004) and Dependency Grammar (Nivre and Nilsson, 2005) incorporate non-local dependencies into their deep syntactic or semantic representations. A common characteristic of all these approaches 1 (Jijkoun, 2003; Jijkoun and Rijke, 2004) also describe postprocessing methods to recover NLDs, which are applied to syntactic dependency structures c"
D07-1027,P04-1082,0,0.253387,"ced by state-of-the-art broad coverage statistical parsers (Charniak, 2000; Bikel, 2004) are only surface context-free phrase structure trees (CFG-trees) without empty categories and coindexation to represent displaced constituents. Because of the importance of non-local dependencies in the proper determination of predicate-argument structures, recent years have witnessed a considerable amount of research on reconstructing such hidden relationships in CFG-trees. Three strategies have been proposed: (i) post-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006) to recover empty nodes and identify their antecedents;1 (ii) integrating non-local dependency recovery into the parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to CFG-oriented approaches, a number of ric"
D07-1027,E03-1049,0,0.012822,"tical parsers (Charniak, 2000; Bikel, 2004) are only surface context-free phrase structure trees (CFG-trees) without empty categories and coindexation to represent displaced constituents. Because of the importance of non-local dependencies in the proper determination of predicate-argument structures, recent years have witnessed a considerable amount of research on reconstructing such hidden relationships in CFG-trees. Three strategies have been proposed: (i) post-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006) to recover empty nodes and identify their antecedents;1 (ii) integrating non-local dependency recovery into the parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to CFG-oriented approaches, a number of richer treebank-based grammar acquisition and"
D07-1027,P04-1042,0,0.0794336,"Charniak, 2000; Bikel, 2004) are only surface context-free phrase structure trees (CFG-trees) without empty categories and coindexation to represent displaced constituents. Because of the importance of non-local dependencies in the proper determination of predicate-argument structures, recent years have witnessed a considerable amount of research on reconstructing such hidden relationships in CFG-trees. Three strategies have been proposed: (i) post-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006) to recover empty nodes and identify their antecedents;1 (ii) integrating non-local dependency recovery into the parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to CFG-oriented approaches, a number of richer treebank-based grammar acquisition and parsing methods based on"
D07-1027,A00-2018,0,0.0088518,"ociated with another position. These relationships are referred to Non-Local Dependencies (NLDs), where the surface location of the constituent is called antecedent , and the site where the antecedent should be interpreted semantically is called trace . Capturing non-local dependencies is crucial to the accurate and complete determination of semantic interpretation in the form of predicateargument-modifier structures or deep dependencies. / / 0 0 However, with few exceptions (Model 3 of Collins, 1999; Schmid, 2006), output trees produced by state-of-the-art broad coverage statistical parsers (Charniak, 2000; Bikel, 2004) are only surface context-free phrase structure trees (CFG-trees) without empty categories and coindexation to represent displaced constituents. Because of the importance of non-local dependencies in the proper determination of predicate-argument structures, recent years have witnessed a considerable amount of research on reconstructing such hidden relationships in CFG-trees. Three strategies have been proposed: (i) post-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manni"
D07-1027,P06-1023,0,0.0130432,"control constructions, permit a constituent in one position to bear the grammatical role associated with another position. These relationships are referred to Non-Local Dependencies (NLDs), where the surface location of the constituent is called antecedent , and the site where the antecedent should be interpreted semantically is called trace . Capturing non-local dependencies is crucial to the accurate and complete determination of semantic interpretation in the form of predicateargument-modifier structures or deep dependencies. / / 0 0 However, with few exceptions (Model 3 of Collins, 1999; Schmid, 2006), output trees produced by state-of-the-art broad coverage statistical parsers (Charniak, 2000; Bikel, 2004) are only surface context-free phrase structure trees (CFG-trees) without empty categories and coindexation to represent displaced constituents. Because of the importance of non-local dependencies in the proper determination of predicate-argument structures, recent years have witnessed a considerable amount of research on reconstructing such hidden relationships in CFG-trees. Three strategies have been proposed: (i) post-processing parser output with pattern matchers (Johnson, 2002), lin"
D07-1027,N06-1024,0,0.183553,"004) are only surface context-free phrase structure trees (CFG-trees) without empty categories and coindexation to represent displaced constituents. Because of the importance of non-local dependencies in the proper determination of predicate-argument structures, recent years have witnessed a considerable amount of research on reconstructing such hidden relationships in CFG-trees. Three strategies have been proposed: (i) post-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006) to recover empty nodes and identify their antecedents;1 (ii) integrating non-local dependency recovery into the parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to CFG-oriented approaches, a number of richer treebank-based grammar acquisition and parsing methods based on HPSG (Miyao et al., 20"
D07-1027,P05-1013,0,0.014892,"parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to CFG-oriented approaches, a number of richer treebank-based grammar acquisition and parsing methods based on HPSG (Miyao et al., 2003), CCG (Clark and Hockenmaier, 2002), LFG (Riezler et al., 2002; Cahill et al., 2004) and Dependency Grammar (Nivre and Nilsson, 2005) incorporate non-local dependencies into their deep syntactic or semantic representations. A common characteristic of all these approaches 1 (Jijkoun, 2003; Jijkoun and Rijke, 2004) also describe postprocessing methods to recover NLDs, which are applied to syntactic dependency structures converted from CFG-trees. 257 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 257–266, Prague, June 2007. 2007 Association for Computational Linguistics is that, to date, the research has focused almost entirely on"
D07-1027,P02-1018,0,0.543789,"1999; Schmid, 2006), output trees produced by state-of-the-art broad coverage statistical parsers (Charniak, 2000; Bikel, 2004) are only surface context-free phrase structure trees (CFG-trees) without empty categories and coindexation to represent displaced constituents. Because of the importance of non-local dependencies in the proper determination of predicate-argument structures, recent years have witnessed a considerable amount of research on reconstructing such hidden relationships in CFG-trees. Three strategies have been proposed: (i) post-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006) to recover empty nodes and identify their antecedents;1 (ii) integrating non-local dependency recovery into the parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to"
D07-1027,Y04-1016,1,0.885568,"Missing"
D07-1027,C02-1145,0,0.0249609,"Missing"
D07-1027,P03-1055,0,0.291446,"ost-processing parser output with pattern matchers (Johnson, 2002), linguistic principles (Campbell, 2004) or machine learning methods (Higgins, 2003; Levy and Manning, 2004; Gabbard et al., 2006) to recover empty nodes and identify their antecedents;1 (ii) integrating non-local dependency recovery into the parser by enriching a simple PCFG model with GPSG-style gap features (Collins, 1999; Schmid, 2006); (iii) pre-processing the input sentence with a finite-state trace tagger which detects empty nodes before parsing, and identify the antecedents on the parser output with the gap information (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b). In addition to CFG-oriented approaches, a number of richer treebank-based grammar acquisition and parsing methods based on HPSG (Miyao et al., 2003), CCG (Clark and Hockenmaier, 2002), LFG (Riezler et al., 2002; Cahill et al., 2004) and Dependency Grammar (Nivre and Nilsson, 2005) incorporate non-local dependencies into their deep syntactic or semantic representations. A common characteristic of all these approaches 1 (Jijkoun, 2003; Jijkoun and Rijke, 2004) also describe postprocessing methods to recover NLDs, which are applied to syntactic dependency structures c"
D07-1027,P03-2006,0,0.0357181,"Missing"
D07-1027,P04-1040,0,0.0267384,"Missing"
D07-1027,J03-4003,0,\N,Missing
D07-1027,P02-1042,0,\N,Missing
D07-1027,P02-1035,0,\N,Missing
D07-1030,J00-1004,0,0.0191954,"section 2 we summarize the related work. We then describe our method Using RBMT systems to produce bilingual corpus for SMT in section 3. Section 4 describes the resources used in the experiments. Section 5 presents the experiment result, followed by the discussion in section 6. Finally, we conclude and present the future work in section 7. 2 Related Work In the MT field, by far the most dominant paradigm is SMT. SMT has evolved from the original word-based approach (Brown et al., 1993) into phrase-based approaches (Koehn et al., 2003; Och and Ney, 2004) and syntax-based approaches (Wu, 1997; Alshawi et al., 2000; Yamada and Knignt, 2001; Chiang, 2005). On the other hand, much important work continues to be carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focuse"
D07-1030,J93-2003,0,0.00855924,"Missing"
D07-1030,W03-0310,0,0.0577463,"Missing"
D07-1030,E06-1032,0,0.0473225,"Missing"
D07-1030,2005.mtsummit-ebmt.3,0,0.0352193,"ed in the experiments. Section 5 presents the experiment result, followed by the discussion in section 6. Finally, we conclude and present the future work in section 7. 2 Related Work In the MT field, by far the most dominant paradigm is SMT. SMT has evolved from the original word-based approach (Brown et al., 1993) into phrase-based approaches (Koehn et al., 2003; Och and Ney, 2004) and syntax-based approaches (Wu, 1997; Alshawi et al., 2000; Yamada and Knignt, 2001; Chiang, 2005). On the other hand, much important work continues to be carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focused on the hybrid of various corpus-based MT methods, such as SMT and EBMT (Vogel and Ney, 2000; Marcu, 2001; Groves and Way, 2006; Menezes and Quirk, 2005). Others tried to"
D07-1030,P05-1033,0,0.037433,"n describe our method Using RBMT systems to produce bilingual corpus for SMT in section 3. Section 4 describes the resources used in the experiments. Section 5 presents the experiment result, followed by the discussion in section 6. Finally, we conclude and present the future work in section 7. 2 Related Work In the MT field, by far the most dominant paradigm is SMT. SMT has evolved from the original word-based approach (Brown et al., 1993) into phrase-based approaches (Koehn et al., 2003; Och and Ney, 2004) and syntax-based approaches (Wu, 1997; Alshawi et al., 2000; Yamada and Knignt, 2001; Chiang, 2005). On the other hand, much important work continues to be carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focused on the hybrid of various corpus-based"
D07-1030,A94-1016,0,0.0396351,"ased methods. Habash et al. (2006) built an Arabic-English generationheavy MT system and boosted it with SMT components. METIS-II is a hybrid machine translation system, in which insights from SMT, EBMT, and RBMT are used (Vandeghinste et al., 2006). Seneff et al. (2006) combined an interlingual translation 288 framework with phrase-based SMT for spoken language translation in a limited domain. They automatically generated a corpus of EnglishChinese pairs from the same interlingual representation by parsing the English corpus and then paraphrasing each utterance into both English and Chinese. Frederking and Nirenburg (1994) produced the first MEMT system by combining outputs from three different MT engines based on their knowledge of the inner workings of the engines. Nomoto (2004) used voted language models to select the best output string at sentence level. Some recent approaches to MEMT used word alignment techniques for comparison between the MT systems (Jayaraman and Lavie, 2005; Zaanen and Somers, 2005; Matusov et al. 2006). All the above MEMT systems operate on MT outputs for complete input sentences. Mellebeek et al. (2006) presented a different approach, using a recursive decomposition algorithm that pr"
D07-1030,2006.eamt-1.15,0,0.0246764,"in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focused on the hybrid of various corpus-based MT methods, such as SMT and EBMT (Vogel and Ney, 2000; Marcu, 2001; Groves and Way, 2006; Menezes and Quirk, 2005). Others tried to exploit the advantages of both rule-based and corpus-based methods. Habash et al. (2006) built an Arabic-English generationheavy MT system and boosted it with SMT components. METIS-II is a hybrid machine translation system, in which insights from SMT, EBMT, and RBMT are used (Vandeghinste et al., 2006). Seneff et al. (2006) combined an interlingual translation 288 framework with phrase-based SMT for spoken language translation in a limited domain. They automatically generated a corpus of EnglishChinese pairs from the same interlingual representation"
D07-1030,P05-3026,0,0.0135751,"ion in a limited domain. They automatically generated a corpus of EnglishChinese pairs from the same interlingual representation by parsing the English corpus and then paraphrasing each utterance into both English and Chinese. Frederking and Nirenburg (1994) produced the first MEMT system by combining outputs from three different MT engines based on their knowledge of the inner workings of the engines. Nomoto (2004) used voted language models to select the best output string at sentence level. Some recent approaches to MEMT used word alignment techniques for comparison between the MT systems (Jayaraman and Lavie, 2005; Zaanen and Somers, 2005; Matusov et al. 2006). All the above MEMT systems operate on MT outputs for complete input sentences. Mellebeek et al. (2006) presented a different approach, using a recursive decomposition algorithm that produces simple chunks as input to the MT engines. A consensus translation is produced by combining the best chunk translation. This paper uses RBMT outputs to improve the performance of SMT systems. Instead of RBMT outputs, other researchers have used SMT outputs to boost translation quality. Callision-Burch and Osborne (2003) used co-training to extend existing par"
D07-1030,koen-2004-pharaoh,0,0.25782,"rase. d ( a i − bi −1 ) is the distortion probability. p w ( f i |e i , a) is the lexical weight, and λ is the strength of the lexical weight. 3.2 Interpolated Models We train synthetic models with the synthetic bilingual corpus produced by the RBMT systems. We can also train a translation model, namely standard model, if a real bilingual corpus is available. In order to make full use of these two kinds of corpora, we conduct linear interpolation between them. In this paper, the distortion probability in equation (2) is estimated during decoding, using the same method as described in Pharaoh (Koehn, 2004). For the phrase translation probability and lexical weight, we interpolate them as shown in (3) and (4). n φ ( f |e) = ∑ α i φ i ( f |e) (3) i =0 n p w ( f |e, a ) = ∑ β i p w,i ( f |e, a ) ∑α i =0 Where the translation model p (f |e ) can be decomposed into I ficients, ensuring (4) i =0 Where φ 0 ( f |e) and p w,0 ( f |e, a) denote the phrase translation probability and lexical weight trained with the real bilingual corpus, respectively. φi ( f |e) and p w,i ( f |e, a) ( i = 1,..., n ) are the phrase translation probability and lexical weight estimated by n synthetic corpora produced by the"
D07-1030,W06-3114,0,0.127593,"For the decoder, we use Pharaoh (Koehn, 2004). We run the decoder with its default settings (maximum phrase length 7) and then use Koehn's implementation of minimum error rate training (Och, 2003) to tune the feature weights on the de2 The full name of HTRDP is National High Technology Research and Development Program of China, also named as 863 Program. 3 It is located at http://www.statmt.org/wmt06/sharedtask/baseline.html. velopment set. The translation quality is evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). We use the same method described in (Koehn and Monz, 2006) to perform the significance test. 5 Experimental Results 5.1 Results on Synthetic Corpus Only With the monolingual English corpus and the English side of the real bilingual corpus, we translate them into Chinese using the two commercial RBMT systems and produce two synthetic bilingual corpora. With the corpora, we train two synthetic models as described in section 3.1. Based on the synthetic models, we also perform linear interpolation as shown in section 3.2, without the standard models. We tune the interpolation weights using the development set, and achieve the best performance when α 1 ="
D07-1030,N03-1017,0,0.0752916,"h the standard model. The remainder of this paper is organized as follows. In section 2 we summarize the related work. We then describe our method Using RBMT systems to produce bilingual corpus for SMT in section 3. Section 4 describes the resources used in the experiments. Section 5 presents the experiment result, followed by the discussion in section 6. Finally, we conclude and present the future work in section 7. 2 Related Work In the MT field, by far the most dominant paradigm is SMT. SMT has evolved from the original word-based approach (Brown et al., 1993) into phrase-based approaches (Koehn et al., 2003; Och and Ney, 2004) and syntax-based approaches (Wu, 1997; Alshawi et al., 2000; Yamada and Knignt, 2001; Chiang, 2005). On the other hand, much important work continues to be carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Rese"
D07-1030,P01-1050,0,0.0226073,"e carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focused on the hybrid of various corpus-based MT methods, such as SMT and EBMT (Vogel and Ney, 2000; Marcu, 2001; Groves and Way, 2006; Menezes and Quirk, 2005). Others tried to exploit the advantages of both rule-based and corpus-based methods. Habash et al. (2006) built an Arabic-English generationheavy MT system and boosted it with SMT components. METIS-II is a hybrid machine translation system, in which insights from SMT, EBMT, and RBMT are used (Vandeghinste et al., 2006). Seneff et al. (2006) combined an interlingual translation 288 framework with phrase-based SMT for spoken language translation in a limited domain. They automatically generated a corpus of EnglishChinese pairs from the same interl"
D07-1030,E06-1005,0,0.0133574,"d a corpus of EnglishChinese pairs from the same interlingual representation by parsing the English corpus and then paraphrasing each utterance into both English and Chinese. Frederking and Nirenburg (1994) produced the first MEMT system by combining outputs from three different MT engines based on their knowledge of the inner workings of the engines. Nomoto (2004) used voted language models to select the best output string at sentence level. Some recent approaches to MEMT used word alignment techniques for comparison between the MT systems (Jayaraman and Lavie, 2005; Zaanen and Somers, 2005; Matusov et al. 2006). All the above MEMT systems operate on MT outputs for complete input sentences. Mellebeek et al. (2006) presented a different approach, using a recursive decomposition algorithm that produces simple chunks as input to the MT engines. A consensus translation is produced by combining the best chunk translation. This paper uses RBMT outputs to improve the performance of SMT systems. Instead of RBMT outputs, other researchers have used SMT outputs to boost translation quality. Callision-Burch and Osborne (2003) used co-training to extend existing parallel corpora, wherein machine translations are"
D07-1030,2006.amta-papers.13,0,0.0257063,"Missing"
D07-1030,2005.mtsummit-ebmt.13,0,0.0148516,"ine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focused on the hybrid of various corpus-based MT methods, such as SMT and EBMT (Vogel and Ney, 2000; Marcu, 2001; Groves and Way, 2006; Menezes and Quirk, 2005). Others tried to exploit the advantages of both rule-based and corpus-based methods. Habash et al. (2006) built an Arabic-English generationheavy MT system and boosted it with SMT components. METIS-II is a hybrid machine translation system, in which insights from SMT, EBMT, and RBMT are used (Vandeghinste et al., 2006). Seneff et al. (2006) combined an interlingual translation 288 framework with phrase-based SMT for spoken language translation in a limited domain. They automatically generated a corpus of EnglishChinese pairs from the same interlingual representation by parsing the English cor"
D07-1030,P04-1063,0,0.0201967,"insights from SMT, EBMT, and RBMT are used (Vandeghinste et al., 2006). Seneff et al. (2006) combined an interlingual translation 288 framework with phrase-based SMT for spoken language translation in a limited domain. They automatically generated a corpus of EnglishChinese pairs from the same interlingual representation by parsing the English corpus and then paraphrasing each utterance into both English and Chinese. Frederking and Nirenburg (1994) produced the first MEMT system by combining outputs from three different MT engines based on their knowledge of the inner workings of the engines. Nomoto (2004) used voted language models to select the best output string at sentence level. Some recent approaches to MEMT used word alignment techniques for comparison between the MT systems (Jayaraman and Lavie, 2005; Zaanen and Somers, 2005; Matusov et al. 2006). All the above MEMT systems operate on MT outputs for complete input sentences. Mellebeek et al. (2006) presented a different approach, using a recursive decomposition algorithm that produces simple chunks as input to the MT engines. A consensus translation is produced by combining the best chunk translation. This paper uses RBMT outputs to imp"
D07-1030,P03-1021,0,0.0116753,"ource sentence in the test set and the development set has 4 different references. 4.2 Tools In this paper, we use two off-the-shelf commercial English to Chinese RBMT systems to produce the synthetic bilingual corpus. We also need a trainer and a decoder to perform phrase-based SMT. We use Koehn's training scripts 3 to train the translation model, and the SRILM toolkit (Stolcke, 2002) to train language model. For the decoder, we use Pharaoh (Koehn, 2004). We run the decoder with its default settings (maximum phrase length 7) and then use Koehn's implementation of minimum error rate training (Och, 2003) to tune the feature weights on the de2 The full name of HTRDP is National High Technology Research and Development Program of China, also named as 863 Program. 3 It is located at http://www.statmt.org/wmt06/sharedtask/baseline.html. velopment set. The translation quality is evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). We use the same method described in (Koehn and Monz, 2006) to perform the significance test. 5 Experimental Results 5.1 Results on Synthetic Corpus Only With the monolingual English corpus and the English side of the real bilingual co"
D07-1030,J04-4002,0,0.019069,". The remainder of this paper is organized as follows. In section 2 we summarize the related work. We then describe our method Using RBMT systems to produce bilingual corpus for SMT in section 3. Section 4 describes the resources used in the experiments. Section 5 presents the experiment result, followed by the discussion in section 6. Finally, we conclude and present the future work in section 7. 2 Related Work In the MT field, by far the most dominant paradigm is SMT. SMT has evolved from the original word-based approach (Brown et al., 1993) into phrase-based approaches (Koehn et al., 2003; Och and Ney, 2004) and syntax-based approaches (Wu, 1997; Alshawi et al., 2000; Yamada and Knignt, 2001; Chiang, 2005). On the other hand, much important work continues to be carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids"
D07-1030,P02-1040,0,0.0775652,"nd the SRILM toolkit (Stolcke, 2002) to train language model. For the decoder, we use Pharaoh (Koehn, 2004). We run the decoder with its default settings (maximum phrase length 7) and then use Koehn's implementation of minimum error rate training (Och, 2003) to tune the feature weights on the de2 The full name of HTRDP is National High Technology Research and Development Program of China, also named as 863 Program. 3 It is located at http://www.statmt.org/wmt06/sharedtask/baseline.html. velopment set. The translation quality is evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). We use the same method described in (Koehn and Monz, 2006) to perform the significance test. 5 Experimental Results 5.1 Results on Synthetic Corpus Only With the monolingual English corpus and the English side of the real bilingual corpus, we translate them into Chinese using the two commercial RBMT systems and produce two synthetic bilingual corpora. With the corpora, we train two synthetic models as described in section 3.1. Based on the synthetic models, we also perform linear interpolation as shown in section 3.2, without the standard models. We tune the interpolation weights using the d"
D07-1030,2006.amta-papers.24,0,0.0339716,"Missing"
D07-1030,2006.iwslt-papers.3,0,0.0519484,"m that produces simple chunks as input to the MT engines. A consensus translation is produced by combining the best chunk translation. This paper uses RBMT outputs to improve the performance of SMT systems. Instead of RBMT outputs, other researchers have used SMT outputs to boost translation quality. Callision-Burch and Osborne (2003) used co-training to extend existing parallel corpora, wherein machine translations are selectively added to training corpora with multiple source texts. They also created training data for a language pair without a parallel corpus by using multiple source texts. Ueffing (2006) explored monolingual source-language data to improve an existing machine translation system via selftraining. The source data is translated by a SMT system, and the reliable translations are automatically identified. Both of the methods improved translation quality. 3 Method In this paper, we use the synthetic and real bilingual corpus to train the phrase-based translation models. 3.1 Phrase-Based Models According to the translation model presented in (Koehn et al., 2003), given a source sentence f , the best target translation e best can be obtained using the following model e best = arg max"
D07-1030,vandeghinste-etal-2006-metis,0,0.0197189,"of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focused on the hybrid of various corpus-based MT methods, such as SMT and EBMT (Vogel and Ney, 2000; Marcu, 2001; Groves and Way, 2006; Menezes and Quirk, 2005). Others tried to exploit the advantages of both rule-based and corpus-based methods. Habash et al. (2006) built an Arabic-English generationheavy MT system and boosted it with SMT components. METIS-II is a hybrid machine translation system, in which insights from SMT, EBMT, and RBMT are used (Vandeghinste et al., 2006). Seneff et al. (2006) combined an interlingual translation 288 framework with phrase-based SMT for spoken language translation in a limited domain. They automatically generated a corpus of EnglishChinese pairs from the same interlingual representation by parsing the English corpus and then paraphrasing each utterance into both English and Chinese. Frederking and Nirenburg (1994) produced the first MEMT system by combining outputs from three different MT engines based on their knowledge of the inner workings of the engines. Nomoto (2004) used voted language models to select the best output str"
D07-1030,C00-2172,0,0.0315135,"t work continues to be carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years. Some research focused on the hybrid of various corpus-based MT methods, such as SMT and EBMT (Vogel and Ney, 2000; Marcu, 2001; Groves and Way, 2006; Menezes and Quirk, 2005). Others tried to exploit the advantages of both rule-based and corpus-based methods. Habash et al. (2006) built an Arabic-English generationheavy MT system and boosted it with SMT components. METIS-II is a hybrid machine translation system, in which insights from SMT, EBMT, and RBMT are used (Vandeghinste et al., 2006). Seneff et al. (2006) combined an interlingual translation 288 framework with phrase-based SMT for spoken language translation in a limited domain. They automatically generated a corpus of EnglishChinese pairs from th"
D07-1030,J97-3002,0,0.0330266,"llows. In section 2 we summarize the related work. We then describe our method Using RBMT systems to produce bilingual corpus for SMT in section 3. Section 4 describes the resources used in the experiments. Section 5 presents the experiment result, followed by the discussion in section 6. Finally, we conclude and present the future work in section 7. 2 Related Work In the MT field, by far the most dominant paradigm is SMT. SMT has evolved from the original word-based approach (Brown et al., 1993) into phrase-based approaches (Koehn et al., 2003; Och and Ney, 2004) and syntax-based approaches (Wu, 1997; Alshawi et al., 2000; Yamada and Knignt, 2001; Chiang, 2005). On the other hand, much important work continues to be carried out in Example-Based Machine Translation (EBMT) (Carl et al., 2005; Way and Gough, 2005), and many existing commercial systems are rule-based. Although we are not aware of any previous attempt to use an existing RBMT system as a black box to produce synthetic bilingual training corpus for general purpose SMT systems, there exists a great deal of work on MT hybrids and MultiEngine Machine Translation (MEMT). Research into MT hybrids has increased over the last few years"
D07-1030,P01-1067,0,0.123664,"Missing"
D07-1030,2005.mtsummit-papers.23,0,0.110715,"Missing"
D07-1030,2006.amta-papers.7,0,\N,Missing
D07-1030,2005.eamt-1.20,0,\N,Missing
D09-1051,P06-1120,0,0.174786,"w. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted 1 . In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be partially solved by introducing more resources into collocation extraction, such as chunker (Wermter and Hahn, 2004), parser (Lin, 1998; Seretan and Wehrli, 2006) and WordNet (Pearce, 2001). This paper proposes a novel monolingual word alignment (MWA) method to extract collocation of higher quality and with longer spans only from monolingual corpus, without using any additional resources. The difference between MWA and bilingual word alignment (Brown et al., 1993) is that the MWA method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment. The 1 Here, &quot;span of collocation&quot; means the distance of two words in a collocation. For example, if the span of the collocation (w1, w2) is 6, it means there are 5 words i"
D09-1051,C04-1141,0,0.115627,"cations from the word pairs in a given window. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted 1 . In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be partially solved by introducing more resources into collocation extraction, such as chunker (Wermter and Hahn, 2004), parser (Lin, 1998; Seretan and Wehrli, 2006) and WordNet (Pearce, 2001). This paper proposes a novel monolingual word alignment (MWA) method to extract collocation of higher quality and with longer spans only from monolingual corpus, without using any additional resources. The difference between MWA and bilingual word alignment (Brown et al., 1993) is that the MWA method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment. The 1 Here, &quot;span of collocation&quot; means the distance of two words in a collocation. For example, if the span of the collocati"
D09-1051,J93-2003,0,0.0505828,"Missing"
D09-1051,J93-1003,0,0.0325009,"ns in this paper include phrasal verbs (e.g. &quot;put on&quot;), proper nouns (e.g. &quot;New York&quot;), idioms (e.g. &quot;dry run&quot;), compound nouns (e.g. &quot;ice cream&quot;), correlative conjunctions (e.g. &quot;either … or&quot;), and the other commonly used combinations in following types: verb+noun, adjective+noun, adverb+verb, adverb+adjective and adjective+preposition (e.g. &quot;break rules&quot;, &quot;strong tea&quot;, &quot;softly whisper&quot;, &quot;fully aware&quot;, and &quot;fond of&quot;). Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts (Choueka et al., 1983; Church and Hanks, 1990; Smadja, 1993; Dunning, 1993; Pearce, 2002; Evert, 2004). These approaches use association measures to discover collocations from the word pairs in a given window. To avoid explosion, these approaches generally limit the window size to a small number. As a result, long-span collocations can not be extracted 1 . In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist. Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high. The above problems could be part"
D09-1051,pearce-2002-comparative,0,\N,Missing
D09-1051,J90-1003,0,\N,Missing
D09-1051,J93-1007,0,\N,Missing
D13-1050,P05-1074,0,0.0485877,"nodes (s source phrases and p pivot phrases) to represent the translation graph. (6) A =  gij  ( s + p )×( s + p ) where gij is the i,j-th elements of matrix A. We can split the matrix A into 4 sub-matrixes: 0 s×s Asp  (7) A=  A 0 ps p × p   where the sub-matrix Asp = [ pik ]s× p represents the translation probabilities from source to pivot language, and Aps represents the similar meaning. Take 3 steps walks as an example: Step1: 0 s×s Asp  A=   Aps 0 p× p  Step2:  Asp × Aps A2 =   0 p× s 4.3  Aps × Asp  Step3: 0 s× s  A3 =   Aps × Asp × Aps analogous to paraphrasing (Bannard and Callison-Burch, 2005). For the example shown in figure 1 as an example, the hidden relation between “很可口 henkekou” and “非常好吃 feichanghaochi” can be found through Step 2. 3. The third step describes the following procedure: S-P-S’-P’. An extended source-pivot phrase table is generated by 3-step random walks. Compared with the initial phrase table in Step1, although the number of phrases is not increased, the relations between phrase pairs are increased and more translation rules can be obtained. Still for the example in Figure 1 , the hidden relation between “很可口 henkekou” and “really delicious” can be generated in"
D13-1050,I11-1154,0,0.0328767,"Missing"
D13-1050,I11-1153,0,0.02207,"ly, they can be classified into 3 kinds of methods: Transfer Method: Within the transfer framework (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011), a source sentence is first translated to n pivot sentences via a sourcepivot translation system, and then each pivot sentence is translated to m target sentences via a pivot-target translation system. At each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayesrisk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). A problem with the transfer method is that it needs to decode twice. On one 525 hand, the time cost is doubled; on the other hand, the translation error of the source-pivot translation system will be transferred to the pivot-target translation. Synthetic Method: A synthetic method creates a synthetic source-target corpus using source-pivot translation model or pivot-target translation model (Utiyama et al., 2008; Wu and Wang, 2009). For example, we can translate each pivot sentence in the pivot-target corpus to source language with a pivot-source model, and then combine the translated source"
D13-1050,2005.mtsummit-papers.11,0,0.217955,"Missing"
D13-1050,C00-2163,0,0.176928,"Missing"
D13-1050,N07-1061,0,0.224847,"ional pivot-based method, triangulation method. The remainder of this paper is organized as follows. In section 2, we describe the related work. We review the triangulation method for pivotbased machine translation in section 3. Section 4 describes the random walk models. In section 5 and section 6, we describe the experiments and analyze the performance, respectively. Section 7 gives a conclusion of the paper. 2 Related Work Several methods have been proposed for pivotbased translation. Typically, they can be classified into 3 kinds of methods: Transfer Method: Within the transfer framework (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011), a source sentence is first translated to n pivot sentences via a sourcepivot translation system, and then each pivot sentence is translated to m target sentences via a pivot-target translation system. At each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayesrisk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). A problem with the transfer method is that it needs to decode twice. On one 525 hand, the time cost is dou"
D13-1050,2008.iwslt-evaluation.18,1,0.886415,"Missing"
D13-1050,P09-1018,1,0.897372,"ranslation outputs will be generated, thus a minimum Bayesrisk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). A problem with the transfer method is that it needs to decode twice. On one 525 hand, the time cost is doubled; on the other hand, the translation error of the source-pivot translation system will be transferred to the pivot-target translation. Synthetic Method: A synthetic method creates a synthetic source-target corpus using source-pivot translation model or pivot-target translation model (Utiyama et al., 2008; Wu and Wang, 2009). For example, we can translate each pivot sentence in the pivot-target corpus to source language with a pivot-source model, and then combine the translated source sentence with the target sentence to obtain a synthetic source-target corpus, and vice versa. However, it is difficult to build a high quality translation system with a corpus created by a machine translation system. Triangulation Method: The triangulation method obtains source-target model by combining source-pivot and pivot-target translation models (Wu and Wang, 2007; Cohn and Lapata 2007), which has been shown to work better tha"
D13-1050,D07-1103,0,\N,Missing
D13-1050,P02-1040,0,\N,Missing
D13-1050,P07-1108,1,\N,Missing
D13-1050,P07-2045,0,\N,Missing
D13-1050,2008.iwslt-evaluation.11,0,\N,Missing
D13-1050,N03-1017,0,\N,Missing
D13-1050,P09-2031,1,\N,Missing
D13-1050,W04-3250,0,\N,Missing
D14-1007,W13-4035,0,0.0301826,"Missing"
D14-1007,N07-2038,0,0.164032,"Missing"
D14-1007,W10-4323,0,0.0266216,"Missing"
D14-1007,W06-1302,0,0.0591506,"Processes (POMDPs) (Thomson and Young, 2010; Young et al., 2010; Williams and Young, 2007), in conjunction with reinforcement learning techniques (Jurˇc´ıcˇ ek et al., 2011; Jurˇc´ıcˇ ek et al., 2012; Gaˇsi´c et al., 2013a) to seek optimal dialogue policies that maximise long-term expected (discounted) rewards and are robust to ASR errors. However, to the best of our knowledge, most of the existing multi-domain SDS in public use are rule-based (e.g. (Gruber et al., 2012; Mirkovic and Cavedon, 2006)). The application of statistical models in multi-domain dialogue systems is still preliminary. Komatani et al. (2006) and Nakano et al. (2011) utilised a distributed architecture (Lin et al., 1999) to integrate expert dialogue systems in different domains into a unified framework, where a central controller trained as a data-driven classifier selects a domain expert at each turn to address user’s query. Alternatively, Hakkani-T¨ur et al. (2012) adopted the well-known Information State mechanism (Traum and Larsson, 2003) to construct a multi-domain SDS and proposed a discriminative classification model for more accurate state updates. More recently, Gaˇsi´c et al. (2013b) proposed that by a simple expansion o"
D14-1007,W11-2004,0,0.0505052,"Missing"
D14-1012,P05-1001,0,0.0142042,"Missing"
D14-1012,J92-4003,0,0.478376,"tomatically extracted prototypes for each target label. More foundationally, the reason for the above factors lies in the high-dimensional and sparse lexical feature representation, which completely ignores the similarity between features, especially word features. To overcome this weakness, an effective way is to learn more generalized representations of words by exploiting the numerous unlabeled data, in a semi-supervised manner. After which, the generalized word representations can be used as extra features to facilitate the supervised systems. Liang (2005) learned Brown clusters of words (Brown et al., 1992) from unlabeled data and use them as features to promote the supervised NER and Chinese word segmentation. Brown clusters of words can be seen as a generalized word representation distributed in a discrete and low-dimensional vocabulary space. Contextually similar words are grouped in the same cluster. The Brown clustering of words was also adopted in dependency parsing (Koo et al., 2008) and POS tagging for online conversational text (Owoputi et al., 2013), demonstrating significant improvements. We carefully compare and analyze these approaches in the task of NER. Experimental results are pr"
D14-1012,P05-1045,0,0.0106467,"Missing"
D14-1012,C14-1048,1,0.150649,"Missing"
D14-1012,N04-1043,0,0.0453105,"rd is an NE. To verify this, we extract new prototypes considering only two labels, namely, NE and non-NE, using the same metric in Section 3.4. As shown in the last row of Table 5, higher performance is achieved. Table 5: Performance of the NE/non-NE classification on the CoNLL-2003 development dataset using different embedding features. 6 Related Studies Semi-supervised learning with generalized word representations is a simple and general way of improving supervised NLP systems. One common approach for inducing generalized word representations is to use clustering (e.g., Brown clustering) (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Huang and Yates, 2009). Aside from word clustering, word embeddings have been widely studied. Bengio et al. (2003) propose a feed-forward neural network based language model (NNLM), which uses an embedding layer to map each word to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by t"
D14-1012,N06-1041,0,0.0307987,"Similarities between words and clusters are measured by Euclidean distance. Moreover, different number of clusters n contain information of different granularities. Therefore, we combine the cluster features of different ns to better utilize the embeddings. 3.4 NE Type B-PER I-PER B-ORG I-ORG B-LOC I-LOC B-MISC I-MISC O Table 1: Prototypes extracted from the CoNLL2003 NER training data using NPMI. Distributional Prototype Features We propose a novel kind of embedding features, named distributional prototype features for supervised models. This is mainly inspired by prototype-driven learning (Haghighi and Klein, 2006) which was originally introduced as a primarily unsupervised approach for sequence modeling. In prototype-driven learning, a few prototypical examples are specified for each target label, which can be treated as an injection of prior knowledge. This sparse prototype information is then propagated across an unlabeled corpus through distributional similarities. The basic motivation of the distributional prototype features is that similar words are supposed to be tagged with the same label. This hypothesis makes great sense in tasks such as NER and POS tagging. For example, suppose Michael is a p"
D14-1012,P09-1056,0,0.00803374,"es considering only two labels, namely, NE and non-NE, using the same metric in Section 3.4. As shown in the last row of Table 5, higher performance is achieved. Table 5: Performance of the NE/non-NE classification on the CoNLL-2003 development dataset using different embedding features. 6 Related Studies Semi-supervised learning with generalized word representations is a simple and general way of improving supervised NLP systems. One common approach for inducing generalized word representations is to use clustering (e.g., Brown clustering) (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Huang and Yates, 2009). Aside from word clustering, word embeddings have been widely studied. Bengio et al. (2003) propose a feed-forward neural network based language model (NNLM), which uses an embedding layer to map each word to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by the feed-forward NNLM, including the Hierarchical Log-Bi"
D14-1012,N13-1039,0,0.0161671,"Missing"
D14-1012,W09-1119,0,0.0510686,"gorithm is a binary tree, where each word is uniquely identified by its path from the root. Thus each word can be represented as a bit-string with a specific length. Following the setting of Owoputi et al. (2013), we will use the prefix features of hierarchical clusters to take advantage of the word similarity in different granularities. Concretely, the Brown cluster feature template is: 5.2 Table 3 shows the performances of NER on the test dataset. Our baseline is slightly lower than that of Turian et al. (2010), because they use the BILOU encoding of NE types which outperforms BIO encoding (Ratinov and Roth, 2009).8 Nonetheless, our conclusions hold. As we can see, all of the three approaches we investigate in this study achieve better performance than the direct use of the dense continuous embedding features. To our surprise, even the binarized embedding features (BinarizedEmb) outperform the continuous version (DenseEmb). This provides clear evidence that directly using the dense continuous embeddings as features in CRF indeed cannot fully • bci+k , −2 ≤ k ≤ 2. • prefix (bci+k , p), p ∈ {2,4,6,...,16}, −2 ≤ k ≤ 2. prefix takes the p-length prefix of the Brown cluster coding bci+k . 5 Experiments 5.1"
D14-1012,P12-1092,0,0.0416081,"points higher than the performance of the dense embedding features 9 Statistical significant with p-value &lt; 0.001 by two-tailed t-test. 116 Setting Baseline +DenseEmb +BinarizedEmb +ClusterEmb +DistPrototype Time (ms) / sent 1.04 4.75 1.25 1.16 2.31 very frequent words, while lower sparsity for midfrequent words. It indicates that for words that are very rare or very frequent, BinarizedEmb just omit most of the features. This is reasonable also for the very frequent words, since they usually have rich and diverse context distributions and their embeddings cannot be well learned by our models (Huang et al., 2012). 0.70 Table 4: Running time of different features on a Intel(R) Xeon(R) E5620 2.40GHz machine. to accelerate the DistPrototype, by increasing the threshold of DistSim(z, w). However, this is indeed an issue of trade-off between efficiency and accuracy. ● ● 0.60 Sparsity 0.65 ● ● ● ● Analysis 0.55 5.3 In this section, we conduct analyses to show the reasons for the improvements. ● ● ● 0.50 256 5.3.1 ● ● Rare words 1k 4k 16k 64k Frequency of word in unlabeled data As discussed by Turian et al. (2010), much of the NER F1 is derived from decisions regarding rare words. Therefore, in order to show"
D14-1012,W03-0419,0,0.0367445,"Missing"
D14-1012,P08-1068,0,0.70957,"tributional prototype approach performs the best. Moreover, the combination of the approaches provides additive improvements, outperforming the dense and continuous embedding features by nearly 2 points of F1 score. 1 Introduction Learning generalized representation of words is an effective way of handling data sparsity caused by high-dimensional lexical features in NLP systems, such as named entity recognition (NER) and dependency parsing. As a typical lowdimensional and generalized word representation, Brown clustering of words has been studied for a long time. For example, Liang (2005) and Koo et al. (2008) used the Brown cluster features for semi-supervised learning of various NLP tasks and achieved significant improvements. ∗ • Are the continuous embedding features fit for the generalized linear models that are most widely adopted in NLP? • How can the generalized linear models better utilize the embedding features? According to the results provided by Turian et 1 Generalized linear models refer to the models that describe the data as a combination of linear basis functions, either directly in the input variables space or through some transformation of the probability distributions (e.g., logl"
D14-1012,P10-1040,0,0.526341,"on is on, word embedding preserves rich linguistic regularities of words with each dimension hopefully representing a latent feature. Similar words are expected to be distributed close to one another in the embedding space. Consequently, word embeddings can be beneficial for a variety of NLP applications in different ways, among which the most simple and general way is to be fed as features to enhance existing supervised NLP systems. Previous work has demonstrated effectiveness of the continuous word embedding features in several tasks such as chunking and NER using generalized linear models (Turian et al., 2010).1 However, there still remain two fundamental problems that should be addressed: Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems, which is regarded as a simple semi-supervised learning mechanism. However, fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain. In this study, we investigate and analyze three different approaches, including a new proposed distributional prototype approach, for utilizing the embedding features. The presen"
D14-1012,P06-1141,0,0.0341421,"Missing"
D14-1012,I13-1183,0,0.226518,"Missing"
D14-1012,W13-5708,1,0.852635,"Missing"
D14-1012,N13-1063,0,0.380291,"ith the one-hot vector of w). The probability of its context word c is then computed using a log-linear function: exp(vc&gt; vw ) &gt; c0 ∈V exp(vc0 vw ) P (c|w; θ) = P Binarization of Embeddings where mean(v) is the mean value of vector v, U+ is a string feature which turns on when the value (Cij ) falls into the upper part of the positive list. Similarly, B− refers to the bottom part of the negative list. The insight behind φ is that we only consider the features with strong opinions (i.e., positive or negative) on each dimension and omit the values close to zero. 3.3 (1) Clustering of Embeddings Yu et al. (2013) introduced clustering embeddings to overcome the disadvantage that word embeddings are not suitable for linear models. They suggested that the high-dimensional cluster features make samples from different classes better separated by linear models. where V is the vocabulary. The parameters θ are vwi , vci for w, c ∈ V and i = 1, ..., d. Then, the 2 The term similar should be viewed depending on the specific task. 112 In this study, we again investigate this approach. Concretely, each word is treated as a single sample. The batch k-means clustering algorithm (Sculley, 2010) is used,3 and each c"
D14-1015,P00-1054,0,0.0612324,"for single languages. Recently, a lot of progress has 142 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tween bilingual embedding representation is considered as score function. The score function should be discriminative between target phrases and other candidate phrases. Our score function is in the form: been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word s"
D14-1015,D10-1005,0,0.02413,"2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tween bilingual embedding representation is considered as score function. The score function should be discriminative between target phrases and other candidate phrases. Our score function is in the form: been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this"
D14-1015,P07-1066,0,0.0239875,"ethods in Natural Language Processing (EMNLP), pages 142–146, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tween bilingual embedding representation is considered as score function. The score function should be discriminative between target phrases and other candidate phrases. Our score function is in the form: been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bil"
D14-1015,D07-1007,0,0.035916,"h as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bilingual semantic embeddings for source content and target phrase, and incorporate it into a phrasebased SMT system to improve translation quality. 3 f (x, y; W, U) = cos(WT x, UT y) where x is contextual feature vector in source sentence, and y is the representation of target phrase, W ∈ R|X|×k , U ∈ R|Y|×k are low rank matrix. In our model, we allow y to be bag-of-words representation. Our embedding model is memoryefficient in that dimensionality of x and y can be very large in practical setting. We use |X |and |Y| means dimensionality o"
D14-1015,N13-1011,0,0.0470984,"Missing"
D14-1015,P14-1066,0,0.232256,"u1 Daxiang Dong1 Wei He1 Xiaoguang Hu1 Dianhai Yu1 Hua Wu1 Haifeng Wang1 Ting Liu2 1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2 Harbin Institute of Technology, Harbin, China wuhaiyang,dongdaxiang,hewei,huxiaoguang,yudianhai, wu hua,wanghaifeng@baidu.com tliu@ir.hit.edu.cn Abstract where labels are automatically generated from phrase-pairs. For each source phrase, the aligned target phrase is marked as the positive label whereas other phrases in our phrase table are treated as negative labels. Different from previous work in bilingual embedding learning(Zou et al., 2013; Gao et al., 2014), our framework is a supervised model that utilizes contextual information in source sentence as features and make use of phrase pairs as weak labels. Bilingual semantic embeddings are trained automatically from our supervised learning task. Our learned bilingual semantic embedding model is used to measure the similarity of phrase pairs which is treated as a feature in decoding. We integrate our learned model into a phrase-based translation system and experimental results indicate that our system significantly outperform the baseline system. On the NIST08 Chinese-English translation task, we o"
D14-1015,P12-1092,0,0.0602901,"guation highly depends on the language model which is trained only on target corpus. To solve this problem, we present to learn context-sensitive bilingual semantic embedding. Our methodology is to train a supervised model 2 Related Work Using vectors to represent word meanings is the essence of vector space models (VSM). The representations capture words’ semantic and syntactic information which can be used to measure semantic similarities by computing distance between the vectors. Although most VSMs represent one word with only one vector, they fail to capture homonymy and polysemy of word. Huang et al. (2012) introduced global document context and multiple word prototypes which distinguishes and uses both local and global context via a joint training objective. Much of the research focus on the task of inducing representations for single languages. Recently, a lot of progress has 142 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tween bilingual embedding representation is considered as score function. The score function should be discriminative between"
D14-1015,P06-2124,0,0.0365331,"uage Processing (EMNLP), pages 142–146, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tween bilingual embedding representation is considered as score function. The score function should be discriminative between target phrases and other candidate phrases. Our score function is in the form: been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bilingual semantic embedding"
D14-1015,D13-1141,0,0.491805,"ng Model Haiyang Wu1 Daxiang Dong1 Wei He1 Xiaoguang Hu1 Dianhai Yu1 Hua Wu1 Haifeng Wang1 Ting Liu2 1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2 Harbin Institute of Technology, Harbin, China wuhaiyang,dongdaxiang,hewei,huxiaoguang,yudianhai, wu hua,wanghaifeng@baidu.com tliu@ir.hit.edu.cn Abstract where labels are automatically generated from phrase-pairs. For each source phrase, the aligned target phrase is marked as the positive label whereas other phrases in our phrase table are treated as negative labels. Different from previous work in bilingual embedding learning(Zou et al., 2013; Gao et al., 2014), our framework is a supervised model that utilizes contextual information in source sentence as features and make use of phrase pairs as weak labels. Bilingual semantic embeddings are trained automatically from our supervised learning task. Our learned bilingual semantic embedding model is used to measure the similarity of phrase pairs which is treated as a feature in decoding. We integrate our learned model into a phrase-based translation system and experimental results indicate that our system significantly outperform the baseline system. On the NIST08 Chinese-English tra"
D14-1015,N12-1005,0,0.0245693,"n bilingual embedding representation is considered as score function. The score function should be discriminative between target phrases and other candidate phrases. Our score function is in the form: been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bilingual semantic embeddings for source content and target phrase, and incorporate it into a phrasebased SMT system to improve translation quality. 3"
D14-1015,J03-1002,0,0.00507761,"mantic model into the phrase-based SMT system. Experimental results show that our method achieves significant improvements over the baseline on large scale Chinese-English translation task. Our model is memory-efficient and practical for industrial usage that training can be done on large scale data set with large number of classes. Prediction time is also negligible with regard to SMT decoding phase. In the future, we will explore more features to refine the model and try to utilize contextual information in target sentences. For word alignment, we align all of the training data with GIZA++ (Och and Ney, 2003), using the grow-diag-final heuristic to improve recall. For language model, we train a 5-gram modified Kneser-Ney language model and use Minimum Error Rate Training (Och, 2003) to tune the SMT. For both OpenMT08 task and WebData task, we use NIST06 as the tuning set, and use NIST08 as the testing set. Our baseline system is a standard phrase-based SMT system, and a language model is trained with the target side of bilingual corpus. Results on Chinese-English translation task are reported in Table 1. Word position features and partof-speech tagging features are both useful for our bilingual se"
D14-1015,P03-1021,0,0.0157548,"ask. Our model is memory-efficient and practical for industrial usage that training can be done on large scale data set with large number of classes. Prediction time is also negligible with regard to SMT decoding phase. In the future, we will explore more features to refine the model and try to utilize contextual information in target sentences. For word alignment, we align all of the training data with GIZA++ (Och and Ney, 2003), using the grow-diag-final heuristic to improve recall. For language model, we train a 5-gram modified Kneser-Ney language model and use Minimum Error Rate Training (Och, 2003) to tune the SMT. For both OpenMT08 task and WebData task, we use NIST06 as the tuning set, and use NIST08 as the testing set. Our baseline system is a standard phrase-based SMT system, and a language model is trained with the target side of bilingual corpus. Results on Chinese-English translation task are reported in Table 1. Word position features and partof-speech tagging features are both useful for our bilingual semantic embedding learning. Based on our trained bilingual embedding model, we can easily compute a translation score between any bilingual phrase pair. We list some cases in tab"
D14-1015,N10-1135,0,0.0461665,"Missing"
D14-1016,P03-1021,0,0.010994,"ested our system on WMT test sets from 2010 to 2013. The baseline systems are trained on the training corpus with initial word alignment, which was obtained via GIZA++ and “grow-diag-final” method. Based on the initial word alignment, we computed word translation probabilities and used the proposed method to obtain a refined word alignment. Then we used the refined word alignment to train our SMT systems. The translation results are evaluated by caseinsensitive BLEU-4 (Papineni et al., 2002). The feature weights of the translation system are tuned with the standard minimum-error-ratetraining (Och, 2003) to maximize the systems BLEU score on the development set. Experiment To demonstrate the effect of the proposed method, we use the state-of-the-art phrase-based system and hierarchical phrase-based system implemented in Moses (Koehn et al., 2007). The phrasebased system uses continuous phrase pair as the main translation knowledge. While the hierarchical phrase-based system uses both continuous and discontinuous phrase pairs, which has an ability to capture long distance phrase reordering. we carried out experiments on two translation tasks: the Chinese-to-English task comes from the NIST Ope"
D14-1016,J93-2003,0,0.0521236,"Continuous Word Alignment Improves Translation Quality Zhongjun He1 Hua Wu1 Haifeng Wang1 Ting Liu2 1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2 Harbin Institute of Technology, Harbin, China {hezhongjun,wu hua,wanghaifeng}@baidu.com tliu@ir.hit.edu.cn Abstract of the word alignment has big impact on the quality of translation output. Word alignments are usually automatically obtained from a large amount of bilingual training corpus. The most widely used toolkit for word alignment in SMT community is GIZA++ (Och and Ney, 2004), which implements the well known IBM models (Brown et al., 1993) and the HMM model (Vogel and Ney, 1996). Koehn et al. (2003) proposed some heuristic methods (e.g. the “grow-diag-final” method) to refine word alignments trained by GIZA++. Another group of word alignment methods (Liu et al., 2005; Moore et al., 2006; Riesa and Marcu, 2010) define feature functions to describe word alignment. They need manually aligned bilingual texts to train the model. However, the manually annotated data is too expensive to be available for all languages. Although these models reported high accuracy, the GIZA++ and “grow-diag-final” method are dominant in practice. Howeve"
D14-1016,P02-1040,0,0.0889285,"h translation quality of the phrase-based system. the shared translation task 2013. We used WMT08 as the development set and tested our system on WMT test sets from 2010 to 2013. The baseline systems are trained on the training corpus with initial word alignment, which was obtained via GIZA++ and “grow-diag-final” method. Based on the initial word alignment, we computed word translation probabilities and used the proposed method to obtain a refined word alignment. Then we used the refined word alignment to train our SMT systems. The translation results are evaluated by caseinsensitive BLEU-4 (Papineni et al., 2002). The feature weights of the translation system are tuned with the standard minimum-error-ratetraining (Och, 2003) to maximize the systems BLEU score on the development set. Experiment To demonstrate the effect of the proposed method, we use the state-of-the-art phrase-based system and hierarchical phrase-based system implemented in Moses (Koehn et al., 2007). The phrasebased system uses continuous phrase pair as the main translation knowledge. While the hierarchical phrase-based system uses both continuous and discontinuous phrase pairs, which has an ability to capture long distance phrase re"
D14-1016,P05-1033,0,0.501326,"glish translation tasks. Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems. Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system, and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system. 1 Introduction Word alignment, indicating the correspondence between the source and target words in bilingual sentences, plays an important role in statistical machine translation (SMT). Almost all of the SMT models, not only phrase-based (Koehn et al., 2003), but also syntax-based (Chiang, 2005; Liu et al., 2006; Huang et al., 2006), derive translation knowledge from large amount bilingual text annotated with word alignment. Therefore, the quality 147 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 147–152, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 1 {I meiguo The 1 2 ´ shi United 2 3 ê shaoshu States 3 was 4 4 A‡ jige among 5 5 Ý tou the 6 6 e xia 7 ‡é fandui handful 7 of 8 8 ¦ piao nations 9 9 10 I[ guojia de that 10 cast 11 a 12 11 ƒ˜ zhiyi nay 13 note 14 Figure 1: An example of word al"
D14-1016,P10-1017,0,0.0192555,"n Abstract of the word alignment has big impact on the quality of translation output. Word alignments are usually automatically obtained from a large amount of bilingual training corpus. The most widely used toolkit for word alignment in SMT community is GIZA++ (Och and Ney, 2004), which implements the well known IBM models (Brown et al., 1993) and the HMM model (Vogel and Ney, 1996). Koehn et al. (2003) proposed some heuristic methods (e.g. the “grow-diag-final” method) to refine word alignments trained by GIZA++. Another group of word alignment methods (Liu et al., 2005; Moore et al., 2006; Riesa and Marcu, 2010) define feature functions to describe word alignment. They need manually aligned bilingual texts to train the model. However, the manually annotated data is too expensive to be available for all languages. Although these models reported high accuracy, the GIZA++ and “grow-diag-final” method are dominant in practice. However, automatic word alignments are usually very noisy. The example in Figure 1 shows a Chinese and English sentence pair, with word alignment automatically trained by GIZA++ and the “grow-diag-final” method. We find many errors (dashed links) are caused by discontinuous alignme"
D14-1016,C96-2141,0,0.376364,"slation Quality Zhongjun He1 Hua Wu1 Haifeng Wang1 Ting Liu2 1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2 Harbin Institute of Technology, Harbin, China {hezhongjun,wu hua,wanghaifeng}@baidu.com tliu@ir.hit.edu.cn Abstract of the word alignment has big impact on the quality of translation output. Word alignments are usually automatically obtained from a large amount of bilingual training corpus. The most widely used toolkit for word alignment in SMT community is GIZA++ (Och and Ney, 2004), which implements the well known IBM models (Brown et al., 1993) and the HMM model (Vogel and Ney, 1996). Koehn et al. (2003) proposed some heuristic methods (e.g. the “grow-diag-final” method) to refine word alignments trained by GIZA++. Another group of word alignment methods (Liu et al., 2005; Moore et al., 2006; Riesa and Marcu, 2010) define feature functions to describe word alignment. They need manually aligned bilingual texts to train the model. However, the manually annotated data is too expensive to be available for all languages. Although these models reported high accuracy, the GIZA++ and “grow-diag-final” method are dominant in practice. However, automatic word alignments are usually"
D14-1016,2006.amta-papers.8,0,0.030158,"ental results show statistically significant improvements of BLEU score in both cases over the baseline systems. Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system, and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system. 1 Introduction Word alignment, indicating the correspondence between the source and target words in bilingual sentences, plays an important role in statistical machine translation (SMT). Almost all of the SMT models, not only phrase-based (Koehn et al., 2003), but also syntax-based (Chiang, 2005; Liu et al., 2006; Huang et al., 2006), derive translation knowledge from large amount bilingual text annotated with word alignment. Therefore, the quality 147 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 147–152, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 1 {I meiguo The 1 2 ´ shi United 2 3 ê shaoshu States 3 was 4 4 A‡ jige among 5 5 Ý tou the 6 6 e xia 7 ‡é fandui handful 7 of 8 8 ¦ piao nations 9 9 10 I[ guojia de that 10 cast 11 a 12 11 ƒ˜ zhiyi nay 13 note 14 Figure 1: An example of word alignment between a Chinese and English s"
D14-1016,D07-1103,0,0.0248816,"tandard definition of phrase in SMT, phrase pairs cannot be extracted from the discontinuous alignments. By transforming discontinuous alignments into continuous alignment, we can extract more phrase pairs. Table 7 shows the number of standard phrases and hierarchical phrases extracted from the initial and refined word alignments. We find that the number of both phrases and hierarchical phrases grows heavily. This is because that the word alignment constraint for phrase extraction is loosed by removing noisy links. Although the phrase table becomes larger, fortunately, there are some methods (Johnson et al., 2007; He et al., 2009) to prune phrase table without hurting translation quality. For further illustration, we compare the phrase pairs extracted from the initial alignment and refined alignment in Figure 1. From the initial alignments, we extracted only 3 standard phrase pairs and no hierarchical phrase pairs (Table 8). After discarding noisy alignments (dashed links) by using the proposed method, we extracted 21 standard phrase pairs and 36 hierarchical phrases. Table 9 and Table 10 show selected phrase pairs and hierarchical phrase pairs, respectively. Acknowlegement This paper is supported by"
D14-1016,N03-1017,0,0.0381442,"1 Haifeng Wang1 Ting Liu2 1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2 Harbin Institute of Technology, Harbin, China {hezhongjun,wu hua,wanghaifeng}@baidu.com tliu@ir.hit.edu.cn Abstract of the word alignment has big impact on the quality of translation output. Word alignments are usually automatically obtained from a large amount of bilingual training corpus. The most widely used toolkit for word alignment in SMT community is GIZA++ (Och and Ney, 2004), which implements the well known IBM models (Brown et al., 1993) and the HMM model (Vogel and Ney, 1996). Koehn et al. (2003) proposed some heuristic methods (e.g. the “grow-diag-final” method) to refine word alignments trained by GIZA++. Another group of word alignment methods (Liu et al., 2005; Moore et al., 2006; Riesa and Marcu, 2010) define feature functions to describe word alignment. They need manually aligned bilingual texts to train the model. However, the manually annotated data is too expensive to be available for all languages. Although these models reported high accuracy, the GIZA++ and “grow-diag-final” method are dominant in practice. However, automatic word alignments are usually very noisy. The exam"
D14-1016,P07-2045,0,0.0254523,"any external knowledge, as the word translation probabilities can be estimated from the bilingual corpus with the original word alignment. We notice that the discontinuous alignment is helpful for hierarchical phrase-based model, as the model allows discontinuous phrases. Thus, for the hierarchical phrase-based model, our method may lost some discontinuous phrases. To solve the problem, we keep the original discontinuous alignment in the training corpus. We carry out experiment with the state-of-theart phrase-based and hierarchical phrase-based (Chiang, 2005) SMT systems implemented in Moses (Koehn et al., 2007). Experiments on large scale Chinese-to-English and German-to-English translation tasks demonstrate significant improvements in both cases over the baseline systems. 2 into several continuous groups, and select the best group with the highest score computed by word translation probabilities as the final alignment. For further understanding, we first describe some definitions. Given a word-aligned sentence pair (F1I , E1J , A), an alignment set Aset (i) is the set of target word positions that aligned to the source word Fii : Aset (i) = {j|(i, j) ∈ A} For example, in Figure 1, the alignment set"
D14-1016,P05-1057,0,0.0305017,"nghaifeng}@baidu.com tliu@ir.hit.edu.cn Abstract of the word alignment has big impact on the quality of translation output. Word alignments are usually automatically obtained from a large amount of bilingual training corpus. The most widely used toolkit for word alignment in SMT community is GIZA++ (Och and Ney, 2004), which implements the well known IBM models (Brown et al., 1993) and the HMM model (Vogel and Ney, 1996). Koehn et al. (2003) proposed some heuristic methods (e.g. the “grow-diag-final” method) to refine word alignments trained by GIZA++. Another group of word alignment methods (Liu et al., 2005; Moore et al., 2006; Riesa and Marcu, 2010) define feature functions to describe word alignment. They need manually aligned bilingual texts to train the model. However, the manually annotated data is too expensive to be available for all languages. Although these models reported high accuracy, the GIZA++ and “grow-diag-final” method are dominant in practice. However, automatic word alignments are usually very noisy. The example in Figure 1 shows a Chinese and English sentence pair, with word alignment automatically trained by GIZA++ and the “grow-diag-final” method. We find many errors (dashe"
D14-1016,P06-1077,0,0.0327986,"ion tasks. Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems. Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system, and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system. 1 Introduction Word alignment, indicating the correspondence between the source and target words in bilingual sentences, plays an important role in statistical machine translation (SMT). Almost all of the SMT models, not only phrase-based (Koehn et al., 2003), but also syntax-based (Chiang, 2005; Liu et al., 2006; Huang et al., 2006), derive translation knowledge from large amount bilingual text annotated with word alignment. Therefore, the quality 147 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 147–152, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 1 {I meiguo The 1 2 ´ shi United 2 3 ê shaoshu States 3 was 4 4 A‡ jige among 5 5 Ý tou the 6 6 e xia 7 ‡é fandui handful 7 of 8 8 ¦ piao nations 9 9 10 I[ guojia de that 10 cast 11 a 12 11 ƒ˜ zhiyi nay 13 note 14 Figure 1: An example of word alignment between a"
D14-1016,P06-1065,0,0.0610096,"Missing"
D14-1016,J04-4002,0,0.277293,"Missing"
D14-1174,2008.iwslt-papers.1,0,0.0679481,"mental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems. 1 Introduction Statistical Machine Translation (SMT) relies on large bilingual parallel data to produce high quality translation results. Unfortunately, for some language pairs, large bilingual corpora are not readily available. To alleviate the parallel data scarceness, a conventional solution is to introduce a “bridge” language (named pivot language) to connect the source and target language (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Paul et al., 2011; El Kholy et al., 2013; Zahabi et al., 2013), where there are large amounts of source-pivot and pivot-target parallel corpora. Among various pivot-based approaches, the triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007) is a representative work in * pivot-based machine translation. The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table. One of the key issues in this method is to estimate the translation probabilities for the generated source-target phrase pairs. Conventionally, the probabilities are"
D14-1174,P07-1092,0,0.593184,"igh quality translation results. Unfortunately, for some language pairs, large bilingual corpora are not readily available. To alleviate the parallel data scarceness, a conventional solution is to introduce a “bridge” language (named pivot language) to connect the source and target language (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Paul et al., 2011; El Kholy et al., 2013; Zahabi et al., 2013), where there are large amounts of source-pivot and pivot-target parallel corpora. Among various pivot-based approaches, the triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007) is a representative work in * pivot-based machine translation. The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table. One of the key issues in this method is to estimate the translation probabilities for the generated source-target phrase pairs. Conventionally, the probabilities are estimated by multiplying the posterior probabilities of source-pivot and pivottarget phrase pairs. However, it has been shown that the generated probabilities are not accurate enough (Cui et al., 2013). One possible reason may lie"
D14-1174,I11-1154,0,0.515643,"t translation model. See Figure 2. (b) and (c) for further illustration. 1666 The remainder of this paper is organized as follows. In Section 2, we describe the related work. We introduce the co-occurrence count method in Section 3, and the mixed model in Section 4. In Section 5 and Section 6, we describe and analyze the experiments. Section 7 gives a conclusion of the paper. 2 Related Work Several methods have been proposed for pivotbased translation. Typically, they can be classified into 3 kinds as follows: Transfer Method: The transfer method (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011) connects two translation systems: a source-pivot MT system and a pivottarget MT system. Given a source sentence, (1) the source-pivot MT system translates it into the pivot language, (2) and the pivot-target MT system translates the pivot sentence into the target sentence. During each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayes-risk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). The problem with the transfer method is that it needs to decode twice. On o"
D14-1174,I11-1153,0,0.0607625,"Missing"
D14-1174,P11-1127,0,0.0142768,"s as follows: Transfer Method: The transfer method (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011) connects two translation systems: a source-pivot MT system and a pivottarget MT system. Given a source sentence, (1) the source-pivot MT system translates it into the pivot language, (2) and the pivot-target MT system translates the pivot sentence into the target sentence. During each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayes-risk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). The problem with the transfer method is that it needs to decode twice. On one hand, the time cost is doubled; on the other hand, the translation error of the source-pivot translation system will be transferred to the pivot-target translation. Synthetic Method: It aims to create a synthetic source-target corpus by: (1) translate the pivot part in source-pivot corpus into target language with a pivot-target model; (2) translate the pivot part in pivot-target corpus into source language with a pivot-source model; (3) combine the source sentences with translated target sentenc"
D14-1174,N03-1017,0,0.372818,"the co-occurrence count of source-target phrase pairs to estimate phrase translation probabilities more precisely. Different from the triangulation method, which merges the source-pivot and pivot-target phrase pairs after training the translation model, we propose to merge the source-pivot and pivot-target phrase pairs immediately after the phrase extraction step, and estimate the co-occurrence count of the source-pivot-target phrase pairs. Finally, we compute the translation probabilities according to the estimated co-occurrence counts, using the standard training method in phrase-based SMT (Koehn et al., 2003). As Figure 1. (b) shows, the This work was done when the first author was visiting Baidu. 1665 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1665–1675, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (a) the triangulation method (b) the co-occurrence count method Figure 1: An example of probability space evolution in pivot translation. Standard ST corpus Large SP corpus Large PT corpus Standard ST corpus Large SP corpus Phrase Extraction Phrase Extraction Phrase Extraction SP phrase pairs PT phrase pair"
D14-1174,W04-3250,0,0.269372,"Missing"
D14-1174,2005.mtsummit-papers.11,0,0.294746,"Missing"
D14-1174,2010.iwslt-papers.12,0,0.0373385,"Missing"
D14-1174,C00-2163,0,0.0997045,"parison of different merging methods on out-of-domain test set. 5 Experiments on Europarl Corpus Our first experiment is carried out on Europarl1 corpus, which is a multi-lingual corpus including 21 European languages (Koehn, 2005). In our work, we perform translations among French (fr), German (de) and Spanish (es). Due to the richness of available language resources, we choose English (en) as the pivot language. Table 1 summarized the statistics of training data. For the language model, the same monolingual data extracted from the Europarl are used. The word alignment is obtained by GIZA++ (Och and Ney, 2000) and the heuristics “growdiag-final” refinement rule (Koehn et al., 2003). Our translation system is an in-house phrasebased system analogous to Moses (Koehn et al., 2007). The baseline system is the triangulation method (Wu and Wang, 2007), including an interpolated model which linearly interpolate the direct and pivot translation model. 1 http://www.statmt.org/europarl We use WMT082 as our test data, which contains 2000 in-domain sentences and 2051 out-ofdomain sentences with single reference. The translation results are evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). The"
D14-1174,W11-2601,0,0.017343,"arl data and web data show that our method leads to significant improvements over the baseline systems. 1 Introduction Statistical Machine Translation (SMT) relies on large bilingual parallel data to produce high quality translation results. Unfortunately, for some language pairs, large bilingual corpora are not readily available. To alleviate the parallel data scarceness, a conventional solution is to introduce a “bridge” language (named pivot language) to connect the source and target language (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Paul et al., 2011; El Kholy et al., 2013; Zahabi et al., 2013), where there are large amounts of source-pivot and pivot-target parallel corpora. Among various pivot-based approaches, the triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007) is a representative work in * pivot-based machine translation. The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table. One of the key issues in this method is to estimate the translation probabilities for the generated source-target phrase pairs. Conventionally, the probabilities are estimated by multi"
D14-1174,I11-1091,0,0.0343951,"Missing"
D14-1174,P02-1040,0,0.0885743,"Missing"
D14-1174,E12-1015,0,0.0948513,"Missing"
D14-1174,P07-2050,0,0.0731859,"Missing"
D14-1174,N07-1061,0,0.872022,"ce-pivot and pivot-target phrase pairs. Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems. 1 Introduction Statistical Machine Translation (SMT) relies on large bilingual parallel data to produce high quality translation results. Unfortunately, for some language pairs, large bilingual corpora are not readily available. To alleviate the parallel data scarceness, a conventional solution is to introduce a “bridge” language (named pivot language) to connect the source and target language (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Paul et al., 2011; El Kholy et al., 2013; Zahabi et al., 2013), where there are large amounts of source-pivot and pivot-target parallel corpora. Among various pivot-based approaches, the triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007) is a representative work in * pivot-based machine translation. The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table. One of the key issues in this method is to estimate the translation probabilities for the generated source-target phrase pa"
D14-1174,2008.iwslt-evaluation.11,0,0.0527301,"Missing"
D14-1174,P07-1108,1,0.954596,"hrase pairs. Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems. 1 Introduction Statistical Machine Translation (SMT) relies on large bilingual parallel data to produce high quality translation results. Unfortunately, for some language pairs, large bilingual corpora are not readily available. To alleviate the parallel data scarceness, a conventional solution is to introduce a “bridge” language (named pivot language) to connect the source and target language (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Paul et al., 2011; El Kholy et al., 2013; Zahabi et al., 2013), where there are large amounts of source-pivot and pivot-target parallel corpora. Among various pivot-based approaches, the triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007) is a representative work in * pivot-based machine translation. The approach proposes to build a source-target phrase table by merging the source-pivot and pivot-target phrase table. One of the key issues in this method is to estimate the translation probabilities for the generated source-target phrase pairs. Conventionally"
D14-1174,P09-1018,1,0.899443,"one hand, the time cost is doubled; on the other hand, the translation error of the source-pivot translation system will be transferred to the pivot-target translation. Synthetic Method: It aims to create a synthetic source-target corpus by: (1) translate the pivot part in source-pivot corpus into target language with a pivot-target model; (2) translate the pivot part in pivot-target corpus into source language with a pivot-source model; (3) combine the source sentences with translated target sentences or/and combine the target sentences with translated source sentences (Utiyama et al., 2008; Wu and Wang, 2009). However, it is difficult to build a high quality translation system with a corpus created by a machine translation system. Triangulation Method: The triangulation method obtains source-target phrase table by merging source-pivot and pivot-target phrase table entries with identical pivot language phrases and multiplying corresponding posterior probabilities (Wu and Wang, 2007; Cohn and Lapata, 2007), which has been shown to work better than the other pivot approaches (Utiyama and Isahara, 2007). A problem of this approach is that the probability space of the source-target phrase pairs is non-"
D14-1174,2008.iwslt-evaluation.18,1,\N,Missing
D14-1174,P07-2045,0,\N,Missing
D14-1174,C08-2032,0,\N,Missing
D14-1174,P13-2057,0,\N,Missing
D14-1174,I13-1167,0,\N,Missing
D14-1174,P13-2073,0,\N,Missing
D14-1174,2009.mtsummit-papers.7,0,\N,Missing
D17-1134,D15-1262,0,0.269261,"text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 200"
D17-1134,D16-1020,0,0.10315,"Missing"
D17-1134,P16-1163,0,0.391635,"many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition. It performs tw"
D17-1134,Q15-1024,0,0.123199,"ognition on natural (i.e., genuine) discourse data with the use of traditional NLP techniques to extract linguistically informed features and traditional machine learning algorithms (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015). Later, to make a full use of unlabelled data, several studies performed multi-task or unsupervised learning methods (Lan et al., 2013; Braud and Denis, 2015; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Recently, with the development of deep learning, researchers resorted to neural networks methods (Ji and Eisenstein, 2015; Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). 5.2 Multi-task learning Multi-task learning framework adopts traditional machine learning with human-selected effective knowledge and the shared part is integrated into the cost function to prefer the main task learning. (Collobert and Weston, 2008) proposed a multitask neural network trained jointly on the relevant tasks using weight-sharing (sharing the word embeddings with tasks). (Liu et al., 2016a) proposed the multi-task neural network by modifying the"
D17-1134,P13-1047,1,0.763423,"isher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition. It performs two types of representation learning at the same time. An attention-based neural network conducts discourse relationship representation learning through interaction between two discourse arguments. Meanwhile, a multi-task learning framework leverages knowledge from auxiliary task to enhance the performance of main task. Furthermore, these two types of learn"
D17-1134,D09-1036,0,0.0378244,"Missing"
D17-1134,D16-1130,0,0.582461,"analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition. It performs two types of representation learning at the same time. An"
D17-1134,P09-1077,0,0.0836555,"tion (or rhetorical relation) identification is to recognize how two adjacent text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognit"
D17-1134,prasad-etal-2008-penn,0,0.0261204,"r proposed model outperforms the state-of-the-art systems on benchmark corpora. 1 Introduction The task of implicit discourse relation (or rhetorical relation) identification is to recognize how two adjacent text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the sh"
D17-1134,C16-1180,0,0.627907,", QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition. It performs two types of representation learning at"
D17-1134,D16-1246,0,0.534199,", QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition. It performs two types of representation learning at"
D17-1134,N15-1081,0,0.107539,"ask. 5 5.1 Related Work Implicit Discourse With the release of PDTB 2.0, a number of studies performed discourse relation recognition on natural (i.e., genuine) discourse data with the use of traditional NLP techniques to extract linguistically informed features and traditional machine learning algorithms (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015). Later, to make a full use of unlabelled data, several studies performed multi-task or unsupervised learning methods (Lan et al., 2013; Braud and Denis, 2015; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Recently, with the development of deep learning, researchers resorted to neural networks methods (Ji and Eisenstein, 2015; Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). 5.2 Multi-task learning Multi-task learning framework adopts traditional machine learning with human-selected effective knowledge and the shared part is integrated into the cost function to prefer the main task learning. (Collobert and Weston, 2008) proposed a multitask neural network trained jointly on the relevant tasks using weight-sh"
D17-1134,K16-2007,0,0.175529,"Missing"
D17-1134,P16-1044,0,0.0113504,"Wu et al., 2016) use bilingually-constrained synthetic implicit data for implicit discourse relation recognition a multi-task neural network. (Liu et al., 2016b) propose a convolutional neural network embedded multi-task learning system to improve the performance of implicit discourse identification. 5.3 Deep learning with Attention Recently deep learning with attention has been widely adopted by NLP researchers. (Zhou et al., 2016) proposed an attention-based Bi-LSTM for relation classification. (Wang et al., 2016c) proposed an attention-based LSTM for aspect-level sentiment classification. (Tan et al., 2016) proposed a attentive LSTMs for Question Answer Matching. (Wang et al., 2016a) proposed an inner attention based RNN (add attention information before RNN hidden representation) for Answer Selection in QA. (Wang et al., 2016b) proposed multi-level attention CNNs for relation classification. (Yin et al., 2016) proposed an attentive convolutional neural network for QA. 6 Concluding Remarks We present a novel multi-task attention-based neural network model for implicit discourse relationship representation and identification. Our method captures both the discourse relationships through interactio"
D17-1134,P16-1122,0,0.0143279,"ed system which can effectively use synthetic data for implicit discourse relation recognition. (Wu et al., 2016) use bilingually-constrained synthetic implicit data for implicit discourse relation recognition a multi-task neural network. (Liu et al., 2016b) propose a convolutional neural network embedded multi-task learning system to improve the performance of implicit discourse identification. 5.3 Deep learning with Attention Recently deep learning with attention has been widely adopted by NLP researchers. (Zhou et al., 2016) proposed an attention-based Bi-LSTM for relation classification. (Wang et al., 2016c) proposed an attention-based LSTM for aspect-level sentiment classification. (Tan et al., 2016) proposed a attentive LSTMs for Question Answer Matching. (Wang et al., 2016a) proposed an inner attention based RNN (add attention information before RNN hidden representation) for Answer Selection in QA. (Wang et al., 2016b) proposed multi-level attention CNNs for relation classification. (Yin et al., 2016) proposed an attentive convolutional neural network for QA. 6 Concluding Remarks We present a novel multi-task attention-based neural network model for implicit discourse relationship represent"
D17-1134,K16-2004,1,0.887562,"Missing"
D17-1134,P16-1123,0,0.0214013,"Missing"
D17-1134,P10-1073,0,0.0615245,"ation is to recognize how two adjacent text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine lea"
D17-1134,D16-1058,0,0.0138575,"ed system which can effectively use synthetic data for implicit discourse relation recognition. (Wu et al., 2016) use bilingually-constrained synthetic implicit data for implicit discourse relation recognition a multi-task neural network. (Liu et al., 2016b) propose a convolutional neural network embedded multi-task learning system to improve the performance of implicit discourse identification. 5.3 Deep learning with Attention Recently deep learning with attention has been widely adopted by NLP researchers. (Zhou et al., 2016) proposed an attention-based Bi-LSTM for relation classification. (Wang et al., 2016c) proposed an attention-based LSTM for aspect-level sentiment classification. (Tan et al., 2016) proposed a attentive LSTMs for Question Answer Matching. (Wang et al., 2016a) proposed an inner attention based RNN (add attention information before RNN hidden representation) for Answer Selection in QA. (Wang et al., 2016b) proposed multi-level attention CNNs for relation classification. (Yin et al., 2016) proposed an attentive convolutional neural network for QA. 6 Concluding Remarks We present a novel multi-task attention-based neural network model for implicit discourse relationship represent"
D17-1134,D16-1253,0,0.70684,"Missing"
D17-1134,C16-1164,0,0.0159977,"Missing"
D17-1134,D15-1266,0,0.09134,"tion and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Collobert and Weston, 2008; Lan et al., 2013) or recently in neural network framework (Wu et al., 2016; Liu et al., 2016b). In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognit"
D17-1134,P16-2034,0,0.00957092,"k for text classification tasks. (Lan et al., 2013) present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. (Wu et al., 2016) use bilingually-constrained synthetic implicit data for implicit discourse relation recognition a multi-task neural network. (Liu et al., 2016b) propose a convolutional neural network embedded multi-task learning system to improve the performance of implicit discourse identification. 5.3 Deep learning with Attention Recently deep learning with attention has been widely adopted by NLP researchers. (Zhou et al., 2016) proposed an attention-based Bi-LSTM for relation classification. (Wang et al., 2016c) proposed an attention-based LSTM for aspect-level sentiment classification. (Tan et al., 2016) proposed a attentive LSTMs for Question Answer Matching. (Wang et al., 2016a) proposed an inner attention based RNN (add attention information before RNN hidden representation) for Answer Selection in QA. (Wang et al., 2016b) proposed multi-level attention CNNs for relation classification. (Yin et al., 2016) proposed an attentive convolutional neural network for QA. 6 Concluding Remarks We present a novel multi-tas"
D17-1134,C10-2172,1,0.40213,"ze how two adjacent text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast). It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc. With the release of PDTB 2.0 (Prasad et al., 2008), lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data (Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Braud and Denis, 2015; Fisher and Simmons, 2015) with the use of traditional NLP linguistically informed features and machine learning algorithms. Recently, more and more researchers resorted to neural networks for implicit discourse recognition (Zhang et al., 2015; Chen et al., 2016; Liu et al., 2016b; Qin et al., 2016a; Liu and Li, 2016; Braud and Denis, 2016; Wu et al., 2016). Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework (Co"
D19-1079,D16-1139,0,0.123117,"Missing"
D19-1079,N16-1046,0,0.140054,"Missing"
D19-1079,P18-1129,0,0.305262,"nse disambiguation (Tang et al., 2018; Domhan, 2018). In this paper, we investigate the effects of two teams of multi-agents: a team of alternative agents mentioned above, and a uniform team of different initialization for the same model. • We extend the study on training with two agents to the multi-agent scenario, and propose a general learning strategy to train multiple agents. To resolve the second problem, we simplify the many-to-many learning to the one-to-many (one teacher vs. many students) learning, extending ensemble knowledge distillation (Fukuda et al., 2017; Freitag et al., 2017; Liu et al., 2018; Zhu et al., 2018). During the training, each agent performs better by learning from the ensemble model (Teacher) of all agents integrating the knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016) into the training objective. This procedure can be viewed as the introduction of an additional regularization term into the training objective, with which each agent can learn advantages from the ensemble model gradually. With this method, each agent is optimized not only to the maximization of the likelihood of the training data, but also to the minimization of the divergence between it"
D19-1079,E17-2060,0,0.0560241,"Missing"
D19-1079,P18-1167,0,0.02985,"requires an effective learning strategy. There have been many alternatives to improve the diversity of models even based on the Transformer model (Vaswani et al., 2017). For example, decoding in the opposite direction usually results in different preferences: good prefixes and bad prefixes (Zhang et al., 2019b). Rather, selfattention with relative position representations enhances the generalization to sequence lengths unseen during training (Shaw et al., 2018). Furthermore, increasing the size of layers in the encoder is expected to specialize in word sense disambiguation (Tang et al., 2018; Domhan, 2018). In this paper, we investigate the effects of two teams of multi-agents: a team of alternative agents mentioned above, and a uniform team of different initialization for the same model. • We extend the study on training with two agents to the multi-agent scenario, and propose a general learning strategy to train multiple agents. To resolve the second problem, we simplify the many-to-many learning to the one-to-many (one teacher vs. many students) learning, extending ensemble knowledge distillation (Fukuda et al., 2017; Freitag et al., 2017; Liu et al., 2018; Zhu et al., 2018). During the trai"
D19-1079,P16-1162,0,0.0912544,"ss: LiKD ← (q(yt ), p(yti ), Yt , Ysi , Xg , Yg ); Compute NLL loss: LN LL ← (p(yti ), Xg , Yg ) ; Compute Agent loss: Lia ← λLN LL + (1 − λ)LiKD ; end P i Model loss: Lf inal = N i=1 La ; Update gradients for each agent; end until convergence; 4 Experiments In this paper, we evaluate our model on four translation tasks: NIST Chinese-English Translation Task, IWSLT 2014 German-English Translation Task, WMT 2014 English-German Translation Task and large-scale Chinese-English Translation Task. Joint Learning 4.1 Data Preprocessing To compare with previous studies, we conduct byte-pair encoding (Sennrich et al., 2016) for Chinese, English and German sentences, setting the In the work of Zhang et al. (2019b), they proposed a relatively complex joint learning framework for training two agents. In this paper, according to the 860 MODEL Wang et al. (2018) a.L2R b.R2L c.Enc d.Rel a+b a+c a+d c+d a×2 d×2 a+b+c a+b+d a+c+d a×3 d×3 a+b+c+d MT02 MT03 MT04 MT08 Results for Best Agent 46.60 47.73 48.53 47.07 48.43 42.21 47.06 45.58 47.14 41.04 48.86 47.54 48.57 42.93 48.12 48.19 48.33 42.51 48.82 47.65 48.45 42.49 48.79 48.30 49.32 43.44 48.76 48.40 48.74 43.27 49.45 49.01 49.52 43.71 48.64 47.98 49.08 43.07 48.23 48"
D19-1079,P12-1000,0,0.244265,"Missing"
D19-1079,D18-1045,0,0.0255087,"by the pretraining of the i − th agent, and the Bavg is the average BLEU score of all agents. The above formula suggests the agent learns more from the ensemble model as its performance 2 LDC2002E18, LDC2002L27, LDC2002T01, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24, LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92, LDC2006E93, LDC2004T08(HK News, HK Hansards ) 3 https://github.com/paddlepaddle/ paddle 861 Models ConvS2S Gehring et al. (2017) Transformer Vaswani et al. (2017) Rel Shaw et al. (2018) DynamicConv Wu et al. (2019) Back-translation Sergey et al. (2018) Dual-3 Wang et al. (2019) Dual-3 + Mono Data L2R Rel Rel-4 Task L2R Rel KD-4 Dual-5 Rel-4 De-En 33.63 34.91 35.53 34.70 36.27 Table 3: BLEU score on IWSLT 2014 German-English translation. KD-4 stands for ensemble knowledge distillation with four agents. Dual-5 is the SOTA model from the work of Wang et al. (2019). And Rel-4 is our best model (Rel) training with four diverse agents. is worse than the majority vote, rather than focusing on exploring by its own prediction. We train our model with parallelization at data batch level. For NIST Chinese-English task, it takes about 1.5 days to train"
D19-1079,N18-2074,0,0.0640489,"al., 2013) Second, learning in multi-agent scenario is many-to-many, as opposed to the relatively simpler one-to-one learning in two-agent training, and requires an effective learning strategy. There have been many alternatives to improve the diversity of models even based on the Transformer model (Vaswani et al., 2017). For example, decoding in the opposite direction usually results in different preferences: good prefixes and bad prefixes (Zhang et al., 2019b). Rather, selfattention with relative position representations enhances the generalization to sequence lengths unseen during training (Shaw et al., 2018). Furthermore, increasing the size of layers in the encoder is expected to specialize in word sense disambiguation (Tang et al., 2018; Domhan, 2018). In this paper, we investigate the effects of two teams of multi-agents: a team of alternative agents mentioned above, and a uniform team of different initialization for the same model. • We extend the study on training with two agents to the multi-agent scenario, and propose a general learning strategy to train multiple agents. To resolve the second problem, we simplify the many-to-many learning to the one-to-many (one teacher vs. many students)"
D19-1079,D18-1458,0,0.0233259,"Missing"
D19-1079,C18-1124,0,0.0535505,"Missing"
D19-1187,E17-1013,0,0.0246841,"rom Wikipedia or user generated content, some work focus on either modeling of conversation generation with unstructured texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016; Xu et al., 2017), or building benchmark dialogue data grounded on knowledge (Dinan et al., 2019; Moghe et al., 2018). In comparison with them, we adopt a graph based representation scheme for unstructured texts, which enables better explainability and generalization capability of our system. Knowledge Graph Reasoning: Previous studies on KG reasoning can be categorized into three lines, path-based symbolic models (Das et al., 2017a; Lao et al., 2011), embedding-based neural models (Bordes et al., 2013; Wang et al., 2014), and models in unifying embedding and path-based 1783 technology (Das et al., 2018; Lin et al., 2018; Xiong et al., 2017), which can predict missing links for completion of KG. In this work, for knowledge selection on a graph, we follow the third line of works. Furthermore, our problem setting is different from theirs in that some of our vertices contain long texts, which motivates the use of machine reading technology for graph reasoning. Fusion of KG triples and texts: In the task of QA, combination"
D19-1187,P17-2057,0,0.0194558,"rom Wikipedia or user generated content, some work focus on either modeling of conversation generation with unstructured texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016; Xu et al., 2017), or building benchmark dialogue data grounded on knowledge (Dinan et al., 2019; Moghe et al., 2018). In comparison with them, we adopt a graph based representation scheme for unstructured texts, which enables better explainability and generalization capability of our system. Knowledge Graph Reasoning: Previous studies on KG reasoning can be categorized into three lines, path-based symbolic models (Das et al., 2017a; Lao et al., 2011), embedding-based neural models (Bordes et al., 2013; Wang et al., 2014), and models in unifying embedding and path-based 1783 technology (Das et al., 2018; Lin et al., 2018; Xiong et al., 2017), which can predict missing links for completion of KG. In this work, for knowledge selection on a graph, we follow the third line of works. Furthermore, our problem setting is different from theirs in that some of our vertices contain long texts, which motivates the use of machine reading technology for graph reasoning. Fusion of KG triples and texts: In the task of QA, combination"
D19-1187,P18-1138,0,0.133721,"(Ritter et al., 2011; Shang et al., 2015). 1 Data and codes are available at https://github. com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/EMNLP2019-AKGCM However, these models tend to produce generic responses or incoherent responses for a given topic, since it is quite challenging to learn semantic interactions merely from dialogue data without help of background knowledge. Recently, some previous studies have been conducted to introduce external knowledge, either unstructured knowledge texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016) or structured knowledge triples (Liu et al., 2018; Young et al., 2018; Zhou et al., 2018) to help open-domain conversation generation by producing responses conditioned on selected knowledge. In the first research line, their knowledge graph can help narrowing down knowledge candidates for conversation generation with the use of prior information, e.g., triple attributes or graph paths. Moreover, these prior information can enhance generalization capability of knowledge selection models. But it suffers from information insufficiency for response generation since there is simply a single word or entity to facilitate generation. In the second"
D19-1187,D18-1255,0,0.442446,"and integrate more explainable and flexible multi-hop graph reasoning models into conversation systems. Wu et al. (2018) used document reasoning network for modeling of conversational contexts, but not for knowledge selection. Conversation with Unstructured Texts: With availability of a large amount of knowledge texts from Wikipedia or user generated content, some work focus on either modeling of conversation generation with unstructured texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016; Xu et al., 2017), or building benchmark dialogue data grounded on knowledge (Dinan et al., 2019; Moghe et al., 2018). In comparison with them, we adopt a graph based representation scheme for unstructured texts, which enables better explainability and generalization capability of our system. Knowledge Graph Reasoning: Previous studies on KG reasoning can be categorized into three lines, path-based symbolic models (Das et al., 2017a; Lao et al., 2011), embedding-based neural models (Bordes et al., 2013; Wang et al., 2014), and models in unifying embedding and path-based 1783 technology (Das et al., 2018; Lin et al., 2018; Xiong et al., 2017), which can predict missing links for completion of KG. In this work"
D19-1187,P02-1040,0,0.103053,"Missing"
D19-1187,D11-1054,0,0.0255171,"ne reading comprehension technology. We demonstrate the effectiveness of our system on two datasets in comparison with state-of-the-art models1 . 1 Introduction One of the key goals of AI is to build a machine that can talk with humans when given an initial topic. To achieve this goal, the machine should be able to understand language with background knowledge, recall knowledge from memory or external resource, reason about these concepts together, and finally output appropriate and informative responses. Lots of research efforts have been devoted to chitchat oriented conversation generation (Ritter et al., 2011; Shang et al., 2015). 1 Data and codes are available at https://github. com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/EMNLP2019-AKGCM However, these models tend to produce generic responses or incoherent responses for a given topic, since it is quite challenging to learn semantic interactions merely from dialogue data without help of background knowledge. Recently, some previous studies have been conducted to introduce external knowledge, either unstructured knowledge texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016) or structured knowledge triples (Liu et al., 2018; You"
D19-1187,P17-1099,0,0.32399,"P (ASt )), PKS (vd |vX , G, X) = P (AST −1 ). Please see Section 3.1 for definition of PKS (∗). When the agent finally arrives at ST , we obtain vT as the answer vY for response generation. Training: For the policy network (πθ ) described above, we want to find parameters θ that maximize the expected reward: (8) (9) (10) J(θ) = E(v0 ,X,vgt )∼D EA0 ,...,AT −1 ∼πθ [R(ST )|S0 = (v0 , v0 , X, vgt )], (11) where we assume there is a true underlying distribution D, and (v0 , X, vgt ) ∼ D. 3.4 Knowledge Aware Generation Following the work of Moghe et al. (2018), we modify a text summarization model (See et al., 2017) to suit this generation task. In the summarization task, its input is a document and its output is a summary, but in our case the input is a [selected knowledge, message] pair and the output is a response. Therefore we introduce two RNNs: one is for computing the representation of the selected knowledge, and the other for the message. The decoder accepts the two representations and its own internal state representation as input, and then compute (1) a probability score which indicates whether the next word should be generated or copied, (2) a probability distribution over the vocabulary if th"
D19-1187,W15-4616,0,0.135638,"reover, our graph differs from previous KGs in that: some vertices in ours contain long texts, not a single entity or word. To fully leverage this long text information, we improve the reasoning algorithm with machine reading comprehension (MRC) technology (Seo et al., 2017) to conduct fine-grained semantic matching between an input message and candidate vertices. Finally, for response generation, we use an encoder-decoder model to produce responses conditioned on selected knowledge. 2 Related Work Conversation with Knowledge Graph: There are growing interests in leveraging factoid knowledge (Han et al., 2015; Liu et al., 2018; Zhu et al., 2017) or commonsense knowledge (Young et al., 2018; Zhou et al., 2018) with graph based representation for generation of appropriate and informative responses. Compared with them, we augment previous KGs with knowledge texts and integrate more explainable and flexible multi-hop graph reasoning models into conversation systems. Wu et al. (2018) used document reasoning network for modeling of conversational contexts, but not for knowledge selection. Conversation with Unstructured Texts: With availability of a large amount of knowledge texts from Wikipedia or user"
D19-1187,P15-1152,0,0.252756,"Missing"
D19-1187,D11-1049,0,0.020697,"er generated content, some work focus on either modeling of conversation generation with unstructured texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016; Xu et al., 2017), or building benchmark dialogue data grounded on knowledge (Dinan et al., 2019; Moghe et al., 2018). In comparison with them, we adopt a graph based representation scheme for unstructured texts, which enables better explainability and generalization capability of our system. Knowledge Graph Reasoning: Previous studies on KG reasoning can be categorized into three lines, path-based symbolic models (Das et al., 2017a; Lao et al., 2011), embedding-based neural models (Bordes et al., 2013; Wang et al., 2014), and models in unifying embedding and path-based 1783 technology (Das et al., 2018; Lin et al., 2018; Xiong et al., 2017), which can predict missing links for completion of KG. In this work, for knowledge selection on a graph, we follow the third line of works. Furthermore, our problem setting is different from theirs in that some of our vertices contain long texts, which motivates the use of machine reading technology for graph reasoning. Fusion of KG triples and texts: In the task of QA, combination of a KG and a text c"
D19-1187,W04-1013,0,0.0594973,"Missing"
D19-1187,P04-1077,0,0.0682962,"Missing"
D19-1187,D18-1362,0,0.0491171,"2017), or building benchmark dialogue data grounded on knowledge (Dinan et al., 2019; Moghe et al., 2018). In comparison with them, we adopt a graph based representation scheme for unstructured texts, which enables better explainability and generalization capability of our system. Knowledge Graph Reasoning: Previous studies on KG reasoning can be categorized into three lines, path-based symbolic models (Das et al., 2017a; Lao et al., 2011), embedding-based neural models (Bordes et al., 2013; Wang et al., 2014), and models in unifying embedding and path-based 1783 technology (Das et al., 2018; Lin et al., 2018; Xiong et al., 2017), which can predict missing links for completion of KG. In this work, for knowledge selection on a graph, we follow the third line of works. Furthermore, our problem setting is different from theirs in that some of our vertices contain long texts, which motivates the use of machine reading technology for graph reasoning. Fusion of KG triples and texts: In the task of QA, combination of a KG and a text corpus has been studied with a strategy of late fusion (Gardner and Krishnamurthy, 2017; Ryu et al., 2014) or early fusion (Das et al., 2017b; Sun et al., 2018), which can he"
D19-1187,D18-1455,0,0.0405211,"Missing"
D19-1187,C16-1318,0,0.265115,"e been devoted to chitchat oriented conversation generation (Ritter et al., 2011; Shang et al., 2015). 1 Data and codes are available at https://github. com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/EMNLP2019-AKGCM However, these models tend to produce generic responses or incoherent responses for a given topic, since it is quite challenging to learn semantic interactions merely from dialogue data without help of background knowledge. Recently, some previous studies have been conducted to introduce external knowledge, either unstructured knowledge texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016) or structured knowledge triples (Liu et al., 2018; Young et al., 2018; Zhou et al., 2018) to help open-domain conversation generation by producing responses conditioned on selected knowledge. In the first research line, their knowledge graph can help narrowing down knowledge candidates for conversation generation with the use of prior information, e.g., triple attributes or graph paths. Moreover, these prior information can enhance generalization capability of knowledge selection models. But it suffers from information insufficiency for response generation since there is simply a single word"
D19-1187,N18-1186,0,0.0166739,"or response generation, we use an encoder-decoder model to produce responses conditioned on selected knowledge. 2 Related Work Conversation with Knowledge Graph: There are growing interests in leveraging factoid knowledge (Han et al., 2015; Liu et al., 2018; Zhu et al., 2017) or commonsense knowledge (Young et al., 2018; Zhou et al., 2018) with graph based representation for generation of appropriate and informative responses. Compared with them, we augment previous KGs with knowledge texts and integrate more explainable and flexible multi-hop graph reasoning models into conversation systems. Wu et al. (2018) used document reasoning network for modeling of conversational contexts, but not for knowledge selection. Conversation with Unstructured Texts: With availability of a large amount of knowledge texts from Wikipedia or user generated content, some work focus on either modeling of conversation generation with unstructured texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016; Xu et al., 2017), or building benchmark dialogue data grounded on knowledge (Dinan et al., 2019; Moghe et al., 2018). In comparison with them, we adopt a graph based representation scheme for unstructured texts, which"
D19-1187,D17-1060,0,0.0207557,"benchmark dialogue data grounded on knowledge (Dinan et al., 2019; Moghe et al., 2018). In comparison with them, we adopt a graph based representation scheme for unstructured texts, which enables better explainability and generalization capability of our system. Knowledge Graph Reasoning: Previous studies on KG reasoning can be categorized into three lines, path-based symbolic models (Das et al., 2017a; Lao et al., 2011), embedding-based neural models (Bordes et al., 2013; Wang et al., 2014), and models in unifying embedding and path-based 1783 technology (Das et al., 2018; Lin et al., 2018; Xiong et al., 2017), which can predict missing links for completion of KG. In this work, for knowledge selection on a graph, we follow the third line of works. Furthermore, our problem setting is different from theirs in that some of our vertices contain long texts, which motivates the use of machine reading technology for graph reasoning. Fusion of KG triples and texts: In the task of QA, combination of a KG and a text corpus has been studied with a strategy of late fusion (Gardner and Krishnamurthy, 2017; Ryu et al., 2014) or early fusion (Das et al., 2017b; Sun et al., 2018), which can help address the issue"
D19-5828,P18-1157,0,0.0118879,"dsourcing workers, examine writers, search logs, synthetics, etc. Machine reading comprehension (MRC) requires machines to understand text and answer questions about the text, and it is an important task in natural language processing (NLP). With the increasing availability of large-scale datasets for MRC (Rajpurkar et al., 2016; Bajaj et al., 2016; Dunn et al., 2017; Joshi et al., 2017; He et al., 2018) and the development of deep learning techniques, MRC has achieved remarkable advancements in the last few years (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Liu et al., 2018; Wang et al., 2018; Yu et al., 2018). Although a number of neural models obtain even human parity performance on several datasets, these models may generalize poorly on other datasets (Talmor and Berant, 2019). We expect that a truly effective question answering system works well on both the examples drawn from the same distribution as the training • Documents: They involve passages from different sources, e.g. wikipedia, news, movies, textbook, etc. • Language Understanding Ability: They might require different language understanding abilities, e.g. matching, reasoning and arithmetic. To add"
D19-5828,P19-1620,0,0.0163425,", 2019) uses multi-layer Transformer encoding blocks as its encoder. The pre-training tasks include masked language model and next sentence prediction, which enable the model to capture bidirectional and global information. In our system, we use the BERT large configuration that contains 24 Transformer encoding blocks, each with 16 self attention heads and 1024 hidden units. Note that we use this pre-trained model for experimental purpose, and it is not included in the final submission. In our experiments, we initialize the parameters of the encoding layers from the checkpoint 2 of the model (Alberti et al., 2019) namely BERT + N-Gram Masking + Synthetic Self-Training. The model is initialized from Whole Word Masking BERT (BERTwwm ), further fine-tuned on the SQuAD 2.0 task with synthetic generated question answering corpora. In our experiments, we find that this model performs consistently better than the original BERTlarge and 3 2 The checkpoint can be downloaded from https: //worksheets.codalab.org/worksheets/ 0xd7b08560b5b24bd1874b9429d58e2df1 https://github.com/google-research/ bert 4 https://github.com/zihangdai/xlnet/ 214 Model ID BERT Pre-trained XLNET Model ERNIE In-domain Masked Search Snippe"
D19-5828,D16-1264,0,0.030307,"els and we conduct experiments to examine the effectiveness of these strategies. Our system is ranked at top 1 of all the participants in terms of averaged F1 score. Our codes and models will be released at PaddleNLP 1 . 1 Introduction • Questions: They come from different sources, e.g. crowdsourcing workers, examine writers, search logs, synthetics, etc. Machine reading comprehension (MRC) requires machines to understand text and answer questions about the text, and it is an important task in natural language processing (NLP). With the increasing availability of large-scale datasets for MRC (Rajpurkar et al., 2016; Bajaj et al., 2016; Dunn et al., 2017; Joshi et al., 2017; He et al., 2018) and the development of deep learning techniques, MRC has achieved remarkable advancements in the last few years (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Liu et al., 2018; Wang et al., 2018; Yu et al., 2018). Although a number of neural models obtain even human parity performance on several datasets, these models may generalize poorly on other datasets (Talmor and Berant, 2019). We expect that a truly effective question answering system works well on both the examples drawn from"
D19-5828,N19-1213,0,0.343888,"C models based on the same pre-trained models. • Multi-task Learning: Since the pre-training is usually performed on corpus with restricted domains, it is expected that increasing the domain diversity by further pretraining on other corpus may improve the generalization capability. Hence, we incorporate masked language model by using corpus from various domains as an auxiliary task in the fine-tuning phase, along with MRC. The side effect of adding a language modeling objective to MRC is that it can avoid catastrophic forgetting and keep the most useful features learned from pretraining task (Chronopoulou et al., 2019). Additionally, we explore multi-task learning (Liu et al., 2019) by incorporating the supervised dataset from other NLP tasks (e.g. natural language inference and paragraph ranking) to learn better language representation. • The auxiliary task of masked language model can help improve the generalization of MRC models. • We do not observe much improvements from the auxiliary tasks of natural language inference and paragraph ranking. The remainder of this paper is structured as follows: Section 2 describes the detailed overview of our system. Section 3 shows the experimental settings and result"
D19-5828,P19-1485,0,0.0135196,"k in natural language processing (NLP). With the increasing availability of large-scale datasets for MRC (Rajpurkar et al., 2016; Bajaj et al., 2016; Dunn et al., 2017; Joshi et al., 2017; He et al., 2018) and the development of deep learning techniques, MRC has achieved remarkable advancements in the last few years (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Liu et al., 2018; Wang et al., 2018; Yu et al., 2018). Although a number of neural models obtain even human parity performance on several datasets, these models may generalize poorly on other datasets (Talmor and Berant, 2019). We expect that a truly effective question answering system works well on both the examples drawn from the same distribution as the training • Documents: They involve passages from different sources, e.g. wikipedia, news, movies, textbook, etc. • Language Understanding Ability: They might require different language understanding abilities, e.g. matching, reasoning and arithmetic. To address the above challenge, we introduce a simple framework of pre-training and fine-tuning, namely D-NET, for improving the generalization of MRC models by exploring the following techniques: • Pre-trained Model"
D19-5828,N19-1300,0,0.122698,"the pre-trained models, the corpus for the masked language model task, the types of supervised NLP tasks. The hyper-parameters include the max sequence length, batch size and the mix ratio λ used the auxiliary tasks in multi-task learning. version 5 . task (Chronopoulou et al., 2019). Supervised Tasks Motivated by (Liu et al., 2019), we explore multi-task learning by incorporating the supervised datasets from other NLP tasks to learn more general language representation. Specifically, we incorporate natural language inference and paragraph ranking as auxiliary tasks to MRC. (1) Previous work (Clark et al., 2019; Liu et al., 2019) show that MNLI (Williams et al., 2017) (a popular natural language inference dataset) can help improve the performance of the major task in a multi-task setting. In our system, we also leverage MNLI as an auxiliary task. (2) Previous work (Tan et al., 2017; Wang et al., 2018) examine the effectiveness of the joint learning of MRC and paragraph ranking. In our system, we also leverage paragraph ranking as an auxiliary task. We generate the datasets of paragraph ranking from MRQA in-domain datasets. The generated data and the details of data generation will be released at Pad"
D19-5828,N19-1423,0,0.121965,"ystem works well on both the examples drawn from the same distribution as the training • Documents: They involve passages from different sources, e.g. wikipedia, news, movies, textbook, etc. • Language Understanding Ability: They might require different language understanding abilities, e.g. matching, reasoning and arithmetic. To address the above challenge, we introduce a simple framework of pre-training and fine-tuning, namely D-NET, for improving the generalization of MRC models by exploring the following techniques: • Pre-trained Models: We leverage multiple pre-trained models, e.g. BERT (Devlin et al., 2019), XLNET (Yang et al., 2019) and ERNIE 2.0 (Sun et al., 2019). Since different pre-trained models are trained on various 1 https://github.com/PaddlePaddle/ models/tree/develop/PaddleNLP/Research/ MRQA2019-D-NET 212 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 212–219 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics Dataset SQuAD NewsQA TriviaQA SearchQA HotpotQA NaturalQuestions BioASQ DROP DuoRC RACE RelationExtraction TextbookQA BioProcess ComplexWebQuestions MCTest QAMR QAST TREC Question Sources Crowdsourced Crowdsou"
D19-5828,P17-1018,0,0.0292485,"sources, e.g. crowdsourcing workers, examine writers, search logs, synthetics, etc. Machine reading comprehension (MRC) requires machines to understand text and answer questions about the text, and it is an important task in natural language processing (NLP). With the increasing availability of large-scale datasets for MRC (Rajpurkar et al., 2016; Bajaj et al., 2016; Dunn et al., 2017; Joshi et al., 2017; He et al., 2018) and the development of deep learning techniques, MRC has achieved remarkable advancements in the last few years (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Liu et al., 2018; Wang et al., 2018; Yu et al., 2018). Although a number of neural models obtain even human parity performance on several datasets, these models may generalize poorly on other datasets (Talmor and Berant, 2019). We expect that a truly effective question answering system works well on both the examples drawn from the same distribution as the training • Documents: They involve passages from different sources, e.g. wikipedia, news, movies, textbook, etc. • Language Understanding Ability: They might require different language understanding abilities, e.g. matching, reasoning and"
D19-5828,P18-1178,1,0.840226,"examine writers, search logs, synthetics, etc. Machine reading comprehension (MRC) requires machines to understand text and answer questions about the text, and it is an important task in natural language processing (NLP). With the increasing availability of large-scale datasets for MRC (Rajpurkar et al., 2016; Bajaj et al., 2016; Dunn et al., 2017; Joshi et al., 2017; He et al., 2018) and the development of deep learning techniques, MRC has achieved remarkable advancements in the last few years (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Liu et al., 2018; Wang et al., 2018; Yu et al., 2018). Although a number of neural models obtain even human parity performance on several datasets, these models may generalize poorly on other datasets (Talmor and Berant, 2019). We expect that a truly effective question answering system works well on both the examples drawn from the same distribution as the training • Documents: They involve passages from different sources, e.g. wikipedia, news, movies, textbook, etc. • Language Understanding Ability: They might require different language understanding abilities, e.g. matching, reasoning and arithmetic. To address the above chal"
D19-5828,W18-2605,1,0.872523,"r system is ranked at top 1 of all the participants in terms of averaged F1 score. Our codes and models will be released at PaddleNLP 1 . 1 Introduction • Questions: They come from different sources, e.g. crowdsourcing workers, examine writers, search logs, synthetics, etc. Machine reading comprehension (MRC) requires machines to understand text and answer questions about the text, and it is an important task in natural language processing (NLP). With the increasing availability of large-scale datasets for MRC (Rajpurkar et al., 2016; Bajaj et al., 2016; Dunn et al., 2017; Joshi et al., 2017; He et al., 2018) and the development of deep learning techniques, MRC has achieved remarkable advancements in the last few years (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Liu et al., 2018; Wang et al., 2018; Yu et al., 2018). Although a number of neural models obtain even human parity performance on several datasets, these models may generalize poorly on other datasets (Talmor and Berant, 2019). We expect that a truly effective question answering system works well on both the examples drawn from the same distribution as the training • Documents: They involve passages from"
D19-5828,P17-1147,0,0.0547512,"these strategies. Our system is ranked at top 1 of all the participants in terms of averaged F1 score. Our codes and models will be released at PaddleNLP 1 . 1 Introduction • Questions: They come from different sources, e.g. crowdsourcing workers, examine writers, search logs, synthetics, etc. Machine reading comprehension (MRC) requires machines to understand text and answer questions about the text, and it is an important task in natural language processing (NLP). With the increasing availability of large-scale datasets for MRC (Rajpurkar et al., 2016; Bajaj et al., 2016; Dunn et al., 2017; Joshi et al., 2017; He et al., 2018) and the development of deep learning techniques, MRC has achieved remarkable advancements in the last few years (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Liu et al., 2018; Wang et al., 2018; Yu et al., 2018). Although a number of neural models obtain even human parity performance on several datasets, these models may generalize poorly on other datasets (Talmor and Berant, 2019). We expect that a truly effective question answering system works well on both the examples drawn from the same distribution as the training • Documents: They inv"
I05-1041,P98-1004,0,0.0718758,"erforms consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way (weighted or unweighted voting) to classify new examples. Many methods for constructing ensembles have been developed [5]. One kind of methods is to resample the training examples. These methods include bagging [2], cross-validation co"
I05-1041,J93-2003,0,0.00630544,"these two methods, both weighted voting and unweighted voting are compared under the word alignment task. In addition, we analyze the effect of different sizes of training sets on the bagging method. Experimental results indicate that both bagging and cross-validation committees improve the word alignment results regardless of weighted voting or unweighted voting. Weighted voting performs consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were propo"
I05-1041,P03-1012,0,0.0365007,"improve the word alignment results regardless of weighted voting or unweighted voting. Weighted voting performs consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way (weighted or unweighted voting) to classify new examples. Many methods for constructing ensembles have been developed [5]. One kin"
I05-1041,C04-1032,0,0.0254337,"ove the word alignment results regardless of weighted voting or unweighted voting. Weighted voting performs consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way (weighted or unweighted voting) to classify new examples. Many methods for constructing ensembles have been developed [5]. One kind of"
I05-1041,1996.amta-1.13,0,0.0738635,"r the word alignment task. In addition, we analyze the effect of different sizes of training sets on the bagging method. Experimental results indicate that both bagging and cross-validation committees improve the word alignment results regardless of weighted voting or unweighted voting. Weighted voting performs consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a s"
I05-1041,W01-1406,0,0.0657252,"n addition, we analyze the effect of different sizes of training sets on the bagging method. Experimental results indicate that both bagging and cross-validation committees improve the word alignment results regardless of weighted voting or unweighted voting. Weighted voting performs consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a set of classifiers whose indiv"
I05-1041,P00-1056,0,0.15695,"he word alignment results regardless of weighted voting or unweighted voting. Weighted voting performs consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way (weighted or unweighted voting) to classify new examples. Many methods for constructing ensembles have been developed [5]. One kind of method"
I05-1041,J96-1001,0,0.0489376,"alignment results on the training data. As described in Section 3.1, on each bootstrap replicate j, we train a word aligner M stj in the source to target direction and a word aligner M tsj in the target to source direction. That is to say, we obtain two different word alignment sets S stj and S tsj for each of the bootstrap replicate. For each word alignment link ( s, t ) produced by M stj or M tsj , we calculate its weight as shown in (3). This weight measures the association of the source part and the target part in an alignment link. This measure is like the Dice Coefficient. Smadja et al. [13] showed that the Dice Coefficient is a good indicator of translation association. Wi ( s, t ) = 2 * count ( s, t ) ∑ count(s, t &apos; ) + ∑ count(s&apos; , t ) t&apos; (3) s&apos; Where, count ( s, t ) is the occurring frequency of the alignment link ( s, t ) ∈ S stj ∪ S tsj . 6 Experiments 6.1 Training and Testing Set We perform experiments on a sentence aligned English-Chinese bilingual corpus in general domain. There are about 320,000 bilingual sentence pairs in the corpus, from which, we randomly select 1,000 sentence pairs as testing data. The remainder is used as training data. In the sentence pairs, the a"
I05-1041,tufis-barbu-2002-lexical,0,0.0266233,"ms consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way (weighted or unweighted voting) to classify new examples. Many methods for constructing ensembles have been developed [5]. One kind of methods is to resample the training examples. These methods include bagging [2], cross-validation committe"
I05-1041,J97-3002,0,0.061287,"d alignment results regardless of weighted voting or unweighted voting. Weighted voting performs consistently better than unweighted voting on different sizes of training sets. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) [3]. Besides being used in SMT, it is also used in translation lexicon building [9], transfer rule learning [10], example-based machine translation [14], etc. In previous alignment methods, some researchers employed statistical word alignment models to build alignment links [3], [4], [8], [11], [16]. Some researchers used similarity and association measures to build alignment links [1], [15]. One issue about word alignment is how to improve the performance of a word aligner when the training data are fixed. One possible solution is to use ensemble methods [5], [6]. The ensemble methods were proposed to improve the performance of classifiers. An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way (weighted or unweighted voting) to classify new examples. Many methods for constructing ensembles have been developed [5]. One kind of methods is t"
I05-1041,wu-wang-2004-improving-domain,1,0.343875,"Missing"
I05-1041,2001.mtsummit-ebmt.4,0,\N,Missing
I05-1041,C98-1004,0,\N,Missing
I11-1090,N03-1017,0,0.0601274,"Missing"
I11-1090,D09-1040,0,0.596584,"This problem becomes more serious for higher-order n-grams, and for morphologically richer languages. To overcome the coverage problem of SMT, besides the efforts of mining larger parallel corpora from various resources, some researchers have investigated to use paraphrasing approaches. The studies can be classified into two categories by the target of paraphrasing: (1) paraphrasing the input source sentences; (2) paraphrasing the training corpus. In the first category, the proposed approaches mainly focus on handling ngrams that are unknown to the SMT model. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite unknown terms with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) build paraphrase lattices for input sentences and select the best translations using a lattice-based SMT decoder. In the second category of paraphrasing training corpus, Bond et al. (2008) and Nakov (2008) paraphrase the source side of training corpus using hand-crafted rules. In this paper, we propose a method that enriches SMT tra"
I11-1090,P03-1021,0,0.0904712,"Missing"
I11-1090,P02-1040,0,0.0799961,"Missing"
I11-1090,P09-1094,1,0.947045,"ce on Natural Language Processing, pages 803–810, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Original training data e1 e2 … en Expanded training data f1 f2 … fn e1 e2 … en e 1’ e 2’ … en’ paraphrasing e 1’ e 2’ … en’ f1 f2 … fn f1 f 2’ … fn Figure 1. Sketch map of the paraphrasing based translation corpus expansion. tences of the bilingual parallel data, which are then paired with the target-side sentences to generate new parallel data. The procedure is illustrated in Figure 1. The SPG framework can be considered as an application-specific source-tosource translating procedure (Zhao et al. 2009) which is similar to phrase based statistical machine translation. We employ an object function, named Sentence Novelty, to select paraphrases that introduce the most novel information to the bilingual training corpus. In our approach, the context of paraphrasing substitution is considered during generating paraphrasing sentences, which yields paraphrases with higher precision. Experimental results show that the performance of a state-of-the-art phrase based SMT system (Moses in this work) can be improved from 17.91 to 19.57 in terms of BLEU on a small training set, and from 25.46 to 26.52 on"
I11-1090,P05-1074,0,0.177853,"Missing"
I11-1090,2008.iwslt-papers.2,0,0.277463,"the first category, the proposed approaches mainly focus on handling ngrams that are unknown to the SMT model. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite unknown terms with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) build paraphrase lattices for input sentences and select the best translations using a lattice-based SMT decoder. In the second category of paraphrasing training corpus, Bond et al. (2008) and Nakov (2008) paraphrase the source side of training corpus using hand-crafted rules. In this paper, we propose a method that enriches SMT training data using a statistical paraphrase generating (SPG) model. The method generates paraphrases for the source-side sen803 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 803–810, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Original training data e1 e2 … en Expanded training data f1 f2 … fn e1 e2 … en e 1’ e 2’ … en’ paraphrasing e 1’ e 2’ … en’ f1 f2 … fn f1 f 2’ … fn Figure 1. Sketch map"
I11-1090,N06-1003,0,0.527336,"select paraphrases that introduce the most novel information to the bilingual training corpus. In our approach, the context of paraphrasing substitution is considered during generating paraphrasing sentences, which yields paraphrases with higher precision. Experimental results show that the performance of a state-of-the-art phrase based SMT system (Moses in this work) can be improved from 17.91 to 19.57 in terms of BLEU on a small training set, and from 25.46 to 26.52 on a training corpus of medium size. Results also indicate that our method gains a significant improvement over the method of Callison-Burch et al. (2006). The rest of this paper is structured as follows. We review related work on improving SMT through paraphrasing in Section 2. The proposed statistical paraphrase generation model is described in Section 3. Section 4 presents our method of enlarging training data via paraphrasing. Section 5 and 6 present the experiments and results. We discuss our work in Section 7 and conclude the paper in Section 8. 2 Related Work Previous studies on improving SMT through paraphrasing input sentences mainly focus on finding translations for unknown terms using phrasal paraphrases. In these methods, an unknown"
I11-1090,D10-1041,0,0.229909,"hrasing approaches. The studies can be classified into two categories by the target of paraphrasing: (1) paraphrasing the input source sentences; (2) paraphrasing the training corpus. In the first category, the proposed approaches mainly focus on handling ngrams that are unknown to the SMT model. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite unknown terms with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) build paraphrase lattices for input sentences and select the best translations using a lattice-based SMT decoder. In the second category of paraphrasing training corpus, Bond et al. (2008) and Nakov (2008) paraphrase the source side of training corpus using hand-crafted rules. In this paper, we propose a method that enriches SMT training data using a statistical paraphrase generating (SPG) model. The method generates paraphrases for the source-side sen803 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 803–810, c Chiang Mai, Thailand, November 8 – 1"
I11-1090,P07-2045,0,\N,Missing
I11-1090,P09-1089,0,\N,Missing
I11-1090,P10-2001,0,\N,Missing
I11-1090,W04-3250,0,\N,Missing
I11-1104,2010.jeptalnrecital-court.36,0,0.0666853,"2 Question Generation Question generation is a branch of natural language generation, which is defined as the task of automatically generating questions from some form of input (Rus and Graesser, 2009). The input may vary from a deep semantic representation to a raw text. Previous studies on question generation have mostly focused on text-to-question generation, which generates questions from declarative sentences or paragraphs. This technique is useful in education, especially in reading tutoring. Most previous studies employed rule-based methods in their text-to-question generation systems (Ali et al., 2010; Kalady et al., 2010; Mannem et al., 2010; Pal et al., 2010; Piwek and Stoyanchev, 2010; Varga and Ha, 2010). Query-to-question generation is a sub-task of question generation, which was first proposed by Lin (2008). Lin has suggested to learn query-toquestion generation models with query logs. However, no detail method or evaluation has been presented. There has been no other research since, 935 either, which may be mainly because few researchers can access the query log data. tents. Paraphrase templates can be learned from monolingual corpora based on distributional hypothesis (Lin and Pant"
I11-1104,N03-1003,0,0.010863,"Piwek and Stoyanchev, 2010; Varga and Ha, 2010). Query-to-question generation is a sub-task of question generation, which was first proposed by Lin (2008). Lin has suggested to learn query-toquestion generation models with query logs. However, no detail method or evaluation has been presented. There has been no other research since, 935 either, which may be mainly because few researchers can access the query log data. tents. Paraphrase templates can be learned from monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001), from comparable news articles based on alignment (Barzilay and Lee, 2003), or from bilingual corpora based on pivot approaches (Zhao et al., 2008). The abovementioned studies are all related to our work. However, none of the previous work addresses the problem of query-to-question template generation. 4.3 Query Reformulation Query reformulation is an important topic in the IR community, since it can improve users’ search experience. Query reformulation mainly involves query reduction, expansion, and spelling correction. Query-to-question generation is closely related to query expansion. However, its goal is not only expanding useful information for the original que"
I11-1104,P02-1006,0,0.0158559,"Missing"
I11-1104,C08-1093,0,0.0196931,"uery sessions. The basic idea is that queries from the same session are more likely to be related to each other (Fonseca et al., 2005; Jones et al., 2006; Zhang and Nasraoui, 2006). The second kind of method identifies related queries using click-through information. They assume that queries leading to similar clicks are related in meaning (Wen et al., 2002; Baeza-Yates and Tiberi, 2007). The third category of method directly learns expansion terms from the clicked documents of the query. Their hypothesis is that terms in a query and a user-clicked document might be related (Cui et al., 2002; Riezler et al., 2008). 5 Conclusions and Future Work This paper addresses the problem of query-toquestion generation for cQA and proposes a method based on search engine query logs. Several conclusions can be drawn from the experimental results. First, search engine query logs are powerful data for the research of query-toquestion generation, from which we have acquired a large volume of question generation templates. Second, the proposed method is effective, which achieves promising precision and outperforms a baseline method. Third, the query-to-question generation technique can be used to improve the search of"
I11-1104,P08-1089,1,0.353017,"is a sub-task of question generation, which was first proposed by Lin (2008). Lin has suggested to learn query-toquestion generation models with query logs. However, no detail method or evaluation has been presented. There has been no other research since, 935 either, which may be mainly because few researchers can access the query log data. tents. Paraphrase templates can be learned from monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001), from comparable news articles based on alignment (Barzilay and Lee, 2003), or from bilingual corpora based on pivot approaches (Zhao et al., 2008). The abovementioned studies are all related to our work. However, none of the previous work addresses the problem of query-to-question template generation. 4.3 Query Reformulation Query reformulation is an important topic in the IR community, since it can improve users’ search experience. Query reformulation mainly involves query reduction, expansion, and spelling correction. Query-to-question generation is closely related to query expansion. However, its goal is not only expanding useful information for the original query, but also organizing the information to produce a question, with which"
I11-1114,P08-1004,0,0.0235139,"dependency rules. WOE (Wu and Weld, 2010) improves the precision and recall of TEXTRUNNER by mining clues from semi-structured texts in online encyclopedia and adopting different learning algorithms. Another descendent of TEXTRUNNER is the work of Mintz et al. (2009), which uses Freebase tuples as initial supervising information for training extractors. It is worth noting that building the learners is not mandatory. For example, Eichler et al. (2008) directly use syntactic patterns to perform Open IE. There are also approaches that combine traditional relation extraction and Open IE together. Banko and Etzioni (2008) present H-CRF combining the two types of systems’ output. StatSnowBall (Zhu et al., 2009) also performs both relationspecific extraction and Open IE. Like the technique proposed in (Banko and Etzioni, 2008), it formalizes the extraction problem as sequence labeling, but uses Markov Logic Networks (MLN) instead of Conditional Random Field (CRF). In thesaurus construction, mainstream efforts related to our work consist of synonym / comparable entities clustering (Lin, 1998; Pantel, 2003; Wang and Cohen, 2007). Lin (1998) popularized the automatic clustering of similar words using distributional"
I11-1114,W04-3205,0,0.0246191,"presents a more sophisticated clustering algorithm that first collects a small set of representative elements for each concept and then assigns words to their mostsimilar concept. Wang and Cohen (2007) alternatively investigate set expansion problem, that is, how to retrieve similar entities given a small number of seeds. With flexible matching patterns and random walk based ranking algorithm, their system outperforms Google SetsT M in terms of Mean Average Precision (MAP). There are also researches forcusing on the relationship between verbs or adjectives, such as (Turney et al., 2003) and (Chklovski and Pantel, 2004). In comparison to previous works on relation extraction, our work does not restrict itself to identifying pairs of entities whose relation is explicitly described by “infix”-like patterns, such as “Louis XVI was born in 1754”. Alternatively, we mine related entities from context similarity and cooccurrence points of views. Moreover, through empirical studies, we find that query log is an effective data source in the extraction of related entities. On the other hand, different from synonym / comparable entities mining, our work retrieves a more extensive scope of results. In addition to entiti"
I11-1114,eichler-etal-2008-unsupervised,0,0.0143331,"2006) and Web environment (Banko et al., 2007). Banko et al. (2007) build an Open IE system, TEXTRUNNER, that trains a Naïve Bayes classifier under the supervision of dependency rules. WOE (Wu and Weld, 2010) improves the precision and recall of TEXTRUNNER by mining clues from semi-structured texts in online encyclopedia and adopting different learning algorithms. Another descendent of TEXTRUNNER is the work of Mintz et al. (2009), which uses Freebase tuples as initial supervising information for training extractors. It is worth noting that building the learners is not mandatory. For example, Eichler et al. (2008) directly use syntactic patterns to perform Open IE. There are also approaches that combine traditional relation extraction and Open IE together. Banko and Etzioni (2008) present H-CRF combining the two types of systems’ output. StatSnowBall (Zhu et al., 2009) also performs both relationspecific extraction and Open IE. Like the technique proposed in (Banko and Etzioni, 2008), it formalizes the extraction problem as sequence labeling, but uses Markov Logic Networks (MLN) instead of Conditional Random Field (CRF). In thesaurus construction, mainstream efforts related to our work consist of synon"
I11-1114,C10-1058,0,0.0242895,"o vectors, and v ′ = (vi + vj )/2. Note that the larger the JSD is, the less similar two vectors are. Thus the similarity between vectors vi and vj is computed as 1 − JSD(vi , vj ). 3.1.3 Query Text Co-occurrence Web search queries represent the demand of information from the users. Queries are known to be noisy and of little syntactic structure. However, previous works have demonstrated that there is sufficient knowledge encoded in the query texts to perform information extraction tasks (Paca, 2007), and that extracting information within query logs can better represent the users’ interests (Jain and Pennacchiotti, 2010). We found from Baidu query logs that related persons are often searched together for users’ curiosity about their relationships. Such pairs of persons consist of not only those with persistent relationships like couples and friends, but also those related in certain events, especially some hot news. In this spirit, we employ a Baidu query log containing approximately 9.08 billion raw queries to extract candidate related persons. For each query person q, we traverse the query log and extract persons that co-occur with q in the same queries. We filter out the persons that co-occur with q for le"
I11-1114,N07-1015,0,0.0572023,"Missing"
I11-1114,P99-1004,0,0.0481229,"lection. We use logarithm on the frequency tfi to reduce the influence of the words with extremely high frequency. We extract candidate related persons for each query person q via selecting top 10 persons from the collection according to the contextual similarity with q. We compute the similarity between two context vectors vi and vj using Jensen-Shannon divergence (JSD), as it performs better than some other similarity computation methods, such as cosine similarity, in our experiments. We first normalize the input vectors by the sum of their components, and calculate the JSD as described in (Lee, 1999): 1 JSD(vi , vj ) = [KL(vi ∥v ′ ) + KL(vj ∥v ′ )] (2) 2 where KL denotes the Kullback-Leibler divergence between two vectors, and v ′ = (vi + vj )/2. Note that the larger the JSD is, the less similar two vectors are. Thus the similarity between vectors vi and vj is computed as 1 − JSD(vi , vj ). 3.1.3 Query Text Co-occurrence Web search queries represent the demand of information from the users. Queries are known to be noisy and of little syntactic structure. However, previous works have demonstrated that there is sufficient knowledge encoded in the query texts to perform information extractio"
I11-1114,C10-1112,0,0.0177511,"a state-ofthe-art baseline. 1 Introduction Facilitating efficient navigation in the knowledge space is essential to satisfying the current Web search demands. Named entities are vital building blocks of such a space, and retrieving related entities provides an efficient way of navigation. Related entity extraction refers to mining from text resources named entities with certain relationships between them, e.g. personaffiliation and organization-location. To this end, great efforts have been made recently in both academic (Banko et al., 2007; Wu and Weld, 2010) and industry (Zhu et al., 2009; Shi et al., 2010) circles. A wide range of NLP applications could benefit from the high-quality repository of related entities. For query suggestion in Web search and ebusiness, given a query concerning some entity e, one can suggest entities related to e, in which users ∗ This work was done when the first author was visiting Baidu. • Persons with definite relationships. The relationships in this category can be explicitly represented with definite concepts, e.g. parent, friend, colleague, etc. Most previous literature focuses on such definite relationships between persons (Brin,1998; Etzioni et al.,2005; Bank"
I11-1114,N06-1039,0,0.0209414,"cation. On the other hand, semi-supervised approaches avoid the heavy human labor in providing training examples by using bootstrapping techniques. For instance, DIPRE (Brin, 1998), Snowball (Agichtein and Gravano, 2000), and KNOWITALL (Etzioni et al., 2005) all adopt seed-pattern iterations to aggregate related entities. Besides the works on traditional relation extraction, studies on open information extraction (Open IE) have emerged recently, which avoid pre-defining types of relations, and enjoy the capability of mining arbitrary types of semantic relations from both document collections (Shinyama and Sekine, 2006) and Web environment (Banko et al., 2007). Banko et al. (2007) build an Open IE system, TEXTRUNNER, that trains a Naïve Bayes classifier under the supervision of dependency rules. WOE (Wu and Weld, 2010) improves the precision and recall of TEXTRUNNER by mining clues from semi-structured texts in online encyclopedia and adopting different learning algorithms. Another descendent of TEXTRUNNER is the work of Mintz et al. (2009), which uses Freebase tuples as initial supervising information for training extractors. It is worth noting that building the learners is not mandatory. For example, Eichl"
I11-1114,I08-2119,0,0.0224889,"Missing"
I11-1114,P10-1013,0,0.0171028,"no, 2000), and KNOWITALL (Etzioni et al., 2005) all adopt seed-pattern iterations to aggregate related entities. Besides the works on traditional relation extraction, studies on open information extraction (Open IE) have emerged recently, which avoid pre-defining types of relations, and enjoy the capability of mining arbitrary types of semantic relations from both document collections (Shinyama and Sekine, 2006) and Web environment (Banko et al., 2007). Banko et al. (2007) build an Open IE system, TEXTRUNNER, that trains a Naïve Bayes classifier under the supervision of dependency rules. WOE (Wu and Weld, 2010) improves the precision and recall of TEXTRUNNER by mining clues from semi-structured texts in online encyclopedia and adopting different learning algorithms. Another descendent of TEXTRUNNER is the work of Mintz et al. (2009), which uses Freebase tuples as initial supervising information for training extractors. It is worth noting that building the learners is not mandatory. For example, Eichler et al. (2008) directly use syntactic patterns to perform Open IE. There are also approaches that combine traditional relation extraction and Open IE together. Banko and Etzioni (2008) present H-CRF co"
I11-1114,P98-2127,0,0.0476361,"erform Open IE. There are also approaches that combine traditional relation extraction and Open IE together. Banko and Etzioni (2008) present H-CRF combining the two types of systems’ output. StatSnowBall (Zhu et al., 2009) also performs both relationspecific extraction and Open IE. Like the technique proposed in (Banko and Etzioni, 2008), it formalizes the extraction problem as sequence labeling, but uses Markov Logic Networks (MLN) instead of Conditional Random Field (CRF). In thesaurus construction, mainstream efforts related to our work consist of synonym / comparable entities clustering (Lin, 1998; Pantel, 2003; Wang and Cohen, 2007). Lin (1998) popularized the automatic clustering of similar words using distributional similarity. Pantel (2003) presents a more sophisticated clustering algorithm that first collects a small set of representative elements for each concept and then assigns words to their mostsimilar concept. Wang and Cohen (2007) alternatively investigate set expansion problem, that is, how to retrieve similar entities given a small number of seeds. With flexible matching patterns and random walk based ranking algorithm, their system outperforms Google SetsT M in terms of"
I11-1114,P09-1113,0,0.00781033,"ged recently, which avoid pre-defining types of relations, and enjoy the capability of mining arbitrary types of semantic relations from both document collections (Shinyama and Sekine, 2006) and Web environment (Banko et al., 2007). Banko et al. (2007) build an Open IE system, TEXTRUNNER, that trains a Naïve Bayes classifier under the supervision of dependency rules. WOE (Wu and Weld, 2010) improves the precision and recall of TEXTRUNNER by mining clues from semi-structured texts in online encyclopedia and adopting different learning algorithms. Another descendent of TEXTRUNNER is the work of Mintz et al. (2009), which uses Freebase tuples as initial supervising information for training extractors. It is worth noting that building the learners is not mandatory. For example, Eichler et al. (2008) directly use syntactic patterns to perform Open IE. There are also approaches that combine traditional relation extraction and Open IE together. Banko and Etzioni (2008) present H-CRF combining the two types of systems’ output. StatSnowBall (Zhu et al., 2009) also performs both relationspecific extraction and Open IE. Like the technique proposed in (Banko and Etzioni, 2008), it formalizes the extraction probl"
I11-1114,J90-1003,0,\N,Missing
I11-1114,C98-2122,0,\N,Missing
I13-1034,W06-2919,0,0.149004,"tal results show that the precision of NEs mined with the proposed method is 0.96 and 0.94 on Chinese and English corpora, respectively. Comparison result also shows that the proposed method significantly outperforms a representative method that mines NEs from large-scale query logs. 1 Introduction The task of named entity mining (NEM) aims to mine named entities (NE) of given categories from raw data. NEM is essential in many applications. For example, NEM can generate NE gazetteers necessary for the task of named entity recognition (NER) (Cohen and Sarawagi, 2004; Kazama and Torisawa, 2008; Talukdar et al., 2006). It can also help improve the search results in web search (Pas¸ca, 2004), and increase the coverage of knowledge graphs. Extensive research has been conducted on NEM, in which pattern-based methods are the most popular. Handcrafted or automatically learnt patterns are usually used to extract NE instances from various corpora, such as web documents, 2 Related Work In this section, we review the previous studies on NEM from three aspects: the data resource used, the proposed methods, and particularly the bootstrapping strategy. Various resources have been exploited for NEM. Many researchers ma"
I13-1034,W02-1028,0,0.165318,"Missing"
I13-1034,P08-1047,0,0.0142112,"and NE instances. Experimental results show that the precision of NEs mined with the proposed method is 0.96 and 0.94 on Chinese and English corpora, respectively. Comparison result also shows that the proposed method significantly outperforms a representative method that mines NEs from large-scale query logs. 1 Introduction The task of named entity mining (NEM) aims to mine named entities (NE) of given categories from raw data. NEM is essential in many applications. For example, NEM can generate NE gazetteers necessary for the task of named entity recognition (NER) (Cohen and Sarawagi, 2004; Kazama and Torisawa, 2008; Talukdar et al., 2006). It can also help improve the search results in web search (Pas¸ca, 2004), and increase the coverage of knowledge graphs. Extensive research has been conducted on NEM, in which pattern-based methods are the most popular. Handcrafted or automatically learnt patterns are usually used to extract NE instances from various corpora, such as web documents, 2 Related Work In this section, we review the previous studies on NEM from three aspects: the data resource used, the proposed methods, and particularly the bootstrapping strategy. Various resources have been exploited for"
I13-1034,P09-1050,0,0.0624854,"Missing"
I13-1034,D10-1108,0,0.0700494,"Missing"
I13-1068,N09-1003,0,0.0931124,"Missing"
I13-1068,J10-4006,0,0.171311,"we focus on the semantic similarity between words. Well-known implementations of word-level distributional similarity scheme (DSS) mainly fall ∗ This work was done when the first author was visiting Baidu. 596 International Joint Conference on Natural Language Processing, pages 596–604, Nagoya, Japan, 14-18 October 2013. which distributional similarity and semantic relations are available. Third, our DSS’s hierarchical nature allows us to individually replace each component with better implementations to adapt to specific applications or new languages. sibling / hypernym / hyponym relations (Baroni and Lenci, 2010; Bansal and Klein, 2012), concept properties (Baroni et al., 2010), and attribute information (Baroni and Lenci, 2010). Compared with these studies, our approach systematically exploits specific semantic relations instead of counting cooccurrence under surface patterns. We also develop a hierarchical similarity fusion architecture, rather than blending the heterogeneous evidences in a single distribution vector (Baroni and Lenci, 2010). It is also notable that sibling term extraction, in which various semantic evidences (e.g. hypernyms) also help, is not in the track of our study. Sibling ter"
I13-1068,P08-1028,0,0.0378182,". A variety of context types have been proposed to capture the underlying semantic interactions between linguistic objects, including textwindow based collocations (Rapp, 2003; Agirre et al., 2009), lexico-syntactic patterns (Turney, 2006; Baroni and Lenci, 2010), grammatical dependencies (Lin, 1998; Pad´o and Lapata, 2007; Thater et al., 2010), click-through data (Jain and Pennacchiotti, 2010), selectional preferences (Erk and Pad´o, 2008), synsets in thesaurus (Agirre et al., 2009), and latent topics (Dinu and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which exist ample efforts. Most of them make use of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain texts (Hearst, 1992; Pas¸ca, 2004), semi-structured HTML tags (Shinzato and Torisawa, 2007), or their combinations (Shi et al., 2010). Bootstrapping approach is shown useful given a number of seeds, which could be either relati"
I13-1068,D08-1094,0,0.059866,"Missing"
I13-1068,J07-2002,0,0.0665187,"Missing"
I13-1068,P06-1015,0,0.0390744,"context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which exist ample efforts. Most of them make use of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain texts (Hearst, 1992; Pas¸ca, 2004), semi-structured HTML tags (Shinzato and Torisawa, 2007), or their combinations (Shi et al., 2010). Bootstrapping approach is shown useful given a number of seeds, which could be either relation instances(Snow et al., 2004; Pantel and Pennacchiotti, 2006), or initial patterns(Pantel et al., 2004). To improve the quality of raw extraction results, some studies also resort to optimizing one relation using other relations (Zhang et al., 2011; Kozareva et al., 2011) or using distributional similarity (Shi et al., 2010). Despite the great progress made in the field of semantic relation extraction, few studies explicitly use semantic relations to guide the similarity calculation. In this paper, we use instance-pattern iteration on a massive corpus to populate semantic relation instances, and derive relation-specific similarities on top of text-windo"
I13-1068,W13-0112,0,0.0123148,"s have been proposed to capture the underlying semantic interactions between linguistic objects, including textwindow based collocations (Rapp, 2003; Agirre et al., 2009), lexico-syntactic patterns (Turney, 2006; Baroni and Lenci, 2010), grammatical dependencies (Lin, 1998; Pad´o and Lapata, 2007; Thater et al., 2010), click-through data (Jain and Pennacchiotti, 2010), selectional preferences (Erk and Pad´o, 2008), synsets in thesaurus (Agirre et al., 2009), and latent topics (Dinu and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which exist ample efforts. Most of them make use of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain texts (Hearst, 1992; Pas¸ca, 2004), semi-structured HTML tags (Shinzato and Torisawa, 2007), or their combinations (Shi et al., 2010). Bootstrapping approach is shown useful given a number of seeds, which could be either relation instances(Snow et al., 20"
I13-1068,C92-2082,0,0.129187,"ional preferences (Erk and Pad´o, 2008), synsets in thesaurus (Agirre et al., 2009), and latent topics (Dinu and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which exist ample efforts. Most of them make use of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain texts (Hearst, 1992; Pas¸ca, 2004), semi-structured HTML tags (Shinzato and Torisawa, 2007), or their combinations (Shi et al., 2010). Bootstrapping approach is shown useful given a number of seeds, which could be either relation instances(Snow et al., 2004; Pantel and Pennacchiotti, 2006), or initial patterns(Pantel et al., 2004). To improve the quality of raw extraction results, some studies also resort to optimizing one relation using other relations (Zhang et al., 2011; Kozareva et al., 2011) or using distributional similarity (Shi et al., 2010). Despite the great progress made in the field of semantic relat"
I13-1068,D09-1025,0,0.0142559,"us evidences in a single distribution vector (Baroni and Lenci, 2010). It is also notable that sibling term extraction, in which various semantic evidences (e.g. hypernyms) also help, is not in the track of our study. Sibling term extraction focuses on words sharing the same super concept, and does not quantify the pair-wise similarity between them. Nevertheless, sibling terms work fine as an evidence of semantic similarity, as shown in our experiments. Machine learning based integration of multiple evidences are shown useful in semantic class construction and semantic similarity calculation. Pennacchiotti and Pantel (2009) use gradient boosting decision tree to combine evidences from Web page, query log, Web tablet, and Wikipedia to populate instances of Actors, Athletes, and Musicians. There are also studies that combine distribution and pattern information in lexical entailment (Mirkin et al., 2006) and word clustering (Kaji and Kitsuregawa, 2008). Close to our work, Agirre et al. (2009) train SVM classification models to combine individual similarities derived from dependency path, text-window, and WordNet synsets. The synsets are highly accurate in representing words’ meanings. However, the size of the thes"
I13-1068,2003.mtsummit-papers.42,0,0.0718911,"not equally available in different languages. Although Agirre et al. (2009) tried machine translation techniques to tackle with this issue, abundant named entities and translation errors in the Web corpus still challenge the performance of their approach. 2 Related Work Distributional similarity (a.k.a. contextual similarity) has been elaborately studied to predict semantic similarity, and the type of the context is a main concern. A variety of context types have been proposed to capture the underlying semantic interactions between linguistic objects, including textwindow based collocations (Rapp, 2003; Agirre et al., 2009), lexico-syntactic patterns (Turney, 2006; Baroni and Lenci, 2010), grammatical dependencies (Lin, 1998; Pad´o and Lapata, 2007; Thater et al., 2010), click-through data (Jain and Pennacchiotti, 2010), selectional preferences (Erk and Pad´o, 2008), synsets in thesaurus (Agirre et al., 2009), and latent topics (Dinu and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relation"
I13-1068,C10-1112,0,0.0130589,"and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which exist ample efforts. Most of them make use of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain texts (Hearst, 1992; Pas¸ca, 2004), semi-structured HTML tags (Shinzato and Torisawa, 2007), or their combinations (Shi et al., 2010). Bootstrapping approach is shown useful given a number of seeds, which could be either relation instances(Snow et al., 2004; Pantel and Pennacchiotti, 2006), or initial patterns(Pantel et al., 2004). To improve the quality of raw extraction results, some studies also resort to optimizing one relation using other relations (Zhang et al., 2011; Kozareva et al., 2011) or using distributional similarity (Shi et al., 2010). Despite the great progress made in the field of semantic relation extraction, few studies explicitly use semantic relations to guide the similarity calculation. In this paper,"
I13-1068,C10-1058,0,0.012838,"us still challenge the performance of their approach. 2 Related Work Distributional similarity (a.k.a. contextual similarity) has been elaborately studied to predict semantic similarity, and the type of the context is a main concern. A variety of context types have been proposed to capture the underlying semantic interactions between linguistic objects, including textwindow based collocations (Rapp, 2003; Agirre et al., 2009), lexico-syntactic patterns (Turney, 2006; Baroni and Lenci, 2010), grammatical dependencies (Lin, 1998; Pad´o and Lapata, 2007; Thater et al., 2010), click-through data (Jain and Pennacchiotti, 2010), selectional preferences (Erk and Pad´o, 2008), synsets in thesaurus (Agirre et al., 2009), and latent topics (Dinu and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which exist ample efforts. Most of them make use of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain"
I13-1068,C08-1051,0,0.0167289,"between them. Nevertheless, sibling terms work fine as an evidence of semantic similarity, as shown in our experiments. Machine learning based integration of multiple evidences are shown useful in semantic class construction and semantic similarity calculation. Pennacchiotti and Pantel (2009) use gradient boosting decision tree to combine evidences from Web page, query log, Web tablet, and Wikipedia to populate instances of Actors, Athletes, and Musicians. There are also studies that combine distribution and pattern information in lexical entailment (Mirkin et al., 2006) and word clustering (Kaji and Kitsuregawa, 2008). Close to our work, Agirre et al. (2009) train SVM classification models to combine individual similarities derived from dependency path, text-window, and WordNet synsets. The synsets are highly accurate in representing words’ meanings. However, the size of the thesaurus is limited, and not equally available in different languages. Although Agirre et al. (2009) tried machine translation techniques to tackle with this issue, abundant named entities and translation errors in the Web corpus still challenge the performance of their approach. 2 Related Work Distributional similarity (a.k.a. contex"
I13-1068,D11-1011,0,0.0145597,"e of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain texts (Hearst, 1992; Pas¸ca, 2004), semi-structured HTML tags (Shinzato and Torisawa, 2007), or their combinations (Shi et al., 2010). Bootstrapping approach is shown useful given a number of seeds, which could be either relation instances(Snow et al., 2004; Pantel and Pennacchiotti, 2006), or initial patterns(Pantel et al., 2004). To improve the quality of raw extraction results, some studies also resort to optimizing one relation using other relations (Zhang et al., 2011; Kozareva et al., 2011) or using distributional similarity (Shi et al., 2010). Despite the great progress made in the field of semantic relation extraction, few studies explicitly use semantic relations to guide the similarity calculation. In this paper, we use instance-pattern iteration on a massive corpus to populate semantic relation instances, and derive relation-specific similarities on top of text-window based distributional similarity. Indeed, previous studies did resort to a closed set of lexical patterns that indicate 3 Hierarchical Semantics-aware DSS Our proposed DSS has a four-layer structure, as shown i"
I13-1068,P97-1009,0,0.270765,"Missing"
I13-1068,P10-1097,0,0.0302936,"Missing"
I13-1068,P98-2127,0,0.240588,"th this issue, abundant named entities and translation errors in the Web corpus still challenge the performance of their approach. 2 Related Work Distributional similarity (a.k.a. contextual similarity) has been elaborately studied to predict semantic similarity, and the type of the context is a main concern. A variety of context types have been proposed to capture the underlying semantic interactions between linguistic objects, including textwindow based collocations (Rapp, 2003; Agirre et al., 2009), lexico-syntactic patterns (Turney, 2006; Baroni and Lenci, 2010), grammatical dependencies (Lin, 1998; Pad´o and Lapata, 2007; Thater et al., 2010), click-through data (Jain and Pennacchiotti, 2010), selectional preferences (Erk and Pad´o, 2008), synsets in thesaurus (Agirre et al., 2009), and latent topics (Dinu and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which exist ample efforts. Most of them make use of hand-crafted or au"
I13-1068,J06-3003,0,0.0392718,"e et al. (2009) tried machine translation techniques to tackle with this issue, abundant named entities and translation errors in the Web corpus still challenge the performance of their approach. 2 Related Work Distributional similarity (a.k.a. contextual similarity) has been elaborately studied to predict semantic similarity, and the type of the context is a main concern. A variety of context types have been proposed to capture the underlying semantic interactions between linguistic objects, including textwindow based collocations (Rapp, 2003; Agirre et al., 2009), lexico-syntactic patterns (Turney, 2006; Baroni and Lenci, 2010), grammatical dependencies (Lin, 1998; Pad´o and Lapata, 2007; Thater et al., 2010), click-through data (Jain and Pennacchiotti, 2010), selectional preferences (Erk and Pad´o, 2008), synsets in thesaurus (Agirre et al., 2009), and latent topics (Dinu and Lapata, 2010). There are also researches that focus on distribution compositions (Mitchell and Lapata, 2008; Grefenstette et al., 2013) or context constrained similarity calculation (Erk and Pad´o, 2008). Extracting sibling or hierarchical semantic relations from corpora forms a different track of research, in which ex"
I13-1068,P06-2075,0,0.0260698,"does not quantify the pair-wise similarity between them. Nevertheless, sibling terms work fine as an evidence of semantic similarity, as shown in our experiments. Machine learning based integration of multiple evidences are shown useful in semantic class construction and semantic similarity calculation. Pennacchiotti and Pantel (2009) use gradient boosting decision tree to combine evidences from Web page, query log, Web tablet, and Wikipedia to populate instances of Actors, Athletes, and Musicians. There are also studies that combine distribution and pattern information in lexical entailment (Mirkin et al., 2006) and word clustering (Kaji and Kitsuregawa, 2008). Close to our work, Agirre et al. (2009) train SVM classification models to combine individual similarities derived from dependency path, text-window, and WordNet synsets. The synsets are highly accurate in representing words’ meanings. However, the size of the thesaurus is limited, and not equally available in different languages. Although Agirre et al. (2009) tried machine translation techniques to tackle with this issue, abundant named entities and translation errors in the Web corpus still challenge the performance of their approach. 2 Rela"
I13-1068,P11-1116,1,0.832072,"Most of them make use of hand-crafted or automatically bootstrapped patterns. Various types of patterns have been tried out, including plain texts (Hearst, 1992; Pas¸ca, 2004), semi-structured HTML tags (Shinzato and Torisawa, 2007), or their combinations (Shi et al., 2010). Bootstrapping approach is shown useful given a number of seeds, which could be either relation instances(Snow et al., 2004; Pantel and Pennacchiotti, 2006), or initial patterns(Pantel et al., 2004). To improve the quality of raw extraction results, some studies also resort to optimizing one relation using other relations (Zhang et al., 2011; Kozareva et al., 2011) or using distributional similarity (Shi et al., 2010). Despite the great progress made in the field of semantic relation extraction, few studies explicitly use semantic relations to guide the similarity calculation. In this paper, we use instance-pattern iteration on a massive corpus to populate semantic relation instances, and derive relation-specific similarities on top of text-window based distributional similarity. Indeed, previous studies did resort to a closed set of lexical patterns that indicate 3 Hierarchical Semantics-aware DSS Our proposed DSS has a four-lay"
I13-1068,P12-1041,0,\N,Missing
I13-1068,C98-2122,0,\N,Missing
I13-1068,D10-1113,0,\N,Missing
P04-3002,P98-1004,0,0.081698,"Missing"
P04-3002,J93-2003,0,0.00754756,"Missing"
P04-3002,P03-1012,0,0.0492941,"Missing"
P04-3002,J93-1003,0,0.0206975,"order to filter some noise caused by the error alignment links, we only retain those translation pairs whose translation probabilities are above a threshold δ 1 or co-occurring frequencies are above a threshold δ 2 . When we train the IBM statistical word alignment model with a limited bilingual corpus in the specific domain, we build another translation dictionary D 2 with the same method as for the dictionary D1 . But we adopt a different filtering strategy for the translation dictionary D 2 . We use log-likelihood ratio to estimate the association strength of each translation pair because Dunning (1993) proved that log-likelihood ratio performed very well on small-scale data. Thus, we get the translation dictionary D 2 by keeping those entries whose log-likelihood ratio scores are greater than a threshold δ 3 . 2.3 Word Alignment Adaptation Algorithm Based on the bi-directional word alignment, we define SI as SI = SG ∩ SF and UG as UG = PG ∪ PF − SI . The word alignment links in the set SI are very reliable. Thus, we directly accept them as correct links and add them into the final alignment set WA . Input: Alignment set SI and UG (1) For alignment links in SI , we directly add them into the"
P04-3002,J97-2004,0,0.0743463,"Missing"
P04-3002,P00-1056,0,0.645519,"in the specific domain respectively, and then improves the domain-specific word alignment with these two models. Experimental results show a significant improvement in terms of both alignment precision and recall. And the alignment results are applied in a computer assisted translation system to improve human translation efficiency. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In previous alignment methods, some researchers modeled the alignments with different statistical models (Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). However, All of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997). However, few works address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available. This paper addres"
P04-3002,2001.mtsummit-papers.60,0,0.029265,"Missing"
P04-3002,tufis-barbu-2002-lexical,0,0.0244294,"Missing"
P04-3002,J97-3002,0,0.048517,"Missing"
P04-3002,C98-1004,0,\N,Missing
P05-1058,J93-1003,0,0.0333044,"When we train the bi-directional statistical word alignment models with the training data, we get two word alignment results for the training data. By taking the intersection of the two word alignment results, we build a new alignment set. The alignment links in this intersection set are extended by iteratively adding word alignment links into it as described in (Och and Ney, 2000). Based on the extended alignment links, we build a translation dictionary. In order to filter the noise caused by the error alignment links, we only retain those translation pairs whose log-likelihood ratio scores (Dunning, 1993) are above a threshold. Based on the alignment results on the 470 out-of-domain corpus, we build a translation dictionary D1 filtered with a threshold δ 1 . Based on the alignment results on a small-scale in-domain corpus, we build another translation dictionary D 2 filtered with a threshold δ 2 . After obtaining the two dictionaries, we combine two dictionaries through linearly interpolating the translation probabilities in the two dictionaries, which is shown in (11). The symbols f and e represent a single word or a phrase in the source and target languages. This differs from the translation"
P05-1058,J97-2004,0,0.0597441,"on Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links. In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997). However, only a few studies (Wu and Wang, 2004) directly address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available. In this paper, we address the problem of word alignment in a specific domain, in which only a small-scale corpus is available. In the domain-specific (in-domain) corpus, there are two kinds of words: general words, which also frequently occur in the out-of-domain corpus, and domain-specific words, which only occur in the specific domain. Thus, we can use the out"
P05-1058,P97-1063,0,0.0166212,"orithms include two parts: a training algorithm and a testing algorithm. The training algorithm is shown in Figure 1. After getting the two adaptation models and the translation dictionary, we apply them to the in-domain corpus to perform word alignment. Here we call it testing algorithm. The detailed algorithm is shown in Figure 2. For each sentence pair, there are two different word alignment results, from which the final alignment links are selected according to their translation probabilities in the dictionary D. The selection order is similar to that in the competitive linking algorithm (Melamed, 1997). The difference is that we allow many-to-one and one-to-many alignments. 6 Evaluation We compare our method with four other methods. The first method is descried in (Wu and Wang, 2004). We call it “Result Adaptation (ResAdapt)”. 3 We also tried an adaptation coefficient to calculate the interpolation weight as in (8). However, the alignment results are not improved by using this coefficient for the dictionary. models using the training data. Then we build a translation dictionary based on the alignment results on the training data and filter it using log-likelihood ratio as described in secti"
P05-1058,P00-1056,0,0.487452,"j ≠ 0 j =1, a j ≠ 0 1 A cept is defined as the set of target words connected to a source word (Brown et al., 1993). 468 , ρ i = max{i &apos; : φ i &apos; > 0 ∧ 0 &lt; i &apos; &lt; i} ; else ρ i = 0 . (τ ,π ) l |{i &apos; : φ i &apos; > 0 ∧ 0 &lt; i &apos; &lt; i} |> 0 If However, both model 3 and model 4 do not take the multiword cept into account. Only one-to-one and many-to-one word alignments are considered. Thus, some multi-word units in the domain-specific corpus cannot be correctly aligned. In order to deal with this problem, we perform word alignment in two directions (source to target, and target to source) as described in (Och and Ney, 2000). The GIZA++ toolkit2 is used to perform statistical word alignment. We use SG1 and SG2 to represent the bi-directional alignment sets, which are shown in Equation (4) and (5). For alignment in both sets, we use j for source words and i for target words. If a target word in position i is connected to source words in positions j1 and j 2 , then Ai = { j1 , j 2 } . We call an element in the alignment set an alignment link. 3 SG1 = {( Ai , i) |Ai = { j |a j = i, a j ≥ 0}} (4) SG 2 = {( j , A j ) |A j = {i |i = a j , a j ≥ 0}} (5) Word Alignment Model Adaptation In this paper, we first train two m"
P05-1058,J96-1001,0,0.0535151,"Missing"
P05-1058,tufis-barbu-2002-lexical,0,0.0255598,"Missing"
P05-1058,J97-3002,0,0.0809803,"Missing"
P05-1058,wu-wang-2004-improving-domain,1,0.643727,"iate result of statistical machine translation (Brown et al., 1993). In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links. In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997). However, only a few studies (Wu and Wang, 2004) directly address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available. In this paper, we address the problem of word alignment in a specific domain, in which only a small-scale corpus is available. In the domain-specific (in-domain) corpus, there are two kinds of words: general words, which also frequently occur in the out-of-domain corpus, and domain-specific words, which only occur in the specific domain. Thus, we can use the out-of-domain bilingual corpus to improve the alignm"
P05-1058,J93-2003,0,\N,Missing
P05-1058,P03-1012,0,\N,Missing
P05-1058,P98-1004,0,\N,Missing
P05-1058,C98-1004,0,\N,Missing
P06-1058,P91-1034,0,0.166095,"s the value of EP for unsupervised WSD. 1 Introduction Word sense disambiguation (WSD) has been a hot topic in natural language processing, which is to determine the sense of an ambiguous word in a specific context. It is an important technique for applications such as information retrieval, text mining, machine translation, text classification, automatic text summarization, and so on. Statistical solutions to WSD acquire linguistic knowledge from the training corpus using machine learning technologies, and apply the knowledge to disambiguation. The first statistical model of WSD was built by Brown et al. (1991). Since then, most machine learning methods have been applied to WSD, including decision tree, Bayesian model, neural network, SVM, maximum entropy, genetic algorithms, and so on. For different learning methods, supervised methods usually achieve good performance at a cost of human tagging of training corpus. The precision improves with larger size of training corpus. Compared with supervised methods, unsupervised methods do not require tagged corpus, but the precision is usually lower than that of the supervised methods. Thus, knowledge acquisition is critical to WSD methods. This paper propo"
P06-1058,P04-1039,0,0.0364432,"Missing"
P06-1058,W04-1609,0,0.0455949,"Missing"
P06-1058,P02-1033,0,0.0295354,"further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to the bottleneck of knowledge acquisition. Ide et al. (2001 and 2002), Ng et al. (2003), and Diab (2003, 2004a, and 2004b) made research on the use of alignment for WSD. Diab and Resnik (2002) investigated the feasibility of automatically annotating large amounts of data in parallel corpora using an unsupervised algorithm, making use of two languages simultaneously, only one of which has an available sense inventory. The results showed that wordlevel translation correspondences are a valuable source of information for sense disambiguation. The method by Li and Li (2002) does not require parallel corpus. It avoids the alignment work and takes advantage of bilingual corpus. In short, technology of automatic corpus tagging is based on the manually labeled corpus. That is to say, it st"
P06-1058,1992.tmi-1.9,0,0.418271,"of the ACL, pages 457–464, c Sydney, July 2006. 2006 Association for Computational Linguistics tagged corpus. Unsupervised method is an alternative, which often involves automatic generation of tagged corpus, bilingual corpus alignment, etc. The value of unsupervised methods lies in the knowledge acquisition solutions they adopt. 2.1 Automatic Generation of Training Corpus Automatic corpus tagging is a solution to WSD, which generates large-scale corpus from a small seed corpus. This is a weakly supervised learning or semi-supervised learning method. This reinforcement algorithm dates back to Gale et al. (1992a). Their investigation was based on a 6word test set with 2 senses for each word. Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to"
P06-1058,P92-1032,0,0.0812357,"of the ACL, pages 457–464, c Sydney, July 2006. 2006 Association for Computational Linguistics tagged corpus. Unsupervised method is an alternative, which often involves automatic generation of tagged corpus, bilingual corpus alignment, etc. The value of unsupervised methods lies in the knowledge acquisition solutions they adopt. 2.1 Automatic Generation of Training Corpus Automatic corpus tagging is a solution to WSD, which generates large-scale corpus from a small seed corpus. This is a weakly supervised learning or semi-supervised learning method. This reinforcement algorithm dates back to Gale et al. (1992a). Their investigation was based on a 6word test set with 2 senses for each word. Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to"
P06-1058,W02-0808,0,0.0785777,"Missing"
P06-1058,P02-1044,0,0.0136108,"l Corpus Parallel corpus is a solution to the bottleneck of knowledge acquisition. Ide et al. (2001 and 2002), Ng et al. (2003), and Diab (2003, 2004a, and 2004b) made research on the use of alignment for WSD. Diab and Resnik (2002) investigated the feasibility of automatically annotating large amounts of data in parallel corpora using an unsupervised algorithm, making use of two languages simultaneously, only one of which has an available sense inventory. The results showed that wordlevel translation correspondences are a valuable source of information for sense disambiguation. The method by Li and Li (2002) does not require parallel corpus. It avoids the alignment work and takes advantage of bilingual corpus. In short, technology of automatic corpus tagging is based on the manually labeled corpus. That is to say, it still need human intervention and is not a completely unsupervised method. Large-scale parallel corpus; especially wordaligned corpus is highly unobtainable, which has limited the WSD methods based on parallel corpus. 3 Equivalent Pseudoword This section describes how to obtain equivalent pseudowords without a seed corpus. Monosemous words are unambiguous priori knowledge. According"
P06-1058,mihalcea-2002-bootstrapping,0,0.0531623,"involves automatic generation of tagged corpus, bilingual corpus alignment, etc. The value of unsupervised methods lies in the knowledge acquisition solutions they adopt. 2.1 Automatic Generation of Training Corpus Automatic corpus tagging is a solution to WSD, which generates large-scale corpus from a small seed corpus. This is a weakly supervised learning or semi-supervised learning method. This reinforcement algorithm dates back to Gale et al. (1992a). Their investigation was based on a 6word test set with 2 senses for each word. Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to the bottleneck of knowledge acquisition. Ide et al. (2001 and 2002), Ng et al. (2003), and Diab (2003, 2004a, and 2004b) made research on the use of alignment"
P06-1058,N03-2023,0,0.154789,"n be used as a knowledge source for WSD. 3.1 Definition of Equivalent Pseudoword If the ambiguous words in the corpus are replaced with its synonymous monosemous word, then is it convenient to acquire knowledge from raw corpus? For example in table 1, the ambiguous word &quot;把握&quot; has three senses, whose synonymous monosemous words are listed on the right column. These synonyms contain some information for disambiguation task. An artificial ambiguous word can be coined with the monosemous words in table 1. This process is similar to the use of general pseudowords (Gale et al., 1992b; Gaustad, 2001; Nakov and Hearst, 2003), but has some essential differences. This artificial ambiguous word need to simulate the function of the real ambiguous word, and to acquire semantic knowledge as the real ambiguous word does. Thus, we call it an equivalent pseudoword (EP) for its equivalence with the real ambiguous word. It's apparent that the equivalent pseudoword has provided a new way to unsupervised WSD. S1 信心/自信心 把握(ba3 wo4) S2 握住/在握/把住/抓住/控制 S3 领会/理解/领悟/深谙/体会 Table 1. Synonymous Monosemous Words for the Ambiguous Word &quot;把握&quot; The equivalence of the EP with the real ambiguous word is a kind of semantic synonym or similarit"
P06-1058,P03-1058,0,0.0859216,"Missing"
P06-1058,P05-1049,0,0.0167964,"solutions they adopt. 2.1 Automatic Generation of Training Corpus Automatic corpus tagging is a solution to WSD, which generates large-scale corpus from a small seed corpus. This is a weakly supervised learning or semi-supervised learning method. This reinforcement algorithm dates back to Gale et al. (1992a). Their investigation was based on a 6word test set with 2 senses for each word. Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to the bottleneck of knowledge acquisition. Ide et al. (2001 and 2002), Ng et al. (2003), and Diab (2003, 2004a, and 2004b) made research on the use of alignment for WSD. Diab and Resnik (2002) investigated the feasibility of automatically annotating large amounts of data in parallel corpora using an unsupervise"
P06-1058,J98-1004,0,0.0966695,"P-based WSD It is based on the following assumptions that EPs can substitute the original ambiguous word for knowledge acquisition in WSD model training. Assumption 1: Words of the same meaning play the same role in a language. The sense is an important attribute of a word. This plays as the basic assumption in this paper. Assumption 2: Words of the same meaning occur in similar context. This assumption is widely used in semantic analysis and plays as a basis for much related research. For example, some researchers cluster the contexts of ambiguous words for WSD, which shows good performance (Schutze, 1998). Because an EP has a higher similarity with the ambiguous word in syntax and semantics, it is a useful knowledge source for WSD. 3.4 Implementation of the EP-based Solution 3.3 Design and Construction of EPs Because of the special characteristics of EPs, it's more difficult to construct an EP than a general pseudo word. To ensure the maximum similarity between the EP and the original ambiguous word, the following principles should be followed. 1) Every EP should map to one and only one original ambiguous word. 2) The morphemes of an EP should map one by one to those of the original ambiguous"
P06-1058,P94-1013,0,0.0628952,"d corpus. Unsupervised method is an alternative, which often involves automatic generation of tagged corpus, bilingual corpus alignment, etc. The value of unsupervised methods lies in the knowledge acquisition solutions they adopt. 2.1 Automatic Generation of Training Corpus Automatic corpus tagging is a solution to WSD, which generates large-scale corpus from a small seed corpus. This is a weakly supervised learning or semi-supervised learning method. This reinforcement algorithm dates back to Gale et al. (1992a). Their investigation was based on a 6word test set with 2 senses for each word. Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to the bottleneck of knowledge acquisition. Ide et al. (2001 and 2002), Ng et al. (2003), and Diab ("
P06-1058,P95-1026,0,0.70339,"Missing"
P06-1058,J04-1001,0,\N,Missing
P06-1075,W00-1312,0,0.0299122,"and the KIDS IR system. In section 4, we describe our experimental method. Section 5 and section 6 reports and discusses the experimental results. Finally we present our conclusion and future work in section 7. 593 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 593–600, c Sydney, July 2006. 2006 Association for Computational Linguistics 2 2.1 Related Work Effect of Translation Resources Previous studies have explored the effect of translation resources such as bilingual wordlists or parallel corpora on CLIR performance. Xu and Weischedel (2000) measured CLIR performance as a function of bilingual dictionary size. Their English-Chinese CLIR experiments on TREC 5&6 Chinese collections showed that the initial retrieval performance increased sharply with lexicon size but the performance was not improved after the lexicon exceeded 20,000 terms. Demner-Fushman and Oard (2003) identified eight types of terms that affected retrieval effectiveness in CLIR applications through their coverage by general-purpose bilingual term lists. They reported results from an evaluation of the coverage of 35 bilingual term lists in news retrieval applicatio"
P06-1126,P00-1073,0,0.0138271,"discusses the related work on language model pruning. Section 3 proposes our discriminative pruning method for Chinese word segmentation. Section 4 describes the experimental settings and results. Result analysis and discussions are also presented in this section. We draw the conclusions in section 5. 2 Related Work A simple way to reduce the size of an n-gram language model is to exclude those n-grams occurring infrequently in training corpus. It is named as count cut-off method (Jelinek, 1990). Because counts are always integers, the size of the model can only be reduced to discrete values. Gao and Lee (2000) proposed a distributionbased pruning. Instead of pruning n-grams that are infrequent in training data, they prune ngrams that are likely to be infrequent in a new document. Experimental results show that it is better than traditional count cut-off method. Seymore and Rosenfeld (1996) proposed a method to measure the difference of the models before and after pruning each n-gram, and the difference is computed as: − N ( h j , wi ) × [log P ′( wi |h j ) − log P ( wi |h j )] (1) Where P(wi|hj) denotes the conditional probabilities assigned by the original model, and P′(wi|hj) denotes the probabil"
P06-1126,P03-1035,0,0.0364599,"Missing"
P06-1126,J05-4005,0,0.0305322,"Missing"
P06-1126,P02-1023,0,0.0884085,"del pruning techniques are used to produce smaller models. Pruning a language model is to eliminate a number of parameters explicitly stored in it, according to some pruning criteria. The goal of research for language model pruning is to find criteria or methods, using which the model size could be reduced effectively, while the performance loss is kept as small as possible. A few criteria have been presented for language model pruning, including count cut-off (Jelinek, 1990), weighted difference factor (Seymore and Rosenfeld, 1996), KullbackLeibler distance (Stolcke, 1998), rank and entropy (Gao and Zhang, 2002). These criteria are general for language model pruning, and are not optimized according to the performance of language model in specific tasks. In recent years, discriminative training has been introduced to natural language processing applications such as parsing (Collins, 2000), machine translation (Och and Ney, 2002) and language model building (Kuo et al., 2002; Roark et al., 2004). To the best of our knowledge, it has not been applied to language model pruning. In this paper, we propose a discriminative pruning method of n-gram language model for Chinese word segmentation. It differentia"
P06-1126,P02-1038,0,0.026601,"le the performance loss is kept as small as possible. A few criteria have been presented for language model pruning, including count cut-off (Jelinek, 1990), weighted difference factor (Seymore and Rosenfeld, 1996), KullbackLeibler distance (Stolcke, 1998), rank and entropy (Gao and Zhang, 2002). These criteria are general for language model pruning, and are not optimized according to the performance of language model in specific tasks. In recent years, discriminative training has been introduced to natural language processing applications such as parsing (Collins, 2000), machine translation (Och and Ney, 2002) and language model building (Kuo et al., 2002; Roark et al., 2004). To the best of our knowledge, it has not been applied to language model pruning. In this paper, we propose a discriminative pruning method of n-gram language model for Chinese word segmentation. It differentiates from the previous pruning approaches in two respects. First, the pruning criterion is based on performance variation of word segmentation. Second, the model of desired size is achieved by adding valuable bigrams to a base model, instead of by pruning bigrams from an unpruned model. We define a misclassification funct"
P06-1126,C04-1081,0,0.0437014,"Missing"
P06-1126,P04-1007,0,0.0204108,"ia have been presented for language model pruning, including count cut-off (Jelinek, 1990), weighted difference factor (Seymore and Rosenfeld, 1996), KullbackLeibler distance (Stolcke, 1998), rank and entropy (Gao and Zhang, 2002). These criteria are general for language model pruning, and are not optimized according to the performance of language model in specific tasks. In recent years, discriminative training has been introduced to natural language processing applications such as parsing (Collins, 2000), machine translation (Och and Ney, 2002) and language model building (Kuo et al., 2002; Roark et al., 2004). To the best of our knowledge, it has not been applied to language model pruning. In this paper, we propose a discriminative pruning method of n-gram language model for Chinese word segmentation. It differentiates from the previous pruning approaches in two respects. First, the pruning criterion is based on performance variation of word segmentation. Second, the model of desired size is achieved by adding valuable bigrams to a base model, instead of by pruning bigrams from an unpruned model. We define a misclassification function that approximately represents the likelihood that a sentence wi"
P06-1126,J96-3004,0,0.140619,"Missing"
P06-1126,W03-1730,0,0.104016,"Missing"
P06-2112,W05-0819,0,0.133738,"Missing"
P06-2112,J93-2003,0,0.0221963,"Missing"
P06-2112,P03-1012,0,0.0439111,"Missing"
P06-2112,J97-2004,0,0.0671268,"nly using the small bilingual corpus in L1 and L2. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). Many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Zhang and Gildea, 2005). In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is unavailable, some researchers acquired class-based alignment rules with existing dictionaries to improve word alignment (Ker and Chang, 1997). Wu et al. (2005) used a large-scale bilingual corpus in general domain to improve domain-specific word alignment when only a small-scale domainspecific bilingual corpus is available. This paper proposes an approach to improve word alignment for languages with scarce resources using bilingual corpora of other language pairs. To perform word alignment between languages L1 and L2, we introduce a third language L3 as the pivot language. Although only small amounts of bilingual data are available for the desired language pair L1-L2, large-scale bilingual corpora in L1-L3 and L2-L3 are available."
P06-2112,W05-0812,0,0.0732401,"Missing"
P06-2112,W05-0809,0,0.0478672,"Missing"
P06-2112,W02-2026,0,0.0964339,"nse of an instance of the ambiguous English word e can be determined by the context in which the instance appears. Thus, the cross-language word similarity between the Chinese word c and the Japanese word f can be calculated according to the contexts of their English translation e. We use the feature vector constructed using the context words in the English sentence to represent the context. So we can calculate the cross-language word similarity using the feature vectors. The detailed algorithm is shown in figure 1. This idea is similar to translation lexicon extraction via a bridge language (Schafer and Yarowsky, 2002). For example, the Chinese word ""河岸"" and its English translation ""bank"" (the border of a river) appears in the following Chinese-English sentence pair: (a) 他们沿着河岸走回家。 (b) They walked home along the river bank. The Japanese word "" 銀 行 "" and its English translation ""bank"" (a financial organization) appears in the following English-Japanese sentence pair: (c) He has plenty of money in the bank. (d) 彼は銀行預金が相当ある。 The context words of the English word ""bank"" in sentences (b) and (c) are quite different. The difference indicates the cross language word similarity of the Chinese word ""河岸"" and the Japa"
P06-2112,W05-0817,0,0.14113,"Missing"
P06-2112,P00-1056,0,0.245002,"et Words 237,834 4,480,034 1,685,204 Besides the training data, we also have heldout data and testing data. The held-out data includes 500 Chinese-Japanese sentence pairs, which is used to set the interpolated weights described in section 5. We use another 1,000 Chinese-Japanese sentence pairs as testing data, which is not included in the training data and the held-out data. The alignment links in the held-out data and the testing data are manually annotated. Testing data includes 4,926 alignment links 2 . We use the same metrics as described in Wu et al. (2005), which is similar to those in (Och and Ney, 2000). The difference lies in that Wu et al. (2005) took all alignment links as sure links. If we use S G to represent the set of alignment links identified by the proposed methods and S C to denote the reference alignment set, the methods to calculate the precision, recall, f-measure, and alignment error rate (AER) are shown in equations (18), (19), (20), and (21), respectively. It can be seen that the higher the f-measure is, the lower the alignment error rate is. Thus, we will only show precision, recall and AER scores in the evaluation results. recall = |SG ∩ SC | |SG | |SG ∩ SC | |SC | (18) (1"
P06-2112,J03-1002,0,0.0118618,"Missing"
P06-2112,J97-3002,0,0.110144,"Missing"
P06-2112,P05-1058,1,0.888651,"Missing"
P06-2112,P05-1059,0,0.0222278,"Missing"
P06-2117,J93-2003,0,0.0177669,"Missing"
P06-2117,P03-1012,0,0.0418338,"Missing"
P06-2117,W99-0613,0,0.0619412,"by automatically building a pseudo reference set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Basu et al., 2004), named entity classification (Collins and Singer, 1999), and parsing (Sarkar, 2001). In this paper, we propose a semi-supervised boosting method to improve statistical word alignment with both limited labeled data and large amounts of unlabeled data. The proposed approach modifies the supervised AdaBoost algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data. Therefore, it should address the following three problems. The first is to build a word alignment model with both labeled and unlabeled data. In this paper, with the labeled data, we build a supervised model by directly estimating the parameters in 913 Proceedin"
P06-2117,P00-1056,0,0.779615,"ly 2006. 2006 Association for Computational Linguistics the model instead of using the Expectation Maximization (EM) algorithm in Brown et al. (1993). With the unlabeled data, we build an unsupervised model by estimating the parameters with the EM algorithm. Based on these two word alignment models, an interpolated model is built through linear interpolation. This interpolated model is used as a learner in the semi-supervised AdaBoost algorithm. The second is to build a reference set for the unlabeled data. It is automatically built with a modified &quot;refined&quot; combination method as described in Och and Ney (2000). The third is to calculate the error rate on each round. Although we build a reference set for the unlabeled data, it still contains alignment errors. Thus, we use the reference set of the labeled data instead of that of the entire training data to calculate the error rate on each round. With the interpolated model as a learner in the semi-supervised AdaBoost algorithm, we investigate two boosting methods in this paper to improve statistical word alignment. The first method uses the unlabeled data only in the interpolated model. During training, it only changes the distribution of the labeled"
P06-2117,J03-1002,0,0.0172725,"Missing"
P06-2117,N01-1023,0,0.0160004,"ence set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Basu et al., 2004), named entity classification (Collins and Singer, 1999), and parsing (Sarkar, 2001). In this paper, we propose a semi-supervised boosting method to improve statistical word alignment with both limited labeled data and large amounts of unlabeled data. The proposed approach modifies the supervised AdaBoost algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data. Therefore, it should address the following three problems. The first is to build a word alignment model with both labeled and unlabeled data. In this paper, with the labeled data, we build a supervised model by directly estimating the parameters in 913 Proceedings of the COLING/ACL 2006 Ma"
P06-2117,J97-3002,0,0.0577505,"Missing"
P06-2117,2005.mtsummit-papers.41,1,0.850989,"he performances of the word aligners with available data and available alignment models. One possible solution is to use the boosting method (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 2000). The underlying idea of boosting is to combine simple &quot;rules&quot; to form an ensemble such that the performance of the single ensemble is improved. The AdaBoost (Adaptive Boosting) algorithm by Freund and Schapire (1996) was developed for supervised learning. When it is applied to word alignment, it should solve the problem of building a reference set for the unlabeled data. Wu and Wang (2005) developed an unsupervised AdaBoost algorithm by automatically building a pseudo reference set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Bas"
P06-2117,P05-1058,1,0.884677,"Missing"
P06-2117,P95-1026,0,0.0797415,"alignment, it should solve the problem of building a reference set for the unlabeled data. Wu and Wang (2005) developed an unsupervised AdaBoost algorithm by automatically building a pseudo reference set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Basu et al., 2004), named entity classification (Collins and Singer, 1999), and parsing (Sarkar, 2001). In this paper, we propose a semi-supervised boosting method to improve statistical word alignment with both limited labeled data and large amounts of unlabeled data. The proposed approach modifies the supervised AdaBoost algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data. Therefore, it should address the following three problems. The first is to build a word alignmen"
P06-2117,P05-1059,0,0.0246094,"Missing"
P07-1108,W05-0819,0,0.0168646,"word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 2005; Tufis et al., 2005) used language-dependent resources such as dictionary, thesaurus, and dependency parser to improve word alignment results. 857 In this paper, we address the translation problem for language pairs with scarce resources by bringing in a pivot language, via which we can make use of large bilingual corpora. Our method does not need language-dependent resources or deep linguistic processing. Thus, the method is easy to be adapted to any language pair where a pivot language and corresponding large bilingual corpora are available. 3 Phrase-Based SMT Accord"
P07-1108,J93-2003,0,0.0513623,"Missing"
P07-1108,N06-1003,0,0.0365215,"two kinds of methods: those using pivot language and those using a small bilingual corpus or scarce resources. For the first kind, pivot languages are employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001; Kishida and Kando, 2003). These methods only used the available dictionaries to perform word by word translation. In addition, NTCIR 4 workshop organized a shared task for CLIR using pivot language. Machine translation systems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-sy"
P07-1108,2000.eamt-1.5,0,0.099614,"Missing"
P07-1108,C00-1015,0,0.0314286,"ind, pivot languages are employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001; Kishida and Kando, 2003). These methods only used the available dictionaries to perform word by word translation. In addition, NTCIR 4 workshop organized a shared task for CLIR using pivot language. Machine translation systems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) us"
P07-1108,P05-1033,0,0.0520512,"Missing"
P07-1108,P03-1021,0,0.0523928,"and English-Le since Europarl corpus is a multilingual corpus. For the language models, we use the same data provided in the shared task. We also use the same development set and test set provided by the shared task. The in-domain test set includes 2,000 sentences and the out-of-domain test set includes 1,064 sentences for each language. 6.2 Translation Method System and Evaluation To perform phrase-based SMT, we use Koehn&apos;s training scripts1 and the Pharaoh decoder (Koehn, 2004). We run the decoder with its default settings and then use Koehn&apos;s implementation of minimum error rate training (Och, 2003) to tune the feature weights on the development set. The translation quality was evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). And we also use the tool provided in the NAACL/HLT 2006 shared task on SMT to calculate the BLEU scores. 6.3 Comparison of Different Lexical Weights As described in section 4, we employ two methods to estimate the lexical weight in the translation model. In order to compare the two methods, we translate from French to Spanish, using English as the pivot language. We use the French-English and English-Spanish corpora described"
P07-1108,P02-1033,0,0.00633302,"sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 200"
P07-1108,J04-4002,0,0.0289727,"Missing"
P07-1108,koen-2004-pharaoh,0,0.0244223,"3,134,411 12,155,876 15,222,105 16,052,269 14,362,615 Table 1. Training Corpus for European Languages extracted from Lf-English and English-Le since Europarl corpus is a multilingual corpus. For the language models, we use the same data provided in the shared task. We also use the same development set and test set provided by the shared task. The in-domain test set includes 2,000 sentences and the out-of-domain test set includes 1,064 sentences for each language. 6.2 Translation Method System and Evaluation To perform phrase-based SMT, we use Koehn&apos;s training scripts1 and the Pharaoh decoder (Koehn, 2004). We run the decoder with its default settings and then use Koehn&apos;s implementation of minimum error rate training (Och, 2003) to tune the feature weights on the development set. The translation quality was evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). And we also use the tool provided in the NAACL/HLT 2006 shared task on SMT to calculate the BLEU scores. 6.3 Comparison of Different Lexical Weights As described in section 4, we employ two methods to estimate the lexical weight in the translation model. In order to compare the two methods, we translate"
P07-1108,2005.mtsummit-papers.11,0,0.0230044,"( f |e) and p w,0 ( f |e, a) denote the phrase translation probability and lexical weight trained with the Lf-Le bilingual corpus, respectively. φi ( f |e) and p w,i ( f |e, a ) ( i = 1,..., n ) are the phrase translation probability and lexical weight estimated by using the pivot languages. α i and β i are the interpolation coefficients. 6 6.1 Experiments on the Europarl Corpus Data A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006). The shared task used the Europarl corpus (Koehn, 2005), in which four languages are involved: English, French, Spanish, and German. The shared task performed translation between English and the other three languages. In our work, we perform translation from French to the other three languages. We select French to Spanish and French to German translation that are not in the shared task because we want to use English as the pivot language. In general, for most of the languages, there exist bilingual corpora between these languages and English since English is an internationally used language. Table 1 shows the information about the bilingual traini"
P07-1108,W06-3114,0,0.0171445,"=0 n p w ( f |e, a) = ∑ β i p w,i ( f |e, a) (11) i =0 Where φ 0 ( f |e) and p w,0 ( f |e, a) denote the phrase translation probability and lexical weight trained with the Lf-Le bilingual corpus, respectively. φi ( f |e) and p w,i ( f |e, a ) ( i = 1,..., n ) are the phrase translation probability and lexical weight estimated by using the pivot languages. α i and β i are the interpolation coefficients. 6 6.1 Experiments on the Europarl Corpus Data A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006). The shared task used the Europarl corpus (Koehn, 2005), in which four languages are involved: English, French, Spanish, and German. The shared task performed translation between English and the other three languages. In our work, we perform translation from French to the other three languages. We select French to Spanish and French to German translation that are not in the shared task because we want to use English as the pivot language. In general, for most of the languages, there exist bilingual corpora between these languages and English since English is an internationally used language."
P07-1108,N03-1017,0,0.28741,"5) used language-dependent resources such as dictionary, thesaurus, and dependency parser to improve word alignment results. 857 In this paper, we address the translation problem for language pairs with scarce resources by bringing in a pivot language, via which we can make use of large bilingual corpora. Our method does not need language-dependent resources or deep linguistic processing. Thus, the method is easy to be adapted to any language pair where a pivot language and corresponding large bilingual corpora are available. 3 Phrase-Based SMT According to the translation model presented in (Koehn et al., 2003), given a source sentence f , the best target translation e best can be obtained according to the following model e best = arg max e p (e |f ) (1) = arg max e p (f |e ) pLM (e )ω length ( e ) Where the translation model p (f |e ) can be decomposed into I I p ( f 1 |e1 ) = I ∏φ( f i |e i )d (a i − bi −1 ) p w ( f i |e i , a) λ (2) i =1 Where φ ( f i |e i ) and d (ai − bi −1 ) denote phrase translation probability and distortion probability, respectively. p w ( f i |e i , a ) is the lexical weight, and λ is the strength of the lexical weight. 4 Phrase-Based SMT Via Pivot Language This section wi"
P07-1108,W05-0812,0,0.00821932,"iab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 2005; Tufis et al., 2005) used language-dependent resources such as dictionary, thesaurus, and dependency parser to improve word alignment results. 857 In this paper, we address the translation problem for language pairs with scarce resources by bringing in a pivot language, via which we can make use of large bilingual corpora. Our method does not need language-dependent resources or deep linguistic processing. Thus, the method is easy to be adapted to any language pair where a pivot language and corresponding large bilingual corpora are available. 3 Phrase-Based SMT According to the translation m"
P07-1108,W05-0809,0,0.0684983,"Missing"
P07-1108,P04-1083,0,0.0852707,"Missing"
P07-1108,P05-1034,0,0.0286919,"Missing"
P07-1108,W02-2026,0,0.0103997,"ystems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers"
P07-1108,P06-2112,1,0.843722,"ges are employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001; Kishida and Kando, 2003). These methods only used the available dictionaries to perform word by word translation. In addition, NTCIR 4 workshop organized a shared task for CLIR using pivot language. Machine translation systems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictiona"
P07-1108,W05-0817,0,0.00821302,"nd so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 2005; Tufis et al., 2005) used language-dependent resources such as dictionary, thesaurus, and dependency parser to improve word alignment results. 857 In this paper, we address the translation problem for language pairs with scarce resources by bringing in a pivot language, via which we can make use of large bilingual corpora. Our method does not need language-dependent resources or deep linguistic processing. Thus, the method is easy to be adapted to any language pair where a pivot language and corresponding large bilingual corpora are available. 3 Phrase-Based SMT According to the translation model presented in (Ko"
P07-1108,J97-3002,0,0.0177418,"Missing"
P07-1108,P01-1067,0,0.020506,"Missing"
P07-1108,J03-3002,0,\N,Missing
P07-1108,P02-1040,0,\N,Missing
P07-1108,N07-1061,0,\N,Missing
P07-1108,J04-2003,0,\N,Missing
P07-1108,P02-1038,0,\N,Missing
P07-1108,2006.eamt-1.24,0,\N,Missing
P07-1108,vandeghinste-etal-2006-metis,0,\N,Missing
P07-1108,2006.amta-papers.11,0,\N,Missing
P08-1089,P05-1074,0,0.836647,"el, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Ibrahim et al., 2003; Pang et al., 2003; Szpektor et al., 2004). However, these methods have some shortcomings. Especially, the precisions of the paraphrase patterns extracted with these methods are relatively low. In this paper, we extract paraphrase patterns from bilingual parallel corpora based on a pivot approach. We assume that if two English patterns are aligned with the same pattern in another language, they are likely to be paraphrase patterns. This assumption is an extension of the one presented in (Bannard and Callison-Burch, 2005), which was used for deriving phrasal paraphrases from bilingual corpora. Our method involves three steps: (1) corpus preprocessing, including English monolingual dependency 780 Proceedings of ACL-08: HLT, pages 780–788, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics parsing and English-foreign language word alignment, (2) aligned patterns induction, which produces English patterns along with the aligned pivot patterns in the foreign language, (3) paraphrase patterns extraction, in which paraphrase patterns are extracted based on a log-linear model. Our contri"
P08-1089,N06-1003,0,0.288784,"Missing"
P08-1089,W03-1608,0,0.229068,"or sentence) by filling the pattern slots with specific words. Paraphrase patterns are useful in both paraphrase recognition and generation. In paraphrase recognition, if two text units match a pair of paraphrase patterns and the corresponding slot-fillers are identical, they can be identified as paraphrases. In paraphrase generation, a text unit that matches a pattern P can be rewritten using the paraphrase patterns of P. A variety of methods have been proposed on paraphrase patterns extraction (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Ibrahim et al., 2003; Pang et al., 2003; Szpektor et al., 2004). However, these methods have some shortcomings. Especially, the precisions of the paraphrase patterns extracted with these methods are relatively low. In this paper, we extract paraphrase patterns from bilingual parallel corpora based on a pivot approach. We assume that if two English patterns are aligned with the same pattern in another language, they are likely to be paraphrase patterns. This assumption is an extension of the one presented in (Bannard and Callison-Burch, 2005), which was used for deriving phrasal paraphrases from bilingual corpora."
P08-1089,N06-1058,0,0.313374,"Missing"
P08-1089,N03-1017,0,0.0127798,"hm 1 denotes the POS tag of wk . 783 weight. In this paper, 4 feature functions are used in our log-linear model, which include: h1 (e1 , e2 , c) = scoreM LE (c|e1 ) h2 (e1 , e2 , c) = scoreM LE (e2 |c) h3 (e1 , e2 , c) = scoreLW (c|e1 ) h4 (e1 , e2 , c) = scoreLW (e2 |c) Feature functions h1 (e1 , e2 , c) and h2 (e1 , e2 , c) are based on MLE. scoreM LE (c|e) is computed as: scoreM LE (c|e) = log pM LE (c|e) (4) scoreM LE (e|c) is computed in the same way. h3 (e1 , e2 , c) and h4 (e1 , e2 , c) are based on LW. LW was originally used to validate the quality of a phrase translation pair in MT (Koehn et al., 2003). It checks how well the words of the phrases translate to each other. This paper uses LW to measure the quality of aligned patterns. We define scoreLW (c|e) as the logarithm of the lexical weight3 : scoreLW (c|e) = n X 1X 1 log( w(ci |ej )) n |{j|(i, j) ∈ a}| i=1 4 (5) ∀(i,j)∈a where a denotes the word alignment between c and e. n is the number of words in c. ci and ej are words of c and e. w(ci |ej ) is computed as follows: count(ci , ej ) w(ci |ej ) = P 0 c0 count(ci , ej ) (6) i where count(ci , ej ) is the frequency count of the aligned word pair (ci , ej ) in the corpus. scoreLW (e|c) is"
P08-1089,W06-2931,1,0.814779,"Missing"
P08-1089,P00-1056,0,0.00494939,"p(e2 |e1 ) = X pM LE (c|e1 )pM LE (e2 |c) (1) c In Equation (1), pM LE (c|e1 ) and pM LE (e2 |c) are the probabilities of translating e1 to c and c to e2 , which are computed based on MLE: count(c, e1 ) pM LE (c|e1 ) = P 0 c0 count(c , e1 ) Figure 2: Examples of a subtree and a partial subtree. 3 Proposed Method 3.1 In this paper, we use English paraphrase patterns extraction as a case study. An English-Chinese (EC) bilingual parallel corpus is employed for training. The Chinese part of the corpus is used as pivots to extract English paraphrase patterns. We conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag heuristic (Koehn et al., 2005) for symmetrization. Since the paraphrase patterns are extracted from dependency trees, we parse the English sentences in the corpus with MaltParser (Nivre et al., 2007). Let SE be an English sentence, TE the parse tree of SE , e a word of SE , we define the subtree and partial subtree following the definitions in (Ouangraoua et al., 2007). In detail, a subtree STE (e) is a particular connected subgraph of the tree TE , which is rooted at e and includes all the descendants of e. A partial subtree P STE (e) is a conn"
P08-1089,N03-1024,0,0.274941,"g the pattern slots with specific words. Paraphrase patterns are useful in both paraphrase recognition and generation. In paraphrase recognition, if two text units match a pair of paraphrase patterns and the corresponding slot-fillers are identical, they can be identified as paraphrases. In paraphrase generation, a text unit that matches a pattern P can be rewritten using the paraphrase patterns of P. A variety of methods have been proposed on paraphrase patterns extraction (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Ibrahim et al., 2003; Pang et al., 2003; Szpektor et al., 2004). However, these methods have some shortcomings. Especially, the precisions of the paraphrase patterns extracted with these methods are relatively low. In this paper, we extract paraphrase patterns from bilingual parallel corpora based on a pivot approach. We assume that if two English patterns are aligned with the same pattern in another language, they are likely to be paraphrase patterns. This assumption is an extension of the one presented in (Bannard and Callison-Burch, 2005), which was used for deriving phrasal paraphrases from bilingual corpora. Our method involve"
P08-1089,W04-3219,0,0.6299,"Missing"
P08-1089,P02-1006,0,0.183123,". Hence the precision can be enhanced. In this experiment, we also estimated a threshold T 0 for MLE-Model using the development set (T 0 = −5.1). The pattern pairs whose score based on Equation (1) exceed T 0 were extracted as paraphrase patterns. 6 785 It is necessary to compare our method with another paraphrase patterns extraction method. However, it is difficult to find methods that are suitable for comparison. Some methods only extract paraphrase patterns using news articles on certain topics (Shinyama et al., 2002; Barzilay and Lee, 2003), while some others need seeds as initial input (Ravichandran and Hovy, 2002). In this paper, we compare our method with DIRT (Lin and Pantel, 2001), which does not need to specify topics or input seeds. As mentioned in Section 2, DIRT learns paraphrase patterns from a parsed monolingual corpus based on an extended distributional hypothesis. In our experiment, we implemented DIRT and extracted paraphrase patterns from the English part of our bilingual parallel corpus. Our corpus is smaller than that reported in (Lin and Pantel, 2001). To alleviate the data sparseness problem, we only kept patterns appearing more than 10 times in the corpus for extracting paraphrase pat"
P08-1089,N03-1003,0,\N,Missing
P08-1089,2005.iwslt-1.8,0,\N,Missing
P08-1089,W04-3206,0,\N,Missing
P09-1006,abeille-etal-2000-building,0,0.0765618,"Missing"
P09-1006,W00-1201,0,0.0822755,"Missing"
P09-1006,N07-1051,0,0.068459,"Missing"
P09-1006,D08-1092,0,0.11849,"Missing"
P09-1006,A00-2018,0,0.602877,"Missing"
P09-1006,P07-1078,0,0.0304062,"Missing"
P09-1006,P05-1022,0,0.22895,"Missing"
P09-1006,N03-1027,0,0.0627931,"Missing"
P09-1006,P94-1034,0,0.283773,"a higher performance when in-domain data was weighted more heavily than out-of-domain data. McClosky et al. (2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Brown corpus. Their results indicated that both unlabeled in-domain data and labeled out-of-domain data can help domain adaptation. In comparison with these works, we conduct our study in a different setting where we work with multiple heterogeneous treebanks. Grammar formalism conversion makes it possible to reuse existing source treebanks for the study of target grammar parsing. Wang et al. (1994) employed a parser to help conversion of a treebank from a simple phrase structure to a more informative phrase structure and then used this converted treebank to train their parser. Collins et al. (1999) performed statistical constituency parsing of Czech on a treebank that was converted from the Prague Dependency Treebank under the guidance of conversion rules and heuristic rules, e.g., one level of projection for any category, minimal projection for any dependents, and fixed position of attachment. Xia and Palmer (2001) adopted better heuristic rules to build converted trees, which Availabl"
P09-1006,C02-1126,0,0.0609279,"Missing"
P09-1006,W01-0904,0,0.0582808,"Missing"
P09-1006,P99-1065,0,0.772284,"). It is important to acquire additional labeled data for the target grammar parsing through exploitation of existing source treebanks since there is often a shortage of labeled data. However, to our knowledge, there is no previous study on this issue. Recently there have been some works on using multiple treebanks for domain adaptation of parsers, where these treebanks have the same grammar formalism (McClosky et al., 2006b; Roark and Bacchiani, 2003). Other related works focus on converting one grammar formalism of a treebank to another and then conducting studies on the converted treebank (Collins et al., 1999; Forst, 2003; Wang et al., 1994; Watkinson and Manandhar, 2001). These works were done either on multiple treebanks with the same grammar formalism or on only one converted treebank. We see that their scenarios are different from ours as we work with multiple heterogeneous treebanks. For the use of heterogeneous treebanks1 , we propose a two-step solution: (1) converting the grammar formalism of the source treebank to the target one, (2) refining converted trees and using them as additional training data to build a target grammar parser. For grammar formalism conversion, we choose the DS to P"
P09-1006,H01-1014,0,0.695341,"works were done either on multiple treebanks with the same grammar formalism or on only one converted treebank. We see that their scenarios are different from ours as we work with multiple heterogeneous treebanks. For the use of heterogeneous treebanks1 , we propose a two-step solution: (1) converting the grammar formalism of the source treebank to the target one, (2) refining converted trees and using them as additional training data to build a target grammar parser. For grammar formalism conversion, we choose the DS to PS direction for the convenience of the comparison with existing works (Xia and Palmer, 2001; Xia et al., 2008). Specifically, we assume that the source grammar formalism is dependency We address the issue of using heterogeneous treebanks for parsing by breaking it down into two sub-problems, converting grammar formalisms of the treebanks to the same one, and parsing on these homogeneous treebanks. First we propose to employ an iteratively trained target grammar parser to perform grammar formalism conversion, eliminating predefined heuristic rules as required in previous methods. Then we provide two strategies to refine conversion results, and adopt a corpus weighting technique for p"
P09-1006,W03-2404,0,0.113977,"Missing"
P09-1006,han-etal-2002-development,0,0.0632138,"Missing"
P09-1006,I05-1007,0,0.0333223,"Missing"
P09-1006,P03-1056,0,0.0665153,"Missing"
P09-1006,J93-2004,0,0.0348317,"Missing"
P09-1006,N06-1020,0,0.0892901,"Missing"
P09-1006,P06-1043,0,0.102023,"Missing"
P09-1006,P06-1054,0,\N,Missing
P09-1006,N04-1032,0,\N,Missing
P09-1018,P07-1038,0,0.0535445,"to the pseudo references) to a score that indicates the quality of the translation. Scores are first generated independently for each translation, then the translations are ranked by their respective scores. The candidate with the highest score is selected as the final translation. This is achieved by optimizing the regression learning model’s output to correlate against a set of training examples, where the source sentences are provided with several reference translations, instead of manually labeling the translations produced by various systems with quantitative assessments as described in (Albrecht and Hwa, 2007; Duh, 2008). The advantage of our method is that we do not need to manually label the translations produced by each translation system, therefore enabling our method suitable for translation selection among any systems without additional manual work. 2 Pivot Methods for Phrase-based SMT 2.1 Triangulation Method Following the method described in Wu and Wang (2007), we train the source-pivot and pivot-target translation models using the source-pivot and pivot-target corpora, respectively. Based on these two models, we induce a source-target translation model, in which two important elements nee"
P09-1018,P03-1021,0,0.0771557,"m target sentences ti1 , ti2 , ..., tim . We rescore all the n × m candidates using both the source-pivot and pivot-target translation scores following the method described in Utiyama and Isahara (2007). If we use hf p and hpt to denote the features in the source-pivot and pivot-target systems, respectively, we get the optimal target translation according to the following formula. tˆ = argmax t L X sp pt pt (λsp k hk (s, p)+λk hk (p, t)) (4) k=1 Where L is the number of features used in SMT systems. λsp and λpt are feature weights set by performing minimum error rate training as described in Och (2003). 2.3 Synthetic Method There are two possible methods to obtain a sourcetarget corpus using the source-pivot and pivottarget corpora. One is to obtain target translations for the source sentences in the source-pivot corpus. This can be achieved by translating the pivot sentences in source-pivot corpus to target sentences with the pivot-target SMT system. The other is to obtain source translations for the target sentences in the pivot-target corpus using the pivot-source SMT system. And we can combine these two source-target corpora to produced a final synthetic corpus. Given a pivot sentence,"
P09-1018,2008.iwslt-papers.1,0,0.240307,"d Lapata 2007; Wu and Wang, 2007). It multiples corresponding translation probabilities and lexical weights in source-pivot and pivot-target translation models to induce a new source-target phrase table. We name it the triangulation method. The second is the sentence translation strategy, which first translates the source sentence to the pivot sentence, and then to the target sentence (Utiyama and Isahara, 2007; Khalilov et al., 2008). We name it the transfer method. The third is to use existing models to build a synthetic source-target corpus, from which a source-target model can be trained (Bertoldi et al., 2008). For example, we can obtain a source-pivot corpus by translating the pivot sentence in the source-pivot corpus into the target language with pivot-target translation models. We name it the synthetic method. The working condition with the pivot language approach is that the source-pivot and pivot-target parallel corpora are independent, in the sense that they are not derived from the same set of sentences, namely independently sourced corpora. Thus, some linguistic phenomena in the sourcepivot corpus will lost if they do not exist in the pivot-target corpus, and vice versa. In order to fill up"
P09-1018,P02-1040,0,0.0974401,"ative values of the scores assigned by the subject systems may change. In order to train a reliable learner, we need to prepare a balanced training set, where the translations produced by different systems under different conditions are required to be manually evaluated. In extreme cases, we need to relabel the training data to obtain better performance. In this paper, we modify the method in Albrecht and Hwa (2007) to only prepare human reference translations for the training examples, and then evaluate the translations produced by the subject systems against the references using BLEU score (Papineni et al., 2002). We use smoothed sentence-level BLEU score to replace the human assessments, where we use additive smoothing to avoid zero BLEU scores when we calculate the n-gram precisions. In this case, we 9-12 13-14 15-19 Description n-gram precisions against pseudo references (1 ≤ n ≤ 4) PER and WER precision, recall, fragmentation from METEOR (Lavie and Agarwal, 2007) precisions and recalls of nonconsecutive bigrams with a gap size of m (1 ≤ m ≤ 2) longest common subsequences n-gram precision against a target corpus (1 ≤ n ≤ 5) Table 1: Feature sets for regression learning can easily retrain the learne"
P09-1018,P07-1092,0,0.410823,"}@rdc.toshiba.com.cn Abstract approach (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang 2007; Bertoldi et al., 2008). This approach introduces a third language, named the pivot language, for which there exist large source-pivot and pivot-target bilingual corpora. A pivot task was also designed for spoken language translation in the evaluation campaign of IWSLT 2008 (Paul, 2008), where English is used as a pivot language for Chinese to Spanish translation. Three different pivot strategies have been investigated in the literature. The first is based on phrase table multiplication (Cohn and Lapata 2007; Wu and Wang, 2007). It multiples corresponding translation probabilities and lexical weights in source-pivot and pivot-target translation models to induce a new source-target phrase table. We name it the triangulation method. The second is the sentence translation strategy, which first translates the source sentence to the pivot sentence, and then to the target sentence (Utiyama and Isahara, 2007; Khalilov et al., 2008). We name it the transfer method. The third is to use existing models to build a synthetic source-target corpus, from which a source-target model can be trained (Bertoldi et a"
P09-1018,W08-0331,0,0.0612152,") to a score that indicates the quality of the translation. Scores are first generated independently for each translation, then the translations are ranked by their respective scores. The candidate with the highest score is selected as the final translation. This is achieved by optimizing the regression learning model’s output to correlate against a set of training examples, where the source sentences are provided with several reference translations, instead of manually labeling the translations produced by various systems with quantitative assessments as described in (Albrecht and Hwa, 2007; Duh, 2008). The advantage of our method is that we do not need to manually label the translations produced by each translation system, therefore enabling our method suitable for translation selection among any systems without additional manual work. 2 Pivot Methods for Phrase-based SMT 2.1 Triangulation Method Following the method described in Wu and Wang (2007), we train the source-pivot and pivot-target translation models using the source-pivot and pivot-target corpora, respectively. Based on these two models, we induce a source-target translation model, in which two important elements need to be indu"
P09-1018,N07-1061,0,0.569659,"where English is used as a pivot language for Chinese to Spanish translation. Three different pivot strategies have been investigated in the literature. The first is based on phrase table multiplication (Cohn and Lapata 2007; Wu and Wang, 2007). It multiples corresponding translation probabilities and lexical weights in source-pivot and pivot-target translation models to induce a new source-target phrase table. We name it the triangulation method. The second is the sentence translation strategy, which first translates the source sentence to the pivot sentence, and then to the target sentence (Utiyama and Isahara, 2007; Khalilov et al., 2008). We name it the transfer method. The third is to use existing models to build a synthetic source-target corpus, from which a source-target model can be trained (Bertoldi et al., 2008). For example, we can obtain a source-pivot corpus by translating the pivot sentence in the source-pivot corpus into the target language with pivot-target translation models. We name it the synthetic method. The working condition with the pivot language approach is that the source-pivot and pivot-target parallel corpora are independent, in the sense that they are not derived from the same"
P09-1018,D07-1030,1,0.916827,"Missing"
P09-1018,2008.iwslt-evaluation.18,1,0.934056,"tive is to infer a function that maps a feature vector (which measures the similarity of a translation from one system to the pseudo references) to a score that indicates the quality of the translation. Scores are first generated independently for each translation, then the translations are ranked by their respective scores. The candidate with the highest score is selected. The similar ideas have been explored in previous studies. Albrecht and Hwa (2007) proposed a method to evaluate MT outputs with pseudo references using support vector regression as the learner to evaluate translations. Duh (2008) proposed a ranking method to compare the translations proposed by several systems. These two methods require quantitative quality assessments by human judges for the translations produced by various systems in the training set. When we apply such methods to translation selection, the relative values of the scores assigned by the subject systems are important. In different data conditions, the relative values of the scores assigned by the subject systems may change. In order to train a reliable learner, we need to prepare a balanced training set, where the translations produced by different sy"
P09-1018,P07-1108,1,0.717108,"bstract approach (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang 2007; Bertoldi et al., 2008). This approach introduces a third language, named the pivot language, for which there exist large source-pivot and pivot-target bilingual corpora. A pivot task was also designed for spoken language translation in the evaluation campaign of IWSLT 2008 (Paul, 2008), where English is used as a pivot language for Chinese to Spanish translation. Three different pivot strategies have been investigated in the literature. The first is based on phrase table multiplication (Cohn and Lapata 2007; Wu and Wang, 2007). It multiples corresponding translation probabilities and lexical weights in source-pivot and pivot-target translation models to induce a new source-target phrase table. We name it the triangulation method. The second is the sentence translation strategy, which first translates the source sentence to the pivot sentence, and then to the target sentence (Utiyama and Isahara, 2007; Khalilov et al., 2008). We name it the transfer method. The third is to use existing models to build a synthetic source-target corpus, from which a source-target model can be trained (Bertoldi et al., 2008). For examp"
P09-1018,N03-1017,0,0.00975991,"models using the source-pivot and pivot-target corpora, respectively. Based on these two models, we induce a source-target translation model, in which two important elements need to be induced: phrase translation probability and lexical weight. Phrase Translation Probability We induce the phrase translation probability by assuming the independence between the source and target phrases when given the pivot phrase. X φ(¯ s|t¯) = φ(¯ s|¯ p)φ(¯ p|t¯) (1) p¯ Where s¯, p¯ and t¯ represent the phrases in the languages Ls , Lp and Lt , respectively. Lexical Weight According to the method described in Koehn et al. (2003), there are two important elements in the lexical weight: word alignment information a in a phrase pair (¯ s, t¯) and lexical translation probability w(s|t). Let a1 and a2 represent the word alignment information inside the phrase pairs (¯ s, p¯) and (¯ p, t¯) respectively, then the alignment information inside (¯ s, t¯) can be obtained as shown in Eq. (2). We conducted experiments for spoken language translation on the pivot task in the IWSLT 2008 evaluation campaign, where Chinese sentences in travel domain need to be translated into Spanish, with English as the pivot language. Experimental"
P09-1018,W07-0734,0,0.0278038,"Missing"
P09-1018,2008.iwslt-evaluation.17,0,\N,Missing
P09-1018,P07-2045,0,\N,Missing
P09-1018,2008.iwslt-evaluation.1,0,\N,Missing
P09-1091,C00-1007,0,0.287256,"2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank. Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization. The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al. 2007). The other is log-linear model with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). However, little work has been done on probabilistic models learning direct mapping from input to surface strings, without the effort to construct a grammar. Guo et al. (2008) develop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree al"
P09-1091,P06-1130,0,0.0340316,"Missing"
P09-1091,W07-2303,0,0.0447939,"zation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al. 2007). The other is log-linear model with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). However, little work has been done on probabilistic models learning direct mapping from input to surface strings, without the effort to construct a grammar. Guo et al. (2008) develop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word ordering, which first builds dependency trees to decide linear precedence between heads and modifiers then uses an n-gram language model to order siblings. Compared with n-gram model, log-linear model is more powerful in that it i"
P09-1091,P07-1041,0,0.0700134,"ose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word ordering, which first builds dependency trees to decide linear precedence between heads and modifiers then uses an n-gram language model to order siblings. Compared with n-gram model, log-linear model is more powerful in that it is easy to integrate a variety of features, and to tune feature weights to maximize the probability. A few papers have presented maximum entropy models for word or phrase ordering (Ratnaparkhi, 2000; Filippova and Strube, 2007). However, those attempts have been limited to specialized applications, such as air travel reservation or ordering constituents of a main clause in German. This paper presents a general-purpose realizer based on log-linear models for directly linearizing dependency relations given dependency structures. We reduce the generation space by 809 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 809–816, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP two techniques: the first is dividing the entire dependency tree into one-depth sub-trees and solving"
P09-1091,C08-1038,1,0.870281,"Missing"
P09-1091,D07-1028,0,0.0241365,"Missing"
P09-1091,A00-2023,0,0.0669447,"2007) resources derived from the Penn-II Treebank. Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization. The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al. 2007). The other is log-linear model with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). However, little work has been done on probabilistic models learning direct mapping from input to surface strings, without the effort to construct a grammar. Guo et al. (2008) develop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word o"
P09-1091,W02-2103,0,0.141967,"he features used in the log-linear model. The examples listed in the table are features of the linearization <node 6, node 4, node 3, node 5>, extracted from the sub-tree in Figure 4. In this paper, all the feature functions used in the log-linear model are n-gram probabilities. However, the log-linear framework has great potential for including other types of features. 4.3 Parameter Estimation BLEU score, a method originally proposed to automatically evaluate machine translation quality (Papineni et al., 2002), has been widely used as a metric to evaluate general-purpose sentence generation (Langkilde, 2002; White et al., 2007; Guo et al. 2008, Wan et al. 2009). The BLEU measure computes the geometric mean of the precision of n-grams of various lengths between a sentence realization and a (set of) reference(s). To estimate the parameters (λ1 ,..., λ M ) for the feature functions (h1 ,..., hM ) , we use BLEU 3 as optimization objective function and adopt the approach of minimum error rate training 2 Here the term “headword” is used to describe the word that occurs at head nodes in dependency trees. 3 The BLEU scoring script is supplied by NIST Open Machine Translation Evaluation at ftp://jaguar.n"
P09-1091,W05-1510,0,0.0148029,"ningful, grammatically correct and fluent text of a particular language. Most previous general-purpose realization systems are developed via the application of a set of grammar rules based on particular linguistic theories, e.g. Lexical Functional Grammar (LFG), Head Driven Phrase Structure Grammar (HPSG), Combinatory Categorical Grammar (CCG), Tree Adjoining Grammar (TAG) etc. The grammar rules are either developed by hand, such as those used in LinGo (Carroll et al., 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or extracted automatically from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank. Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization. The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over differe"
P09-1091,P03-1021,0,0.0262535,"Missing"
P09-1091,P02-1040,0,0.0766672,"thus the headword model is more likely to relax the data sparseness. Table 2 gives some examples of all the features used in the log-linear model. The examples listed in the table are features of the linearization <node 6, node 4, node 3, node 5>, extracted from the sub-tree in Figure 4. In this paper, all the feature functions used in the log-linear model are n-gram probabilities. However, the log-linear framework has great potential for including other types of features. 4.3 Parameter Estimation BLEU score, a method originally proposed to automatically evaluate machine translation quality (Papineni et al., 2002), has been widely used as a metric to evaluate general-purpose sentence generation (Langkilde, 2002; White et al., 2007; Guo et al. 2008, Wan et al. 2009). The BLEU measure computes the geometric mean of the precision of n-grams of various lengths between a sentence realization and a (set of) reference(s). To estimate the parameters (λ1 ,..., λ M ) for the feature functions (h1 ,..., hM ) , we use BLEU 3 as optimization objective function and adopt the approach of minimum error rate training 2 Here the term “headword” is used to describe the word that occurs at head nodes in dependency trees."
P09-1091,A00-2026,0,0.0386295,"elop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word ordering, which first builds dependency trees to decide linear precedence between heads and modifiers then uses an n-gram language model to order siblings. Compared with n-gram model, log-linear model is more powerful in that it is easy to integrate a variety of features, and to tune feature weights to maximize the probability. A few papers have presented maximum entropy models for word or phrase ordering (Ratnaparkhi, 2000; Filippova and Strube, 2007). However, those attempts have been limited to specialized applications, such as air travel reservation or ordering constituents of a main clause in German. This paper presents a general-purpose realizer based on log-linear models for directly linearizing dependency relations given dependency structures. We reduce the generation space by 809 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 809–816, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP two techniques: the first is dividing the entire dependency tree into on"
P09-1091,2005.mtsummit-papers.15,0,0.0230485,"gm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al. 2007). The other is log-linear model with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). However, little work has been done on probabilistic models learning direct mapping from input to surface strings, without the effort to construct a grammar. Guo et al. (2008) develop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word ordering, which first builds dependency trees to decide linear precedence between heads and modifiers then uses an n-gram language model to order siblings. Compared with n-gram model,"
P09-1091,2007.mtsummit-ucnlg.4,0,0.470821,"eral-purpose realization systems are developed via the application of a set of grammar rules based on particular linguistic theories, e.g. Lexical Functional Grammar (LFG), Head Driven Phrase Structure Grammar (HPSG), Combinatory Categorical Grammar (CCG), Tree Adjoining Grammar (TAG) etc. The grammar rules are either developed by hand, such as those used in LinGo (Carroll et al., 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or extracted automatically from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank. Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization. The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langki"
P09-1091,E09-1097,0,\N,Missing
P09-1091,W06-2931,1,\N,Missing
P10-1085,ahrenberg-etal-2000-evaluation,0,0.0345317,"ng decision rule. Chinese words 6.3M To investigate the quality of the generated word alignments, we randomly selected a subset from the bilingual corpus as test set, including 500 sentence pairs. Then word alignments in the subset were manually labeled, referring to the guideline of the Chinese-to-English alignment (LDC2006E93), but we made some modifications for the guideline. For example, if a preposition appears after a verb as a phrase aligned to one single word in the corresponding sentence, then they are glued together. There are several different evaluation metrics for word alignment (Ahrenberg et al., 2000). We use precision (P), recall (R) and alignment error ratio (AER), which are similar to those in Och and Ney (2000), except that we consider each alignment as a sure link. 828 Single word alignments Multi-word alignments P R AER P R AER Baseline 0.77 0.45 0.43 0.23 0.71 0.65 CM-1 0.70 0.50 0.42 0.35 0.86 0.50 Improved BWA methods CM-2 0.73 0.48 0.42 0.36 0.89 0.49 CM-3 0.73 0.48 0.41 0.39 0.78 0.47 Experiments Table 2. English-to-Chinese word alignment results 中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。 China&apos;s science and technology research has made achievements which have gained the attention of t"
P10-1085,J93-2003,0,0.0187108,"Missing"
P10-1085,P03-1012,0,0.0613872,"Missing"
P10-1085,J07-2003,0,0.12447,"Missing"
P10-1085,P09-1105,0,0.0386071,"Missing"
P10-1085,W04-3250,0,0.0422656,"the baseline phrase-based SMT system. We use SRI language modeling toolkit (Stolcke, 2002) to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST MT-2002 set as the development set and the NIST MT-2004 test set as the test set. And Koehn&apos;s implementation of minimum error rate training (Och, 2003) is used to tune the feature weights on the development set. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using paired bootstrap re-sample method (Koehn, 2004). 6.2 Effect of improved word alignment on phrase-based SMT We investigate the effectiveness of the improved word alignments on the phrase-based SMT system. The bi-directional alignments are obtained 830 T1: We must adopt effective measures in order to avoid problems . 我们 必须 wo-men bi-xu we must T2: 采取 有效 措施 才能 避免 出 问题 。 cai-qu you-xiao cuo-shi cai-neng bi-mian chu use effective measure can avoid out wen-ti . problem . We must adopt effective measures can we avoid out of the question . Figure 3. Example of the translations generated by the baseline system and the system where the phrase colloc"
P10-1085,2005.iwslt-1.8,0,0.0194514,"we use the collocation probability of the whole source sentence, r (F ) , as the collocation probability of one-word cept. 3.2 Improving bi-directional bilingual word alignments In word alignment models implemented in GIZA++, only one-to-one and many-to-one word alignment links can be found. Thus, some multiword units cannot be correctly aligned. The symmetrization method is used to effectively overcome this deficiency (Och and Ney, 2003). Bi-directional alignments are generally obtained from source-to-target alignments As 2t and targetto-source alignments At 2 s , using some heuristic rules (Koehn et al., 2005). This method ignores the correlation of the words in the same alignment unit, so an alignment may include many unrelated words 2 , which influences the performances of SMT systems. 1 2 827 http://www.fjoch.com/GIZA++.html In our experiments, a multi-word unit may include up to 40 words. In order to solve the above problem, we incorporate the collocation probabilities into the bidirectional word alignment process. Given alignment sets As 2t and At 2 s . We can obtain the union As t  As 2t  At 2s . The source sentence f f m 1 Corpora Bilingual corpus Additional monolingual corpora can be se"
P10-1085,N03-1017,0,0.122261,"Missing"
P10-1085,P09-4007,0,0.0287087,"Missing"
P10-1085,P05-1057,0,0.0448216,"Missing"
P10-1085,D09-1051,1,0.637339,"f the words in a phrase. Some This work was partially done at Toshiba (China) Research and Development Center. researches used soft syntactic constraints to predict whether source phrase can be translated together (Marton and Resnik, 2008; Xiong et al., 2009). However, the constraints were learned from the parsed corpus, which is not available for many languages. In this paper, we propose to use monolingual collocations to improve SMT. We first identify potentially collocated words and estimate collocation probabilities from monolingual corpora using a Monolingual Word Alignment (MWA) method (Liu et al., 2009), which does not need any additional resource or linguistic preprocessing, and which outperforms previous methods on the same experimental data. Then the collocation information is employed to improve Bilingual Word Alignment (BWA) for various kinds of SMT systems and to improve phrase table for phrase-based SMT. To improve BWA, we re-estimate the alignment probabilities by using the collocation probabilities of words in the same cept. A cept is the set of source words that are connected to the same target word (Brown et al., 1993). An alignment between a source multi-word cept and a target wo"
P10-1085,W02-1018,0,0.0462638,"Missing"
P10-1085,P08-1114,0,0.0333448,"Missing"
P10-1085,P00-1056,0,0.0899882,"subset from the bilingual corpus as test set, including 500 sentence pairs. Then word alignments in the subset were manually labeled, referring to the guideline of the Chinese-to-English alignment (LDC2006E93), but we made some modifications for the guideline. For example, if a preposition appears after a verb as a phrase aligned to one single word in the corresponding sentence, then they are glued together. There are several different evaluation metrics for word alignment (Ahrenberg et al., 2000). We use precision (P), recall (R) and alignment error ratio (AER), which are similar to those in Och and Ney (2000), except that we consider each alignment as a sure link. 828 Single word alignments Multi-word alignments P R AER P R AER Baseline 0.77 0.45 0.43 0.23 0.71 0.65 CM-1 0.70 0.50 0.42 0.35 0.86 0.50 Improved BWA methods CM-2 0.73 0.48 0.42 0.36 0.89 0.49 CM-3 0.73 0.48 0.41 0.39 0.78 0.47 Experiments Table 2. English-to-Chinese word alignment results 中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。 China&apos;s science and technology research has made achievements which have gained the attention of the people of the world . 中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。 zhong-guo de china DE ke-xue-ji-shu yan-jiu science and"
P10-1085,P03-1021,0,0.0152963,"nces of Moses using the different bi-directional word alignments (Significantly better than baseline with p &lt; 0.01) 6 6.1 Experiments on Phrase-Based SMT Experimental settings We use FBIS corpus to train the Chinese-toEnglish SMT systems. Moses (Koehn et al., 2007) is used as the baseline phrase-based SMT system. We use SRI language modeling toolkit (Stolcke, 2002) to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST MT-2002 set as the development set and the NIST MT-2004 test set as the test set. And Koehn&apos;s implementation of minimum error rate training (Och, 2003) is used to tune the feature weights on the development set. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using paired bootstrap re-sample method (Koehn, 2004). 6.2 Effect of improved word alignment on phrase-based SMT We investigate the effectiveness of the improved word alignments on the phrase-based SMT system. The bi-directional alignments are obtained 830 T1: We must adopt effective measures in order to avoid problems . 我们 必须 wo-men bi-xu we must T2: 采取 有效 措施 才能 避免 出 问题"
P10-1085,J03-1002,0,0.00555258,"To calculate the collocation probability of the alignment sequence, we should also consider the collocation probabilities of such one-to-one alignments. To solve this problem, we use the collocation probability of the whole source sentence, r (F ) , as the collocation probability of one-word cept. 3.2 Improving bi-directional bilingual word alignments In word alignment models implemented in GIZA++, only one-to-one and many-to-one word alignment links can be found. Thus, some multiword units cannot be correctly aligned. The symmetrization method is used to effectively overcome this deficiency (Och and Ney, 2003). Bi-directional alignments are generally obtained from source-to-target alignments As 2t and targetto-source alignments At 2 s , using some heuristic rules (Koehn et al., 2005). This method ignores the correlation of the words in the same alignment unit, so an alignment may include many unrelated words 2 , which influences the performances of SMT systems. 1 2 827 http://www.fjoch.com/GIZA++.html In our experiments, a multi-word unit may include up to 40 words. In order to solve the above problem, we incorporate the collocation probabilities into the bidirectional word alignment process. Given"
P10-1085,P02-1040,0,0.0794477,"er than baseline with p &lt; 0.01) 6 6.1 Experiments on Phrase-Based SMT Experimental settings We use FBIS corpus to train the Chinese-toEnglish SMT systems. Moses (Koehn et al., 2007) is used as the baseline phrase-based SMT system. We use SRI language modeling toolkit (Stolcke, 2002) to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST MT-2002 set as the development set and the NIST MT-2004 test set as the test set. And Koehn&apos;s implementation of minimum error rate training (Och, 2003) is used to tune the feature weights on the development set. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using paired bootstrap re-sample method (Koehn, 2004). 6.2 Effect of improved word alignment on phrase-based SMT We investigate the effectiveness of the improved word alignments on the phrase-based SMT system. The bi-directional alignments are obtained 830 T1: We must adopt effective measures in order to avoid problems . 我们 必须 wo-men bi-xu we must T2: 采取 有效 措施 才能 避免 出 问题 。 cai-qu you-xiao cuo-shi cai-neng bi-mian chu use effective measure can avoid out wen-ti . prob"
P10-1085,J97-3002,0,0.38215,"Missing"
P10-1085,P09-1036,0,0.0240816,"Missing"
P10-1085,P07-2045,0,\N,Missing
P11-1104,P06-1067,0,0.0460858,"Missing"
P11-1104,J93-2003,0,0.0367397,"Missing"
P11-1104,P05-1033,0,0.640525,"EU score over the baseline methods. 1 Introduction Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source"
P11-1104,N10-1127,0,0.0615614,"n IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA)"
P11-1104,W04-3250,0,0.0191744,"toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Translation results We compare the proposed method with various reordering methods in previous work. Monotone model: no reordering model is used. Distortion based reordering (DBR) model: a distortion based reordering method (AlOnaizan & Papineni, 2006). In this method, the distortion cost is defined in terms of words, rather than phrases. This method considers outbound, inbound, and pairwise distortions that Reorder models Monotone model DBR model MSDR model (Baseline) DBR model SCBR Model 1 SCBR Model 2 MSDR+ SCBR Model 3 SCBR models (1+2) SCBR models (1+2+3) MT04 MT08 26.99 18.30 26.64"
P11-1104,N03-1017,0,0.0420923,"rce token is covered when it is translated into a new target token. In 1997, another model called ITG constraint was presented, in which the reordering order can be hierarchically modeled as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our metho"
P11-1104,2005.iwslt-1.8,0,0.0286277,"or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straig"
P11-1104,D09-1051,1,0.918596,"ence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA) method without employing additional resources (Liu et al., 2009), and then the reordering model based on the detected collocations is learned from the word-aligned bilingual corpus. The source collocation based reordering model is integrated into SMT systems as an additional feature to softly constrain the translation orders of the source collocations in the sentence to be translated, so as to constrain the translation orders of those source phrases containing these collocated words. This method has two advantages: (1) it can automatically detect and leverage collocated words in a sentence, including long-distance collocated words; (2) such a reordering mo"
P11-1104,P08-1114,0,0.0864168,"r SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignm"
P11-1104,P03-1021,0,0.00926147,"ic function for estimating future score be determined according to the relative positions of the words and the corresponding reordering probability is employed. 4.2 Settings We use the FBIS corpus (LDC2003E14) to train a Chinese-to-English phrase-based translation model. And the SRI language modeling toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Translation results We compare the proposed method with various reordering methods in previous work. Monotone model: no reordering model is used. Distortion based reordering (DBR) model: a distortion based reordering method (AlOnaizan & Papineni, 2006). In this method, the distortion cost i"
P11-1104,J03-1002,0,0.00360503,"in (10). L Input: Input sentence F  f 1 Initialization: Score = 0 for each uncovered word f i do e4 e3 for each word f j ( j  ci or r ( f i , f j )   ) do e2 if f j is covered then e1 f1 f2 f3 if i > j then Score+= r ( f i , f j ) log p (o  straight |f i , f j ) f4 f5 else Score+= r ( f i , f j ) log p (o  inverted |f i , f j ) Figure 2. An example for reordering 4 4.1 else Score += arg max o r ( f i , f j ) log p (o |f i , f j ) Evaluation of Our Method Output: Score Implementation We implemented our method in a phrase-based SMT system (Koehn et al., 2007). Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. Thus, given a sentence to be translated, we first identify the collocations in the sentence, and then estimate the reordering score according to the translation hypothesis. For a translation option to be expanded, the reordering score inside this source phrase is calculated according to their translation orders of the collocations in the corresponding target phrase. The reordering score crossing the current translation option and the covered parts can be calculated according to the relative position of the collocated words. If the source p"
P11-1104,P02-1040,0,0.0825739,"d the corresponding reordering probability is employed. 4.2 Settings We use the FBIS corpus (LDC2003E14) to train a Chinese-to-English phrase-based translation model. And the SRI language modeling toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Translation results We compare the proposed method with various reordering methods in previous work. Monotone model: no reordering model is used. Distortion based reordering (DBR) model: a distortion based reordering method (AlOnaizan & Papineni, 2006). In this method, the distortion cost is defined in terms of words, rather than phrases. This method considers outbound, inbound, and pairwise distortions t"
P11-1104,N04-4026,0,0.0295724,"led as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimat"
P11-1104,P05-1069,0,0.0191896,"though the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering by con"
P11-1104,C10-1126,0,0.0841098,"ls (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA) method without employing add"
P11-1104,J97-3002,0,0.675665,"ng decoding, the model is employed to softly constrain the translation orders of the source language collocations, so as to constrain the translation orders of those source phrases containing these collocated words. The experimental results show that the proposed method significantly improves the translation quality, achieving the absolute improvements of 1.1~1.4 BLEU score over the baseline methods. 1 Introduction Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the"
P11-1104,P06-1066,0,0.120655,"ted in the same order as in the source language, or in the inverted order. We name the first case as straight, and the second inverted. Based on the observation that some collocations tend to have fixed translation orders such as “金融 jin-rong „financial‟ 危 机 wei-ji „crisis‟” (financial crisis) whose English translation order is usually straight, and “ 法 律 fa-lv „law‟ 范 围 fan-wei „scope‟” (scope of law) whose English translation order is generally inverted, some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al., 2006). We further notice that some words are translated in different orders when they are collocated with different words. For instance, when “潮流 chao-liu „trend‟” is collocated with “时代 shi-dai „times‟”, they are often translated into the “trend of times”; when collocated with “历史 li-shi „history‟”, the translation usually becomes the “historical trend”. Thus, if we can automatically detect the collocations in the sentence to be translated and their orders in the target language, the reordering information of the collocations could be used to constrain the reordering of phrases during decoding. Th"
P11-1104,P03-1019,0,0.0237065,"n quality. 6 Related Work Reordering was first proposed in the IBM models (Brown et al., 1993), later was named IBM constraint by Berger et al. (1996). This model treats the source word sequence as a coverage set that is processed sequentially and a source token is covered when it is translated into a new target token. In 1997, another model called ITG constraint was presented, in which the reordering order can be hierarchically modeled as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target"
P11-1104,W06-3108,0,0.0263897,"allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering by considering both consecu"
P11-1104,D07-1056,0,0.0844338,"uction Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using"
P11-1104,P07-2045,0,\N,Missing
P12-1048,J05-4003,0,0.0943735,"Missing"
P12-1048,W09-0432,0,0.0300305,"t Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of"
P12-1048,D07-1007,0,0.0296716,") also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of addit"
P12-1048,P07-1005,0,0.406273,"uccessfully in NLP community. Based on the “bag-of-words” assumption that the order of words can be ignored, these methods model the text corpus by using a co-occurrence matrix of words and documents, and build generative models to infer the latent aspects or topics. Using these models, the words can be clustered into the derived topics with a probability distribution, and the correlation between words can be automatically captured via topics. However, the “bag-of-words” assumption is an unrealistic oversimplification because it ignores the order of words. To remedy this problem, Gruber et al.(2007) propose HTMM, which models the topics of words in the document as a Markov chain. Based on the assumption that all words in the same sentence have the same topic and the successive sentences are more likely to have the same topic, HTMM incorporates the local dependency between words by Hidden Markov Model for better topic estimation. HTMM can also be viewed as a soft clustering tool for words in training corpus. That is, HTMM can estimate the probability distribution of a topic over words, i.e. the topic-word distribution P (word|topic) during training. Besides, HTMM derives inherent topics i"
P12-1048,P10-1086,0,0.0111327,"in SMT. Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity. Tam et al.(2007) proposed a bilingual LSA, which enforces one-to-one topic correspondence and enables latent topic distributions to be efficiently transferred across languages, to crosslingual language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; C"
P12-1048,P10-1146,0,0.0129201,"ship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific tra"
P12-1048,W07-0722,0,0.159911,"dapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpo"
P12-1048,eck-etal-2004-language,0,0.0665867,"Missing"
P12-1048,2005.mtsummit-papers.30,0,0.060786,"Missing"
P12-1048,W07-0717,0,0.227606,"re we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Lingui"
P12-1048,D10-1044,0,0.0292696,"in SMT. Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity. Tam et al.(2007) proposed a bilingual LSA, which enforces one-to-one topic correspondence and enables latent topic distributions to be efficiently transferred across languages, to crosslingual language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; C"
P12-1048,P06-1121,0,0.0055969,"ility estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-d"
P12-1048,D08-1039,0,0.0258649,"Missing"
P12-1048,C08-1041,1,0.583545,"odeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain m"
P12-1048,2005.eamt-1.19,0,0.0557771,"Missing"
P12-1048,J03-1002,0,0.00333165,"conduct topic model training. During this process, we empirically set the same parameter values for the HTMM training of different corpora: topics = 50, α = 1.5, β = 1.01, iters = 100. See (Gruber et al., 2007) for the meanings of these parameters. Besides, we set the interpolation weight θ in formula (10) to 0.5 by observing the results on development set in the additional experiments. We choose MOSES, a famous open-source 1 2 http://blog.sohu.com/ http://u.cs.biu.ac.il/ koppel/BlogCorpus.html phrase-based machine translation system (Koehn et al., 2007), as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al.,"
P12-1048,J04-4002,0,0.647358,"translation probability estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the"
P12-1048,N03-1017,0,0.0215688,"Missing"
P12-1048,W04-3250,0,0.0780006,"generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Result and Analysis 4.2.1 Effect of Different Smoothing Methods Our first experiments investigate the effect of different smoothing methods for the in-domain phrasetopic distribution: “Noisy-OR” and “Averaging”. We build adapted phrase tables with these two methods, and then respectively use them in place of the out-of-domain phrase table to test the system performance. For the purpose of studying the generality of our approach, we carry out comparative experiments on two sizes of in-domain monolingual corpora: 5K and 40K. Adaptation Met"
P12-1048,P07-2045,0,0.00336531,"vely, we use HTMM tool developed by Gruber et al.(2007) to conduct topic model training. During this process, we empirically set the same parameter values for the HTMM training of different corpora: topics = 50, α = 1.5, β = 1.01, iters = 100. See (Gruber et al., 2007) for the meanings of these parameters. Besides, we set the interpolation weight θ in formula (10) to 0.5 by observing the results on development set in the additional experiments. We choose MOSES, a famous open-source 1 2 http://blog.sohu.com/ http://u.cs.biu.ac.il/ koppel/BlogCorpus.html phrase-based machine translation system (Koehn et al., 2007), as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evalu"
P12-1048,P06-1077,1,0.129484,"method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual co"
P12-1048,D07-1036,1,0.789966,"l, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach i"
P12-1048,D09-1022,0,0.0366086,"l language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to"
P12-1048,D09-1074,0,0.027418,"Missing"
P12-1048,W11-2133,0,0.0654102,"gnoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ ang”, and occurs in the sentences related to the geography topic when translated to “h´ ea `n”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases. In a monolingual corpus, if “bank” occurs more often in the sentences rela"
P12-1048,P02-1040,0,0.105802,"ch and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Result and Analysis 4.2.1 Effect of Different Smoothing Methods Our first experiments investigate the effect of different smoothing methods for the in-domain phrasetopic distribution: “Noisy-OR” and “Averaging”. We build adapted phrase tables with these two methods, and then respectively use them in place of the out-of-domain phrase table to test the system performance. For the purpose of studying the generality of our approach, we carry out comparative experiments on two sizes"
P12-1048,2009.mtsummit-posters.17,0,0.0128304,"ion emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computati"
P12-1048,2007.mtsummit-tutorials.1,0,0.0760626,"rpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ ang”, and occurs in the sentences related to the geography topic when translated to “h´ ea `n”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases. In a monolingual corpus, if"
P12-1048,C08-1125,1,0.73343,"or’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings"
P12-1048,N04-1033,0,0.157585,"which is decomposed into the topic posterior distribution at the word level, and θ is the interpolation weight that can be optimized over the development data. Given the number of the phrase f˜ in sentence f denoted as countf (f˜), we compute the in-domain phrase-topic distribution in the following way: Pmle (tf P = f ∈Cf P tf in ˜ in |f ) countf (f˜) · P (tf in |f ) in countf (f˜) · P (tf P f ∈Cf in in |f ) (11) Under the assumption that the topics of all words in the same phrase are independent, we consider two methods to calculate Pword (tf in |f˜). One is a “Noisy-OR” combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. Using this method, Pword (tf in |f˜) is defined as: Pword (tf in |f˜) = 1 − Pword (t¯f in |f˜) Y ≈ 1− P (t¯f in |fj ) fj ∈f˜ = 1− Y (1 − P (tf in |fj )) (12) fj ∈f˜ where Pword (t¯f in |f˜) represents the probability that tf in is not the topic of the phrase f˜. Similarly, P (t¯f in |fj ) indicates the probability that tf in is not the topic of the word fj . The other method is an “Averaging” combination one. With the assumption that tf in is the topic of f˜ if at least one of the words"
P12-1048,W06-1626,0,0.0730106,"Missing"
P12-1048,C04-1059,0,0.0592364,"Missing"
P12-1048,P06-2124,0,0.82898,"hes focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ ang”, and occurs in the sentences related to the geography topic when translated to “h´ ea `n”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates"
P12-1048,D08-1010,1,0.848211,"in adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain monolingual corpora,"
P12-1048,J07-2003,0,\N,Missing
P12-1048,2006.iwslt-evaluation.15,0,\N,Missing
P12-1103,2008.iwslt-papers.2,0,0.110087,"ranslation models, either from the input side, which targets on rewriting the input sentences to the MT-favored expressions, or from the side of translation models, which tries to enrich the translation models to cover more expressions. In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use"
P12-1103,N06-1003,0,0.0203314,"SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods, only word or phrasal paraphrases are used for input sentence rewriting. No structured paraphrases on the sentence level have been investigated. However, the information in the sentence level is very important for d"
P12-1103,P05-1033,0,0.063803,"eel interest that N/A blue handbag 欢迎 乘坐 I to that N/A blue handbag have interest 我 对 那 只 蓝色 手提包 有 兴趣 。 我 很 感 兴趣 那 个 蓝色 手提包 I very feel interest that N/A blue handbag 乘坐 ride 。 Figure 2: Example for Word Alignment Filtration 2. Stop words (including some function words and punctuations) can only be aligned to either stop words or null. Figure 2 illustrates an example of using the heuristics to filter alignment. 3.4 Extracting Paraphrase Rules From the word-aligned sentence pairs, we then extract a set of rules that are consistent with the word alignments. We use the rule extracting methods of Chiang (2005). Take the sentence pair in Figure 2 as an example, two initial phrase pairs 感 兴趣 那 个 蓝色 手提包” are identified, and PP1 is contained by PP2, then we could form the rule: 对 X1 有 兴趣  很 感 兴趣 X1 to have interest very feel interest 4 Paraphrasing the Input Sentences The extracted paraphrase rules aim to rewrite the input sentences to an MT-favored form which may lead to a better translation. However, it is risky to directly replace the input sentence with a paraphrased sentence, since the errors in automatic paraphrase substitution may jeopardize the translation result seriously. To avoid such damag"
P12-1103,D10-1041,0,0.0787359,"), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods, only word or phrasal paraphrases are used for input sentence rewriting. No structured paraphrases on the sentence level have been investigated. However, the information in the sentence level is very important for disambiguation. For example, we can only substitute play with drama in a context related to stage or theatre. Phrasal paraphrase substitutions can hardly solve such kind of problems. In this paper, we propose a method that rewrites This work was done when the first author was visit"
P12-1103,I11-1090,1,0.511378,"entences and the translation models, either from the input side, which targets on rewriting the input sentences to the MT-favored expressions, or from the side of translation models, which tries to enrich the translation models to cover more expressions. In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and D"
P12-1103,N03-1017,0,0.0378794,"Missing"
P12-1103,C10-1069,0,0.0127161,"e MT-favored expressions, or from the side of translation models, which tries to enrich the translation models to cover more expressions. In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods,"
P12-1103,D09-1040,0,0.0211672,"can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods, only word or phrasal paraphrases are used for input sentence rewriting. No structured paraphrases on the sentence level have been investigated. However, the information in the sentence level is very important for disambiguation. For exampl"
P12-1103,P00-1056,0,0.0382139,"e is: T1 = SYS_ST(S0), S1 = SYS_TS(T0), T2 = SYS_ST(S1). Finally we compute BLEU (Papineni et al. 2002) score for every sentence in T2 and T1, using the corresponding sentence in T0 as reference. If the sentence in T2 has a higher BLEU score than the aligned sentence in T1, the corresponding sentences in S0 and S1 are selected as candidate paraphrase sentence pairs, which are used in the following steps of paraphrase extractions. 3.3 Word Alignments Filtering We can construct word alignment between S0 and S1 through T0. On the initial corpus of (S0, T0), we conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag-final heuristic (Koehn et al., 2005) for symmetrization. Because S1 is generated by feeding T0 into the PBMT system SYS_TS, the word alignment between T0 and S1 can be acquired from the verbose information of the decoder. The word alignments of S0 and S1 contain noises which are produced by either wrong alignment of GIZA++ or translation errors of SYS_TS. To ensure the alignment quality, we use some heuristics to filter the alignment between S0 and S1: 1. If two identical words are aligned in S0 and S1, then remove all the other links to the two"
P12-1103,P03-1021,0,0.0399546,". Taking α as an example, firstly, p – 1 nodes are created, and then p edges labeled with αj (1 ≤ j ≤ p) are generated to connect node θx-1, p-1 nodes and θy-1. Via step 2, word lattices are generated by adding new nodes and edges coming from paraphrases. 983 Experiments Experimental Data In our experiments, we used Moses (Koehn et al., 2007) as the baseline system which can support lattice decoding. The alignment was obtained using GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the growdiag-final heuristic. Parameters were tuned using Minimum Error Rate Training (Och, 2003). To comprehensively evaluate the proposed methods in different domains, two groups of experiments were carried out, namely, the oral group (Goral) and the news group (Gnews). The experiments were conducted in both Chinese-English and EnglishChinese directions for the oral group, and ChineseEnglish direction for the news group. The English sentences were all tokenized and lowercased, and the Chinese sentences were segmented into words by Language Technology Platform (LTP) 1 . We used SRILM2 for the training of language models (5-gram in all the experiments). The metrics for automatic evaluatio"
P12-1103,P02-1040,0,0.0861493,"n be paraphrased to RHS. Taking Chinese as a case 981 study, some examples of paraphrase rules are shown in Table 1. 3.2 Selecting Paraphrase Sentence Pairs Following the methods in Section 2, the initial bilingual corpus is (S0, T0). We train a source-totarget PBMT system (SYS_ST) and a target-tosource PBMT system (SYS_TS) on the parallel corpus. Then a Forward-Translation is performed on S0 using SYS_ST, and a Back-Translation is performed on T0 using SYS_TS and SYS_ST. As mentioned above, the detailed procedure is: T1 = SYS_ST(S0), S1 = SYS_TS(T0), T2 = SYS_ST(S1). Finally we compute BLEU (Papineni et al. 2002) score for every sentence in T2 and T1, using the corresponding sentence in T0 as reference. If the sentence in T2 has a higher BLEU score than the aligned sentence in T1, the corresponding sentences in S0 and S1 are selected as candidate paraphrase sentence pairs, which are used in the following steps of paraphrase extractions. 3.3 Word Alignments Filtering We can construct word alignment between S0 and S1 through T0. On the initial corpus of (S0, T0), we conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag-final heuristic (Koehn et al., 2005)"
P12-1103,P09-2034,0,0.0583384,"Missing"
P12-1103,2010.eamt-1.34,0,0.0705043,"Missing"
P12-1103,D10-1064,0,\N,Missing
P12-1103,P07-2045,0,\N,Missing
P12-1103,P09-1089,0,\N,Missing
P12-1103,P10-2001,0,\N,Missing
P12-1103,W04-3250,0,\N,Missing
P12-1103,2005.iwslt-1.8,0,\N,Missing
P14-1113,D07-1017,0,0.0489721,"of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chu"
P14-1113,C10-3004,1,0.465681,"uned on a development dataset. 3.3.3 Training Data To learn the projection matrices, we extract training data from a Chinese semantic thesaurus, Tongyi Cilin (Extended) (CilinE for short) which 1202 δ Root 物 object B 昆虫 : 动物 (insect : animal) 蜻蜓 : 动物 (dragonfly : animal) 动物 animal … Sense Code: Bi 昆虫 insect 18 -- … 蜻蜓 : 昆虫 Sense Code: Bi18A 蜻蜓 dragonfly Sense Code: Bi18A06@ A … … Level 3 x Figure 4: In this example, Φk x is located in the circle with center y and radius δ. So y is considered as a hypernym of x. Conversely, y is not a hypernym of x0 . Level 5 06@ CilinE contains 100,093 words (Che et al., 2010).3 CilinE is organized as a hierarchy of five levels, in which the words are linked by hypernym–hyponym relations (right panel, Figure 3). Each word in CilinE has one or more sense codes (some words are polysemous) that indicate its position in the hierarchy. The senses of words in the first level, such as “物 (object)” and “时间 (time),” are very general. The fourth level only has sense codes without real words. Therefore, we extract words in the second, third and fifth levels to constitute hypernym– hyponym pairs (left panel, Figure 3). Note that mapping one hyponym to multiple hypernyms with t"
P14-1113,W09-0215,0,0.0162212,"ance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (T"
P14-1113,D13-1122,1,0.267748,"versely, “dog” is a hyponym of “canine.” As key sources of knowledge, semantic thesauri and ontologies can support many natural language processing applications. However, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming. Therefore, many researchers Email correspondence. 毛茛科 Ranunculaceae 乌头属 Aconitum Introduction ∗ 生物 organism 植物 plant have attempted to automatically extract semantic relations or to construct taxonomies. A major challenge for this task is the automatic discovery of hypernym-hyponym relations. Fu et al. (2013) propose a distant supervision method to extract hypernyms for entities from multiple sources. The output of their model is a list of hypernyms for a given enity (left panel, Figure 1). However, there usually also exists hypernym–hyponym relations among these hypernyms. For instance, “植 物 (plant)” and “毛 茛 科 (Ranunculaceae)” are both hypernyms of the entity “乌 头 (aconit),” and “植 物 (plant)” is also a hypernym of “毛 茛 科 (Ranunculaceae).” Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel, Figure 1).1 Some pre"
P14-1113,P05-1014,0,0.450387,"hod to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Soche"
P14-1113,C92-2082,0,0.775886,"物 (plant)” is also a hypernym of “毛 茛 科 (Ranunculaceae).” Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel, Figure 1).1 Some previous works extend and refine manually-built semantic hierarchies by using other resources (e.g., Wikipedia) (Suchanek et al., 2008). However, the coverage is limited by the scope of the resources. Several other works relied heavily on lexical patterns, which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic circumstances (Hearst, 1992; Snow et al., 2005). 1 In this study, we focus on Chinese semantic hierarchy construction. The proposed method can be easily adapted to other languages. 1199 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1199–1209, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Besides, distributional similarity methods (Kotlerman et al., 2010; Lenci and Benotto, 2012) are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts whe"
P14-1113,S12-1012,0,0.493654,"icult to guarantee. Generally speaking, these pattern-based methods often suffer from low recall or precision because of the coverage or the quality of the patterns. The distributional methods assume that the contexts of hypernyms are broader than the ones of their hyponyms. For distributional similarity computing, each word is represented as a semantic vector composed of the pointwise mutual information (PMI) with its contexts. Kotlerman et al. (2010) design a directional distributional measure to infer hypernym–hyponym relations based on the standard IR Average Precision evaluation measure. Lenci and Benotto (2012) propose another measure focusing on the contexts that hypernyms do not share with their hyponyms. However, broader semantics may not always infer broader contexts. For example, for terms “Obama’ and 1200 “American people”, it is hard to say whose contexts are broader. Our previous work (Fu et al., 2013) applies a web mining method to discover the hypernyms of Chinese entities from multiple sources. We assume that the hypernyms of an entity co-occur with it frequently. It works well for named entities. But for class names (e.g., singers in Hong Kong, tropical fruits) with wider range of meanin"
P14-1113,I08-2112,0,0.0347144,". One possible solution may be adding more data of this kind to the training set. 6 Related Work In addition to the works mentioned in Section 2, we introduce another set of related studies in this section. Evans (2004), Ortega-Mendoza et al. (2007), and Sang (2007) consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst (1992). However, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction. Following the method for discovering patterns automatically (Snow et al., 2005), McNamee et al. (2008) apply the same method to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (W"
P14-1113,N13-1090,0,0.480991,"ks well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics. Moreover, all of these methods do not use the word semantics effectively. This paper proposes a novel approach for semantic hierarchy construction based on word embeddings. Word embeddings, also known as distributed word representations, typically represent words with dense, low-dimensional and realvalued vectors. Word embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words (Mikolov et al., 2013b). For example, v(king) − v(queen) ≈ v(man) − v(woman), where v(w) is the embedding of the word w. We observe that a similar property also applies to the hypernym–hyponym relationship (Section 3.3), which is the main inspiration of the present study. However, we further observe that hypernym– hyponym relations are more complicated than a single offset can represent. To address this challenge, we propose a more sophisticated and general method — learning a linear projection which maps words to their hypernyms (Section 3.3.1). Furthermore, we propose a piecewise linear projection method based o"
P14-1113,P07-2042,0,0.242856,"Missing"
P14-1113,P06-1101,0,0.241765,"tes that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the"
P14-1113,D11-1014,0,0.0421,"ethods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors. Their work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications. In this paper, we im"
P14-1113,P07-1058,0,0.026901,"to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity"
P14-1113,P10-1040,0,0.084559,"). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors. Their work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications. In this paper, we improve on their work by learning multiple linear projections in the embedding space, to model hypernym–hypony"
P14-1113,C04-1146,0,0.0870413,"Missing"
P15-1119,J92-4003,0,0.140167,"n Section 4 to induce the cross-lingual word clusters. We re-implement the PROJECTED cluster approach in T¨ackstr¨om et al. (2012), which assigns a target word to the cluster with which it is most often aligned: c(wiT ) = arg max k ∑ (i,j)∈AT ∣S ci,j ⋅ 1[c(wjS ) = k] This method also has the drawback that words that do not occur in the alignment dictionary (OOV) cannot be assigned a cluster. Therefore, we use the same strategy as described in Section 4.2 to find the most likely clusters for the OOV words. Instead of the clustering model of Uszkoreit and Brants (2008), we use Brown clustering (Brown et al., 1992) to induce hierarchical word clusters, where each word is represented as a bit-string. We use the same word cluster feature templates from T¨ackstr¨om et al. (2012), and set the number of Brown clusters to 256. 5.3 Experimental Results All of the parsing models are trained using the development data from English for early-stopping. Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4 further summarizes each of the experimental gains detailed in Table 3. Our delexicalized system obtains slightly lower performance than those reported in McDonald et"
P15-1119,W06-2920,0,0.0171356,"e templates from T¨ackstr¨om et al. (2012), and set the number of Brown clusters to 256. 5.3 Experimental Results All of the parsing models are trained using the development data from English for early-stopping. Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4 further summarizes each of the experimental gains detailed in Table 3. Our delexicalized system obtains slightly lower performance than those reported in McDonald et al. (2013) (McD13), because we’re using Before this dataset was carried out, the CoNLL multilingual dependency treebanks (Buchholz and Marsi, 2006) were often used for evaluation. However, the major problem is that the dependency annotations vary for different languages (e.g. the choice of lexical versus functional head), which makes it impossible to evaluate the LAS. 1239 D ELEX P ROJ P ROJ+Cluster CCA CCA+Cluster Unlabeled Attachment Score (UAS) EN DE ES FR AVG 83.67 57.01 68.05 68.85 64.64 91.96 60.07 71.42 71.36 67.62 92.33 60.35 71.90 72.93 68.39 90.62† 59.42 68.87 69.58 65.96 92.03† 60.66 71.33 70.87 67.62 EN 79.42 90.48 90.91 88.88† 90.49† M C D13 83.33 58.50 68.07 70.14 65.57 78.54 48.11 56.86 58.20 54.39 84.44 90.21 57.30 60.55"
P15-1119,P14-1063,0,0.0232538,"ture templates used in our system are shown in Table 1. Then, feature compositions are performed at the hidden layer via a cube activation function: g(x) = x3 . The cube activation function can be viewed as a special case of low-rank tensor. Formally, g(x) can be expanded as: g(w1 x1 + ... + wm xm + b) = ∑ (wi wj wk )xi xj xk + ∑ b(wi wj )xi xj + ... i,j,k i,j If we treat the bias term as b × x0 where x0 = 1, then the weight corresponding to each feature combination xi xj xk is wi wj wk , which is exactly the same as a rank-1 component tensor in the lowrank form using CP tensor decomposition (Cao and Khudanpur, 2014). Consequently, the cube activation function implicitly derives full feature combinations. An advantage of the cube activation function is that it is flexible for adding extra features to the input. In fact, we can add as many features as possible to the input layer to improve the parsing accuracy. We will show in Section 5.2 that the Brown cluster features can be readily incorporated into our model. Cross-lingual Transfer. The idea of crosslingual transfer using the parser we examined above is straightforward. In contrast to traditional approaches that have to discard rich lexical features (d"
P15-1119,N13-1006,1,0.167482,"(Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao"
P15-1119,D14-1082,0,0.779303,"atures used in traditional dependency parsers, distributed representations map symbolic features into a continuous representation space, that can be shared across languages. Therefore, our model has the ability to utilize both lexical and non-lexical features naturally. Specifically, our framework contains two primary components: • A neural network-based dependency parser. We expect a non-linear model for dependency parsing in our study, because distributed feature representations are shown to be more effective in non-linear architectures than in linear architectures (Wang and Manning, 2013). Chen and Manning (2014) propose a transition-based dependency parser using a neural network architecture, which is simple but works well on several datasets. Briefly, this model simply replaces the predictor in transition-based dependency parser with a well-designed neural network classifier. We will provide explanations for the merits of this model in Section 3, as well as how we adapt it to the cross-lingual task. • Cross-lingual word representation learning. The key to filling the lexical feature gap is to project the representations of these features from different languages into a common vector space, preservin"
P15-1119,P11-1061,0,0.0110737,"ion method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49"
P15-1119,W08-1301,0,0.0204693,"Missing"
P15-1119,de-marneffe-etal-2006-generating,0,0.034227,"Missing"
P15-1119,D12-1001,0,0.0275469,"uding NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-"
P15-1119,P10-4002,0,0.0314634,"r-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the number of hidden units to 400. The dimension of embeddings for different features are shown in Table 2. Dim. Word 50 POS 50 Label 50 Dist. 5 Val. 5 Cluster 8 Table 2: Dimensions of feature embeddings. Adaptive stochastic gradient descent (AdaGrad) (Duchi et al., 2011) is used for optimization. For the CCA approach, we use the implementation of Faruqui and Dyer (2014). The dimensions of the monolin"
P15-1119,E14-1049,0,0.441226,"which is typically smaller than the monolingual datasets. Therefore, in order to improve the robustness of projection, we utilize a morphology-inspired mechanism, to propagate embeddings from in-vocabulary words to out-ofvocabulary (OOV) words. Specifically, for each T , we extract a list of candidate OOV word woov words that is similar to it in terms of edit distance, and then set the averaged vector as the embedding T of woov . Formally, T v(woov ) = Avg (v(w′ )) w′ ∈C T where C = {w∣EditDist(woov , w) ≤ τ } 4.3 ?1 Canonical Correlation Analysis The second approach we consider is similar to Faruqui and Dyer (2014), which use CCA to improve monolingual word embeddings with multilingual correlation. CCA is a way of measur?2 Σ ?1 Ω′ Ω ? ? ? ? ?2 ?2 Ω∗ CCA ?1 Σ∗ ? ? Figure 3: CCA for cross-lingual word representation learning. ing the linear relationship between multidimensional variables. For two multidimensional variables, CCA aims to find two projection matrices to map the original variables to a new basis (lowerdimensional), such that the correlation between the two variables is maximized. Let’s treat CCA as a blackbox here, and see how to apply CCA for inducing bilingual word embeddings. Suppose there"
P15-1119,P09-1042,0,0.0255939,"Missing"
P15-1119,P10-1156,0,0.0165341,"46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, whi"
P15-1119,D14-1012,1,0.642138,"major ways of applying distributed representations to NLP tasks. First, they can be fed into existing supervised NLP systems as augmented features in a semi-supervised manner. This kind of approach has been adopted in a variety of applications (Turian et al., 2010). Despite its simplicity and effectiveness, it has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the existing NLP systems (Wang and Manning, 2013). One remedy is to discretize the distributed feature representations, as studied in Guo et al. (2014). However, we believe that a non-linear system, e.g. a neural network, is a more powerful and effective solution. Some decent progress has already been made in this paradigm of NLP on various tasks (Collobert et al., 2011; Chen and Manning, 2014; Sutskever et al., 2014). 3 Transition actions In this paper, these two terms are used interchangeably. Stack Buffer ROOT has_VBZ good_JJ control_NN ._. nsubj He_PRP Configuration Figure 2: Neural network model for dependency parsing. The Cluster features are introduced in Section 5.2. which typically consists of a stack S, a buffer B, and a partially"
P15-1119,P08-1088,0,0.0084253,"projection approach, CCA assigns embeddings for every word in the monolingual vocabulary. However, one potential limitation is that CCA assumes linear transformation of word embeddings, which is difficult to satisfy. 4 T ∣S A is also worth trying, but we observed slight performance degradation in our experimental setting. 1238 Note that both approaches can be generalize to lower-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the number of hidden units"
P15-1119,P14-1006,0,0.0265905,"Missing"
P15-1119,D11-1110,0,0.0181767,"cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the b"
P15-1119,C10-1063,0,0.0172979,".). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingua"
P15-1119,P12-2010,0,0.021569,"oposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES U"
P15-1119,P12-1073,0,0.017508,"tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison"
P15-1119,P04-1061,0,0.0694958,"ng central problems. The majority of work on dependency parsing has been dedicated to resource-rich languages, such as English and Chinese. For these languages, there exist large-scale ∗ This work was done while the author was visiting JHU. annotated treebanks that can be used for supervised training of dependency parsers. However, for most of the languages in the world, there are few or even no labeled training data for parsing, and it is labor intensive and time-consuming to manually build treebanks for all languages. This fact has given rise to a number of research on unsupervised methods (Klein and Manning, 2004), annotation projection methods (Hwa et al., 2005), and model transfer methods (McDonald et al., 2011) for predicting linguistic structures. In this study, we focus on the model transfer methods, which attempt to build parsers for low-resource languages by exploiting treebanks from resourcerich languages. The major obstacle in transferring a parsing system from one language to another is the lexical features, e.g. words, which are not directly transferable across languages. To solve this problem, McDonald et al. (2011) build a delexicalized parser - a parser that only has non-lexical features."
P15-1119,C12-1089,0,0.351816,"Missing"
P15-1119,W02-0902,0,0.0119728,"dings for our cross-lingual task. Contrary to the projection approach, CCA assigns embeddings for every word in the monolingual vocabulary. However, one potential limitation is that CCA assumes linear transformation of word embeddings, which is difficult to satisfy. 4 T ∣S A is also worth trying, but we observed slight performance degradation in our experimental setting. 1238 Note that both approaches can be generalize to lower-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network depe"
P15-1119,P13-1105,0,0.0787465,"Missing"
P15-1119,W14-1614,0,0.05557,"-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. That said, these methods have the advantage that they are capable of capturing some language-specific syntactic patterns which our approach cannot.15 These two kinds of approaches 15 For example, in Spanish and French, adjectives often appears after nouns, thus forming a right-directed arc labeled by amod, whereas in English, the amod"
P15-1119,P14-1126,0,0.19646,"pplied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. That said, these methods have the advantage that they are capable of capturing some language-specific syntactic patterns which our approach cannot.15 These two kinds of approaches 15 For example, in Spanish and French, adjectives often appears after nouns, thus forming a right-directed arc labeled by amod, whereas in English, the amod arcs are mostly leftdirected. ‡ FR LAS 58.78 46.66 58.08"
P15-1119,C14-1175,0,0.133083,"dency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel"
P15-1119,N01-1020,1,0.605674,"ual task. Contrary to the projection approach, CCA assigns embeddings for every word in the monolingual vocabulary. However, one potential limitation is that CCA assumes linear transformation of word embeddings, which is difficult to satisfy. 4 T ∣S A is also worth trying, but we observed slight performance degradation in our experimental setting. 1238 Note that both approaches can be generalize to lower-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the"
P15-1119,D07-1013,0,0.00913989,"d cluster features can be effectively embedded into our model, leading to significant additive improvements. 2 2.1 Background Dependency Parsing Given an input sentence x = w0 w1 ...wn , the goal of dependency parsing is to build a dependency tree (Figure 1), which can be denoted by d = {(h, m, l) ∶ 0 ≤ h ≤ n; 0 &lt; m ≤ n, l ∈ L}. (h, m, l) indicates a directed arc from the head word wh to the modifier wm with a dependency label l, and L is the label set. The mainstream models that have been proposed for dependency parsing can be described as either graph-based models or transitionbased models (McDonald and Nivre, 2007). Graph-based models view the parsing problem as finding the highest scoring tree from a directed graph. The score of a dependency tree is typically factored into scores of some small structures (e.g. arcs) depending on the order of a model. Transition-based models aim to predict a transition sequence from an initial parser state to some terminal states, depending on the parsing history. This approach has a lot of interest since it is fast (linear time) and can incorporate rich non-local features (Zhang and Nivre, 2011). It has been considered that simple transitionbased parsing using greedy d"
P15-1119,D11-1006,0,0.219143,"uages, such as English and Chinese. For these languages, there exist large-scale ∗ This work was done while the author was visiting JHU. annotated treebanks that can be used for supervised training of dependency parsers. However, for most of the languages in the world, there are few or even no labeled training data for parsing, and it is labor intensive and time-consuming to manually build treebanks for all languages. This fact has given rise to a number of research on unsupervised methods (Klein and Manning, 2004), annotation projection methods (Hwa et al., 2005), and model transfer methods (McDonald et al., 2011) for predicting linguistic structures. In this study, we focus on the model transfer methods, which attempt to build parsers for low-resource languages by exploiting treebanks from resourcerich languages. The major obstacle in transferring a parsing system from one language to another is the lexical features, e.g. words, which are not directly transferable across languages. To solve this problem, McDonald et al. (2011) build a delexicalized parser - a parser that only has non-lexical features. A delexicalized parser makes sense in that POS tag features are significantly predictive for unlabele"
P15-1119,D10-1120,0,0.021069,"Missing"
P15-1119,P12-1066,0,0.0410584,"Missing"
P15-1119,W04-0308,0,0.0244311,"ROOT has_VBZ good_JJ control_NN ._. nsubj He_PRP Configuration Figure 2: Neural network model for dependency parsing. The Cluster features are introduced in Section 5.2. which typically consists of a stack S, a buffer B, and a partially derived forest, i.e. a set of dependency arcs A. Given an input word sequence x = w1 w2 , ..., wn , the initial configuration can be represented as a tuple: ⟨[w0 ]S , [w1 w2 , ..., wn ]B , ∅⟩, and the terminal configuration is ⟨[w0 ]S , []B , A⟩, where w0 is a pseudo word indicating the root of the whole dependency tree. We consider the arc-standard algorithm (Nivre, 2004) in this paper, which defines three types of transition actions: L EFT-A RC(l), R IGHT-A RC(l), and S HIFT, l is the dependency label. The typical approach for greedy arc-standard parsing is to build a multi-class classifier (e.g., SVM, MaxEnt) of predicting the transition action given a feature vector extracted from a specific configuration. While conventional feature engineering suffers from the problem of sparsity, incompleteness and expensive feature computation (Chen and Manning, 2014), the neural network model provides a potential solution. The architecture of the neural network-based de"
P15-1119,petrov-etal-2012-universal,0,0.0336392,"Missing"
P15-1119,D09-1086,0,0.00730072,". 6 Related Studies Existing approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized"
P15-1119,W15-1824,0,0.0443701,"be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Kle"
P15-1119,P12-1068,0,0.00760257,"UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser"
P15-1119,P10-1040,0,0.0337388,"in the NLP research community of learning distributed representations for different natural language units, from morphemes, words and phrases, to sentences and documents. Using distributed representations, these symbolic units are embedded into a lowdimensional and continuous space, thus it is often referred to as embeddings.1 In general, there are two major ways of applying distributed representations to NLP tasks. First, they can be fed into existing supervised NLP systems as augmented features in a semi-supervised manner. This kind of approach has been adopted in a variety of applications (Turian et al., 2010). Despite its simplicity and effectiveness, it has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the existing NLP systems (Wang and Manning, 2013). One remedy is to discretize the distributed feature representations, as studied in Guo et al. (2014). However, we believe that a non-linear system, e.g. a neural network, is a more powerful and effective solution. Some decent progress has already been made in this paradigm of NLP on various tasks (Collobert et al., 2011; Chen and Manning, 2014; Su"
P15-1119,P08-1086,0,0.0144334,"). We use the same alignment dictionary as described in Section 4 to induce the cross-lingual word clusters. We re-implement the PROJECTED cluster approach in T¨ackstr¨om et al. (2012), which assigns a target word to the cluster with which it is most often aligned: c(wiT ) = arg max k ∑ (i,j)∈AT ∣S ci,j ⋅ 1[c(wjS ) = k] This method also has the drawback that words that do not occur in the alignment dictionary (OOV) cannot be assigned a cluster. Therefore, we use the same strategy as described in Section 4.2 to find the most likely clusters for the OOV words. Instead of the clustering model of Uszkoreit and Brants (2008), we use Brown clustering (Brown et al., 1992) to induce hierarchical word clusters, where each word is represented as a bit-string. We use the same word cluster feature templates from T¨ackstr¨om et al. (2012), and set the number of Brown clusters to 256. 5.3 Experimental Results All of the parsing models are trained using the development data from English for early-stopping. Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4 further summarizes each of the experimental gains detailed in Table 3. Our delexicalized system obtains slightly lower p"
P15-1119,I13-1183,0,0.0132607,"Missing"
P15-1119,Q14-1005,0,0.032741,"om, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cr"
P15-1119,W14-1613,0,0.186749,"2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. T"
P15-1119,H01-1035,1,0.652706,"ks. It is worth noting that we don’t assume/require bilingual parallel data in CCA and P ROJ. What we need in practice is a bilingual lexicon for each paired languages. This is especially important for generalizing our approaches to lower-resource languages, where parallel texts are not available. 6 Related Studies Existing approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorp"
P15-1119,P11-2033,0,0.022,"an be described as either graph-based models or transitionbased models (McDonald and Nivre, 2007). Graph-based models view the parsing problem as finding the highest scoring tree from a directed graph. The score of a dependency tree is typically factored into scores of some small structures (e.g. arcs) depending on the order of a model. Transition-based models aim to predict a transition sequence from an initial parser state to some terminal states, depending on the parsing history. This approach has a lot of interest since it is fast (linear time) and can incorporate rich non-local features (Zhang and Nivre, 2011). It has been considered that simple transitionbased parsing using greedy decoding and local training is not as accurate as graph-based parsers or transition-based parsers with beam-search and 1235 global training (Zhang and Clark, 2011). Recently, Chen and Manning (2014) show that greedy transition-based parsers can be greatly improved by using a well-designed neural network architecture. This approach can be considered as a new paradigm of parsing, in that it is based on pure distributed feature representations. In this study, we choose Chen and Manning’s architecture to build our basic depe"
P15-1119,P09-1007,0,0.00764621,"ting approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B"
P15-1119,D10-1030,0,0.0157323,"l., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings"
P15-1119,N12-1052,0,0.445863,"Missing"
P15-1119,J11-1005,0,\N,Missing
P15-1119,P13-2017,0,\N,Missing
P15-1166,W14-4012,0,0.0364514,"Missing"
P15-1166,P07-1092,0,0.0299357,"iments that demonstrate the effectiveness of our framework will be described in section 4. Lastly, we will conclude our work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for resource-poor language pa"
P15-1166,D13-1107,0,0.0175694,"work, where the convolutional neural network model was used. Hatori et al. (2012) proposed to jointly train word segmentation, POS tagging and dependency parsing, which can also be seen as a multi-task learning approach. Similar idea has also been proposed by Li et al. (2014) in Chinese dependency parsing. Most of multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005) where they jointly trained models and shared center parameters in NLP tasks. Researchers have also explored similar approaches (Sennrich et al., 2013; Cui et al., 2013) in statistical machine translation which are often refered as domain adaption. Our work explores the possibility of machine translation under the multitask framework by using the recurrent neural networks. To the best of our knowledge, this is the first trial of end to end machine translation under multi-task learning framework. 3 Multi-task Model for Multiple Language Translation Our model is a general framework for translating from one source language to many targets. The model we build in this section is a recurrent neural network based encoder-decoder model with multiple target tasks, and"
P15-1166,P14-1129,0,0.016516,"ework will be described in section 4. Lastly, we will conclude our work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for resource-poor language pairs, such as Spanish to Chinese. Considering the p"
P15-1166,P14-1066,0,0.0224282,"ed in section 4. Lastly, we will conclude our work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for resource-poor language pairs, such as Spanish to Chinese. Considering the problem of translat"
P15-1166,P12-1110,0,0.00596837,"s model. As a specific example model in this paper, we adopt a RNN encoder-decoder neural machine translation model for multi-task learning, though all neural network based model can be adapted in our framework. 1724 In the natural language processing field, a notable work related with multi-task learning was proposed by Collobert et al. (2011) which shared common representation for input words and solve different traditional NLP tasks such as part-of-Speech tagging, name entity recognition and semantic role labeling within one framework, where the convolutional neural network model was used. Hatori et al. (2012) proposed to jointly train word segmentation, POS tagging and dependency parsing, which can also be seen as a multi-task learning approach. Similar idea has also been proposed by Li et al. (2014) in Chinese dependency parsing. Most of multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005) where they jointly trained models and shared center parameters in NLP tasks. Researchers have also explored similar approaches (Sennrich et al., 2013; Cui et al., 2013) in statistical machine translation which are often refered as d"
P15-1166,D13-1176,0,0.1771,"to pivot language bilingual corpus and large-scale pivot language to target languages corpus. However, in reality, language pairs between English and many other target languages may not be large enough, and pivot-based SMT sometimes fails to handle this problem. Our approach handles one to many target language translation in a different way that we directly learn an end to multi-end translation system that does not need a pivot language based on the idea of neural machine translation. Neural Machine translation is a emerging new field in machine translation, proposed by several work recently (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014), aiming at end-to-end machine translation without phrase table extraction and language model training. Different from traditional statistical machine translation, neural machine translation encodes a variable-length source sentence with a recurrent neural network into a fixed-length vector representation and decodes it with another recurrent neural network from a fixed-length vector into variable-length target sentence. A typical model is the RNN encoder-decoder approach proposed by Bahdanau et al. (2014), which utilizes a bidirectional recurren"
P15-1166,koen-2004-pharaoh,0,0.00875525,"mization for end to multi-end model 3.4 Translation with Beam Search Although parallel corpora are available for the encoder and the decoder modeling in the training phrase, the ground truth is not available during test time. During test time, translation is produced by finding the most likely sequence via beam search. ˆ = argmax p(YTp |STp ) Y Y (15) Given the target direction we want to translate to, beam search is performed with the shared encoder and a specific target decoder where search space belongs to the decoder Tp . We adopt beam search algorithm similar as it is used in SMT system (Koehn, 2004) except that we only utilize scores produced by each decoder as features. The size of beam is 10 in our experiments for speedup consideration. Beam search is ended until the endof-sentence eos symbol is generated. Dataset The Europarl corpus is a multi-lingual corpus including 21 European languages. Here we only choose four language pairs for our experiments. The source language is English for all language pairs. And the target languages are Spanish (Es), French (Fr), Portuguese (Pt) and Dutch (Nl). To demonstrate the validity of our learning framework, we do some preprocessing on the training"
P15-1166,P14-1140,0,0.0169606,"Missing"
P15-1166,P02-1040,0,0.123027,"er is set to 1000. We trained our multi-task model with a multiGPU implementation due to the limitation of Graphic memory. And each target decoder is trained within one GPU card, and we synchronize our source encoder every 1000 batches among all GPU card. Our model costs about 72 hours on full large parallel corpora training until convergence and about 24 hours on partial parallel corpora training. During decoding, our implementation on GPU costs about 0.5 second per sentence. 4.3 Evaluation We evaluate the effectiveness of our method with EuroParl Common testset and WMT 2013 dataset. BLEU-4 (Papineni et al., 2002) is used as the evaluation metric. We evaluate BLEU scores on EuroParl Common test set with multi-task NMT models and single NMT models to demonstrate the validity of our multi-task learning framework. On the WMT 2013 data sets, we compare performance of separately trained NMT models, multi-task NMT models and Moses. We use the EuroParl Common test set as a development set in both neural machine translation experiments and Moses experiments. For single NMT models and multi-task NMT models, we select the best model with the highest BLEU score in the EuroParl Common testset and apply it to the W"
P15-1166,P13-1082,0,0.0405335,"beling within one framework, where the convolutional neural network model was used. Hatori et al. (2012) proposed to jointly train word segmentation, POS tagging and dependency parsing, which can also be seen as a multi-task learning approach. Similar idea has also been proposed by Li et al. (2014) in Chinese dependency parsing. Most of multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005) where they jointly trained models and shared center parameters in NLP tasks. Researchers have also explored similar approaches (Sennrich et al., 2013; Cui et al., 2013) in statistical machine translation which are often refered as domain adaption. Our work explores the possibility of machine translation under the multitask framework by using the recurrent neural networks. To the best of our knowledge, this is the first trial of end to end machine translation under multi-task learning framework. 3 Multi-task Model for Multiple Language Translation Our model is a general framework for translating from one source language to many targets. The model we build in this section is a recurrent neural network based encoder-decoder model with multipl"
P15-1166,D14-1003,0,0.0161313,"astly, we will conclude our work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for resource-poor language pairs, such as Spanish to Chinese. Considering the problem of translating one source language to"
P15-1166,P07-1108,1,0.525798,"rning method. Experiments that demonstrate the effectiveness of our framework will be described in section 4. Lastly, we will conclude our work in section 5. 2 Related Work Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality. Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus. Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang, 2007; Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014; Gao et al., 2014; Sundermeyer et al., 2014; Liu et al., 2014). On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation. A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a ”bridge” to generate source-target translation for language pair with few training corpus. Pivot based statistical machine translation is crucial in machine translation for r"
P16-1033,D14-1108,0,0.0232974,"nnotated sentence, where only the heads of “saw” and “with” are decided. upsurge of web data (e.g., tweets, blogs, and product comments) imposes great challenges to existing parsing techniques. Meanwhile, previous research on out-of-domain dependency parsing gains little success (Dredze et al., 2007; Petrov and McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial"
P16-1033,C10-1011,0,0.131693,"Missing"
P16-1033,W09-2307,0,0.0617144,"Missing"
P16-1033,P10-1001,0,0.0312588,"lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate. Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives. In this work, we for the first time apply a probabilistic CRF-based pa"
P16-1033,D07-1015,0,0.0416854,"Missing"
P16-1033,N06-1019,0,0.0701527,"Missing"
P16-1033,W08-1301,0,0.158099,"Missing"
P16-1033,C12-2067,0,0.0659251,"Missing"
P16-1033,C14-1075,1,0.946592,"certain metric for AL for sequence labeling problems. In the case of dependency parsing, the marginal probability of a dependency is the sum of probabilities of all legal trees that contain the dependency. ∑ p(h ↷ m|x; w) = p(d|x; w) (4) d∈Y(x):h↷m∈d Intuitively, marginal probability is a more principled metric for measuring reliability of a dependency since it considers all legal parses in the search space, compared to previous methods based on scores of local classifiers (Sassano and Kurohashi, 2010; Flannery and Mori, 2015) or votes of n-best parses (Mirroshandel and Nasr, 2011). Moreover, Li et al. (2014) find strong correlation between marginal probability and correctness of a dependency in cross-lingual syntax projection. Score(x, d∗ ) (5) n1.5 Normalized tree probability. The CRF-based parser allows us, for the first time in AL for dependency parsing, to directly use tree probabilities for uncertainty measurement. Unlike previous approximate methods based on k-best parses (Mirroshandel and Nasr, 2011), tree probabilities globally consider all parse trees in the search space, and thus are intuitively more consistent and proper for measuring the reliability of a tree. Our initial assumption i"
P16-1033,U12-1005,0,0.0219821,"2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They f"
P16-1033,P14-1126,0,0.0479024,"Missing"
P16-1033,W15-2202,0,0.24441,"notation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting"
P16-1033,I11-1087,0,0.0377813,"Missing"
P16-1033,W13-5711,0,0.102764,"arsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences pro"
P16-1033,D14-1097,0,0.0830651,"Missing"
P16-1033,I11-1100,0,0.0281049,"Missing"
P16-1033,P15-1119,1,0.88885,"Missing"
P16-1033,E06-1011,0,0.0810264,"4th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate. Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives. In this work, we for the first time apply a probabilistic CRF-based parsing model to AL for dependency parsing. We adopt the second-order graphbased model of McDonald and Pereira (2006), which casts the problem as finding an optimal tree from a fully-connect directed graph and factors the score of a dependency tree into scores of pairs of sibling dependencies. (1) This is the first work that applies a stateof-the-art probabilistic parsing model to AL for dependency parsing. The CRF-based dependency parser on the one hand allows us to use probabilities of trees or marginal probabilities of single dependencies for uncertainty measurement, and on the other hand can directly learn parameters from partially annotated trees. Using probabilistic models may be ubiquitous in AL for r"
P16-1033,P99-1010,0,0.894454,"Missing"
P16-1033,W07-2216,0,0.0829738,"Missing"
P16-1033,J04-3001,0,0.119634,"d McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and"
P16-1033,N12-1053,0,0.0241059,"omain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing perf"
P16-1033,C08-1113,0,0.0347094,"Missing"
P16-1033,P15-1134,0,0.075254,"Missing"
P16-1033,D14-1122,0,0.0228308,"where only the heads of “saw” and “with” are decided. upsurge of web data (e.g., tweets, blogs, and product comments) imposes great challenges to existing parsing techniques. Meanwhile, previous research on out-of-domain dependency parsing gains little success (Dredze et al., 2007; Petrov and McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which"
P16-1033,W11-2917,0,0.448477,"es full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units r"
P16-1033,P11-2033,1,0.841112,"ng trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate. Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives. In this work, we for the first time apply a probabilistic CRF-based parsing model to AL for de"
P16-1033,P92-1017,0,0.8556,"Missing"
P16-1033,D15-1039,0,0.0453421,"Missing"
P16-1033,P02-1035,0,0.254628,"e, and may be asked to annotate another selected word in the same sentence in next AL iteration. Obviously, frequently switching sentences incurs great waste of cognitive effort, 3.4 Learning from PA A major challenge for AL with PA is how to learn from partially labeled sentences, as depicted in Figure 1. Li et al. (2014) show that a probabilistic CRF-based parser can naturally and effectively learn from PA. The basic idea is converting a partial tree into a forest as shown in Figure 2, 347 and using the forest as the gold-standard reference during training, also known as ambiguous labeling (Riezler et al., 2002; T¨ackstr¨om et al., 2013). For each remaining word without head, we add all dependencies linking to it as long as the new dependency does not violate the existing dependencies. We denote the resulting forest as Fj, whose probability is naturally the sum of probabilities of each tree d in F. ∑ p(F|x; w) = p(d|x; w) d∈F ∑ eScore(x,d;w) = ∑ d∈F Score(x,d′ ;w) d′ ∈Y(x) e Train Chinese English ∑N i=1 log p(Fi |xi ; w) #Sentences 14,304 803 1,910 #Tokens 318,408 20,454 50,319 #Sentences 39,115 1,700 2,416 #Tokens 908,154 40,117 56,684 are selected and annotated at each iteration. In the case of si"
P16-1033,P10-1037,0,0.419473,"09). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Lin"
P16-1033,D08-1112,0,0.139903,"Missing"
P16-1033,D07-1014,0,0.220985,"Missing"
P16-1033,N13-1126,0,0.0792451,"Missing"
P16-1033,P02-1016,0,0.0777853,"l., 2007; Petrov and McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; F"
P18-1178,P16-1223,0,0.0449865,"answer based on three factors: the answer boundary, the answer content and the cross-passage answer verification. The experimental results show that our method outperforms the baseline by a large margin and achieves the state-of-the-art performance on the English MS-MARCO dataset and the Chinese DuReader dataset, both of which are designed for MRC in real-world settings. 1 Introduction Machine reading comprehension (MRC), empowering computers with the ability to acquire knowledge and answer questions from textual data, is believed to be a crucial step in building a general intelligent agent (Chen et al., 2016). Recent years have seen rapid growth in the MRC community. With the release of various datasets, the MRC task has evolved from the early cloze-style test (Hermann et al., 2015; Hill et al., 2015) to answer extraction from a single passage (Rajpurkar et al., 2016) and to the latest more complex question answering on web data (Nguyen et al., 2016; Dunn et al., 2017; He et al., 2017). Great efforts have also been made to develop models for these MRC tasks , especially for the answer extraction on single passage (Wang and Jiang, 2016; Seo et al., 2016; Pan et al., 2017). A significant milestone i"
P18-1178,W18-2605,1,0.891557,"Missing"
P18-1178,D17-1215,0,0.0195692,"h question, they use the search engine to retrieve multiple passages and the MRC models are required to read these passages in order to give the final answer. One of the intrinsic challenges for such multipassage MRC is that since all the passages are question-related but usually independently written, it’s probable that multiple confusing answer candidates (correct or incorrect) exist. Table 1 shows an example from MS-MARCO. We can see that all the answer candidates have semantic matching with the question while they are literally different and some of them are even incorrect. As is shown by Jia and Liang (2017), these confusing answer candidates could be quite difficult for MRC models to distinguish. Therefore, special consideration is required for such multi-passage MRC problem. In this paper, we propose to leverage the answer candidates from different passages to verify the final correct answer and rule out the noisy incorrect answers. Our hypothesis is that the cor* This work was done while the first author was doing internship at Baidu Inc. 1 https://rajpurkar.github.io/SQuAD-explorer/ 1918 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pag"
P18-1178,P17-1147,0,0.0608865,"ages. For the model training, Xiong et al. (2017) argues that the boundary loss encourages exact answers at the cost of penalizing overlapping answers. Therefore they propose a mixed objective that incorporates rewards derived from word overlap. Our joint training approach has a similar function. By taking the content and verification loss into consideration, our model will give less loss for overlapping answers than those unmatched answers, and our loss function is totally differentiable. Recently, we also see emerging interests in multi-passage MRC from both the academic (Dunn et al., 2017; Joshi et al., 2017) and industrial community (Nguyen et al., 2016; He et al., 2017). Early studies (Shen et al., 2017; Wang et al., 2017c) usually concat those passages and employ the same models designed for singlepassage MRC. However, more and more latest studies start to design specific methods that can read multiple passages more effectively. In the aspect of passage selection, Wang et al. (2017a) introduced a pipelined approach that rank the passages first and then read the selected passages for answering questions. Tan et al. (2017) treats the passage ranking as an auxiliary task that can be trained jointl"
P18-1178,P14-5010,0,0.00415096,"s one single answer here. Therefore, we also report the proportion of questions that have multiple answer spans to match with the human-generated answers. A span is taken as valid if it can achieve F1 score larger than 0.7 compared with any reference answer. From these statistics, we can see that the phenomenon of multiple answers is quite common for both MS-MARCO and DuReader. These answers will provide strong signals for answer verification if we can leverage them properly. 3.2 Implementation Details For MS-MARCO, we preprocess the corpus with the reversible tokenizer from Stanford CoreNLP (Manning et al., 2014) and we choose the span that achieves the highest ROUGE-L score with the reference answers as the gold span for training. We employ the 300-D pre-trained Glove embeddings (Pennington et al., 2014) and keep it fixed during training. The character embeddings are randomly initialized with its dimension as 30. For DuReader, we follow the preprocessing described in He et al. (2017). We tune the hyper-parameters according to the 1922 Model FastQA Ext (Weissenborn et al., 2017) Prediction (Wang and Jiang, 2016) ReasoNet (Shen et al., 2017) R-Net (Wang et al., 2017c) S-Net (Tan et al., 2017) Our Model"
P18-1178,P02-1040,0,0.101008,"or “No”. For DuReader, the retrieved document usually contains a large number of paragraphs that cannot be fed into MRC models directly (He et al., 2017). The original paper employs a simple a simple heuristic strategy to select a representative paragraph for each document, while we train a paragraph ranking model for this. We will demonstrate the effects of these two technologies later. 3.3 Results on MS-MARCO Table 3 shows the results of our system and other state-of-the-art models on the MS-MARCO test set. We adopt the official evaluation metrics, including ROUGE-L (Lin, 2004) and BLEU-1 (Papineni et al., 2002). As we can see, for both metrics, our single model outperforms all the other competing models with an evident margin, which is extremely hard considering the near-human perModel Match-LSTM BiDAF PR + BiDAF Our Model Human BLEU-4 31.8 31.9 37.55 40.97 56.1 ROUGE-L 39.0 39.2 41.81 44.18 57.4 Table 4: Performance on the DuReader test set Model Complete Model Answer Verification Content Modeling Joint Training YesNo Classification Boundary Baseline ROUGE-L 45.65 44.38 44.27 44.12 41.87 38.95 ∆ -1.27 -1.38 -1.53 -3.78 -6.70 Table 5: Ablation study on MS-MARCO development set formance. If we ensemb"
P18-1178,D14-1162,0,0.0827217,"ieve F1 score larger than 0.7 compared with any reference answer. From these statistics, we can see that the phenomenon of multiple answers is quite common for both MS-MARCO and DuReader. These answers will provide strong signals for answer verification if we can leverage them properly. 3.2 Implementation Details For MS-MARCO, we preprocess the corpus with the reversible tokenizer from Stanford CoreNLP (Manning et al., 2014) and we choose the span that achieves the highest ROUGE-L score with the reference answers as the gold span for training. We employ the 300-D pre-trained Glove embeddings (Pennington et al., 2014) and keep it fixed during training. The character embeddings are randomly initialized with its dimension as 30. For DuReader, we follow the preprocessing described in He et al. (2017). We tune the hyper-parameters according to the 1922 Model FastQA Ext (Weissenborn et al., 2017) Prediction (Wang and Jiang, 2016) ReasoNet (Shen et al., 2017) R-Net (Wang et al., 2017c) S-Net (Tan et al., 2017) Our Model S-Net (Ensemble) Our Model (Ensemble) Human ROUGE-L 33.67 37.33 38.81 42.89 45.23 46.15 46.65 46.66 47 BLEU-1 33.93 40.72 39.86 42.22 43.78 44.47 44.78 45.41 46 Table 3: Performance of our method"
P18-1178,C02-1169,0,0.176529,"Missing"
P18-1178,P17-1018,0,0.637994,"we formally present the details of modeling the question and passages. Encoding We first map each word into the vector space by concatenating its word embedding and sum of its character embeddings. Then we employ bi-directional LSTMs (BiLSTM) to encode the question Q and passages {Pi } as follows: Q Q Q uQ t = BiLSTMQ (ut−1 , [et , ct ]) (1) i uPt i = BiLSTMP (uPt−1 , [ePt i , cPt i ]) (2) Q Pi Pi where eQ t , ct , et , ct are the word-level and character-level embeddings of the tth word. uQ t and uPt i are the encoding vectors of the tth words in Q and Pi respectively. Unlike previous work (Wang et al., 2017c) that simply concatenates all the passages, we process the passages independently at the encoding and matching steps. Q-P Matching One essential step in MRC is to match the question with passages so that important information can be highlighted. We use the Attention Flow Layer (Seo et al., 2016) to conduct the Q-P matching in two directions. The similarity matrix S ∈ R|Q|×|Pi |between the question and passage i is changed to a simpler version, where the similarity between the tth word in the question and the k th word in passage i is computed as: | Pi St,k = uQ t · uk (3) Then the context-to"
P18-1178,K17-1028,0,0.0888184,"Missing"
P18-1178,D16-1264,0,\N,Missing
P19-1289,D18-1337,0,0.06727,"ranslation” model which also outputs target-side words before the whole input sentence is fed in, but there are several crucial differences: (a) their work still aims to translate full sentences using beam search, and is therefore, as the authors admit, “not a simultaneous translation model”; (b) their work does not anticipate future words; and (c) they use word alignments to learn the reordering and achieve it in decoding by emitting the  token, while our work integrates reordering into a single wait-k prediction model that is agnostic of, yet capable of, reordering. In another recent work, Alinejad et al. (2018) adds a prediction action to the work of Gu et al. (2017). Unlike Grissom II et al. (2014) who predict the source verb which might come after several words, they instead predict the immediate next source words, which we argue is not as useful in SOV-to-SVO translation. 4 In any case, we are the first to predict directly on the target side, thus integrating anticipation in a single translation model. Jaitly et al. (2016) propose an online neural transducer for speech recognition that is conditioned on prefixes. This problem does not have reorderings and thus no anticipation is needed. 8 Conclus"
P19-1289,J82-2005,0,0.638759,"Missing"
P19-1289,D14-1140,0,0.371479,"Missing"
P19-1289,K16-1010,0,0.0322637,"Missing"
P19-1289,E17-1099,0,0.472658,"hu`ıw`u. (a) Our wait-k policy (here k = 2) translates concurrently with the source sentence, but always k words behind. It correclty predicts the English verb given just the first 4 Chinese words (in bold), lit. “Bush president in Moscow”, because it is trained in a prefix-to-prefix fashion (Sec. 3), and the training data contains many prefix-pairs in the form of (X z`ai Y ..., X met ...). (c) The test-time wait-k decoding (Sec. 3.2) using the full-sentence model in (b) can not anticipate and produces nonsense translation. (d) A simultaneous translator without anticipation such as Gu et al. (2017) has to wait 5 words. (Grissom II et al., 2016), and have attempted to reduce latency by explicitly predicting the sentencefinal German (Grissom II et al., 2014) or English verbs (Matsubarayx et al., 2000), which is limited to this particular case, or unseen syntactic constituents (Oda et al., 2015; He et al., 2015), which requires incremental parsing on the source sentence. Some researchers propose to translate on an optimized sentence segment level to get better translation accuracy (Oda et al., 2014; Fujita et al., 2013; Bangalore et al., 2012). More recently, Gu et al. (2017) propose a two"
P19-1289,D15-1006,0,0.0494593,"Missing"
P19-1289,D17-1227,1,0.906619,"Missing"
P19-1289,P17-4012,0,0.0164908,"German↔English evaluation, we use newstest-2013 (dev) as our dev set and newstest-2015 (test) as our test set, with 3,000 and 2,169 sentence pairs, respectively. For Chinese↔English evaluation, we use NIST 2006 and NIST 2008 as our dev and test sets. They contain 616 and 691 Chinese sentences, each with 4 English references. When translating from Chinese to English, we report 4-reference BLEU scores, and in the reverse direction, we use the second among the four English references as the source text, and report 1-reference BLEU scores. Our implementation is adapted from PyTorchbased OpenNMT (Klein et al., 2017). Our Transformer is essentially the same as the base model from the original paper (Vaswani et al., 2017). 6.2 Quality and Latency of Wait-k Model Tab. 1 shows the results of a model trained with wait-k 0 but decoded with wait-k (where ∞ means full-sentence). Our wait-k is the diagonal, and the last row is the “test-time wait-k” decoding. Also, the best results of wait-k decoding is often from a model trained with a slightly larger k 0 . Figs. 5–8 plot translation quality (in BLEU) against latency (in AL and CW) for full-sentence baselines, our wait-k, test-time wait-k (using fullsentence mod"
P19-1289,P14-2090,0,0.0597317,"nd produces nonsense translation. (d) A simultaneous translator without anticipation such as Gu et al. (2017) has to wait 5 words. (Grissom II et al., 2016), and have attempted to reduce latency by explicitly predicting the sentencefinal German (Grissom II et al., 2014) or English verbs (Matsubarayx et al., 2000), which is limited to this particular case, or unseen syntactic constituents (Oda et al., 2015; He et al., 2015), which requires incremental parsing on the source sentence. Some researchers propose to translate on an optimized sentence segment level to get better translation accuracy (Oda et al., 2014; Fujita et al., 2013; Bangalore et al., 2012). More recently, Gu et al. (2017) propose a two-stage model whose base model is a full-sentence model, On top of that, they use a READ/WRITE (R/W) model to decide, at every step, whether to wait for another source word (READ) or to emit a target word using the pretrained base model (WRITE), and this R/W model is trained by reinforcement learning to prefer (rather than enforce) a specific latency, without updating the base model. All these efforts have the following major limitations: (a) none of them can achieve any arbitrary given latency such as"
P19-1289,P15-1020,0,0.0583849,"hion (Sec. 3), and the training data contains many prefix-pairs in the form of (X z`ai Y ..., X met ...). (c) The test-time wait-k decoding (Sec. 3.2) using the full-sentence model in (b) can not anticipate and produces nonsense translation. (d) A simultaneous translator without anticipation such as Gu et al. (2017) has to wait 5 words. (Grissom II et al., 2016), and have attempted to reduce latency by explicitly predicting the sentencefinal German (Grissom II et al., 2014) or English verbs (Matsubarayx et al., 2000), which is limited to this particular case, or unseen syntactic constituents (Oda et al., 2015; He et al., 2015), which requires incremental parsing on the source sentence. Some researchers propose to translate on an optimized sentence segment level to get better translation accuracy (Oda et al., 2014; Fujita et al., 2013; Bangalore et al., 2012). More recently, Gu et al. (2017) propose a two-stage model whose base model is a full-sentence model, On top of that, they use a READ/WRITE (R/W) model to decide, at every step, whether to wait for another source word (READ) or to emit a target word using the pretrained base model (WRITE), and this R/W model is trained by reinforcement learnin"
P19-1289,D18-1342,1,0.898779,"Missing"
P19-1289,P19-1582,1,0.895955,"Missing"
P19-1369,P18-1138,0,0.0708081,"formation (Zhang et al., 2018), recommending something (Li et al., 2018), and completing tasks (Bordes et al., 2016), most of which rely on background knowledge. However, many 1 https://github.com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-DuConv dialogue systems only rely on utterances and responses as training data, without explicitly exploiting knowledge associated with them, which sometimes results in uninformative and inappropriate responses (Wang et al., 2018). Although there exist some work that use external background knowledge to generate more informative responses (Liu et al., 2018; Yin et al., 2015; Zhu et al., 2017), these systems usually generate responses to answer questions instead of asking questions or leading the conversation. In order to solve the above problems, some new datasets have been created, where external background knowledge is explicitly linked to utterances (Dinan et al., 2019; Moghe et al., 2018), to facilitate the development of knowledge aware conversation models. With these datasets, conversation systems can be built to talk with humans given a topic based on the provided external knowledge. Unlike taskoriented systems (Bordes et al., 2016), the"
P19-1369,D18-1255,0,0.122597,"thout explicitly exploiting knowledge associated with them, which sometimes results in uninformative and inappropriate responses (Wang et al., 2018). Although there exist some work that use external background knowledge to generate more informative responses (Liu et al., 2018; Yin et al., 2015; Zhu et al., 2017), these systems usually generate responses to answer questions instead of asking questions or leading the conversation. In order to solve the above problems, some new datasets have been created, where external background knowledge is explicitly linked to utterances (Dinan et al., 2019; Moghe et al., 2018), to facilitate the development of knowledge aware conversation models. With these datasets, conversation systems can be built to talk with humans given a topic based on the provided external knowledge. Unlike taskoriented systems (Bordes et al., 2016), these conversation systems don’t have an explicit goal to achieve, thereof not able to plan over the background knowledge. In this paper, we take a radical step towards building another type of human-like conversational agent: endowing it with the ability of proactively leading the conversation with an explicit conversation goal. To this end, w"
P19-1369,C16-1318,0,0.128485,"tate the development of such conversation systems. 2.2 # dialogs # utterances average # utterances per dialog average # words per utterance average # words per dialog average # knowledge per dialogue Proactive Conversation Knowledge Grounded Conversation Leveraging knowledge for better dialogue modeling has drawn lots of research interests in past years and researchers have shown the multi-fold benefits of exploiting knowledge in dialogue modeling. One major research line is using knowledge to generate engaging, meaningful or personalized responses in chitchatting (Ghazvininejad et al., 2018; Vougiouklis et al., 2016; Zhou et al., 2018a; Zhang et al., 2018). In addition to proposing better conversation models, researchers also released several knowledge grounded datasets (Dinan et al., 2019; Moghe et al., 2018). Our work is most related to Mogh et al., (2018) and Dinan et al., (2019), where each utterance in their released datasets is aligned to the related knowledge, including both structured triplets and unstructured sentences. We extend their work, by including the whole knowledge graph into dialogue modeling and propose a new task of proactively leading the conversation via planning over the knowledge"
P19-1369,P18-1204,0,0.150476,"l agent is one of long-cherished goals in Artificial Intelligence (AI) (Turing, 2009). Typical conversations involve exchanging information (Zhang et al., 2018), recommending something (Li et al., 2018), and completing tasks (Bordes et al., 2016), most of which rely on background knowledge. However, many 1 https://github.com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-DuConv dialogue systems only rely on utterances and responses as training data, without explicitly exploiting knowledge associated with them, which sometimes results in uninformative and inappropriate responses (Wang et al., 2018). Although there exist some work that use external background knowledge to generate more informative responses (Liu et al., 2018; Yin et al., 2015; Zhu et al., 2017), these systems usually generate responses to answer questions instead of asking questions or leading the conversation. In order to solve the above problems, some new datasets have been created, where external background knowledge is explicitly linked to utterances (Dinan et al., 2019; Moghe et al., 2018), to facilitate the development of knowledge aware conversation models. With these datasets, conversation systems can be built to"
P19-1369,P17-1046,0,0.06064,"se generation model. We will give a detailed description of those two knowledgeaware models in next two sub-sections. 4.1 Retrieval-based Model Given a dialogue context X, the retrieval-based dialogue system responds to that context via searching for the best response Y from DuConv. Thus retrieval-based dialogue system often has a pipeline structure with two major steps: 1) retrieve response candidates from a database and 2) select the best one from the response candidates (Zhou et al., 2018b). In our retrieval-based method, the candidate responses are collected similar to most existing work (Wu et al., 2017; Zhou et al., 2018b) with one notable difference that we normalize the entities with their entity types in the knowledge graph to improve generalization capabilities. For each retrieved candidate response Y , the goal of our response ranker is to measure if Y is a good response to the context X considering the given dialogue goal G = [start, topic a, topic b] and related knowledge K. The matching 4 The workers are collected from a Chinese crowdsourcing platform http://test.baidu.com/. The workers are paid 2.5 Chinese Yuan per conversation. 3797 NLL Loss knowledge1 p(k1 x) knowledge2 p(k2 x) k"
P19-1369,D17-1233,0,0.0244852,"P ([x; y])) p(ki |x, y) = PN (3) j=1 exp(kj · M LP ([x; y])) exp(ki · x) p(ki |x) = PN j=1 exp(kj · x) (4) N 1 X p(ki |x, y) LKL (θ) = p(ki |x, y)log N p(ki |x) (5) i=1 Given that knowledge distribution p(ki |x) and p(ki |x, y), we fused all related P knowledge information into a vector kc = i p(ki |x, y) ∗ ki , same as our retrieval-based method, and feed it to the decoder for response generation. In the testing phase, the fused Pknowledge is estimated by the formula kc = i p(ki |x) ∗ ki without gold responses . The decoder is implemented with the Hierarchical Gated Fusion Unit described in (Yao et al., 2017), which is a standard GRU based decoder enhanced with external knowledge gates. Besides the KLDivLoss, our knowledge-aware generator introduces two additional loss functions: NLL Loss: the Negative Log-Likelihood (NLL) LN LL (θ) measures the difference between the true response and the response generated by our model. BOW Loss: We use the BOW loss proposed by Zhao et al., (2017), to ensure the accuracy of the fused knowledge kc by enforcing the relevancy between the knowledge and the true response. Specifically, let w = MLP(kc ) ∈ R|V |, where |V |is the vocabulary size and we define: exp(wyt"
P19-1369,P18-1205,0,0.377324,"d plan over the given knowledge graph. We establish baseline results on this dataset (about 270K utterances and 30k dialogues) using several state-of-the-art models. Experimental results show that dialogue models that plan over the knowledge graph can make full use of related knowledge to generate more diverse multi-turn conversations. The baseline systems along with the dataset are publicly available 1 . 1 Introduction Building a human-like conversational agent is one of long-cherished goals in Artificial Intelligence (AI) (Turing, 2009). Typical conversations involve exchanging information (Zhang et al., 2018), recommending something (Li et al., 2018), and completing tasks (Bordes et al., 2016), most of which rely on background knowledge. However, many 1 https://github.com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-DuConv dialogue systems only rely on utterances and responses as training data, without explicitly exploiting knowledge associated with them, which sometimes results in uninformative and inappropriate responses (Wang et al., 2018). Although there exist some work that use external background knowledge to generate more informative responses (Liu et al., 2018; Yin et al.,"
P19-1369,P17-1061,0,0.0525952,"r response generation. In the testing phase, the fused Pknowledge is estimated by the formula kc = i p(ki |x) ∗ ki without gold responses . The decoder is implemented with the Hierarchical Gated Fusion Unit described in (Yao et al., 2017), which is a standard GRU based decoder enhanced with external knowledge gates. Besides the KLDivLoss, our knowledge-aware generator introduces two additional loss functions: NLL Loss: the Negative Log-Likelihood (NLL) LN LL (θ) measures the difference between the true response and the response generated by our model. BOW Loss: We use the BOW loss proposed by Zhao et al., (2017), to ensure the accuracy of the fused knowledge kc by enforcing the relevancy between the knowledge and the true response. Specifically, let w = MLP(kc ) ∈ R|V |, where |V |is the vocabulary size and we define: exp(wyt ) p(yt |kc ) = P v exp(wv ) (6) Then, the BOW loss is defined to minimize: m LBOW (θ) = − 1 X logp(yt |kc ) m (7) t=1 In summary, the final loss of our generative model is: L(θ) = LKL (θ) + LN LL (θ) + LBOW (θ) 5 5.1 (8) Experiments Setting Our proposed models are tested under two settings: 1) automatic evaluation and 2) human evaluation. For automatic evaluation, we leverage se"
P19-1369,P18-1103,1,\N,Missing
W08-1112,A00-2018,0,0.226682,"(ni ), and (↑) refers to the Lexical Functional Grammar Lexical Functional Grammar (Kaplan and Bresnan, 1982) is a constraint-based grammar formalism which postulates (minimally) two levels of rep87 Model PCFG HB-PCFG LEX-PCFG Grammar Rule VP[↑=↓] → VV[↑=↓] NP[↑OBJ=↓] VP[↑=↓] → VV[↑=↓] NP[↑OBJ=↓] VP( )[↑=↓] → VV( )[↑=↓] NP( ¬ ¬ on)[↑OBJ=↓] Conditions VP[↑=↓], {PRED, SUBJ, OBJ} VP[↑=↓], {PRED, SUBJ, OBJ}, TOP VP( )[↑=↓], {PRED, SUBJ, OBJ} ¬ Table 1: Examples of f-structure annotated CFG rules (from Figure 1) in different models phenomena. Methodologies such as lexicalisation (Collins, 1997; Charniak, 2000) and tree transformations (Johnson, 1998), weaken the independence assumptions and have been applied successfully to parsing and shown significant improvements over simple PCFGs. In this section we study the effect of such methods in LFG-based generation for Chinese. f-structure associated with the mother (M ) node of ni , i.e. φ(M (ni )). 2.2 Generation from f-Structures The generation task in LFG is to determine which sentences correspond to a specified f-structure, given a particular grammar, such as (1). Kaplan and Wedekind (2000) proved that the set of strings generated by an LFG grammar"
W08-1112,P97-1003,0,0.0679979,"ADJUNCT and TOPIC etc., in the form of hierarchical attribute-value matrices. C-structures and f-structures are related by a piecewise correspondence function φ that goes from the nodes of a cstructure tree into units of f-structure spaces (Kaplan, 1995). As illustrated in Figure 1, given a c-structure node ni , the corresponding f-structure component fj is φ(ni ). Admissible c-structures are specified by a context-free grammar. The corresponding f-structures are derived from functional annotations attached to the CFG rewriting rules. of grammar transforms (Johnson, 1998) and lexicalisation (Collins, 1997)) has attracted substantial attention, to our knowledge, there has been a lot less research on this subject for surface realisation, a process that is generally regarded as the reverse process of parsing. Moreover, while most of the research so far has concentrated on English or European languages, we are also interested in generation for other languages with diverse properties, such as Chinese which is currently a focus language in parsing (Bikel, 2004; Cao et al., 2007). In this paper, we investigate three generative PCFG models for Chinese generation based on wide-coverage LFG grammars auto"
W08-1112,D07-1028,1,0.867227,"Missing"
W08-1112,J98-4004,0,0.25336,"(GFs) such as SUBJ(ect), OBJ(ect), ADJUNCT and TOPIC etc., in the form of hierarchical attribute-value matrices. C-structures and f-structures are related by a piecewise correspondence function φ that goes from the nodes of a cstructure tree into units of f-structure spaces (Kaplan, 1995). As illustrated in Figure 1, given a c-structure node ni , the corresponding f-structure component fj is φ(ni ). Admissible c-structures are specified by a context-free grammar. The corresponding f-structures are derived from functional annotations attached to the CFG rewriting rules. of grammar transforms (Johnson, 1998) and lexicalisation (Collins, 1997)) has attracted substantial attention, to our knowledge, there has been a lot less research on this subject for surface realisation, a process that is generally regarded as the reverse process of parsing. Moreover, while most of the research so far has concentrated on English or European languages, we are also interested in generation for other languages with diverse properties, such as Chinese which is currently a focus language in parsing (Bikel, 2004; Cao et al., 2007). In this paper, we investigate three generative PCFG models for Chinese generation based"
W08-1112,C00-1062,0,0.0343699,"models phenomena. Methodologies such as lexicalisation (Collins, 1997; Charniak, 2000) and tree transformations (Johnson, 1998), weaken the independence assumptions and have been applied successfully to parsing and shown significant improvements over simple PCFGs. In this section we study the effect of such methods in LFG-based generation for Chinese. f-structure associated with the mother (M ) node of ni , i.e. φ(M (ni )). 2.2 Generation from f-Structures The generation task in LFG is to determine which sentences correspond to a specified f-structure, given a particular grammar, such as (1). Kaplan and Wedekind (2000) proved that the set of strings generated by an LFG grammar from fully specified f-structures is a context-free language. Based on this theoretical cornerstone, Cahill and van Genabith (2006) presented a PCFG-based chart generator using wide-coverage LFG approximations automatically extracted from the Penn-II treebank. The LFG-based statistical generation model defines the conditional probability P (T |F ), for each candidate functionally annotated c-structure tree T (which fully specifies a surface realisation) given an fstructure F . The generation model searches for the Tbest that maximises"
W08-1112,P96-1027,0,0.0616902,"wn in the last line (LEX-PCFG) of Table 1. As in CKY chart parsing, generation grammars are binarised in our chart generator. Thus all grammar rules are either unary of the form X → H or binary X → Y H (or X → HY ), where H is the head constituent and Y is the modifier. To handle the problem of sparse data while estimating rule probabilities, a back-off to baseline model is employed. As, from a linguistic perspective, it is the modifier 4 Chart Generation and Smoothing Algorithms 4.1 Chart Generation Algorithm The PCFG-based generation algorithms are implemented in terms of a chart generator (Kay, 1996). In the generation algorithm, each (sub-)f-structure indexes a (sub-)chart. Each local chart generates the most probable trees for the local f-structure in a bottom-up manner: • generating lexical edges from the the local GF PRED and some atomic features representing function words, mood or aspect etc. 2 3 We use a mechanism similar to (Collins, 1997) but adapted to Chinese data to find lexical heads in the treebank data. Except for prepositional phrases, localiser and some verbal phrases. 89 m NP( ) NN NR NN NN [↓∈↑ADJUNCT] [↓∈↑ADJUNCT] [↓∈↑ADJUNCT] þ° ¥ Shanghai  tennis m m masters cup ("
W08-1112,A00-2023,0,0.0909096,"Missing"
W08-1112,W02-2103,0,0.0254319,"such as [↑SUBJ =↓], [↑=↓] etc. As a heuristic based on linguistic experience, we define the order of importance of these elements as follows: Type PCFG HB-PCFG LEX-PCFG X(c) &gt; H(c) &gt; Y (a) &gt; Y (c) &gt; X(a) &gt; H(a) (4) IP[↑COMP=↓] → NP[↑SUBJ=↓] VP[↑=↓] For the above example rule (4), the importance of the elements is: with features 22,372 28,487 325,094 without features 8,548 11,969 286,468 Table 3: Number of rules in the training set IP &gt; VP &gt; [↑SUBJ=↓] &gt; NP &gt; [↑COMP=↓] &gt; [↑=↓] The generation system is evaluated against the raw text of the test data in terms of accuracy and coverage. Following (Langkilde, 2002) and other work on general-purpose generators, we adopt BLEU score (Papineni et al., 2002), average simple string accuracy (SSA) and percentage of exactly matched sentences for accuracy evaluation.6 For coverage evaluation, we measure the percentage of input fstructures that generate a sentence. The elements can be deleted from the rules in an importance order from low to high.5 The partial rules adopted in our system ignore the least important 3 elements, viz. the functional annotation of the head node H(a), the functional annotation on LHS X(a) and constituent category of the modifier node Y"
W08-1112,C00-1007,0,0.0965467,"Missing"
W08-1112,P06-1130,1,0.842207,"Missing"
W08-1112,W07-2303,0,0.0271024,"Missing"
W08-1112,W05-1510,0,0.0346457,"Missing"
W08-1112,P02-1040,0,0.0762368,"fine the order of importance of these elements as follows: Type PCFG HB-PCFG LEX-PCFG X(c) &gt; H(c) &gt; Y (a) &gt; Y (c) &gt; X(a) &gt; H(a) (4) IP[↑COMP=↓] → NP[↑SUBJ=↓] VP[↑=↓] For the above example rule (4), the importance of the elements is: with features 22,372 28,487 325,094 without features 8,548 11,969 286,468 Table 3: Number of rules in the training set IP &gt; VP &gt; [↑SUBJ=↓] &gt; NP &gt; [↑COMP=↓] &gt; [↑=↓] The generation system is evaluated against the raw text of the test data in terms of accuracy and coverage. Following (Langkilde, 2002) and other work on general-purpose generators, we adopt BLEU score (Papineni et al., 2002), average simple string accuracy (SSA) and percentage of exactly matched sentences for accuracy evaluation.6 For coverage evaluation, we measure the percentage of input fstructures that generate a sentence. The elements can be deleted from the rules in an importance order from low to high.5 The partial rules adopted in our system ignore the least important 3 elements, viz. the functional annotation of the head node H(a), the functional annotation on LHS X(a) and constituent category of the modifier node Y (c). Examples of the two types of smoothed rules are shown in Table 2. Table 4 reports th"
W08-1112,2005.mtsummit-papers.15,0,0.0421581,"Missing"
W08-1112,2007.mtsummit-ucnlg.4,0,0.0346131,"Missing"
W08-1112,J96-2001,0,\N,Missing
W13-5708,C10-1011,0,0.109851,"Missing"
W13-5708,J08-4003,0,0.015643,"iments in this paper. In a typical transition-based parsing process, the input words are stored in a queue and partially built dependency structures (e.g., sub-trees) are organized by a configuration (or state). A parser configuration (or, state) can be represented by a tuple &lt; S, N, A &gt;, where S is the stack, N is the queue of incoming words, and A is the set of dependency arcs that have been built. A set of shift-reduce actions are defined, which are used to construct new dependency arcs by connecting the top word of the queue and the top word of the stack. We adopt the arc-standard system (Nivre, 2008), whose actions include: about an exponential number of semantically neighbouring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. The general process of neural language model based word embedding is as follows: • associate with each word in the vocabulary a distributed word feature vector (a real valued vector in Rm ); • express the joint probability function of word sequences in terms of the feature vectors of these words in the sequence; and, • sh"
W13-5708,W09-2307,0,0.0605908,"Missing"
W13-5708,Q13-1025,0,0.0225926,"Missing"
W13-5708,P12-2003,0,0.0222484,"Missing"
W13-5708,Q13-1012,0,0.0292666,"Missing"
W13-5708,P10-1110,0,0.397686,"who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templates by making use of words, POS tags, CPOS tags, NLMWEbased word classes and their combinations. NLMWEbased word classes is shown to be an important supplement of POS-tags. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWEbased word classes yield the best UASs. 1.1 Shift-reduce parsing We use a transition-based shift-reduce parser (Kudo and Matsumoto, 2002; Nivre, 2003; Nivre et al., 2006; Huang and Sagae, 2010) to 73 perform all the experiments in this paper. In a typical transition-based parsing process, the input words are stored in a queue and partially built dependency structures (e.g., sub-trees) are organized by a configuration (or state). A parser configuration (or, state) can be represented by a tuple &lt; S, N, A &gt;, where S is the stack, N is the queue of incoming words, and A is the set of dependency arcs that have been built. A set of shift-reduce actions are defined, which are used to construct new dependency arcs by connecting the top word of the queue and the top word of the stack. We ado"
W13-5708,P08-1068,0,0.113017,"a correct tree if there are out-of-vocabulary (OOV) words (compared to the training data of the treebank) and/or the POS-tags are wrongly annotated? Words need to be generalized to solve this problem in a sense. Indeed, POS-tag itself is a way to generalize words into word classes. This is because POS-taggers can be trained on larger-scale data compared with treebanks. Annotating trees is far more difficult than annotating POS-tags. Considering that unsupervised word clustering methods can make use of TB/PB-level Web data, these approaches have been shown to be helpful for dependency parsing (Koo et al., 2008). In this paper, we investigate the influence of generalization of words to the accuracies of Chinese dependency parsing. Specially, in our shift-reduce parser, we use a neural language model based word embedding method (Bengio et al., 2003) to generate distributed word feature vectors and then perform Kmeans based word clustering (Yu et al., 2013) to generate word classes. Our usage of word embedding is in line with Turian et al. (2010) and Yu et al. (2013), who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templat"
W13-5708,W02-2016,0,0.0433687,"is in line with Turian et al. (2010) and Yu et al. (2013), who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templates by making use of words, POS tags, CPOS tags, NLMWEbased word classes and their combinations. NLMWEbased word classes is shown to be an important supplement of POS-tags. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWEbased word classes yield the best UASs. 1.1 Shift-reduce parsing We use a transition-based shift-reduce parser (Kudo and Matsumoto, 2002; Nivre, 2003; Nivre et al., 2006; Huang and Sagae, 2010) to 73 perform all the experiments in this paper. In a typical transition-based parsing process, the input words are stored in a queue and partially built dependency structures (e.g., sub-trees) are organized by a configuration (or state). A parser configuration (or, state) can be represented by a tuple &lt; S, N, A &gt;, where S is the stack, N is the queue of incoming words, and A is the set of dependency arcs that have been built. A set of shift-reduce actions are defined, which are used to construct new dependency arcs by connecting the to"
W13-5708,C12-1103,0,0.287843,"Missing"
W13-5708,P13-2020,0,0.0212508,"Missing"
W13-5708,P10-1040,0,0.141137,"onsidering that unsupervised word clustering methods can make use of TB/PB-level Web data, these approaches have been shown to be helpful for dependency parsing (Koo et al., 2008). In this paper, we investigate the influence of generalization of words to the accuracies of Chinese dependency parsing. Specially, in our shift-reduce parser, we use a neural language model based word embedding method (Bengio et al., 2003) to generate distributed word feature vectors and then perform Kmeans based word clustering (Yu et al., 2013) to generate word classes. Our usage of word embedding is in line with Turian et al. (2010) and Yu et al. (2013), who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templates by making use of words, POS tags, CPOS tags, NLMWEbased word classes and their combinations. NLMWEbased word classes is shown to be an important supplement of POS-tags. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWEbased word classes yield the best UASs. 1.1 Shift-reduce parsing We use a transition-based shift-reduce parser (Kudo and Matsumoto, 2002; Nivre, 200"
W13-5708,N13-1063,1,0.760145,"ared with treebanks. Annotating trees is far more difficult than annotating POS-tags. Considering that unsupervised word clustering methods can make use of TB/PB-level Web data, these approaches have been shown to be helpful for dependency parsing (Koo et al., 2008). In this paper, we investigate the influence of generalization of words to the accuracies of Chinese dependency parsing. Specially, in our shift-reduce parser, we use a neural language model based word embedding method (Bengio et al., 2003) to generate distributed word feature vectors and then perform Kmeans based word clustering (Yu et al., 2013) to generate word classes. Our usage of word embedding is in line with Turian et al. (2010) and Yu et al. (2013), who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templates by making use of words, POS tags, CPOS tags, NLMWEbased word classes and their combinations. NLMWEbased word classes is shown to be an important supplement of POS-tags. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWEbased word classes yield the best UASs. 1.1 Shift-reduce"
W13-5708,D08-1059,0,0.579374,"tion of word sequences in terms of the feature vectors of these words in the sequence; and, • shift, which removes the top word in the queue and pushes it onto the top of the stack; • left-arc, which pops the top item off the stack, and adds it as a modifier to the front of the queue; • right-arc, which removes the front of the queue, and adds it as a modifier to the top of the stack. In addition, the top of the stack is popped and added to the front of the queue. We follow Kudo and Matsumoto (2002) and use the Support Vector Machines (SVMs) for action classification training and beam search (Zhang and Clark, 2008) for decoding. 2 Neural Language Model Based Word Embedding Following (Bengio et al., 2003), we use a neural network with two hidden layers to learn distributed word feature vectors from large-scale training data. Recall that, the goal of statistical language modelling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training (so called OOV words and/or sequences). Ngram based a"
W13-5708,D12-1030,0,0.0295383,"Missing"
W13-5708,P11-2033,0,0.0280779,"periments. 3 Feature Templates At each step during shift-reducing, a parser configuration (or, state) can be represented by a tuple &lt; S, N, A &gt;. We denote the top of stack with S0 , the front items from the queue with N0 , N1 , N2 , and N3 , the leftmost and rightmost modifiers of S0 (if any) with S0l and S0r , respectively, and the leftmost modifier of N0 (if any) with N0l (refer to Figure 1). The baseline feature templates without any word class level information (such as POS-tags) are shown in Table 1. These features are mostly taken from Zhang and Clark (2008), Huang and Sagae (2010), and Zhang and Nivre (2011). In this table, w, l and d represents the word, dependency label, and the distance between S0 and N0 , respectively. For example, S0 wN0 w represents the feature template that takes the word of S0 , and combines it with the word of N0 . In Table 1, (S/N )0l2 , (S/N )0r2 , and (S/N )0rn refer to the second leftmost modifier, the second rightmost modifier, and the right nearest modifier of (S/N )0 , respectively. It should be mentioned that, for the arc-standard algorithm used in this paper, S0 or N0 never contain a head word. The reason is that, once the head is found for a node, that node wil"
W13-5708,E06-1011,0,0.055922,"Missing"
W13-5708,nivre-etal-2006-maltparser,0,0.11463,"Missing"
W13-5708,W03-3017,0,0.058527,"al. (2010) and Yu et al. (2013), who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templates by making use of words, POS tags, CPOS tags, NLMWEbased word classes and their combinations. NLMWEbased word classes is shown to be an important supplement of POS-tags. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWEbased word classes yield the best UASs. 1.1 Shift-reduce parsing We use a transition-based shift-reduce parser (Kudo and Matsumoto, 2002; Nivre, 2003; Nivre et al., 2006; Huang and Sagae, 2010) to 73 perform all the experiments in this paper. In a typical transition-based parsing process, the input words are stored in a queue and partially built dependency structures (e.g., sub-trees) are organized by a configuration (or state). A parser configuration (or, state) can be represented by a tuple &lt; S, N, A &gt;, where S is the stack, N is the queue of incoming words, and A is the set of dependency arcs that have been built. A set of shift-reduce actions are defined, which are used to construct new dependency arcs by connecting the top word of the"
W18-2605,P17-1055,0,0.0293444,"Dataset from Real-world Applications Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, Haifeng Wang Baidu Inc., Beijing, China {hewei06, liukai20, liujing46, lvyajuan, zhaoshiqi, xiaoxinyan, liuyuan04, wangyizhong01, wu hua, sheqiaoqiao, liuxuan, wutian, wanghaifeng}@baidu.com Abstract 2016; Nguyen et al., 2016). In recent years, a number of datasets have been developed for MRC, as shown in Table 1. These datasets have led to advances such as Match-LSTM (Wang and Jiang, 2017), BiDAF (Seo et al., 2016), AoA Reader (Cui et al., 2017), DCN (Xiong et al., 2017) and RNet (Wang et al., 2017). This paper hopes to advance MRC even further with the release of DuReader, challenging the community to deal with more realistic data sources, more types of questions and more scale, as illustrated in Tables 1-4. Table 1 highlights DuReader’s advantages over previous datasets in terms of data sources and scale. Tables 2-4 highlight DuReader’s advantages in the range of questions. Ideally, a good dataset should be based on questions from real applications. However, many existing datasets have been forced to make various compromises such a"
W18-2605,C16-1167,0,0.21916,"Reader, challenging the community to deal with more realistic data sources, more types of questions and more scale, as illustrated in Tables 1-4. Table 1 highlights DuReader’s advantages over previous datasets in terms of data sources and scale. Tables 2-4 highlight DuReader’s advantages in the range of questions. Ideally, a good dataset should be based on questions from real applications. However, many existing datasets have been forced to make various compromises such as: (1) cloze task: Data is synthesized missing a keyword. The task is to fill in the missing keyword (Hermann et al., 2015; Cui et al., 2016; Hill et al., 2015). (2) multiple-choice exams: Richardson et al. (2013) collect both fictional stories and the corresponding multiple-choice questions by crowdsourcing. Lai et al. (2017) collect the multiple-choice questions from English exams. (3) crowdsourcing: Turkers are given documents (e.g., articles from the news and/or Wikipedia) and are asked to construct questions after reading the documents(Trischler et al., 2017; Rajpurkar et al., 2016; Koˇcisk`y et al., 2017). The limitations of the datasets lead to build datasets based on queries that real users submitted to real search engines"
W18-2605,P17-1147,0,0.0514491,"du.com) is the largest Chinese community-based question answering (CQA) site in the world. 2 http://ai.baidu.com/broad/download? dataset=dureader 3 https://github.com/baidu/DuReader 37 Proceedings of the Workshop on Machine Reading for Question Answering, pages 37–46 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics Dataset CNN/DM (Hermann et al., 2015) HLF-RC (Cui et al., 2016) CBT (Hill et al., 2015) RACE (Lai et al., 2017) MCTest (Richardson et al., 2013) NewsQA (Trischler et al., 2017) SQuAD (Rajpurkar et al., 2016) SearchQA (Dunn et al., 2017) TrivaQA (Joshi et al., 2017) NarrativeQA (Koˇcisk`y et al., 2017) MS-MARCO (Nguyen et al., 2016) DuReader (this paper) Lang EN ZH EN EN EN EN EN EN EN EN EN ZH #Que. 1.4M 100K 688K 870K 2K 100K 100K 140K 40K 46K 100K 200k #Docs 300K 28K 108 50K 500 10K 536 6.9M 660K 1.5K 200K1 1M Source of Que. Synthetic cloze Synthetic cloze Synthetic cloze English exam Crowdsourced Crowdsourced Crowdsourced QA site Trivia websites Crowdsourced User logs User logs Source of Docs News Fairy/News Children’s books English exam Fictional stories CNN Wiki. Web doc. Wiki./Web doc. Book&movie Web doc. Web doc./CQA Answer Type Fill in entity Fi"
W18-2605,W04-1013,0,0.0483886,"paragraph or passage. In contrast, DuReader provides the full body text of each document to stimulate the research in a real-world setting. get a vector representation for each position. Implementation Details We randomly initialize the word embeddings with a dimension of 300 and set the hidden vector size as 150 for all layers. We use the Adam algorithm (Kingma and Ba, 2014) to train both MRC models with an initial learning rate of 0.001 and a batch size of 32. 4.2 Results and Analysis We evaluate the reading comprehension task via character-level BLEU-4 (Papineni et al., 2002) and Rouge-L (Lin, 2004), which are widely used for evaluating the quality of language generation. The experimental results on test set are shown in Table 6. For comparison, we also evaluate the Selected Paragraph that has the largest overlap with the question among all documents. We also assess human performance by involving a new annotator to annotate on the test data and treat his first answer as the prediction. The results demonstrate that current reading comprehension models can achieve an impressive improvement compared with the selected paragraph baseline, which approves the effectiveness of these models. Howe"
W18-2605,P02-1040,0,0.100891,"uppose to find the answer in a small paragraph or passage. In contrast, DuReader provides the full body text of each document to stimulate the research in a real-world setting. get a vector representation for each position. Implementation Details We randomly initialize the word embeddings with a dimension of 300 and set the hidden vector size as 150 for all layers. We use the Adam algorithm (Kingma and Ba, 2014) to train both MRC models with an initial learning rate of 0.001 and a batch size of 32. 4.2 Results and Analysis We evaluate the reading comprehension task via character-level BLEU-4 (Papineni et al., 2002) and Rouge-L (Lin, 2004), which are widely used for evaluating the quality of language generation. The experimental results on test set are shown in Table 6. For comparison, we also evaluate the Selected Paragraph that has the largest overlap with the question among all documents. We also assess human performance by involving a new annotator to annotate on the test data and treat his first answer as the prediction. The results demonstrate that current reading comprehension models can achieve an impressive improvement compared with the selected paragraph baseline, which approves the effectivene"
W18-2605,D16-1264,0,0.485744,"ke various compromises such as: (1) cloze task: Data is synthesized missing a keyword. The task is to fill in the missing keyword (Hermann et al., 2015; Cui et al., 2016; Hill et al., 2015). (2) multiple-choice exams: Richardson et al. (2013) collect both fictional stories and the corresponding multiple-choice questions by crowdsourcing. Lai et al. (2017) collect the multiple-choice questions from English exams. (3) crowdsourcing: Turkers are given documents (e.g., articles from the news and/or Wikipedia) and are asked to construct questions after reading the documents(Trischler et al., 2017; Rajpurkar et al., 2016; Koˇcisk`y et al., 2017). The limitations of the datasets lead to build datasets based on queries that real users submitted to real search engines. MS-MARCO (Nguyen et al., 2016) is based on Bing logs (in English), and DuReader (this paper) is based on the logs of Baidu Search (in Chinese). Besides question sources, DuReader complements MS-MARCO and other datasets in the following ways: question types: DuReader contains a richer inThis paper introduces DuReader, a new large-scale, open-domain Chinese machine reading comprehension (MRC) dataset, designed to address real-world MRC. DuReader has"
W18-2605,D13-1020,0,0.161908,"a sources, more types of questions and more scale, as illustrated in Tables 1-4. Table 1 highlights DuReader’s advantages over previous datasets in terms of data sources and scale. Tables 2-4 highlight DuReader’s advantages in the range of questions. Ideally, a good dataset should be based on questions from real applications. However, many existing datasets have been forced to make various compromises such as: (1) cloze task: Data is synthesized missing a keyword. The task is to fill in the missing keyword (Hermann et al., 2015; Cui et al., 2016; Hill et al., 2015). (2) multiple-choice exams: Richardson et al. (2013) collect both fictional stories and the corresponding multiple-choice questions by crowdsourcing. Lai et al. (2017) collect the multiple-choice questions from English exams. (3) crowdsourcing: Turkers are given documents (e.g., articles from the news and/or Wikipedia) and are asked to construct questions after reading the documents(Trischler et al., 2017; Rajpurkar et al., 2016; Koˇcisk`y et al., 2017). The limitations of the datasets lead to build datasets based on queries that real users submitted to real search engines. MS-MARCO (Nguyen et al., 2016) is based on Bing logs (in English), and"
W18-2605,W17-2623,0,0.246887,"s have been forced to make various compromises such as: (1) cloze task: Data is synthesized missing a keyword. The task is to fill in the missing keyword (Hermann et al., 2015; Cui et al., 2016; Hill et al., 2015). (2) multiple-choice exams: Richardson et al. (2013) collect both fictional stories and the corresponding multiple-choice questions by crowdsourcing. Lai et al. (2017) collect the multiple-choice questions from English exams. (3) crowdsourcing: Turkers are given documents (e.g., articles from the news and/or Wikipedia) and are asked to construct questions after reading the documents(Trischler et al., 2017; Rajpurkar et al., 2016; Koˇcisk`y et al., 2017). The limitations of the datasets lead to build datasets based on queries that real users submitted to real search engines. MS-MARCO (Nguyen et al., 2016) is based on Bing logs (in English), and DuReader (this paper) is based on the logs of Baidu Search (in Chinese). Besides question sources, DuReader complements MS-MARCO and other datasets in the following ways: question types: DuReader contains a richer inThis paper introduces DuReader, a new large-scale, open-domain Chinese machine reading comprehension (MRC) dataset, designed to address real"
W18-2605,P17-1018,0,0.0958802,"Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, Haifeng Wang Baidu Inc., Beijing, China {hewei06, liukai20, liujing46, lvyajuan, zhaoshiqi, xiaoxinyan, liuyuan04, wangyizhong01, wu hua, sheqiaoqiao, liuxuan, wutian, wanghaifeng}@baidu.com Abstract 2016; Nguyen et al., 2016). In recent years, a number of datasets have been developed for MRC, as shown in Table 1. These datasets have led to advances such as Match-LSTM (Wang and Jiang, 2017), BiDAF (Seo et al., 2016), AoA Reader (Cui et al., 2017), DCN (Xiong et al., 2017) and RNet (Wang et al., 2017). This paper hopes to advance MRC even further with the release of DuReader, challenging the community to deal with more realistic data sources, more types of questions and more scale, as illustrated in Tables 1-4. Table 1 highlights DuReader’s advantages over previous datasets in terms of data sources and scale. Tables 2-4 highlight DuReader’s advantages in the range of questions. Ideally, a good dataset should be based on questions from real applications. However, many existing datasets have been forced to make various compromises such as: (1) cloze task: Data is synthesized missing a keywor"
W19-5341,N13-1073,0,0.0800459,"a, increasing the model diversity for the model ensemble. The experimental results indicate that this method achieves absolute improvements over the single system (at most a 1.7 BLEU point improvements). 2.9 Experiments and Results 3.1 Pre-processing and Post-processing The Chinese data has been tokenized using the Jieba tokenizer3 . For English data, punctuation normalization, aggressive tokenization and truecasing are applied orderly to all sentences with the scripts provided in Moses. We also filter the parallel sentences which are duplicated or bad alignment scores obtained by fast-align (Dyer et al., 2013), and then we have a preprocessed bilingual training data consisting of 18M parallel sentences. In post-processing phase, the English translations are true-cased and de-tokenized with the scripts provided in Moses. We use simple rules to normalize the punctuations and Arabic numerals in the Chinese translations. Re-ranking In order to get better translation results, we generate n-best hypotheses with an ensemble model and then train a re-ranker using k-best MIRA (Cherry and Foster, 2012) on the validation set. K-best MIRA is a version of MIRA (Chiang et al., 2008) that works with a batch tunin"
W19-5341,D18-1045,0,0.109803,"er improved. In the next iteration, the two improved models can potentially generate better synthetic parallel data. This procedure can be applied in several iterations until no further improvement can be obtained. In addition, we also augment the training data by exploring the bilingual corpus rather than the monolingual corpus. Specifically, we translate the sentences in the target language back into the source language by diverse training models, such as Left-to-right model and Right-to-left model. This procedure can be viewed as one alternative Large-scale Back-Translation In recent work, Edunov et al. (2018) proposed an effective approach to improve the translation quality by exploiting back-translation mechanism on the large-scale monolingual corpus. Following their work, we also train our model on the synthetic bilingual corpus to further improve the performance. However, the provided monolingual data contains a certain amount of noise and out-ofdomain data which may affect the translation quality implicitly. Therefore, we use a language model to select high-quality and in-domain data from the 376 Source CWMT UN Wiki Titles Total solution for alleviating the exposure bias problem (Ranzato et al"
W19-5341,D16-1139,0,0.0541292,"ck-translation mechanism on the large-scale monolingual corpus. Following their work, we also train our model on the synthetic bilingual corpus to further improve the performance. However, the provided monolingual data contains a certain amount of noise and out-ofdomain data which may affect the translation quality implicitly. Therefore, we use a language model to select high-quality and in-domain data from the 376 Source CWMT UN Wiki Titles Total solution for alleviating the exposure bias problem (Ranzato et al., 2016). 2.6 Knowledge Distillation The early adoption of knowledge distillation (Kim and Rush, 2016) is for model compression, where the goal is to deliver a compact student model that matches the accuracy of a large teacher model or the ensemble of models. In our knowledge distillation approach, we translate the source side of the bilingual data with a Right-to-Left (R2L) (Liu et al., 2016) model teacher and different architecture NMT teachers to use the translations as additional training data for the student network. Considering that distillation from a bad teacher model is likely to hurt the student model and thus result in inferior accuracy, we selectively use distillation in the traini"
W19-5341,N16-1046,0,0.0898531,"Missing"
W19-5341,N12-1047,0,0.0413295,"Moses. We also filter the parallel sentences which are duplicated or bad alignment scores obtained by fast-align (Dyer et al., 2013), and then we have a preprocessed bilingual training data consisting of 18M parallel sentences. In post-processing phase, the English translations are true-cased and de-tokenized with the scripts provided in Moses. We use simple rules to normalize the punctuations and Arabic numerals in the Chinese translations. Re-ranking In order to get better translation results, we generate n-best hypotheses with an ensemble model and then train a re-ranker using k-best MIRA (Cherry and Foster, 2012) on the validation set. K-best MIRA is a version of MIRA (Chiang et al., 2008) that works with a batch tuning to learn a re-ranker for the n-best hypotheses. The features we use for re-ranking are: • NMT Features: Ensemble model score and Right-to-Left model score. 3.2 • Language Model Features: Multiple n-gram language models and backward n-gram language models. For Chinese→English task, we do not use all of the 18M preprocessed parallel sentences, in that there is much out-of-domain data in UN corpus. Table 1 shows that the 6.7M CWMT corpus and 9M UN corpus which are selected ran• Length Fea"
W19-5341,2015.iwslt-evaluation.11,0,0.0627984,"a teacher model are filtered if BLEU scores are below a threshold τ . According to our previous empirical results, we select English translations with BLEU score higher than 30 and Chinese translations with BLEU score higher than 42. There are two kinds of teacher models to help a student model improve translation performance: En→Chn 6.7M 3.5M 0.6M 10.8M Table 1: Statistics of the bilingual training data (Chn indicates Chinese while En indicates English). data. The dominant approach for domain adaptation is training on large-scale out-of-domain data and then fine-tuning on the in-domain data (Luong and Manning, 2015). Thus the effectiveness of the domain adaptation depends on the selected in-domain data. According to our previous empirical results, using the WMT 18 dev set to fine-tune the models straightforwardly achieves the best results. In our final submission, we set the batch size to 1,024 and fine-tune the model for a few iterations on the WMT 18 dev set. It is surprising to find a gain of almost +2 BLEU improvement on WMT 18 Chinese→English test set. However, on WMT 18 English→Chinese test set, the improvement is not significant. In WMT 17 and 18, the source side of both dev set and test set are c"
W19-5341,D08-1024,0,0.0139368,"scores obtained by fast-align (Dyer et al., 2013), and then we have a preprocessed bilingual training data consisting of 18M parallel sentences. In post-processing phase, the English translations are true-cased and de-tokenized with the scripts provided in Moses. We use simple rules to normalize the punctuations and Arabic numerals in the Chinese translations. Re-ranking In order to get better translation results, we generate n-best hypotheses with an ensemble model and then train a re-ranker using k-best MIRA (Cherry and Foster, 2012) on the validation set. K-best MIRA is a version of MIRA (Chiang et al., 2008) that works with a batch tuning to learn a re-ranker for the n-best hypotheses. The features we use for re-ranking are: • NMT Features: Ensemble model score and Right-to-Left model score. 3.2 • Language Model Features: Multiple n-gram language models and backward n-gram language models. For Chinese→English task, we do not use all of the 18M preprocessed parallel sentences, in that there is much out-of-domain data in UN corpus. Table 1 shows that the 6.7M CWMT corpus and 9M UN corpus which are selected ran• Length Features: Length ratio and length difference between source sentences and hypothe"
W19-5341,N18-1202,0,0.00961915,"ion heads to 16. Both Chinese and English pre-training took 7 days to complete. In the fine-tuning procedure of the translation task, we employ a pre-trained language model as encoder of NMT, and the parameters of decoders are learned during fine-tuning. The decoder has 6 self-attention layers, and the hidden size is 1024, which is same with the decoder of standard big Transformer. During fine-tuning, we only fix the parameters of the language model for the first 10,000 steps. Pre-trained Transformer Recent empirical improvements with language models have showed that unsupervised pretraining (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Dai et al., 2019; Sun et al., 2019) on very large corpora is an integral part of many NLP tasks. We implement a big Transformer language model using PaddlePaddle1 , an end-to-end open source deep learning platform developed by Baidu. It provides a complete suite of deep learning libraries, tools and service platforms to make the research and development of deep learning simple and reliable. The language model is pre-trained only with masked language model task (Taylor, 1953; Devlin et al., 2018; Sun et al., 2019) on a monolingual corpus of the sourc"
W19-5341,P19-1285,0,0.0714852,"The Transformer model (Vaswani et al., 2017), which exploits self-attention mechanism both in the encoder and decoder, has significantly improved the translation quality in recent years. It is also adopted by most participants as the basic Neural Machine Translation (NMT) system in the previous translation campaigns (Bojar et al., 2018; Niehues et al., 2018). In this year’s translation task, we focus on the improvement of single system, and propose three novel Transformer variants: • Pre-trained Transformer: We train a big Transformer language model (Radford et al., 2018; Devlin et al., 2018; Dai et al., 2019; Sun et al., 2019) on monolingual corpora, and use the language model as the encoder of the Transformer model. 2 • Deeper Transformer: We increase the encoder layers to better learn the representation of the source sentences. Specifically, we increase the number of encoder layers from 6 to 30 for the base version, and from 6 to 15 layers for the big version. System Overview Figure 1 depicts the overall process of our submissions in this year’s evaluation task, in which we train our advanced Transformer models on the bilingual corpus together with synthetic corpora, fine-tune them on the well-"
W19-5341,P16-1009,0,0.0741956,"Missing"
W19-5341,D18-1458,0,0.0491168,"Missing"
W19-5341,W18-6429,0,0.0370671,"e also use an iterative approach (Zhang et al., 2018) to extend the back translation method by jointly training source-totarget and target-to-source NMT models. For bilingual data augmentation, a target-to-source baseline system is used to translate the target of the bilingual corpus as the synthetic data. Moreover, the sequence-level knowledge distillation (Hassan et al., 2018) mechanism is employed to boost the performance by means of using the model decoding from right to left (Right-to-Left) and the aforementioned Transformer variants to generate synthetic data for training the NMT model (Wang et al., 2018). The remainder of paper is structured as follows: Section 2 describes the detailed overview of our training strategy. Section 3 shows the experimental settings and results. Finally, we conclude our work in Section 4. Introduction The Transformer model (Vaswani et al., 2017), which exploits self-attention mechanism both in the encoder and decoder, has significantly improved the translation quality in recent years. It is also adopted by most participants as the basic Neural Machine Translation (NMT) system in the previous translation campaigns (Bojar et al., 2018; Niehues et al., 2018). In this"
wu-wang-2004-improving-domain,J96-1001,0,\N,Missing
wu-wang-2004-improving-domain,ahrenberg-etal-2000-evaluation,0,\N,Missing
wu-wang-2004-improving-domain,W02-1405,0,\N,Missing
wu-wang-2004-improving-domain,J93-2003,0,\N,Missing
wu-wang-2004-improving-domain,W01-1406,0,\N,Missing
wu-wang-2004-improving-domain,2001.mtsummit-ebmt.4,0,\N,Missing
wu-wang-2004-improving-domain,P03-1012,0,\N,Missing
wu-wang-2004-improving-domain,2001.mtsummit-papers.60,0,\N,Missing
wu-wang-2004-improving-domain,J97-2004,0,\N,Missing
wu-wang-2004-improving-domain,1996.amta-1.13,0,\N,Missing
wu-wang-2004-improving-domain,J97-3002,0,\N,Missing
wu-wang-2004-improving-domain,P98-1004,0,\N,Missing
wu-wang-2004-improving-domain,C98-1004,0,\N,Missing
wu-wang-2004-improving-domain,tufis-barbu-2002-lexical,0,\N,Missing
wu-wang-2004-improving-domain,P00-1056,0,\N,Missing
