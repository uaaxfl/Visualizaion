2021.findings-acl.408,Semi-Supervised Data Programming with Subset Selection,2021,-1,-1,4,1,8439,ayush maheshwari,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.findings-acl.436,Rule Augmented Unsupervised Constituency Parsing,2021,-1,-1,4,0,8505,atul sahay,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.findings-acl.447,Automatic Speech Recognition in {S}anskrit: A New Speech Corpus and Modelling Insights,2021,-1,-1,5,0,8536,devaraja adiga,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.eacl-main.247,Joint Learning of Hyperbolic Label Embeddings for Hierarchical Multi-label Classification,2021,-1,-1,3,0,10872,soumya chatterjee,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"We consider the problem of multi-label classification, where the labels lie on a hierarchy. However, unlike most existing works in hierarchical multi-label classification, we do not assume that the label-hierarchy is known. Encouraged by the recent success of hyperbolic embeddings in capturing hierarchical relations, we propose to jointly learn the classifier parameters as well as the label embeddings. Such a joint learning is expected to provide a twofold advantage: i) the classifier generalises better as it leverages the prior knowledge of existence of a hierarchy over the labels, and ii) in addition to the label co-occurrence information, the label-embedding may benefit from the manifold structure of the input datapoints, leading to embeddings that are more faithful to the label hierarchy. We propose a novel formulation for the joint learning and empirically evaluate its efficacy. The results show that the joint learning improves over the baseline that employs label co-occurrence based pre-trained hyperbolic embeddings. Moreover, the proposed classifiers achieve state-of-the-art generalization on standard benchmarks. We also present evaluation of the hyperbolic embeddings obtained by joint learning and show that they represent the hierarchy more accurately than the other alternatives."
2021.eacl-main.314,Meta-Learning for Effective Multi-task and Multilingual Modelling,2021,-1,-1,4,0,10984,ishan tarunesh,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Natural language processing (NLP) tasks (e.g. question-answering in English) benefit from knowledge of other tasks (e.g., named entity recognition in English) and knowledge of other languages (e.g., question-answering in Spanish). Such shared representations are typically learned in isolation, either across tasks or across languages. In this work, we propose a meta-learning approach to learn the interactions between both tasks and languages. We also investigate the role of different sampling strategies used during meta-learning. We present experiments on five different tasks and six different languages from the XTREME multilingual benchmark dataset. Our meta-learned model clearly improves in performance compared to competitive baseline models that also include multi-task baselines. We also present zero-shot evaluations on unseen target languages to demonstrate the utility of our proposed model."
2020.aacl-main.78,Vocabulary Matters: A Simple yet Effective Approach to Paragraph-level Question Generation,2020,-1,-1,3,1,3534,vishwajeet kumar,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"Question generation (QG) has recently attracted considerable attention. Most of the current neural models take as input only one or two sentences, and perform poorly when multiple sentences or complete paragraphs are given as input. However, in real-world scenarios it is very important to be able to generate high-quality questions from complete paragraphs. In this paper, we present a simple yet effective technique for answer-aware question generation from paragraphs. We augment a basic sequence-to-sequence QG model with dynamic, paragraph-specific dictionary and copy attention that is persistent across the corpus, without requiring features generated by sophisticated NLP pipelines or handcrafted rules. Our evaluation on SQuAD shows that our model significantly outperforms current state-of-the-art systems in question generation from paragraphs in both automatic and human evaluation. We achieve a 6-point improvement over the best system on BLEU-4, from 16.38 to 22.62."
P19-1481,Cross-Lingual Training for Automatic Question Generation,2019,25,4,4,1,3534,vishwajeet kumar,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Automatic question generation (QG) is a challenging problem in natural language understanding. QG systems are typically built assuming access to a large number of training instances where each instance is a question and its corresponding answer. For a new language, such training instances are hard to obtain making the QG problem even more challenging. Using this as our motivation, we study the reuse of an available large QG dataset in a secondary language (e.g. English) to learn a QG model for a primary language (e.g. Hindi) of interest. For the primary language, we assume access to a large amount of monolingual text but only a small QG dataset. We propose a cross-lingual QG model which uses the following training regime: (i) Unsupervised pretraining of language models in both primary and secondary languages and (ii) joint supervised training for QG in both languages. We demonstrate the efficacy of our proposed approach using two different primary languages, Hindi and Chinese. Our proposed framework clearly outperforms a number of baseline models, including a fully-supervised transformer-based model trained on the QG datasets in the primary language. We also create and release a new question answering dataset for Hindi consisting of 6555 sentences."
K19-1076,Putting the Horse before the Cart: A Generator-Evaluator Framework for Question Generation from Text,2019,0,2,2,1,3534,vishwajeet kumar,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"Automatic question generation (QG) is a useful yet challenging task in NLP. Recent neural network-based approaches represent the state-of-the-art in this task. In this work, we attempt to strengthen them significantly by adopting a holistic and novel generator-evaluator framework that directly optimizes objectives that reward semantics and structure. The \textit{generator} is a sequence-to-sequence model that incorporates the \textit{structure} and \textit{semantics} of the question being generated. The generator predicts an answer in the passage that the question can pivot on. Employing the copy and coverage mechanisms, it also acknowledges other contextually important (and possibly rare) keywords in the passage that the question needs to conform to, while not redundantly repeating words. The \textit{evaluator} model evaluates and assigns a reward to each predicted question based on its conformity to the \textit{structure} of ground-truth questions. We propose two novel QG-specific reward functions for text conformity and answer conformity of the generated question. The evaluator also employs structure-sensitive rewards based on evaluation measures such as BLEU, GLEU, and ROUGE-L, which are suitable for QG. In contrast, most of the previous works only optimize the cross-entropy loss, which can induce inconsistencies between training (objective) and testing (evaluation) measures. Our evaluation shows that our approach significantly outperforms state-of-the-art systems on the widely-used SQuAD benchmark as per both automatic and human evaluation."
D19-3030,{P}ara{QG}: A System for Generating Questions and Answers from Paragraphs,2019,0,0,3,1,3534,vishwajeet kumar,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations,0,"Generating syntactically and semantically valid and relevant questions from paragraphs is useful with many applications. Manual generation is a labour-intensive task, as it requires the reading, parsing and understanding of long passages of text. A number of question generation models based on sequence-to-sequence techniques have recently been proposed. Most of them generate questions from sentences only, and none of them is publicly available as an easy-to-use service. In this paper, we demonstrate ParaQG, a Web-based system for generating questions from sentences and paragraphs. ParaQG incorporates a number of novel functionalities to make the question generation process user-friendly. It provides an interactive interface for a user to select answers with visual insights on generation of questions. It also employs various faceted views to group similar questions as well as filtering techniques to eliminate unanswerable questions."
N18-5010,Entity Resolution and Location Disambiguation in the {A}ncient {H}indu Temples Domain using Web Data,2018,0,0,3,1,8439,ayush maheshwari,Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"We present a system for resolving entities and disambiguating locations based on publicly available web data in the domain of ancient Hindu Temples. Scarce, unstructured information poses a challenge to Entity Resolution(ER) and snippet ranking. Additionally, because the same set of entities may be associated with multiple locations, Location Disambiguation(LD) is a problem. The mentions and descriptions of temples exist in the order of hundreds of thousands, with such data generated by various users in various forms such as text (Wikipedia pages), videos (YouTube videos), blogs, etc. We demonstrate an integrated approach using a combination of grammar rules for parsing and unsupervised (clustering) algorithms to resolve entity and locations with high confidence. A demo of our system is accessible at \url{tinyurl.com/templedemos}. Our system is open source and available on GitHub."
W15-5934,An Approach to Collective Entity Linking,2015,0,0,5,1,36396,ashish kulkarni,Proceedings of the 12th International Conference on Natural Language Processing,0,None
P15-1054,Summarization of Multi-Document Topic Hierarchies using Submodular Mixtures,2015,41,8,3,1,37499,ramakrishna bairi,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We study the problem of summarizing DAG-structured topic hierarchies over a given set of documents. Example applications include automatically generating Wikipedia disambiguation pages for a set of articles, and generating candidate multi-labels for preparing machine learning datasets (e.g., for text classification, functional genomics, and image classification). Unlike previous work, which focuses on clustering the set of documents using the topic hierarchy as features, we directly pose the problem as a submodular optimization problem on a topic hierarchy using the documents as features. Desirable properties of the chosen topics include document coverage, specificity, topic diversity, and topic homogeneity, each of which, we show, is naturally modeled by a submodular function. Other information, provided say by unsupervised approaches such as LDA and its variants, can also be utilized by defining a submodular function that expresses coherence between the chosen topics and this information. We use a large-margin framework to learn convex mixtures over the set of submodular components. We empirically evaluate our method on the problem of automatically generating Wikipedia disambiguation pages using human generated clusterings as ground truth. We find that our framework improves upon several baselines according to a variety of standard evaluation metrics including the Jaccard Index, F1 score and NMI, and moreover, can be scaled to extremely large scale problems."
N15-1090,Optimizing Multivariate Performance Measures for Learning Relation Extraction Models,2015,19,2,3,0.428937,5932,gholamreza haffari,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
2015.mtsummit-users.20,A machine-assisted human translation system for technical documents,2015,10,0,4,1,3534,vishwajeet kumar,Proceedings of Machine Translation Summit XV: User Track,0,"Translation systems are known to benefit from the availability of a bilingual lexicon for a domain of interest. A system, aiming to build such a lexicon from source language corpus, often requires human assistance and is confronted by conflicting requirements of minimizing human translation effort while improving the translation quality. We present an approach that exploits redundancy in the source corpus and extracts recurring patterns which are: frequent, syntactically well-formed, and provide maximum corpus coverage. The patterns generalize over phrases and word types and our approach finds a succinct set of good patterns with high coverage. Our interactive system leverages these patterns in multiple iterations of translation and post-editing, thereby progressively generating a high quality bilingual lexicon."
gavankar-etal-2014-efficient,Efficient Reuse of Structured and Unstructured Resources for Ontology Population,2014,41,0,3,0,39495,chetana gavankar,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We study the problem of ontology population for a domain ontology and present solutions based on semi-automatic techniques. A domain ontology for an organization, often consists of classes whose instances are either specific to, or independent of the organization. E.g. in an academic domain ontology, classes like Professor, Department could be organization (university) specific, while Conference, Programming languages are organization independent. This distinction allows us to leverage data sources bothâwithin the organization and those in the Internet â to extract entities and populate an ontology. We propose techniques that build on those for open domain IE. Together with user input, we show through comprehensive evaluation, how these semi-automatic techniques achieve high precision. We experimented with the academic domain and built an ontology comprising of over 220 classes. Intranet documents from five universities formed our organization specific corpora and we used open domain knowledge bases like Wikipedia, Linked Open Data, and web pages from the Internet as the organization independent data sources. The populated ontology that we built for one of the universities comprised of over 75,000 instances. We adhere to the semantic web standards and tools and make the resources available in the OWL format. These could be useful for applications such as information extraction, text annotation, and information retrieval."
D14-1208,Noisy Or-based model for Relation Extraction using Distant Supervision,2014,10,9,3,1,10366,ajay nagesh,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Distant supervision, a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus, is an attractive approach for training relation extractors. Various models are proposed in recent literature to align the facts in the database to their mentions in the corpus. In this paper, we discuss and critically analyse a popular alignment strategy called the xe2x80x9cat least onexe2x80x9d heuristic. We provide a simple, yet effective relaxation to this strategy. We formulate the inference procedures in training as integer linear programming (ILP) problems and implement the relaxation to the xe2x80x9cat least one xe2x80x9d heuristic via a soft constraint in this formulation. Empirically, we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches."
S13-2037,{SATTY} : Word Sense Induction Application in Web Search Clustering,2013,7,3,4,0,41202,satyabrata behera,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,The aim of this paper is to perform Word Sense induction (WSI); which clusters web search results and produces a diversified list of search results. It describes the WSI system developed for Task 11 of SemEval - 2013. This paper implements the idea of monotone submodular function optimization using greedy algorithm.
I13-1087,Learning to Generate Diversified Query Interpretations using Biconvex Optimization,2013,15,1,3,1,37499,ramakrishna bairi,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"The wealth of information present in the World Wide Web has made internet search a de-facto medium for obtaining any required information. Users typically specify short and/or ambiguous queries and expect the answer to appear at the top. Hence, it can be extremely important to produce a diverse but relevant set of results in the precious top k positions. This calls for addressing two types of needs: (i) producing relevant results for queries that are often short and ambiguous and (ii) selecting a set of k diverse results to satisfy different classes of information needs. In this paper, we present a novel technique using a Biconvex optimization formulation as well as adaptations of existing techniques from other areas, for addressing these two problems simultaneously. We propose a graph based iterative method to choose diversified results. We evaluate these approaches on the QRU (Query Representation and Understanding) dataset used in SIGIR 2011 workshop as well as on the AMBIENT (Ambiguous Entities) dataset and present results on generating diversified query interpretations. We also compare these approaches against other online systems such as Surf Canyon, Carrot2, Exalead and DBpedia and empirically demonstrate that our system produces competitive results."
I13-1131,Structure Cognizant Pseudo Relevance Feedback,2013,14,2,4,1,19176,arjun atreya,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"We propose a structure cognizant framework for pseudo relevance feedback (PRF). This has an application, for example, in selecting expansion terms for general search from subsets such as Wikipedia, wherein documents typically have a minimally fixed set of fields, viz., Title, Body, Infobox and Categories. In existing approaches to PRF based expansion, weights of expansion terms do not depend on their field(s) of origin. This, we feel, is a weakness of current PRF approaches. We propose a per field EM formulation for finding the importance of the expansion terms, in line with traditional PRF. However, the final weight of an expansion term is found by weighting these importance based on whether the term belongs to the title, the body, the infobox or the category field(s). In our experiments with four languages, viz., English, Spanish, Finnish and Hindi, we find that this structure-aware PRF yields a 2% to 30% improvement in performance (MAP) over the vanilla PRF. We conduct ablation tests to evaluate the importance of various fields. As expected, results from these tests emphasize the importance of fields in the order of title, body, categories and infobox."
W12-5801,Effective Mentor Suggestion System for Online Collaboration Platform,2012,0,0,4,0,42046,advait raut,Proceedings of the Workshop on Speech and Language Processing Tools in Education,0,None
W12-5807,Enriching An Academic knowledge base using Linked Open Data,2012,16,2,4,0,39495,chetana gavankar,Proceedings of the Workshop on Speech and Language Processing Tools in Education,0,"In this paper we present work done towards populating a domain ontology using a public knowledge base like DBpedia. Using an academic ontology as our target we identify mappings between a subset of its predicates and those in DBpedia and other linked datasets. In the semantic web context, ontology mapping allows linking of independently developed ontologies and inter-operation of heterogeneous resources. Linked open data is an initiative in this direction. We populate our ontology by querying the linked open datasets for extracting instances from these resources. We show how these along with semantic web standards and tools enable us to populate the academic ontology. Resulting instances could then be used as seeds in spirit of the typical bootstrapping paradigm."
W12-5809,Content Bookmarking and Recommendation,2012,13,0,3,0,42055,ananth vyasarayamut,Proceedings of the Workshop on Speech and Language Processing Tools in Education,0,"Personalized services are increasingly becoming popular in the Internet. This work proposes a way of generating personalized content and simultaneously recommending users, web pages that, he/she might be interested in, based on his/her personalized content. In this work, we portray a system that not only helps the user in bookmarking the URL and snippets from the web page but also recommends web pages relevant to his/her interest. It applies a content-based filtering approach. We describe the details of the approach and implementation as well as address the challenges associated with it."
W12-5018,Building Multilingual Search Index using open source framework,2012,1,2,4,1,19176,arjun atreya,Proceedings of the 3rd Workshop on South and Southeast {A}sian Natural Language Processing,0,"This paper presents a comparison of open source search engine development frameworks in the context of their malleability for constructing multilingual search index. The comparison study reveals that none of these frameworks are designed for this task. This paper elicits the challenges involved in building a multilingual index. We also discuss policy decisions and the implementation changes made to an open source framework for building such an index. As a main contribution of this work, we propose an architecture that can be used for building multilingual index. It also lists some of the open research challenges involved."
W12-5020,Error tracking in search engine development,2012,1,1,4,0,42126,swapnil chaudhari,Proceedings of the 3rd Workshop on South and Southeast {A}sian Natural Language Processing,0,"In this paper, we describe a tool that allows one to track the output of every module of a search engine. The tool provides the ability to perform pseudo error-correction by allowing the user to modify these outputs or tune parameters of the modules to check for improvement of results. Often it is important to see if certain surface level changes can help in the improvement of the result quality. This is crucial since it saves the immediate need to make changes in the system in terms of resource updation or development efforts. We describe query processing pipeline in sufficient detail and then show the efficacy of our tool for an example in Marathi along with giving a thorough error analysis for the example considered. We believe this paper will establish that such a tool is of significant importance for instant detection and correction of errors along with giving the readers an idea on how to develop the same."
D12-1012,Towards Efficient Named-Entity Rule Induction for Customizability,2012,22,6,2,1,10366,ajay nagesh,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Generic rule-based systems for Information Extraction (IE) have been shown to work reasonably well out-of-the-box, and achieve state-of-the-art accuracy with further domain customization. However, it is generally recognized that manually building and customizing rules is a complex and labor intensive process. In this paper, we discuss an approach that facilitates the process of building customizable rules for Named-Entity Recognition (NER) tasks via rule induction, in the Annotation Query Language (AQL). Given a set of basic features and an annotated document collection, our goal is to generate an initial set of rules with reasonable accuracy, that are interpretable and thus can be easily refined by a human developer. We present an efficient rule induction process, modeled on a four-stage manual rule development process and present initial promising results with our system. We also propose a simple notion of extractor complexity as a first step to quantify the interpretability of an extractor, and study the effect of induction bias and customization of basic features on the accuracy and complexity of induced rules. We demonstrate through experiments that the induced rules have good accuracy and low complexity according to our complexity measure."
I08-2118,Learning Decision Lists with Known Rules for Text Mining,2008,20,3,3,0,48638,venkatesan chakravarthy,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"Many real-world systems for handling unstructured text data are rule-based. Examples of such systems are named entity annotators, information extraction systems, and text classifiers. In each of these applications, ordering rules into a decision list is an important issue. In this paper, we assume that a set of rules is given and study the problem (MaxDL) of ordering them into an optimal decision list with respect to a given training set. We formalize this problem and show that it is NP-Hard and cannot be approximated within any reasonable factors. We then propose some heuristic algorithms and conduct exhaustive experiments to evaluate their performance. In our experiments we also observe performance improvement over an existing decision list learning algorithm, by merely re-ordering the rules output by it."
S07-1099,{USP}-{IBM}-1 and {USP}-{IBM}-2: The {ILP}-based Systems for Lexical Sample {WSD} in {S}em{E}val-2007,2007,4,5,5,0,2509,lucia specia,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"We describe two systems participating of the English Lexical Sample task in SemEval-2007. The systems make use of Inductive Logic Programming for supervised learning in two different ways: (a) to build Word Sense Disambiguation (WSD) models from a rich set of background knowledge sources; and (b) to build interesting features from the same knowledge sources, which are then used by a standard model-builder for WSD, namely, Support Vector Machines. Both systems achieved comparable accuracy (0.851 and 0.857), which outperforms considerably the most frequent sense baseline (0.787)."
W06-1658,Entity Annotation based on Inverse Index Operations,2006,6,14,1,1,8442,ganesh ramakrishnan,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,Entity annotation involves attaching a label such as 'name' or 'organization' to a sequence of tokens in a document. All the current rule-based and machine learning-based approaches for this task operate at the document level. We present a new and generic approach to entity annotation which uses the inverse index typically created for rapid key-word based searching of a document collection. We define a set of operations on the inverse index that allows us to create annotations defined by cascading regular expressions. The entity annotations for an entire document corpus can be created purely of the index with no need to access the original documents. Experiments on two publicly available data sets show very significant performance improvements over the document-based annotators.
W04-0853,A gloss-centered algorithm for disambiguation,2004,5,7,1,1,8442,ganesh ramakrishnan,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,"The task of word sense disambiguation is to assign a sense label to a word in a passage. We report our algorithms and experiments for the two tasks that we participated in viz. the task of WSD of WordNet glosses and the task of WSD of English lexical sample. For both the tasks, we explore a method of sense disambiguation through a process of xe2x80x9ccomparingxe2x80x9d the current context for a word against a repository of contextual clues or glosses for each sense of each word. We compile these glosses in two different ways for the two tasks. For the first task, these glosses are all compiled using WordNet and are of various types viz. hypernymy glosses, holonymy mixture, descriptive glosses and some hybrid mixtures of these glosses. The xe2x80x9ccomparisonxe2x80x9d could be done in a variety of ways that could include/exclude stemming, expansion of one gloss type with another gloss type, etc. The results show that the system does best when stemming is used and glosses are expanded. However, it appears that the evidence for word-senses ,accumulated through WordNet, in the form of glosses, are quite sparse. Generating dense glosses for all WordNet senses requires a massive sense tagged corpus - which is currently unavailable. Hence, as part of the English lexical sample task, we try the same approach on densely populated glosses accumulated from the training data for this task."
bellare-etal-2004-generic,Generic Text Summarization Using {W}ord{N}et,2004,14,25,6,0,47301,kedar bellare,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper presents a WordNet based approach to text summarization. The document to be summarized is used to extract a xe2x80x9crelevantxe2x80x9d sub-graph from the WordNet graph. Weights are assigned to each node of this sub-graph using a strategy similar to the Google Pageranking algorithm. These weights capture the relevance of the respective synsets with respect to the whole document. A matrix in which each row repesents a sentence and each column a node of the sub-graph (i.e., a synset) is created. Principal Component Analysis is performed on this matrix to help extract the sentences for the summary. Our approach is generic unlike most previous approaches which address specific genres of documents like news articles and biographies. Testing our system on the standard DUC2002 extracts shows that our results are promising and comparable to existing summarizers."
W03-1201,Question Answering via {B}ayesian Inference on Lexical Relations,2003,13,45,1,1,8442,ganesh ramakrishnan,Proceedings of the {ACL} 2003 Workshop on Multilingual Summarization and Question Answering,0,"Many researchers have used lexical networks and ontologies to mitigate synonymy and polysemy problems in Question Answering (QA), systems coupled with taggers, query classifiers, and answer extractors in complex and ad-hoc ways. We seek to make QA systems reproducible with shared and modest human effort, carefully separating knowledge from algorithms. To this end, we propose an aesthetically clean Bayesian inference scheme for exploiting lexical relations for passage-scoring for QA. The factors which contribute to the efficacy of Bayesian Inferencing on lexical relations are soft word sense disambiguation, parameter smoothing which ameliorates the data sparsity problem and estimation of joint probability over words which overcomes the deficiency of naive-bayes-like approaches. Our system is superior to vector-space ranking techniques from IR, and its accuracy approaches that of the top contenders at the TREC QA tasks in recent years."
