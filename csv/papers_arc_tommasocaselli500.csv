2021.woah-1.3,{H}ate{BERT}: Retraining {BERT} for Abusive Language Detection in {E}nglish,2021,-1,-1,1,1,6,tommaso caselli,Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),0,"We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena."
2021.woah-1.6,{DALC}: the {D}utch Abusive Language Corpus,2021,-1,-1,1,1,6,tommaso caselli,Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),0,"As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually an- notated for abusive language. The resource ad- dress a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and .489 for target classification."
2021.nlp4posimpact-1.4,Guiding Principles for Participatory Design-inspired Natural Language Processing,2021,-1,-1,1,1,6,tommaso caselli,Proceedings of the 1st Workshop on NLP for Positive Impact,0,"We introduce 9 guiding principles to integrate Participatory Design (PD) methods in the development of Natural Language Processing (NLP) systems. The adoption of PD methods by NLP will help to alleviate issues concerning the development of more democratic, fairer, less-biased technologies to process natural language data. This short paper is the outcome of an ongoing dialogue between designers and NLP experts and adopts a non-standard format following previous work by Traum (2000); Bender (2013); Abzianidze and Bos (2019). Every section is a guiding principle. While principles 1{--}3 illustrate assumptions and methods that inform community-based PD practices, we used two fictional design scenarios (Encinas and Blythe, 2018), which build on top of situations familiar to the authors, to elicit the identification of the other 6. Principles 4{--}6 describes the impact of PD methods on the design of NLP systems, targeting two critical aspects: data collection {\&} annotation, and the deployment {\&} evaluation. Finally, principles 7{--}9 guide a new reflexivity of the NLP research with respect to its context, actors and participants, and aims. We hope this guide will offer inspiration and a road-map to develop a new generation of PD-inspired NLP."
2021.nlp4if-1.18,Fighting the {COVID}-19 Infodemic with a Holistic {BERT} Ensemble,2021,-1,-1,3,0,2898,georgios tziafas,"Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda",0,"This paper describes the TOKOFOU system, an ensemble model for misinformation detection tasks based on six different transformer-based pre-trained encoders, implemented in the context of the COVID-19 Infodemic Shared Task for English. We fine tune each model on each of the task{'}s questions and aggregate their prediction scores using a majority voting approach. TOKOFOU obtains an overall F1 score of 89.7{\%}, ranking first."
2021.nllp-1.5,A Multilingual Approach to Identify and Classify Exceptional Measures against {COVID}-19,2021,-1,-1,5,0,2898,georgios tziafas,Proceedings of the Natural Legal Language Processing Workshop 2021,0,"The COVID-19 pandemic has witnessed the implementations of exceptional measures by governments across the world to counteract its impact. This work presents the initial results of an on-going project, EXCEPTIUS, aiming to automatically identify, classify and com- pare exceptional measures against COVID-19 across 32 countries in Europe. To this goal, we created a corpus of legal documents with sentence-level annotations of eight different classes of exceptional measures that are im- plemented across these countries. We evalu- ated multiple multi-label classifiers on a manu- ally annotated corpus at sentence level. The XLM-RoBERTa model achieves highest per- formance on this multilingual multi-label clas- sification task, with a macro-average F1 score of 59.8{\%}."
2021.lchange-1.3,The Corpora They Are a-Changing: a Case Study in {I}talian Newspapers,2021,-1,-1,3,0,5405,pierpaolo basile,Proceedings of the 2nd International Workshop on Computational Approaches to Historical Language Change 2021,0,"The use of automatic methods for the study of lexical semantic change (LSC) has led to the creation of evaluation benchmarks. Benchmark datasets, however, are intimately tied to the corpus used for their creation questioning their reliability as well as the robustness of automatic methods. This contribution investigates these aspects showing the impact of unforeseen social and cultural dimensions. We also identify a set of additional issues (OCR quality, named entities) that impact the performance of the automatic methods, especially when used to discover LSC."
2021.findings-emnlp.56,"Fighting the {COVID}-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society",2021,-1,-1,13,0,1633,firoj alam,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"With the emergence of the COVID-19 pandemic, the political and the medical aspects of disinformation merged as the problem got elevated to a whole new level to become the first global infodemic. Fighting this infodemic has been declared one of the most important focus areas of the World Health Organization, with dangers ranging from promoting fake cures, rumors, and conspiracy theories to spreading xenophobia and panic. Addressing the issue requires solving a number of challenging problems such as identifying messages containing claims, determining their check-worthiness and factuality, and their potential to do harm as well as the nature of that harm, to mention just a few. To address this gap, we release a large dataset of 16K manually annotated tweets for fine-grained disinformation analysis that (i) focuses on COVID-19, (ii) combines the perspectives and the interests of journalists, fact-checkers, social media platforms, policy makers, and society, and (iii) covers Arabic, Bulgarian, Dutch, and English. Finally, we show strong evaluation results using pretrained Transformers, thus confirming the practical utility of the dataset in monolingual vs. multilingual, and single task vs. multitask settings."
2021.case-1.4,{PROTEST}-{ER}: Retraining {BERT} for Protest Event Extraction,2021,-1,-1,1,1,6,tommaso caselli,Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021),0,"We analyze the effect of further retraining BERT with different domain specific data as an unsupervised domain adaptation strategy for event extraction. Portability of event extraction models is particularly challenging, with large performance drops affecting data on the same text genres (e.g., news). We present PROTEST-ER, a retrained BERT model for protest event extraction. PROTEST-ER outperforms a corresponding generic BERT on out-of-domain data of 8.1 points. Our best performing models reach 51.91-46.39 F1 across both domains."
2020.semeval-1.202,{G}ru{P}a{T}o at {S}em{E}val-2020 Task 12: Retraining m{BERT} on Social Media and Fine-tuned Offensive Language Models,2020,-1,-1,2,0,15292,davide colla,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"We introduce an approach to multilingual Offensive Language Detection based on the mBERT transformer model. We download extra training data from Twitter in English, Danish, and Turkish, and use it to re-train the model. We then fine-tuned the model on the provided training data and, in some configurations, implement transfer learning approach exploiting the typological relatedness between English and Danish. Our systems obtained good results across the three languages (.9036 for EN, .7619 for DA, and .7789 for TR)."
2020.restup-1.4,"Lower Bias, Higher Density Abusive Language Datasets: A Recipe",2020,-1,-1,2,0,15629,juliet rosendaal,Proceedings of the Workshop on Resources and Techniques for User and Author Profiling in Abusive Language,0,"Datasets to train models for abusive language detection are at the same time necessary and still scarce. One the reasons for their limited availability is the cost of their creation. It is not only that manual annotation is expensive, it is also the case that the phenomenon is sparse, causing human annotators having to go through a large number of irrelevant examples in order to obtain some significant data. Strategies used until now to increase density of abusive language and obtain more meaningful data overall, include data filtering on the basis of pre-selected keywords and hate-rich sources of data. We suggest a recipe that at the same time can provide meaningful data with possibly higher density of abusive language and also reduce top-down biases imposed by corpus creators in the selection of the data to annotate. More specifically, we exploit the controversy channel on Reddit to obtain keywords that are used to filter a Twitter dataset. While the method needs further validation and refinement, our preliminary experiments show a higher density of abusive tweets in the filtered vs unfiltered dataset, and a more meaningful topic distribution after filtering."
2020.peoples-1.9,Topic and Emotion Development among {D}utch {COVID}-19 {T}witter Communities in the early Pandemic,2020,-1,-1,3,0,15738,boris marinov,"Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media",0,"The paper focuses on a large collection of Dutch tweets from the Netherlands to get an insight into the perception and reactions of users during the early months of the COVID-19 pandemic. We focused on five major user communities of users: government and health organizations, news media, politicians, the general public and conspiracy theory supporters, investigating differences among them in topic dominance and the expressions of emotions. Through topic modeling we monitor the evolution of the conversation about COVID-19 among these communities. Our results indicate that the national focus on COVID-19 shifted from the virus itself to its impact on the economy between February and April. Surprisingly, the overall emotional public response appears to be substantially positive and expressing trust, although differences can be observed in specific group of users."
2020.lrec-1.760,"{I} Feel Offended, Don{'}t Be Abusive! Implicit/Explicit Messages in Offensive and Abusive Language",2020,-1,-1,1,1,6,tommaso caselli,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Abusive language detection is an unsolved and challenging problem for the NLP community. Recent literature suggests various approaches to distinguish between different language phenomena (e.g., hate speech vs. cyberbullying vs. offensive language) and factors (degree of explicitness and target) that may help to classify different abusive language phenomena. There are data sets that annotate the target of abusive messages (i.e.OLID/OffensEval (Zampieri et al., 2019a)). However, there is a lack of data sets that take into account the degree of explicitness. In this paper, we propose annotation guidelines to distinguish between explicit and implicit abuse in English and apply them to OLID/OffensEval. The outcome is a newly created resource, AbuseEval v1.0, which aims to address some of the existing issues in the annotation of offensive and abusive language (e.g., explicitness of the message, presence of a target, need of context, and interaction across different phenomena)."
2020.lrec-1.769,Norm It! Lexical Normalization for {I}talian and Its Downstream Effects for Dependency Parsing,2020,0,0,3,0,3851,rob goot,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Lexical normalization is the task of translating non-standard social media data to a standard form. Previous work has shown that this is beneficial for many downstream tasks in multiple languages. However, for Italian, there is no benchmark available for lexical normalization, despite the presence of many benchmarks for other tasks involving social media data. In this paper, we discuss the creation of a lexical normalization dataset for Italian. After two rounds of annotation, a Cohen{'}s kappa score of 78.64 is obtained. During this process, we also analyze the inter-annotator agreement for this task, which is only rarely done on datasets for lexical normalization,and when it is reported, the analysis usually remains shallow. Furthermore, we utilize this dataset to train a lexical normalization model and show that it can be used to improve dependency parsing of social media data. All annotated data and the code to reproduce the results are available at: http://bitbucket.org/robvanderg/normit."
W18-4306,Crowdsourcing {S}tory{L}ines: Harnessing the Crowd for Causal Relation Annotation,2018,0,0,1,1,6,tommaso caselli,Proceedings of the Workshop Events and Stories in the News 2018,0,"This paper describes a crowdsourcing experiment on the annotation of plot-like structures in English news articles. CrowdThruth methodology and metrics have been applied to select valid annotations from the crowd. We further run an in-depth analysis of the annotated data by comparing them with available expert data. Our results show a valuable use of crowdsourcing annotations for such complex semantic tasks, and suggest a new annotation approach which combine crowd and experts."
L18-1051,Systems{'} Agreements and Disagreements in Temporal Processing: An Extensive Error Analysis of the {T}emp{E}val-3 Task,2018,0,0,1,1,6,tommaso caselli,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"In this article we review Temporal Processing systems that participated in the TempEval-3 task as a basis to develop our own system, that we also present and release. The system incorporates high level lexical semantic features, obtaining the best scores for event detection (F1-Class 72.24) and second best result for temporal relation classification from raw text (F1 29.69) when evaluated on the TempEval-3 data. Additionally, we analyse the errors of all TempEval-3 systems for which the output is publicly available with the purpose of finding out what are the weaknesses of current approaches. Although incorporating lexical semantics features increases the performance of our system, the error analysis shows that systems should incorporate inference mechanisms and world knowledge, as well as having strategies to compensate for data skewness."
L18-1725,The Circumstantial Event Ontology ({CEO}) and {ECB}+/{CEO}: an Ontology and Corpus for Implicit Causal Relations between Events,2018,0,0,2,0.769231,30052,roxane segers,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"In this paper, we describe the Circumstantial Event Ontology (CEO), a newly developed ontology for calamity events that models semantic circumstantial relations between event classes, where we define circumstantial as inferred implicit causal relations. The circumstantial relations are inferred from the assertions of the event classes that involve a change to the same property of a participant. Our model captures that the change yielded by one event, explains to people the happening of the next event when observed. We describe the meta model and the contents of the ontology, the creation of a manually annotated corpus for circumstantial relations based on ECB and the first results on the evaluation of the ontology."
W17-2706,The Circumstantial Event Ontology ({CEO}),2017,10,1,2,0.769231,30052,roxane segers,Proceedings of the Events and Stories in the News Workshop,0,"In this paper we describe the ongoing work on the Circumstantial Event Ontology (CEO), a newly developed ontology for calamity events that models semantic circumstantial relations between event classes. The circumstantial relations are designed manually, based on the shared properties of each event class. We discuss and contrast two types of event circumstantial relations: semantic circumstantial relations and episodic circumstantial relations. Further, we show the metamodel and the current contents of the ontology and outline the evaluation of the CEO."
W17-2711,The Event {S}tory{L}ine Corpus: A New Benchmark for Causal and Temporal Relation Extraction,2017,0,7,1,1,6,tommaso caselli,Proceedings of the Events and Stories in the News Workshop,0,"This paper reports on the Event StoryLine Corpus (ESC) v1.0, a new benchmark dataset for the temporal and causal relation detection. By developing this dataset, we also introduce a new task, the StoryLine Extraction from news data, which aims at extracting and classifying events relevant for stories, from across news documents spread in time and clustered around a single seminal event or topic. In addition to describing the dataset, we also report on three baselines systems whose results show the complexity of the task and suggest directions for the development of more robust systems."
E17-2042,The Content Types Dataset: a New Resource to Explore Semantic and Functional Characteristics of Texts,2017,0,1,2,0,16573,rachele sprugnoli,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"This paper presents a new resource, called Content Types Dataset, to promote the analysis of texts as a composition of units with specific semantic and functional roles. By developing this dataset, we also introduce a new NLP task for the automatic classification of Content Types. The annotation scheme and the dataset are described together with two sets of classification experiments."
W16-5708,The Storyline Annotation and Representation Scheme ({S}ta{R}): A Proposal,2016,-1,-1,1,1,6,tommaso caselli,Proceedings of the 2nd Workshop on Computing News Storylines ({CNS} 2016),0,None
W16-2819,Unshared Task at the 3rd Workshop on Argument Mining: Perspective Based Local Agreement and Disagreement in Online Debate,2016,7,0,2,0,17878,chantal son,Proceedings of the Third Workshop on Argument Mining ({A}rg{M}ining2016),0,This paper proposes a new task in argument mining in online debates. The task includes three annotations steps that result in fine-grained annotations of agreement and disagreement at a propositional level. We report on the results of a pilot annotation task on identifying sentences that are directly addressed in the comment.
S16-1193,{VUACLTL} at {S}em{E}val 2016 Task 12: A {CRF} Pipeline to Clinical {T}emp{E}val,2016,14,3,1,1,6,tommaso caselli,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"This paper describes VUACLTL, the system the CLTL Lab submitted to the SemEval 2016 Task Clinical TempEval. The system is based on a purely data-driven approach based on a cascade of seven CRF classifiers which use generic features and little domain knowledge. The challenge consisted in six subtasks related to temporal processing clinical notes from raw text (event and temporal expression detection and attribute classification, temporal relation classification between events and the Document Creation Time, and narrative container detection). The system was initially developed to process newswire texts and then re-trained to process clinical notes. This had an impact on the results, which are not equally competitive for all the subtasks."
L16-1063,{NLP} and Public Engagement: The Case of the {I}talian School Reform,2016,8,0,1,1,6,tommaso caselli,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper we present PIERINO (PIattaforma per l{'}Estrazione e il Recupero di INformazione Online), a system that was implemented in collaboration with the Italian Ministry of Education, University and Research to analyse the citizens{'} comments given in {\#}labuonascuola survey. The platform includes various levels of automatic analysis such as key-concept extraction and word co-occurrences. Each analysis is displayed through an intuitive view using different types of visualizations, for example radar charts and sunburst. PIERINO was effectively used to support shaping the last Italian school reform, proving the potential of NLP in the context of policy making."
L16-1187,{GR}a{SP}: A Multilayered Annotation Scheme for Perspectives,2016,17,3,2,0,17878,chantal son,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents a framework and methodology for the annotation of perspectives in text. In the last decade, different aspects of linguistic encoding of perspectives have been targeted as separated phenomena through different annotation initiatives. We propose an annotation scheme that integrates these different phenomena. We use a multilayered annotation approach, splitting the annotation of different aspects of perspectives into small subsequent subtasks in order to reduce the complexity of the task and to better monitor interactions between layers. Currently, we have included four layers of perspective annotation: events, attribution, factuality and opinion. The annotations are integrated in a formal model called GRaSP, which provides the means to represent instances (e.g. events, entities) and propositions in the (real or assumed) world in relation to their mentions in text. Then, the relation between the source and target of a perspective is characterized by means of perspective annotations. This enables us to place alternative perspectives on the same entity, event or proposition next to each other."
L16-1557,Temporal Information Annotation: Crowd vs. Experts,2016,16,2,1,1,6,tommaso caselli,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper describes two sets of crowdsourcing experiments on temporal information annotation conducted on two languages, i.e., English and Italian. The first experiment, launched on the CrowdFlower platform, was aimed at classifying temporal relations given target entities. The second one, relying on the CrowdTruth metric, consisted in two subtasks: one devoted to the recognition of events and temporal expressions and one to the detection and classification of temporal relations. The outcomes of the experiments suggest a valuable use of crowdsourcing annotations also for a complex task like Temporal Processing."
L16-1625,Crowdsourcing Salient Information from News and Tweets,2016,0,3,2,0,28182,oana inel,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The increasing streams of information pose challenges to both humans and machines. On the one hand, humans need to identify relevant information and consume only the information that lies at their interests. On the other hand, machines need to understand the information that is published in online data streams and generate concise and meaningful overviews. We consider events as prime factors to query for information and generate meaningful context. The focus of this paper is to acquire empirical insights for identifying salience features in tweets and news about a target event, i.e., the event of {``}whaling{''}. We first derive a methodology to identify such features by building up a knowledge space of the event enriched with relevant phrases, sentiments and ranked by their novelty. We applied this methodology on tweets and we have performed preliminary work towards adapting it to news articles. Our results show that crowdsourcing text relevance, sentiments and novelty (1) can be a main step in identifying salient information, and (2) provides a deeper and more precise understanding of the data at hand compared to state-of-the-art approaches."
W15-4507,Storylines for structuring massive streams of news,2015,16,16,2,0,5469,piek vossen,Proceedings of the First Workshop on Computing News Storylines,0,Stories are the most natural ways for people to deal with information about the changing world. They provide an efficient schematic structure to order and relate events according to some explanation. We describe (1) a formal model for representing storylines to handle streams of news and (2) a first implementation of a system that automatically extracts the ingredients of a storyline from news articles according to the model. Our model mimics the basic notions from narratology by adding bridging relations to timelines of events in relation to a climax point. We provide a method for defining the climax score of each event and the bridging relations between them. We generate a JSON structure for any set of news articles to represent the different stories they contain and visualize these stories on a timeline with climax and bridging relations. This visualization helps inspecting the validity of the generated structures.
S15-2077,{S}em{E}val-2015 Task 9: {CLIPE}val Implicit Polarity of Events,2015,16,11,2,0.833333,1876,irene russo,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"Sentiment analysis tends to focus on the polarity of words, combining their values to detect which portion of a text is opinionated. CLIPEval wants to promote a more holistic approach, looking at psychological researches that frame the connotations of words as the emotional values activated by them. The implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences annotated as instantiations of pleasant and unpleasant events previously collected in psychological research as the ones on which human judgments converge."
S15-2133,{SPINOZA}{\\_}{VU}: An {NLP} Pipeline for Cross Document {T}ime{L}ines,2015,13,8,1,1,6,tommaso caselli,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes the system SPINOZA VU developed for the SemEval 2015 Task 4: Cross Document TimeLines. The system integrates output from the NewsReader Natural Language Processing pipeline and is designed following an entity based model. The poor performance of the submitted runs are mainly a consequence of error propagation. Nevertheless, the error analysis has shown that the interpretation module behind the system performs correctly. An out of competition version of the system has fixed some errors and obtained competitive results. Therefore, we consider the system an important step towards a more complex task such as storyline extraction."
W14-0140,Aligning an {I}talian {W}ord{N}et with a Lexicographic Dictionary: Coping with limited data,2014,18,2,1,1,6,tommaso caselli,Proceedings of the Seventh Global {W}ordnet Conference,0,"This work describes the evaluations of two approaches, Lexical Matching and Sense Similarity, for word sense alignment between MultiWordNet and a lexicographic dictionary, Senso Comune De Mauro, when having few sense descriptions (MultiWordNet) and no structure over senses (Senso Comune De Mauro). The results obtained from the merging of the two approaches are satisfying, with F1 values of 0.47 for verbs and 0.64 for nouns."
S14-2046,{FBK}-{TR}: Applying {SVM} with Multiple Linguistic Features for Cross-Level Semantic Similarity,2014,15,4,2,0,19497,ngoc vo,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"Recently, the task of measuring semantic similarity between given texts has drawn much attention from the Natural Language Processing community. Especially, the task becomes more interesting when it comes to measuring the semantic similarity between different-sized texts, e.g paragraph-sentence, sentence-phrase, phrase-word, etc. In this paper, we, the FBK-TR team, describe our system participating in Task 3 Cross-Level Semantic Similarity, at SemEval 2014. We also report the results obtained by our system, compared to the baseline and other participating systems in this task."
S14-2047,{FBK}-{TR}: {SVM} for Semantic Relatedeness and Corpus Patterns for {RTE},2014,11,5,3,0,19497,ngoc vo,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This paper reports the description and scores of our system, FBK-TR, which participated at the SemEval 2014 task #1 Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Entailment. The system consists of two parts: one for computing semantic relatedness, based on SVM, and the other for identifying the entailment values on the basis of both semantic relatedness scores and entailment patterns based on verb-specific semantic frames. The system ranked 11 th on both tasks with competitive results."
caselli-etal-2014-enriching,Enriching the {``}Senso Comune{''} Platform with Automatically Acquired Data,2014,26,0,1,1,6,tommaso caselli,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper reports on research activities on automatic methods for the enrichment of the Senso Comune platform. At this stage of development, we will report on two tasks, namely word sense alignment with MultiWordNet and automatic acquisition of Verb Shallow Frames from sense annotated data in the MultiSemCor corpus. The results obtained are satisfying. We achieved a final F-measure of 0.64 for noun sense alignment and a F-measure of 0.47 for verb sense alignment, and an accuracy of 68{\textbackslash}{\%} on the acquisition of Verb Shallow Frames."
D14-1046,Automatic Domain Assignment for Word Sense Alignment,2014,17,0,1,1,6,tommaso caselli,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"This paper reports on the development of a hybrid and simple method based on a machine learning classifier (Naive Bayes), Word Sense Disambiguation and rules, for the automatic assignment of WordNet Domains to nominal entries of a lexicographic dictionary, the Senso Comune De Mauro Lexicon. The system obtained an F1 score of 0.58, with a Precision of 0.70. We further used the automatically assigned domains to filter out word sense alignments between MultiWordNet and Senso Comune. This has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments."
W13-5406,From Glosses to Qualia: Qualia Extraction from Senso Comune,2013,26,2,1,1,6,tommaso caselli,Proceedings of the 6th International Conference on Generative Approaches to the Lexicon ({GL}2013),0,None
W13-3816,Aligning Verb Senses in Two {I}talian Lexical Semantic Resources,2013,19,1,1,1,6,tommaso caselli,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,"This work describes the evaluations of three different approaches, Lexical Match, Sense Similarity based on Personalized Page Rank, and Semantic Match based on Shallow Frame Structures, for word sense alignment of verbs between two Italian lexical-semantic resources, MultiWordNet and the Senso Comune Lexicon. The results obtained are quite satisfying with a final F1 score of 0.47 when merging together Lexical Match and Sense Similarity."
caselli-etal-2012-customizable,Customizable {SCF} Acquisition in {I}talian,2012,11,1,1,1,6,tommaso caselli,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Lexica of predicate-argument structures constitute a useful tool for several tasks in NLP. This paper describes a web-service system for automatic acquisition of verb subcategorization frames (SCFs) from parsed data in Italian. The system acquires SCFs in an unsupervised manner. We created two gold standards for the evaluation of the system, the first by mixing together information from two lexica (one manually created and the second automatically acquired) and manual exploration of corpus data and the other annotating data extracted from a specialized corpus (environmental domain). Data filtering is accomplished by means of the maximum likelihood estimate (MLE). The evaluation phase has allowed us to identify the best empirical MLE threshold for the creation of a lexicon (P=0.653, R=0.557, F1=0.601). In addition to this, we assigned to the extracted entries of the lexicon a confidence score based on the relative frequency and evaluated the extractor on domain specific data. The confidence score will allow the final user to easily select the entries of the lexicon in terms of their reliability: one of the most interesting feature of this work is the possibility the final users have to customize the results of the SCF extractor, obtaining different SCF lexica in terms of size and accuracy."
caselli-etal-2012-assigning,Assigning Connotation Values to Events,2012,17,2,1,1,6,tommaso caselli,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Sentiment Analysis (SA) and Opinion Mining (OM) have become a popular task in recent years in NLP with the development of language resources, corpora and annotation schemes. The possibility to discriminate between objective and subjective expressions contributes to the identification of a document's semantic orientation and to the detection of the opinions and sentiments expressed by the authors or attributed to other participants in the document. Subjectivity word sense disambiguation helps in this task, automatically determining which word senses in a corpus are being used subjectively and which are being used objectively. This paper reports on a methodology to assign in a semi-automatic way connotative values to eventive nouns usually labelled as neutral through syntagmatic patterns that express cause-effect relations between emotion cause events and emotion words. We have applied our method to nouns and we have been able reduce the number of OBJ polarity values associated to event noun."
C12-2121,Sourcing the Crowd for a Few Good Ones: Event Type Detection,2012,28,2,1,1,6,tommaso caselli,Proceedings of {COLING} 2012: Posters,0,"This paper reports a crowdsourcing experiment on the identification and classification of event types in Italian. The data collected show that the task is not trivial (360 trusted judgments collected vs. 475 untrsuted ones) but it has been shown to be linguistically felicitous. The overall accuracy of the annotation is 61.6%. A reliability threshold assigned to the workers allows us to indentify the sub-population who has the awareness to perform this complex task and the accuracy of this sub-population is raised to 93%. Our hypothesis is that although the initial crowdsourced data is necessarily noisy, it can yield high quality results if the sub-population of xe2x80x98goodxe2x80x99 workers can be identified. In other words, crowdsourcing offers a solution to difficult annotation tasks as long as there is an effective way to identify the reliable workers. TITLE AND ABSTRACT IN ANOTHER LANGUAGE, L2 (OPTIONAL, AND ON SAME PAGE) Identificare Annotatori Affidabili: Riconoscimento di Tipi di Evento Questo articolo descrive un esperimento di crowdsourcing per il riconoscimento e la classificazione dei tipi di evento in Italiano. I dati raccolti mostrano che il compito non e banale (360 giudizi affidabili vs. 475 giudizi non affidabili), ma dimostra di essere linguisticamente xe2x80x9cfelicexe2x80x9d. Lxe2x80x99accuratezza globale della annotazione e del 61,6%. Una soglia di affidabilita assegnata ai lavoratori ci permette di identificare la sotto-popolazione che ha la consapevolezza di svolgere questo compito complesso la cui accuratezza arriva fino al 93%. La nostra ipotesi e che, sebbene i dati iniziali ottenuti tramite tecniche di crowdsourcing siano necessariamente rumorosi, dei risultati di buona qualita possono essere ottenuti se la sotto-popolazione di buoni lavoratori e identificabile. In altre parole, il crowdsourcing offre una soluzione per compiti di annotazione difficili finche vi e un modo efficace per identificare i lavoratori affidabili."
W11-1720,{EMOC}ause: An Easy-adaptable Approach to Extract Emotion Cause Contexts,2011,8,3,2,0.833333,1876,irene russo,Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis ({WASSA} 2.011),0,None
W11-0418,"Annotating Events, Temporal Expressions and Relations in {I}talian: the It-Timeml Experience for the Ita-{T}ime{B}ank",2011,15,23,1,1,6,tommaso caselli,Proceedings of the 5th Linguistic Annotation Workshop,0,"This paper presents the annotation guidelines and specifications which have been developed for the creation of the Italian TimeBank, a language resource composed of two corpora manually annotated with temporal and event information. In particular, the adaptation of the TimeML scheme to Italian is described, and a special attention is given to the methodology used for the realization of the annotation specifications, which are strategic in order to create good quality annotated resources and to justify the annotated items. The reliability of the It-TimeML guidelines and specifications is evaluated on the basis of the results of the inter-coder agreement performed during the annotation of the two corpora."
R11-1074,Data-Driven Approach Using Semantics for Recognizing and Classifying {T}ime{ML} Events in {I}talian,2011,11,4,1,1,6,tommaso caselli,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"We present a data-driven approach for recognizing and classifying TimeML events in Italian. A high-performance stateof-the-art approach, TIPSem, is adopted and extended with Italian-specific semantic features from a lexical resource. The resulting approach has been evaluated over the official TempEval2 Italian test data. The analysis of the results shows a positive impact of the semantic features both for event recognition and classification. Moreover, the presented data-driven approach has been compared with an existing rule-based prototype over the same data set. The results are directly comparable and show that the machine learning strategy better deals with the complexity of the tasks."
S10-1010,{S}em{E}val-2010 Task 13: {T}emp{E}val-2,2010,7,220,3,0,3193,marc verhagen,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"Tempeval-2 comprises evaluation tasks for time expressions, events and temporal relations, the latter of which was split up in four sub tasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier. Manually annotated data were provided for six languages: Chinese, English, French, Italian, Korean and Spanish."
caselli-prodanof-2010-annotating,Annotating Event Anaphora: A Case Study,2010,6,2,1,1,6,tommaso caselli,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In recent years we have resgitered a renewed interest in event detection and temporal processing of text/discourse. TimeML (Pustejovsky et al., 2003a) has shed new lights on the notion of event and developed a new methodology for its annotation. On a parallel, works on anaphora resolution have developed a reliable methodology for the annotation and pointed out the core role of this phenomenon for the improvement of NLP systems. This paper tries to put together these two lines of research by describing a case study for the creation of an annotation scheme on event anaphora. We claim that this work could have consequences for the annotation of eventualities as proposed in TimeML and on the use of the tag and on the study of anaphora and its annotation. The annotation scheme and its guidelines have been developed on the basis of a coarse grained bottom up approach. In order to do this, we have performed a small sampling annotation which has highlighted shortcomings and open issues which need to be resolved."
caselli-etal-2008-bilingual,A Bilingual Corpus of Inter-linked Events,2008,6,2,1,1,6,tommaso caselli,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper describes the creation of a bilingual corpus of inter-linked events for Italian and English. Linkage is accomplished through the Inter-Lingual Index (ILI) that links ItalWordNet with WordNet. The availability of this resource, on the one hand, enables contrastive analysis of the linguistic phenomena surrounding events in both languages, and on the other hand, can be used to perform multilingual temporal analysis of texts. In addition to describing the methodology for construction of the inter-linked corpus and the analysis of the data collected, we demonstrate that the ILI could potentially be used to bootstrap the creation of comparable corpora by exporting layers of annotation for words that have the same sense."
del-gratta-etal-2008-ufra,{UFRA}: a {UIMA}-based Approach to Federated Language Resource Architecture,2008,9,0,3,0,29721,riccardo gratta,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this paper we address the issue of developing an interoperable infrastructure for language resources and technologies. In our approach, called UFRA, we extend the Federate Database Architecture System adding typical functionalities caming from UIMA. In this way, we capitalize the advantages of a federated architecture, such as autonomy, heterogeneity and distribution of components, monitored by a central authority responsible for checking both the integration of components and user rights on performing different tasks. We use the UIMA approach to manage and define one common front-end, enabling users and clients to query, retrieve and use language resources and technologies. The purpose of this paper is to show how UIMA leads from a Federated Database Architecture to a Federated Resource Architecture, adding to a registry of available components both static resources such as lexicons and corpora and dynamic ones such as tools and general purpose language technologies. At the end of the paper, we present a case-study that adopts this framework to integrate the SIMPLE lexicon and TIMEML annotation guidelines to tag natural language texts."
W07-1606,Inferring the Semantics of Temporal Prepositions in {I}talian,2007,11,1,1,1,6,tommaso caselli,Proceedings of the Fourth {ACL}-{SIGSEM} Workshop on Prepositions,0,"In this work we report on the results of a preliminary corpus study of Italian on the semantics of temporal prepositions, which is part of a wider project on the automatic recognition of temporal relations. The corpus data collected supports our hypothesis that each temporal preposition can be associated with one prototypical temporal relation, and that deviations from the prototype can be explained as determined by the occurrence of different semantic patterns. The motivation behind this approach is to improve methods for temporal annotation of texts for content based access to information. The corpus study described in this paper led to the development of a preliminary set of heuristics for automatic annotation of temporal relations in text/discourse."
caselli-prodanof-2006-annotating,Annotating Bridging Anaphors in {I}talian: in Search of Reliability,2006,7,4,1,1,6,tommaso caselli,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"The aim of this work is the presentation and preliminary evaluation of an XML annotation scheme for marking bridging anaphors of the form Âdefinite article + NÂ in Italian. The scheme is based on a corpus-study. The data we collected from the evaluation experiment seem to support the reliability of the scheme, although some problems still remain open."
