2005.mtsummit-posters.21,2003.mtsummit-systems.15,0,0.227061,"Missing"
2005.mtsummit-posters.21,2003.mtsummit-papers.19,0,0.0490042,"Missing"
2005.mtsummit-posters.8,2003.mtsummit-systems.15,0,0.0440617,"amese. An account of the structural and categorical divergence problem for English and Hindi have been given in (Gupta and Chatterjee, 2003). They have proposed an algorithm for identification of this divergence for an English-Hindi EBMT system. This identification helps in partitioning the example database into divergence / non-divergence categories, which in turn should facilitate efficient retrieval and adaptation in an EBMT system. The development of an English to Hindi MachineAided Translation System named AnglaHindi based on the ANGLABHARTI translation methodology has been described in (Sinha and Jain, 2003). AnglaBharti is a pattern directed rule based system with context free grammar like structure for analysis of English as source language. It analyses English sentences and creates an intermediate structure that has the word and word-group order as per the structure of the group of target languages. The intermediate structure is then converted to final translation in the target language through a process of text-generation. The next three sections 2, 3 and 4 describe the English NPs, VPs and PPs and how these phrases are translated in Bengali. Section 5 discusses about the phrase ordering rule"
2005.mtsummit-posters.8,2003.mtsummit-papers.19,0,0.0258981,"phrase may refer to a plural entity, it does not have a plural ending, e.g., anek-jon sadasso ‘many members’, du-ti boi ‘two books’. An NP in English also includes pronouns and adjectives and in the next two subsections (2.1 and 2.2) we discuss how such words are handled in English and Bengali. Subsection 2.3 includes a snapshot of the phrasal example base for NPs that encode the NP transfer rules. Vietnamese bilingual corpus in order to transfer word orders from English into Vietnamese. An account of the structural and categorical divergence problem for English and Hindi have been given in (Gupta and Chatterjee, 2003). They have proposed an algorithm for identification of this divergence for an English-Hindi EBMT system. This identification helps in partitioning the example database into divergence / non-divergence categories, which in turn should facilitate efficient retrieval and adaptation in an EBMT system. The development of an English to Hindi MachineAided Translation System named AnglaHindi based on the ANGLABHARTI translation methodology has been described in (Sinha and Jain, 2003). AnglaBharti is a pattern directed rule based system with context free grammar like structure for analysis of English"
2005.mtsummit-posters.8,P02-1040,0,0.0774275,"Missing"
2005.mtsummit-posters.8,1992.tmi-1.12,0,0.498967,"n, number information of the subject and tense and aspect information of the verb. But for any particular root verb, there are only a few verb forms in English, whereas in Bengali it shows a lot of variations. The structural divergence between English and Bengali suggests that a Phrasal EBMT system would suit best for translating English to Bengali. In the present system, grammar examples expressed as context-sensitive rewrite rules (using semantic features) are stored in a phrasal example base. Some references on Example Based Machine Translation (EBMT) strategy can be found in (Nagao, 1984; Furuse and Iida, 1992; Somers, 2000; Carl and Andy, 2003). A hybrid approach based on fixed rules (rulebased) and stochastic methods (corpus-based) which extracts word order transfer rules between two languages has been presented in (Dien et. al., 2003). They have used this approach for translating from English to Vietnamese. They have used the Transfer-based Learning (TBL) machine learning approach for learning transfer rules from EnglishAbstract The present work describes a Phrasal Example Based Machine Translation system from English to Bengali that identifies the phrases in the input through a shallow analysis"
2009.eamt-1.32,P07-1005,0,0.0290351,"(Vickrey et al., 2005) built classifiers inspired by those used in word-sense disambiguation (WSD) to fill in any blanks in a partially completed translation. (Giménez and Màrquez, 2007) extended this work by considering the slightly more general case of very frequent phrases and moved to full translation rather than blank-filling on the target side. Initial attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007) of integrating state-of-the-art WSD methods into SMT to improve the overall translation quality have met with more success. Language models arguably play the most significant role in today’s PB-SMT systems. It is obvious that a straightforward addition of a source language model will make no contribution as this will be cancelled out by the denominator in the noisy-channel model of SMT. However, for some time now the feeling was that some incorporation of source language information into SMT systems had to help. (Stroppa et al., 2007) added source-side contextual f"
2009.eamt-1.32,J07-2003,0,0.0231926,"ch. Using the same sorts of local contextual features as (Stroppa et al., 2007), as well as using broader context in addition to grammatical dependency information, (Max et al., 2008) show modest gains over a PB-SMT baseline model in terms of automatic evaluation scores, but more improvements come to light in a manual investigation. One final paper in this strand of research is that of (He et al., 2008), who despite not mentioning the obvious link between the two pieces of work, show that the source language features used by (Stroppa et al., 2007) are also of benefit when used with the Hiero (Chiang, 2007) decoder. As regards supertagged models of translation, (Hassan et al., 2006, 2007b, 2008; Hassan, 2009) have demonstrated clearly that adding supertags (essentially, part-of-speech tags of words plus local subcategorisation requirements) in the target language model and on the target side of the translation model improve state-of-the-art PBSMT systems. The system of (Hassan et al., 2007a) was ranked first according to human evaluators on the IWSLT 2007 Arabic–English task, despite the improvements in system design not being shown to their best advantage by the automatic evaluation metrics. Mo"
2009.eamt-1.32,C04-1041,0,0.00701127,"onference of the EAMT, pages 234–241, Barcelona, May 2009 234 that the word projects upwards. Like (Hassan et al., 2006, 2007, 2008; Hassan, 2009), in this work we employ the lexical entries but exchange the algebraic combinatory operators with the more robust and efficient supertagging approach: like standard taggers, supertaggers employ probabilities based on local context and can be implemented using finite state technology, e.g. Hidden Markov Models (Bangalore and Joshi, 1999). There are currently two supertagging approaches available: LTAG-based (Bangalore and Joshi, 1999) and CCG-based (Clark and Curran, 2004). Both the LTAG (Chen et al., 2006) and the CCG supertag sets (Hockenmaier, 2003) were acquired from the WSJ section of the PennII Treebank using hand-built extraction rules. Here we test both the LTAG and CCG supertaggers. We extract the supertagged components of context words (±1/±2) along with the source phrase (Koehn et al., 2003) in a standard PBSMT system. We use a memory-based classification approach to obtain the probability for the given additional contexts with the source phrase. In this paper we discuss these and other empirical issues. The remainder of the paper is organized as fol"
2009.eamt-1.32,W06-1628,0,0.0207219,"orating context-dependent phrasal translation probabilities learned using decision trees. They considered up to two words and/or POS tags on either side of the source focus word as contextual features. In order to overcome problems of estimation of such features, they used a decision-tree classifier which implicitly smoothes the probability estimates. Significant improvements over a baseline state-of-the-art PB-SMT system were obtained on Italian—English and Chinese— English IWSLT tasks. Unlike other recent proposals to exploit the accuracy and the flexibility of discriminative learning (e.g. Cowan et al., 2006; Liang et al., 2006), the strength of the approach of (Stroppa et al., 2007) is that no redefinition of one’s training procedures is required. Like the work of (Max et al., 2008), the present work is directly motivated by and an extension of the approach of (Stroppa et al., 2007). 235 The work of both (Max et al., 2008) and (Gimpel and Smith, 2008) focus on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for EnglishGerman, (Max et al., 2008) conduct experiments from EnglishFrench. Using the same sorts of local contextual fea"
2009.eamt-1.32,W06-1607,0,0.0145345,"sted reader to (Stroppa et al., 2007) for more details of how MemoryBased Learning (MBL) is used for classification of source examples for use in the log-linear MT framework. Memory-Based Classification As (Stroppa et al., 2007) point out, directly estimating P( eˆk |fˆk , CI( fˆk )) using relative frequencies (say) is problematic. Indeed, Zens and Ney (2004) showed that the estimation of P( eˆk |fˆk ) using relative frequencies results in the overestimation of the probabilities of long phrases, so smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of context informed features, since the context is also Experimental Set-Up 6.1 Features Used The distribution of target phrases given a source phrase and its contextual information is normalised to estimate P( eˆk |fˆk ,CI( fˆk )). Therefore our expected feature is derived as in (9): hˆmbl = log P( eˆk |fˆk ,CI( fˆk )) (9) In addition to the above feature, we derived two more features hˆmod and hˆbest from the posterior probability P( eˆk |fˆk ) and P( eˆk |fˆk ,CI( fˆk )). The feature hˆmod is defined as in (10): hˆmod = log [α P( eˆk |fˆk ,CI( fˆk )) + (1- α) P( eˆk |fˆk )] (1"
2009.eamt-1.32,P01-1027,0,0.376294,"Missing"
2009.eamt-1.32,J99-2004,0,0.0114076,"rtaggers exist only for English. In this paper, we begin to explore whether such a system could indeed generate improvements across all PB-SMT system components. Our novel approach combines the methods of (Stroppa et al., 2007) and (Hassan et al., 2006, 2007, 2008; Hassan, 2009) in one model. We extend a standard PB-SMT system with syntactic descriptions on the source side. Crucially, the kind of lexical descriptions that we employ are those that are commonly devised within lexicondriven approaches to linguistic syntax, namely Lexicalized Tree-Adjoining Grammar (LTAG: Joshi and Schabes, 1992; Bangalore and Joshi, 1999) and Combinatory Categorial Grammar (CCG: Steedman, 2000). In such approaches, the grammar consists of a very rich lexicon and a small set of combinatory operators that assemble lexical entries together into parse-trees. The lexical entries consist of syntactic constructs (‘supertags’) that describe information such as the POS tag of the word, its subcategorisation information and the hierarchy of phrase categories Proceedings of the 13th Annual Conference of the EAMT, pages 234–241, Barcelona, May 2009 234 that the word projects upwards. Like (Hassan et al., 2006, 2007, 2008; Hassan, 2009), i"
2009.eamt-1.32,J96-1002,0,0.0206265,"we discuss these and other empirical issues. The remainder of the paper is organized as follows. In section 2 we discuss related work. Section 3 gives a brief overview of PBSMT. In section 4 we describe the context-informed features contained in our baseline log-linear phrase-based SMT system. In section 5 we describe the memory-based classification approach. Section 6 describes the features used in the experiments, and the pre-processing required. Section 7 includes the results obtained, together with some analysis. Section 8 concludes, and provides avenues for further work. 2 Related Work (Berger et al., 1996) first suggested contextsensitive modelling of word translations in order to integrate local contextual information into their IBM translation models using a Maximum Entropy (MaxEnt) model, but the work is not supported by any significant evaluation results. García Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Using such a model on the German—English Verbmobil corpus, they obtained better alignment quality in terms of improved alignment error rate"
2009.eamt-1.32,W07-0719,0,0.0333399,"nt lexicon model. Using such a model on the German—English Verbmobil corpus, they obtained better alignment quality in terms of improved alignment error rate (AER). However, since alignment is not an end task in itself and most often used as an intermediate task to generate phrase pairs for the t-tables in PB-SMT systems, improved AER scores do not necessarily result in improved translation quality, as noted by a number of researchers. (Vickrey et al., 2005) built classifiers inspired by those used in word-sense disambiguation (WSD) to fill in any blanks in a partially completed translation. (Giménez and Màrquez, 2007) extended this work by considering the slightly more general case of very frequent phrases and moved to full translation rather than blank-filling on the target side. Initial attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007) of integrating state-of-the-art WSD methods into SMT to improve the overall translation quality have met with more success. Language models"
2009.eamt-1.32,P05-1048,0,0.0449464,"result in improved translation quality, as noted by a number of researchers. (Vickrey et al., 2005) built classifiers inspired by those used in word-sense disambiguation (WSD) to fill in any blanks in a partially completed translation. (Giménez and Màrquez, 2007) extended this work by considering the slightly more general case of very frequent phrases and moved to full translation rather than blank-filling on the target side. Initial attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007) of integrating state-of-the-art WSD methods into SMT to improve the overall translation quality have met with more success. Language models arguably play the most significant role in today’s PB-SMT systems. It is obvious that a straightforward addition of a source language model will make no contribution as this will be cancelled out by the denominator in the noisy-channel model of SMT. However, for some time now the feeling was that some incorporation of source language information into SMT"
2009.eamt-1.32,W08-0302,0,0.263931,"imates. Significant improvements over a baseline state-of-the-art PB-SMT system were obtained on Italian—English and Chinese— English IWSLT tasks. Unlike other recent proposals to exploit the accuracy and the flexibility of discriminative learning (e.g. Cowan et al., 2006; Liang et al., 2006), the strength of the approach of (Stroppa et al., 2007) is that no redefinition of one’s training procedures is required. Like the work of (Max et al., 2008), the present work is directly motivated by and an extension of the approach of (Stroppa et al., 2007). 235 The work of both (Max et al., 2008) and (Gimpel and Smith, 2008) focus on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for EnglishGerman, (Max et al., 2008) conduct experiments from EnglishFrench. Using the same sorts of local contextual features as (Stroppa et al., 2007), as well as using broader context in addition to grammatical dependency information, (Max et al., 2008) show modest gains over a PB-SMT baseline model in terms of automatic evaluation scores, but more improvements come to light in a manual investigation. One final paper in this strand of research is that of (He et al."
2009.eamt-1.32,D07-1007,0,0.0665287,"umber of researchers. (Vickrey et al., 2005) built classifiers inspired by those used in word-sense disambiguation (WSD) to fill in any blanks in a partially completed translation. (Giménez and Màrquez, 2007) extended this work by considering the slightly more general case of very frequent phrases and moved to full translation rather than blank-filling on the target side. Initial attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007) of integrating state-of-the-art WSD methods into SMT to improve the overall translation quality have met with more success. Language models arguably play the most significant role in today’s PB-SMT systems. It is obvious that a straightforward addition of a source language model will make no contribution as this will be cancelled out by the denominator in the noisy-channel model of SMT. However, for some time now the feeling was that some incorporation of source language information into SMT systems had to help. (Stroppa et al., 2007) added sourc"
2009.eamt-1.32,N03-1017,0,0.0540342,"probabilities based on local context and can be implemented using finite state technology, e.g. Hidden Markov Models (Bangalore and Joshi, 1999). There are currently two supertagging approaches available: LTAG-based (Bangalore and Joshi, 1999) and CCG-based (Clark and Curran, 2004). Both the LTAG (Chen et al., 2006) and the CCG supertag sets (Hockenmaier, 2003) were acquired from the WSJ section of the PennII Treebank using hand-built extraction rules. Here we test both the LTAG and CCG supertaggers. We extract the supertagged components of context words (±1/±2) along with the source phrase (Koehn et al., 2003) in a standard PBSMT system. We use a memory-based classification approach to obtain the probability for the given additional contexts with the source phrase. In this paper we discuss these and other empirical issues. The remainder of the paper is organized as follows. In section 2 we discuss related work. Section 3 gives a brief overview of PBSMT. In section 4 we describe the context-informed features contained in our baseline log-linear phrase-based SMT system. In section 5 we describe the memory-based classification approach. Section 6 describes the features used in the experiments, and the"
2009.eamt-1.32,2007.iwslt-1.10,1,0.852685,"ty when adding supertags as context-informed features. 1 Introduction In log-linear phrase-based SMT, the probability P(eI1|fJ1) of a target phrase eI1 given a source phrase fJ1 is modelled as a log-linear combination of features which normally consist of a finite set © 2009 European Association for Machine Translation. of translational features, and a language model (Och and Ney, 2002). The usual translational features involved in those models express dependencies between the source and target phrases, but not dependencies between the phrases in the source language themselves. Stroppa et al. (2007) were the first to show that incorporating source language context using neighbouring words and part-of-speech tags had the potential to improve translation quality. In a separate strand of research, Hassan et al. (2006, 2007, 2008) showed that incorporating lexical syntactic descriptions in the form of supertags in the target language model and on the target side of the translation model could improve significantly on state-of-the-art approaches to MT. Despite the significance of this work, it is currently not possible to develop a fully supertagged PB-SMT system given that supertaggers exist"
2009.eamt-1.32,P06-1096,0,0.0190838,"ndent phrasal translation probabilities learned using decision trees. They considered up to two words and/or POS tags on either side of the source focus word as contextual features. In order to overcome problems of estimation of such features, they used a decision-tree classifier which implicitly smoothes the probability estimates. Significant improvements over a baseline state-of-the-art PB-SMT system were obtained on Italian—English and Chinese— English IWSLT tasks. Unlike other recent proposals to exploit the accuracy and the flexibility of discriminative learning (e.g. Cowan et al., 2006; Liang et al., 2006), the strength of the approach of (Stroppa et al., 2007) is that no redefinition of one’s training procedures is required. Like the work of (Max et al., 2008), the present work is directly motivated by and an extension of the approach of (Stroppa et al., 2007). 235 The work of both (Max et al., 2008) and (Gimpel and Smith, 2008) focus on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for EnglishGerman, (Max et al., 2008) conduct experiments from EnglishFrench. Using the same sorts of local contextual features as (Stroppa et"
2009.eamt-1.32,P07-1037,1,0.163249,"Missing"
2009.eamt-1.32,2008.eamt-1.17,0,0.179946,"contextual features. In order to overcome problems of estimation of such features, they used a decision-tree classifier which implicitly smoothes the probability estimates. Significant improvements over a baseline state-of-the-art PB-SMT system were obtained on Italian—English and Chinese— English IWSLT tasks. Unlike other recent proposals to exploit the accuracy and the flexibility of discriminative learning (e.g. Cowan et al., 2006; Liang et al., 2006), the strength of the approach of (Stroppa et al., 2007) is that no redefinition of one’s training procedures is required. Like the work of (Max et al., 2008), the present work is directly motivated by and an extension of the approach of (Stroppa et al., 2007). 235 The work of both (Max et al., 2008) and (Gimpel and Smith, 2008) focus on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for EnglishGerman, (Max et al., 2008) conduct experiments from EnglishFrench. Using the same sorts of local contextual features as (Stroppa et al., 2007), as well as using broader context in addition to grammatical dependency information, (Max et al., 2008) show modest gains over a PB-SMT baseline mo"
2009.eamt-1.32,P03-1021,0,0.00387557,"ise, We performed three different experiments by integrating these three features hˆmbl , hˆmod and hˆbest directly into the log-linear model. In the first experiment E1, the baseline feature log P( eˆk |fˆk ) is directly replaced by hˆmod . In the second experiment (E2), we integrated the hˆmbl feature together with the baseline features, keeping all the features unaffected. In the third experiment (E3), both the features hˆmbl and hˆbest are integrated into the model in the same manner. As for the standard phrase-based approach, their weights are optimized using minimum-error-rate training (Och, 2003) for each of the experiments we carried out. 6.2 Pre-Processing As (Stroppa et al., 2007) point out, PB-SMT decoders such as Pharaoh (Koehn, 2004) or Moses (Koehn, 2007) rely on a static phrase-table represented as a list of aligned phrases accompanied with several features. Since these features do not express the context in which those phrases occur, no context information is kept in the phrase-table, and there is no way to recover this information from the phrase-table. In order to take into account the contextinformed features for use with such decoders, the devset and test set that need to"
2009.eamt-1.32,C08-1041,0,0.0220602,"th, 2008) focus on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for EnglishGerman, (Max et al., 2008) conduct experiments from EnglishFrench. Using the same sorts of local contextual features as (Stroppa et al., 2007), as well as using broader context in addition to grammatical dependency information, (Max et al., 2008) show modest gains over a PB-SMT baseline model in terms of automatic evaluation scores, but more improvements come to light in a manual investigation. One final paper in this strand of research is that of (He et al., 2008), who despite not mentioning the obvious link between the two pieces of work, show that the source language features used by (Stroppa et al., 2007) are also of benefit when used with the Hiero (Chiang, 2007) decoder. As regards supertagged models of translation, (Hassan et al., 2006, 2007b, 2008; Hassan, 2009) have demonstrated clearly that adding supertags (essentially, part-of-speech tags of words plus local subcategorisation requirements) in the target language model and on the target side of the translation model improve state-of-the-art PBSMT systems. The system of (Hassan et al., 2007a)"
2009.eamt-1.32,P02-1038,0,0.133102,"f words plus local subcategorisation requirements) in the target language model and on the target side of the translation model improve state-of-the-art PBSMT systems. The system of (Hassan et al., 2007a) was ranked first according to human evaluators on the IWSLT 2007 Arabic–English task, despite the improvements in system design not being shown to their best advantage by the automatic evaluation metrics. More recently, (Hassan, 2009) has demonstrated that improvements can even be gained over the leading NIST07 Arabic–English system of (Ittycheriah and Roukos, 2007). combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): 3 4 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation e1I = e1 . . . eI of a source sentence f1 J = f1 . . . fJ is chosen to maximize (1): arg max P (e1I |f1J )  arg max P ( f1 J |e1I ).P (e1I ) I ,e1I (1) where P ( f1 |e ) and P (e ) denote respectively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( e1I |f1J ) is directly modelled as a (log-linear) I 1 I 1 m 1  LM log P"
2009.eamt-1.32,P02-1040,0,0.107341,"Missing"
2009.eamt-1.32,N07-1008,0,0.00591983,"hat adding supertags (essentially, part-of-speech tags of words plus local subcategorisation requirements) in the target language model and on the target side of the translation model improve state-of-the-art PBSMT systems. The system of (Hassan et al., 2007a) was ranked first according to human evaluators on the IWSLT 2007 Arabic–English task, despite the improvements in system design not being shown to their best advantage by the automatic evaluation metrics. More recently, (Hassan, 2009) has demonstrated that improvements can even be gained over the leading NIST07 Arabic–English system of (Ittycheriah and Roukos, 2007). combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): 3 4 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation e1I = e1 . . . eI of a source sentence f1 J = f1 . . . fJ is chosen to maximize (1): arg max P (e1I |f1J )  arg max P ( f1 J |e1I ).P (e1I ) I ,e1I (1) where P ( f1 |e ) and P (e ) denote respectively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( e1I |f1J ) is directly modell"
2009.eamt-1.32,C92-2066,0,0.328103,"MT system given that supertaggers exist only for English. In this paper, we begin to explore whether such a system could indeed generate improvements across all PB-SMT system components. Our novel approach combines the methods of (Stroppa et al., 2007) and (Hassan et al., 2006, 2007, 2008; Hassan, 2009) in one model. We extend a standard PB-SMT system with syntactic descriptions on the source side. Crucially, the kind of lexical descriptions that we employ are those that are commonly devised within lexicondriven approaches to linguistic syntax, namely Lexicalized Tree-Adjoining Grammar (LTAG: Joshi and Schabes, 1992; Bangalore and Joshi, 1999) and Combinatory Categorial Grammar (CCG: Steedman, 2000). In such approaches, the grammar consists of a very rich lexicon and a small set of combinatory operators that assemble lexical entries together into parse-trees. The lexical entries consist of syntactic constructs (‘supertags’) that describe information such as the POS tag of the word, its subcategorisation information and the hierarchy of phrase categories Proceedings of the 13th Annual Conference of the EAMT, pages 234–241, Barcelona, May 2009 234 that the word projects upwards. Like (Hassan et al., 2006,"
2009.eamt-1.32,2007.tmi-papers.28,1,0.413276,"Missing"
2009.eamt-1.32,koen-2004-pharaoh,0,0.00921006,"n the first experiment E1, the baseline feature log P( eˆk |fˆk ) is directly replaced by hˆmod . In the second experiment (E2), we integrated the hˆmbl feature together with the baseline features, keeping all the features unaffected. In the third experiment (E3), both the features hˆmbl and hˆbest are integrated into the model in the same manner. As for the standard phrase-based approach, their weights are optimized using minimum-error-rate training (Och, 2003) for each of the experiments we carried out. 6.2 Pre-Processing As (Stroppa et al., 2007) point out, PB-SMT decoders such as Pharaoh (Koehn, 2004) or Moses (Koehn, 2007) rely on a static phrase-table represented as a list of aligned phrases accompanied with several features. Since these features do not express the context in which those phrases occur, no context information is kept in the phrase-table, and there is no way to recover this information from the phrase-table. In order to take into account the contextinformed features for use with such decoders, the devset and test set that need to be translated is pre-processed. Each word appearing in the test set and devset is assigned a unique id. First we prepare the phrase table using t"
2009.eamt-1.32,H05-1097,0,0.208056,"1) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Using such a model on the German—English Verbmobil corpus, they obtained better alignment quality in terms of improved alignment error rate (AER). However, since alignment is not an end task in itself and most often used as an intermediate task to generate phrase pairs for the t-tables in PB-SMT systems, improved AER scores do not necessarily result in improved translation quality, as noted by a number of researchers. (Vickrey et al., 2005) built classifiers inspired by those used in word-sense disambiguation (WSD) to fill in any blanks in a partially completed translation. (Giménez and Màrquez, 2007) extended this work by considering the slightly more general case of very frequent phrases and moved to full translation rather than blank-filling on the target side. Initial attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches (Carpuat and Wu, 2007; Chan et al., 2007; Gim"
2009.eamt-1.32,P07-2045,0,0.0527183,"Missing"
2009.eamt-1.32,N04-1033,0,0.0258205,"ifier, we modify the standard phrase-extraction method of (Koehn et al., 2003) to extract the context of the source phrases at the same time as the phrases themselves. Importantly, therefore, the context extraction comes at no extra cost. We refer the interested reader to (Stroppa et al., 2007) for more details of how MemoryBased Learning (MBL) is used for classification of source examples for use in the log-linear MT framework. Memory-Based Classification As (Stroppa et al., 2007) point out, directly estimating P( eˆk |fˆk , CI( fˆk )) using relative frequencies (say) is problematic. Indeed, Zens and Ney (2004) showed that the estimation of P( eˆk |fˆk ) using relative frequencies results in the overestimation of the probabilities of long phrases, so smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of context informed features, since the context is also Experimental Set-Up 6.1 Features Used The distribution of target phrases given a source phrase and its contextual information is normalised to estimate P( eˆk |fˆk ,CI( fˆk )). Therefore our expected feature is derived as in (9): hˆmbl = log P( eˆk |fˆk ,CI( fˆk )) (9) I"
2009.eamt-1.32,2006.iwslt-evaluation.4,1,\N,Missing
2010.amta-papers.16,W09-0432,0,0.184409,"Missing"
2010.amta-papers.16,eck-etal-2004-language,0,0.25838,"he classifier used in our experiment. Section 4 presents the experimental setup, the data and provides details of the specific experiments carried out. The results and analysis are presented in Section 5. In Section 6, we perform a manual analysis of the system performance using example sentences from the test corpora followed by conclusions and future research directions in Section 7. 2 Related Work Langlais (2002) imported the concept of Domain Adaptation to SMT by integrating domain-specific lexicons in the translation model resulting in significant improvement in terms of Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Their approach was further refined by Zhao et al. (2004). Hildebrand (2005) also utilized the approach of Eck et al. shortciteeck:04 to select similar sentences from available training data to adapt translation models, which significantly improved translation performance over baseline systems. Hasan and Ney (2005) proposed a method for building class-based language models by clustering sentences into specific classes and interpolating them with global language models achieving improvements in term"
2010.amta-papers.16,Y09-2027,1,0.845331,"Missing"
2010.amta-papers.16,2005.eamt-1.17,0,0.0567223,"is (2002) imported the concept of Domain Adaptation to SMT by integrating domain-specific lexicons in the translation model resulting in significant improvement in terms of Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Their approach was further refined by Zhao et al. (2004). Hildebrand (2005) also utilized the approach of Eck et al. shortciteeck:04 to select similar sentences from available training data to adapt translation models, which significantly improved translation performance over baseline systems. Hasan and Ney (2005) proposed a method for building class-based language models by clustering sentences into specific classes and interpolating them with global language models achieving improvements in terms of perplexity reduction and error rates in MT. This work was further extended by Yamamoto and Sumita (2007) as well as Foster and Kuhn (2007) to include translation models. Using unsupervised clustering techniques on the bilingual training data, automatic clusters were created and each cluster was treated as a domain (Yamamoto and Sumita, 2007). Using domain-specific language models and translation models to"
2010.amta-papers.16,2005.eamt-1.19,0,0.298303,"Missing"
2010.amta-papers.16,P07-2045,0,0.0121647,"Missing"
2010.amta-papers.16,W07-0733,0,0.569288,"n and error rates in MT. This work was further extended by Yamamoto and Sumita (2007) as well as Foster and Kuhn (2007) to include translation models. Using unsupervised clustering techniques on the bilingual training data, automatic clusters were created and each cluster was treated as a domain (Yamamoto and Sumita, 2007). Using domain-specific language models and translation models to translate sentences from the generated domains resulted in improved translation quality. Integrating in-domain and out-of-domain language models one using log-linear features of an SMT model was carried out by Koehn and Schroeder (2007). This work also saw the first use of multiple decoding paths for combining multiple domain translation tables within the framework of the Moses decoder (Koehn et al., 2007). The same idea was explored using a different approach by Nakov (2008) using data-source indicator features to distinguish between phrases from different domains within the phrase tables. Xu et al. (2007) investigated the usage of information retrieval approaches to classify the input test sentences based on the domains, along with domain-dependent language modeling or feature weights combination for domain-specific traini"
2010.amta-papers.16,W02-1405,0,0.521618,"e combined training model. The remainder of the paper is organized as follows. In Section 2, we present previous research in the field of Domain Adaptation in SMT. Section 3 describes details of the classifier used in our experiment. Section 4 presents the experimental setup, the data and provides details of the specific experiments carried out. The results and analysis are presented in Section 5. In Section 6, we perform a manual analysis of the system performance using example sentences from the test corpora followed by conclusions and future research directions in Section 7. 2 Related Work Langlais (2002) imported the concept of Domain Adaptation to SMT by integrating domain-specific lexicons in the translation model resulting in significant improvement in terms of Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Their approach was further refined by Zhao et al. (2004). Hildebrand (2005) also utilized the approach of Eck et al. shortciteeck:04 to select similar sentences from available training data to adapt translation models, which significantly improved translation performance over baseline systems. Hasan and"
2010.amta-papers.16,W08-0320,0,0.404527,"d each cluster was treated as a domain (Yamamoto and Sumita, 2007). Using domain-specific language models and translation models to translate sentences from the generated domains resulted in improved translation quality. Integrating in-domain and out-of-domain language models one using log-linear features of an SMT model was carried out by Koehn and Schroeder (2007). This work also saw the first use of multiple decoding paths for combining multiple domain translation tables within the framework of the Moses decoder (Koehn et al., 2007). The same idea was explored using a different approach by Nakov (2008) using data-source indicator features to distinguish between phrases from different domains within the phrase tables. Xu et al. (2007) investigated the usage of information retrieval approaches to classify the input test sentences based on the domains, along with domain-dependent language modeling or feature weights combination for domain-specific training of SMT models. This effort resulted in significant improvement in domain-dependent translation when compared to domain-independent translation. Bertoldi and Federico’s (2009) experiments utilizing in-domain monolingual resources to improve d"
2010.amta-papers.16,J03-1002,0,0.00407199,"for too short sentences (Papineni et al., 2002). Usually this score reflects the fluency of the translated sentence. On the other hand NIST scores are quite similar to BLEU 4 http://www.openmatrex.org/ scores, but use an arithmetic average rather than a geometric one (Doddington, 2002). NIST weighs more informative n-grams higher than the others. Hence NIST scores reflect the adequacy of the translated sentences. In the context of our work, we report both BLEU and NIST scores to capture different aspects of the quality of a translation. For training, word alignment was performed using Giza++ (Och and Ney, 2003), followed by the creation of the phrase and the re-ordering tables using Moses training (Koehn et al., 2007). 5gram language models were built on the domainspecific training data using the SRILM toolset (Stolcke, 2002). After training, each of the model components was tuned using Minimum Error-Rate Training (MERT) (Och, 2003) on the BLEU metric. This process helps us tune models to domain-specific development sets. Finally the models were tested on both domainspecific as well as combined domain test sets. Since the primary objective of our experiments was to achieve better translation of a mi"
2010.amta-papers.16,P03-1021,0,0.00737576,"the others. Hence NIST scores reflect the adequacy of the translated sentences. In the context of our work, we report both BLEU and NIST scores to capture different aspects of the quality of a translation. For training, word alignment was performed using Giza++ (Och and Ney, 2003), followed by the creation of the phrase and the re-ordering tables using Moses training (Koehn et al., 2007). 5gram language models were built on the domainspecific training data using the SRILM toolset (Stolcke, 2002). After training, each of the model components was tuned using Minimum Error-Rate Training (MERT) (Och, 2003) on the BLEU metric. This process helps us tune models to domain-specific development sets. Finally the models were tested on both domainspecific as well as combined domain test sets. Since the primary objective of our experiments was to achieve better translation of a mix of sentences coming from multiple domains, we tested all our translation models using a combined test set from both domains. Moreover, we also tested the same models using domain-specific test sets to get a clear understanding of the effect of domain-specific data. 4.3 Domain Adaptation Experiments In this section we describ"
2010.amta-papers.16,2007.mtsummit-papers.68,0,0.656756,"o translate sentences from the generated domains resulted in improved translation quality. Integrating in-domain and out-of-domain language models one using log-linear features of an SMT model was carried out by Koehn and Schroeder (2007). This work also saw the first use of multiple decoding paths for combining multiple domain translation tables within the framework of the Moses decoder (Koehn et al., 2007). The same idea was explored using a different approach by Nakov (2008) using data-source indicator features to distinguish between phrases from different domains within the phrase tables. Xu et al. (2007) investigated the usage of information retrieval approaches to classify the input test sentences based on the domains, along with domain-dependent language modeling or feature weights combination for domain-specific training of SMT models. This effort resulted in significant improvement in domain-dependent translation when compared to domain-independent translation. Bertoldi and Federico’s (2009) experiments utilizing in-domain monolingual resources to improve domain adaptation also achieved considerable improvements. They used domain-specific baseline systems to translate in-domain monolingua"
2010.amta-papers.16,C04-1059,0,0.0344827,"he results and analysis are presented in Section 5. In Section 6, we perform a manual analysis of the system performance using example sentences from the test corpora followed by conclusions and future research directions in Section 7. 2 Related Work Langlais (2002) imported the concept of Domain Adaptation to SMT by integrating domain-specific lexicons in the translation model resulting in significant improvement in terms of Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Their approach was further refined by Zhao et al. (2004). Hildebrand (2005) also utilized the approach of Eck et al. shortciteeck:04 to select similar sentences from available training data to adapt translation models, which significantly improved translation performance over baseline systems. Hasan and Ney (2005) proposed a method for building class-based language models by clustering sentences into specific classes and interpolating them with global language models achieving improvements in terms of perplexity reduction and error rates in MT. This work was further extended by Yamamoto and Sumita (2007) as well as Foster and Kuhn (2007) to include"
2010.amta-papers.16,P02-1040,0,\N,Missing
2010.amta-papers.16,2006.iwslt-evaluation.4,1,\N,Missing
2010.amta-papers.16,D07-1054,0,\N,Missing
2010.amta-papers.16,W07-0717,0,\N,Missing
2010.amta-papers.23,J96-1002,0,0.0209051,"a brief overview of HPB. In Section 4 we describe the context-informed features contained in our baseline HPB model. In Section 5 we describe our memory-based classification approach. Section 6 describes experimental set-ups. Section 7 presents the results obtained, and offers a brief qualitative analysis. In Section 8 we formulate our conclusions, and offer some avenues for further work. 2 Related Work MT research on incorporating contexts into SMT models can be broadly divided into two categories: source-context modelling such as (Stroppa et al., 2007), and target-context modelling such as (Berger et al., 1996; Hasan et al., 2008). The present study relates to the first category, which further divides into the following approaches: Discriminative word alignment: Garc´ıa-Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Subsequently, more recent discriminative approaches employ source-side contexts for creating finer-grained word-to-word lexicons (Brunning et al., 2009; Mauser et al., 2009; Patry and Langlais, 2009). Phrase-based SMT: Vickrey et al. (2005) b"
2010.amta-papers.23,2009.mtsummit-papers.12,0,0.0121654,"l., 2007), and target-context modelling such as (Berger et al., 1996; Hasan et al., 2008). The present study relates to the first category, which further divides into the following approaches: Discriminative word alignment: Garc´ıa-Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Subsequently, more recent discriminative approaches employ source-side contexts for creating finer-grained word-to-word lexicons (Brunning et al., 2009; Mauser et al., 2009; Patry and Langlais, 2009). Phrase-based SMT: Vickrey et al. (2005) build WSD-inspired classifiers to fill in blanks in partially completed translations. Stroppa et al. (2007) were the first to add source-side contextual features into a state-of-the-art log-linear PBSMT system by incorporating context-dependent phrasal translation probabilities learned using a decision-tree classifier (Daelemans and van den Bosch, 2005). Significant improvements over a baseline PBSMT system were obtained on Italian-to-English and Chinese-toEnglish IWSLT tasks. Discriminative learning approaches in SMT such as (Cowan et al., 2006) gener"
2010.amta-papers.23,D09-1022,0,0.0362701,"such as (Stroppa et al., 2007), and target-context modelling such as (Berger et al., 1996; Hasan et al., 2008). The present study relates to the first category, which further divides into the following approaches: Discriminative word alignment: Garc´ıa-Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Subsequently, more recent discriminative approaches employ source-side contexts for creating finer-grained word-to-word lexicons (Brunning et al., 2009; Mauser et al., 2009; Patry and Langlais, 2009). Phrase-based SMT: Vickrey et al. (2005) build WSD-inspired classifiers to fill in blanks in partially completed translations. Stroppa et al. (2007) were the first to add source-side contextual features into a state-of-the-art log-linear PBSMT system by incorporating context-dependent phrasal translation probabilities learned using a decision-tree classifier (Daelemans and van den Bosch, 2005). Significant improvements over a baseline PBSMT system were obtained on Italian-to-English and Chinese-toEnglish IWSLT tasks. Discriminative learning approaches in SMT such as"
2010.amta-papers.23,C92-2066,0,0.242831,"α+1 , ..., wα+i ) of a given source phrase α. In our experiments, we consider a context size of 2 (i.e., i := 2). It also includes boundary words (wntstart j and wntend ) of subphrases covered by nonterminals j in the α. Like (Chiang, 2007), we restrict the number of nonterminals to two (i.e., j := 2). The resultant lexical features form a window of size 2(i+j) features. Thus, lexical contextual information (CIlex ) can be described as in (4): Figure 1: CCG and LTAG supertag sequences. In our experiments two kinds of supertags are employed: those from lexicalized tree-adjoining grammar, LTAG (Joshi and Schabes, 1992), and combinatory categorial grammar, CCG (Steedman, 2000). Both the LTAG and the CCG supertag sets were acquired from the WSJ section of the Penn-II Treebank using hand-built extraction rules. Here we use both the LTAG and CCG supertaggers. In LTAG, a lexical item is associated with an elementary tree, while in CCG the supertag constitutes a CCG lexical category with a set of word-to-word dependencies. The two alternative supertag descriptions can be viewed as closely related functional descriptors of words. Like CIpos , we define the contextual information (CIst ) defining supertags as in (6"
2010.amta-papers.23,W06-1628,0,0.0642608,"Missing"
2010.amta-papers.23,J07-2003,0,0.394007,"ssfully employed in PBSMT by taking various contextual information of the source phrase into account. These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information, position-specific neighbouring words (Gim´enez and M`arquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features (Gimpel and Smith, 2008), full sentential context (Carpuat and Wu, 2007), lexical syntactic descriptions in the form of supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighbouring words and their part-of-speech (POS) prove to be effective source contexts in the HPB model (He et al., 2008). In a study involving PBSMT, Haque et al. (2009b) showed that the translations of ambiguous words are also in"
2010.amta-papers.23,N09-1025,0,0.0267371,"focused on solving ambiguities for those Chinese phrases that consist of only one or two terminal symbols. More recently, Shen et al. (2009) proposed a method to include linguistic and contextual information in the HPB system. The features employed in the system are non-terminal labels, non-terminal length distribution, source context and a language model created from source-side grammatical dependency structures. While their source-side dependency language model does not produce any improvement, the other features seem to be effective in Arabic-to-English and Chinese-to-English translation. Chiang et al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system. In order to limit the the size of their model, they restrict words to being among the 100 most frequently occurring words from the training data; all other words are replaced with a special token. One final paper in this strand of research is that of (He et al., 2008), who despite not mentioning the link between the two pieces of work, show that the low-level source-language features used by (Stroppa et al., 2007) are also of benefi"
2010.amta-papers.23,H05-1097,0,0.0642552,"as (Berger et al., 1996; Hasan et al., 2008). The present study relates to the first category, which further divides into the following approaches: Discriminative word alignment: Garc´ıa-Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Subsequently, more recent discriminative approaches employ source-side contexts for creating finer-grained word-to-word lexicons (Brunning et al., 2009; Mauser et al., 2009; Patry and Langlais, 2009). Phrase-based SMT: Vickrey et al. (2005) build WSD-inspired classifiers to fill in blanks in partially completed translations. Stroppa et al. (2007) were the first to add source-side contextual features into a state-of-the-art log-linear PBSMT system by incorporating context-dependent phrasal translation probabilities learned using a decision-tree classifier (Daelemans and van den Bosch, 2005). Significant improvements over a baseline PBSMT system were obtained on Italian-to-English and Chinese-toEnglish IWSLT tasks. Discriminative learning approaches in SMT such as (Cowan et al., 2006) generally require a redefinition of the traini"
2010.amta-papers.23,P02-1038,0,0.80659,"based SMT model (Chiang, 2007) uses the bilingual phrase pairs of phrase-based SMT (PBSMT) (Koehn et al., 2003) as a starting point to learn hierarchial rules using probabilistic synchronous context-free grammar (PSCFG). The decoding process in the hierarchical phrase-based SMT (HPB) model is based on bottom-up chart parsing (Chiang, 2007). This chart parsing decoder, also known as Hiero, does not require explicit syntactic representation on either side of the phrases in rules. State-of-the-art SMT models (Koehn et al., 2003; Chiang, 2007) can be viewed as log-linear combinations of features (Och and Ney, 2002) that usually comprise translational features and the language model. The translational features typically involved in these models express dependencies between the source and target phrases, but not dependencies between the phrases in the source language themselves, i.e. they do not take into account the contexts of those phrases. Word sense disambiguation (WSD), a task intricately related to MT, typically employs rich contextsensitive features to determine contextually the most likely sense of a polysemous word. Inspired by these context-rich WSD techniques, researchers have tried to integra"
2010.amta-papers.23,W06-1607,0,0.0192106,"e, when supertags are combined with lexical features, the CI is S formed by the union of these features, i.e., CI= CIst CIlex . 5 Memory-Based Disambiguation As Stroppa et al. (2007) point out, directly estimating context-dependent phrase translation probabilities using relative frequencies is problematic. Indeed, (Zens and Ney, 2004) showed that the estimation of phrase translation probabilities using relative frequencies results in overestimation of the probabilities of long phrases. Accordingly, smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of context-informed features, since the context is also taken into account, this estimation problem can only become worse. As an alternative, in this work we make use of memory-based machine learning classifiers able to estimate P(γ|α, CI(α)) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations, matched to a new source phrase to be translated. In this work we use the approximate memory-based classifier IGTree1 (Daelemans and van den Bosch, 2005). IGTree makes a heuristic approximation of knearest neighbour search by storing"
2010.amta-papers.23,P01-1027,0,0.0761219,"Missing"
2010.amta-papers.23,N09-1013,0,0.0447881,"Missing"
2010.amta-papers.23,W07-0719,0,0.0564061,"Missing"
2010.amta-papers.23,2008.eamt-1.17,0,0.0184333,"nse of a polysemous word. Inspired by these context-rich WSD techniques, researchers have tried to integrate various contextual knowledge sources into state-of-the-art SMT models. In recent years, source context modelling has been successfully employed in PBSMT by taking various contextual information of the source phrase into account. These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information, position-specific neighbouring words (Gim´enez and M`arquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features (Gimpel and Smith, 2008), full sentential context (Carpuat and Wu, 2007), lexical syntactic descriptions in the form of supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighb"
2010.amta-papers.23,D09-1023,0,0.0210909,"009a) and grammatical dependency relations (Haque et al., 2009b) have been modelled as useful source context to improve phrase selection in PBSMT. Alternative SMT architectures: Bangalore et al. (2007) propose an SMT architecture based on stochastic finite state transducers, that addresses global lexical selection in which parameters are discriminatively trained using a MaxEnt model considering n-gram features from the source sentence. Specia et al. (2008) integrate WSD predictions for the reranking of n-best translations, limited to a small set of words from different grammatical categories. Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasisynchronous grammar that can incorporate arbitrary features from both source and target sentences. Hierarchial phrase-based SMT: Chan et al. (2007) were the first to use a WSD system to integrate additional features in the state-of-the-art HPB system (Chiang, 2007), achieving statistically significant performance improvements for several automatic measures for Chinese-to-English translation. However, they only focused on solving ambiguities for those Chinese phrases that consist of only one or two terminal symbols. More recently, Sh"
2010.amta-papers.23,D09-1008,0,0.0200964,"9) present an MT framework based on lattice parsing with a quasisynchronous grammar that can incorporate arbitrary features from both source and target sentences. Hierarchial phrase-based SMT: Chan et al. (2007) were the first to use a WSD system to integrate additional features in the state-of-the-art HPB system (Chiang, 2007), achieving statistically significant performance improvements for several automatic measures for Chinese-to-English translation. However, they only focused on solving ambiguities for those Chinese phrases that consist of only one or two terminal symbols. More recently, Shen et al. (2009) proposed a method to include linguistic and contextual information in the HPB system. The features employed in the system are non-terminal labels, non-terminal length distribution, source context and a language model created from source-side grammatical dependency structures. While their source-side dependency language model does not produce any improvement, the other features seem to be effective in Arabic-to-English and Chinese-to-English translation. Chiang et al. (2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated i"
2010.amta-papers.23,D07-1007,0,0.1589,"t-rich WSD techniques, researchers have tried to integrate various contextual knowledge sources into state-of-the-art SMT models. In recent years, source context modelling has been successfully employed in PBSMT by taking various contextual information of the source phrase into account. These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information, position-specific neighbouring words (Gim´enez and M`arquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features (Gimpel and Smith, 2008), full sentential context (Carpuat and Wu, 2007), lexical syntactic descriptions in the form of supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighbouring words and their part-of-speech (POS) prov"
2010.amta-papers.23,2007.tmi-papers.28,1,0.703467,"Missing"
2010.amta-papers.23,W04-3250,0,0.0940844,"Missing"
2010.amta-papers.23,N03-1017,0,0.00659903,"contextual information of the source phrase into account. These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information, position-specific neighbouring words (Gim´enez and M`arquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features (Gimpel and Smith, 2008), full sentential context (Carpuat and Wu, 2007), lexical syntactic descriptions in the form of supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighbouring words and their part-of-speech (POS) prove to be effective source contexts in the HPB model (He et al., 2008). In a study involving PBSMT, Haque et al. (2009b) showed that the translations of ambiguous words are also influenced by more distant words in the sentence. S"
2010.amta-papers.23,Y09-1019,1,0.821505,"Missing"
2010.amta-papers.23,2009.eamt-1.32,1,0.68043,"owledge sources into state-of-the-art SMT models. In recent years, source context modelling has been successfully employed in PBSMT by taking various contextual information of the source phrase into account. These contextual features may include lexical features of words appearing in the context and bearing sense discriminatory information, position-specific neighbouring words (Gim´enez and M`arquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features (Gimpel and Smith, 2008), full sentential context (Carpuat and Wu, 2007), lexical syntactic descriptions in the form of supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighbouring words and their part-of-speech (POS) prove to be effective source contexts in the HPB model (He et al., 2008). In a st"
2010.amta-papers.23,N04-1033,0,0.0277164,"a window of size 2(i + j). We compare the effect of supertag features in contrastive experiments using words and POS tags as context in order to observe the relative effects of different features. In addition, we combine the syntactic features with the lexical features. For instance, when supertags are combined with lexical features, the CI is S formed by the union of these features, i.e., CI= CIst CIlex . 5 Memory-Based Disambiguation As Stroppa et al. (2007) point out, directly estimating context-dependent phrase translation probabilities using relative frequencies is problematic. Indeed, (Zens and Ney, 2004) showed that the estimation of phrase translation probabilities using relative frequencies results in overestimation of the probabilities of long phrases. Accordingly, smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of context-informed features, since the context is also taken into account, this estimation problem can only become worse. As an alternative, in this work we make use of memory-based machine learning classifiers able to estimate P(γ|α, CI(α)) by similarity-based reasoning over memorized nearest-neighb"
2010.amta-papers.23,D08-1039,0,0.0238295,"Missing"
2010.amta-papers.23,P07-1020,0,0.0166708,"training procedure; in contrast, Stroppa et al. (2007) introduce new features while retaining the strength of existing stateof-the-art systems. Other recent approaches to integrate state-of-the-art WSD methods into PBSMT (Gim´enez and M`arquez, 2007; Carpuat and Wu, 2007) have met with success as well. Following the work of (Stroppa et al., 2007), rich and complex syntactic structures such as supertags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b) have been modelled as useful source context to improve phrase selection in PBSMT. Alternative SMT architectures: Bangalore et al. (2007) propose an SMT architecture based on stochastic finite state transducers, that addresses global lexical selection in which parameters are discriminatively trained using a MaxEnt model considering n-gram features from the source sentence. Specia et al. (2008) integrate WSD predictions for the reranking of n-best translations, limited to a small set of words from different grammatical categories. Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasisynchronous grammar that can incorporate arbitrary features from both source and target sentences. Hierarchial phras"
2010.amta-papers.23,P07-1005,0,0.0391009,"MT architecture based on stochastic finite state transducers, that addresses global lexical selection in which parameters are discriminatively trained using a MaxEnt model considering n-gram features from the source sentence. Specia et al. (2008) integrate WSD predictions for the reranking of n-best translations, limited to a small set of words from different grammatical categories. Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasisynchronous grammar that can incorporate arbitrary features from both source and target sentences. Hierarchial phrase-based SMT: Chan et al. (2007) were the first to use a WSD system to integrate additional features in the state-of-the-art HPB system (Chiang, 2007), achieving statistically significant performance improvements for several automatic measures for Chinese-to-English translation. However, they only focused on solving ambiguities for those Chinese phrases that consist of only one or two terminal symbols. More recently, Shen et al. (2009) proposed a method to include linguistic and contextual information in the HPB system. The features employed in the system are non-terminal labels, non-terminal length distribution, source cont"
2010.amta-papers.23,C08-1041,0,0.211341,"rtags (Haque et al., 2009a) and grammatical dependency relations (Haque et al., 2009b). A limitation that Hiero (Chiang, 2007) shares with the PBSMT model (Koehn et al., 2003) is that it does not take into account the contexts in which the source-sides of the rules appear. In other words, it can be argued that rule selection in Hiero is suboptimally modelled. So far, a small number of studies have made use of source-language context for improving rule selection in Hiero. Position-specific neighbouring words and their part-of-speech (POS) prove to be effective source contexts in the HPB model (He et al., 2008). In a study involving PBSMT, Haque et al. (2009b) showed that the translations of ambiguous words are also influenced by more distant words in the sentence. Syntactic contexts that capture long-distance dependencies between words in a sentence can be a useful means to disambiguate among translations. Accordingly, integration of such syntactic contexts could lead to improved translation quality in PBSMT. For instance, Haque et al. (2009a) showed that supertags are more powerful source contexts than neighbouring words and part-of-speech tags to disambiguate a source phrase in PBSMT. Inspired by"
2011.eamt-1.29,1999.tmi-1.3,0,0.0226373,"segment receives a score. The best alignment is the one with the lowest score. The alignment score is the weighted sum of the values of eight features, which include: the number of SL words with no correspondences in the TL segment, the number of TL words with no correspondences in the SL fragment, the number of SL words with a correspondence in the TL sentence but not in the relevant TL segment, and the difference in length between the SL and the TL segment. Each translation is passed on to the recombination step as long as its score does not exceed five times the length of the SL fragment. Brown (1999) proposed an extension to CMUEBMT that makes use of semantic and syntactic generalized templates. He referred to the template categories as equivalence classes. Examples of semantic and syntactic equivalence classes are given in Table 2. The table shows that class members can in turn contain classes. This is evident from the last line (shown in bold). The system generalizes both the training and the test set: it recursively replaces words and phrases that are part of an equivalence class with the corresponding class tag. Syntactic classes are applied before semantic classes, and disambiguation"
2011.eamt-1.29,1996.amta-1.35,0,0.195739,"hunk with the name of its category, e. g., of a marathon → <PREP> a marathon. The generalized template extension is not part of the current Marclator system. We reimplemented it for our experiments. 3.2 CMU-EBMT The second EBMT system which we used for our experiments is CMU-EBMT.7 The system forms 4 http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC96L14 5 Note that both word and chunk alignment involve statistical knowledge. 6 This is a common procedure for recombinators that do not incorporate a language model. 7 http://sourceforge.net/projects/ cmu-ebmt/ 211 part of PanLite (Frederking and Brown, 1996), an MT architecture developed at Carnegie-Mellon University (CMU). It can also be invoked on its own. The system requires a parallel corpus and a bilingual dictionary. Brown (1996) used entries from a commercial bilingual dictionary for his experiments in translation from Spanish to English. Unlike Marclator, CMU-EBMT does not require subsentential units to be compiled before the actual translation step. The matching step resembles closely that of a traditional EBMT system: CMU-EBMT extracts every substring of the input sentence with a minimum length of two tokens that appears in the SL half"
2011.eamt-1.29,2003.mtsummit-papers.18,1,0.761469,". Each chunk that is not found in the example base is then split into single words. If several TL correspondences for an SL chunk or word are found in the example base, the one with the highest probability is chosen.6 Thus, for each input sentence, the recombinator outputs a single hypothesis. A problem inherent in the approach described above is that the chunks of an input sentence often cannot be found in the example base. Since translating a chunk as a whole is likely to yield a better translation than translating it word by word, it is desirable to increase the chunk coverage of a system. Gough and Way (2003) extended the precursor to Marclator by including an additional layer of abstraction: they produced generalized chunks from word form chunks by replacing the Marker word at the beginning of a word form chunk with the name of its category, e. g., of a marathon → <PREP> a marathon. The generalized template extension is not part of the current Marclator system. We reimplemented it for our experiments. 3.2 CMU-EBMT The second EBMT system which we used for our experiments is CMU-EBMT.7 The system forms 4 http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC96L14 5 Note that both word and"
2011.eamt-1.29,2004.tmi-1.11,1,0.827846,"tor recombination module: in its original form, the recombination module checks for the presence of matching sentences and word form chunks8 before reverting to word-byword translation. We added an additional matching step to follow the chunk matching: in this step, the system replaces the Marker word at the beginning of a chunk by its corresponding Marker tag and searches for the resulting generalized chunk in the example base. Where this attempt fails, the system reverts to word-by-word translation. The only difference remaining to the approach described in Section 3.1 is that the system of Gough and Way (2004) outputs all possible hypotheses for an input sentence, while the Marclator recombinator only outputs the one-best hypothesis. This means that once our system has 8 We subsequently refer to word form chunks (as opposed to generalized chunks) simply as chunks. established a generalized chunk match with the SL side of the example base and has extracted the corresponding TL generalized chunk, it has to make a decision as to which Marker word to insert for the Marker tag. For this, it identifies the SL Marker word underlying the SL generalized chunk that was matched. It gathers the word alignment"
2011.eamt-1.29,W05-0833,1,0.783819,"Translation (CBMT) paradigm. Hence, both SMT and EBMT rely on a sententially aligned bilingual corpus. EBMT systems make use of the parallel corpus by consulting the training set (their example base) directly at runtime. In contrast, SMT systems consult the probabilities of sourcelanguage–target-language (SL–TL) word or phrase pairs which they have learned from the training data offline. Hence, the main feature that distinguishes the two paradigms is the type of knowledge used during the translation step. EBMT systems have often performed worse than SMT systems in the past (cf., for example, Groves and Way (2005)). The biggest shortcoming of EBMT is that it does not combine translations of phrases well. This problem is known as boundary friction (Way, 2001, p. 2). It is particularly frequent when translating into a morphologically rich language. As an example for translating from English into German, assume that the sentence pairs listed in Example 1 are contained in the example base (Way, 2001). (1) A big dog eats a lot of meat. – Ein großer Hund frisst viel Fleisch. I have two ears. – Ich habe zwei Ohren. An EBMT system might make use of the phrases shown in bold to translate a sentence like I have"
2011.eamt-1.29,C92-2101,0,0.269809,"discussion thereof. In Section 5, we give an overview of the issues which we tackled and offer an outlook on future research questions. 2 Related Work When compiling generalized templates, there is a risk of replacing too many parts of an SL–TL pair with variables. To avoid this risk of overgeneralization, generalized templates are usually restricted to certain categories of words. Common candidates for generalization are content words, as replacing them with other content words does not affect the grammar of the sentence. Semantic generalization was explored by Kitamura and Matsumoto (1995). Kaji et al. (1992) applied semantic constraints to their approach to syntactic generalization. Pure syntactic generalization was performed by G¨uvenir and Tunc (1996). Cicekli and G¨uvenir (2001) generalized over sequences of words. The underlying assumption is that given two SL–TL sentence pairs, if the two SL sentences have certain word form sequences in common, the corresponding TL sentences are expected to exhibit the same similarities among each other. The similar parts of the SL sentences are then assumed to be translations of the similar parts of the TL sentences, and the same applies for the differing p"
2011.eamt-1.29,N03-1017,0,0.0135127,"rsity, Dublin 9, Ireland {away,snaskar}@computing.dcu.ie Abstract In this paper, we report our experiments in combining two EBMT systems that rely on generalized templates, Marclator and CMU-EBMT, on an English–German translation task. Our goal was to see whether a statistically significant improvement could be achieved over the individual performances of these two systems. We observed that this was not the case. However, our system consistently outperformed a lexical EBMT baseline system. 1 Introduction The state-of-the-art approach in MT is phrasebased Statistical Machine Translation (SMT) (Koehn et al., 2003). Together with ExampleBased Machine Translation (EBMT) (Nagao, 1984), SMT belongs to the Corpus-Based Machine Translation (CBMT) paradigm. Hence, both SMT and EBMT rely on a sententially aligned bilingual corpus. EBMT systems make use of the parallel corpus by consulting the training set (their example base) directly at runtime. In contrast, SMT systems consult the probabilities of sourcelanguage–target-language (SL–TL) word or phrase pairs which they have learned from the training data offline. Hence, the main feature that distinguishes the two paradigms is the type of knowledge used during"
2011.eamt-1.29,P07-2045,0,0.00906307,"lator OpenMaTrEx Moses 0.1274 0.1269 0.1277 0.0995 0.2763 0.2709 4.3948 4.3815 4.3937 4.2411 5.7880 5.7472 0.4052 0.4047 0.4051 0.3990 0.4914 0.4854 Table 3: Evaluation scores clusive. There are a number of overlaps, i. e., the CMU classes contain 50 words that are also Marker words for English (e. g., after, and, before), and 19 for German (e. g., aber, allen, er). We prompted the system to generalize over the Marker words first, thereby giving preference to the DCU scheme in case of overlaps. Baselines: We established three baselines: Marclator, OpenMaTrEx (Dandapat et al., 2010) and Moses (Koehn et al., 2007). The Marclator baseline was the purely lexical system described in Section 3.1. For the Moses baseline, we used the default system included in OpenMaTrEx. The system uses a 5-gram language model and modified Kneser-Ney smoothing. Training is performed according to the default options and thus includes tuning via MERT (Och, 2003). In addition, a lexicalized reordering model is learnt. The OpenMaTrEx baseline system makes use of EBMT chunk pairs from Marclator and SMT phrase pairs from Moses. We used the default configuration, which includes a 5-gram language model with modified Kneser-Ney smoo"
2011.eamt-1.29,W04-3250,0,0.0609552,"U, while System 1 performed best according to NIST and METEOR. The three systems outperformed the lexical baseline system Marclator according to all three training data English Marker file German Marker file chunking module chunking module English Marker chunks German Marker chunks word alignment module chunk alignment module aligned sentences aligned chunks generalization module aligned generalized chunks IV. III. I. input aligned words II. output recombination module Figure 1: System 1: training and translation process metrics.10 We measured statistical significance by bootstrap resampling (Koehn, 2004) on BLEU.11 The improvement of System 3 over System 2 is statistically significant, while the improvement of System 3 over System 1 is not. The improvements of Systems 1, 2 and 3 over the baseline Marclator system are all significant, as are the improvements of the baseline OpenMaTrEx and Moses system over Systems 1 to 3. 4.3 Chunk Coverage and Chunk-Internal Boundary Friction The evaluation results in Table 3 show that our generalized EBMT systems achieved higher scores than the lexical EBMT system Marclator. This observation supports earlier findings according to which EBMT systems benefit f"
2011.eamt-1.29,J03-1002,0,0.00330683,"ds were ex2 http://www.openmatrex.org/marclator/ marclator.html 3 http://www.apertium.org/ 210 tracted from the Celex database.4 The lists contain a total of 450 Marker words for English and 550 for German. Table 1 lists a sample Marker word for each category. The examples show that entries are included in their inflected forms. Stroppa and Way (2006) found that treating the punctuation marks ! ? , . : ; as additional Marker elements improved performance in their experiments. Following the chunking of the training data, Marclator performs word and chunk alignment. The system relies on Giza++ (Och and Ney, 2003) for word alignment. The chunk alignment algorithm is an edit-distance style algorithm in which the distances are replaced by opposite-log conditional probabilities (Tinsley et al., 2008).5 The recombinator of Marclator is a left-to-right monotone recombinator. When translating an input sentence, it first looks for a matching sentence in the example base. If none is found, the sentence is chunked. Each chunk that is not found in the example base is then split into single words. If several TL correspondences for an SL chunk or word are found in the example base, the one with the highest probabi"
2011.eamt-1.29,P03-1021,0,0.00534394,"an (e. g., aber, allen, er). We prompted the system to generalize over the Marker words first, thereby giving preference to the DCU scheme in case of overlaps. Baselines: We established three baselines: Marclator, OpenMaTrEx (Dandapat et al., 2010) and Moses (Koehn et al., 2007). The Marclator baseline was the purely lexical system described in Section 3.1. For the Moses baseline, we used the default system included in OpenMaTrEx. The system uses a 5-gram language model and modified Kneser-Ney smoothing. Training is performed according to the default options and thus includes tuning via MERT (Och, 2003). In addition, a lexicalized reordering model is learnt. The OpenMaTrEx baseline system makes use of EBMT chunk pairs from Marclator and SMT phrase pairs from Moses. We used the default configuration, which includes a 5-gram language model with modified Kneser-Ney smoothing and tuning via MERT. We included the optional binary feature that records whether a phrase pair is an EBMT chunk pair or not. To train the language models for Moses and OpenMaTrEx, we used the TL side of the training data. 4.2 Results of the MT Systems Table 3 shows the results of our experiments. The best of our systems (S"
2011.eamt-1.29,2006.iwslt-evaluation.4,1,0.926847,"ctic generalization. Category Example determiner personal pronoun demonstrative pronoun possessive pronoun interrogative pronoun indefinite pronoun relative pronoun preposition coordinative conjunction subordinative conjunction cardinal numeral numeric expression auxiliary/modal verb punctuation den euch jenem seine welch andere denen abseits aber falls eins neunundneunzig darf ! Table 1: German Marker categories and examples 3 3.1 Syntactic and Semantic Generalized Templates EBMT at DCU: Marclator Marclator was developed at Dublin City University (DCU) and is part of the MaTrEx architecture (Stroppa and Way, 2006).2 The system does not apply the greedy matching strategy typical of many EBMT systems. Instead, it segments both the training and the test data into chunks. Chunking is based on the Marker Hypothesis (Green, 1979). This is a psycholinguistic hypothesis stating that every language has a closed set of elements that are used to mark certain syntactic constructions. The set of elements includes function words and bound morphemes, such as -ing as an indicator of English progressive-tense verbs and -ly as an indicator of English adverbs. The Marclator chunking module solely considers function words"
2011.eamt-1.29,2001.mtsummit-ebmt.8,1,0.748398,"ting the training set (their example base) directly at runtime. In contrast, SMT systems consult the probabilities of sourcelanguage–target-language (SL–TL) word or phrase pairs which they have learned from the training data offline. Hence, the main feature that distinguishes the two paradigms is the type of knowledge used during the translation step. EBMT systems have often performed worse than SMT systems in the past (cf., for example, Groves and Way (2005)). The biggest shortcoming of EBMT is that it does not combine translations of phrases well. This problem is known as boundary friction (Way, 2001, p. 2). It is particularly frequent when translating into a morphologically rich language. As an example for translating from English into German, assume that the sentence pairs listed in Example 1 are contained in the example base (Way, 2001). (1) A big dog eats a lot of meat. – Ein großer Hund frisst viel Fleisch. I have two ears. – Ich habe zwei Ohren. An EBMT system might make use of the phrases shown in bold to translate a sentence like I have a big dog. into Ich habe ein großer Hund. In doing so, it would neglect the fact that German uses different inflectional forms to mark grammatical"
2011.eamt-1.29,C96-1030,0,\N,Missing
2011.eamt-1.29,W08-0326,1,\N,Missing
2011.eamt-1.29,W10-1720,1,\N,Missing
2011.eamt-1.4,W05-0909,0,0.233131,"Missing"
2011.eamt-1.4,E06-1032,0,0.0374124,"ystems over rule-based ones (CallisonBurch et al., 2006). Using BLEU is fast and intuitive, but while this metric has been shown to produce good correlations with human judgment at the document level (Papineni et al., 2002), especially when a large number of reference translations are available, correlation at sentence level is generally low. The NIST evaluation metric (Doddington, 2002) is also string-based, and gives more weight in the evaluation to less frequent n-grams. While this metric has a strong bias in favour of statistical systems, it provides better adequacy correlation than BLEU (Callison-Burch et al., 2006). The GTM metric (Turian et al., 2003) is based on standard measures adopted in other NLP applications (precision, recall and F-measure), which makes its use rather straightforward for NLP practitioners. It focuses on unigrams and rewards sequences of correct unigrams, applying moderate penalties for incorrect word order. METEOR (Banerjee and Lavie, 2005) uses stemming and synonymy relations to provide a 14 more fine-grained evaluation at the lexical level, which reduces its bias towards statistical systems. One drawback of this metric is that it is language-dependent since it requires a stemm"
2011.eamt-1.4,niessen-etal-2000-evaluation,0,0.060211,"r et al., 2006) adopts a different approach, in that it computes the number of substitutions, insertions, deletions and shifts that are required to modify the output translation so that it completely matches the reference translation(s). Its results are affected less by the number of reference translations than is the case for BLEU, and the rationale behind this evaluation metric is quite simple to understand for people who are not MT experts, as it provides an estimation of the amount of post-editing effort needed by an end-user. Another metric based on error rates which preceded TER is WER (Nießen et al., 2000). We omitted WER and its extension mWER (Nießen et al., 2000) from the experiments reported here as they seem to have been superceded by more recent metrics. TER-plus (Snover et al., 2009) is an extension of TER using phrasal substitutions relying on automatically generated paraphrases, stemming, synonyms and relaxed shifting constraints. This metric is language-dependent and requires WordNet. It has been shown to have the highest average rank in terms of Pearson and Spearman correlation (Przybocki et al., 2008). The DCU-LFG metric (Owczarzak et al., 2007) exploits LFG dependencies and has onl"
2011.eamt-1.4,W07-0718,0,0.0281462,"verview of some of the most widely used automatic MT evaluation metrics, discussing their advantages as well as drawbacks, laying particular emphasis on the metrics used in the comparative evaluation presented in Section 5. The performance of the CoSyne MT system in the early stages of its development can be measured, and its improvement can be monitored over time, against these standard metrics in a reliable and replicable fashion. To ensure the best possible coverage, we decided to use a wide array of metrics, particularly those judged best by recent meta-evaluation exercises (e.g. Callison-Burch et al., 2007; Callison-Burch et al., 2010), without confining ourselves to prominent n-gram based metrics. Since there is no consensus on a single individual metric which is thought to accurately measure MT performance, we decided to adopt an inclusive approach, considering the results of a variety of measures. This should provide a picture that is as reliable and fine-grained as possible. One of the most widely used automatic MT evaluation metrics is BLEU (Papineni et al., 2002), a string-based metric which has come to represent something of a de facto standard in the last few years. This is not surprisi"
2011.eamt-1.4,W07-0411,1,0.87845,"Missing"
2011.eamt-1.4,P02-1040,0,0.0893689,"erage, we decided to use a wide array of metrics, particularly those judged best by recent meta-evaluation exercises (e.g. Callison-Burch et al., 2007; Callison-Burch et al., 2010), without confining ourselves to prominent n-gram based metrics. Since there is no consensus on a single individual metric which is thought to accurately measure MT performance, we decided to adopt an inclusive approach, considering the results of a variety of measures. This should provide a picture that is as reliable and fine-grained as possible. One of the most widely used automatic MT evaluation metrics is BLEU (Papineni et al., 2002), a string-based metric which has come to represent something of a de facto standard in the last few years. This is not surprising given that today most MT research and development efforts are concentrated on statistical approaches; BLEU’s critics argue that it tends to favour statistical systems over rule-based ones (CallisonBurch et al., 2006). Using BLEU is fast and intuitive, but while this metric has been shown to produce good correlations with human judgment at the document level (Papineni et al., 2002), especially when a large number of reference translations are available, correlation"
2011.eamt-1.4,W10-1751,0,0.018734,"tioners. It focuses on unigrams and rewards sequences of correct unigrams, applying moderate penalties for incorrect word order. METEOR (Banerjee and Lavie, 2005) uses stemming and synonymy relations to provide a 14 more fine-grained evaluation at the lexical level, which reduces its bias towards statistical systems. One drawback of this metric is that it is language-dependent since it requires a stemmer and WordNet,3 and it can currently be applied in full only to English, and partly to French, Spanish and Czech, due to the limited availability of synonymy and paraphrase modules. METEORNEXT (Denkowski and Lavie, 2010) is an updated version of the same metric. The TER metric (Snover et al., 2006) adopts a different approach, in that it computes the number of substitutions, insertions, deletions and shifts that are required to modify the output translation so that it completely matches the reference translation(s). Its results are affected less by the number of reference translations than is the case for BLEU, and the rationale behind this evaluation metric is quite simple to understand for people who are not MT experts, as it provides an estimation of the amount of post-editing effort needed by an end-user."
2011.eamt-1.4,2006.amta-papers.25,0,0.0482843,"oderate penalties for incorrect word order. METEOR (Banerjee and Lavie, 2005) uses stemming and synonymy relations to provide a 14 more fine-grained evaluation at the lexical level, which reduces its bias towards statistical systems. One drawback of this metric is that it is language-dependent since it requires a stemmer and WordNet,3 and it can currently be applied in full only to English, and partly to French, Spanish and Czech, due to the limited availability of synonymy and paraphrase modules. METEORNEXT (Denkowski and Lavie, 2010) is an updated version of the same metric. The TER metric (Snover et al., 2006) adopts a different approach, in that it computes the number of substitutions, insertions, deletions and shifts that are required to modify the output translation so that it completely matches the reference translation(s). Its results are affected less by the number of reference translations than is the case for BLEU, and the rationale behind this evaluation metric is quite simple to understand for people who are not MT experts, as it provides an estimation of the amount of post-editing effort needed by an end-user. Another metric based on error rates which preceded TER is WER (Nießen et al.,"
2011.eamt-1.4,W09-0441,0,0.0132577,"completely matches the reference translation(s). Its results are affected less by the number of reference translations than is the case for BLEU, and the rationale behind this evaluation metric is quite simple to understand for people who are not MT experts, as it provides an estimation of the amount of post-editing effort needed by an end-user. Another metric based on error rates which preceded TER is WER (Nießen et al., 2000). We omitted WER and its extension mWER (Nießen et al., 2000) from the experiments reported here as they seem to have been superceded by more recent metrics. TER-plus (Snover et al., 2009) is an extension of TER using phrasal substitutions relying on automatically generated paraphrases, stemming, synonyms and relaxed shifting constraints. This metric is language-dependent and requires WordNet. It has been shown to have the highest average rank in terms of Pearson and Spearman correlation (Przybocki et al., 2008). The DCU-LFG metric (Owczarzak et al., 2007) exploits LFG dependencies and has only a moderate bias towards statistical systems. It requires a dependency parser. It should be noted that among the above measures, METEOR, METEOR-NEXT, TER-plus and DCU-LFG can only be used"
2011.eamt-1.4,2007.mtsummit-papers.27,1,0.605945,"our free online MT systems were used for the baseline evaluation of the CoSyne MT system developed by the University of Amsterdam (Martzoukos and Monz, 2010): • Google Translate13 • Bing Translator14 • Systran15 • FreeTranslation16 These four online MT services were selected first of all because they all cover the three language pairs addressed in year 1 of the CoSyne project (German—English, Italian—English and Dutch—English in both directions). In addition, these are among the most popular free web-based MT systems and are heavily used by the general public of Internet users (Gaspari, 2006; Gaspari and Hutchins, 2007). A final consideration was that three of these five systems are statistical (CoSyne, Google Translate and Bing Translator), while the other two are rule-based (FreeTranslation and Systran). As a result, this mixture of systems offers a good picture of the MT quality currently offered by state-of-the-art representatives of both approaches. 4 Dutch—English NISV provided three different data sets: België Diplomatie consists of 418 HTML document pairs extracted from the Belgian Foreign Affairs website.9 • Video Active is an XML file containing 1,076 document pairs concerning the description of te"
2011.eamt-1.4,2003.mtsummit-papers.51,0,0.0834814,"Missing"
2011.eamt-1.4,W10-1753,1,0.874768,"Missing"
2011.eamt-1.4,C08-1141,0,0.0128615,"and/or language directions needing improvement. By repeating evaluations based on the well-established metrics presented in Section 2 at regular intervals, the improvement of the CoSyne MT system will be gradually monitored and its overall success measured. This evaluation study has shown that rulebased MT systems are outperformed by statistical MT systems for data from the news domain. Plans currently underway to extend the evaluation of the CoSyne MT system include the development of a methodology for diagnostic MT evaluation based on linguistic checkpoints, similar to the one presented in Zhou et al. (2008), who used an ad-hoc tool called Woodpecker. 0.9000 0.8000 0.7000 0.6000 Google Bing Systran Freetranslation CoSyne M12 0.5000 0.4000 0.3000 0.2000 0.1000 0.0000 BLEU NIST METEOR METEOR-NEXT TERp TER GTM DCU-LFG For the Dutch—English translation task, the three statistical MT systems consistently and clearly outperform Systran and FreeTranslation based on all the automatic evaluation metrics. Google outperforms Bing for only three of the metrics (NIST, TER and GTM), whereas for the others Bing receives the higher score. Interestingly, based on TER, the CoSyne MT system does better than Bing, b"
2011.eamt-1.4,P04-1077,0,0.0745498,"Missing"
2011.eamt-1.4,W10-1703,0,\N,Missing
2011.eamt-1.4,2010.iwslt-evaluation.28,0,\N,Missing
2011.eamt-1.4,W07-0700,0,\N,Missing
2011.iwslt-evaluation.4,2011.iwslt-evaluation.1,0,0.040356,"Missing"
2011.iwslt-evaluation.4,N03-1017,0,0.236145,"provides a brief description of the different SMT models and adaptation techniques used in our experiments. Section 3 details our experimental setup with descriptions on the specific toolsets and data used. Section 4 provides the results of each set of experiments as well as analyses, followed by conclusion and future work in Section 5. 2. Translation Systems This section focuses on the different translation techniques used in the experiments. 2 http://www.ted.com/talks 1 http://iwslt2011.org 3 http://www.euromatrixplus.eu/downloads/35 41 2.1. Phrase-based SMT Systems Phrase-based SMT systems [2] are the most commonly used technique in statistical machine translation nowadays. In this approach, source and target phrase pairs consistent with the word alignment are extracted from the parallel training data. Phrases in PBSMT are just contiguous chunks of text, and are not linguistically motivated. The extracted source-target phrase pairs along with their translation probabilities (computed from the same training data) are stored in a structure known as the ‘phrase table’. During translation, an input sentence is split up into phrases and their corresponding translations are looked up fro"
2011.iwslt-evaluation.4,P05-1033,0,0.443143,"used for adapting the translation model in SMT with limited success [9]. For the given task, since the size of the ‘in-domain’ data was not significantly large, we used ‘suitable’ subsets of data from the other available ‘out-ofdomain’ corpora to enrich the models. For a mixture adapted language model, the probability of an n-gram hw is given as in ( 2): ∗ ¯ P rmix (w|h) = fmix (w|h) + λmix (h)P rmix (w|h) (2) where w is the current word, h is the corresponding ∗ history, fmix is the mixture model discounted relative fre2.3. Hierarchical Phrase-Based System Hierarchical Phrase-Based (HPB) SMT [3] is a tree-based model which extracts a synchronous Context-Free Grammar (CFG) automatically from the training corpus. HPB SMT is based on phrases extracted according to the PB model [2]. Thus, HPB SMT tries to build upon the strengths of PB SMT and adds to it the ability to translate discontinuous phrases and learn phrase-reordering in hierarchical rules without a separate reordering model. HPB SMT uses hierarchical rules as a translation unit. These rules are rewrite rules with aligned pairs of right-hand sides, taking the following form: X →&lt; α, β, ∼&gt; (7) where X is a non-terminal, α and β"
2011.iwslt-evaluation.4,P07-2045,0,0.0348805,"n the training corpora. This mixture model was used to combine the ‘in-domain’ language model with an ‘out-of-domain’ one, with the mixture weights being estimated on the ‘in-domain’ training data by applying a cross-validation scheme. Further improvements on this mixture models were achieved using parameter tying to the most-recent context words [4]. i=1 where, hi (f, e) denotes the different components for translating the source sentence f into the target sentence e. K is the number of components (or features) used and λi are the corresponding weights of the components. The Moses SMT system [6], which implements this particular model, was used for all our PBSMT translation experiments. Different component weights (λi ) were estimated using a discriminative training method known as Minimum Error Rate Training (MERT) [7], on a held out development set (devset). 2.2. Mixture Adaptation of Language Models Mixture Modelling [8], a well-established technique for combining multiple models, has been extensively used for language model adaptation in SMT [4]. This technique has also been used for adapting the translation model in SMT with limited success [9]. For the given task, since the siz"
2011.iwslt-evaluation.4,P03-1021,0,0.0301456,"cheme. Further improvements on this mixture models were achieved using parameter tying to the most-recent context words [4]. i=1 where, hi (f, e) denotes the different components for translating the source sentence f into the target sentence e. K is the number of components (or features) used and λi are the corresponding weights of the components. The Moses SMT system [6], which implements this particular model, was used for all our PBSMT translation experiments. Different component weights (λi ) were estimated using a discriminative training method known as Minimum Error Rate Training (MERT) [7], on a held out development set (devset). 2.2. Mixture Adaptation of Language Models Mixture Modelling [8], a well-established technique for combining multiple models, has been extensively used for language model adaptation in SMT [4]. This technique has also been used for adapting the translation model in SMT with limited success [9]. For the given task, since the size of the ‘in-domain’ data was not significantly large, we used ‘suitable’ subsets of data from the other available ‘out-ofdomain’ corpora to enrich the models. For a mixture adapted language model, the probability of an n-gram hw"
2011.iwslt-evaluation.4,J03-1002,0,0.00250058,"ammaticality of the output. Our experiments will show the effects of this trade-off between label accuracy and sparsity. 3. Experimental Setups This section details the setup for the different experiments. We also provide a brief account of the different tools and datasets used along with the preprocessing and postprocessing procedures employed. 3.1. Tools and Datasets For our PBSMT-based translation experiments we used OpenMaTrEx [15], an open source SMT system which provides a wrapper around the standard log-linear phrase-based SMT system Moses [6]. Word alignment was performed using Giza++ [16]. The phrase and the reordering tables were built on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (MERT) [7] on the devset with respect to BLEU [17]. We used 5-gram language models in all our experiments created using the IRSTLM language modelling toolkit [18] using Modified Kneser-Ney smoothing [19]. Mixture adaptation of language models mentioned in Section 2.2 was also performed using the features of the IRSTLM toolkit. Results of translations in every phase of ou"
2011.iwslt-evaluation.4,P02-1040,0,0.0815708,"d along with the preprocessing and postprocessing procedures employed. 3.1. Tools and Datasets For our PBSMT-based translation experiments we used OpenMaTrEx [15], an open source SMT system which provides a wrapper around the standard log-linear phrase-based SMT system Moses [6]. Word alignment was performed using Giza++ [16]. The phrase and the reordering tables were built on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (MERT) [7] on the devset with respect to BLEU [17]. We used 5-gram language models in all our experiments created using the IRSTLM language modelling toolkit [18] using Modified Kneser-Ney smoothing [19]. Mixture adaptation of language models mentioned in Section 2.2 was also performed using the features of the IRSTLM toolkit. Results of translations in every phase of our experiments were evaluated using BLEU, METEOR [20] and TER [21] metrics. Table 1: Number of Sentences for bilingual and monolingual data sets Data Set TED parallel Multi-UN Development Set Test Set TED Monolingual Multi-UN Monolingual Ar–En Zh–En 90,379 106,776 5,231,931 5,6"
2011.iwslt-evaluation.4,2011.mtsummit-papers.32,1,0.751983,"e components. The Moses SMT system [6], which implements this particular model, was used for all our PBSMT translation experiments. Different component weights (λi ) were estimated using a discriminative training method known as Minimum Error Rate Training (MERT) [7], on a held out development set (devset). 2.2. Mixture Adaptation of Language Models Mixture Modelling [8], a well-established technique for combining multiple models, has been extensively used for language model adaptation in SMT [4]. This technique has also been used for adapting the translation model in SMT with limited success [9]. For the given task, since the size of the ‘in-domain’ data was not significantly large, we used ‘suitable’ subsets of data from the other available ‘out-ofdomain’ corpora to enrich the models. For a mixture adapted language model, the probability of an n-gram hw is given as in ( 2): ∗ ¯ P rmix (w|h) = fmix (w|h) + λmix (h)P rmix (w|h) (2) where w is the current word, h is the corresponding ∗ history, fmix is the mixture model discounted relative fre2.3. Hierarchical Phrase-Based System Hierarchical Phrase-Based (HPB) SMT [3] is a tree-based model which extracts a synchronous Context-Free Gra"
2011.iwslt-evaluation.4,W07-0734,0,0.068226,"t on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (MERT) [7] on the devset with respect to BLEU [17]. We used 5-gram language models in all our experiments created using the IRSTLM language modelling toolkit [18] using Modified Kneser-Ney smoothing [19]. Mixture adaptation of language models mentioned in Section 2.2 was also performed using the features of the IRSTLM toolkit. Results of translations in every phase of our experiments were evaluated using BLEU, METEOR [20] and TER [21] metrics. Table 1: Number of Sentences for bilingual and monolingual data sets Data Set TED parallel Multi-UN Development Set Test Set TED Monolingual Multi-UN Monolingual Ar–En Zh–En 90,379 106,776 5,231,931 5,624,637 934 934 1,664 1,664 125,948 5,796,505 The datasets used for the experiments included the specific datasets released by the IWSLT 2011 evaluation campaign. The primary bi-lingual training data comprised of a collection of public speech transcriptions on a variety of topics from TED Talks. The development data released for the task, comprised of both the IWSLT-20104 d"
2011.iwslt-evaluation.4,2006.amta-papers.25,0,0.0278312,"alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (MERT) [7] on the devset with respect to BLEU [17]. We used 5-gram language models in all our experiments created using the IRSTLM language modelling toolkit [18] using Modified Kneser-Ney smoothing [19]. Mixture adaptation of language models mentioned in Section 2.2 was also performed using the features of the IRSTLM toolkit. Results of translations in every phase of our experiments were evaluated using BLEU, METEOR [20] and TER [21] metrics. Table 1: Number of Sentences for bilingual and monolingual data sets Data Set TED parallel Multi-UN Development Set Test Set TED Monolingual Multi-UN Monolingual Ar–En Zh–En 90,379 106,776 5,231,931 5,624,637 934 934 1,664 1,664 125,948 5,796,505 The datasets used for the experiments included the specific datasets released by the IWSLT 2011 evaluation campaign. The primary bi-lingual training data comprised of a collection of public speech transcriptions on a variety of topics from TED Talks. The development data released for the task, comprised of both the IWSLT-20104 development an"
2011.iwslt-evaluation.4,W06-3119,0,0.0256821,"sted in the translation table recursively from longer phrases and replacing them with the non-terminal symbol X. Non-terminals in hierarchical rules act as placeholders that are replaced with other phrases during translation in a bottom-up fashion. Hierarchical rules are extracted from the training corpus without using any syntactic information. As the resulting system is syntactically unaware, the HPB SMT system can produce ungrammatical translations. Therefore, several approaches have tried to provide the HPB SMT system with syntactic information. Syntax augmented Machine Translation (SAMT) [11] uses target-side phrase-structure grammar syntactic trees to label non-terminals in hierarchical rules. These non-terminal labels represent syntactic constraints imposed on target phrase replacements during translation aiming to produce more grammatical translations. 2.4. CCG-augmented HPB System Following the SAMT approach, CCG-augmented HPB SMT [12] uses CCG [5] to label non-terminals. CCG has distinct advantages over phrase-structure grammar in the general SMT context, particularly in extracting non-terminal labels in HPB SMT. This section gives a brief introduction to CCG followed by a de"
2011.iwslt-evaluation.4,2010.iwslt-papers.1,1,0.902746,"resulting system is syntactically unaware, the HPB SMT system can produce ungrammatical translations. Therefore, several approaches have tried to provide the HPB SMT system with syntactic information. Syntax augmented Machine Translation (SAMT) [11] uses target-side phrase-structure grammar syntactic trees to label non-terminals in hierarchical rules. These non-terminal labels represent syntactic constraints imposed on target phrase replacements during translation aiming to produce more grammatical translations. 2.4. CCG-augmented HPB System Following the SAMT approach, CCG-augmented HPB SMT [12] uses CCG [5] to label non-terminals. CCG has distinct advantages over phrase-structure grammar in the general SMT context, particularly in extracting non-terminal labels in HPB SMT. This section gives a brief introduction to CCG followed by a description of the approach of extracting non-terminal labels using the same. 2.4.1. Combinatory Categorial Grammar CCG [5] is a grammar formalism which consists of a lexicon that pairs words with lexical categories (supertags) and a set of combinatory rules which specify how the categories are combined. A supertag is a rich syntactic description that sp"
2011.iwslt-evaluation.4,J99-2004,0,0.024692,"s in using statistically extracted phrases which do not necessarily correspond to syntactic constituents. Secondly, CCG categories reflect rich information about the syntactic structure to which the word/phrase belongs at the lexical level without the need to build a full parse tree for the sentence. Thirdly, CCG parsing is more efficient in comparison to phrase-structure grammar parsing. Because most of the CCG grammar is contained in the lexicon, the process of supertagging, which is to assign supertags (i.e. complex CCG categories) to the words in a sentence, is considered “almost parsing” [13]. After supertagging, the CCG parser is only required to combine the supertags using CCG simple combinatory operators. For the aforementioned reasons, CCG is considered more suitable to be used in SMT than phrase-structure grammar. Attaching CCG categories to non-terminals in hierarchical rules is done in a way similar to that of SAMT approach: • First, each target-side sentence from the parallel corpus is supertagged by assigning the best sequence of CCG supertags to its words. • Next, phrase pairs are extracted from the parallel corpus according to the PBSMT phrase extraction method [2]. • T"
2011.iwslt-evaluation.4,2011.eamt-1.38,1,0.804863,"thermore, some atomic CCG categories have features expressed between brackets which describe certain syntactic information. For example, the atomic category S might have a feature attached to it which distinguishes types of sentences such as declarative S[dcl] or wh-question S[wq]. All the additional information represented in a single CCG category increases the number of different CCG categories and leads to label sparsity problem. In order to address this problem, we simplify CCG non-terminal labels by reducing the amount of the information represented in them using the following approaches [14]: • Feature-dropped CCG labels: these labels are extracted from CCG categories by dropping the syntactic features attached to atomic categories from the label representation. For example, if a phrase has a CCG category S[dcl]/NP, then its feature-dropped CCG label is S/NP. • CCG Contextual Labels: in a CCG contextual label, only left and right argument categories are used in the label representation whereas the resulting category (i.e. the functor) is dropped from the label representation. The resulting CCG contextual label takes the form L R. If any of the argument categories is missing, an X"
2011.iwslt-evaluation.4,Y09-2027,1,0.83049,"we perform case restoration and detokenization for the English data. Case restoration, or truecasing is treated as a translation task. A simple phrase-based translation model is trained on aligned lower-case and truecase data to successfully achieve the task of true-casing. 3.3. PBSMT based Language Model Adaptation Experiments As shown in Table 1, the size of the ‘in-domain’ TED training data is much smaller than the ‘out-of-domain’ Multi-UN training data. Since adding a significant amount of out-ofdomain data to an in-domain corpus reduces the quality of translation for in-domain sentences [23], we decided to use only a part of the out-of-domain data to enhance the translation quality. In order to achieve this, we constructed a language model on the TED monolingual data and computed sentence-level perplexity score for all the sentences in MultiUN, with respect to the TED language model. After sorting the sentences in the ascending order of the perplexity values, only sentences below a specific threshold were selected. This method provided us with the most ‘TED-like’ sentences from the Multi-UN corpora. In order to decide which specific threshold gives us the best possible translatio"
2011.iwslt-evaluation.4,2009.iwslt-papers.4,0,0.0154921,"ents Figure 1 shows the variation of BLEU scores for different adapted language models pertaining to different thresholds. According to our experiments, the best cut-off thresholds were 43.00 and 53.00 for Zh–En and Ar–En language pairs, respectively. For Ar–En language pair, the best BLEU 45 score is achieved for multiple thresholds, and we select the one with the maximum number of sentences in it. The number of Multi-UN sentences thus selected were 55,841 and 89,310 for Zh–En and Ar–En language pairs, respectively. 3.4. HPB Experiments We built our HPB baseline using the Moses Chart Decoder [24]. Continuous phrases are extracted according to the phrase based system settings explained in Section 3.1. Maximum phrase length and maximum rule span are both set to 12 words. The maximum span for the chart during decoding is set to 20 words, above which only monotone concatenation of phrases is used. Rules extracted contain up to 2 non-terminals. Adjacent non-terminals on the source side are not allowed. 3.5. CCG-augmented HPB Experiments We built our CCG-augmented HPB system using the Moses Chart Decoder, which has an option to extract syntaxaugmented rules from an annotated corpus. We used"
2011.iwslt-evaluation.4,W04-3250,0,0.160582,"Missing"
2011.mtsummit-papers.32,N09-1025,0,0.0602809,"Missing"
2011.mtsummit-papers.32,W07-0722,0,0.0178866,"oved translation performance with respect to a baseline system. Wu et al. (2008) used a combination of in-domain bilingual dictionaries and monolingual data to perform domain adaptation for SMT in a setting where in-domain bilingual data was absent. Integrating an in-domain language model with an out-of-domain one using log-linear features of a phrase-based SMT system is reported by Koehn and Schroeder (2007). Foster and Kuhn (2007) used mixture modelling to combine multiple models trained on different sources and learn mixture weights based on distance of the test set from the training data. Civera and Juan (2007) further suggested a mixture adaptation approach to word alignment, generating domainspeciﬁc Viterbi alignments to feed a state-of-the-art phrase-based SMT system. Our work follows the line of research presented in Foster and Kuhn (2007) using mixture modelling and linear/log-linear combination frameworks, but differs in terms of the test set and development sets used for tuning and evaluation. While Foster and Kuhn (2007) used test and development sets which were essentially a combination of data from different training genres, in our case test data (user forum) are inherently different from"
2011.mtsummit-papers.32,eck-etal-2004-language,0,0.126201,"tensively used for language model adaptation, especially in speech recognition. Iyer and Ostendorf (1996) use this technique to capture topic dependencies of words across sentences within language models. Cache-based language models (Kuhn and De Mori, 1990) and dynamic adaptation of language models (Kneser and Steinbiss, 1993) for speech recognition successfully use this technique for sub-model combinations. Langlais (2002) introduced the concept of domain adaptation in SMT by integrating domain-speciﬁc lexicons in the translation model, resulting in signiﬁcant improvement in Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Hildebrand (2005) utilized this approach to select similar sentences from available training data to adapt translation models, which improved translation performance with respect to a baseline system. Wu et al. (2008) used a combination of in-domain bilingual dictionaries and monolingual data to perform domain adaptation for SMT in a setting where in-domain bilingual data was absent. Integrating an in-domain language model with an out-of-domain one using log-linear features of a phrase-based SMT s"
2011.mtsummit-papers.32,2010.amta-commercial.5,0,0.224215,"Introduction In recent years, Statistical Machine Translation (SMT) technology has been used in many online applications, concentrating on professionally edited enterprise quality online content. At the same time, very little research has gone into adapting ∗ Work done while at CNGL, School of Computing, DCU 285 SMT technology to the translation of user-generated content on the web. While translation of online chats (Flournoy and Callison-Burch, 2000) has received some attention, there is surprisingly little work on translation of online user forum data, despite growing interest in the area (Flournoy and Rueppel, 2010). In this paper we describe our efforts in building a system to address this particular application area. Our experiments are conducted on data collected from online forums on Symantec Security tools and services.1 For a multinational company like Symantec, the primary motivation behind translation of user forum data is to enable access across language barriers to information in the forums. Forum posts are rich in information about issues and problems with tools and services provided by the company, and often provide solutions to problems even before traditional customer-care help lines are ev"
2011.mtsummit-papers.32,W04-3250,0,0.0274837,"Missing"
2011.mtsummit-papers.32,2005.mtsummit-papers.11,0,0.0155595,"ms with tools and services provided by the company, and often provide solutions to problems even before traditional customer-care help lines are even aware of them. The major challenge in developing MT systems for user forum data concerns the lack of proper parallel training material. Forum data is monolingual and hence cannot be used directly to train SMT systems. We use parallel training data in the form of Symantec Enterprise Translation Memories (TMs) from different product and service domains to train the SMT models. As an auxiliary source, we also used portions of the Europarl dataset2 (Koehn, 2005), selected according to their similarity with the forum data (Section 3.2), to supplement the TMbased training data. Symantec TM data, being a part of enterprise documentation, is professionally 1 2 http://community.norton.com/ http://www.statmt.org/europarl/ edited and by and large conforms to the Symantec controlled language guidelines, and is signiﬁcantly different in nature from the user forum data, which is loosely moderated and does not use controlled language at all. In contrast Europarl data is outof-domain with respect to the forum data. The differences between available training and"
2011.mtsummit-papers.32,P07-2045,0,0.0156353,"Missing"
2011.mtsummit-papers.32,W07-0733,0,0.0321071,"retrieval theories to propose a language model adaptation technique in SMT. Hildebrand (2005) utilized this approach to select similar sentences from available training data to adapt translation models, which improved translation performance with respect to a baseline system. Wu et al. (2008) used a combination of in-domain bilingual dictionaries and monolingual data to perform domain adaptation for SMT in a setting where in-domain bilingual data was absent. Integrating an in-domain language model with an out-of-domain one using log-linear features of a phrase-based SMT system is reported by Koehn and Schroeder (2007). Foster and Kuhn (2007) used mixture modelling to combine multiple models trained on different sources and learn mixture weights based on distance of the test set from the training data. Civera and Juan (2007) further suggested a mixture adaptation approach to word alignment, generating domainspeciﬁc Viterbi alignments to feed a state-of-the-art phrase-based SMT system. Our work follows the line of research presented in Foster and Kuhn (2007) using mixture modelling and linear/log-linear combination frameworks, but differs in terms of the test set and development sets used for tuning and eval"
2011.mtsummit-papers.32,W02-1405,0,0.0297662,"ts, followed by conclusions and future work in Section 6. 2 Related Work Mixture Modelling (Hastie et al., 2001), a wellestablished technique for combining multiple mod286 els, has been extensively used for language model adaptation, especially in speech recognition. Iyer and Ostendorf (1996) use this technique to capture topic dependencies of words across sentences within language models. Cache-based language models (Kuhn and De Mori, 1990) and dynamic adaptation of language models (Kneser and Steinbiss, 1993) for speech recognition successfully use this technique for sub-model combinations. Langlais (2002) introduced the concept of domain adaptation in SMT by integrating domain-speciﬁc lexicons in the translation model, resulting in signiﬁcant improvement in Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Hildebrand (2005) utilized this approach to select similar sentences from available training data to adapt translation models, which improved translation performance with respect to a baseline system. Wu et al. (2008) used a combination of in-domain bilingual dictionaries and monolingual data to perform domain"
2011.mtsummit-papers.32,J03-1002,0,0.00718605,"Missing"
2011.mtsummit-papers.32,P03-1021,0,0.0354487,"Missing"
2011.mtsummit-papers.32,C08-1125,0,0.0125168,"nbiss, 1993) for speech recognition successfully use this technique for sub-model combinations. Langlais (2002) introduced the concept of domain adaptation in SMT by integrating domain-speciﬁc lexicons in the translation model, resulting in signiﬁcant improvement in Word Error Rate. Eck et al. (2004) utilized information retrieval theories to propose a language model adaptation technique in SMT. Hildebrand (2005) utilized this approach to select similar sentences from available training data to adapt translation models, which improved translation performance with respect to a baseline system. Wu et al. (2008) used a combination of in-domain bilingual dictionaries and monolingual data to perform domain adaptation for SMT in a setting where in-domain bilingual data was absent. Integrating an in-domain language model with an out-of-domain one using log-linear features of a phrase-based SMT system is reported by Koehn and Schroeder (2007). Foster and Kuhn (2007) used mixture modelling to combine multiple models trained on different sources and learn mixture weights based on distance of the test set from the training data. Civera and Juan (2007) further suggested a mixture adaptation approach to word a"
2011.mtsummit-papers.60,ruimy-etal-2002-clips,0,0.0411229,"Missing"
2011.mtsummit-papers.60,2011.eamt-1.36,0,0.0501854,"rors based on the use of morpho-syntactic information, which shows that their linguistically-informed evaluation measures provide useful insights to understand the weaknesses of their MT system, while also indicating the best ways and methods to take remedial action. Popoviü and Ney (2007) propose a method to zoom in on translation errors involving different Part-of-Speech (PoS) classes in the output. They apply this method to the estimation of inﬂectional errors and to the distribution of missing targetlanguage words over PoS classes. Following the hierarchy proposed in (Vilar et al., 2006), Popoviü and Burchardt (2011) present a tool that classifies errors into five categories. Parton and McKeown (2010) describe a novel algorithm to detect MT errors, focusing specifically on content words that are deleted. Xiong et al. (2010) attempt to automatically detect incorrect segments Linguistic Checkpoints-based Diagnostic Evaluation In this section, we first give an overview of linguistic checkpoints and then detail the evaluation framework and the key components of the system. 3.1 Linguistic Checkpoints A linguistic checkpoint can be defined as a linguistically-motivated unit, (e.g. an ambiguous word, a verb-obje"
2011.mtsummit-papers.60,W06-3101,0,0.421307,"Missing"
2011.mtsummit-papers.60,W07-0707,0,0.0460502,"word order in translation from Chinese into English. Farrús et al. (2011) carry out a manual error analysis on an MT system for Spanish— Catalan and classify the errors into linguistic levels (orthographic, morphological, lexical, semantic, and syntactic). Popoviü et al. (2006) adopt a framework for the automatic analysis of MT errors based on the use of morpho-syntactic information, which shows that their linguistically-informed evaluation measures provide useful insights to understand the weaknesses of their MT system, while also indicating the best ways and methods to take remedial action. Popoviü and Ney (2007) propose a method to zoom in on translation errors involving different Part-of-Speech (PoS) classes in the output. They apply this method to the estimation of inﬂectional errors and to the distribution of missing targetlanguage words over PoS classes. Following the hierarchy proposed in (Vilar et al., 2006), Popoviü and Burchardt (2011) present a tool that classifies errors into five categories. Parton and McKeown (2010) describe a novel algorithm to detect MT errors, focusing specifically on content words that are deleted. Xiong et al. (2010) attempt to automatically detect incorrect segments"
2011.mtsummit-papers.60,vilar-etal-2006-error,0,0.183659,"Missing"
2011.mtsummit-papers.60,W10-3301,0,0.114671,"ord alignment. This is an XML format for linguistic analysis (e.g., PoS tagging, parsing, etc.) and alignment (sentence/word) based on XCES. 8 Scripts to convert the output of well-established tools (GIZA++, Treetagger, etc.) are available. (iii) It uses the KYOTO Annotation Format (KAF) (Bosma et al., 2009), established in the FP7 KYOTO project, 9 to represent textual analysis. KAF represents each level of linguistic analysis based on ISO standards (i.e. MAF, SynAF, SemAF) and it is compatible with the Linguistic Annotation Framework (LAF) (Ide and Romary, 2003). (iv) It makes use of Kybots (Vossen et al., 2010), established in the FP7 KYOTO project, to define the evaluation targets (linguistic check3 http://research.microsoft.com/en-us/downloads/ad240799a9a7-4a14-a556-d6a7c7919b4a/MSR%20noncommercial%20license%20agreement.txt 4 http://www.computing.dcu.ie/~atoral/delic4mt (under the license GPL-v3). 5 http://www.ims.unistuttgart.de/projekte/corplex/TreeTagger/ 6 http://code.google.com/p/giza-pp/ 7 http://panacea-lr.eu/ 8 http://www.xces.org/ 9 http://www.kyoto-project.eu/ points). A Kybot profile can be thought of as a regular expression over elements and attributes in KAF documents. The benefits of"
2011.mtsummit-papers.60,P10-1062,0,0.0464647,"best ways and methods to take remedial action. Popoviü and Ney (2007) propose a method to zoom in on translation errors involving different Part-of-Speech (PoS) classes in the output. They apply this method to the estimation of inﬂectional errors and to the distribution of missing targetlanguage words over PoS classes. Following the hierarchy proposed in (Vilar et al., 2006), Popoviü and Burchardt (2011) present a tool that classifies errors into five categories. Parton and McKeown (2010) describe a novel algorithm to detect MT errors, focusing specifically on content words that are deleted. Xiong et al. (2010) attempt to automatically detect incorrect segments Linguistic Checkpoints-based Diagnostic Evaluation In this section, we first give an overview of linguistic checkpoints and then detail the evaluation framework and the key components of the system. 3.1 Linguistic Checkpoints A linguistic checkpoint can be defined as a linguistically-motivated unit, (e.g. an ambiguous word, a verb-object collocation, a POS-n-gram, a constituent, etc.) which is predefined in a linguistic taxonomy for diagnostic evaluation. Such a taxonomy is an inventory of linguistic phenomena of the source language that can"
2011.mtsummit-papers.60,W03-1901,0,0.0286593,"ablished in the FP7 PANACEA project,7 to represent word alignment. This is an XML format for linguistic analysis (e.g., PoS tagging, parsing, etc.) and alignment (sentence/word) based on XCES. 8 Scripts to convert the output of well-established tools (GIZA++, Treetagger, etc.) are available. (iii) It uses the KYOTO Annotation Format (KAF) (Bosma et al., 2009), established in the FP7 KYOTO project, 9 to represent textual analysis. KAF represents each level of linguistic analysis based on ISO standards (i.e. MAF, SynAF, SemAF) and it is compatible with the Linguistic Annotation Framework (LAF) (Ide and Romary, 2003). (iv) It makes use of Kybots (Vossen et al., 2010), established in the FP7 KYOTO project, to define the evaluation targets (linguistic check3 http://research.microsoft.com/en-us/downloads/ad240799a9a7-4a14-a556-d6a7c7919b4a/MSR%20noncommercial%20license%20agreement.txt 4 http://www.computing.dcu.ie/~atoral/delic4mt (under the license GPL-v3). 5 http://www.ims.unistuttgart.de/projekte/corplex/TreeTagger/ 6 http://code.google.com/p/giza-pp/ 7 http://panacea-lr.eu/ 8 http://www.xces.org/ 9 http://www.kyoto-project.eu/ points). A Kybot profile can be thought of as a regular expression over elemen"
2011.mtsummit-papers.60,C08-1141,0,0.178031,"nce between the two languages involved in the translation process. The level of detail and the specific linguistic phenomena included in the taxonomy can vary, depending on what the developers and/or the end-users want to investigate as part of the diagnostic evaluation and on the number of aspects that they are interested in. Linguistic checkpoints form the basis of linguistic test suites which are the means by which the MT output is evaluated. 3.2 Diagnostic Evaluation Framework This approach evaluates a system’s ability to handle various linguistic checkpoints. These were first proposed by Zhou et al. (2008), who developed Woodpecker,2 a tool supporting diagnostic evaluation based on linguistic checkpoints. However, this tool has two important drawbacks. Firstly, language-dependent data for English–Chinese (the language pair considered in their paper) is hardcoded in the software, which means that adapting it 2 http://research.microsoft.com/en-us/downloads/ad240799a9a7-4a14-a556-d6a7c7919b4a/ 530 to other language pairs is not straightforward. Secondly, its license (MSR-LA)3 is quite restrictive, to the extent that researchers would not be able to publicly release their adaptations of the tool. F"
2011.mtsummit-papers.60,J03-1002,0,0.00348558,"ation of the matching checkpoints. The requirements we stipulated for this new tool include: (i) the code had to be well-organized and fully documented; (ii) creating new evaluation targets for any language pair has to be as easy as possible (no coding involved); and (iii) the tool should support different evaluation metrics. Our novel tool, DELiC4MT (Diagnostic Evaluation using Linguistic Checkpoints For Machine Translation), 4 makes extensive use of already available components and representation standards. (i) It uses state-of-the-art PoS taggers and word aligners. Treetagger5 and GIZA++6 (Och and Ney, 2003), respectively, are used in the current version, although any similar tool could be used. (ii) It exploits the Travelling Object (TO) format, established in the FP7 PANACEA project,7 to represent word alignment. This is an XML format for linguistic analysis (e.g., PoS tagging, parsing, etc.) and alignment (sentence/word) based on XCES. 8 Scripts to convert the output of well-established tools (GIZA++, Treetagger, etc.) are available. (iii) It uses the KYOTO Annotation Format (KAF) (Bosma et al., 2009), established in the FP7 KYOTO project, 9 to represent textual analysis. KAF represents each l"
2012.eamt-1.41,2011.mtsummit-papers.32,1,0.894345,"Missing"
2012.eamt-1.41,P11-2071,0,0.0421364,"Missing"
2012.eamt-1.41,2005.eamt-1.19,0,0.0387288,"briefly reviews relevant related work. Section 3 provides a detailed discussion on the normalization techniques as well as the acquisition of supplementary training material. Section 4 presents the datasets and the experiments and corresponding results, followed by our conclusions and pointers to future work in Section 5. 2 Related Work The technique of using ‘out-of-domain’ datasets to supplement ‘in-domain’ training data has been widely used in domain adaptation of SMT. Information retrieval techniques were used by Eck et al. (2004) to propose a language model adaptation technique for SMT. Hildebrand et al. (2005) utilized this approach to select similar sentences from available bitext to adapt translation models, which improved translation performance. Habash (2008) used spelling expansion, morphological expansion, dictionary term expansion and proper name 170 transliteration to enhance or reuse existing phrase table entries to handle OOVs in Arabic–English MT. More recently an effort to adapt MT by mining bilingual dictionaries from comparable corpora using untranslated OOV words was carried out by Daume III and Jagarlamudi (2011). Our current line of work is related to the work reported in Daume III"
2012.eamt-1.41,W04-3250,0,0.147683,"Missing"
2012.eamt-1.41,2005.mtsummit-papers.11,0,0.038138,"y the spell checker were replaced with the highest ranking suggestion from the spell checker. As in Section 3.4, the spelling corrections were applied only to the test sets to ensure a reduction in the number of spelling error-based OOVs. 3.6 2. Supplementary Data Selection To take care of the VAL tokens which are valid words but absent in the training data, we explored techniques of mining supplementary data to improve the chances of successfully translating these tokens. We used the following freely available parallel data collections as potential sources of supplementary data: 1. Europarl (Koehn, 2005): Parallel corpus comprising of the proceedings of the European 172 Parliament. News Commentary Corpus: Released as a part of the WMT 2011 Translation Task.3 OpenOffice Corpus: Parallel documentation of the Office package from OpenOffice.org, released as part of the OPUS corpus (Tiedemann, 2009). KDE4 Corpus: A parallel corpus of the KDE4 localization files released as part of OPUS. PHP Corpus: Parallel corpus generated from multilingual PHP manuals also released as part of OPUS. OpenSubtitles2011 Corpus:4 A collection of documents released as part of OPUS. EMEA Corpus: A parallel corpus from"
2012.eamt-1.41,J03-1002,0,0.00436446,"lders, the line number and the actual token replaced. This mapping file is used later in the post-processing step to substitute the actual tokens in the position of the unique placeholders. For target sentences having multiple placeholders of the same type, the corresponding actual tokens are replaced in the order in which they appeared in the source. 4.2 Tools For all our translation experiments we used OpenMaTrEx (Dandapat et al., 2010), an open source SMT system which wraps the standard log-linear phrase-based SMT system Moses (Koehn et al., 2007). Word alignment was performed with Giza++ (Och and Ney, 2003). The phrase and reordering tables were built on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (Och, 2003) on the devset in terms of BLEU (Papineni et al., 2002). We used 5gram language models in all our experiments created using the IRSTLM (Federico et al., 2008) language modelling toolkit using Modified KneserNey smoothing. Results of translations in every phase of our experiments were evaluated using BLEU and TER (Snover et al., 2006). For the spell checking task w"
2012.eamt-1.41,P03-1021,0,0.0207458,"the same type, the corresponding actual tokens are replaced in the order in which they appeared in the source. 4.2 Tools For all our translation experiments we used OpenMaTrEx (Dandapat et al., 2010), an open source SMT system which wraps the standard log-linear phrase-based SMT system Moses (Koehn et al., 2007). Word alignment was performed with Giza++ (Och and Ney, 2003). The phrase and reordering tables were built on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (Och, 2003) on the devset in terms of BLEU (Papineni et al., 2002). We used 5gram language models in all our experiments created using the IRSTLM (Federico et al., 2008) language modelling toolkit using Modified KneserNey smoothing. Results of translations in every phase of our experiments were evaluated using BLEU and TER (Snover et al., 2006). For the spell checking task we used a combination of two off-the-shelf spelling correction toolkits. Using the ‘After the Deadline toolkit’ (AtD)5 as our primary spell checker, we also used a Java wrapper on Google’s spellchecking API6 to supplement the AtD spell"
2012.eamt-1.41,eck-etal-2004-language,0,0.354691,"Missing"
2012.eamt-1.41,P02-1040,0,0.084038,"kens are replaced in the order in which they appeared in the source. 4.2 Tools For all our translation experiments we used OpenMaTrEx (Dandapat et al., 2010), an open source SMT system which wraps the standard log-linear phrase-based SMT system Moses (Koehn et al., 2007). Word alignment was performed with Giza++ (Och and Ney, 2003). The phrase and reordering tables were built on the word alignments using the Moses training script. The feature weights for the log-linear combination of the feature functions were tuned using Minimum Error Rate Training (Och, 2003) on the devset in terms of BLEU (Papineni et al., 2002). We used 5gram language models in all our experiments created using the IRSTLM (Federico et al., 2008) language modelling toolkit using Modified KneserNey smoothing. Results of translations in every phase of our experiments were evaluated using BLEU and TER (Snover et al., 2006). For the spell checking task we used a combination of two off-the-shelf spelling correction toolkits. Using the ‘After the Deadline toolkit’ (AtD)5 as our primary spell checker, we also used a Java wrapper on Google’s spellchecking API6 to supplement the AtD spell checking results. However, the ‘in-domain’ adaptation"
2012.eamt-1.41,2011.mtsummit-papers.27,1,0.836761,"12 European Association for Machine Translation. 169 multinational company, Symantec hosts its forums in different languages (English, German, French etc), but currently the content is siloed in each language. Clearly, translating the forums to make information available across languages would be beneficial for Symantec as well as its multilingual customer base. This forms the primary motivation of techniques presented here. Despite growing interest in translation of forum data (Flournoy and Rueppel, 2010), to date, surprisingly little research has actually focussed on forum data translation (Roturier and Bensadoun, 2011). Compared to professionally edited text, user-generated forum data is often more noisy, taking some liberty with commonly established grammar, punctuation and spelling norms. For our research, we use translation memory (TM) data from Symantec, which is part of their corporate documentation, professionally edited and generally conforming to the Symantec controlled language guidelines. On the other hand, our target data (forum) is only lightly moderated and does not conform to any publication quality guidelines. Hence despite being from the same IT domain, there is a significant difference in s"
2012.eamt-1.41,2010.amta-commercial.5,0,0.0905654,"provide an easy source of information and a viable alternative to traditional customer service options. Being a c 2012 European Association for Machine Translation. 169 multinational company, Symantec hosts its forums in different languages (English, German, French etc), but currently the content is siloed in each language. Clearly, translating the forums to make information available across languages would be beneficial for Symantec as well as its multilingual customer base. This forms the primary motivation of techniques presented here. Despite growing interest in translation of forum data (Flournoy and Rueppel, 2010), to date, surprisingly little research has actually focussed on forum data translation (Roturier and Bensadoun, 2011). Compared to professionally edited text, user-generated forum data is often more noisy, taking some liberty with commonly established grammar, punctuation and spelling norms. For our research, we use translation memory (TM) data from Symantec, which is part of their corporate documentation, professionally edited and generally conforming to the Symantec controlled language guidelines. On the other hand, our target data (forum) is only lightly moderated and does not conform to a"
2012.eamt-1.41,P08-2015,0,0.31078,"aterial. Section 4 presents the datasets and the experiments and corresponding results, followed by our conclusions and pointers to future work in Section 5. 2 Related Work The technique of using ‘out-of-domain’ datasets to supplement ‘in-domain’ training data has been widely used in domain adaptation of SMT. Information retrieval techniques were used by Eck et al. (2004) to propose a language model adaptation technique for SMT. Hildebrand et al. (2005) utilized this approach to select similar sentences from available bitext to adapt translation models, which improved translation performance. Habash (2008) used spelling expansion, morphological expansion, dictionary term expansion and proper name 170 transliteration to enhance or reuse existing phrase table entries to handle OOVs in Arabic–English MT. More recently an effort to adapt MT by mining bilingual dictionaries from comparable corpora using untranslated OOV words was carried out by Daume III and Jagarlamudi (2011). Our current line of work is related to the work reported in Daume III and Jagarlamudi (2011) and that of Habash (2008). In our case, however, the target domain (web-forum) is different from the training data (Symantec TMs) mo"
2012.eamt-1.41,P07-2045,0,\N,Missing
2013.mtsummit-papers.17,W05-0909,0,0.100328,"evaluation results on growdiag-final alignments a n v r dt misc pro avg w-avg m-ratio Google 0.2748b,c 0.3108b,c 0.2423c 0.3191b 0.4787b,c 0.4916b,c 0.4281b 0.3636 0.3575 0.6873 Systems Moses 0.2281 0.2690 0.2305c 0.3016 0.4324 0.4453 0.3865 0.3276 0.3218 0.6661 Systran 0.2195 0.2650 0.2113 0.2937 0.4552 0.4447 0.4272 0.3309 0.3214 0.6674 Table 3: Diagnostic evaluation results on union alignments 5.2 Automatic Metrics We also evaluated the performances of the MT systems using a set of state-of-the-art automatic evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). Table 5 presents the system-level evaluation results for the different types of metrics considered (automatic, diagnostic and human judgements). For diagnostic evaluation it reports the weighted averages (see w-avg in Tables 1, 2, 3 and 4). According to BLEU, NIST and METEOR, Google 139 Systems Moses 0.4365 0.5042 0.4261c 0.5431 0.5926 0.6628 0.5493 0.5307 0.5285 1.0940 Systran 0.4365 0.4989 0.3496 0.5603 0.6248 0.6542 0.6030 0.5325 0.5183 1.0764 Table 4: Diagnostic evaluation results on intersection alignments is the best system, followed by Moses and Systran,"
2013.mtsummit-papers.17,E06-1032,0,0.0994004,"Missing"
2013.mtsummit-papers.17,fishel-etal-2012-terra,0,0.0426114,"Missing"
2013.mtsummit-papers.17,2005.mtsummit-papers.11,0,0.00728791,"or this study consists of 447 English–French word-aligned sentence pairs drawn from the Canadian Hansard Corpus, consisting of parliamentary debates (Och and Ney, 2000), for a total of 7,020 tokens in English and 7,761 in French. It should be noted that we did not differentiate between ‘sure’ and ‘probable’ word alignments in this dataset and treat them as having the same weight. Choosing a bilingual dataset from the domain of parliamentary speeches allowed us to conduct a 137 fair and direct comparison with a closely related baseline English–French MT system built using the Europarl corpus4 (Koehn, 2005). 4 4.1 Experimental Setup MT Systems We experimented with three MT systems: Google Translate5 , Systran6 and a baseline Moses7 system. Among the three MT systems, Google Translate and Moses are statistical MT systems while Systran is predominantly a rule-based system. The Moses system used for our experiments was trained on 3.6 million English–French sentence pairs taken from Europarl, the News Commentary corpus and a randomly selected section of the UN corpus. The system was tuned on a heldout development set consisting of 1,025 sentence pairs and used a 5-gram language model built using the"
2013.mtsummit-papers.17,max-etal-2010-contrastive,0,0.0192167,"rrelation between their automatic measures and human judgements across various error classes for different MT output. Popovi´c (2011) describes a tool for automatic classification of MT errors, which are grouped into five classes (morphological, lexical, reordering, omissions and unnecessary additions). The tool needs full-form reference translation(s) and hypotheses with their corresponding base forms. Additional information at the word level (such as PoS tags) can be used for a more delicate analysis. The tool computes the number of errors for each class at the document and sentence levels. Max et al. (2010) propose an approach to contrastive diagnostic MT evaluation based on comparing the ability of different systems (or implementations of the same system) to correctly translate source-language words. Their contrastive lexical evaluation method does not rely on the direct comparison of the system’s hypotheses with the reference translations, but for each sourcelanguage word it identifies which of the MT systems under consideration provide the correct output matching the reference. Their study is devoted to English–French and they point out the crucial role played by the quality of the alignment,"
2013.mtsummit-papers.17,W03-0301,0,0.0467017,"ions in terms of manually annotated aligned English–French data to serve as gold standard, and considered, for example, using Biblical texts made available as part of the Blinker Annotation Project (Melamed, 1998). However, the syntax and vocabulary of this dataset presented some specific features which were not in line with actual uses envisaged for diagnostic evaluation in research or industrial settings. The dataset that was chosen for our experiment was initially created for the shared task on word alignment held as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003). The dataset used for this study consists of 447 English–French word-aligned sentence pairs drawn from the Canadian Hansard Corpus, consisting of parliamentary debates (Och and Ney, 2000), for a total of 7,020 tokens in English and 7,761 in French. It should be noted that we did not differentiate between ‘sure’ and ‘probable’ word alignments in this dataset and treat them as having the same weight. Choosing a bilingual dataset from the domain of parliamentary speeches allowed us to conduct a 137 fair and direct comparison with a closely related baseline English–French MT system built using th"
2013.mtsummit-papers.17,C00-2163,0,0.0506058,"(Melamed, 1998). However, the syntax and vocabulary of this dataset presented some specific features which were not in line with actual uses envisaged for diagnostic evaluation in research or industrial settings. The dataset that was chosen for our experiment was initially created for the shared task on word alignment held as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003). The dataset used for this study consists of 447 English–French word-aligned sentence pairs drawn from the Canadian Hansard Corpus, consisting of parliamentary debates (Och and Ney, 2000), for a total of 7,020 tokens in English and 7,761 in French. It should be noted that we did not differentiate between ‘sure’ and ‘probable’ word alignments in this dataset and treat them as having the same weight. Choosing a bilingual dataset from the domain of parliamentary speeches allowed us to conduct a 137 fair and direct comparison with a closely related baseline English–French MT system built using the Europarl corpus4 (Koehn, 2005). 4 4.1 Experimental Setup MT Systems We experimented with three MT systems: Google Translate5 , Systran6 and a baseline Moses7 system. Among the three MT s"
2013.mtsummit-papers.17,2011.eamt-1.36,0,0.0403556,"Missing"
2013.mtsummit-papers.17,W06-3101,0,0.0563408,"Missing"
2013.mtsummit-papers.17,2006.amta-papers.25,0,0.0409265,"l alignments a n v r dt misc pro avg w-avg m-ratio Google 0.2748b,c 0.3108b,c 0.2423c 0.3191b 0.4787b,c 0.4916b,c 0.4281b 0.3636 0.3575 0.6873 Systems Moses 0.2281 0.2690 0.2305c 0.3016 0.4324 0.4453 0.3865 0.3276 0.3218 0.6661 Systran 0.2195 0.2650 0.2113 0.2937 0.4552 0.4447 0.4272 0.3309 0.3214 0.6674 Table 3: Diagnostic evaluation results on union alignments 5.2 Automatic Metrics We also evaluated the performances of the MT systems using a set of state-of-the-art automatic evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). Table 5 presents the system-level evaluation results for the different types of metrics considered (automatic, diagnostic and human judgements). For diagnostic evaluation it reports the weighted averages (see w-avg in Tables 1, 2, 3 and 4). According to BLEU, NIST and METEOR, Google 139 Systems Moses 0.4365 0.5042 0.4261c 0.5431 0.5926 0.6628 0.5493 0.5307 0.5285 1.0940 Systran 0.4365 0.4989 0.3496 0.5603 0.6248 0.6542 0.6030 0.5325 0.5183 1.0764 Table 4: Diagnostic evaluation results on intersection alignments is the best system, followed by Moses and Systran, while TER ranks Systran over M"
2013.mtsummit-papers.17,C08-1141,0,0.0184393,"provide the correct output matching the reference. Their study is devoted to English–French and they point out the crucial role played by the quality of the alignment, suggesting that inaccuracies in the automatic alignment are bound to impair the reliability of this approach for lexical diagnostic evaluation. Fishel et al. (2012) provide an overview of the field of diagnostic evaluation of MT, presenting a collection of freely available translation errorannotation corpora for various language pairs and comparing the performance of two state-of-the-art tools on automatic error analysis of MT. Zhou et al. (2008) describe a tool for diagnostic MT evaluation called Woodpecker,1 which is based on linguistic checkpoints. These are particularly interesting (or problematic) linguistic phenomena for MT processing identified by the user or developer who conducts the evaluation, e.g. ambiguous words, challenging collocations or PoS-ngram constructs, etc. One needs to define a linguistic taxonomy which describes the phenomena to be captured in the diagnostic evaluation, deciding which elements of the source language one wants to investigate. This scheme is extremely flexible, and can be formulated at different"
2013.mtsummit-papers.17,J03-1002,0,0.00562696,"consisting of 1,025 sentence pairs and used a 5-gram language model built using the SRILM toolkit (Stolcke, 2002). 4.2 Word Alignment The diagnostic evaluation was carried out using both gold standard human alignments and three sets of automatic alignments. Thus, in total we carried out experiments on 4 different sets of word alignments. The idea behind this study was primarily to show whether the different possible alignments had an impact on the effectiveness of the diagnostic MT evaluation metric, also in comparison with gold-standard manual alignment and human evaluation. We used GIZA++8 (Och and Ney, 2003) to derive the automatic alignments between the source and target sides of the testset. We extracted three sets of alignments using the union, intersection and grow-diag-final heuristics, as implemented by the Moses training scripts. Since the testset is far too small to be accurately word-aligned using a statistical word-aligner and would suffer from data sparseness, additional parallel training data from the Europarl corpus was used. The additional training data was first tokenised, filtered (using source-target length ratio) and lower-cased. The testset was also subjected to tokenisation an"
2013.mtsummit-papers.17,P02-1040,0,0.0955639,"ed across Tables 2, 3 and 4 as well. Table 2: Diagnostic evaluation results on growdiag-final alignments a n v r dt misc pro avg w-avg m-ratio Google 0.2748b,c 0.3108b,c 0.2423c 0.3191b 0.4787b,c 0.4916b,c 0.4281b 0.3636 0.3575 0.6873 Systems Moses 0.2281 0.2690 0.2305c 0.3016 0.4324 0.4453 0.3865 0.3276 0.3218 0.6661 Systran 0.2195 0.2650 0.2113 0.2937 0.4552 0.4447 0.4272 0.3309 0.3214 0.6674 Table 3: Diagnostic evaluation results on union alignments 5.2 Automatic Metrics We also evaluated the performances of the MT systems using a set of state-of-the-art automatic evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). Table 5 presents the system-level evaluation results for the different types of metrics considered (automatic, diagnostic and human judgements). For diagnostic evaluation it reports the weighted averages (see w-avg in Tables 1, 2, 3 and 4). According to BLEU, NIST and METEOR, Google 139 Systems Moses 0.4365 0.5042 0.4261c 0.5431 0.5926 0.6628 0.5493 0.5307 0.5285 1.0940 Systran 0.4365 0.4989 0.3496 0.5603 0.6248 0.6542 0.6030 0.5325 0.5183 1.0764 Table 4: Diagnostic evaluation results on intersection al"
2013.mtsummit-papers.8,W05-0909,0,0.502789,"Missing"
2013.mtsummit-papers.8,W12-5108,0,0.0173497,"y extract bilingual MWEs has been described in Ren et al. (2009). They examined the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al., 2007). They observed the highest improvement with an additional feature that identifies whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010) who replaced the binary feature by a count feature representing the number of MWEs in the source language phrase. A hybrid approach to identify MWEs from the English-French parallel corpus proposed by (Bouamor et al., 2012a), they aligned only many to many correspondences and deals with highly correlated MWE in a sentence pair, those are then integrated into the MOSES SMT System (Bouamor et al., 2012b). MWEs in SMT for Verbmobil corpus has also been proposed by (Lambert et al., 2005), the performance of the system evaluated in terms of alignment and translation quality. Instinctively, MWEs on the source and the target sides should be both aligned in the parallel corpus and translated as a whole. However, the constituents of an MWE are identified and aligned as parts of consecutive phrases in the state-of-the-ar"
2013.mtsummit-papers.8,bouamor-etal-2012-identifying,0,0.039193,"Missing"
2013.mtsummit-papers.8,J93-2003,0,0.0662809,"Missing"
2013.mtsummit-papers.8,N10-1029,0,0.0691116,"discriminative approach to use the compositionality information of verb-based multi-word expressions in order to improve the word alignment quality. A log likelihood ratio based hierarchical reducing algorithm to automatically extract bilingual MWEs has been described in Ren et al. (2009). They examined the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al., 2007). They observed the highest improvement with an additional feature that identifies whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010) who replaced the binary feature by a count feature representing the number of MWEs in the source language phrase. A hybrid approach to identify MWEs from the English-French parallel corpus proposed by (Bouamor et al., 2012a), they aligned only many to many correspondences and deals with highly correlated MWE in a sentence pair, those are then integrated into the MOSES SMT System (Bouamor et al., 2012b). MWEs in SMT for Verbmobil corpus has also been proposed by (Lambert et al., 2005), the performance of the system evaluated in terms of alignment and translation quality. Instinctively, MWEs on"
2013.mtsummit-papers.8,W10-3710,1,0.92874,"ailable. Furthermore, this is the first time when the identification of MWEs in Bengali language is used to enhance the performance of an EnglishBengali Machine Translation System. 3 3.1 System Description Preprocessing of the parallel corpus We considered several types of multi-word expressions: noun-noun MWEs, reduplicated phrases, complex predicates, phrasal prepositions, and verb-object combination. For the identification of complex predicates, we adopted a similar technique as reported in (Das et al., 2010). There are no frequent occurrences of reduplicated phrases in the English Corpus (Chakraborty and Bandyopadhyay, 2010) in comparison with the Bengali corpus, so this plays very crucial role in machine translation as they occur with high frequently in the Bengali corpus. Once the MWEs are identified, they are converted into single-tokens by replacing the spaces with underscores (‗_‘) so that we can establish 1to-1 alignments between the source and target MWEs. 63 3.2 MWE Identification Noun-Noun MWE Identification: When two or more nouns are united together to form a solo phrase such as ‗bed room‘ or ‗dining table‘ (Baldwin and Kim, 2010), these are termed as compound nouns or nominal compounds. Compound noun"
2013.mtsummit-papers.8,J90-1003,0,0.487629,"Missing"
2013.mtsummit-papers.8,J93-1003,0,0.47067,"Missing"
2013.mtsummit-papers.8,C04-1114,0,0.0325266,"unds are automatically extracted on the source side while noun-noun compounds, reduplicated phrases and complex predicates are identified on the target side of the parallel corpus. We use simple rule-based and statistical approaches to identify these MWEs. We have also extracted MWEs from comparable corpora which enhance not only MWE identification quality but also provide out-of-vocabulary words to SMT system. This also helps to accrue some knowledge of out of domain data. Source and target language MWEs are aligned using a Hybrid technique. A well-known practice in domain adaptation in SMT (Eck et al., 2004; Wu et al., 2008) is to incorporate bilingual dictionaries to the training corpus; which affects on the instances of atomic translation pairs. The work has been carried out into three direction (i) The parallel corpus has been modified by single tokenization of MWEs, (ii) The alignment of MWEs are added in the parallel corpus as additional data to improve the word alignment as well as the phrase alignment quality and (iii) The alignment of MWE has been directly incorporated into the word alignment model. The preprocessing of the parallel corpus results in improved MT quality in terms of autom"
2013.mtsummit-papers.8,N03-1017,0,0.0246219,"eline system: GIZA++ implementation of IBM word alignment 2 The EILMT project is funded by the Department of Electronics and Information Technology (DEITY), Ministry of Communications and Information Technology (MCIT), Government of India. 3 http://nlp.stanford.edu/software/lex-parser.shtml 4 http://crfchunker.sourceforge.net/ 5 http://wordnet.princeton.edu/ 6 The IL-ILMT project is funded by the Department of Electronics and Information Technology (DEITY), Ministry of Communications and Information Technology (MCIT), Government of India. 65 model 4, phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model trained using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al., 2007) have been used in the present study. 5 Experiments and Evaluations We have randomly identified 500 sentences each for the development set and the test set from the initial parallel corpus. The rest are considered as the training corpus. The training corpus was filtered with the maximum allowable sentence length of 100 words and sentence length ratio of 1:2 (either wa"
2013.mtsummit-papers.8,P07-2045,0,0.016243,"eriments. Section 5 includes the results obtained, together with some analysis. Section 6 concludes and provides avenues for further work. 2 Related Work Venkatapathy and Joshi (2006) reported a discriminative approach to use the compositionality information of verb-based multi-word expressions in order to improve the word alignment quality. A log likelihood ratio based hierarchical reducing algorithm to automatically extract bilingual MWEs has been described in Ren et al. (2009). They examined the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al., 2007). They observed the highest improvement with an additional feature that identifies whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010) who replaced the binary feature by a count feature representing the number of MWEs in the source language phrase. A hybrid approach to identify MWEs from the English-French parallel corpus proposed by (Bouamor et al., 2012a), they aligned only many to many correspondences and deals with highly correlated MWE in a sentence pair, those are then integrated into the MOSES SMT System (Bouamor et al., 20"
2013.mtsummit-papers.8,W04-3250,0,0.388494,"Missing"
2013.mtsummit-papers.8,2005.mtsummit-posters.11,0,0.059502,"Missing"
2013.mtsummit-papers.8,P03-1021,0,0.0185684,"ignment 2 The EILMT project is funded by the Department of Electronics and Information Technology (DEITY), Ministry of Communications and Information Technology (MCIT), Government of India. 3 http://nlp.stanford.edu/software/lex-parser.shtml 4 http://crfchunker.sourceforge.net/ 5 http://wordnet.princeton.edu/ 6 The IL-ILMT project is funded by the Department of Electronics and Information Technology (DEITY), Ministry of Communications and Information Technology (MCIT), Government of India. 65 model 4, phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model trained using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al., 2007) have been used in the present study. 5 Experiments and Evaluations We have randomly identified 500 sentences each for the development set and the test set from the initial parallel corpus. The rest are considered as the training corpus. The training corpus was filtered with the maximum allowable sentence length of 100 words and sentence length ratio of 1:2 (either way). Finally the training corpus contained"
2013.mtsummit-papers.8,W10-3707,1,0.735788,"Missing"
2013.mtsummit-papers.8,P02-1040,0,0.0918365,"s and 1,223 of noun-noun compounds. The experiments have been carried out in various experimental settings: (i) single tokenization of MWEs on both sides in the parallel corpus, (ii) single tokenized MWEs added with the parallel training data, (iii) single tokenized MWEs directly integrated into the word alignment model, and finally, (iv) bootstrapping with single iteration using the experimental setup (ii) and (iii) to examine how the parallel MWE alignment set can be increased. Extrinsic evaluation was carried out on the MT quality using the well-known automatic MT evaluation metrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) and the evaluation results are reported in Table 2. By considering single tokenization (experiment 2), the system achieves performance improvement to some extent. Use of comparable corpora (experiment 3) improves the MWE identification performance which in turn improves the translation quality. Training set CPs English T U 8142 388 9 55 15 Bengali T U 2017 7154 4 185 150 reduplicated word Noun-noun 892 711 489 300 compound Noun-noun 1792 981 889 700 compound with Comparable corpora Phrasal prep- 1782 137 osition 9 Verb-object 231 145 combination Phrasal verb 549 53"
2013.mtsummit-papers.8,W09-2907,0,0.0230463,"work. The English-Bengali PB-SMT system is described in Section 3. Section 4 states the tools and resources used for the various experiments. Section 5 includes the results obtained, together with some analysis. Section 6 concludes and provides avenues for further work. 2 Related Work Venkatapathy and Joshi (2006) reported a discriminative approach to use the compositionality information of verb-based multi-word expressions in order to improve the word alignment quality. A log likelihood ratio based hierarchical reducing algorithm to automatically extract bilingual MWEs has been described in Ren et al. (2009). They examined the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al., 2007). They observed the highest improvement with an additional feature that identifies whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010) who replaced the binary feature by a count feature representing the number of MWEs in the source language phrase. A hybrid approach to identify MWEs from the English-French parallel corpus proposed by (Bouamor et al., 2012a), they aligned only many to many correspon"
2013.mtsummit-papers.8,W09-2906,0,0.0684193,"Missing"
2013.mtsummit-papers.8,W03-1803,0,0.101085,"Missing"
2013.mtsummit-papers.8,W06-1204,0,0.0428091,"Missing"
2013.mtsummit-papers.8,C08-1125,0,0.412897,"Missing"
2014.amta-wptp.5,W05-0909,0,0.0206611,"rent points (as was likely e.g. with the biographies included in our data sets), they might, more or less consciously, end up translating them differently. This variable behaviour applies even more to post-editors: the degree and the type of corrections made by the same as well as by different individuals to the MT output for one language pair are likely to be unpredictably inconsistent. 68 We thus evaluated the raw MT output against both the human translations and the post-edited MT output using three state-of-the-art automatic evaluation metrics, namely BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). The automatic MT evaluation scores are shown in Figure 3. Following the conventions used in Snover et al. (2006), the scores against references translated from scratch are named after the metric (i.e. BLEU, METEOR and TER), while the scores against the post-edited references are named appending the prefix H (i.e. HBLEU, HMETEOR and HTER, respectively). If we compare the human translation scores against the PE scores, we can see that the PE scores are consistently better than the human translation scores for all the translation directions across all the metrics ("
2014.amta-wptp.5,2009.mtsummit-btm.7,0,0.312419,"Missing"
2014.amta-wptp.5,2009.mtsummit-papers.8,0,0.0774514,"involved. Guerberof (2009) studied the effectiveness of using MT output as opposed to translation memory fuzzy matches for the purpose of post-editing in an EnglishĺSpanish translation task. She used Language Weaver’s statistical MT engine and trained it on the same TM, performing both quantitative and qualitative analyses. The main result was that the productivity of the translators as well as the quality of the translation improved when post-editing MT output, compared to when processing fuzzy matches from the translation memory database. 1 http://langtech.autodesk.com/productivity.html. 61 Koehn and Haddow (2009) describe Caitra, a tool that makes suggestions for sentence completion, shows word and phrase translation options, and supports PE of MT output. They report a user study carried out with the tool involving 7 translators for the English–French language pair. Among the different types of assistance offered by Caitra, users prefer the prediction of sentence completion and the options from the translation table over the other types of assistance available for post-editing MT output. To the authors’ surprise, PE received the lowest scores among all the options, both in terms of enjoyment and subje"
2014.amta-wptp.5,W12-3123,0,0.52659,"t with the tool involving 7 translators for the English–French language pair. Among the different types of assistance offered by Caitra, users prefer the prediction of sentence completion and the options from the translation table over the other types of assistance available for post-editing MT output. To the authors’ surprise, PE received the lowest scores among all the options, both in terms of enjoyment and subjective usefulness, although PE was as productive as the other types of assistance. In an effort to extend the initial insights presented in particular by Koehn and Haddow (2009) and Koponen (2012), this paper investigates perceived vs real productivity gains brought about by post-editing MT output compared against manual translation from scratch in the relatively open – and thus particularly challenging – news-oriented domain. 3 Set-up of the Study 3.1 Methodology and Materials Output from the CoSyne statistical MT systems (Martzoukos and Monz, 2010) was used in this experiment, and a facility was in place to track the time required by the users to post-edit MT output and to perform manual translations from scratch on texts of similar length and complexity. The texts chosen for the stu"
2014.amta-wptp.5,2012.amta-wptp.2,0,0.115752,"Missing"
2014.amta-wptp.5,2013.mtsummit-wptp.10,0,0.227975,"Missing"
2014.amta-wptp.5,2010.iwslt-evaluation.28,0,0.0213712,"west scores among all the options, both in terms of enjoyment and subjective usefulness, although PE was as productive as the other types of assistance. In an effort to extend the initial insights presented in particular by Koehn and Haddow (2009) and Koponen (2012), this paper investigates perceived vs real productivity gains brought about by post-editing MT output compared against manual translation from scratch in the relatively open – and thus particularly challenging – news-oriented domain. 3 Set-up of the Study 3.1 Methodology and Materials Output from the CoSyne statistical MT systems (Martzoukos and Monz, 2010) was used in this experiment, and a facility was in place to track the time required by the users to post-edit MT output and to perform manual translations from scratch on texts of similar length and complexity. The texts chosen for the study were extracted from “Today in History/Kalenderblatt” and “Beeld en Geluidwiki”, the public wiki sites of the two media organizations that acted as end-user partners in the CoSyne project, namely Deutsche Welle (DW) and the Netherlands Institute for Sound and Vision (NISV).2 These two bilingual wiki sites cover news, accounts of historical events, biograph"
2014.amta-wptp.5,P02-1040,0,0.102721,"cross identical phrases at different points (as was likely e.g. with the biographies included in our data sets), they might, more or less consciously, end up translating them differently. This variable behaviour applies even more to post-editors: the degree and the type of corrections made by the same as well as by different individuals to the MT output for one language pair are likely to be unpredictably inconsistent. 68 We thus evaluated the raw MT output against both the human translations and the post-edited MT output using three state-of-the-art automatic evaluation metrics, namely BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). The automatic MT evaluation scores are shown in Figure 3. Following the conventions used in Snover et al. (2006), the scores against references translated from scratch are named after the metric (i.e. BLEU, METEOR and TER), while the scores against the post-edited references are named appending the prefix H (i.e. HBLEU, HMETEOR and HTER, respectively). If we compare the human translation scores against the PE scores, we can see that the PE scores are consistently better than the human translation scores for all the translation"
2014.amta-wptp.5,2012.amta-wptp.7,0,0.104374,"Missing"
2014.amta-wptp.5,2006.amta-papers.25,0,0.404541,"age input, revising and improving it as required to obtain a final target text of publishable quality. The purpose of this was to add the final revised translation to the public wiki of their respective media organization; hence, the scenario was that of full PE, aiming for optimal quality of the final revised text (Allen, 2003: 306). In addition, it should be noted that while all participants in the experiment had experience in manual translation, none of them had been specifically trained to carry out PE in a realistic professional task. This is quite different from previous studies such as Snover et al. (2006: 227), where monolingual annotators “were coached on how to minimize the edit rate”. To sum up, our study focused on a scenario in which (i) the translators were not trained specifically on PE, and (ii) the objective was publishable quality, as a means of investigating the role of full PE in industrial settings, especially in terms of the perceived vs actual productivity gains. 4 Questionnaire Results 4.1 Profiles of the Participants At the time of completing the questionnaire, the youngest DW staff member was 38 years of age, and the oldest was 59. Overall, the average age of DW staff who co"
2014.amta-wptp.5,2010.jec-1.6,0,0.561873,"Missing"
2014.amta-wptp.5,2012.amta-wptp.10,0,0.0730721,"d translations (columns PEMT), for which the picture is rather mixed (4 expected correlations, 5 no correlations and 3 unexpected ones). Aggregating the data for all the translation directions, we observe consistent results regardless of the metric (TER, BLEU and METEOR) or the translation method (PEMT, HT): all the correlations are as expected, their values ranging from ±0.23 to ±0.42. 6 Conclusions We have presented a study of real vs perceived PE productivity gains for the German— English and Dutch—English bidirectional language pairs. Previous studies such as Plitt and Masselot (2010) and Zhechev (2012) had looked at PE productivity gains compared to manual translation. However, in a similar vein to Koehn and Haddow (2009) and Koponen (2012), this study has crucially brought into the picture the perceptions of the users in terms of PE effort and speed, comparing them to the actual PE time gains. We have found a bias in favour of translation from scratch across all four translation directions for all the levels of perception considered (speed, effort and favourite working method). While the perception of speed and effort seems to correspond to the actual gains to some extent, the favourite wo"
2020.coling-main.524,W19-5402,0,0.0122306,"E task. Apart from the multi-encoder transference architecture described above ({src, mt}tr → pe) and ensembling of this architecture, two simpler versions are also analyzed: first, a ‘mono-lingual’ (mt → pe) APE model using only parallel mt–pe data and therefore only a single encoder, and second, an identical single-encoder architecture, however, using the concatenated src and mt text as input ({src + mt} → pe) (Niehues et al., 2016). 5966 4.1 Data For our experiments, we use the English–German WMT 2016 (Bojar et al., 2016), 2017 (Bojar et al., 2017), 2018 (Chatterjee et al., 2018) and 2019 (Chatterjee et al., 2019) APE task data. All these released APE datasets consist of English–German triplets containing source English text (src) from the IT domain, the corresponding German translations (mt) from a 1st -stage MT system, and the corresponding human-post-edited version (pe). The sizes of the datasets (train; dev; test), in terms of number of sentences, are (12,000; 1,000; 2,000), (11,000; 0; 2,000), and (13,442; 1,000; 1,023), for the 2016 PBSMT, the 2017 PBSMT, and the 2018 NMT data, respectively. The 2019 version of the APE dataset released in WMT is the same as the WMT 2018 NMT data. It is to be note"
2020.coling-main.524,P18-1167,0,0.135825,"tion of encsrc→mt to produce the final translation). The paper makes the following contributions: (i) we propose a multi-encoder model for APE that consists only of standard transformer encoding and decoding blocks, (ii) by using a mix of self- and cross-attention we provide a representation of both src and mt for the decoder, allowing it to better capture errors in mt originating from src; this advances Junczys-Dowmunt and Grundkiewicz (2018) – the WMT 2018 best system (wmt18smt best ) in terms of BLEU and TER, (iii), we analyze the effect of varying the number of encoder and decoder layers (Domhan, 2018), indicating that the encoders contribute more than decoders in neural APE, and (iv) we present and evaluate an APE architecture inspired by a two-step approach professional translators often use during post-editing. In comparison to the shared task system description paper (Pal et al., 2019), this paper (i) provides more detailed explanations and reformation of different components of the transference architecture, (ii) compares it to a single encoder based transformer architecture where only mt or src concatenated with mt are used as an input, (iii) analyzes results when swapping mt and src"
2020.coling-main.524,W16-2378,0,0.11533,"yzes results when swapping mt and src in the multi-encoder setup, and (iv) investigates the importance of encoder and decoder by varying the amount of layers. The rest of the paper is organized as follows. In §2, we survey existing literature on APE. In §3, we describe the multi-encoder architecture. §4 describes our experimental setup. §5 reports the results of our approach against a number of baselines. Finally, §6 concludes the paper with future directions. 2 Related Research Recent advances in APE research are directed towards neural APE, which was first proposed by Pal et al. (2016b) and Junczys-Dowmunt and Grundkiewicz (2016) for the single-source APE scenario which does not consider src, i.e. mt → pe. Junczys-Dowmunt and Grundkiewicz (2016) also generated a large synthetic training dataset, which we also use as additional training data. Exploiting source information as an additional input can help neural APE to disambiguate corrections applied at each time step; this naturally leads to multi-source APE ({src, mt} → pe). A multi-source neural APE system can be configured either by using a single encoder that encodes the concatenation of src and mt (Niehues et al., 2016) or by using two separate encoders for src an"
2020.coling-main.524,W18-6467,0,0.245569,"n suggestion (similar to what our encsrc→mt is doing), then corrections to the MT output are applied based on the encountered errors (in the same way that our decpe uses the encoded representation of encsrc→mt to produce the final translation). The paper makes the following contributions: (i) we propose a multi-encoder model for APE that consists only of standard transformer encoding and decoding blocks, (ii) by using a mix of self- and cross-attention we provide a representation of both src and mt for the decoder, allowing it to better capture errors in mt originating from src; this advances Junczys-Dowmunt and Grundkiewicz (2018) – the WMT 2018 best system (wmt18smt best ) in terms of BLEU and TER, (iii), we analyze the effect of varying the number of encoder and decoder layers (Domhan, 2018), indicating that the encoders contribute more than decoders in neural APE, and (iv) we present and evaluate an APE architecture inspired by a two-step approach professional translators often use during post-editing. In comparison to the shared task system description paper (Pal et al., 2019), this paper (i) provides more detailed explanations and reformation of different components of the transference architecture, (ii) compares"
2020.coling-main.524,W19-5412,0,0.0268686,"stacks an additional cross-attention component for src → pe above the previous cross-attention for mt → pe. In contrast to other multi-encoder based approaches and Libovick´y et al. (2018)’s approach, where the authors focused on cross-attention of two encoders with respect to the decoder within the transformer architecture, we propose a novel architecture where the second encoder block is similar to the transformer decoder block but without masking. In the latest edition of WMT (2019), the submissions are mostly multi-source models extending the transformer implementation (Pal et al., 2019; Lee et al., 2019; Xu et al., 2019) and adapting BERT (Devlin et al., 2018) to the transformer-based framework (Lopes et al., 2019). The winner system (Lopes et al., 2019) (wmt19nmt best ) uses a single pre-trained BERT encoder that receives both the source src and mt strings and applies a BERT-based encoder-decoder model. Additionally, they add a conservativeness penalty factor during beam decoding to avoid over-corrections in APE. Our method outperforms the WMT 2016, 2017, and 2018 winners by 1 BLEU point, and yields comparable performance to the WMT 2019 winner, however, without using a BERT-based architect"
2020.coling-main.524,W16-2361,0,0.0519407,"Missing"
2020.coling-main.524,W18-6326,0,0.0506587,"Missing"
2020.coling-main.524,W19-5413,0,0.0245094,"Missing"
2020.coling-main.524,L18-1004,0,0.0133383,"The 2019 version of the APE dataset released in WMT is the same as the WMT 2018 NMT data. It is to be noted that for WMT 2018, we carried out experiments only for the NMT sub-task and ignored the data for the PBSMT task. Since the WMT APE datasets are small in size, we use ‘artificial training data’ (Junczys-Dowmunt and Grundkiewicz, 2016) containing 4.5M sentences as additional resources, 4M of which are weakly similar to the WMT 2016 training data, while 500K are very similar according to TER statistics. For experimenting on the NMT data, we additionally use the synthetic eScape APE corpus (Negri et al., 2018), consisting of ∼7M triples. For cleaning this noisy eScape dataset containing many unrelated language words (e.g. Chinese), (i) we use the cleaning process described in Tebbifakhr et al. (2018), and (ii) we use the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 100, respectively. After cleaning, we perform punctuation normalization, and then use the Moses tokenizer (Koehn et al., 2007) to tokenize the eScape corpus with ‘no-escape’ option. Finally, we apply true-casing. The cleaned version of the eScape corpus contains ∼6.5M triplets."
2020.coling-main.524,C16-1172,0,0.0545641,"Missing"
2020.coling-main.524,C16-1241,1,0.89551,"Missing"
2020.coling-main.524,P16-2046,1,0.904948,"Missing"
2020.coling-main.524,W18-6468,1,0.880295,"Missing"
2020.coling-main.524,W19-5414,1,0.451809,"Missing"
2020.coling-main.524,P02-1040,0,0.107046,"T (we refer as 1st -stage MT) system to which APE is applied is either a phrase-based statistical machine translation (PBSMT) or a neural machine translation (NMT) model. For the PBSMT task, we compare against four baselines: the raw SMT output provided by the 1st stage PBSMT, the best-performing systems from WMT APE 2018 (wmt18smt best ), which are a single model and an ensemble model by Junczys-Dowmunt and Grundkiewicz (2018), as well as a transformer directly translating from src to pe (Transformer (src → pe)), thus performing translation instead of APE. We evaluate the systems using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). For the NMT task, we consider three baselines: the raw NMT output provided by the 1st -stage NMT system, the best-performing system from the WMT 2018 (wmt18nmt best ) (Tebbifakhr et al., 2018) and nmt WMT 2019 (wmt19best ) (Lopes et al., 2019) NMT APE task. Apart from the multi-encoder transference architecture described above ({src, mt}tr → pe) and ensembling of this architecture, two simpler versions are also analyzed: first, a ‘mono-lingual’ (mt → pe) APE model using only parallel mt–pe data and therefore only a single encoder, and second, an identical single"
2020.coling-main.524,P16-1162,0,0.0126887,"e ) ft with {src, mt}smt tr → pe . Last, we analyze the importance of our second encoder (encsrc→mt ), compared to the source encoder 5967 (encsrc ) and the decoder (decpe ), by reducing and expanding the amount of layers in the encoders and the decoder. Our standard setup, which we use for fine-tuning, ensembling etc., is fixed to 6-6-6 for Nsrc -Nmt -Npe (cf. Figure 1). We investigate what happens in terms of APE performance if we change this setting to 6-6-4 and 6-4-6. To handle out-of-vocabulary words and reduce the vocabulary size, instead of considering words, we consider subword units (Sennrich et al., 2016) by using byte-pair encoding (BPE). In the preprocessing step, instead of learning an explicit mapping between BPEs in the src, mt and pe, we define BPE tokens by jointly processing all triplets. Thus, src, mt and pe derive a single BPE vocabulary. Since mt and pe belong to the same language (German) and src is a close language (English), they naturally share a good fraction of BPE tokens, which reduces the vocabulary size to 28k. We implemented our approach based on the Neutron implementation of the Transformer (Xu and Liu, 2019)1 . 4.3 Hyper-parameter Setup We follow a similar hyper-paramete"
2020.coling-main.524,W18-6470,0,0.0167713,"f src and mt sentences, and a second one using two character-level encoders for mt and src along with a character-level decoder. In the WMT 2018 APE shared task, several adaptations of the transformer architecture were presented for multi-source APE. Pal et al. (2018) introduced a joint encoder that attends over a combination of the two encoded sequences from mt and src. Tebbifakhr et al. (2018), the NMT-subtask winner of WMT 2018 (wmt18nmt best ), employed sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. Shin and Lee (2018) proposed a multi-source transformer where on the decoder side, they added two additional multi-head attention 5964 layers for src → mt and src → pe. Thereafter another multi-head attention between the output of those attention layers helps the decoder to capture common words in mt which should remain in pe. The APE PBSMT-subtask winner of WMT 2018 (wmt18smt best ) (Junczys-Dowmunt and Grundkiewicz, 2018) also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for src → pe above the previous cross-attention for mt → p"
2020.coling-main.524,2006.amta-papers.25,0,0.0324129,"stem to which APE is applied is either a phrase-based statistical machine translation (PBSMT) or a neural machine translation (NMT) model. For the PBSMT task, we compare against four baselines: the raw SMT output provided by the 1st stage PBSMT, the best-performing systems from WMT APE 2018 (wmt18smt best ), which are a single model and an ensemble model by Junczys-Dowmunt and Grundkiewicz (2018), as well as a transformer directly translating from src to pe (Transformer (src → pe)), thus performing translation instead of APE. We evaluate the systems using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). For the NMT task, we consider three baselines: the raw NMT output provided by the 1st -stage NMT system, the best-performing system from the WMT 2018 (wmt18nmt best ) (Tebbifakhr et al., 2018) and nmt WMT 2019 (wmt19best ) (Lopes et al., 2019) NMT APE task. Apart from the multi-encoder transference architecture described above ({src, mt}tr → pe) and ensembling of this architecture, two simpler versions are also analyzed: first, a ‘mono-lingual’ (mt → pe) APE model using only parallel mt–pe data and therefore only a single encoder, and second, an identical single-encoder architecture, however"
2020.coling-main.524,W18-6471,0,0.29825,"multi-source models (Libovick´y et al., 2016) by means of concatenating both weighted contexts of encoded src and mt. Varis and Bojar (2017) compared two multi-source models, one using a single encoder with the concatenation of src and mt sentences, and a second one using two character-level encoders for mt and src along with a character-level decoder. In the WMT 2018 APE shared task, several adaptations of the transformer architecture were presented for multi-source APE. Pal et al. (2018) introduced a joint encoder that attends over a combination of the two encoded sequences from mt and src. Tebbifakhr et al. (2018), the NMT-subtask winner of WMT 2018 (wmt18nmt best ), employed sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. Shin and Lee (2018) proposed a multi-source transformer where on the decoder side, they added two additional multi-head attention 5964 layers for src → mt and src → pe. Thereafter another multi-head attention between the output of those attention layers helps the decoder to capture common words in mt which should remain in pe. The APE PBSMT-subtask winner of WMT 2018 (wmt18smt best ) (Junczys-Do"
2020.coling-main.524,W17-4777,0,0.0128738,"src and mt and passing the concatenation of both encoders’ final states to the decoder (Libovick´y et al., 2016). A few approaches to multi-source neural APE were proposed in the WMT 2017 APE shared task. Junczys-Dowmunt and Grundkiewicz (2017) combine both mt and src in a single neural architecture, exploring different combinations of attention mechanisms including soft attention and hard monotonic attention. Chatterjee et al. (2017) built upon the two-encoder architecture of multi-source models (Libovick´y et al., 2016) by means of concatenating both weighted contexts of encoded src and mt. Varis and Bojar (2017) compared two multi-source models, one using a single encoder with the concatenation of src and mt sentences, and a second one using two character-level encoders for mt and src along with a character-level decoder. In the WMT 2018 APE shared task, several adaptations of the transformer architecture were presented for multi-source APE. Pal et al. (2018) introduced a joint encoder that attends over a combination of the two encoded sequences from mt and src. Tebbifakhr et al. (2018), the NMT-subtask winner of WMT 2018 (wmt18nmt best ), employed sequence-level loss functions in order to avoid expo"
2020.coling-main.524,W19-5417,1,0.887678,"Missing"
2020.icon-main.20,gencheva-etal-2017-context,0,0.0652486,"Missing"
2020.icon-main.20,W16-2117,0,0.0282801,"020 NLP Association of India (NLPAI) cles. With all these features together we make a hypothesis for the final classification. These two tasks are consecutively done in a work (Pepa Gencheva and Koychev, 2017) previously. Else there are many works to influence on individual approaches of the model. The paper is divided into many sections, section 2 describes related works, dataset in section 3, features and proposed methodology in section 4 and 5 respectively, results in section 6 and conclusion is given in section 7. 2 dia sources, evidence extraction and linguistic analysis. Dieu-Thu Le and Blessing (2016) used Convolutional Neural Networks for the task. Rob Ennals and Rosario. (2010) developed another factchecking system, DisputeFinder, which works on already verified claims. Ayush Patwari and Bagchi. (2017) used LDA topic modeling, POS tuples and Bag-of-Words as features and SVM is used for clustering. Wang. (2017) and Nicole OBrien and Boix. (2018), proposed different models to classify factuality of claims aimed at only input claims and their metadata. Related Works 3 Fact-Checking has become a trending topic recently. Zhou and Zafarani (2018) provides a survey on fake news research and the"
2020.icon-main.20,N19-4014,0,0.0378313,"Missing"
2020.icon-main.20,W17-5104,0,0.0622681,"Missing"
2020.icon-main.20,C16-1113,0,0.0559749,"Missing"
2020.icon-main.20,P17-2067,0,0.0623202,"Missing"
2020.icon-main.55,W09-3406,0,0.0191744,"inflexional endings in different steps. An extension of the Lovins algorithm is known as Dawson stemmer (Jivani et al., 2011) was proposed which covers wide range of list of 1200 suffixes. These above mentioned algorithms work very well for English language but we are more interested in different stemming algorithms for Indian languages. Ramanathan and Rao (2003) proposed a lightweight stemmer for Hindi in 2004 which removes the suffixes based on the longest suffix matching from a list of suffixes. They also developed a suffix list in Hindi language to enhance the performance of the stemming. Akram et al. (2009) proposed an affix-exception list based stemmer for Urdu language. They omit prefix and suffix from the word based on looking up the exception list of prefixes or suffixes. This stemmer finds the stem word based on lexical look up method. A successful look up ignores the stripping off of the prefix and suffix of a word. Hussain et al. proposed a stemming mechanism for Urdu language based on n-gram stripping model (Durrani and Hussain, 2010). Kumar and Rana (2011) developed a brute force algorithm to strip off suffixes in order to find stem words in Punjabi language. They have overcome the prob"
2020.icon-main.55,L16-1406,0,0.0611565,"Missing"
2020.icon-main.55,N10-1077,0,0.0296949,"ixes based on the longest suffix matching from a list of suffixes. They also developed a suffix list in Hindi language to enhance the performance of the stemming. Akram et al. (2009) proposed an affix-exception list based stemmer for Urdu language. They omit prefix and suffix from the word based on looking up the exception list of prefixes or suffixes. This stemmer finds the stem word based on lexical look up method. A successful look up ignores the stripping off of the prefix and suffix of a word. Hussain et al. proposed a stemming mechanism for Urdu language based on n-gram stripping model (Durrani and Hussain, 2010). Kumar and Rana (2011) developed a brute force algorithm to strip off suffixes in order to find stem words in Punjabi language. They have overcome the problem of over-stemming and under-stemming. The suffix stripping is replaced sometimes by suffix substitution. Islam et al. (2007) proposed a lightweight stemmer which strips off suffixes and finds the stem word for Bengali. The fundamental idea of this algorithm is to remove suffixes based on the longest suffix matching. They also maintain a list of possible suffixes for Bengali language. Paik et al. (2011) reported a simple corpus based unsu"
2020.icon-main.55,jha-2010-tdil,0,0.0348998,"ry element within the set RVð is checked in the root-verb database RVD . At any point if a match found then the corresponding stem verb (RW ) is returned. 5 Resources Used Bengali WordNet: Bengali WordNet is a part of IndoWordNet1 . Bengali WordNet is a lexical database for Bengali words and it contains around 61 thousand Bengali words along with the Synsets. We imported the Bengali WordNet in MySQL database. We created a separate table for all the root-verbs corresponding to the Bengali verbs. TDIL Corpus: For the corpus, we used the Technology Development for Indian Languages (TDIL) corpus (Jha, 2010) in this work. 6 Dataset http://tdil-dc.in/indowordnet/ Category #Words Noun Verb 1274 1230 #Correctly Stemmed 1234 1227 Accuracy 96.86 99.75 6.2 Implementation We implemented our algorithm in Python 3.6 and MySQL. In the very first step each sentence of the corpus is scanned by our python script. The Natural Language Tool Kit (nltk) package has been used to accomplish the necessary preprocessing tasks. Within the nltk package we have used Stanford Bengali POS Tagger to tag parts of speech for each word of the sentence. Then the sentence is tokenized into set of words. Another python script ha"
2020.icon-main.57,Y18-1033,0,0.0661245,"Missing"
2020.icon-main.57,N16-1030,0,0.13953,"Missing"
2020.icon-main.57,W10-3605,0,0.037518,"sive language morphological features to elevate its accuracy. Finally the model’s performance is compared with the other machine learning based Manipuri MNE models. 1 Introduction Multi Word Named Entity (MNE) is a part of Multiword Expression (MWE) which is an ordered group of words that can exist independently and carries different meaning as opposed to its constituent word (Nongmeikapam and Bandyopadhyay, 2011). Accurate recognition of such MNE plays a vital role in various NLP tasks such as POS tagging (Nongmeikapam et al., 2011b), Chunking (Nongmeikapam et al., 2014), NER classification (Singh and Bandyopadhyay, 2010). Manipuri, being a highly inflectional language where affixes define the nature of the words (Choudhury et al., sudip.naskar@cse.jdvu.ac.in 2004), machine recognition of MNE presents a challenging task for NLP researchers. In Manipuri, the majority of researches on MNE classification are done using machine learning approaches such as CRF (Nongmeikapam and Bandyopadhyay, 2010, 2011; Nongmeikapam et al., 2011a) and SVM (Singh and Bandyopadhyay, 2010). These researches use extensive morphological features to obtain accurate recognition of the MNE. Such morphological features include affixes, con"
2020.icon-main.57,D16-1100,0,0.0288792,"ds, digit features etc. For an agglutinative and low-resource language, the inflectional nature amounts to the large Out of Vocabulary (OOV) words, thus making any sequence labelling task or morphological feature creation tasks difficult. Word embedding is a distributed representation of text in low dimensional real valued vectors and are known to contain semantic or syntactic information and has shown to be an effective feature for many natural language classification tasks of English language (Wang et al., 2015). Word embedding has also been extremely useful for Chinese language processing (Yin et al., 2016), Japanese language processing (Kitagawa and Komachi, 2017) and for some Indian languages processing (Ajay et al., 2016; Bhattacharya et al., 2016). In Manipuri, the effectiveness of word embeddings to any NLP tasks, till this date remains unanswered. The general attempt to work in Manipuri NLP task using embedding follows the idea used in English i.e. to learn the embedding of a word from the context. Unlike English, Manipuri words are a composite of complicated structure with several affixes producing OOV words. Regardless of its context, affixes to a word plays a major role in defining the"
2020.trac-1.14,2020.trac-1.25,0,0.0185909,"iven dataset and no extra data is used for training here. For Hindi dataset, we have used Tf-Idf Vectorizer, aggressive word lexicons , sentiment scores(taking compound score from positive and negative scores of individual words) and part of speech tags (as some POS tags are important in classification likeadverbs,adjectives etc.) as features. And we have used Gradient Boosting Classifier for classification. No extra data is used for training here. Now we describe the vectorizer tool, classification algorithms and other feature models in details. Datasets The TRAC 2020 Shared Task Organizers (Bhattacharya et al., 2020) provided datasets in 3 languages – English, Hindi and Indian Bengali. The English dataset contains 5,120 texts for training and 1,201 texts for testing. The Indian Bengali dataset contains 4,785 texts for training and 1,188 texts for testing (in both Roman and Bangla script). The Hindi dataset contains 4,981 texts for training and 1,200 texts for testing (in both Roman and Devanagari script). Table 1 presents the statistics of the shared task datasets provided by the Organizers. 4.1. Table 1: Dataset statistics Data English Hindi Bengali Training 5,120 4,981 4,785 Methodology Tf-Idf Vectorize"
2020.trac-1.14,P14-1146,0,0.0801807,"Missing"
2020.trac-1.14,W17-3008,0,0.0602269,"Missing"
2020.trac-1.14,W17-3003,0,0.0608743,"Missing"
2020.trac-1.14,S16-1173,0,0.0209777,"r this work. (Kwok and Wang., 2013) used uni-gram model for this task. (Chikashi Nobata and Chang, 2016) used different types of syntactic features and embedding features for aggression detection in text. (Mohammad, 2012) mapped hashtags like ‘yuck’, ‘joy’ into different types of emotions and classified the texts. In (Or˘asan, 2018), they used Support Vector 87 Machine and Random Forest as classifiers and emojis and sentiment scores were used as features. (Nemanja Djuric and Bhamidipati, 2015) used word embeddings which worked better than bag of words to detect aggressive text. (Jan Deriu and Jaggi, 2016) also did the work with the help of emotional sentiment. However, all the research works mentioned above are based on the English language (Jun-Ming Xu and Bellmore, 2012). These days, with the increasing availability of multi-lingual keypads in the mobile devices and the support for multi-lingual contents in the websites, detecting aggression from multi-lingual texts has become a necessity. (Vinay Singh and Shrivastava, 2018) used CNN and LSTM to detect aggression on HindiEnglish code-mixed texts. In (Shukrity Si, 2019), an ensembling method were used with the help of Aggression lexicons, sen"
2020.trac-1.14,N12-1084,0,0.325979,"Missing"
2020.trac-1.14,W14-3914,0,0.0662669,"Missing"
2020.trac-1.14,S12-1033,0,0.0420076,"this task is to handle code-mixing and code-switching in lan2. Related Work Although aggression detection in text is a relatively new research topic, quite a few research work have been carried out on this topic (AmirHRazavi and Matwin., 2010; Ritesh Kumar and Chennuru, 2018; Ritesh Kumar and Zampieri, 2020). (Duyu Tang and Qin, 2014) showed how positive and negative emoticons can be used for this work. (Kwok and Wang., 2013) used uni-gram model for this task. (Chikashi Nobata and Chang, 2016) used different types of syntactic features and embedding features for aggression detection in text. (Mohammad, 2012) mapped hashtags like ‘yuck’, ‘joy’ into different types of emotions and classified the texts. In (Or˘asan, 2018), they used Support Vector 87 Machine and Random Forest as classifiers and emojis and sentiment scores were used as features. (Nemanja Djuric and Bhamidipati, 2015) used word embeddings which worked better than bag of words to detect aggressive text. (Jan Deriu and Jaggi, 2016) also did the work with the help of emotional sentiment. However, all the research works mentioned above are based on the English language (Jun-Ming Xu and Bellmore, 2012). These days, with the increasing avai"
2020.trac-1.14,W18-4414,0,0.0359341,"Missing"
2020.trac-1.14,2020.trac-1.1,0,0.231789,"Missing"
2020.trac-1.14,W18-5106,0,0.0298225,"Missing"
2020.trac-1.14,D14-1105,0,0.0608433,"Missing"
2021.dravidianlangtech-1.46,2020.sltu-1.25,0,0.528227,"atform’s discussion rules are adhered to, including the prohibition of offensive languages. Moderators implement these rules by partly or entirely removing user comments. Typically, platform rules are available to the user in the form of guidelines. However, all user doesn’t follow the rules while commenting on a post. An increase of end-users in social platforms leads to the increasing number of comments that make the moderator restless to identify offensive and non-offensive. To intercept the circumstances recent trends of identification of offensive language have become a scientific asset (Chakravarthi et al., 2020c; Chakravarthi, 2020). In the context of Natural Language Processing (NLP), offensive language identification is a classification task that is aimed to identify and minimize offensive contents in social media (Mandl et al., 2020). There have been advancements in this domain of research both in industrial and academia with increasing access to larger and richer social media data over the years. India is a linguistically diverse country with 22 official languages with common languages used being English and Hindi. There has been a mixing of cultures and languages over the years thus leading to"
2021.dravidianlangtech-1.46,2020.sltu-1.28,0,0.568537,"atform’s discussion rules are adhered to, including the prohibition of offensive languages. Moderators implement these rules by partly or entirely removing user comments. Typically, platform rules are available to the user in the form of guidelines. However, all user doesn’t follow the rules while commenting on a post. An increase of end-users in social platforms leads to the increasing number of comments that make the moderator restless to identify offensive and non-offensive. To intercept the circumstances recent trends of identification of offensive language have become a scientific asset (Chakravarthi et al., 2020c; Chakravarthi, 2020). In the context of Natural Language Processing (NLP), offensive language identification is a classification task that is aimed to identify and minimize offensive contents in social media (Mandl et al., 2020). There have been advancements in this domain of research both in industrial and academia with increasing access to larger and richer social media data over the years. India is a linguistically diverse country with 22 official languages with common languages used being English and Hindi. There has been a mixing of cultures and languages over the years thus leading to"
2021.dravidianlangtech-1.46,2021.dravidianlangtech-1.17,0,0.0202878,"earning. Our proposed system is yet to explore the dataset. This paper aims to solve this research problem by using generalized Deep learning architectures named IndicBERT (Kakwani et al., 2020) and BERT (Devlin et al., 2019). The goal of this task is to identify offensive language content of the code-mixed dataset of comments or posts in Dravidian Languages collected from social media. The code-mixed language for the task was Tamil-English (Chakravarthi et al., 2020b), Malayalam-English (Chakravarthi et al., 2020a), and Kannada-English (Hande et al., 2020) provided by DravidianLangTech-2021 (Chakravarthi et al., 2021). The working system is available in GitHub1 . The rest of the paper has been organized as follows. Section 2 describes the data that was used to build the proposed Offensive Language Identification system. Section 3 describes the proposed model used to build the system and will be followed by the evaluation of the model in section 4. 2 Data The shared task organized by DravidianLangTech2021 provided the gold standard corpus for offensive language identification of code-mixed text for three different sets Tamil-English, MalayalamEnglish, and Kannada-English in Dravidian languages. The dataset"
2021.dravidianlangtech-1.46,N19-1423,0,0.0272018,"in code mixed text posted by users on YouTube social media, rather than in monolingual text from the textbook. Secondly, it has been annotated for two tasks, namely sentiment analysis and offensive language detection for underresourced Kannada language. Hence, KanCMD was meant to stimulate research in under-resourced Kannada language on real-world code-mixed social media text and multi-task learning. Our proposed system is yet to explore the dataset. This paper aims to solve this research problem by using generalized Deep learning architectures named IndicBERT (Kakwani et al., 2020) and BERT (Devlin et al., 2019). The goal of this task is to identify offensive language content of the code-mixed dataset of comments or posts in Dravidian Languages collected from social media. The code-mixed language for the task was Tamil-English (Chakravarthi et al., 2020b), Malayalam-English (Chakravarthi et al., 2020a), and Kannada-English (Hande et al., 2020) provided by DravidianLangTech-2021 (Chakravarthi et al., 2021). The working system is available in GitHub1 . The rest of the paper has been organized as follows. Section 2 describes the data that was used to build the proposed Offensive Language Identification"
2021.dravidianlangtech-1.46,2020.peoples-1.6,0,0.787278,"021 ©2021 Association for Computational Linguistics IndicBERT Text Classwise Confident Score Final one Hot class vector + Tokenization Not Offensive Sub Categories BERT Classwise Confident Score Offensive Figure 1: Framework for Offensive Language Identification whereas (Santosh and Aravind, 2019) dealt the task of identification of hate speech and offensive language from code-mixed social media text using two architecture namely sub-word level LSTM model and Hierarchical LSTM model with attention based on phonemic sub-words. Another real-world issue dataset have been released by the authors (Hande et al., 2020) who have introduced Kannada CodeMixed Dataset (KanCMD). The dataset is a multi-task learning dataset for sentiment analysis and offensive language identification. The KanCMD dataset highlights two real-world issues from the social media text. Firstly, it contains actual comments in code mixed text posted by users on YouTube social media, rather than in monolingual text from the textbook. Secondly, it has been annotated for two tasks, namely sentiment analysis and offensive language detection for underresourced Kannada language. Hence, KanCMD was meant to stimulate research in under-resourced"
2021.dravidianlangtech-1.46,2020.findings-emnlp.445,0,0.0959214,"Missing"
2021.dravidianlangtech-1.46,W18-3504,0,0.027528,"ication on social media texts which are largely code-mixed. Code-mixing refers to a prevalent phenomenon which exists in a multilingual community and the code-mixed texts are sometimes written in nonnative scripts. Any System which is trained on monolingual datasets miserably fails when exposed to code-mixed data due to the complexity of codeswitching at different linguistic levels in the text (Jose et al., 2020; Priyadharshini et al., 2020). Many researcher has proposed many different approaches to achieve the state-of-the-art results in code-mixing scenarios during the recent years. Author (Mathur et al., 2018) has solved the problem of classification of the tweets in Hindi-English Offensive Tweet (HEOT) dataset using transfer learning in which author has employed Convolutional Neural Networks as pre-trained on tweets in English followed by retraining on Hinglish tweets. HEOT dataset consists of Hindi-English code switched language into three classes nonoffensive, abusive and hate-speech. The author (Bohra et al., 2018) proposes a supervised classification system that detects hate speech in the text using various character level, word level, and lexicon based features 319 Proceedings of the First Wo"
2021.smm4h-1.30,2020.smm4h-1.5,0,0.0375634,"Missing"
2021.smm4h-1.30,N19-1423,0,0.0346929,"batching across examples. Earlier in context-representation there were two strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. However, both are limited to the fact that they are unidirectional language models and are unable to learn general language representations. The latest advances in Bidirectional Encoder Representations from Transformers (BERT) address both of these issues, as it is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers (Devlin et al., 2019). We have used small-BERT preprocessing and encoding to get vector representation of sentences, and finetuned BERT for the ternary classification task. The classification of medical symptoms from COVID-19 Twitter posts presents two key issues. Firstly, there is plenty of discourse around news and scientific articles that describe medical symptoms. While this discourse is not related to any user in particular, it enhances the difficulty of identifying valuable user-reported information. Secondly, many users describe symptoms that other people experience, instead of their own, as they are usuall"
2021.smm4h-1.30,D19-1374,0,0.014053,"©2021 Association for Computational Linguistics Dataset Train Validation Test LN 4277 247 - NP 3442 180 - Self 1248 73 - total 9067 500 6500 Table 1: Statistics of the dataset. LN: Lit-News, NP: Non-personal 3 3.1 System Description Data Preprocessing All apostrophe containing words were expanded. Characters like : , & ! ? were removed . Words were lower-cased to avoid capitalized version of the same word being treated as a different word. The emojis were removed using Python ""emoji"" library. Hashtags, mentions (words beginning with @) and urls were also removed. 3.2 Model We used Small_BERT (Tsai et al., 2019) encoder and preprocessing models to extract features from the sentence and used the pooled outputs from the encoder and fed it into a fully connected dense layer, a dropout layer (dropout rate=0.1) and a final dense layer with softmax activation. We used the learning rate of 3e-5 and the adam optimizer. We tested it for 5-10 epochs and obtained the best result after training the model for 9 epochs. On analysing the wrongly classified tweets in the validation set, we observed some interesting patterns. The sentence “Me and my girl swear we have already had COVID-19. We were sick for nearly a m"
C12-1010,D11-1033,0,0.214475,"Missing"
C12-1010,2011.mtsummit-papers.32,1,0.769403,"Missing"
C12-1010,2012.eamt-1.41,1,0.717213,"Missing"
C12-1010,2005.eamt-1.9,0,0.0486176,"Missing"
C12-1010,N09-1025,0,0.0684642,"Missing"
C12-1010,P11-2071,0,0.0542134,"Missing"
C12-1010,eck-etal-2004-language,0,0.0767698,"Missing"
C12-1010,D10-1044,0,0.208686,"Missing"
C12-1010,W07-0717,0,0.261809,"Missing"
C12-1010,2011.mtsummit-papers.10,0,0.207403,"Missing"
C12-1010,W11-2123,0,0.0575828,"Missing"
C12-1010,2005.eamt-1.19,0,0.198143,"Missing"
C12-1010,W04-3250,0,0.18146,"Missing"
C12-1010,2005.mtsummit-papers.11,0,0.0133593,"Missing"
C12-1010,P07-2045,0,0.0155806,"Missing"
C12-1010,W07-0733,0,0.288199,"Missing"
C12-1010,N10-1062,0,0.0748053,"Missing"
C12-1010,P03-1021,0,0.151809,"Missing"
C12-1010,J03-1002,0,0.0126689,"Missing"
C12-1010,P02-1040,0,0.0843489,"Missing"
C12-1010,2011.mtsummit-papers.27,1,0.837951,"Missing"
C12-1010,2006.amta-papers.25,0,0.0693185,"Missing"
C16-1241,2011.mtsummit-papers.35,1,0.926867,"Missing"
C16-1241,P15-2026,0,0.107171,"extracted a grammar for each input sentence and applied it to the model. Rosa et al. (2012) and Mareˇcek et al. (2011) applied a rule-based approach to APE of English–Czech MT outputs on the morphological level. They used 20 hand-written rules based on the most frequent errors encountered in translation. The method efficiently corrects morpho-syntactic categories of a word such as number, case, gender, person as well as dependency labels. The inclusion of source-language information in APE is also useful to improve the APE performance (B´echara et al., 2011). To overcome data sparsity issues, Chatterjee et al. (2015) proposed a pipeline where the best language model and pruned phrase table are selected through task-specific dense features. Recently, a bidirectional recurrent neural network model of APE using Tmt –Tpe was proposed by Pal et al. (2016) which consists of an encoder that encodes the MT output into a fixed-length vector from which a decoder provides a post-edited (PE) translation. They reported statistically significant improvement over a strong first stage MT system baseline. Various automatic or semi-automatic post-processing techniques to implement corrections of repetitive errors have been"
C16-1241,P05-1033,0,0.0631731,"n a different language. The same method was also applied to the monolingual Italian data. Next, the parallel corpus was further cleaned using the Gale-Church filtering method described in Tan and Pal (2014). We sorted the entire parallel training corpus based on sentence length and removed duplicates. We applied tokenization and punctuation normalization using the Moses scripts. 4.3 Experimental Settings In our APE experiments we first integrated the hybrid word alignment model (cf. Section 3.1) into the SAPE engines modelled with PB-SMT (Koehn et al., 2003) and hierarchical PB-SMT (HPB-SMT) (Chiang, 2005). For building our statistical APE system, we used maximum phrase length of 7 and a 5-gram language model trained using KenLM (Heafield, 2011). Model parameters were tuned using MERT (Och, 2003) on the held-out development set. 5 Evaluation During evaluation we take into consideration the output produced by all the three APE systems: PBSAPE with hybrid word alignment, HPB-SAPE with hybrid word alignment and the system combination system (SC-APE) which also includes the output from the first stage system Google MT. As a baseline APE system we use a PB-SAPE system with GIZA++ alignment. The eval"
C16-1241,P11-1043,0,0.137633,"ns in order to produce publishable quality translation (Roturier, 2009; TAUS/CNGL Report, 2010). Even though MT and APE output often need human PE, it is often faster and cheaper to post-edit MT and APE output than to perform human translation from scratch. System combination is a technology where multiple translation outputs from potentially very different MT systems are combined. System combination includes (i) hypothesis selection (Rosti et al., 2007a; Hildebrand and Vogel, 2010), (ii) confusion network based decoding (Matusov et al., 2006; Rosti et al., 2007b) and (iii) model combination (DeNero and Macherey, 2011). The confusion networks are built using backbone selection using either multiple hypotheses as backbones (Leusch and Ney, 2010) or a single backbone (Rosti et al., 2007b; Du et al., 2009) using TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). These alignment metrics select the hypothesis that agrees most with the other hypotheses on average. System combination can improve translation quality significantly which motivated us to apply the strategy for the APE task. Some of the research mentioned above studied the impacts of various factors and methods in APE on productivity gains. How"
C16-1241,W11-2123,0,0.0161942,"g the Gale-Church filtering method described in Tan and Pal (2014). We sorted the entire parallel training corpus based on sentence length and removed duplicates. We applied tokenization and punctuation normalization using the Moses scripts. 4.3 Experimental Settings In our APE experiments we first integrated the hybrid word alignment model (cf. Section 3.1) into the SAPE engines modelled with PB-SMT (Koehn et al., 2003) and hierarchical PB-SMT (HPB-SMT) (Chiang, 2005). For building our statistical APE system, we used maximum phrase length of 7 and a 5-gram language model trained using KenLM (Heafield, 2011). Model parameters were tuned using MERT (Och, 2003) on the held-out development set. 5 Evaluation During evaluation we take into consideration the output produced by all the three APE systems: PBSAPE with hybrid word alignment, HPB-SAPE with hybrid word alignment and the system combination system (SC-APE) which also includes the output from the first stage system Google MT. As a baseline APE system we use a PB-SAPE system with GIZA++ alignment. The evaluation was carried out in two ways: (i) automatic evaluation and (ii) human evaluation of the 1,000 testset sentences automatically post-edite"
C16-1241,W10-1745,0,0.0277968,"f repetitive errors have been developed, although often the overall resulting MT output after APE still needs to be 2560 post-edited by humans in order to produce publishable quality translation (Roturier, 2009; TAUS/CNGL Report, 2010). Even though MT and APE output often need human PE, it is often faster and cheaper to post-edit MT and APE output than to perform human translation from scratch. System combination is a technology where multiple translation outputs from potentially very different MT systems are combined. System combination includes (i) hypothesis selection (Rosti et al., 2007a; Hildebrand and Vogel, 2010), (ii) confusion network based decoding (Matusov et al., 2006; Rosti et al., 2007b) and (iii) model combination (DeNero and Macherey, 2011). The confusion networks are built using backbone selection using either multiple hypotheses as backbones (Leusch and Ney, 2010) or a single backbone (Rosti et al., 2007b; Du et al., 2009) using TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). These alignment metrics select the hypothesis that agrees most with the other hypotheses on average. System combination can improve translation quality significantly which motivated us to apply the strategy"
C16-1241,N03-1017,0,0.189052,"ased on a number of alignment approaches, (ii) PB-SAPE, (iii) HPB-SAPE, and (iv) a system combination module (also including the first stage MT system). The SAPE systems are trained monolingually with Italian Tmt generated by Google Translate (GT) and the manually post-edited translations Tpe . 3.1 A Hybrid Word Alignment Model for Target Side APE Previous research in MT demonstrates that a combination of information coming from multiple alignment models can improve translation quality. This can be achieved in different ways, e.g., by combining exactly two bidirectional alignments (Och, 2003; Koehn et al., 2003; DeNero and Macherey, 2011), combining an arbitrary number of alignments (Tu et al., 2012; Pal et al., 2013), by constructing weighted alignment matrices over 1-best alignments from multiple alignments generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006"
C16-1241,J10-4005,0,0.207854,"ly two bidirectional alignments (Och, 2003; Koehn et al., 2003; DeNero and Macherey, 2011), combining an arbitrary number of alignments (Tu et al., 2012; Pal et al., 2013), by constructing weighted alignment matrices over 1-best alignments from multiple alignments generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on TER (Translation Edit Rate) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow Pal et al. (2013) in combining word alignment tables, however, we additionally use 3-word consistent phrases to generate more alignment links (cf. Section 3.1.3). We integrate the word alignment obtained with this hybrid model into our PB-SAPE (Pal et al., 2015) and HPB-SAPE (Pal, 2015) models. 3.1.1 Statistica"
C16-1241,N04-1022,0,0.166533,"red as Sa . • Step 3: Delete all the alignment points aij ∈ Sc such that ∃aik ∈ a4 ∪ a5 where j 6= k. • Step 4: Update Sc as Sc = Sc ∪ a4 ∪ a5 . 3.3 System Combination for APE Our system combination framework selects the best hypothesis translation from multiple hypotheses produced by different systems. In order to apply the system combination framework on the translations produced by our SAPE systems and the baseline MT system (Google Translate) we implemented the Minimum Bayes Risk (MBR) coupled with the Confusion Network (MBRCN) framework as described in (Du et al., 2009). The MBR decoder (Kumar and Byrne, 2004) selects for each sentence the best system output from the three outputs by minimizing BLEU (Papineni et al., 2002) loss. This output is known as the backbone. A confusion network (Matusov et al., 2006) is built from the backbone while the remaining hypotheses are aligned against the backbone using the edit-distance based alignment methods (cf. Section 3.1.2). The features used to score each arc in the confusion network (CN) are word posterior probability, target language model and length penalties. Minimum Error Rate Training (MERT) (Och, 2003) is applied to tune the CN weights. In our experi"
C16-1241,W07-0734,0,0.071243,"generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on TER (Translation Edit Rate) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow Pal et al. (2013) in combining word alignment tables, however, we additionally use 3-word consistent phrases to generate more alignment links (cf. Section 3.1.3). We integrate the word alignment obtained with this hybrid model into our PB-SAPE (Pal et al., 2015) and HPB-SAPE (Pal, 2015) models. 3.1.1 Statistical Word Alignment GIZA++ is a statistical word alignment tool which implements IBM models 1–5, an HMM alignment model, as well as the IBM-6 model for covering many to many alignments. The Berkeley word aligner uses an extension of Cross Expectation Maximization and is jointly"
C16-1241,W10-1747,0,0.0191016,"need human PE, it is often faster and cheaper to post-edit MT and APE output than to perform human translation from scratch. System combination is a technology where multiple translation outputs from potentially very different MT systems are combined. System combination includes (i) hypothesis selection (Rosti et al., 2007a; Hildebrand and Vogel, 2010), (ii) confusion network based decoding (Matusov et al., 2006; Rosti et al., 2007b) and (iii) model combination (DeNero and Macherey, 2011). The confusion networks are built using backbone selection using either multiple hypotheses as backbones (Leusch and Ney, 2010) or a single backbone (Rosti et al., 2007b; Du et al., 2009) using TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). These alignment metrics select the hypothesis that agrees most with the other hypotheses on average. System combination can improve translation quality significantly which motivated us to apply the strategy for the APE task. Some of the research mentioned above studied the impacts of various factors and methods in APE on productivity gains. However, those studies were not conducted to observe PE effort in commercial environments. The focus of our study is twofold - to e"
C16-1241,N06-1014,0,0.0609824,"Koehn et al., 2003; DeNero and Macherey, 2011), combining an arbitrary number of alignments (Tu et al., 2012; Pal et al., 2013), by constructing weighted alignment matrices over 1-best alignments from multiple alignments generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on TER (Translation Edit Rate) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow Pal et al. (2013) in combining word alignment tables, however, we additionally use 3-word consistent phrases to generate more alignment links (cf. Section 3.1.3). We integrate the word alignment obtained with this hybrid model into our PB-SAPE (Pal et al., 2015) and HPB-SAPE (Pal, 2015) models. 3.1.1 Statistical Word Alignment GIZA++ is a statistical word align"
C16-1241,D09-1106,0,0.0257389,"the manually post-edited translations Tpe . 3.1 A Hybrid Word Alignment Model for Target Side APE Previous research in MT demonstrates that a combination of information coming from multiple alignment models can improve translation quality. This can be achieved in different ways, e.g., by combining exactly two bidirectional alignments (Och, 2003; Koehn et al., 2003; DeNero and Macherey, 2011), combining an arbitrary number of alignments (Tu et al., 2012; Pal et al., 2013), by constructing weighted alignment matrices over 1-best alignments from multiple alignments generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on TER (Translation Edit Rate) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow Pal et al. ("
C16-1241,E06-1005,0,0.461663,"th International Conference on Computational Linguistics: Technical Papers, pages 2559–2570, Osaka, Japan, December 11-17 2016. without the availability of Sip using only sufficient amounts of parallel “target-side” Tmt –Tpe text within the statistical MT (SMT) framework. Usually APE tasks focus on systematic errors made by MT systems - the most frequent ones being incorrect lexical choices, incorrect word ordering, incorrect insertion or deletion of a word. The system presented in this paper explores the use of system combination in APE. System combination in MT has been studied extensively (Matusov et al., 2006; Du et al., 2009; Pal et al., 2014), except in the context of APE. Here we use system combination architectures on three different levels: (i) sequential combination between first-stage system and APE, (ii) parallel combination of alignment systems at the level of the APE and (iii) parallel combination of APE MT systems (including the first stage MT system). More precisely, our approach makes use of a hybrid implementation of multiple alignment combination within phrase-based SAPE (PB-SAPE) and hierarchical PB-SAPE (HPB-SAPE) and a system combination framework (a multi-engine pipeline) – that"
C16-1241,J03-1002,0,0.00748102,"on quality. This can be achieved in different ways, e.g., by combining exactly two bidirectional alignments (Och, 2003; Koehn et al., 2003; DeNero and Macherey, 2011), combining an arbitrary number of alignments (Tu et al., 2012; Pal et al., 2013), by constructing weighted alignment matrices over 1-best alignments from multiple alignments generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on TER (Translation Edit Rate) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow Pal et al. (2013) in combining word alignment tables, however, we additionally use 3-word consistent phrases to generate more alignment links (cf. Section 3.1.3). We integrate the word alignment obtained with this hybrid model into our PB-SAP"
C16-1241,P03-1021,0,0.385927,"ent model based on a number of alignment approaches, (ii) PB-SAPE, (iii) HPB-SAPE, and (iv) a system combination module (also including the first stage MT system). The SAPE systems are trained monolingually with Italian Tmt generated by Google Translate (GT) and the manually post-edited translations Tpe . 3.1 A Hybrid Word Alignment Model for Target Side APE Previous research in MT demonstrates that a combination of information coming from multiple alignment models can improve translation quality. This can be achieved in different ways, e.g., by combining exactly two bidirectional alignments (Och, 2003; Koehn et al., 2003; DeNero and Macherey, 2011), combining an arbitrary number of alignments (Tu et al., 2012; Pal et al., 2013), by constructing weighted alignment matrices over 1-best alignments from multiple alignments generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment"
C16-1241,W13-2814,1,0.950207,"Sip information. System combination and hybrid word alignment strategies are commonly used in MT, however to the best of our knowledge the work presented in this paper is the first approach to APE that uses system combination and hybrid word alignment methods within the APE engine. System combination has been found to be a very useful technique in MT where translation hypotheses from multiple MT engines are available. Motivated by the success of system combination in MT, we applied system combination in APE. Similarly, the use of multiple word alignments has been shown to improve MT results (Pal et al., 2013). For our APE, alignments have to be produced on “monolingual” target-side data (Tmt and Tpe ). A particular focus of our paper is to explore the performance of hybrid alignments based on combinations of statistical and edit-distance based aligners in this “monolingual” setting. The remainder of the paper is organized as follows. Section 2 gives an overview of the related work. Section 3 describes the components of our SAPE system. Section 4 outlines the data and data preprocessing and the experimental setup. Section 5 presents the results of automatic and human evaluation, followed by conclus"
C16-1241,W15-3026,1,0.8818,"Missing"
C16-1241,P16-2046,1,0.84354,"Missing"
C16-1241,P02-1040,0,0.0959305,". System combination is a technology where multiple translation outputs from potentially very different MT systems are combined. System combination includes (i) hypothesis selection (Rosti et al., 2007a; Hildebrand and Vogel, 2010), (ii) confusion network based decoding (Matusov et al., 2006; Rosti et al., 2007b) and (iii) model combination (DeNero and Macherey, 2011). The confusion networks are built using backbone selection using either multiple hypotheses as backbones (Leusch and Ney, 2010) or a single backbone (Rosti et al., 2007b; Du et al., 2009) using TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). These alignment metrics select the hypothesis that agrees most with the other hypotheses on average. System combination can improve translation quality significantly which motivated us to apply the strategy for the APE task. Some of the research mentioned above studied the impacts of various factors and methods in APE on productivity gains. However, those studies were not conducted to observe PE effort in commercial environments. The focus of our study is twofold - to examine how existing word alignment techniques and a system combination framework can be intelligently used to improve monoli"
C16-1241,W12-3146,0,0.370811,"Missing"
C16-1241,N07-1029,0,0.0327304,"plement corrections of repetitive errors have been developed, although often the overall resulting MT output after APE still needs to be 2560 post-edited by humans in order to produce publishable quality translation (Roturier, 2009; TAUS/CNGL Report, 2010). Even though MT and APE output often need human PE, it is often faster and cheaper to post-edit MT and APE output than to perform human translation from scratch. System combination is a technology where multiple translation outputs from potentially very different MT systems are combined. System combination includes (i) hypothesis selection (Rosti et al., 2007a; Hildebrand and Vogel, 2010), (ii) confusion network based decoding (Matusov et al., 2006; Rosti et al., 2007b) and (iii) model combination (DeNero and Macherey, 2011). The confusion networks are built using backbone selection using either multiple hypotheses as backbones (Leusch and Ney, 2010) or a single backbone (Rosti et al., 2007b; Du et al., 2009) using TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). These alignment metrics select the hypothesis that agrees most with the other hypotheses on average. System combination can improve translation quality significantly which motiv"
C16-1241,P07-1040,0,0.0305381,"plement corrections of repetitive errors have been developed, although often the overall resulting MT output after APE still needs to be 2560 post-edited by humans in order to produce publishable quality translation (Roturier, 2009; TAUS/CNGL Report, 2010). Even though MT and APE output often need human PE, it is often faster and cheaper to post-edit MT and APE output than to perform human translation from scratch. System combination is a technology where multiple translation outputs from potentially very different MT systems are combined. System combination includes (i) hypothesis selection (Rosti et al., 2007a; Hildebrand and Vogel, 2010), (ii) confusion network based decoding (Matusov et al., 2006; Rosti et al., 2007b) and (iii) model combination (DeNero and Macherey, 2011). The confusion networks are built using backbone selection using either multiple hypotheses as backbones (Leusch and Ney, 2010) or a single backbone (Rosti et al., 2007b; Du et al., 2009) using TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). These alignment metrics select the hypothesis that agrees most with the other hypotheses on average. System combination can improve translation quality significantly which motiv"
C16-1241,N07-1064,0,0.171061,"paper is to explore the performance of hybrid alignments based on combinations of statistical and edit-distance based aligners in this “monolingual” setting. The remainder of the paper is organized as follows. Section 2 gives an overview of the related work. Section 3 describes the components of our SAPE system. Section 4 outlines the data and data preprocessing and the experimental setup. Section 5 presents the results of automatic and human evaluation, followed by conclusions and avenues for further research in Section 6. 2 Related Research APE approaches cover a wide methodological range. Simard et al. (2007a) and Simard et al. (2007b) applied phrase-based SMT (PB-SMT) for post-editing that handles the repetitive nature of errors typically made by rule-based MT (RBMT) systems. The APE system was trained on the output of the rule-based system as the source language and reference human translations as the target language. This APE system was able to correct systematic errors produced by the RBMT system and reduce the post-editing effort. The approach achieved large improvements in performance not only over the baseline rule-based system but also over a similar PB-SMT used in a standalone mode. Denk"
C16-1241,W07-0728,0,0.0686589,"paper is to explore the performance of hybrid alignments based on combinations of statistical and edit-distance based aligners in this “monolingual” setting. The remainder of the paper is organized as follows. Section 2 gives an overview of the related work. Section 3 describes the components of our SAPE system. Section 4 outlines the data and data preprocessing and the experimental setup. Section 5 presents the results of automatic and human evaluation, followed by conclusions and avenues for further research in Section 6. 2 Related Research APE approaches cover a wide methodological range. Simard et al. (2007a) and Simard et al. (2007b) applied phrase-based SMT (PB-SMT) for post-editing that handles the repetitive nature of errors typically made by rule-based MT (RBMT) systems. The APE system was trained on the output of the rule-based system as the source language and reference human translations as the target language. This APE system was able to correct systematic errors produced by the RBMT system and reduce the post-editing effort. The approach achieved large improvements in performance not only over the baseline rule-based system but also over a similar PB-SMT used in a standalone mode. Denk"
C16-1241,2006.amta-papers.25,0,0.54011,"human translation from scratch. System combination is a technology where multiple translation outputs from potentially very different MT systems are combined. System combination includes (i) hypothesis selection (Rosti et al., 2007a; Hildebrand and Vogel, 2010), (ii) confusion network based decoding (Matusov et al., 2006; Rosti et al., 2007b) and (iii) model combination (DeNero and Macherey, 2011). The confusion networks are built using backbone selection using either multiple hypotheses as backbones (Leusch and Ney, 2010) or a single backbone (Rosti et al., 2007b; Du et al., 2009) using TER (Snover et al., 2006) or BLEU (Papineni et al., 2002). These alignment metrics select the hypothesis that agrees most with the other hypotheses on average. System combination can improve translation quality significantly which motivated us to apply the strategy for the APE task. Some of the research mentioned above studied the impacts of various factors and methods in APE on productivity gains. However, those studies were not conducted to observe PE effort in commercial environments. The focus of our study is twofold - to examine how existing word alignment techniques and a system combination framework can be inte"
C16-1241,W14-3323,1,0.846697,"d a language identifier (Shuyo, 2010) on both bilingual English–Italian MT output and MT 2 3 Empirically best preforming aligner among the individual aligners (a1 , a2 or a3 ), is considered as Sa . https://www.matecat.com/ 2563 output–PE (Italian) parallel data. We discarded those sentence pairs from the bilingual training data which are considered as belonging to a different language or contain segment(s) in a different language. The same method was also applied to the monolingual Italian data. Next, the parallel corpus was further cleaned using the Gale-Church filtering method described in Tan and Pal (2014). We sorted the entire parallel training corpus based on sentence length and removed duplicates. We applied tokenization and punctuation normalization using the Moses scripts. 4.3 Experimental Settings In our APE experiments we first integrated the hybrid word alignment model (cf. Section 3.1) into the SAPE engines modelled with PB-SMT (Koehn et al., 2003) and hierarchical PB-SMT (HPB-SMT) (Chiang, 2005). For building our statistical APE system, we used maximum phrase length of 7 and a 5-gram language model trained using KenLM (Heafield, 2011). Model parameters were tuned using MERT (Och, 2003"
C16-1241,I11-1145,0,0.0155645,"edited translations Tpe . 3.1 A Hybrid Word Alignment Model for Target Side APE Previous research in MT demonstrates that a combination of information coming from multiple alignment models can improve translation quality. This can be achieved in different ways, e.g., by combining exactly two bidirectional alignments (Och, 2003; Koehn et al., 2003; DeNero and Macherey, 2011), combining an arbitrary number of alignments (Tu et al., 2012; Pal et al., 2013), by constructing weighted alignment matrices over 1-best alignments from multiple alignments generated by different models (Liu et al., 2009; Tu et al., 2011) etc. Below we apply an alignment combination model to APE. Our hybrid word alignment method combines word alignments produced by three different statistical word alignment methods: (i) GIZA++ (Och and Ney, 2003) word alignment with grow-diag-finaland (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on TER (Translation Edit Rate) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow Pal et al. (2013) in combining"
C16-1241,C12-2122,1,0.889141,"Missing"
C16-1241,W14-0314,0,0.0161135,"amount of manual effort (TAUS Report, 2010). While MT is often not perfect, post-editing MT can yield productivity gains as post-editing MT output may require less effort compared to translating the same input manually from scratch. MT outputs are often post-edited by professional translators and the use of MT has become an important part of the translation workflow. A number of studies confirm that post-editing MT output can improve translators’ performance in terms of productivity and it may positively impact on translation quality and consistency (Guerberof, 2009; Plitt and Masselot, 2010; Zampieri and Vela, 2014). The wide use of MT in modern translation workflows in the localization industry, in turn, has resulted in substantial quantities of PE data which can be used to develop APE systems. APE (Knight and Chander, 1994) has been proposed as an automatic method for improving raw MT output, before performing actual human post-editing on it. The approach is based on collecting human corrected output of a first stage MT system and using this to train a system to correct errors produced by the MT system, possibly resulting in a productivity increase in the translation process. The advantage of APE relie"
C16-1241,W09-0416,0,\N,Missing
C16-2021,C14-2028,0,0.0270396,"t post-editing MT output increases translators’ productivity and improves translation consistency (Guerberof, 2009; Plitt and Masselot, 2010; Zampieri and Vela, 2014). Alongside classical TM matches, computer-aided translation (CAT) Tools that integrate MT and TM output are a trend in the translation and localization industries providing translators more useful suggestions. Another important trend is the development of web-based CAT tools which require no local software installation and allow teams of translators to work on the same project simultaneously (e.g., WordFast Anywhere1 , MateCat2 (Federico et al., 2014), and Wordbee3 , Lilt4 etc.). This paper presents CATaLog Online, a web-based CAT tool that provides translators MT, TM and APE output and ensures data capture for APE development and translation process research. The MT and APE systems integrated in CATaLog Online are based on Pal et al. (2015) and Pal et al. (2016b), respectively. In this paper, we present the key features implemented in CATaLog Online and their importance to translation project managers, translators, and MT and APE developers. Compared to state-ofthe-art CAT tools (e.g., MateCat, Lilt) CATaLog Online offers the following ad"
C16-2021,W15-5206,1,0.763618,"Missing"
C16-2021,W15-3017,1,0.660709,"Missing"
C16-2021,L16-1095,1,0.88264,"Missing"
C16-2021,W16-2379,1,0.862937,"Missing"
C16-2021,W14-0314,1,0.843535,"th current state-of-the-art CAT tools, CATaLog Online provides an enhanced interface, an option to integrate APE and more informative logs to help translation process research. 1 Introduction Machine translation (MT) technology has improved substantially over the past few decades. MT output is no longer used just for gisting but also for post-editing by professional translators as an important part of the translation workflow. Several studies confirm that post-editing MT output increases translators’ productivity and improves translation consistency (Guerberof, 2009; Plitt and Masselot, 2010; Zampieri and Vela, 2014). Alongside classical TM matches, computer-aided translation (CAT) Tools that integrate MT and TM output are a trend in the translation and localization industries providing translators more useful suggestions. Another important trend is the development of web-based CAT tools which require no local software installation and allow teams of translators to work on the same project simultaneously (e.g., WordFast Anywhere1 , MateCat2 (Federico et al., 2014), and Wordbee3 , Lilt4 etc.). This paper presents CATaLog Online, a web-based CAT tool that provides translators MT, TM and APE output and ensur"
E17-2056,P15-2026,0,0.249424,"human post-editing (Knight and Chander, 1994). APE assumes the availability of source texts (src), corresponding MT output (mt) and the human postedited (pe) version of mt. However, APE systems can also be built without the availability of src, by using only sufficient amounts of target side “mono-lingual” parallel mt–pe data. Usually APE tasks focus on systematic errors made by first stage MT systems, acting as an effective remedy to some of the inaccuracies in raw MT output. APE approaches cover a wide methodological range such as SMT techniques (Simard et al., 2007a; Simard et al., 2007b; Chatterjee et al., 2015; Pal et al., 2015; Pal et al., 2016d) real time integration of post-editing in MT (Denkowski, 2015), rule-based approaches to APE (Mareˇcek et al., 2011; Rosa et al., 2012), neural APE (JunczysDowmunt and Grundkiewicz, 2016; Pal et al., 2016b), multi-engine and multi-alignment APE (Pal et al., 2016a), etc. We present a second-stage machine translation (MT) system based on a neural machine translation (NMT) approach to automatic post-editing (APE) that improves the translation quality provided by a firststage MT system. Our APE system (AP ESym ) is an extended version of an attention based NMT"
E17-2056,N16-1102,0,0.204776,"rate a corresponding aid based on the context swid appears in. The APE words are generated from aid by looking up the hybrid prior alignment look-up table (LUT). Neural MT jointly learns alignment and translation. Replacing the source and target words by swid and aid , respectively, implicitly integrates the prior alignment and lessens the burden of the attention model. Secondly, our approach bears a resemblance to the sense embedding approach (Li and Jurafsky, 2015) since an embedding is generated for each (swid , aid ) pair. quality. Our neural model of APE is based on the work described in Cohn et al. (2016) which implements structural alignment biases into an attention based bidirectional recurrent neural network (RNN) MT model (Bahdanau et al., 2015). Cohn et al. (2016) extends the attentional soft alignment model to traditional word alignment models (IBM models) and agreement over both translation directions (in our case mt → pe and pe → mt) to ensure better alignment consistency. We follow Cohn et al. (2016) in encouraging our alignment models to be symmetric (Och and Ney, 2003) in both translation directions with embedded prior alignments. Different from Cohn et al. (2016), we employed prior"
E17-2056,D15-1200,0,0.0129567,"unique identification number (aid ) and a vector representation is generated for each such aid . Given a swid , the neural APE model is trained to generate a corresponding aid based on the context swid appears in. The APE words are generated from aid by looking up the hybrid prior alignment look-up table (LUT). Neural MT jointly learns alignment and translation. Replacing the source and target words by swid and aid , respectively, implicitly integrates the prior alignment and lessens the burden of the attention model. Secondly, our approach bears a resemblance to the sense embedding approach (Li and Jurafsky, 2015) since an embedding is generated for each (swid , aid ) pair. quality. Our neural model of APE is based on the work described in Cohn et al. (2016) which implements structural alignment biases into an attention based bidirectional recurrent neural network (RNN) MT model (Bahdanau et al., 2015). Cohn et al. (2016) extends the attentional soft alignment model to traditional word alignment models (IBM models) and agreement over both translation directions (in our case mt → pe and pe → mt) to ensure better alignment consistency. We follow Cohn et al. (2016) in encouraging our alignment models to b"
E17-2056,W06-1607,0,0.0220804,"rd pairs from hybrid prior alignment (Section 2.1) between mt–pe (12K data) were used for the additional training data to build AP EB2 . The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both the source and target language. Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (1) in the PB-SMT framework. To compensate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003). kx and ky correspond to the vocabulary sizes of source and target languages, respectively. The hidden state of the decoder at time t is computed as ηt = f (ηt−1 , yt−1 , ct ), where ct is the context vecP x tor computed as ct = Ti=1 αti hi . Here, αti is the weight of each hi and can be computed as in Equation 1 exp(eti ) αti = Pm (1) j=1 exp(etj ) where eti = a(ηt−1 , hi ) is a word alignment model. Based on the input (mt) and output (pe) sequence lengths, Tx and Ty , the alignment model is computed Tx × Ty"
E17-2056,N06-1014,0,0.513033,"corrected by human translators. This task is referred to as post-editing (PE). PE is often understood as the process of improving a translation provided by an MT system with the minimum In this paper we present a neural network based APE system to improve raw first-stage MT output 349 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 349–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013; Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (swid ). Each mt–pe word alignment also gets a unique identification number (aid ) and a vector representation is gener"
E17-2056,D08-1089,0,0.0468216,"ata. For building our AP EB2 system, we set a maximum phrase length of 7 for the translation model, and a 5-gram language model was trained using KenLM (Heafield, 2011). Word alignments between the mt and pe (4.5M synthetic mt-pe data + 12K WMT APE data) were established using the Berkeley Aligner (Liang et al., 2006), while word pairs from hybrid prior alignment (Section 2.1) between mt–pe (12K data) were used for the additional training data to build AP EB2 . The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both the source and target language. Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (1) in the PB-SMT framework. To compensate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003). kx and ky correspond to the vocabulary sizes of source and target languages, respectively. The hidden state of the decoder at time t is computed as ηt = f (ηt−1 , yt−1 , ct ), where ct is th"
E17-2056,W11-2123,0,0.0229802,"mt–pe symmetric model (AP ESym ) against the best performing system (W M TBest ) in the WMT 2016 APE task and the standard log-linear mt–pe PB-SMT model with hybrid prior alignment as described in Section 2.1 (AP EB2 ). AP EB2 and AP ESym models are trained on 4.55M (4.5M + 12K + pre-aligned word pairs) parallel mt–pe data. The pre-aligned word pairs are obtained from the hybrid prior word alignments (Section 2.1) of the 12K WMT APE training data. For building our AP EB2 system, we set a maximum phrase length of 7 for the translation model, and a 5-gram language model was trained using KenLM (Heafield, 2011). Word alignments between the mt and pe (4.5M synthetic mt-pe data + 12K WMT APE data) were established using the Berkeley Aligner (Liang et al., 2006), while word pairs from hybrid prior alignment (Section 2.1) between mt–pe (12K data) were used for the additional training data to build AP EB2 . The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) method (Galley and Manning, 2008) and conditioned on both the source and target language. Phrase pairs that occur only once in the training data are assigned an unduly high pro"
E17-2056,J03-1002,0,0.0268,"edding is generated for each (swid , aid ) pair. quality. Our neural model of APE is based on the work described in Cohn et al. (2016) which implements structural alignment biases into an attention based bidirectional recurrent neural network (RNN) MT model (Bahdanau et al., 2015). Cohn et al. (2016) extends the attentional soft alignment model to traditional word alignment models (IBM models) and agreement over both translation directions (in our case mt → pe and pe → mt) to ensure better alignment consistency. We follow Cohn et al. (2016) in encouraging our alignment models to be symmetric (Och and Ney, 2003) in both translation directions with embedded prior alignments. Different from Cohn et al. (2016), we employed prior alignment computed by a hybrid multi-alignment approach. Evaluation results show consistent improvements over the raw firststage MT system output and over the previous best performing neural APE (Junczys-Dowmunt and Grundkiewicz, 2016) on the WMT 2016 APE test set. In addition we show that re-ranking n-best output from baseline and enhanced PB-SMT APE systems (Section 3) together with our neural APE output provides further statistically significant improvements over all the othe"
E17-2056,W16-2378,0,0.447981,"alignment model to traditional word alignment models (IBM models) and agreement over both translation directions (in our case mt → pe and pe → mt) to ensure better alignment consistency. We follow Cohn et al. (2016) in encouraging our alignment models to be symmetric (Och and Ney, 2003) in both translation directions with embedded prior alignments. Different from Cohn et al. (2016), we employed prior alignment computed by a hybrid multi-alignment approach. Evaluation results show consistent improvements over the raw firststage MT system output and over the previous best performing neural APE (Junczys-Dowmunt and Grundkiewicz, 2016) on the WMT 2016 APE test set. In addition we show that re-ranking n-best output from baseline and enhanced PB-SMT APE systems (Section 3) together with our neural APE output provides further statistically significant improvements over all the other systems. The main contributions of our research are (i) an application of bilingual symmetry of the bidirectional RNN for APE, (ii) using a hybrid multialignment based approach for the prior alignments, (iii) a smart way of embedding word alignment information in neural APE, and (iv) applying reranking for the APE task. The remainder of the paper i"
E17-2056,P03-1021,0,0.127663,"= σ(W r Ex is the word embedding matrix of the MT output, W r ∈ Rm×n and U r ∈ Rn×n are weight matrices, m is the word embedding dimensionality and n represents the number of hidden units. Symmetric Neural Automatic Post Editing Using Prior Alignment Below we describe bilingual symmetry of bidirectional RNN with embedded prior word alignment for APE. 2.1 Symmetric Neural APE Hybrid Prior Alignment The monolingual mt–pe parallel corpus is first word aligned using a hybrid word alignment method based on the alignment combination of three different statistical word alignment methods: (i) GIZA++ (Och, 2003) word alignment with 350 data described in Bojar et al. (2016) and for some experiments we also use the 4.5M artificially developed APE data described in Junczys-Dowmunt and Grundkiewicz (2016). The training data consists of English–German triplets containing source English text (src) from the IT domain, corresponding German translations (mt) from a firststage MT system and the corresponding human post-edited version (pe). Development and test data contain 1,000 and 2,000 triplets respectively. We considered two baselines: (i) the raw MT output provided by the first-stage MT system serves as B"
E17-2056,W13-2814,1,0.838103,"dings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 349–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013; Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (swid ). Each mt–pe word alignment also gets a unique identification number (aid ) and a vector representation is generated for each such aid . Given a swid , the neural APE model is trained to generate a corresponding aid based on the context swid appears in. The APE words are generated from aid by looking up the hybrid prior alignment look-up table (LUT). Neural MT jointly learns alignment and translatio"
E17-2056,W15-3026,1,0.847079,"Missing"
E17-2056,W04-3250,0,0.0339845,"Missing"
E17-2056,J10-4005,0,0.0193819,"ions produced by MT systems often need to be corrected by human translators. This task is referred to as post-editing (PE). PE is often understood as the process of improving a translation provided by an MT system with the minimum In this paper we present a neural network based APE system to improve raw first-stage MT output 349 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 349–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013; Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (swid ). Each mt–pe word alignment also gets a unique identification"
E17-2056,C16-1241,1,0.878149,"Missing"
E17-2056,W07-0734,0,0.0440759,"network based APE system to improve raw first-stage MT output 349 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 349–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013; Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (swid ). Each mt–pe word alignment also gets a unique identification number (aid ) and a vector representation is generated for each such aid . Given a swid , the neural APE model is trained to generate a corresponding aid based on the context swid appears in. The APE words are generated from aid by looking up the hybrid prior alignment look-"
E17-2056,P16-2046,1,0.906567,"Missing"
E17-2056,C16-2021,1,0.88815,"Missing"
E17-2056,W16-2379,1,0.894217,"Missing"
E17-2056,P02-1040,0,0.0983167,"Missing"
E17-2056,W12-3146,0,0.137315,"Missing"
E17-2056,N07-1064,0,0.44034,"ving raw MT output, before performing actual human post-editing (Knight and Chander, 1994). APE assumes the availability of source texts (src), corresponding MT output (mt) and the human postedited (pe) version of mt. However, APE systems can also be built without the availability of src, by using only sufficient amounts of target side “mono-lingual” parallel mt–pe data. Usually APE tasks focus on systematic errors made by first stage MT systems, acting as an effective remedy to some of the inaccuracies in raw MT output. APE approaches cover a wide methodological range such as SMT techniques (Simard et al., 2007a; Simard et al., 2007b; Chatterjee et al., 2015; Pal et al., 2015; Pal et al., 2016d) real time integration of post-editing in MT (Denkowski, 2015), rule-based approaches to APE (Mareˇcek et al., 2011; Rosa et al., 2012), neural APE (JunczysDowmunt and Grundkiewicz, 2016; Pal et al., 2016b), multi-engine and multi-alignment APE (Pal et al., 2016a), etc. We present a second-stage machine translation (MT) system based on a neural machine translation (NMT) approach to automatic post-editing (APE) that improves the translation quality provided by a firststage MT system. Our APE system (AP ESym )"
E17-2056,W07-0728,0,0.0707865,"ving raw MT output, before performing actual human post-editing (Knight and Chander, 1994). APE assumes the availability of source texts (src), corresponding MT output (mt) and the human postedited (pe) version of mt. However, APE systems can also be built without the availability of src, by using only sufficient amounts of target side “mono-lingual” parallel mt–pe data. Usually APE tasks focus on systematic errors made by first stage MT systems, acting as an effective remedy to some of the inaccuracies in raw MT output. APE approaches cover a wide methodological range such as SMT techniques (Simard et al., 2007a; Simard et al., 2007b; Chatterjee et al., 2015; Pal et al., 2015; Pal et al., 2016d) real time integration of post-editing in MT (Denkowski, 2015), rule-based approaches to APE (Mareˇcek et al., 2011; Rosa et al., 2012), neural APE (JunczysDowmunt and Grundkiewicz, 2016; Pal et al., 2016b), multi-engine and multi-alignment APE (Pal et al., 2016a), etc. We present a second-stage machine translation (MT) system based on a neural machine translation (NMT) approach to automatic post-editing (APE) that improves the translation quality provided by a firststage MT system. Our APE system (AP ESym )"
E17-2056,2006.amta-papers.25,0,0.0951164,"In this paper we present a neural network based APE system to improve raw first-stage MT output 349 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 349–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics grow-diag-final-and (GDFA) heuristic (Koehn, 2010), (ii) Berkeley word alignment (Liang et al., 2006), and (iii) SymGiza++ (Junczys-Dowmunt and Szał, 2012) word alignment, as well as two different edit distance based word aligners based on Translation Edit Rate (TER) (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007). We follow the alignment strategy described in (Pal et al., 2013; Pal et al., 2016a). The aligned word pairs are added as additional training examples to train our symmetric neural APE model. Each word in the first stage MT output is assigned a unique id (swid ). Each mt–pe word alignment also gets a unique identification number (aid ) and a vector representation is generated for each such aid . Given a swid , the neural APE model is trained to generate a corresponding aid based on the context swid appears in. The APE words are generated from aid by lookin"
E17-2056,C96-2141,0,0.645894,"α are alignment sumj i αi,j (attention) matrices of Tx × Ty dimensions. The advantage of symmetrical alignment cells is that they are normalized using softmax (values in between 0 and 1), therefore, the trace term is bounded above by min(Tx , Ty ), representing perfect one-to-one alignments in both directions. To train each directional attention model (mt → pe and pe → mt), we follow the work described in Cohn et al. (2016), where absolute positional bias between the MT and PE translation (as in IBM Model 2), fertility relative position bias (as in IBM Models 3, 4, 5) and HMM-based Alignment (Vogel et al., 1996) are incorporated with an attention based soft alignment model. 3 Experiments and Results We carried out our experiments on the 12K English–German WMT 2016 APE task training 1 351 http://www.statmt.org/moses/ For setting up our neural network, previous to training the AP ESym model, we performed a number of preprocessing steps on the mt–pe parallel training data. First, we prepare a LUT containing mt–pe hybrid prior word alignment above (Section 2.1) a certain lexical translation probability threshold (0.3). To ensure efficient use of the hybrid prior alignment we replaced each mt word by a un"
L16-1095,2013.mtsummit-wptp.13,0,0.0502028,"of the translation workflow. In recent years, commercial computer-aided translation (CAT) tools have started to provide not only the popular translation memory (TM) matches but also MT segments to be post-edited by translators. The use of MT output for post-editing is regarded to increase translator’s productivity and also to improve consistency in translation (Federico et al., 2012; Zampieri and Vela, 2014). In light of this, a recent trend in the field is to develop tools that integrate both MT and TM output providing translators a larger number of more useful and more accurate suggestions (Cettolo et al., 2013). Contributing in this direction, this paper presents a new web-based CAT tool called CATaLog online1 developed based on CATaLog, a recently-released desktop CAT tool (Nayek et al., 2015). The tool can be used to post-edit MT output as well as TM segments. CATaLog online records a wide range of logs that are not available in any commercial CAT tool making it a useful tool for project management and translation process research. We have observed a substantial increase in the number of online CAT tools available, both for commercial and non-commercial purposes. This includes tools such as WordFa"
L16-1095,2012.amta-papers.22,0,0.0455595,"lation process and translator’s productivity. Keywords: post-editing, machine translation, translation memories 1. Introduction With the improvement of machine translation (MT) software, post-editing tools have become an important part of the translation workflow. In recent years, commercial computer-aided translation (CAT) tools have started to provide not only the popular translation memory (TM) matches but also MT segments to be post-edited by translators. The use of MT output for post-editing is regarded to increase translator’s productivity and also to improve consistency in translation (Federico et al., 2012; Zampieri and Vela, 2014). In light of this, a recent trend in the field is to develop tools that integrate both MT and TM output providing translators a larger number of more useful and more accurate suggestions (Cettolo et al., 2013). Contributing in this direction, this paper presents a new web-based CAT tool called CATaLog online1 developed based on CATaLog, a recently-released desktop CAT tool (Nayek et al., 2015). The tool can be used to post-edit MT output as well as TM segments. CATaLog online records a wide range of logs that are not available in any commercial CAT tool making it a u"
L16-1095,C14-2028,0,0.467888,"rection, this paper presents a new web-based CAT tool called CATaLog online1 developed based on CATaLog, a recently-released desktop CAT tool (Nayek et al., 2015). The tool can be used to post-edit MT output as well as TM segments. CATaLog online records a wide range of logs that are not available in any commercial CAT tool making it a useful tool for project management and translation process research. We have observed a substantial increase in the number of online CAT tools available, both for commercial and non-commercial purposes. This includes tools such as WordFast Anywhere2 , MateCat3 (Federico et al., 2014), Wordbee4 , and many others. In our opinion, this is a trend in the translation industry and it motivated us to release CATaLog online. Online CAT tools have a number of advantages over desktop tools, most notably: they do not require local installation; they can be used from any computer; projects can be easily shared with multiple translators; project managers can track the progress of projects on the fly. This paper presents CATaLog online and summarizes the 1 The tool is available online. For more information, consult the following URL: http://ttg.uni-saarland.de/software/catalog 2 https:"
L16-1095,2014.eamt-1.2,0,0.038703,"Missing"
L16-1095,W15-4905,1,0.897711,"Missing"
L16-1095,P10-1064,1,0.894586,"Missing"
L16-1095,2010.jec-1.3,0,0.0197391,"f translated segments contained in the TM; 2) the quality of the TM matching and retrieval engine. To improve the latter, developers have been working on incorporating semantic knowledge to TMs by providing paraphrasing (Utiyama et al., 2011; Gupta and Or˘asan, 2014; Gupta et al., 2015), as well as incorporating syntactic information (Clark, 2002; Gotti et al., 2005; Vanallemeersch and Vandeghinste, 2014). To increase the number of suggestions presented to translators, a recent trend in state-of-the-art CAT tools is the aforementioned integration of TM segments and MT output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-of-the-art MT systems, MT output is no 599 longer considered to be suitable just for gisting purposes and it has been used in real-world translation projects as well. CAT tools such as MateCat present MT output along segments retrieved from TMs in the list of suitable suggestions (Cettolo et al., 2013; Federico et al., 2014). Substantial work has been carried out on improving translation recommendation systems which recommends posteditors either to use TM output or MT output (He et al., 2010). To optimize performance these systems use classifier trained to predi"
L16-1095,2010.jec-1.4,0,0.0200997,"t al., 2010). To optimize performance these systems use classifier trained to predict which output (TM or MT) requires less effort to be used for post-editing. Work on integrating MT with TM has also been done to make TM output more suitable for post-editing aiming to diminishing translators’ effort (Kanavos and Kartsaklis, 2010). Simard and Isabelle (2009) present the integration of Phrase-based Statistical MT (PB-SMT) with translation memories in a computer-aided translation environment in which the PB-SMT system exploits the most similar matches by making use of TM-based feature functions. Koehn and Senellart (2010) present another MT-TM integration strategy. In this study an Statistical MT (SMT) system is used to fill in the gaps in retrieved TM segments. 3. The Tool CATaLog online is a language independent tool that enables users to upload their own translation memories on the platform of the tool. It provides three major functionalities: • It provides a novel and user-friendly online CAT environment to post-editors and translators to reduce postediting time and effort, as displayed in Figure 1. • It collects post-editing logs which are a fundamental source of information for the translation process re"
L16-1095,P07-2045,0,0.00699121,"Missing"
L16-1095,J10-4005,0,0.0159378,"to upload the translations produced by third-party MT systems. A new feature in both CATaLog and CATaLog online is the ranking of matched TM segments based on their similarity given by Translation Error Rate (TER) (Snover et al., 2006). The system finds the matched and unmatched parts between the input segment and the five most similar TM segments from the TER alignment. It also finds out the correspondences between the source and target tokens in the matched TM segments and their corresponding translations using GIZA++ (Och and Ney, 2003) word alignments with grow-diag-final-and heuristics (Koehn, 2010). Matched parts and unmatched parts, both in the source and the target text, are colourcoded for better visualisation and displayed in green and red respectively. CATaLog online provides facilities to translate either single sentences or in batch mode i.e., by uploading a file. As shown in Figure 1, for a given input sentence (English in this case), the current version of CATaLog online provides two alternative translation suggestions in the target language (German in this case): MT and TM. The TM suggestion is colour-coded. When the translator selects the colour-coded TM alternative (c.f., Fi"
L16-1095,2008.amta-srw.4,0,0.0371658,"ion 2 presents related studies focusing on the integration between TM matches and MT output to improve CAT tools; Section 3 describes in detail the main functions of CATaLog online; Section 4 presents the language pairs and data that are currently included in CATaLog online; Section 5 discusses the main functions of CATaLog online and their importance for translators, researchers and project managements; finally, Section 6 concludes this paper and presents avenues for future research. 2. Related Work CAT tools are regarded to increase translator’s productivity and improve translation quality (Lagoudaki, 2008). The core component of most commercial CAT tools are translation memories. TMs work under the assumption that previously translated segments are likely to be good examples for new translations. This is particularly true when translating documents from the same domain which share a similar structure and/or vocabulary. Two important aspects should be considered when working with TMs: 1) the quality and number of translated segments contained in the TM; 2) the quality of the TM matching and retrieval engine. To improve the latter, developers have been working on incorporating semantic knowledge"
L16-1095,W15-5206,1,0.787357,"Missing"
L16-1095,J03-1002,0,0.0059723,"e background MT system (Pal et al., 2015a) integrated in the CAT tool or to upload the translations produced by third-party MT systems. A new feature in both CATaLog and CATaLog online is the ranking of matched TM segments based on their similarity given by Translation Error Rate (TER) (Snover et al., 2006). The system finds the matched and unmatched parts between the input segment and the five most similar TM segments from the TER alignment. It also finds out the correspondences between the source and target tokens in the matched TM segments and their corresponding translations using GIZA++ (Och and Ney, 2003) word alignments with grow-diag-final-and heuristics (Koehn, 2010). Matched parts and unmatched parts, both in the source and the target text, are colourcoded for better visualisation and displayed in green and red respectively. CATaLog online provides facilities to translate either single sentences or in batch mode i.e., by uploading a file. As shown in Figure 1, for a given input sentence (English in this case), the current version of CATaLog online provides two alternative translation suggestions in the target language (German in this case): MT and TM. The TM suggestion is colour-coded. Whe"
L16-1095,W15-3017,1,0.667715,"Missing"
L16-1095,W15-3026,1,0.840277,"Missing"
L16-1095,W15-4916,1,0.889319,"Missing"
L16-1095,2009.mtsummit-papers.14,0,0.0528677,"egments retrieved from TMs in the list of suitable suggestions (Cettolo et al., 2013; Federico et al., 2014). Substantial work has been carried out on improving translation recommendation systems which recommends posteditors either to use TM output or MT output (He et al., 2010). To optimize performance these systems use classifier trained to predict which output (TM or MT) requires less effort to be used for post-editing. Work on integrating MT with TM has also been done to make TM output more suitable for post-editing aiming to diminishing translators’ effort (Kanavos and Kartsaklis, 2010). Simard and Isabelle (2009) present the integration of Phrase-based Statistical MT (PB-SMT) with translation memories in a computer-aided translation environment in which the PB-SMT system exploits the most similar matches by making use of TM-based feature functions. Koehn and Senellart (2010) present another MT-TM integration strategy. In this study an Statistical MT (SMT) system is used to fill in the gaps in retrieved TM segments. 3. The Tool CATaLog online is a language independent tool that enables users to upload their own translation memories on the platform of the tool. It provides three major functionalities: •"
L16-1095,2006.amta-papers.25,0,0.0723154,"y to compare various translation engines taking human evaluation into account. A more detailed description of these functionalities is given in the following sections. 3.1. A Novel CAT Environment In CATaLog online, users can choose between MT output and TM segments. The tool allows the user to choose either the background MT system (Pal et al., 2015a) integrated in the CAT tool or to upload the translations produced by third-party MT systems. A new feature in both CATaLog and CATaLog online is the ranking of matched TM segments based on their similarity given by Translation Error Rate (TER) (Snover et al., 2006). The system finds the matched and unmatched parts between the input segment and the five most similar TM segments from the TER alignment. It also finds out the correspondences between the source and target tokens in the matched TM segments and their corresponding translations using GIZA++ (Och and Ney, 2003) word alignments with grow-diag-final-and heuristics (Koehn, 2010). Matched parts and unmatched parts, both in the source and the target text, are colourcoded for better visualisation and displayed in green and red respectively. CATaLog online provides facilities to translate either single"
L16-1095,W14-3323,1,0.897115,"Missing"
L16-1095,2011.mtsummit-papers.37,0,0.027697,"rcial CAT tools are translation memories. TMs work under the assumption that previously translated segments are likely to be good examples for new translations. This is particularly true when translating documents from the same domain which share a similar structure and/or vocabulary. Two important aspects should be considered when working with TMs: 1) the quality and number of translated segments contained in the TM; 2) the quality of the TM matching and retrieval engine. To improve the latter, developers have been working on incorporating semantic knowledge to TMs by providing paraphrasing (Utiyama et al., 2011; Gupta and Or˘asan, 2014; Gupta et al., 2015), as well as incorporating syntactic information (Clark, 2002; Gotti et al., 2005; Vanallemeersch and Vandeghinste, 2014). To increase the number of suggestions presented to translators, a recent trend in state-of-the-art CAT tools is the aforementioned integration of TM segments and MT output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-of-the-art MT systems, MT output is no 599 longer considered to be suitable just for gisting purposes and it has been used in real-world translation projects as well. CAT tools suc"
L16-1095,2014.tc-1.11,0,0.0129636,"anslations. This is particularly true when translating documents from the same domain which share a similar structure and/or vocabulary. Two important aspects should be considered when working with TMs: 1) the quality and number of translated segments contained in the TM; 2) the quality of the TM matching and retrieval engine. To improve the latter, developers have been working on incorporating semantic knowledge to TMs by providing paraphrasing (Utiyama et al., 2011; Gupta and Or˘asan, 2014; Gupta et al., 2015), as well as incorporating syntactic information (Clark, 2002; Gotti et al., 2005; Vanallemeersch and Vandeghinste, 2014). To increase the number of suggestions presented to translators, a recent trend in state-of-the-art CAT tools is the aforementioned integration of TM segments and MT output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-of-the-art MT systems, MT output is no 599 longer considered to be suitable just for gisting purposes and it has been used in real-world translation projects as well. CAT tools such as MateCat present MT output along segments retrieved from TMs in the list of suitable suggestions (Cettolo et al., 2013; Federico et al., 2014). Substantial work ha"
L16-1095,W14-0314,1,0.846882,"slator’s productivity. Keywords: post-editing, machine translation, translation memories 1. Introduction With the improvement of machine translation (MT) software, post-editing tools have become an important part of the translation workflow. In recent years, commercial computer-aided translation (CAT) tools have started to provide not only the popular translation memory (TM) matches but also MT segments to be post-edited by translators. The use of MT output for post-editing is regarded to increase translator’s productivity and also to improve consistency in translation (Federico et al., 2012; Zampieri and Vela, 2014). In light of this, a recent trend in the field is to develop tools that integrate both MT and TM output providing translators a larger number of more useful and more accurate suggestions (Cettolo et al., 2013). Contributing in this direction, this paper presents a new web-based CAT tool called CATaLog online1 developed based on CATaLog, a recently-released desktop CAT tool (Nayek et al., 2015). The tool can be used to post-edit MT output as well as TM segments. CATaLog online records a wide range of logs that are not available in any commercial CAT tool making it a useful tool for project man"
L16-1095,2015.eamt-1.17,1,\N,Missing
L16-1095,2015.eamt-1.6,1,\N,Missing
N13-3005,W05-0909,0,0.016496,"maintenance of the tool. 1 Automatic Evaluation of Machine Translation beyond Overall Scores Machine translation (MT) output can be evaluated using different approaches, which can essentially be divided into human and automatic, both of which, however, present a number of shortcomings. Human evaluation tends to be more reliable in a number of ways and can be tailored to a variety of situations, but is rather expensive (both in terms of resources and time) and is difficult to replicate. On the other hand, standard automatic MT evaluation metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) are considerably cheaper and provide faster results, but return rather crude scores that are difficult to interpret for MT users and developers alike. Crucially, current standard automatic MT evaluation metrics also lack any diagnostic value, i.e. they cannot identify specific weaknesses in the MT output. Diagnostic information can be extremely valuable for MT develCheckpoint Lexical Syntactic Semantic Relevance for MT Words that can have multiple translations in the target. For example, the preposition “de” in Spanish can be translated into English as “of” or “from” depending on the context."
N13-3005,berka-etal-2012-automatic,0,0.012083,"web service from the user’s perspective. Details regarding its implementation, evaluation, etc. can be found in (Toral et al., 2012; Naskar et al., 2011). 2 ity and input/output parameters (which can be easily included, e.g. as part of an online tutorial). While this paradigm is rather new in the field of computational linguistics, it is quite mature and successful in other fields such as bioinformatics (Oinn et al., 2004; Labarga et al., 2007). Related work includes two web applications in the area of MT evaluation. iBLEU (Madnani, 2011) organises BLEU scoring information in a visual manner. Berka et al. (2012) perform automatic error detection and classification of MT output. Web Services for Language Technology Tools There exist many freely available language processing tools, some of which are distributed under open-source licenses. In order to use these tools, they need to be downloaded, installed, configured and maintained, which results in high cost both in terms of manual effort and computing resources. The requirement for in-depth technical knowledge severely limits the usability of these tools amongst non-technical users, particularly in our case amongst translators and post-editors. Web se"
N13-3005,2011.mtsummit-papers.60,1,0.818593,"oes not seem to have been widely used in the community, in spite of its ability to support diagnostic evaluation. DELiC4MT1 is an open-source software that follows the same approach as Woodpecker. However, DELiC4MT is easily portable to any language pair2 and provides additional functionality such as filtering of noisy checkpoint instances and support for statistical significance tests. This paper focuses on the usage of this tool through a web application and a web service from the user’s perspective. Details regarding its implementation, evaluation, etc. can be found in (Toral et al., 2012; Naskar et al., 2011). 2 ity and input/output parameters (which can be easily included, e.g. as part of an online tutorial). While this paradigm is rather new in the field of computational linguistics, it is quite mature and successful in other fields such as bioinformatics (Oinn et al., 2004; Labarga et al., 2007). Related work includes two web applications in the area of MT evaluation. iBLEU (Madnani, 2011) organises BLEU scoring information in a visual manner. Berka et al. (2012) perform automatic error detection and classification of MT output. Web Services for Language Technology Tools There exist many freely"
N13-3005,J03-1002,0,0.00500722,"rface for the web service. 3 Demo The demo presented in this paper consists of a web service and a web application built on top of DELiC4MT that allow to assess the performance of MT systems on different linguistic phenomena deFigure 2: Screenshot of the web application (visualisation of results). fined by the user. The following subsections detail both parts of the demo. 3.1 Web Service A SOAP-compliant web service3 has been built on top of DELiC4MT. It receives the following input parameters (see Figure 1): 1. Word alignment between the source and target sides of the testset, in the GIZA++ (Och and Ney, 2003) output format. 2. Linguistic checkpoint defined as a Kybot4 (Vossen et al., 2010) profile. 3. Output of the MT system to be evaluated, in plain text, tokenised and one sentence per line. 4. Source and target sides of the testset (or gold standard), in KAF format (Bosma et al., 2009).5 The tool then evaluates the performance of the MT system (input parameter 3) on the linguistic phenomenon (parameter 2) by following this procedure: 3 http://registry.elda.org/services/301 Kybot profiles can be understood as regular expressions over KAF documents, http://kyoto.let.vu.nl/svn/ kyoto/trunk/modules/"
N13-3005,P02-1040,0,0.0897883,"any installation, configuration or maintenance of the tool. 1 Automatic Evaluation of Machine Translation beyond Overall Scores Machine translation (MT) output can be evaluated using different approaches, which can essentially be divided into human and automatic, both of which, however, present a number of shortcomings. Human evaluation tends to be more reliable in a number of ways and can be tailored to a variety of situations, but is rather expensive (both in terms of resources and time) and is difficult to replicate. On the other hand, standard automatic MT evaluation metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) are considerably cheaper and provide faster results, but return rather crude scores that are difficult to interpret for MT users and developers alike. Crucially, current standard automatic MT evaluation metrics also lack any diagnostic value, i.e. they cannot identify specific weaknesses in the MT output. Diagnostic information can be extremely valuable for MT develCheckpoint Lexical Syntactic Semantic Relevance for MT Words that can have multiple translations in the target. For example, the preposition “de” in Spanish can be translated into English as “o"
N13-3005,W10-3301,0,0.0137191,"web service and a web application built on top of DELiC4MT that allow to assess the performance of MT systems on different linguistic phenomena deFigure 2: Screenshot of the web application (visualisation of results). fined by the user. The following subsections detail both parts of the demo. 3.1 Web Service A SOAP-compliant web service3 has been built on top of DELiC4MT. It receives the following input parameters (see Figure 1): 1. Word alignment between the source and target sides of the testset, in the GIZA++ (Och and Ney, 2003) output format. 2. Linguistic checkpoint defined as a Kybot4 (Vossen et al., 2010) profile. 3. Output of the MT system to be evaluated, in plain text, tokenised and one sentence per line. 4. Source and target sides of the testset (or gold standard), in KAF format (Bosma et al., 2009).5 The tool then evaluates the performance of the MT system (input parameter 3) on the linguistic phenomenon (parameter 2) by following this procedure: 3 http://registry.elda.org/services/301 Kybot profiles can be understood as regular expressions over KAF documents, http://kyoto.let.vu.nl/svn/ kyoto/trunk/modules/mining_module/ 5 An XML format for text analysis based on representation standards"
N13-3005,C08-1141,0,0.016194,"anslations in the target language. Polysemous words can be collected from electronic dictionaries such as WordNet (Miller, 1995). Table 1: Linguistic Checkpoints Checkpoints can also be built by combining el20 Proceedings of the NAACL HLT 2013 Demonstration Session, pages 20–23, c Atlanta, Georgia, 10-12 June 2013. 2013 Association for Computational Linguistics ements from different categories. For example, by combining lexical and syntantic elements, we could define a checkpoint for prepositional phrases (syntactic element) which start with the preposition “de” (lexical element). Woodpecker (Zhou et al., 2008) is a tool that performs diagnostic evaluation of MT systems over linguistic checkpoints for English–Chinese. Probably due to its limitation to one language pair, its proprietary nature as well as rather restrictive licensing conditions, Woodpecker does not seem to have been widely used in the community, in spite of its ability to support diagnostic evaluation. DELiC4MT1 is an open-source software that follows the same approach as Woodpecker. However, DELiC4MT is easily portable to any language pair2 and provides additional functionality such as filtering of noisy checkpoint instances and supp"
P06-2025,C00-1056,0,0.32554,"Missing"
P06-2025,W98-1005,0,0.218658,"large name collections like census data, electoral roll and railway reservation information must be available to multilingual citizens of the country in their vernacular. In the present work, the various proposed models have been evaluated on a training corpus of person names. A hybrid neural network and knowledge-based system to generate multiple English spellings for Arabic personal names is described in (Arbabi et al., 1994). (Knight and Graehl, 1998) developed a phoneme-based statistical model using finite state transducer that implements transformation rules to do back-transliteration. (Stalls and Knight, 1998) adapted this approach for back transliteration from Arabic to English for English names. A spelling-based model is described in (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c) that directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations. The phonetics-based and spelling-based models have been linearly combined into a single transliteration model in (Al-Onaizan and Knight, 2002b) for transliteration of Arabic named entities into English. Several"
P06-2025,W03-1508,0,0.320963,"Missing"
P06-2025,P02-1051,0,0.0843851,"Missing"
P06-2025,2005.mtsummit-papers.37,0,0.0691749,"Missing"
P06-2025,2005.mtsummit-papers.36,0,\N,Missing
P06-2025,P04-1021,0,\N,Missing
P06-2025,W02-0505,0,\N,Missing
P06-2025,W05-0700,0,\N,Missing
P16-2046,W11-2107,0,0.0154085,"nt between the translators by computing Cohen’s κ coefficient (Cohen, 1960) reported in Table 2. The overall κ coefficient is 0.330. According to (Landis and Koch, 1977) this correlation coefficient can be interpreted as fair. Evaluation The performance of the NNAPE system was evaluated using both automatic and human evaluation methods, as described below. 5.1 BLEU 61.26 62.54a 63.87a,b 65.22a,b,c Automatic Evaluation The output of the NNAPE system on the 1000 sentences testset was evaluated using three MT evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and Meteor (Denkowski and Lavie, 2011). Table 1 provides a comparison of our neural system performance against the baseline phrase-based APE (S1 ), baseline hierarchical phrase-based APE (S2 ) and the original GT output. We use a, b, c, and d to indicate statistical significance over GT, S1 , S2 and our NNAPE system (NN), respectively. For example, the S2 BLEU score 63.87a,b in Table 1 means that the improvement provided by S2 in BLEU is statistically significant over Google Translator and phrase-based 284 Cohen’s κ T1 T2 T3 T4 T1 0.141 0.424 0.398 T2 0.141 0.232 0.540 T3 0.424 0.232 0.248 T4 0.398 0.540 0.248 - Union’s Framework"
P16-2046,W15-3026,1,0.893196,"Missing"
P16-2046,W11-2123,0,0.0755541,"Missing"
P16-2046,P02-1040,0,0.109086,"ncertain’ option. We measured pairwise inter-annotator agreement between the translators by computing Cohen’s κ coefficient (Cohen, 1960) reported in Table 2. The overall κ coefficient is 0.330. According to (Landis and Koch, 1977) this correlation coefficient can be interpreted as fair. Evaluation The performance of the NNAPE system was evaluated using both automatic and human evaluation methods, as described below. 5.1 BLEU 61.26 62.54a 63.87a,b 65.22a,b,c Automatic Evaluation The output of the NNAPE system on the 1000 sentences testset was evaluated using three MT evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and Meteor (Denkowski and Lavie, 2011). Table 1 provides a comparison of our neural system performance against the baseline phrase-based APE (S1 ), baseline hierarchical phrase-based APE (S2 ) and the original GT output. We use a, b, c, and d to indicate statistical significance over GT, S1 , S2 and our NNAPE system (NN), respectively. For example, the S2 BLEU score 63.87a,b in Table 1 means that the improvement provided by S2 in BLEU is statistically significant over Google Translator and phrase-based 284 Cohen’s κ T1 T2 T3 T4 T1 0.141 0.424 0.398 T2 0.141 0.232 0."
P16-2046,W12-3146,0,0.306803,"Missing"
P16-2046,D13-1176,0,0.320157,"be modelled as an MT system between SLIP T LM T and T LP E . However, if we do not have access to SLIP , but have sufficiently large amounts of parallel T LM T T LP E data, we can still build an APE model between T LM T and T LP E . Translations provided by state-of-the-art MT systems suffer from a number of errors including incorrect lexical choice, word ordering, word insertion, word deletion, etc. The APE work presented in this paper is an effort to improve the MT output by rectifying some of these errors. For this purpose we use a deep neural network (DNN) based approach. Neural MT (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Cho et al., 2014b) is a newly emerging approach to MT. On the one hand DNNs represent language in a continuous vector space which eases the modelling of semantic similarities (or distance) between phrases or sentences, and on the other hand it can also consider contextual information, e.g., We present a neural network based automatic post-editing (APE) system to improve raw machine translation (MT) output. Our neural model of APE (NNAPE) is based on a bidirectional recurrent neural network (RNN) model and consists of an encoder that encodes an MT output into a fixed-length"
P16-2046,N07-1064,0,0.752153,"On the other hand, given sufficient amounts of training data, LSTMs may lead to better results. Since our task is monolingual and we have more than 200K sentence pairs for training, we use a full LSTM (as the hidden units) to model our NNAPE system. The model takes T LM T as input and provides T LP E as output. To the best of our knowledge the work presented in this paper is the first approach to APE using neural networks. utilizing all available history information in deciding the next target word, which is not an easy task to model with standard APE systems. Unlike phrase-based APE systems (Simard et al., 2007a; Simard et al., 2007b; Pal, 2015; Pal et al., 2015), our NNAPE system builds and trains a single, large neural network that accepts a ‘draft’ translation (T LM T ) and outputs an improved translation (T LP E ). The remainder of the paper is organized as follows. Section 2 gives an overview of relevant related work. The proposed NNAPE system is described in detail in Section 3. We present the experimental setup in Section 4. Section 5 presents the results of automatic and human evaluation together with some analysis. Section 6 concludes the paper and provides avenues for future work. 2 Relate"
P16-2046,W07-0728,0,0.384103,"On the other hand, given sufficient amounts of training data, LSTMs may lead to better results. Since our task is monolingual and we have more than 200K sentence pairs for training, we use a full LSTM (as the hidden units) to model our NNAPE system. The model takes T LM T as input and provides T LP E as output. To the best of our knowledge the work presented in this paper is the first approach to APE using neural networks. utilizing all available history information in deciding the next target word, which is not an easy task to model with standard APE systems. Unlike phrase-based APE systems (Simard et al., 2007a; Simard et al., 2007b; Pal, 2015; Pal et al., 2015), our NNAPE system builds and trains a single, large neural network that accepts a ‘draft’ translation (T LM T ) and outputs an improved translation (T LP E ). The remainder of the paper is organized as follows. Section 2 gives an overview of relevant related work. The proposed NNAPE system is described in detail in Section 3. We present the experimental setup in Section 4. Section 5 presents the results of automatic and human evaluation together with some analysis. Section 6 concludes the paper and provides avenues for future work. 2 Relate"
P16-2046,2006.amta-papers.25,0,0.127359,"pairwise inter-annotator agreement between the translators by computing Cohen’s κ coefficient (Cohen, 1960) reported in Table 2. The overall κ coefficient is 0.330. According to (Landis and Koch, 1977) this correlation coefficient can be interpreted as fair. Evaluation The performance of the NNAPE system was evaluated using both automatic and human evaluation methods, as described below. 5.1 BLEU 61.26 62.54a 63.87a,b 65.22a,b,c Automatic Evaluation The output of the NNAPE system on the 1000 sentences testset was evaluated using three MT evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and Meteor (Denkowski and Lavie, 2011). Table 1 provides a comparison of our neural system performance against the baseline phrase-based APE (S1 ), baseline hierarchical phrase-based APE (S2 ) and the original GT output. We use a, b, c, and d to indicate statistical significance over GT, S1 , S2 and our NNAPE system (NN), respectively. For example, the S2 BLEU score 63.87a,b in Table 1 means that the improvement provided by S2 in BLEU is statistically significant over Google Translator and phrase-based 284 Cohen’s κ T1 T2 T3 T4 T1 0.141 0.424 0.398 T2 0.141 0.232 0.540 T3 0.424 0.232 0.248 T4"
P16-2046,N09-2055,0,0.646587,"Missing"
P16-2046,W14-0314,1,0.854431,"ms. Lagarda et al. (2009) used statistical information from the trained SMT models for post-editing of rule-based MT output. Rosa et al. (2012) and Mareˇcek et al. (2011) applied a rule-based approach to APE on the morphological level. Denkowski (2015) developed a method for real time integration of post-edited MT output into the translation model by extracting a grammar for each input sentence. Recent studies have even shown that the quality of MT plus PE can exceed the quality of human translation (Fiederer and OBrien, 2009; Koehn, 2009; DePalma and Kelly, 2009) as well as the productivity (Zampieri and Vela, 2014) in some cases. Recently, a number of papers have presented the application of neural networks in MT (Kalchbrenner and Blunsom, 2013; ?; Cho et al., 2014b; Bahdanau et al., 2014). These approaches typically consist of two components: an encoder encodes a source sentence and a decoder decodes into a target sentence. In this paper we present a neural network based approach to automatic PE (NNAPE). Our NNAPE model is inspired by the MT work of Bahdanau et al. (2014) which is based on bidirectional recurrent neural networks (RNN). Unlike Bah3 Neural Network based APE The NNAPE system is based on a"
P16-2046,P03-1021,0,0.320083,"Missing"
P16-2046,P07-2045,0,\N,Missing
pal-etal-2014-word,popovic-ney-2006-pos,0,\N,Missing
pal-etal-2014-word,W11-2127,0,\N,Missing
pal-etal-2014-word,holmqvist-etal-2012-alignment,0,\N,Missing
pal-etal-2014-word,J93-2003,0,\N,Missing
pal-etal-2014-word,D08-1089,0,\N,Missing
pal-etal-2014-word,P02-1040,0,\N,Missing
pal-etal-2014-word,W09-0435,0,\N,Missing
pal-etal-2014-word,N09-1028,0,\N,Missing
pal-etal-2014-word,W10-1735,0,\N,Missing
pal-etal-2014-word,W05-0909,0,\N,Missing
pal-etal-2014-word,P07-2045,0,\N,Missing
pal-etal-2014-word,P10-2033,0,\N,Missing
pal-etal-2014-word,W12-4207,0,\N,Missing
pal-etal-2014-word,C10-1043,0,\N,Missing
pal-etal-2014-word,E09-1011,0,\N,Missing
pal-etal-2014-word,N03-1017,0,\N,Missing
pal-etal-2014-word,J03-1002,0,\N,Missing
pal-etal-2014-word,P05-1066,0,\N,Missing
pal-etal-2014-word,N10-3010,0,\N,Missing
pal-etal-2014-word,2007.mtsummit-papers.28,0,\N,Missing
pal-etal-2014-word,2005.iwslt-1.8,0,\N,Missing
pal-etal-2014-word,P03-1021,0,\N,Missing
S07-1043,vasilescu-etal-2004-evaluating,0,0.0322373,"ances). The proposed WSD algorithm is POS-sense-tagged gloss (from Related Works Banerjee and Pedersen (2002) reports an adaptation of Lesk’s dictionary-based WSD algorithm which makes use of WordNet glosses and tests on English lexical sample from SENSEVAL-2. They define overlap as the longest sequence of one or more consecutive content words that occurs in both glosses. Each overlap contributes a score equal to the square of the number of words in the overlap. A version of Lesk algorithm in combination with WordNet has been reported for achieving good results in (Ramakrishnan et al., 2004). Vasilescu et al. (2004) carried on a series of experiments on the Lesk algorithm, adapted to WordNet, and on some variants. They studied the effect of varying the number of words in the contexts, centered around the target word. But till now no work has been reported which makes use of Extended WordNet for Lesk-like gloss-oriented approach. 203 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 203–206, c Prague, June 2007. 2007 Association for Computational Linguistics 4 Proposed Sense Disambiguation Algorithm The proposed sense disambiguation algorithm is a major modificati"
S07-1043,W99-0501,0,0.0207961,"algorithm relies on the POS-sense tagged synset glosses provided by the Extended WordNet. The basic unit of disambiguation of our algorithm is the entire sentence under consideration. It takes a global approach where all the words in the target sentence are simultaneously disambiguated. The context includes previous and next sentence. The system assigns the default WordNet first sense to a word when the algorithm fails to predict the sense of the word. The system produces a precision and recall of .402 on the SemEval-2007 English All-Words test data. 1 2 Extended WordNet The eXtended WordNet (Harabagiu et al., 1999) project aims to transform the WordNet glosses into a format that allows the derivation of additional semantic and logic relations. It intends to syntactically parse the glosses, transform glosses into logical forms and tag semantically the nouns, verbs, adjectives and adverbs of the glosses automatically. The last release of the Extended WordNet is based on WordNet 2.0 and has three stages: POS tagging and parsing, logic form transformation, and semantic disambiguation. 3 Introduction In Senseval 1, most of the systems disambiguating English words, were outperformed by a Lesk variant serving"
S19-2118,W18-4401,0,0.0913989,"Missing"
S19-2118,W17-1101,0,0.0209865,"ep learning approaches for the sub-tasks. We employed Convolutional Neural Network (CNN) and Recursive Neural Network (RNN) Long Short-Term Memory (LSTM) with pre-trained word embeddings. We used both word2vec and Glove pre-trained word embeddings. We obtained the best F1score using CNN based model for sub-task A, LSTM based model for sub-task B and Logistic Regression based model for sub-task C. Our best submissions achieved 0.7844, 0.5459 and 0.48 F1-scores for sub-task A, sub-task B and sub-task C respectively. 1 2 Related Work Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018), the paper by (Davidson et al., 2017) presenting the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017) and a few other recent papers such as (ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018). Introduction Today, very large amounts of information are available in online documents. As part of the effort to better organize this information for users, researchers have been actively investigating the problem of automatic text categorization. Tweets are short length pieces of text, usually written in informal style that contain ab"
S19-2118,W17-3003,0,0.019065,"al., 2018). Introduction Today, very large amounts of information are available in online documents. As part of the effort to better organize this information for users, researchers have been actively investigating the problem of automatic text categorization. Tweets are short length pieces of text, usually written in informal style that contain abbreviations, misspellings and creative syntax (like emoticons, hashtags etc). In this paper we show that our A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017). For studies on languages other than English see (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Finally, for recent discussion on identifying profanity vs. hate speech see (Malmasi and Zampieri, 2018). This work high∗ These two authors have contributed equally 662 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 662–667 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics lighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language. 3. Other (OTH): The target of the offensive post does"
S19-2118,W17-3012,0,0.0488353,"s such as (ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018). Introduction Today, very large amounts of information are available in online documents. As part of the effort to better organize this information for users, researchers have been actively investigating the problem of automatic text categorization. Tweets are short length pieces of text, usually written in informal style that contain abbreviations, misspellings and creative syntax (like emoticons, hashtags etc). In this paper we show that our A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017). For studies on languages other than English see (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Finally, for recent discussion on identifying profanity vs. hate speech see (Malmasi and Zampieri, 2018). This work high∗ These two authors have contributed equally 662 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 662–667 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics lighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profa"
S19-2118,N19-1144,0,0.109443,"message polarity classification. We used different machine learning algorithm and neural network approaches for all the tasks which are explained in the subsequent sections. The paper is organized as follows: Section 2 lists down the related work and Section 3 describes our approach. Section 4 presents the experiments, results on the development set and discussion about the confusion matrix and Section 5 details about the observation. Section 6 concludes the paper with possible future work. OffensEval@SemEval-2019 shared task description, data and results are described in the overview paper (Zampieri et al., 2019b). This paper describes our system submissions as part of our participation (team name: JU ETCE 17 21) in the SemEval 2019 shared task 6: “OffensEval: Identifying and Categorizing Offensive Language in Social Media”. We participated in all the three sub-tasks: i) Sub-task A: offensive language identification, ii) Sub-task B: automatic categorization of offense types, and iii) Sub-task C: offense target identification. We employed machine learning as well as deep learning approaches for the sub-tasks. We employed Convolutional Neural Network (CNN) and Recursive Neural Network (RNN) Long Short-"
S19-2118,S19-2010,0,0.0791512,"message polarity classification. We used different machine learning algorithm and neural network approaches for all the tasks which are explained in the subsequent sections. The paper is organized as follows: Section 2 lists down the related work and Section 3 describes our approach. Section 4 presents the experiments, results on the development set and discussion about the confusion matrix and Section 5 details about the observation. Section 6 concludes the paper with possible future work. OffensEval@SemEval-2019 shared task description, data and results are described in the overview paper (Zampieri et al., 2019b). This paper describes our system submissions as part of our participation (team name: JU ETCE 17 21) in the SemEval 2019 shared task 6: “OffensEval: Identifying and Categorizing Offensive Language in Social Media”. We participated in all the three sub-tasks: i) Sub-task A: offensive language identification, ii) Sub-task B: automatic categorization of offense types, and iii) Sub-task C: offense target identification. We employed machine learning as well as deep learning approaches for the sub-tasks. We employed Convolutional Neural Network (CNN) and Recursive Neural Network (RNN) Long Short-"
S19-2118,W17-3007,0,0.0316106,"Missing"
W06-2113,W04-2608,0,0.709386,"Missing"
W06-2113,2005.mtsummit-posters.8,1,0.838936,"Missing"
W06-2113,P98-2201,0,0.48185,"Missing"
W06-2113,C98-2196,0,\N,Missing
W09-3523,kang-choi-2000-automatic,0,0.0293469,"ss problems.We carried out experiments both at character and transliteration unit (TU) level. Position-dependent source context features produce significant improvements in terms of all evaluation metrics. 1 Introduction Machine Transliteration is of key importance in many cross-lingual natural language processing applications, such as information retrieval, question answering and machine translation (MT). There are numerous ways of performing automatic transliteration, such as noisy channel models (Knight and Graehl, 1998), joint source channel models (Li et al., 2004), decision-tree models (Kang and Choi, 2000) and statistical MT models (Matthews, 2007). For the shared task, we built our machine transliteration system based on phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) using Moses (Koehn et al., 2007). We adapt PB-SMT models for transliteration by translating characters rather than words as in character-level translation systems (Lepage & Denoual, 2006). However, we go a step further from the basic PBSMT model by using source-language context features (Stroppa et al., 2007). We also create translation models by constraining the characterlevel segmentations, i.e. treating a consonantvo"
W09-3523,P02-1038,0,0.046724,"sults obtained, together with some analysis. Section 5 concludes the paper. 2 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation e1I = e1 . . . eI of a source sentence f1 J = f1 . . . fJ is chosen to maximize (1): arg max P(e1I |f1J )  arg max P( f1J |e1I ).P(e1I ) (1) I , e1I I , e1I where P ( f1J |e1I ) and P (e1I ) denote respectively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( e1I |f1J ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): m log P ( e1I |f 1 J )   m h m ( f 1 J , e1I , s1K ) m 1   LM log P ( e1I ) (2) K 1 where s  s1 ...s k denotes a segmentation of the source and target sentences respectively into the sequences of phrases (eˆ1 ,..., eˆk ) and ( fˆ1 ,..., fˆk ) such that (we set i0 = 0) (3):  1  k  K , sk = (ik ; bk, jk), eˆk  ei k 1 1...ei k , fˆk  fbk ... f j k (3) The translational features involved depend only on a pair of source/target phrases and do not take into account any context of these phrases. This mea"
W09-3523,W06-1607,0,0.0128369,"ation is normalised to estimate P( eˆk |fˆk ,CI( fˆk )). Therefore our expected feature is derived as in (8): hˆm ( fˆk ,CI( fˆk ), eˆk , sk) = log P( eˆk |fˆk , CI( fˆk )) (7) 3.1 Memory-Based Classification As (Stroppa et al., 2007) point out, directly estimating P( eˆk |fˆk , CI( fˆk )) using relative frequencies is problematic. Indeed, Zens and Ney (2004) showed that the estimation of P( eˆk |fˆk ) using relative frequencies results in the overestimation of the probabilities of long phrases, so smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of contextinformed features, since the context is also taken Implementation Issues hˆmbl = log P( eˆk |fˆk ,CI( fˆk )) (8) As for the standard phrase-based approach, their weights are optimized using Minimum Error Rate Training (MERT) of (Och, 2003) for each of the experiments. As (Stroppa et al., 2007) point out, PB-SMT decoders such as Pharaoh (Koehn, 2004) or Moses (Koehn, 2007) rely on a static phrasetable represented as a list of aligned phrases accompanied with several features. Since these fea1 An implementation of IGTree, IB1 and TRIBL is available in the TiMBL software p"
W09-3523,P04-1021,0,0.138344,"se features while avoiding data sparseness problems.We carried out experiments both at character and transliteration unit (TU) level. Position-dependent source context features produce significant improvements in terms of all evaluation metrics. 1 Introduction Machine Transliteration is of key importance in many cross-lingual natural language processing applications, such as information retrieval, question answering and machine translation (MT). There are numerous ways of performing automatic transliteration, such as noisy channel models (Knight and Graehl, 1998), joint source channel models (Li et al., 2004), decision-tree models (Kang and Choi, 2000) and statistical MT models (Matthews, 2007). For the shared task, we built our machine transliteration system based on phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) using Moses (Koehn et al., 2007). We adapt PB-SMT models for transliteration by translating characters rather than words as in character-level translation systems (Lepage & Denoual, 2006). However, we go a step further from the basic PBSMT model by using source-language context features (Stroppa et al., 2007). We also create translation models by constraining the characterleve"
W09-3523,P07-2045,0,0.00641492,"troduction Machine Transliteration is of key importance in many cross-lingual natural language processing applications, such as information retrieval, question answering and machine translation (MT). There are numerous ways of performing automatic transliteration, such as noisy channel models (Knight and Graehl, 1998), joint source channel models (Li et al., 2004), decision-tree models (Kang and Choi, 2000) and statistical MT models (Matthews, 2007). For the shared task, we built our machine transliteration system based on phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) using Moses (Koehn et al., 2007). We adapt PB-SMT models for transliteration by translating characters rather than words as in character-level translation systems (Lepage & Denoual, 2006). However, we go a step further from the basic PBSMT model by using source-language context features (Stroppa et al., 2007). We also create translation models by constraining the characterlevel segmentations, i.e. treating a consonantvowel cluster as one transliteration unit. The remainder of the paper is organized as follows. In section 2 we give a brief overview of PB-SMT. Section 3 describes how contextinformed features are incorporated i"
W09-3523,2009.eamt-1.32,1,0.844095,"., 2005). When predicting a target phrase given a source phrase and its context, the source phrase is intuitively the feature with the highest prediction power; in all our experiments, it is the feature with the highest gain ratio (GR). In order to build the set of examples required to train the classifier, we modify the standard phrase-extraction method of (Koehn et al., 2003) to extract the context of the source phrases at the same time as the phrases themselves. Importantly, therefore, the context extraction comes at no extra cost. We refer interested readers to (Stroppa et al., 2007) and (Haque et al., 2009) as well as the references therein for more details of how MemoryBased Learning (MBL) is used for classification of source examples for use in the log-linear MT framework. K hm ( f1J , e1I , s1K )   hˆm ( fˆk , eˆk , sk ) (4) k 1 where hˆm is a feature that applies to a single phrase-pair. Thus (2) can be rewritten as: m K K m 1 k 1 k 1  m  hˆm ( fˆk , eˆk , sk )   hˆ( fˆk , eˆk , sk ) where, hˆ  m  (5) hˆ . In this context, the translam m m 1 tion process amounts to: (i) choosing a segmentation of the source sentence, (ii) translating each source phrase. 3 Source Context Featur"
W09-3523,2007.tmi-papers.28,1,0.868563,"Missing"
W09-3523,N04-1033,0,0.0196323,"xperiments on both character-level (C-L) and TUlevel (TU-L) data. We use a 5-gram language model for all our experiments. The Moses PBSMT system serves as our baseline system. The distribution of target phrases given a source phrase and its contextual information is normalised to estimate P( eˆk |fˆk ,CI( fˆk )). Therefore our expected feature is derived as in (8): hˆm ( fˆk ,CI( fˆk ), eˆk , sk) = log P( eˆk |fˆk , CI( fˆk )) (7) 3.1 Memory-Based Classification As (Stroppa et al., 2007) point out, directly estimating P( eˆk |fˆk , CI( fˆk )) using relative frequencies is problematic. Indeed, Zens and Ney (2004) showed that the estimation of P( eˆk |fˆk ) using relative frequencies results in the overestimation of the probabilities of long phrases, so smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of contextinformed features, since the context is also taken Implementation Issues hˆmbl = log P( eˆk |fˆk ,CI( fˆk )) (8) As for the standard phrase-based approach, their weights are optimized using Minimum Error Rate Training (MERT) of (Och, 2003) for each of the experiments. As (Stroppa et al., 2007) point out, PB-SMT deco"
W09-3523,J93-2003,0,0.0102579,"e a brief overview of PB-SMT. Section 3 describes how contextinformed features are incorporated into state-ofart log-linear PB-SMT. Section 4 includes the results obtained, together with some analysis. Section 5 concludes the paper. 2 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation e1I = e1 . . . eI of a source sentence f1 J = f1 . . . fJ is chosen to maximize (1): arg max P(e1I |f1J )  arg max P( f1J |e1I ).P(e1I ) (1) I , e1I I , e1I where P ( f1J |e1I ) and P (e1I ) denote respectively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( e1I |f1J ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): m log P ( e1I |f 1 J )   m h m ( f 1 J , e1I , s1K ) m 1   LM log P ( e1I ) (2) K 1 where s  s1 ...s k denotes a segmentation of the source and target sentences respectively into the sequences of phrases (eˆ1 ,..., eˆk ) and ( fˆ1 ,..., fˆk ) such that (we set i0 = 0) (3):  1  k  K , sk = (ik ; bk, jk), eˆk  ei k 1 1...ei k , fˆk  fbk ... f j"
W09-3523,N03-1017,0,0.0324828,"s of all evaluation metrics. 1 Introduction Machine Transliteration is of key importance in many cross-lingual natural language processing applications, such as information retrieval, question answering and machine translation (MT). There are numerous ways of performing automatic transliteration, such as noisy channel models (Knight and Graehl, 1998), joint source channel models (Li et al., 2004), decision-tree models (Kang and Choi, 2000) and statistical MT models (Matthews, 2007). For the shared task, we built our machine transliteration system based on phrase-based statistical MT (PB-SMT) (Koehn et al., 2003) using Moses (Koehn et al., 2007). We adapt PB-SMT models for transliteration by translating characters rather than words as in character-level translation systems (Lepage & Denoual, 2006). However, we go a step further from the basic PBSMT model by using source-language context features (Stroppa et al., 2007). We also create translation models by constraining the characterlevel segmentations, i.e. treating a consonantvowel cluster as one transliteration unit. The remainder of the paper is organized as follows. In section 2 we give a brief overview of PB-SMT. Section 3 describes how contextinf"
W09-3523,koen-2004-pharaoh,0,0.0229922,"ation of P( eˆk |fˆk ) using relative frequencies results in the overestimation of the probabilities of long phrases, so smoothing factors in the form of lexical-based features are often used to counteract this bias (Foster et al., 2006). In the case of contextinformed features, since the context is also taken Implementation Issues hˆmbl = log P( eˆk |fˆk ,CI( fˆk )) (8) As for the standard phrase-based approach, their weights are optimized using Minimum Error Rate Training (MERT) of (Och, 2003) for each of the experiments. As (Stroppa et al., 2007) point out, PB-SMT decoders such as Pharaoh (Koehn, 2004) or Moses (Koehn, 2007) rely on a static phrasetable represented as a list of aligned phrases accompanied with several features. Since these fea1 An implementation of IGTree, IB1 and TRIBL is available in the TiMBL software package (http://ilk.uvt.nl/timbl). 105 tures do not express the context in which those phrases occur, no context information is kept in the phrase-table, and there is no way to recover this information from the phrase-table. In order to take into account the contextinformed features for use with such decoders, the devset and testset that need to be translated are pre-proces"
W09-3523,J98-4003,0,\N,Missing
W10-1720,P96-1041,0,0.070396,"M Target tokens 45M 2.7M 190M 1.6M 69M Table 1: Statistics of en–cs and en–es parallel data. Monolingual data: For language modeling purposes, in addition to the target parts of the bilingual data, we used the monolingual News corpus for cs; and the Gigaword corpus for es. For both languages, we used the SRILM toolkit (Stolcke, 2002) to train a 5-gram language model using all monolingual data provided. However, for en–es we used the IRSTLM toolkit (Federico and Cettolo, 2007) to train a 5-gram language model using the es Gigaword corpus. Both language models use modified Kneser-Ney smoothing (Chen and Goodman, 1996). Statistics for the monolingual corpora are given in Table 2. Corpus E/N/NC/UN Gigaword News Language es es cs Sentences 9,6M 40M 13M Tokens 290M 1,2G 210M Table 2: Statistics of Monolingual Data. E/N/NC/UN refers to Europarl/News/News Commentary/United Nations corpora. For all the systems except Apertium, we first lowercase and tokenize all the monolingual and bilingual data using the tools provided by the WMT10 organizers. After translation, system 145combination output is detokenised and true-cased. 3.2 English–Czech (en–cs) Experiments ˇ The CzEng corpus (Bojar and Zabokrtsk´ y, 2009) is"
W10-1720,J07-2003,0,0.0791111,"ngually chunking both source and target the N -best list generated by the combination modsides of the dataset using a marker-based chunker ule. Figure 1 illustrates the architecture. (Gough and Way, 2004); 3) Factored translation model (Koehn and Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based (HPB) lexemes (Gough and Way, 2004) to define a way approach of Chiang (2007)) and 6) Apertium (Forto segment sentences into chunks, which are then cada et al., 2009) rule-based machine translation aligned using an edit-distance-style algorithm, in (RBMT). Finally, we use a word-level combination framework (Rosti et al., 2007) to combine the 143which edit costs depend on word-to-word translaProceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 143–148, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics the above three translation factors: an SF to SF decoding path and a path which maps lemma to l"
W10-1720,W07-0712,0,0.00757178,"Corpus Langs. Sent. Europarl News-comm UN News-Comm CzEng en–es en–es en–es en–cs en–cs 1.6M 97k 5.9M 85k 7.8M Source tokens 43M 2.4M 160M 1.8M 80M Target tokens 45M 2.7M 190M 1.6M 69M Table 1: Statistics of en–cs and en–es parallel data. Monolingual data: For language modeling purposes, in addition to the target parts of the bilingual data, we used the monolingual News corpus for cs; and the Gigaword corpus for es. For both languages, we used the SRILM toolkit (Stolcke, 2002) to train a 5-gram language model using all monolingual data provided. However, for en–es we used the IRSTLM toolkit (Federico and Cettolo, 2007) to train a 5-gram language model using the es Gigaword corpus. Both language models use modified Kneser-Ney smoothing (Chen and Goodman, 1996). Statistics for the monolingual corpora are given in Table 2. Corpus E/N/NC/UN Gigaword News Language es es cs Sentences 9,6M 40M 13M Tokens 290M 1,2G 210M Table 2: Statistics of Monolingual Data. E/N/NC/UN refers to Europarl/News/News Commentary/United Nations corpora. For all the systems except Apertium, we first lowercase and tokenize all the monolingual and bilingual data using the tools provided by the WMT10 organizers. After translation, system 1"
W10-1720,2009.freeopmt-1.3,1,0.877177,"Missing"
W10-1720,2004.tmi-1.11,1,0.0884387,"English–Spanish (en– phrase-based and tree-based MT. es) and English–Czech (en–cs) translation The combination structure uses the MBR and tasks. For these two tasks, we employ several CN decoders, and is based on a word-level comindividual MT systems: 1) Baseline: phrasebination strategy (Du et al., 2009). In the final based SMT (Koehn et al., 2007); 2) EBMT: stage, we use a new rescoring module to process Monolingually chunking both source and target the N -best list generated by the combination modsides of the dataset using a marker-based chunker ule. Figure 1 illustrates the architecture. (Gough and Way, 2004); 3) Factored translation model (Koehn and Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based (HPB) lexemes (Gough and Way, 2004) to define a way approach of Chiang (2007)) and 6) Apertium (Forto segment sentences into chunks, which are then cada et al., 2009) rule-based machine translation aligned using an edit-distance-style algorithm, in (RBMT). Finally, we"
W10-1720,Y09-1019,1,0.818274,"oding paths based on 1 http://www.apertium.org (1) We use a memory-based machine learning (MBL) classifier (TRIBL:2 Daelemans and van den Bosch (2005)) that is able to estimate P (ˆ ek |fˆk , CI(fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations. In equation (1), SSCI may include any feature (lexical, syntactic, etc.), which can provide useful information to disambiguate a given source phrase. In addition to using local words and PoS-tags as features, as in (Stroppa et al., 2007), we incorporate grammatical dependency relations (Haque et al., 2009a) and supertags (Haque et al., 2009b) as syntactic source context features in the log-linear PB-SMT model. In addition to the above feature, we derived a ˆ best , defined in (2): simple binary feature h ( 1 if eˆk maximizes P (ˆ ek |fˆk , CI(fˆk )) 0 otherwise (2) We performed experiments by integrating these ˆ MBL and h ˆ best , directly into the two features, h log-linear framework of Moses. ˆ best = h 2.6 Hierarchical PB-SMT model For the en–cs translation task, we built a weighted synchronous context-free grammar model (Chiang, 2007) of translation that uses the bilingual phrase pairs of"
W10-1720,2009.eamt-1.32,1,0.0893869,"oding paths based on 1 http://www.apertium.org (1) We use a memory-based machine learning (MBL) classifier (TRIBL:2 Daelemans and van den Bosch (2005)) that is able to estimate P (ˆ ek |fˆk , CI(fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations. In equation (1), SSCI may include any feature (lexical, syntactic, etc.), which can provide useful information to disambiguate a given source phrase. In addition to using local words and PoS-tags as features, as in (Stroppa et al., 2007), we incorporate grammatical dependency relations (Haque et al., 2009a) and supertags (Haque et al., 2009b) as syntactic source context features in the log-linear PB-SMT model. In addition to the above feature, we derived a ˆ best , defined in (2): simple binary feature h ( 1 if eˆk maximizes P (ˆ ek |fˆk , CI(fˆk )) 0 otherwise (2) We performed experiments by integrating these ˆ MBL and h ˆ best , directly into the two features, h log-linear framework of Moses. ˆ best = h 2.6 Hierarchical PB-SMT model For the en–cs translation task, we built a weighted synchronous context-free grammar model (Chiang, 2007) of translation that uses the bilingual phrase pairs of"
W10-1720,W04-3250,0,0.127192,"Missing"
W10-1720,2005.mtsummit-papers.11,0,0.0279389,"gth posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length ratio between source and target sentence; The weights are optimized via MERT. This section describes our experimental setup for the en–cs and en–es translation tasks. 3.1 Data Bilingual data: In the experiments we used data sets provided by the workshop organizers. For the en–cs translation table extraction we employed both parallel corpora (News-Commentary10 and CzEng 0.9), and for the en–es experiments, we used the Europarl(Koehn, 2005), News Commentary and United Nations parallel data. We used a maximum sentence length of 80 for en–es and 40 for en–cs. Detailed statistics are shown in Table 1. Corpus Langs. Sent. Europarl News-comm UN News-Comm CzEng en–es en–es en–es en–cs en–cs 1.6M 97k 5.9M 85k 7.8M Source tokens 43M 2.4M 160M 1.8M 80M Target tokens 45M 2.7M 190M 1.6M 69M Table 1: Statistics of en–cs and en–es parallel data. Monolingual data: For language modeling purposes, in addition to the target parts of the bilingual data, we used the monolingual News corpus for cs; and the Gigaword corpus for es. For both languages"
W10-1720,P07-2045,0,0.0171046,"Examples). This system exploits example-based pects of both the EBMT and SMT paradigms. MT, statistical MT (SMT), and system combinaThe architecture includes various individual systion techniques. tems: phrase-based, example-based, hierarchical We participated in the English–Spanish (en– phrase-based and tree-based MT. es) and English–Czech (en–cs) translation The combination structure uses the MBR and tasks. For these two tasks, we employ several CN decoders, and is based on a word-level comindividual MT systems: 1) Baseline: phrasebination strategy (Du et al., 2009). In the final based SMT (Koehn et al., 2007); 2) EBMT: stage, we use a new rescoring module to process Monolingually chunking both source and target the N -best list generated by the combination modsides of the dataset using a marker-based chunker ule. Figure 1 illustrates the architecture. (Gough and Way, 2004); 3) Factored translation model (Koehn and Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based"
W10-1720,N04-1022,0,0.0614202,"ribe the modular design of our multi-engine machine translation (MT) system with particular focus on the components used in this participation. We participated in the English– Spanish and English–Czech translation tasks, in which we employed our multiengine architecture to translate. We also participated in the system combination task which was carried out by the MBR decoder and confusion network decoder. 1 Introduction multiple translation hypotheses and employ a new rescoring model to generate the final translation. For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the confusion network (CN) (Mangu et al., 2000). We then build the CN using the TER metric (Snover et al., 2006), and finally search for the best translation. The remainder of this paper is organised as follows: Section 2 details the various components of our system, in particular the multi-engine strategies used for the shared task. In Section 3, we outline the complete system setup for the shared task and provide evaluation results on the test set. Section 4 concludes the paper. 2 The M AT R E X System 2.1 System Architect"
W10-1720,P03-1021,0,0.0273052,"output of each individual system. The CN is built by aligning other hypotheses against the backbone, based on the TER metric. Null words are allowed in the alignment. Either votes or different confidence measures are assigned to each word in the network. Each arc in the CN represents an alternative word at that position in the sentence and the number of votes for each word is counted when constructing the network. The features we used are as follows: • • • • word posterior probability (Fiscus, 1997); 3, 4-gram target language model; word length penalty; Null word length penalty; We use MERT (Och, 2003) to tune the weights of the CN. 2.8 Rescoring Rescoring is a very important part in postprocessing which can select a better hypothesis from the N -best list. We augmented our previous rescoring model (Du et al., 2009) with more large-scale data. The features we used include: • Direct and inverse IBM model; • 3, 4-gram target language model; • 3, 4, 5-gram PoS language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length rat"
W10-1720,P02-1038,0,0.00732471,"xtracted by the baseline system adding word alignment information. 2.3 Apertium RBMT Apertium1 is a free/open-source platform for RBMT. The current version of the en–es system in Apertium was used for the system combination task (section 2.7), and its morphological analysers and part-of-speech taggers were used to build a factored Moses model. 2.4 Factored Translation Model We also used a factored model for the en–es translation task. Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002). We used three factors in our factored translation model, which are used in two different decoding paths: a surface form (SF) to SF translation factor, a lemma to lemma translation factor, and a part-ofspeech (PoS) to PoS translation factor. Finally, we used two decoding paths based on 1 http://www.apertium.org (1) We use a memory-based machine learning (MBL) classifier (TRIBL:2 Daelemans and van den Bosch (2005)) that is able to estimate P (ˆ ek |fˆk , CI(fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations. In equation (1), SSCI"
W10-1720,P02-1040,0,0.113346,"Missing"
W10-1720,2006.amta-papers.25,0,0.0454791,"and English–Czech translation tasks, in which we employed our multiengine architecture to translate. We also participated in the system combination task which was carried out by the MBR decoder and confusion network decoder. 1 Introduction multiple translation hypotheses and employ a new rescoring model to generate the final translation. For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the confusion network (CN) (Mangu et al., 2000). We then build the CN using the TER metric (Snover et al., 2006), and finally search for the best translation. The remainder of this paper is organised as follows: Section 2 details the various components of our system, in particular the multi-engine strategies used for the shared task. In Section 3, we outline the complete system setup for the shared task and provide evaluation results on the test set. Section 4 concludes the paper. 2 The M AT R E X System 2.1 System Architecture In this paper, we present the DCU multi-engine The M AT R E X system is a combination-based MT system M AT R E X (Machine Translation using multi-engine architecture, which explo"
W10-1720,2007.tmi-papers.28,1,0.632786,"Missing"
W10-1720,2006.iwslt-evaluation.4,1,0.38824,"tion factors: an SF to SF decoding path and a path which maps lemma to lemma, PoS to PoS, and an SF generated using the TL lemma and PoS. The lemmas and PoS for en and es were obtained using Apertium (section 2.3). 2.5 Source-Side Context-informed PB-SMT One natural way to express a context-informed ˆ MBL ) is to view it as the conditional feature (h probability of the target phrases (ˆ ek ) given the ˆ source phrase (fk ) and its source-side context information (CI): ˆ MBL = log P (ˆ h ek |fˆk , CI(fˆk )) Figure 1: System Framework. tion probabilities and the amount of word-to-word cognates (Stroppa and Way, 2006). Once these phrase pairs were obtained they were merged with the phrase pairs extracted by the baseline system adding word alignment information. 2.3 Apertium RBMT Apertium1 is a free/open-source platform for RBMT. The current version of the en–es system in Apertium was used for the system combination task (section 2.7), and its morphological analysers and part-of-speech taggers were used to build a factored Moses model. 2.4 Factored Translation Model We also used a factored model for the en–es translation task. Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it"
W10-1720,W06-3110,0,0.03498,"word posterior probability (Fiscus, 1997); 3, 4-gram target language model; word length penalty; Null word length penalty; We use MERT (Och, 2003) to tune the weights of the CN. 2.8 Rescoring Rescoring is a very important part in postprocessing which can select a better hypothesis from the N -best list. We augmented our previous rescoring model (Du et al., 2009) with more large-scale data. The features we used include: • Direct and inverse IBM model; • 3, 4-gram target language model; • 3, 4, 5-gram PoS language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length ratio between source and target sentence; The weights are optimized via MERT. This section describes our experimental setup for the en–cs and en–es translation tasks. 3.1 Data Bilingual data: In the experiments we used data sets provided by the workshop organizers. For the en–cs translation table extraction we employed both parallel corpora (News-Commentary10 and CzEng 0.9), and for the en–es experiments, we used the Europarl(Koehn, 2005), News Commentary and United Natio"
W10-1720,W96-0213,0,0.0699016,"ng the network. The features we used are as follows: • • • • word posterior probability (Fiscus, 1997); 3, 4-gram target language model; word length penalty; Null word length penalty; We use MERT (Och, 2003) to tune the weights of the CN. 2.8 Rescoring Rescoring is a very important part in postprocessing which can select a better hypothesis from the N -best list. We augmented our previous rescoring model (Du et al., 2009) with more large-scale data. The features we used include: • Direct and inverse IBM model; • 3, 4-gram target language model; • 3, 4, 5-gram PoS language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length ratio between source and target sentence; The weights are optimized via MERT. This section describes our experimental setup for the en–cs and en–es translation tasks. 3.1 Data Bilingual data: In the experiments we used data sets provided by the workshop organizers. For the en–cs translation table extraction we employed both parallel corpora (News-Commentary10 and CzEng 0.9), and for the en–es experiments, we us"
W10-1720,N07-1029,0,0.00800063,"d Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based (HPB) lexemes (Gough and Way, 2004) to define a way approach of Chiang (2007)) and 6) Apertium (Forto segment sentences into chunks, which are then cada et al., 2009) rule-based machine translation aligned using an edit-distance-style algorithm, in (RBMT). Finally, we use a word-level combination framework (Rosti et al., 2007) to combine the 143which edit costs depend on word-to-word translaProceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 143–148, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics the above three translation factors: an SF to SF decoding path and a path which maps lemma to lemma, PoS to PoS, and an SF generated using the TL lemma and PoS. The lemmas and PoS for en and es were obtained using Apertium (section 2.3). 2.5 Source-Side Context-informed PB-SMT One natural way to express a context-informed ˆ MBL ) is to view it"
W10-1720,D07-1091,0,\N,Missing
W10-1720,2003.mtsummit-systems.3,0,\N,Missing
W10-1720,W10-1742,1,\N,Missing
W10-3707,C04-1114,0,0.441408,"ither. We address this many-to-many alignment problem indirectly. Our objective is to see how to best handle the MWEs in SMT. In this work, two types of MWEs, namely NEs and compound verbs, are automatically identified on both sides of the parallel corpus. Then, source and target language NEs are aligned using a statistical transliteration method. We rely on these automatically aligned NEs and treat them as translation examples. Adding bilingual dictionaries, which in effect are instances of atomic translation pairs, to the parallel corpus is a well-known practice in domain adaptation in SMT (Eck et al., 2004; Wu et al., 2008). We modify the parallel corpus by converting the MWEs into single tokens and adding the aligned NEs in the parallel corpus in a bid to improve the word alignment, and hence the phrase alignment quality. This 46 Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 46–54, Beijing, August 2010 preprocessing results in improved MT quality in terms of automatic MT evaluation metrics. The remainder of the paper is organized as follows. In section 2 we discuss related work. The System is described in Section 3. Section 4 includes the results obtai"
W10-3707,W09-3539,1,0.741895,"Missing"
W10-3707,W05-0909,0,0.209211,"Missing"
W10-3707,W04-3248,0,0.280025,"Missing"
W10-3707,J93-2003,0,0.0150925,"ed MWEs can bring about any further improvement on top of that. We carried out our experiments on an English—Bangla translation task, a relatively hard task with Bangla being a morphologically richer language. 3 System Description 3.1 PB-SMT Translation is modeled in SMT as a decision process, in which the translation e1I = e1 . . . ei . . . eI of a source sentence f1J = f1 . . . fj . . . fJ is chosen to maximize (1): (1) arg max P(e1I |f1J ) = arg max P( f1J |e1I ).P(e1I ) I ,e1I I ,e1I where P ( f1J |e1I ) and P (e1I ) denote respectively the translation model and the target language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( e1I |f1J ) is directly modeled as a log-linear combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): M log P(e1I |f 1J ) = ∑ λ m hm ( f 1J , e1I , s1K ) m =1 + λLM log P(e1I ) (2) where s = s1...sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (eˆ1 ,..., eˆk ) and ( fˆ1 ,..., fˆk ) such that (we set i0 = 0) (3): k 1 ∀1 ≤ k ≤ K , sk = (ik, bk, jk), eˆk = eik −1 +1...eik , fˆ = f ... f . k bk jk (3) and eac"
W10-3707,W03-1502,0,0.128214,"r simultaneous NE identification and translation. He uses capitalization cues for identifying NEs on the English side, and then he applies statistical techniques to decide which portion of the target language corresponds to the specified English NE. Feng et al. (2004) proposed a Maximum Entropy model based approach for English— Chinese NE alignment which significantly outperforms IBM Model4 and HMM. They considered 4 features: translation score, transliteration score, source NE and target NE's co-occurrence score, and the distortion score for distinguishing identical NEs in the same sentence. Huang et al. (2003) proposed a method for automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization. The costs considered are transliteration cost, word-based translation cost, and NE tagging cost. Venkatapathy and Joshi (2006) reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilin"
W10-3707,N10-1029,0,0.108076,"reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into Moses (Koehn et al., 2007) in three ways. They observed the highest improvement when they used an additional feature to represent whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010). In their work, the binary feature was replaced by a count feature representing the number of MWEs in the source language phrase. Intuitively, MWEs should be both aligned in the parallel corpus and translated as a whole. However, in the state-of-the-art PB-SMT, it could well be the case that constituents of an 47 MWE are marked and aligned as parts of consecutive phrases, since PB-SMT (or any other approaches to SMT) does not generally treat MWEs as special tokens. Another problem SMT suffers from is that verb phrases are often wrongly translated, or even sometimes deleted in the output in or"
W10-3707,C08-2007,0,0.104429,"Missing"
W10-3707,P07-2045,0,0.0291762,"gual equivalences between Chinese and English based on multi-feature cost minimization. The costs considered are transliteration cost, word-based translation cost, and NE tagging cost. Venkatapathy and Joshi (2006) reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into Moses (Koehn et al., 2007) in three ways. They observed the highest improvement when they used an additional feature to represent whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010). In their work, the binary feature was replaced by a count feature representing the number of MWEs in the source language phrase. Intuitively, MWEs should be both aligned in the parallel corpus and translated as a whole. However, in the state-of-the-art PB-SMT, it could well be the case that constituents of an 47 MWE are marked and aligned as parts of consecutive phrases, since"
W10-3707,2006.amta-papers.25,0,0.0728121,"Missing"
W10-3707,C96-2141,0,0.388305,"(or spaces)” (Sag et al., 2002). Traditional approaches to word alignment following IBM Models (Brown et al., 1993) do not work well with multi-word expressions, especially with NEs, due to their inability to handle manyto-many alignments. Firstly, they only carry out alignment between words and do not consider the case of complex expressions, such as multiword NEs. Secondly, the IBM Models only allow at most one word in the source language to correspond to a word in the target language (Marcu, 2001, Koehn et al., 2003). In another well-known word alignment approach, Hidden Markov Model (HMM: Vogel et al., 1996), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this many-to-many alignment problem indirectly. Our objective is to see how to best handle the MWEs in SMT. In this work, two types of MWEs, namely NEs and compound verbs, are automatically identified on both sides of the parallel corpus. Then, source and target language NEs are aligned using a statistical transliteration method. We rely on these automatically aligned NEs and treat them as translation examples. Adding bilingual dictionar"
W10-3707,W04-3250,0,0.31373,"Missing"
W10-3707,W06-1204,0,0.435823,"Missing"
W10-3707,P01-1050,0,0.00961498,"pus. Multi-word expressions (MWE) are defined as “idiosyncratic interpretations that cross word boundaries (or spaces)” (Sag et al., 2002). Traditional approaches to word alignment following IBM Models (Brown et al., 1993) do not work well with multi-word expressions, especially with NEs, due to their inability to handle manyto-many alignments. Firstly, they only carry out alignment between words and do not consider the case of complex expressions, such as multiword NEs. Secondly, the IBM Models only allow at most one word in the source language to correspond to a word in the target language (Marcu, 2001, Koehn et al., 2003). In another well-known word alignment approach, Hidden Markov Model (HMM: Vogel et al., 1996), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this many-to-many alignment problem indirectly. Our objective is to see how to best handle the MWEs in SMT. In this work, two types of MWEs, namely NEs and compound verbs, are automatically identified on both sides of the parallel corpus. Then, source and target language NEs are aligned using a statistical transliteration me"
W10-3707,C08-1125,0,0.154066,"Missing"
W10-3707,E03-1035,0,0.114084,"igned NEs in the parallel corpus in a bid to improve the word alignment, and hence the phrase alignment quality. This 46 Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 46–54, Beijing, August 2010 preprocessing results in improved MT quality in terms of automatic MT evaluation metrics. The remainder of the paper is organized as follows. In section 2 we discuss related work. The System is described in Section 3. Section 4 includes the results obtained, together with some analysis. Section 5 concludes, and provides avenues for further work. 2 Related Work Moore (2003) presented an approach for simultaneous NE identification and translation. He uses capitalization cues for identifying NEs on the English side, and then he applies statistical techniques to decide which portion of the target language corresponds to the specified English NE. Feng et al. (2004) proposed a Maximum Entropy model based approach for English— Chinese NE alignment which significantly outperforms IBM Model4 and HMM. They considered 4 features: translation score, transliteration score, source NE and target NE's co-occurrence score, and the distortion score for distinguishing identical N"
W10-3707,P03-1021,0,0.0160755,"guages to Indian Languages Machine Translation (ILILMT) System”. NEs in Bangla are identified using the NER system of Ekbal and Bandyopadhyay (2008). We use the Stanford Parser, Stanford NER and the NER for Bangla along with the default model files provided, i.e., with no additional training. The effectiveness of the MWE-aligned parallel corpus developed in the work is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phraseextraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model with Kneser-Ney smoothing (Kneser and 1 The EILMT and ILILMT projects are funded by the Department of Information Technology (DIT), Ministry of Communications and Information Technology (MCIT), Government of India. 2 http://nlp.stanford.edu/software/lex-parser.shtml 3 4 Ney, 1995) trained with SRILM (Stolcke, 2002), and Moses decoder (Koehn et al., 2007). http://crfchunker.sourceforge.net/ http://nlp.stanford.edu/software/CRF-NER.shtml 50 Experiments and Results We randomly extracted 500 sentences each for the development set and testset fr"
W10-3707,P02-1040,0,0.0819899,"Missing"
W10-3707,W09-2907,0,0.376728,"re, transliteration score, source NE and target NE's co-occurrence score, and the distortion score for distinguishing identical NEs in the same sentence. Huang et al. (2003) proposed a method for automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization. The costs considered are transliteration cost, word-based translation cost, and NE tagging cost. Venkatapathy and Joshi (2006) reported a discriminative approach of using the compositionality information about verb-based multi-word expressions to improve word alignment quality. (Ren et al., 2009) presented log likelihood ratiobased hierarchical reducing algorithm to automatically extract bilingual MWEs, and investigated the usefulness of these bilingual MWEs in SMT by integrating bilingual MWEs into Moses (Koehn et al., 2007) in three ways. They observed the highest improvement when they used an additional feature to represent whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010). In their work, the binary feature was replaced by a count feature representing the number of MWEs in the source language phrase. Intuitively, MWE"
W10-3707,N03-1017,0,\N,Missing
W12-5606,W05-0909,0,0.383134,"ally Hindi) has gained tremendous research interest in India and elsewhere. Many English to Hindi and Indian Languages to Indian Languages MT systems have been designed, for example AnglaBharati (Sinha et al., 1995), Anusaaraka2 (Chaudhury et al., 2010), Anuvadaksh3, Google4, Sampark5, MaTra6 (Ananthakrishnan et al., 2006), to name just a few. However, the issue of evaluating the output of these MT systems has remained rather unexplored. The state-of-the-art methods for automatic MT evaluation are represented by BLEU (Papineni et al., 2002) and closely related NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) and TER (Snover et al., 2006). These metrics have been widely accepted as benchmarks for MT system evaluation. However, the research community is also aware of the deficiencies of these metrics (Callison-Burch et al., 2006). Globally, these automatic MT evaluation metrics (BLEU, NIST, TER, METEOR, etc.) are being studied with great interest for different language pairs. But their direct applicability to Hindi, or other Indian languages for that matter, needs proper investigation. Indian languages are characteristically different from English and other related Europea"
W12-5606,sankaran-etal-2008-common,0,0.030143,"a linguistically-motivated unit e.g., it can be an ambiguous word, a verb-particle construction, a noun-noun compound, a PoS n-gram etc. The level of detail and the specific linguistic phenomena included in the taxonomy can vary depending on what the users want to investigate as part of the diagnostic evaluation. However, the taxonomy of automatic diagnostic evaluation should be widely accepted. The categories that are out of scope for current NLP tools to recognize have been ignored in this study. In light of the above consideration, we adopted the taxonomy introduced by Lata et al. (2012), Baskaran et al. (2008) and the IIIT Tagset11 (Bharati et al., 2006) for Hindi. The taxonomy includes typical checkpoints at word level. Some examples of the representative checkpoints at different levels for English and Hindi languages have been presented in the following subsection. 4.1 English to Hindi Checkpoints The implementation of the English to Hindi checkpoint taxonomy can take into account various checkpoints at word and phrase level. However, only 8 word level categories have been considered for this study. The taxonomy is shown in Table 1. In practice, any tag used by parsers (e.g. NP, VP, PP, etc.) can"
W12-5606,2010.eamt-1.12,0,0.0287093,"Missing"
W12-5606,W12-3105,0,0.0126352,"lexical, semantic, and syntactic level. Some automatic methods for error analysis using base forms and PoS tags have been proposed in (Popović et al., 2006; Popović and Ney, 2011). The proposed methods have been used for estimation of inflectional and reordering errors. Popović and Burchardt (2011) present a method for automatic error classification. Popović (2011) describes a tool that classifies errors into five categories based on the hierarchy proposed by Vilar et al. (2006). Popović (2012) describes RGBF, a tool for automatic evaluation of MT output based on n-gram precision and recall. Fishel et al. (2012) quantifies translation quality based on the frequencies of different error categories. Xiong et al. (2010) used a classifier trained with a set of linguistic features to automatically detect incorrect segments in MT output. EAGLES (1996) distinguishes a type of evaluation whose purpose is to discover the reason(s) why a system did not produce the results it was expected to. Working on these lines Zhou et al. (2008) proposed diagnostic evaluation of linguistic checkpoints. Naskar et al. (2011) proposed a framework for diagnostic MT evaluation which offers similar functionality as proposed in ("
W12-5606,W07-0734,0,0.0190737,"mendous research interest in India and elsewhere. Many English to Hindi and Indian Languages to Indian Languages MT systems have been designed, for example AnglaBharati (Sinha et al., 1995), Anusaaraka2 (Chaudhury et al., 2010), Anuvadaksh3, Google4, Sampark5, MaTra6 (Ananthakrishnan et al., 2006), to name just a few. However, the issue of evaluating the output of these MT systems has remained rather unexplored. The state-of-the-art methods for automatic MT evaluation are represented by BLEU (Papineni et al., 2002) and closely related NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) and TER (Snover et al., 2006). These metrics have been widely accepted as benchmarks for MT system evaluation. However, the research community is also aware of the deficiencies of these metrics (Callison-Burch et al., 2006). Globally, these automatic MT evaluation metrics (BLEU, NIST, TER, METEOR, etc.) are being studied with great interest for different language pairs. But their direct applicability to Hindi, or other Indian languages for that matter, needs proper investigation. Indian languages are characteristically different from English and other related European languages for which thes"
W12-5606,2011.mtsummit-papers.60,1,0.818646,"an languages for which these metrics are mostly used. There have been some efforts in this direction for Indian languages (Chatterjee and Balyan, 2011; Gupta et al., 2010; Ananthakrishnan et al., 2007; Chatterjee et al., 2007; Moona et al., 2004). Barring these few exceptions, the subject has not been studied deeply. Most of these approaches, however, either cover human evaluation, or consider modification of existing automatic metrics (like BLEU and METEOR) to make them more suitable for Indian languages. None of these works has been targeted towards diagnostic evaluation (Zhou et al., 2008; Naskar et al., 2011; Popović, 2011), which not only provides quantitative analysis, but also qualitative feedback of the machine translated text. It also provides feedback and detailed analysis of how an MT system performs for different linguistic features like verbs, nouns, compounds etc. Our final aim is to come up with an approach for diagnostic evaluation of MT that can be adapted to Indian languages. In the present work the experiments have been carried out with the DELiC4MT (Toral et al., 2012) toolkit as it is language independent. The experiments have been carried out to adapt the tool for Hindi, which c"
W12-5606,J03-1002,0,0.00270755,"ysis and KAF conversion, word alignment extraction, defining kybots and evaluation. The tool makes extensive use of already available NLP tools and representation standards. The evaluation pipeline proceeds as follows. 7 http://www.computing.dcu.ie/~atoral/delic4mt(under the GPL-v3 license). 63  The source and target sides of the gold standard (test set) are processed by respective PoS taggers (Treetagger8 for English and a shallow parser for Hindi) and converted into KYOTO Annotation Format (KAF) (Bosma et al., 2009) to represent textual analysis. The test set is word aligned using GIZA++ 9(Och and Ney, 2003), and identifiers of the aligned tokens are stored. Kybot10 (Vossen et al., 2010) profiles specifying the linguistic checkpoints to be extracted are run on the KAF text and the matching terms are extracted. The evaluation module takes kybot output, KAF text, word alignments and the output of an MT system (plain text, no word alignment is performed on it) as inputs. It calculates the performance of the MT system over the linguistic checkpoint(s) considered.    The details of the tool regarding KAF files and kybot profiles can be found in Toral et al. (2012). 4 Linguistic Checkpoints A lingui"
W12-5606,P02-1040,0,0.0875737,"dian languages. In the last 15 years or so, MT into Indian languages (especially Hindi) has gained tremendous research interest in India and elsewhere. Many English to Hindi and Indian Languages to Indian Languages MT systems have been designed, for example AnglaBharati (Sinha et al., 1995), Anusaaraka2 (Chaudhury et al., 2010), Anuvadaksh3, Google4, Sampark5, MaTra6 (Ananthakrishnan et al., 2006), to name just a few. However, the issue of evaluating the output of these MT systems has remained rather unexplored. The state-of-the-art methods for automatic MT evaluation are represented by BLEU (Papineni et al., 2002) and closely related NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) and TER (Snover et al., 2006). These metrics have been widely accepted as benchmarks for MT system evaluation. However, the research community is also aware of the deficiencies of these metrics (Callison-Burch et al., 2006). Globally, these automatic MT evaluation metrics (BLEU, NIST, TER, METEOR, etc.) are being studied with great interest for different language pairs. But their direct applicability to Hindi, or other Indian languages for that matter, needs proper investigation. Indian lan"
W12-5606,J11-4002,0,0.0786144,"rk Although diagnostic evaluation of MT has been occasionally addressed in the literature in the last few years, no widely accepted solution seems to have emerged till date. A framework proposed by Vilar et al. (2006) analyzes the errors manually. The scheme covers five top-level classes: missing words, incorrect words, unknown words, word order and punctuation errors. Farrús et al. (2010) classified errors at orthographic, morphological, lexical, semantic, and syntactic level. Some automatic methods for error analysis using base forms and PoS tags have been proposed in (Popović et al., 2006; Popović and Ney, 2011). The proposed methods have been used for estimation of inflectional and reordering errors. Popović and Burchardt (2011) present a method for automatic error classification. Popović (2011) describes a tool that classifies errors into five categories based on the hierarchy proposed by Vilar et al. (2006). Popović (2012) describes RGBF, a tool for automatic evaluation of MT output based on n-gram precision and recall. Fishel et al. (2012) quantifies translation quality based on the frequencies of different error categories. Xiong et al. (2010) used a classifier trained with a set of linguistic f"
W12-5606,2011.eamt-1.36,0,0.0128152,"o widely accepted solution seems to have emerged till date. A framework proposed by Vilar et al. (2006) analyzes the errors manually. The scheme covers five top-level classes: missing words, incorrect words, unknown words, word order and punctuation errors. Farrús et al. (2010) classified errors at orthographic, morphological, lexical, semantic, and syntactic level. Some automatic methods for error analysis using base forms and PoS tags have been proposed in (Popović et al., 2006; Popović and Ney, 2011). The proposed methods have been used for estimation of inflectional and reordering errors. Popović and Burchardt (2011) present a method for automatic error classification. Popović (2011) describes a tool that classifies errors into five categories based on the hierarchy proposed by Vilar et al. (2006). Popović (2012) describes RGBF, a tool for automatic evaluation of MT output based on n-gram precision and recall. Fishel et al. (2012) quantifies translation quality based on the frequencies of different error categories. Xiong et al. (2010) used a classifier trained with a set of linguistic features to automatically detect incorrect segments in MT output. EAGLES (1996) distinguishes a type of evaluation whose"
W12-5606,W06-3101,0,0.0178988,"ure work. 2 Related work Although diagnostic evaluation of MT has been occasionally addressed in the literature in the last few years, no widely accepted solution seems to have emerged till date. A framework proposed by Vilar et al. (2006) analyzes the errors manually. The scheme covers five top-level classes: missing words, incorrect words, unknown words, word order and punctuation errors. Farrús et al. (2010) classified errors at orthographic, morphological, lexical, semantic, and syntactic level. Some automatic methods for error analysis using base forms and PoS tags have been proposed in (Popović et al., 2006; Popović and Ney, 2011). The proposed methods have been used for estimation of inflectional and reordering errors. Popović and Burchardt (2011) present a method for automatic error classification. Popović (2011) describes a tool that classifies errors into five categories based on the hierarchy proposed by Vilar et al. (2006). Popović (2012) describes RGBF, a tool for automatic evaluation of MT output based on n-gram precision and recall. Fishel et al. (2012) quantifies translation quality based on the frequencies of different error categories. Xiong et al. (2010) used a classifier trained wi"
W12-5606,2006.amta-papers.25,0,0.147819,"and elsewhere. Many English to Hindi and Indian Languages to Indian Languages MT systems have been designed, for example AnglaBharati (Sinha et al., 1995), Anusaaraka2 (Chaudhury et al., 2010), Anuvadaksh3, Google4, Sampark5, MaTra6 (Ananthakrishnan et al., 2006), to name just a few. However, the issue of evaluating the output of these MT systems has remained rather unexplored. The state-of-the-art methods for automatic MT evaluation are represented by BLEU (Papineni et al., 2002) and closely related NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) and TER (Snover et al., 2006). These metrics have been widely accepted as benchmarks for MT system evaluation. However, the research community is also aware of the deficiencies of these metrics (Callison-Burch et al., 2006). Globally, these automatic MT evaluation metrics (BLEU, NIST, TER, METEOR, etc.) are being studied with great interest for different language pairs. But their direct applicability to Hindi, or other Indian languages for that matter, needs proper investigation. Indian languages are characteristically different from English and other related European languages for which these metrics are mostly used. The"
W12-5606,vilar-etal-2006-error,0,0.0231686,"ation tool, DELiC4MT, which has been used for this study. In Section 4, the various linguistic checkpoints considered for the study of English and Hindi have been discussed. Section 5 discusses the experimental setup and compares the results obtained on the EnglishHindi test set using DELiC4MT and automatic evaluation metrics. This is followed by conclusions and avenues for future work. 2 Related work Although diagnostic evaluation of MT has been occasionally addressed in the literature in the last few years, no widely accepted solution seems to have emerged till date. A framework proposed by Vilar et al. (2006) analyzes the errors manually. The scheme covers five top-level classes: missing words, incorrect words, unknown words, word order and punctuation errors. Farrús et al. (2010) classified errors at orthographic, morphological, lexical, semantic, and syntactic level. Some automatic methods for error analysis using base forms and PoS tags have been proposed in (Popović et al., 2006; Popović and Ney, 2011). The proposed methods have been used for estimation of inflectional and reordering errors. Popović and Burchardt (2011) present a method for automatic error classification. Popović (2011) descri"
W12-5606,W10-3301,0,0.0169006,"ion. The tool makes extensive use of already available NLP tools and representation standards. The evaluation pipeline proceeds as follows. 7 http://www.computing.dcu.ie/~atoral/delic4mt(under the GPL-v3 license). 63  The source and target sides of the gold standard (test set) are processed by respective PoS taggers (Treetagger8 for English and a shallow parser for Hindi) and converted into KYOTO Annotation Format (KAF) (Bosma et al., 2009) to represent textual analysis. The test set is word aligned using GIZA++ 9(Och and Ney, 2003), and identifiers of the aligned tokens are stored. Kybot10 (Vossen et al., 2010) profiles specifying the linguistic checkpoints to be extracted are run on the KAF text and the matching terms are extracted. The evaluation module takes kybot output, KAF text, word alignments and the output of an MT system (plain text, no word alignment is performed on it) as inputs. It calculates the performance of the MT system over the linguistic checkpoint(s) considered.    The details of the tool regarding KAF files and kybot profiles can be found in Toral et al. (2012). 4 Linguistic Checkpoints A linguistic checkpoint is a linguistically-motivated unit e.g., it can be an ambiguous w"
W12-5606,P10-1062,0,0.0246481,"tags have been proposed in (Popović et al., 2006; Popović and Ney, 2011). The proposed methods have been used for estimation of inflectional and reordering errors. Popović and Burchardt (2011) present a method for automatic error classification. Popović (2011) describes a tool that classifies errors into five categories based on the hierarchy proposed by Vilar et al. (2006). Popović (2012) describes RGBF, a tool for automatic evaluation of MT output based on n-gram precision and recall. Fishel et al. (2012) quantifies translation quality based on the frequencies of different error categories. Xiong et al. (2010) used a classifier trained with a set of linguistic features to automatically detect incorrect segments in MT output. EAGLES (1996) distinguishes a type of evaluation whose purpose is to discover the reason(s) why a system did not produce the results it was expected to. Working on these lines Zhou et al. (2008) proposed diagnostic evaluation of linguistic checkpoints. Naskar et al. (2011) proposed a framework for diagnostic MT evaluation which offers similar functionality as proposed in (Zhou et al., 2008) but is language independent. 3 DELiC4MT: A Diagnostic MT Evaluation Tool DELiC4MT7 (Diag"
W12-5606,C08-1141,0,0.0832219,"ther related European languages for which these metrics are mostly used. There have been some efforts in this direction for Indian languages (Chatterjee and Balyan, 2011; Gupta et al., 2010; Ananthakrishnan et al., 2007; Chatterjee et al., 2007; Moona et al., 2004). Barring these few exceptions, the subject has not been studied deeply. Most of these approaches, however, either cover human evaluation, or consider modification of existing automatic metrics (like BLEU and METEOR) to make them more suitable for Indian languages. None of these works has been targeted towards diagnostic evaluation (Zhou et al., 2008; Naskar et al., 2011; Popović, 2011), which not only provides quantitative analysis, but also qualitative feedback of the machine translated text. It also provides feedback and detailed analysis of how an MT system performs for different linguistic features like verbs, nouns, compounds etc. Our final aim is to come up with an approach for diagnostic evaluation of MT that can be adapted to Indian languages. In the present work the experiments have been carried out with the DELiC4MT (Toral et al., 2012) toolkit as it is language independent. The experiments have been carried out to adapt the to"
W12-5606,E06-1032,0,\N,Missing
W13-2814,P06-1097,0,0.019944,"re capitalization cues have been used for identifying NEs on the English side. Statistical techniques are applied to decide which portion of the target language corresponds to the specified English NE, for simultaneous NE identification and translation. To improve the learning process of unlabeled data using labeled data (Chapelle et al., 2006), the semi-supervised learning method is the most useful learning technique. Semi-supervised learning is a broader area of Machine Learning. Researchers have begun to explore semisupervised word alignment models that use both labeled and unlabeled data. Fraser and Marcu (2006) proposed a semi-supervised training algo95 guage resources in Bengali are not widely available. 3 ods. Our approach deals with the latter case. The supervised technique of Berkeley aligner helps us to align those words which could not be aligned by rule based word aligner. Hybrid Word Alignment Model The hybrid word alignment model is described as the combination of three word alignment models as follows: 3.1 3.3 The proposed Rule based aligner aligns Named Entities (NEs) and chunks. For NE alignment, we first identify NEs from the source side (i.e. English) using Stanford NER. The NEs on the"
W13-2814,J93-2003,0,0.05003,"Model for Phrase-Based Statistical Machine Translation Santanu Pal*, Sudip Kumar Naskar† and Sivaji Bandyopadhyay* * Department of Computer Science & Engineering Jadavpur University, Kolkata, India santanu.pal.ju@gmail.com, sivaji_cse_ju@yahoo.com † Department of Computer & System Sciences Visva-Bharati University, Santiniketan, India sudip.naskar@gmail.com tion of each other. Statistical machine translation usually suffers from many-to-many word links which existing statistical word alignment algorithms can not handle well. The unsupervised word alignment models are based on IBM models 1–5 (Brown et al., 1993) and the HMM model (Ney and Vogel, 1996; Och and Ney, 2003). Models 3, 4 and 5 are based on fertility based models which are asymmetric. To improve alignment quality, the Berkeley Aligner is based on the symmetric property by intersecting alignments induced in each translation direction. In the present work, we propose improvement of word alignment quality by combining three word alignment tables (i) GIZA++ alignment (ii) Berkeley Alignment and (iii) rule based alignment. Our objective is to perceive the effectiveness of the Hybrid model in word alignment by improving the quality of translatio"
W13-2814,P04-1023,0,0.0291512,"scribed in Section 3. Section 4 presents the tools and resources used for the various experiments. Section 5 includes the results obtained, together with some analysis. Section 6 concludes and provides avenues for further work. 2 rithm. The weighting parameters are learned from discriminative error training on labeled data, and the parameters are estimated by maximum-likelihood EM training on unlabeled data. They have also used a log-linear model which is trained on the available labeled data to improve performance. Interpolating human alignments with automatic alignments has been proposed by Callison-Burch et al. (2004), where the alignments of higher quality have gained much higher weight than the lower-quality alignments. Wu et al. (2006) have developed two separate models of standard EM algorithm which learn separately from both labeled and unlabeled data. Two models are then interpolated as a learner in the semisupervised Ada-Boost algorithm to improve word alignment. Ambati et al. (2010) proposed active learning query strategies to identify highly uncertain or most informative alignment links under an unsupervised word alignment model. Intuitively, multiword NEs on the source and the target sides should"
W13-2814,C04-1114,0,0.0277213,"ork, we have implemented a rule based alignment model by considering several types of chunks which are automatically extracted on the source side. Each individual source chunk is translated using a baseline PBSMT system and validated with the target chunks on the target side. The validated source-target chunks are added in the rule based alignment table. Work has been carried out into three directions: (i) three alignment tables are combined together by taking their union; (ii) extra alignment pairs are added into the alignment table. This is a well-known practice in domain adaptation in SMT (Eck et al., 2004; Wu et al., 2008); (iii) the alignment table is updated through semisupervised alignment technique. Abstract This paper proposes a hybrid word alignment model for Phrase-Based Statistical Machine translation (PB-SMT). The proposed hybrid alignment model provides most informative alignment links which are offered by both unsupervised and semi-supervised word alignment models. Two unsupervised word alignment models (GIZA++ and Berkeley aligner) and a rule based aligner are combined together. The rule based aligner only aligns named entities (NEs) and chunks. The NEs are aligned through translit"
W13-2814,W09-3539,1,0.819859,"re not widely available. 3 ods. Our approach deals with the latter case. The supervised technique of Berkeley aligner helps us to align those words which could not be aligned by rule based word aligner. Hybrid Word Alignment Model The hybrid word alignment model is described as the combination of three word alignment models as follows: 3.1 3.3 The proposed Rule based aligner aligns Named Entities (NEs) and chunks. For NE alignment, we first identify NEs from the source side (i.e. English) using Stanford NER. The NEs on the target side (i.e. Bengali) are identified using a method described in (Ekbal and Bandyopadhyay, 2009). The accuracy of the Bengali Named Entity recognizers (NER) is much poorer compared to that of English NER due to several reasons: (i) there is no capitalization cue for NEs in Bengali; (ii) most of the common nouns in Bengali are frequently used as proper nouns; (iii) suffixes (case markers, plural markers, emphasizers, specifiers) get attached to proper names as well in Bengali. Bengali shallow parser 1 has been used to improve the performance of NE identification by considering proper names as NE. Therefore, NER and shallow parser are jointly employed to detect NEs from the Bengali sentenc"
W13-2814,W04-3248,0,0.105861,"ou et. al., 2004). Pal et, al. (2012) proposed a bootstrapping method for chunk alignment; they used an SMT based model for chunk translation and then aligned the sourcetarget chunk pairs after validating the translated chunk. Ma et. al. (2007) simplified the task of automatic word alignment as several consecutive words together correspond to a single word in the opposite language by using the word aligner itself, i.e., by bootstrapping on its output. A Maximum Entropy model based approach for English—Chinese NE alignment which significantly outperforms IBM Model4 and HMM has been proposed by Feng et al. (2004). They considered 4 features: translation score, transliteration score, source NE and target NE&apos;s co-occurrence score and the distortion score for distinguishing identical NEs in the same sentence. Moore (2003) presented an approach where capitalization cues have been used for identifying NEs on the English side. Statistical techniques are applied to decide which portion of the target language corresponds to the specified English NE, for simultaneous NE identification and translation. To improve the learning process of unlabeled data using labeled data (Chapelle et al., 2006), the semi-supervi"
W13-2814,J03-1002,0,0.189531,"anu Pal*, Sudip Kumar Naskar† and Sivaji Bandyopadhyay* * Department of Computer Science & Engineering Jadavpur University, Kolkata, India santanu.pal.ju@gmail.com, sivaji_cse_ju@yahoo.com † Department of Computer & System Sciences Visva-Bharati University, Santiniketan, India sudip.naskar@gmail.com tion of each other. Statistical machine translation usually suffers from many-to-many word links which existing statistical word alignment algorithms can not handle well. The unsupervised word alignment models are based on IBM models 1–5 (Brown et al., 1993) and the HMM model (Ney and Vogel, 1996; Och and Ney, 2003). Models 3, 4 and 5 are based on fertility based models which are asymmetric. To improve alignment quality, the Berkeley Aligner is based on the symmetric property by intersecting alignments induced in each translation direction. In the present work, we propose improvement of word alignment quality by combining three word alignment tables (i) GIZA++ alignment (ii) Berkeley Alignment and (iii) rule based alignment. Our objective is to perceive the effectiveness of the Hybrid model in word alignment by improving the quality of translation in the SMT system. In the present work, we have implement"
W13-2814,W03-1502,0,0.0165294,"unk. Ma et. al. (2007) simplified the task of automatic word alignment as several consecutive words together correspond to a single word in the opposite language by using the word aligner itself, i.e., by bootstrapping on its output. A Maximum Entropy model based approach for English—Chinese NE alignment which significantly outperforms IBM Model4 and HMM has been proposed by Feng et al. (2004). They considered 4 features: translation score, transliteration score, source NE and target NE&apos;s co-occurrence score and the distortion score for distinguishing identical NEs in the same sentence. Moore (2003) presented an approach where capitalization cues have been used for identifying NEs on the English side. Statistical techniques are applied to decide which portion of the target language corresponds to the specified English NE, for simultaneous NE identification and translation. To improve the learning process of unlabeled data using labeled data (Chapelle et al., 2006), the semi-supervised learning method is the most useful learning technique. Semi-supervised learning is a broader area of Machine Learning. Researchers have begun to explore semisupervised word alignment models that use both la"
W13-2814,P06-2117,0,0.0375047,"Missing"
W13-2814,N03-1017,0,0.0334238,"e source chunks into the target language using a baseline PB-SMT system and subsequently validating the target chunks using a fuzzy matching technique against the target corpus. All the experiments are carried out after single-tokenizing the multi-word NEs. Our best system provided significant improvements over the baseline as measured by BLEU. 1 Introduction Word alignment is the backbone of PB-SMT system or any data driven approaches to Machine Translation (MT) and it has received a lot of attention in the area of statistical machine translation (SMT) (Brown et al., 1993; Och and Ney, 2003; Koehn et al., 2003). Word alignment is not an end task in itself and is usually used as an intermediate step in SMT. Word alignment is defined as the detection of corresponding alignment of words from parallel sentences that are transla94 Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 94–101, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics The remainder of the paper is organized as follows. Section 2 discusses related work. The proposed hybrid word alignment model is described in Section 3. Section 4 presents the tools and resources used for the va"
W13-2814,P07-2045,0,0.0140433,"alignment from A2 and A3 (using A2∩A3). Step 4: Add additional entries with SA. Figure 1: Establishing alignments through Rule based methods. The extracted chunks on the source side may not have a one to one correspondence with the target side chunks. The alignment validation process is focused on the proper identification of the head words and not between the translated source chunk and target chunk. The matching process has been carried out using a fuzzy 97 3.5 language model trained using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al., 2007) have been used in the present study. Berkeley Semi-supervised Alignment The correctness of the alignments is verified by manually checking the performance of the various alignment system. We start with the combined alignment table which is produced by Algorithm 1. Iinitially, we take a subset of the alignments by manually inspecting from the combined alignment table. Then we train the Barkley supervised aligner with this labeled data. A subset of the unlabeled data from the combined alignment table is tested with the supervised model. The output is then added as additional labeled training da"
W13-2814,W04-3250,0,0.170255,"Missing"
W13-2814,P03-1021,0,0.0597904,"Missing"
W13-2814,W12-0113,1,0.855575,"Missing"
W13-2814,W10-3707,1,0.62711,"engali; (ii) most of the common nouns in Bengali are frequently used as proper nouns; (iii) suffixes (case markers, plural markers, emphasizers, specifiers) get attached to proper names as well in Bengali. Bengali shallow parser 1 has been used to improve the performance of NE identification by considering proper names as NE. Therefore, NER and shallow parser are jointly employed to detect NEs from the Bengali sentences. The source NEs are then transliterated using a modified joint source-channel model (Ekbal et al., 2006) and aligned to their target side equivalents following the approach of Pal et al. (2010). The target side equivalents NEs are transformed into canonical form after omitting their ‗matras‘. Similarly Bengali NEs are also transformed into canonical forms as Bengali NEs may differ in their choice of matras (vowel modifiers). The transliterated NEs are then matched with the corresponding parallel target NEs and finally we align the NEs if match is found. After identification of multiword NEs on both sides, we pre-processed the corpus by replacing space with the underscore character (‗_‘). We have used underscore (‗_‘) instead of hyphen (‗‘) since there already exists some hyphenated"
W13-2814,P02-1040,0,0.0883936,"us respectively, of which 22,273 NEs are unique in English and 22,010 NEs in Bengali. A total of 14,023 NEs have been aligned through transliteration. The experiments have been carried out with various experimental settings: (i) single tokenization of NEs on both sides of the parallel corpus, (ii) using Berkeley Aligner with unsupervised training, (iii) union of the three alignment models: rule based, GIZA++ with GDFA and Berkeley Alignment, (iv) hybridization of the three alignment models and (v) supervised Berkeley Aligner. Eextrinsic evaluation was carried out on the MT quality using BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 3 The EILMT project is funded by the Department of Electronics and Information Technology (DEITY), Ministry of Communications and Information Technology (MCIT), Government of India. 4 http://nlp.stanford.edu/software/lex-parser.shtml 5 http://crfchunker.sourceforge.net/ 6 The IL-ILMT project is funded by the Department of Electronics and Information Technology (DEITY), Ministry of Communications and Information Technology (MCIT), Government of India. 98 Experiment BLEU NIST Baseline system using GIZA++ with GDFA Exp no. 1 10.92 4.13 PB-SMT system using Berkeley Al"
W13-2814,C96-2141,0,0.621907,"Missing"
W13-2814,C08-1125,0,0.0411533,"Missing"
W14-1009,W97-0119,0,0.0495516,"agments using three steps: (i) mining comparable corpora form Wikipedia, (ii) sentence level alignment using two-way TE and a baseline Bengali−English SMT system, and finally (iii) clustering the parallel sentence aligned comparable corpora using textual entailment and then aligning parallel fragments of text by textual entailment and a baseline Bengali−English SMT system. Comparable corpora have been used in many research areas in NLP, especially in machine translation. Several earlier works have studied the use of comparable corpora in machine translation. However, most of these approaches (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Kaji, 2005; Otero, 2007; Saralegui et al., 2008; Gupta et al., 2013) are specifically focused on extracting word translations from comparable corpora. Most of the strategies follow a standard method based on the context vector similarity measure such as finding the target words that have the most similar distributions with a given source word. In most of the cases, a starting list contains the “seed expressions” and this list is required to build the context vectors of the words in both the languages. A bilingu"
W14-1009,P98-1069,0,0.065174,"s: (i) mining comparable corpora form Wikipedia, (ii) sentence level alignment using two-way TE and a baseline Bengali−English SMT system, and finally (iii) clustering the parallel sentence aligned comparable corpora using textual entailment and then aligning parallel fragments of text by textual entailment and a baseline Bengali−English SMT system. Comparable corpora have been used in many research areas in NLP, especially in machine translation. Several earlier works have studied the use of comparable corpora in machine translation. However, most of these approaches (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Kaji, 2005; Otero, 2007; Saralegui et al., 2008; Gupta et al., 2013) are specifically focused on extracting word translations from comparable corpora. Most of the strategies follow a standard method based on the context vector similarity measure such as finding the target words that have the most similar distributions with a given source word. In most of the cases, a starting list contains the “seed expressions” and this list is required to build the context vectors of the words in both the languages. A bilingual dictionary can be"
W14-1009,W13-2509,1,0.34305,"system, and finally (iii) clustering the parallel sentence aligned comparable corpora using textual entailment and then aligning parallel fragments of text by textual entailment and a baseline Bengali−English SMT system. Comparable corpora have been used in many research areas in NLP, especially in machine translation. Several earlier works have studied the use of comparable corpora in machine translation. However, most of these approaches (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Kaji, 2005; Otero, 2007; Saralegui et al., 2008; Gupta et al., 2013) are specifically focused on extracting word translations from comparable corpora. Most of the strategies follow a standard method based on the context vector similarity measure such as finding the target words that have the most similar distributions with a given source word. In most of the cases, a starting list contains the “seed expressions” and this list is required to build the context vectors of the words in both the languages. A bilingual dictionary can be used as a starting list. The bilingual list can also be prepared form parallel corpus using bilingual correlation method (Otero, 20"
W14-1009,W05-0909,0,0.0376082,"Missing"
W14-1009,C02-2020,0,0.157566,"form Wikipedia, (ii) sentence level alignment using two-way TE and a baseline Bengali−English SMT system, and finally (iii) clustering the parallel sentence aligned comparable corpora using textual entailment and then aligning parallel fragments of text by textual entailment and a baseline Bengali−English SMT system. Comparable corpora have been used in many research areas in NLP, especially in machine translation. Several earlier works have studied the use of comparable corpora in machine translation. However, most of these approaches (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Kaji, 2005; Otero, 2007; Saralegui et al., 2008; Gupta et al., 2013) are specifically focused on extracting word translations from comparable corpora. Most of the strategies follow a standard method based on the context vector similarity measure such as finding the target words that have the most similar distributions with a given source word. In most of the cases, a starting list contains the “seed expressions” and this list is required to build the context vectors of the words in both the languages. A bilingual dictionary can be used as a starting list. The bilingual l"
W14-1009,P02-1040,0,0.109035,"Missing"
W14-1009,N10-1045,0,0.0226969,"Missing"
W14-1009,P06-1011,0,0.0283236,"parable corpora. The two-way TE system architecture is described in section 4. Section 5 describes the automatic alignment technique of parallel fragment of texts. Section 6 describes the tools and resources used for this work. The In the NIST shared task on Recognizing Textual Entailment Challenge (RTE), several methods have been proposed to tackle the textual entailment problem. Most of these systems use some form of lexical matching, e.g., n-gram, word similarity, etc. and even simple word overlap. A number of systems represent the texts as parse trees (e.g., syntactic or dependency trees) Munteanu and Marcu (2006) suggested that comparable corpora tend to have parallel data at sub-sentential level. Hence, instead of finding sentence level parallel resource from comparable corpora, in the present work we mainly focus on finding parallel fragments of text. 49 before the actual task. Some of the systems use semantic features (e.g., logical inference, Semantic Role Labelling) for solving the text and hypothesis entailment problem. MacCartney et al. (2006) proposed a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment. Agichtein et al. (2008) pres"
W14-1009,P99-1067,0,0.173641,"able corpora form Wikipedia, (ii) sentence level alignment using two-way TE and a baseline Bengali−English SMT system, and finally (iii) clustering the parallel sentence aligned comparable corpora using textual entailment and then aligning parallel fragments of text by textual entailment and a baseline Bengali−English SMT system. Comparable corpora have been used in many research areas in NLP, especially in machine translation. Several earlier works have studied the use of comparable corpora in machine translation. However, most of these approaches (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Kaji, 2005; Otero, 2007; Saralegui et al., 2008; Gupta et al., 2013) are specifically focused on extracting word translations from comparable corpora. Most of the strategies follow a standard method based on the context vector similarity measure such as finding the target words that have the most similar distributions with a given source word. In most of the cases, a starting list contains the “seed expressions” and this list is required to build the context vectors of the words in both the languages. A bilingual dictionary can be used as a s"
W14-1009,W10-0734,0,0.048008,"Missing"
W14-1009,W09-0404,0,0.0146889,"T. Textual Entailment has many applications in NLP tasks, such as summarization, information extraction, question answering, Introduction Comparable corpora have recently attracted huge interest in natural language processing research. Comparable corpora are now considered as a rich 48 Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 48–57, c Gothenburg, Sweden, April 27, 2014. 2014 Association for Computational Linguistics information retrieval, machine translation, etc. In machine translation, textual entailment can be applied to MT evaluation (Pado et al., 2009). A number of research works have been carried out on cross-lingual Textual entailment using MT (Mehdad et al.,2010; Negri et al., 2010; Neogi et al., 2012). However, to the best of our knowledge, the work presented here is the first attempt towards employing textual entailment for the purpose of extracting parallel text fragments from comparable corpora which in turn are used to improve MT system. experiments and evaluation results are presented in section 7. Section 8 concludes and presents avenues for future work. 2 Related Work We carried out the task of automatic alignment of parallel fra"
W14-1009,N10-1063,0,0.115956,"Santanu Pal1, Partha Pakray2, Sudip Kumar Naskar3 1 Universität Des Saarlandes, Saarbrücken, Germany 2 Computer & Information Science, Norwegian University of Science and Technology, Trondheim, Norway 3 Department of Computer Science & Engineering, Jadavpur University, Kolkata, India 1 santanu.pal@uni-saarland.de, 2 partha.pakray@idi.ntnu.no, 3 sudip.naskar@cse.jdvu.ac.in resource for acquiring parallel resources such as parallel corpus or parallel text fragments,. Parallel text extracted from comparable corpora can take an important role in improving the quality of machine translation (MT) (Smith et al. 2010). Parallel text extracted from comparable corpora are typically added with the training corpus as additional training material which is expected to facilitate better performance of SMT systems specifically for low density language pairs. Abstract Building parallel resources for corpus based machine translation, especially Statistical Machine Translation (SMT), from comparable corpora has recently received wide attention in the field Machine Translation research. In this paper, we propose an automatic approach for extraction of parallel fragments from comparable corpora. The comparable corpora"
W14-1009,P03-1021,0,0.0310102,"Missing"
W14-1009,2006.amta-papers.25,0,0.107752,"Missing"
W14-1009,2007.mtsummit-papers.26,0,0.181303,"E and a baseline Bengali−English SMT system, and finally (iii) clustering the parallel sentence aligned comparable corpora using textual entailment and then aligning parallel fragments of text by textual entailment and a baseline Bengali−English SMT system. Comparable corpora have been used in many research areas in NLP, especially in machine translation. Several earlier works have studied the use of comparable corpora in machine translation. However, most of these approaches (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Kaji, 2005; Otero, 2007; Saralegui et al., 2008; Gupta et al., 2013) are specifically focused on extracting word translations from comparable corpora. Most of the strategies follow a standard method based on the context vector similarity measure such as finding the target words that have the most similar distributions with a given source word. In most of the cases, a starting list contains the “seed expressions” and this list is required to build the context vectors of the words in both the languages. A bilingual dictionary can be used as a starting list. The bilingual list can also be prepared form parallel corpus"
W14-1009,W07-1406,0,0.0305496,"Missing"
W14-1009,C98-1066,0,\N,Missing
W14-1009,S12-1103,1,\N,Missing
W14-1009,P07-2045,0,\N,Missing
W14-1009,N03-1017,0,\N,Missing
W14-5114,W10-3208,1,0.839958,"al. (2004), which significantly outperforms IBM Model 4 and HMM. Fung (1994) presented K-vec, an alternative alignment strategy that starts by estimating the lexicon. Sentiment detection is the task of determining positive or negative sentiment of words, phrases, sentences and documents. The computational approach to sentiment analysis in textual data requires annotated lexicons with polarity tags (Patra et al., 2013). Research has been carried out on building sentiment or emotional corpora in English (Strapparava and Valitutti, 2004; Baccianella et al., 2010; Patra et al., 2013) and Bengali (Das and Bandyopadhyay, 2010; Das and Bandyopadhyay, 2010a). Identifying the sentiment holder is another task closely related to subjectivity detection (Kim and Hovy, 2004). Several methods have been implemented to identify the sentiment holders such as rule based methods (using dependency information) (Kolya et al., 2012) and supervised machine learning methods (Kim and Hovy, 2004; Kolya et al., 2012). To the best of our knowledge, no prior work on improving SMT systems using aligned sentiment expressions, holders and their corresponding objects have been developed yet. There is research on creating sentiment lexica and"
W14-5114,strapparava-valitutti-2004-wordnet,0,0.263172,"Missing"
W14-5114,D08-1014,0,0.0219678,"cy information) (Kolya et al., 2012) and supervised machine learning methods (Kim and Hovy, 2004; Kolya et al., 2012). To the best of our knowledge, no prior work on improving SMT systems using aligned sentiment expressions, holders and their corresponding objects have been developed yet. There is research on creating sentiment lexica and cross-lingual sentiment identification. Automatic translation is a viable alternative for the construction of resources and tools for subjectivity or sentiment analysis in a new resource-constrained language using a resourcerich language as a starting point (Banea et al., 2008). Banea et al., (2008) generated resources for subjectivity annotation in Spanish and Romanian using English corpora. In context of Indian languages, Das et al., 2010 have developed a sentiment lexicon for Bengali Languages using an English to Bengali MT system. Similarly, a Hindi sentiment corpus has been developed using English to Hindi MT system (Balamurali et al., 2010). Hiroshi et al., (2004) developed a high-precision sentiment analysis system with low development cost, by making use of an existing transfer-based MT engine. 3 Dataset In our experiment, an English-Bengali parallel corpus"
W14-5114,P05-1033,0,0.267679,"Missing"
W14-5114,W04-3248,0,0.0173424,"nment template approach for PB-SMT (Och et al., 2004) allows many-tomany relations between words. A model that uses hierarchical phrases based on synchronous grammars is presented in (Chiang et al., 2005). To date there is little research on English-Bengali SMT: PB-SMT systems can be improved (Pal et al., 2011; 2013) by single tokenizing Multiword Expressions (MWEs) on both sides of the parallel corpus. Researches on alignment were mostly developed for MT tasks (Brown, 1991; Gale and 90 Church, 1993). A Maximum Entropy model based approach for English-Chinese NE alignment has been proposed in Feng et al. (2004), which significantly outperforms IBM Model 4 and HMM. Fung (1994) presented K-vec, an alternative alignment strategy that starts by estimating the lexicon. Sentiment detection is the task of determining positive or negative sentiment of words, phrases, sentences and documents. The computational approach to sentiment analysis in textual data requires annotated lexicons with polarity tags (Patra et al., 2013). Research has been carried out on building sentiment or emotional corpora in English (Strapparava and Valitutti, 2004; Baccianella et al., 2010; Patra et al., 2013) and Bengali (Das and Ba"
W14-5114,J04-4002,0,0.224891,"Missing"
W14-5114,C08-1125,0,0.0655339,"Missing"
W14-5114,C04-1071,0,0.0377596,"ation is a viable alternative for the construction of resources and tools for subjectivity or sentiment analysis in a new resource-constrained language using a resourcerich language as a starting point (Banea et al., 2008). Banea et al., (2008) generated resources for subjectivity annotation in Spanish and Romanian using English corpora. In context of Indian languages, Das et al., 2010 have developed a sentiment lexicon for Bengali Languages using an English to Bengali MT system. Similarly, a Hindi sentiment corpus has been developed using English to Hindi MT system (Balamurali et al., 2010). Hiroshi et al., (2004) developed a high-precision sentiment analysis system with low development cost, by making use of an existing transfer-based MT engine. 3 Dataset In our experiment, an English-Bengali parallel corpus containing 23,492 parallel sentences comprising of 488,026 word tokens from the travel and tourism domain has been used. We randomly selected 500 sentences each for the development set and the test set from the initial parallel corpus. The rest of the sentences were used as the training corpus. The training corpus was filtered with the maximum allowable sentence length of 100 words and sentence le"
W14-5114,P02-1040,0,0.107491,"rie Actions) (Grant No. 317471) and the “Development of English to Indian Languages Machine Translation (EILMT) System - Phase II” project funded by Department of Information Technology, Government of India. Our experiments have been carried out in two directions. First we improved the baseline model using the aligned sentiment phrases. Then, we automatically post-edited the translation output by using the sentiment knowledge of the source input test sentence. The evaluation results are reported in Table 1. The evaluation was carried out using well-known automatic MT evaluation metrics: BLEU (Papineni et al., 2002, NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006). In experiment 2, the extracted parallel sentiment phrase alignments are incorporated with the existing baseline phrase table and the resulting model performs better than the baseline system. Experiment 3 shows how post-editing the output of experiment 2 brings about further improvements. 7 Conclusions and Future Research In this paper, we successfully illustrated how sentiment analysis can improve the translation of an English-Bengali PB-SMT system. We have also shown how sentiment knowledge is useful"
W14-5114,N10-1029,0,0.0575798,"Missing"
W14-5114,C94-2178,0,0.372118,"Missing"
W14-5114,J93-2003,0,0.107473,"Missing"
W14-5114,N03-1017,0,0.107016,"carried out using the positional information of sentiment components. The rest of the paper is organized in the following manner. Section 2 briefly elaborates the related work. Section 3 provides an overview of the dataset used in our experiments. The proposed system is described in Section 4 while Section 5 provides the system setup for the various experiments. Section 6 includes the experiments and results obtained. Finally, Section 7 concludes and provides avenues for further work. 2 Related Work SMT systems have undergone considerable improvements over the years. Moreover, PB-SMT models (Koehn et al., 2003) outperform wordbased models. The alignment template approach for PB-SMT (Och et al., 2004) allows many-tomany relations between words. A model that uses hierarchical phrases based on synchronous grammars is presented in (Chiang et al., 2005). To date there is little research on English-Bengali SMT: PB-SMT systems can be improved (Pal et al., 2011; 2013) by single tokenizing Multiword Expressions (MWEs) on both sides of the parallel corpus. Researches on alignment were mostly developed for MT tasks (Brown, 1991; Gale and 90 Church, 1993). A Maximum Entropy model based approach for English-Chin"
W14-5114,W04-3250,0,0.269403,"Missing"
W14-5114,2013.mtsummit-papers.8,1,0.870543,"Missing"
W14-5114,W05-0909,0,0.0587776,"English to Indian Languages Machine Translation (EILMT) System - Phase II” project funded by Department of Information Technology, Government of India. Our experiments have been carried out in two directions. First we improved the baseline model using the aligned sentiment phrases. Then, we automatically post-edited the translation output by using the sentiment knowledge of the source input test sentence. The evaluation results are reported in Table 1. The evaluation was carried out using well-known automatic MT evaluation metrics: BLEU (Papineni et al., 2002, NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006). In experiment 2, the extracted parallel sentiment phrase alignments are incorporated with the existing baseline phrase table and the resulting model performs better than the baseline system. Experiment 3 shows how post-editing the output of experiment 2 brings about further improvements. 7 Conclusions and Future Research In this paper, we successfully illustrated how sentiment analysis can improve the translation of an English-Bengali PB-SMT system. We have also shown how sentiment knowledge is useful for automatic post-editing the MT output. In either case, we"
W14-5114,C04-1200,0,0.0836786,"g the lexicon. Sentiment detection is the task of determining positive or negative sentiment of words, phrases, sentences and documents. The computational approach to sentiment analysis in textual data requires annotated lexicons with polarity tags (Patra et al., 2013). Research has been carried out on building sentiment or emotional corpora in English (Strapparava and Valitutti, 2004; Baccianella et al., 2010; Patra et al., 2013) and Bengali (Das and Bandyopadhyay, 2010; Das and Bandyopadhyay, 2010a). Identifying the sentiment holder is another task closely related to subjectivity detection (Kim and Hovy, 2004). Several methods have been implemented to identify the sentiment holders such as rule based methods (using dependency information) (Kolya et al., 2012) and supervised machine learning methods (Kim and Hovy, 2004; Kolya et al., 2012). To the best of our knowledge, no prior work on improving SMT systems using aligned sentiment expressions, holders and their corresponding objects have been developed yet. There is research on creating sentiment lexica and cross-lingual sentiment identification. Automatic translation is a viable alternative for the construction of resources and tools for subjectiv"
W14-5114,W06-0301,0,0.104449,"Missing"
W14-5114,baccianella-etal-2010-sentiwordnet,0,0.071767,"English-Chinese NE alignment has been proposed in Feng et al. (2004), which significantly outperforms IBM Model 4 and HMM. Fung (1994) presented K-vec, an alternative alignment strategy that starts by estimating the lexicon. Sentiment detection is the task of determining positive or negative sentiment of words, phrases, sentences and documents. The computational approach to sentiment analysis in textual data requires annotated lexicons with polarity tags (Patra et al., 2013). Research has been carried out on building sentiment or emotional corpora in English (Strapparava and Valitutti, 2004; Baccianella et al., 2010; Patra et al., 2013) and Bengali (Das and Bandyopadhyay, 2010; Das and Bandyopadhyay, 2010a). Identifying the sentiment holder is another task closely related to subjectivity detection (Kim and Hovy, 2004). Several methods have been implemented to identify the sentiment holders such as rule based methods (using dependency information) (Kolya et al., 2012) and supervised machine learning methods (Kim and Hovy, 2004; Kolya et al., 2012). To the best of our knowledge, no prior work on improving SMT systems using aligned sentiment expressions, holders and their corresponding objects have been dev"
W14-5114,J93-1003,0,0.359133,"Missing"
W14-5114,J93-1004,0,0.625572,"Missing"
W15-3017,W11-2123,0,0.0206342,"lude some NEs that are already there in the parallel NE list, however they might remain untranslated during decoding. Our system post processed the output by replacing each such OOV NE with the corresponding target language NE after looking up the extracted NE list from the parallel corpus (cf. Section 2.1.2). conditioned on both source and target language. The reordering model was built by calculating the probabilities of the phrase pairs being associated with the given orientation such as monotone (m), swap (s) and discontinuous (d). The 5-gram target language model was trained using KENLM (Heafield, 2011). Parameter tuning was carried out using both k-best MIRA (Cherry and Foster, 2012) and Minimum Error Rate Training (MERT) (Och, 2003) on a held-out development set. After the parameters were tuned, decoding was carried out on the held out testset. Note that all the systems described in Section 2 employ the same PB-SMT settings (apart from the feature weights which are obtained via MERT) as the Baseline system. 2.3 System Combination System Combination is a technique, which combines translation hypotheses (outputs) produced by multiple MT systems. We applied a system combination method on the"
W15-3017,W99-0604,0,0.39787,"Missing"
W15-3017,N03-1017,0,0.0361081,"etup • System 4: System 3 with LM2 . 3.1 Baseline Settings • System 5: System 3 with LM3 . The effectiveness of the present work is demonstrated by using the standard log-linear PB-SMT model as our baseline system. For building the baseline system, we used a maximum phrase length of 7 and a 5-gram language model. The other experimental settings were: SymGIZA++ aligner (Junczys-Dowmunt and Szał, 2012), which is a modified version of GIZA++ word alignment models by updating the symmetrizing models between chosen iterations of the original word alignment training algorithms and phraseextraction (Koehn et al., 2003). The reordering model was trained on hier-mslr-bidirectional (i.e. using both forward and backward models) and • System 6: System 3 with LM4 . System 6 provides the individual best system. System combination (System-7 in Table 1) of the 6 best performing individual systems brings considerable improvements over each of the individual component systems. 5 Conclusions and Future Work A hybrid system (System 6) with NE alignment, EBMT phrases, single-tokenized source MWEs, and MIRA-MERT coupled tuning results in the best performing system. However, confusion 155 Systems Baseline System 1 System 2"
W15-3017,J10-4005,0,0.0383815,"s the best performance of each individual system in a multi-engine pipeline. 1 Introduction In this paper, we present Universit¨at des Saarlandes (UdS) submission (named UdS-Sant) to WMT 2015 using a Hybrid MT framework. We participated in the generic translation shared task for the English-German (EN-DE) language pair. Corpus-based MT (CBMT) has delivered progressively improved quality translations since its inception. There are two main approaches to corpus-based MT – Example Based Machine Translation (EBMT) (Carl and Way, 2003) and Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn, 2010). Out of these two, in terms of large-scale evaluations, SMT is the most successful MT paradigm. However, each approach has its own advantages and disadvantages along with its own methods of applying and acquiring translation knowledge from the bilingual parallel training data. EBMT phrases tend to be more linguistically motivated compared to SMT phrases which essentially operate on n-grams. The knowledge extraction as well as representation process, 152 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 152–157, c Lisboa, Portugal, 17-18 September 2015. 2015 Associati"
W15-3017,N04-1022,0,0.154869,"re tuned, decoding was carried out on the held out testset. Note that all the systems described in Section 2 employ the same PB-SMT settings (apart from the feature weights which are obtained via MERT) as the Baseline system. 2.3 System Combination System Combination is a technique, which combines translation hypotheses (outputs) produced by multiple MT systems. We applied a system combination method on the outputs of the different MT system described earlier. We implement the Minimum Bayes Risk coupled with Confusion Network (MBR-CN) framework described in (Du et al., 2009). The MBR decoder (Kumar and Byrne, 2004) selects the single best hypothesis from amongst the multiple candidate translations by minimising BLEU (Papineni et al., 2002) loss. This single best hypothesis serves as the backbone (also referred to as skeleton) of the confusion network and determines the general word order of the confusion network. A confusion network (Matusov et al., 2006) is built from the backbone while the remaining hypotheses are aligned against the backbone using METEOR (Lavie and Agarwal, 2007) and the TER metric (Snover et al., 2006). The features used to score each arc in the confusion network are word posterior"
W15-3017,E06-1005,0,0.0279832,"iple MT systems. We applied a system combination method on the outputs of the different MT system described earlier. We implement the Minimum Bayes Risk coupled with Confusion Network (MBR-CN) framework described in (Du et al., 2009). The MBR decoder (Kumar and Byrne, 2004) selects the single best hypothesis from amongst the multiple candidate translations by minimising BLEU (Papineni et al., 2002) loss. This single best hypothesis serves as the backbone (also referred to as skeleton) of the confusion network and determines the general word order of the confusion network. A confusion network (Matusov et al., 2006) is built from the backbone while the remaining hypotheses are aligned against the backbone using METEOR (Lavie and Agarwal, 2007) and the TER metric (Snover et al., 2006). The features used to score each arc in the confusion network are word posterior probability, target language model (3-gram, 4-gram), and length penalties. Minimum Error Rate Training (MERT) (Och, 2003) is applied to tune the CN weights (Pal et al., 2014). 3 4 Results and Analysis As described in Section 2.2.1, we developed 16 different systems. Instead of using all these 16 different systems, we apply only the 6 best perfor"
W15-3017,J93-2003,0,0.0424795,"hough, SMT is the most popular MT paradigm, it sometimes fails to deliver sufficient quality in translation output for some languages, since each language has its own difficulties. Multiword Expressions (MWEs) and Named Entities (NEs) offer challenges within a language. MWEs are defined as idiosyncratic interpretations that cross word boundaries (Sag et al., 2002). Named entities on the other hand often consist of more than one word, so that they can be considered as a specific type of MWEs such as noun compounds (Jackendoff, 1997). Traditional approaches to word alignment such as IBM Models (Brown et al., 1993) are unable to tackle NEs and MWEs properly due to their inability to handle many-to-many alignments. In another wellknown word alignment approach, Hidden Markov Model (HMM: (Vogel et al., 1996)), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this alignment problem indirectly. The objective of the present work is threefold. Firstly, we would like to determine how treatment of MWEs as a single unit affects the overall MT quality (Pal et al., 2010; Pal et al., 2011). Secondly, whether a"
W15-3017,P03-1021,0,0.503229,"ssed the output by replacing each such OOV NE with the corresponding target language NE after looking up the extracted NE list from the parallel corpus (cf. Section 2.1.2). conditioned on both source and target language. The reordering model was built by calculating the probabilities of the phrase pairs being associated with the given orientation such as monotone (m), swap (s) and discontinuous (d). The 5-gram target language model was trained using KENLM (Heafield, 2011). Parameter tuning was carried out using both k-best MIRA (Cherry and Foster, 2012) and Minimum Error Rate Training (MERT) (Och, 2003) on a held-out development set. After the parameters were tuned, decoding was carried out on the held out testset. Note that all the systems described in Section 2 employ the same PB-SMT settings (apart from the feature weights which are obtained via MERT) as the Baseline system. 2.3 System Combination System Combination is a technique, which combines translation hypotheses (outputs) produced by multiple MT systems. We applied a system combination method on the outputs of the different MT system described earlier. We implement the Minimum Bayes Risk coupled with Confusion Network (MBR-CN) fram"
W15-3017,N12-1047,0,0.0184546,"might remain untranslated during decoding. Our system post processed the output by replacing each such OOV NE with the corresponding target language NE after looking up the extracted NE list from the parallel corpus (cf. Section 2.1.2). conditioned on both source and target language. The reordering model was built by calculating the probabilities of the phrase pairs being associated with the given orientation such as monotone (m), swap (s) and discontinuous (d). The 5-gram target language model was trained using KENLM (Heafield, 2011). Parameter tuning was carried out using both k-best MIRA (Cherry and Foster, 2012) and Minimum Error Rate Training (MERT) (Och, 2003) on a held-out development set. After the parameters were tuned, decoding was carried out on the held out testset. Note that all the systems described in Section 2 employ the same PB-SMT settings (apart from the feature weights which are obtained via MERT) as the Baseline system. 2.3 System Combination System Combination is a technique, which combines translation hypotheses (outputs) produced by multiple MT systems. We applied a system combination method on the outputs of the different MT system described earlier. We implement the Minimum Baye"
W15-3017,W10-3707,1,0.907834,"o word alignment such as IBM Models (Brown et al., 1993) are unable to tackle NEs and MWEs properly due to their inability to handle many-to-many alignments. In another wellknown word alignment approach, Hidden Markov Model (HMM: (Vogel et al., 1996)), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this alignment problem indirectly. The objective of the present work is threefold. Firstly, we would like to determine how treatment of MWEs as a single unit affects the overall MT quality (Pal et al., 2010; Pal et al., 2011). Secondly, whether a prior automatic NE aligned parallel corpus as well as example based parallel phrases can bring about any further improvement on top of that. And finally, whether system combination can provide any additional advantage in terms of translation quality and performance. The remainder of the paper is organised as follows. Section 2 details the components of our system, in particular named entity extraction, translation memory, and EBMT, followed by description of 3 types of Hybrid systems and the system combination module. In Section 3, we outline the comple"
W15-3017,2011.mtsummit-papers.23,1,0.815387,"uch as IBM Models (Brown et al., 1993) are unable to tackle NEs and MWEs properly due to their inability to handle many-to-many alignments. In another wellknown word alignment approach, Hidden Markov Model (HMM: (Vogel et al., 1996)), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this alignment problem indirectly. The objective of the present work is threefold. Firstly, we would like to determine how treatment of MWEs as a single unit affects the overall MT quality (Pal et al., 2010; Pal et al., 2011). Secondly, whether a prior automatic NE aligned parallel corpus as well as example based parallel phrases can bring about any further improvement on top of that. And finally, whether system combination can provide any additional advantage in terms of translation quality and performance. The remainder of the paper is organised as follows. Section 2 details the components of our system, in particular named entity extraction, translation memory, and EBMT, followed by description of 3 types of Hybrid systems and the system combination module. In Section 3, we outline the complete experimental set"
W15-3017,P02-1040,0,0.0921099,"-SMT settings (apart from the feature weights which are obtained via MERT) as the Baseline system. 2.3 System Combination System Combination is a technique, which combines translation hypotheses (outputs) produced by multiple MT systems. We applied a system combination method on the outputs of the different MT system described earlier. We implement the Minimum Bayes Risk coupled with Confusion Network (MBR-CN) framework described in (Du et al., 2009). The MBR decoder (Kumar and Byrne, 2004) selects the single best hypothesis from amongst the multiple candidate translations by minimising BLEU (Papineni et al., 2002) loss. This single best hypothesis serves as the backbone (also referred to as skeleton) of the confusion network and determines the general word order of the confusion network. A confusion network (Matusov et al., 2006) is built from the backbone while the remaining hypotheses are aligned against the backbone using METEOR (Lavie and Agarwal, 2007) and the TER metric (Snover et al., 2006). The features used to score each arc in the confusion network are word posterior probability, target language model (3-gram, 4-gram), and length penalties. Minimum Error Rate Training (MERT) (Och, 2003) is ap"
W15-3017,2006.amta-papers.25,0,0.0454059,"ion Network (MBR-CN) framework described in (Du et al., 2009). The MBR decoder (Kumar and Byrne, 2004) selects the single best hypothesis from amongst the multiple candidate translations by minimising BLEU (Papineni et al., 2002) loss. This single best hypothesis serves as the backbone (also referred to as skeleton) of the confusion network and determines the general word order of the confusion network. A confusion network (Matusov et al., 2006) is built from the backbone while the remaining hypotheses are aligned against the backbone using METEOR (Lavie and Agarwal, 2007) and the TER metric (Snover et al., 2006). The features used to score each arc in the confusion network are word posterior probability, target language model (3-gram, 4-gram), and length penalties. Minimum Error Rate Training (MERT) (Och, 2003) is applied to tune the CN weights (Pal et al., 2014). 3 4 Results and Analysis As described in Section 2.2.1, we developed 16 different systems. Instead of using all these 16 different systems, we apply only the 6 best performing systems for system combination. Performance is measured on the devset. Table 1 reports the final evaluation results obtained on the test dataset. The best 6 systems a"
W15-3017,W14-3323,1,0.798583,"ve followed Point-wise Mutual InforOur system is designed with three basic components: (i) preprocessing, (ii) hybrid systems and (iii) system combination. 2.1 Preprocessing Data pre-processing plays a very crucial part in any data-driven approach. We carried out preprocessing in two steps: • Cleaning and clustering sentences based on sentence length. • Effective preprocessing of data in the form of explicit alignment of bilingual terminology (viz. NEs and MWEs). The preprocessing has been shown (cf. Section 2.1.2) to improve the output quality of the baseline PB-SMT system (Pal et al., 2013; Tan and Pal, 2014). 2.1.1 Corpus cleaning We utilized all the parallel training data provided by the WMT 2015 shared task organizers for English–German translation. The training data include Europarl, News Commentary and Common Crawl. The provided corpus is noisy and contains some non-German as well as non-English words and sentences. Therefore, we applied a Language Identifier (Shuyo, 2010) on both bilingual English–German parallel data and monolingual German corpora. We discarded those parallel sentences from the bilingual training data which were detected as belonging to some different language by the langua"
W15-3017,C96-2141,0,0.420797,"ressions (MWEs) and Named Entities (NEs) offer challenges within a language. MWEs are defined as idiosyncratic interpretations that cross word boundaries (Sag et al., 2002). Named entities on the other hand often consist of more than one word, so that they can be considered as a specific type of MWEs such as noun compounds (Jackendoff, 1997). Traditional approaches to word alignment such as IBM Models (Brown et al., 1993) are unable to tackle NEs and MWEs properly due to their inability to handle many-to-many alignments. In another wellknown word alignment approach, Hidden Markov Model (HMM: (Vogel et al., 1996)), the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. We address this alignment problem indirectly. The objective of the present work is threefold. Firstly, we would like to determine how treatment of MWEs as a single unit affects the overall MT quality (Pal et al., 2010; Pal et al., 2011). Secondly, whether a prior automatic NE aligned parallel corpus as well as example based parallel phrases can bring about any further improvement on top of that. And finally, whether system combination can provide"
W15-3017,W07-0734,0,\N,Missing
W15-3017,W09-0416,0,\N,Missing
W15-3017,W10-1720,1,\N,Missing
W15-3026,W06-1607,0,0.072029,"ning algorithms (described in Section 3) and the phraseextraction (Koehn et al., 2003). The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslrbidirectional) (Galley and Manning, 2008) method and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimised with k-best MIRA (Cherry and Foster, 2012) on a held out development set. After the parameters where sk1 = s1 . . . sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (ˆ ek1 = eˆ1 . . . eˆk ) and (fˆ1k = fˆ1 . . . fˆk ) such that (we set i0 = 0) in equation (6): k−1 11,272 1,000 1,817 EN 238,335 21,617 38,244 Table 1: Statistics. SEN: Sentences, EN: English and ES: Spanish λLM logP (eL 1) ∀1 ≤ k ≤ K, sk = (ik , bk , jk ), eˆk = ei +1 ...ei , fˆk ="
W15-3026,D08-1089,0,0.0720062,"nted with various maximum phrase lengths for the translation model and n–gram settings for the language model. We found that using a maximum phrase length of 7 for the translation model and a 5-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with hybrid word alignment training algorithms (described in Section 3) and the phraseextraction (Koehn et al., 2003). The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslrbidirectional) (Galley and Manning, 2008) method and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimised with k-best MIRA (Cherry and Foster, 2012) on a held out development set. After the parameters where sk1 = s1 . . . sk denot"
W15-3026,W11-2123,0,0.102511,"rase length of 7 for the translation model and a 5-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with hybrid word alignment training algorithms (described in Section 3) and the phraseextraction (Koehn et al., 2003). The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslrbidirectional) (Galley and Manning, 2008) method and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimised with k-best MIRA (Cherry and Foster, 2012) on a held out development set. After the parameters where sk1 = s1 . . . sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (ˆ ek1 = eˆ1 . . . eˆk ) and (fˆ1k ="
W15-3026,N03-1017,0,0.00755213,"tings The effectiveness of the present work is demonstrated by using the standard log-linear PBSMT model. For building our SAPE system, we experimented with various maximum phrase lengths for the translation model and n–gram settings for the language model. We found that using a maximum phrase length of 7 for the translation model and a 5-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with hybrid word alignment training algorithms (described in Section 3) and the phraseextraction (Koehn et al., 2003). The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslrbidirectional) (Galley and Manning, 2008) method and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training"
W15-3026,J10-4005,0,0.0634225,"stemmed using the Porter stemmer • WN synonymy: maps if they are considered synonyms in WordNet If multiple alignments exist, METEOR selects the alignment for which the word order in the two strings is most similar (i.e. having fewest crossing alignment links). The final alignment is produced between H and R as the union of all stage alignments (e.g. exact, Porter stemming and WN synonymy). 3.2.3 Hybridization The hybrid word alignment method combines two different kinds of word alignment: the statistical alignment tools such as GIZA++ word alignment with grow-diag-final-and (GDFA) heuristic (Koehn, 2010) and SymGiza++ (JunczysDowmunt and Szał, 2012) and the Berkeley aligner (Liang et al., 2006), as well as edit distance-based aligners (Snover et al., 2006; Lavie and Agarwal, 2007). In order to combine these different word alignment tables (Pal et al., 2013) we used a mathematical union method. For the union method, we hypothesise that all alignments are correct. Duplicate entries are removed. Edit Distance-Based Word Alignment We use two different kind of edit distance based word aligners, where alignment is based on TER (Translation Edit Rate) and the METEOR word aligner. TER (Snover et al.,"
W15-3026,N12-1047,0,0.0310726,"rarchical, monotone, swap, left to right bidirectional (hier-mslrbidirectional) (Galley and Manning, 2008) method and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimised with k-best MIRA (Cherry and Foster, 2012) on a held out development set. After the parameters where sk1 = s1 . . . sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (ˆ ek1 = eˆ1 . . . eˆk ) and (fˆ1k = fˆ1 . . . fˆk ) such that (we set i0 = 0) in equation (6): k−1 11,272 1,000 1,817 EN 238,335 21,617 38,244 Table 1: Statistics. SEN: Sentences, EN: English and ES: Spanish λLM logP (eL 1) ∀1 ≤ k ≤ K, sk = (ik , bk , jk ), eˆk = ei +1 ...ei , fˆk = fb ...fj Data Table 1 presents the statistics of the training, development and test sets released for the English– Spanish SAPE Task orga"
W15-3026,N09-2055,0,0.121014,"Missing"
W15-3026,W07-0734,0,0.742474,"entence aligned training data provided by the organizers of the WMT2015 APE task. The training data consist of 11,272 parallel segments of English to Spanish MT translations as well as the post-edited translations of the MT output. The English source text, 217 the machine translated Spanish output and the corresponding post-edited version contain 238,335, 257,644 and 257,881 tokens respectively. The preprocessing of the training corpus was carried out first by stemming the Spanish MT output and the PE data using Freeling (Padr´o and Stanilovsky, 2012). 3.2 3.2.1 T ER(H, R) = METEOR Alignment (Lavie and Agarwal, 2007) is also an automatic MT evaluation metric which provides an alignment between hypothesis (here the MT output) and reference (here the PE translation). Given a pair of strings such as H and R to be compared, METEOR initially establishes a word alignment between them. The alignment is provided by a mapping method between the words in the hypothesis H an reference R transaltion, which is built incrementally by the following sequence of word-mapping modules: Hybrid Word Alignment Statistical Word Alignment GIZA++ (Och and Ney, 2003) is a statistical word alignment tool which implements maximum li"
W15-3026,N10-1062,0,0.0224964,"ting the MT decoder decide whether the errors should be corrected and about the method of correcting it. Parton et al. (2012) evaluated their approach with human evaluators and found that the adequacy of post-edited MT output improved both for rule-based and feedback APE. In terms of fluency the human evaluation has shown that adequacy increase in feedback APE is related to fluency but not for rule-based APE. Denkowski (2015) has developed a method for integrating in real time post-edited MT output into a translation model, by extracting for each input sentence a grammar. The method, based on Levenberg et al. (2010) and Lopez (2008), allows the indexing of the the source and post-edited MT output, as well as the union of the already existing sentence pairs with the new post-edited data. The system can also remember the rules that are consistent with the post-edited data. This way, rules learned from human corections can be preferred. The experiments Denkowski (2015) ran on from English into and out of Spanish and Arabic data show that the process of translating with an adaptive grammar improves performance on postediting tasks. ing enough to perform the task (Vela and van Genabith, 2015) . The aim of aut"
W15-3026,N07-1064,0,0.424423,"enkowski (2015) ran on from English into and out of Spanish and Arabic data show that the process of translating with an adaptive grammar improves performance on postediting tasks. ing enough to perform the task (Vela and van Genabith, 2015) . The aim of automatic post-editing (APE) is to improve the output of MT by post-processing it. One of the first approaches was the one introduced by Chen and Chen (1997) who proposed a combination of rule-based MT (RBMT) and statistical MT (SMT) systems aiming at merging the positive properties of each system type for a better machine translation output. Simard et al. (2007a) and Simard et al. (2007b) have shown how a PBSMT system can be used for automatic post-editing of an RBMT system for translations from English to French and French to English. Because RBMT systems tend to produce repetitive errors, they train a SMT system to correct errors, with the aim of reducing the postediting effort. The SMT system trains on the output of the RBMT system as the source language and the reference human translations as the target language. The evaluation of their system shows that the post-edited output had a better quality than the output of the RBMT system as well as th"
W15-3026,N06-1014,0,0.353606,"in WordNet If multiple alignments exist, METEOR selects the alignment for which the word order in the two strings is most similar (i.e. having fewest crossing alignment links). The final alignment is produced between H and R as the union of all stage alignments (e.g. exact, Porter stemming and WN synonymy). 3.2.3 Hybridization The hybrid word alignment method combines two different kinds of word alignment: the statistical alignment tools such as GIZA++ word alignment with grow-diag-final-and (GDFA) heuristic (Koehn, 2010) and SymGiza++ (JunczysDowmunt and Szał, 2012) and the Berkeley aligner (Liang et al., 2006), as well as edit distance-based aligners (Snover et al., 2006; Lavie and Agarwal, 2007). In order to combine these different word alignment tables (Pal et al., 2013) we used a mathematical union method. For the union method, we hypothesise that all alignments are correct. Duplicate entries are removed. Edit Distance-Based Word Alignment We use two different kind of edit distance based word aligners, where alignment is based on TER (Translation Edit Rate) and the METEOR word aligner. TER (Snover et al., 2006) was developed for automatic evaluation of MT outputs. TER can align two strings such"
W15-3026,W07-0728,0,0.135807,"enkowski (2015) ran on from English into and out of Spanish and Arabic data show that the process of translating with an adaptive grammar improves performance on postediting tasks. ing enough to perform the task (Vela and van Genabith, 2015) . The aim of automatic post-editing (APE) is to improve the output of MT by post-processing it. One of the first approaches was the one introduced by Chen and Chen (1997) who proposed a combination of rule-based MT (RBMT) and statistical MT (SMT) systems aiming at merging the positive properties of each system type for a better machine translation output. Simard et al. (2007a) and Simard et al. (2007b) have shown how a PBSMT system can be used for automatic post-editing of an RBMT system for translations from English to French and French to English. Because RBMT systems tend to produce repetitive errors, they train a SMT system to correct errors, with the aim of reducing the postediting effort. The SMT system trains on the output of the RBMT system as the source language and the reference human translations as the target language. The evaluation of their system shows that the post-edited output had a better quality than the output of the RBMT system as well as th"
W15-3026,2006.amta-papers.25,0,0.11155,"ignment for which the word order in the two strings is most similar (i.e. having fewest crossing alignment links). The final alignment is produced between H and R as the union of all stage alignments (e.g. exact, Porter stemming and WN synonymy). 3.2.3 Hybridization The hybrid word alignment method combines two different kinds of word alignment: the statistical alignment tools such as GIZA++ word alignment with grow-diag-final-and (GDFA) heuristic (Koehn, 2010) and SymGiza++ (JunczysDowmunt and Szał, 2012) and the Berkeley aligner (Liang et al., 2006), as well as edit distance-based aligners (Snover et al., 2006; Lavie and Agarwal, 2007). In order to combine these different word alignment tables (Pal et al., 2013) we used a mathematical union method. For the union method, we hypothesise that all alignments are correct. Duplicate entries are removed. Edit Distance-Based Word Alignment We use two different kind of edit distance based word aligners, where alignment is based on TER (Translation Edit Rate) and the METEOR word aligner. TER (Snover et al., 2006) was developed for automatic evaluation of MT outputs. TER can align two strings such as the reference (in this case the PE translation) and the hyp"
W15-3026,J03-1002,0,0.0404429,"anilovsky, 2012). 3.2 3.2.1 T ER(H, R) = METEOR Alignment (Lavie and Agarwal, 2007) is also an automatic MT evaluation metric which provides an alignment between hypothesis (here the MT output) and reference (here the PE translation). Given a pair of strings such as H and R to be compared, METEOR initially establishes a word alignment between them. The alignment is provided by a mapping method between the words in the hypothesis H an reference R transaltion, which is built incrementally by the following sequence of word-mapping modules: Hybrid Word Alignment Statistical Word Alignment GIZA++ (Och and Ney, 2003) is a statistical word alignment tool which implements maximum likelihood estimators for all the IBM-1 to IBM-5 models, a HMM alignment model as well as the IBM-6 model covering many to many alignments. GIZA++ facilitates fast development of statistical machine translation (SMT) systems. Like GIZA++, the Berkley Aligner (Liang et al., 2006) is also used to align words across sentence pairs. The Berkeley word aligner uses an extension of Cross Expectation Maximization and is jointly trained with HMM models. We use a third statistical word aligner called SymGiza++ (JunczysDowmunt and Szał, 2012)"
W15-3026,P03-1021,0,0.126312,"ordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslrbidirectional) (Galley and Manning, 2008) method and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the GoodTuring smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) optimised with k-best MIRA (Cherry and Foster, 2012) on a held out development set. After the parameters where sk1 = s1 . . . sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (ˆ ek1 = eˆ1 . . . eˆk ) and (fˆ1k = fˆ1 . . . fˆk ) such that (we set i0 = 0) in equation (6): k−1 11,272 1,000 1,817 EN 238,335 21,617 38,244 Table 1: Statistics. SEN: Sentences, EN: English and ES: Spanish λLM logP (eL 1) ∀1 ≤ k ≤ K, sk = (ik , bk , jk ), eˆk = ei +1 ...ei , fˆk = fb ...fj Data Table 1 presents the statistics of the training, development and test"
W15-3026,W15-4921,1,0.884399,"Missing"
W15-3026,padro-stanilovsky-2012-freeling,0,0.0343196,"Missing"
W15-3026,W14-0314,1,0.613986,"ection of repetitive errors in the MT output, various automatic or semi-automatic post-processing or automatic PE techniques have been developed. Although MT output needs to be post-edited by humans to produce publishable quality translation (Roturier, 2009; TAUS/CNGL Report, 2010), it is faster and cheaper to post-edit MT output than to perform human translation from scratch. In some cases, recent studies have shown that the quality of MT output plus PE can exceed the quality of human translation (Fiederer and O’Brien, 2009; Koehn, 2009; De Palma and Kelly, 2009) as well as the productivity (Zampieri and Vela, 2014). Aimed at cost-effective and timesaving use of MT, the PE process needs to be further optimised (TAUS/CNGL Report, 2010). Post-editing can be also used as a MT evaluation method, implying at least source and target language skills, different from ranking, that does nor require specific skills, a homogeneous group of evaluators beIntroduction In this paper, we present the submission of Saarland University (USAAR) to the WMT2015 APE task. The system combines a hybrid word alignment system implementation with a monolingual PBSMT for the language pair English-Spanish (EN-ES), translating from Eng"
W15-3026,W13-2814,1,0.716633,"links). The final alignment is produced between H and R as the union of all stage alignments (e.g. exact, Porter stemming and WN synonymy). 3.2.3 Hybridization The hybrid word alignment method combines two different kinds of word alignment: the statistical alignment tools such as GIZA++ word alignment with grow-diag-final-and (GDFA) heuristic (Koehn, 2010) and SymGiza++ (JunczysDowmunt and Szał, 2012) and the Berkeley aligner (Liang et al., 2006), as well as edit distance-based aligners (Snover et al., 2006; Lavie and Agarwal, 2007). In order to combine these different word alignment tables (Pal et al., 2013) we used a mathematical union method. For the union method, we hypothesise that all alignments are correct. Duplicate entries are removed. Edit Distance-Based Word Alignment We use two different kind of edit distance based word aligners, where alignment is based on TER (Translation Edit Rate) and the METEOR word aligner. TER (Snover et al., 2006) was developed for automatic evaluation of MT outputs. TER can align two strings such as the reference (in this case the PE translation) and the hypothesis (MT output). In the our work, the reference string has been chosen to be the confusion network s"
W15-3026,P02-1040,0,0.100602,"single where h phrase-pair. It thus follows (8): M X m=1 λm K X ˆ m (fˆk , eˆk , sk ) = h k=1 ˆ= where h K X k=1 PK k=1 Tokens ES-MT 257,644 23,213 40,925 ES-PE 257,881 23,098 – Experimental Settings The effectiveness of the present work is demonstrated by using the standard log-linear PBSMT model. For building our SAPE system, we experimented with various maximum phrase lengths for the translation model and n–gram settings for the language model. We found that using a maximum phrase length of 7 for the translation model and a 5-gram language model produces the best results in terms of BLEU (Papineni et al., 2002) scores for our SAPE model. The other experimental settings were concerned with hybrid word alignment training algorithms (described in Section 3) and the phraseextraction (Koehn et al., 2003). The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslrbidirectional) (Galley and Manning, 2008) method and conditioned on both source and target language. The 5-gram target language model was trained using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e. 1). To allevia"
W15-3026,C10-2109,0,0.0179871,"ng raw MT output, before performing human post-editing on it. The objective is to decreases the amount of errors produced by the MT systems, achieving in the end a productivity increase in the translation process. 216 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 216–221, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. methodologies, a rule-abased APE and a feedback APE. The rule-based APE performs either insertions or replacement to address an identified error. The feedback APE, an approach similar to the one proposed by Parton and McKeown (2010), passes the possible correction to the MT system, letting the MT decoder decide whether the errors should be corrected and about the method of correcting it. Parton et al. (2012) evaluated their approach with human evaluators and found that the adequacy of post-edited MT output improved both for rule-based and feedback APE. In terms of fluency the human evaluation has shown that adequacy increase in feedback APE is related to fluency but not for rule-based APE. Denkowski (2015) has developed a method for integrating in real time post-edited MT output into a translation model, by extracting fo"
W15-3026,2012.eamt-1.34,0,0.114117,"Missing"
W15-3026,W12-3146,0,0.195552,"Missing"
W15-3026,W11-2152,0,\N,Missing
W15-3026,O96-2005,0,\N,Missing
W15-3026,2015.eamt-1.22,1,\N,Missing
W15-5206,2013.mtsummit-wptp.13,0,0.122457,"2011; Gupta and Orăsan, 2014; Gupta et al., 2015), as well as syntax (Clark, 2002; Gotti et al., 2005) in this process. Another recent direction that research in CAT tools is taking is the integration of both TM and machine translation (MT) output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-ofthe-art MT systems, MT output is no longer used just for gisting, it is now being used in real-world translation projects. Taking advantage of these improvements, CAT tools such as MateCat1 , have been integrating MT output along TMs in the list of suitable suggestions (Cettolo et al., 2013). In this paper we are concerned both with retrieval and with the post-editing interface of TMs. We present a new CAT tool called CATaLog2 , which is language pair independent and allows users to upload their own memories in This paper explores a new TM-based CAT tool entitled CATaLog. New features have been integrated into the tool which aim to improve post-editing both in terms of performance and productivity. One of the new features of CATaLog is a color coding scheme that is based on the similarity between a particular input sentence and the segments retrieved from the TM. This color codin"
W15-5206,2012.amta-papers.22,0,0.124202,"presented using English - Bengali data. 2 ments and mismatched portions are translated by an SMT system to ﬁll in the gaps. Even though this paper describes work in progress, our aim is to develop a tool that is as intuitive as possible for end users and this should have direct impact on translators’ performance and productivity. In the recent years, several productive studies were also carried out measuring diﬀerent aspects of the translation process such as cognitive load, eﬀort, time, quality as well as other criteria (Bowker, 2005; O’Brien, 2006; Guerberof, 2009; Plitt and Masselot, 2010; Federico et al., 2012; Guerberof, 2012; Zampieri and Vela, 2014). User studies were taken into account when developing CATaLog as our main motivation is to improve the translation workﬂow. In this paper, however, we do not yet explore the impact of our tool in the translation process, because the functionalities required for this kind of study are currently under development in CATaLog. Future work aims to investigate the impact of the new features we are proposing on the translator’s work. Related Work CAT tools have become very popular in the translation and localization industries in the last two decades. They"
W15-5206,2014.eamt-1.2,0,0.105775,"retrieved segments suggested by the CAT tool or translating new segments from scratch. This process is done iteratively and every new translation increases the size of the translation memory making it both more useful and more helpful to future translations. Although in the ﬁrst place it might sound very simplistic, the process of matching source and target segments, and retrieving translated segments from the TM is far from trivial. To improve the retrieval engines, researchers have been working on diﬀerent ways of incorporating semantic knowledge, such as paraphrasing (Utiyama et al., 2011; Gupta and Orăsan, 2014; Gupta et al., 2015), as well as syntax (Clark, 2002; Gotti et al., 2005) in this process. Another recent direction that research in CAT tools is taking is the integration of both TM and machine translation (MT) output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-ofthe-art MT systems, MT output is no longer used just for gisting, it is now being used in real-world translation projects. Taking advantage of these improvements, CAT tools such as MateCat1 , have been integrating MT output along TMs in the list of suitable suggestions (Cettolo et al., 2013). In th"
W15-5206,2009.mtsummit-papers.14,0,0.0410983,"achines) to decide which output (TM or MT) is most suitable to use for post-editing. Work on integrating MT with TM has also been done to make TM output more suitable for post-editing diminishing translators’ eﬀort (Kanavos and Kartsaklis, 2010). Another study presented a Dynamic Translation Memory which identiﬁes the longest common subsequence in the the closest matching source segment, identiﬁes the corresponding subsequence in its translation, and dynamically adds this source-target phrase pair to the phrase table of a phrasebased ststistical MT (PB-SMT) system (Biçici and Dymetman, 2008). Simard and Isabelle (2009) reported a work on integration of PB-SMT with TM technology in a CAT environment in which the PBSMT system exploits the most similar matches by making use of TM-based feature functions. Koehn and Senellart (2010) reported another MT-TM integration strategy where TM is used to retrieve matching source seg3 System Description We demonstrate the functionalities and features of CATaLog in an English - Bengali translation task. The TM database consists of English sentences taken from BTEC3 (Basic Travel Expression Corpus) corpus and their Bengali translations4 . Unseen input or test segments are p"
W15-5206,W15-4905,1,0.742271,"Missing"
W15-5206,W09-0441,0,0.0370839,"is used to retrieve matching source seg3 System Description We demonstrate the functionalities and features of CATaLog in an English - Bengali translation task. The TM database consists of English sentences taken from BTEC3 (Basic Travel Expression Corpus) corpus and their Bengali translations4 . Unseen input or test segments are provided to the post-editing tool and the tool matches each of the input segments to the most similar segments contained in the TM. TM segments are then ranked according their the similarity to the test sentence using the popular Translation Error Rate (TER) metric (Snover et al., 2009). The top 5 most similar segments are chosen and presented to the translator ordered by their similarity. One very important aspect of computing similarity is alignment. Each test (input) segment in the source language (SL) is aligned with the reference SL sentences in the TM and each SL sentence in the TM is aligned to its respective translation. From these two sets 3 BTEC corpus contains tourism-related sentences similar to those that are usually found in phrase books for tourists going abroad 4 Work in progress. 37 of alignments we apply a method to ﬁnd out which parts of the translation ar"
W15-5206,P10-1064,1,0.924209,"Missing"
W15-5206,2011.eamt-1.12,0,0.0128172,"udy are currently under development in CATaLog. Future work aims to investigate the impact of the new features we are proposing on the translator’s work. Related Work CAT tools have become very popular in the translation and localization industries in the last two decades. They are used by many language service providers, freelance translators to improve translation quality and to increase translator’s productivity (Lagoudaki, 2008). Although the work presented in this paper focuses on TM, it should also be noted that there were many studies on MT post-editing published in the last few years (Specia, 2011; Green et al., 2013; Green, 2014) and as mentioned in the last section, one of the recent trends is the development of hybrid systems that are able to combine MT with TM output. Therefore work on MT post-editing presents signiﬁcant overlap with state-of-the-art CAT tools and to what we propose in this paper. Substantial work have also been carried out on improving translation recommendation systems which recommends post-editors either to use TM output or MT output (He et al., 2010). To achieve good performance with this kind of systems, researchers typically train a binary classiﬁer (e.g., Su"
W15-5206,2010.jec-1.3,0,0.508262,"lthough in the ﬁrst place it might sound very simplistic, the process of matching source and target segments, and retrieving translated segments from the TM is far from trivial. To improve the retrieval engines, researchers have been working on diﬀerent ways of incorporating semantic knowledge, such as paraphrasing (Utiyama et al., 2011; Gupta and Orăsan, 2014; Gupta et al., 2015), as well as syntax (Clark, 2002; Gotti et al., 2005) in this process. Another recent direction that research in CAT tools is taking is the integration of both TM and machine translation (MT) output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-ofthe-art MT systems, MT output is no longer used just for gisting, it is now being used in real-world translation projects. Taking advantage of these improvements, CAT tools such as MateCat1 , have been integrating MT output along TMs in the list of suitable suggestions (Cettolo et al., 2013). In this paper we are concerned both with retrieval and with the post-editing interface of TMs. We present a new CAT tool called CATaLog2 , which is language pair independent and allows users to upload their own memories in This paper explores a new TM-based CAT tool entit"
W15-5206,2011.mtsummit-papers.37,0,0.259019,"editors by correcting retrieved segments suggested by the CAT tool or translating new segments from scratch. This process is done iteratively and every new translation increases the size of the translation memory making it both more useful and more helpful to future translations. Although in the ﬁrst place it might sound very simplistic, the process of matching source and target segments, and retrieving translated segments from the TM is far from trivial. To improve the retrieval engines, researchers have been working on diﬀerent ways of incorporating semantic knowledge, such as paraphrasing (Utiyama et al., 2011; Gupta and Orăsan, 2014; Gupta et al., 2015), as well as syntax (Clark, 2002; Gotti et al., 2005) in this process. Another recent direction that research in CAT tools is taking is the integration of both TM and machine translation (MT) output (He et al., 2010; Kanavos and Kartsaklis, 2010). With the improvement of state-ofthe-art MT systems, MT output is no longer used just for gisting, it is now being used in real-world translation projects. Taking advantage of these improvements, CAT tools such as MateCat1 , have been integrating MT output along TMs in the list of suitable suggestions (Cett"
W15-5206,W14-0314,1,0.778874,"2 ments and mismatched portions are translated by an SMT system to ﬁll in the gaps. Even though this paper describes work in progress, our aim is to develop a tool that is as intuitive as possible for end users and this should have direct impact on translators’ performance and productivity. In the recent years, several productive studies were also carried out measuring diﬀerent aspects of the translation process such as cognitive load, eﬀort, time, quality as well as other criteria (Bowker, 2005; O’Brien, 2006; Guerberof, 2009; Plitt and Masselot, 2010; Federico et al., 2012; Guerberof, 2012; Zampieri and Vela, 2014). User studies were taken into account when developing CATaLog as our main motivation is to improve the translation workﬂow. In this paper, however, we do not yet explore the impact of our tool in the translation process, because the functionalities required for this kind of study are currently under development in CATaLog. Future work aims to investigate the impact of the new features we are proposing on the translator’s work. Related Work CAT tools have become very popular in the translation and localization industries in the last two decades. They are used by many language service providers"
W15-5206,2010.jec-1.4,0,0.0590105,"t (Kanavos and Kartsaklis, 2010). Another study presented a Dynamic Translation Memory which identiﬁes the longest common subsequence in the the closest matching source segment, identiﬁes the corresponding subsequence in its translation, and dynamically adds this source-target phrase pair to the phrase table of a phrasebased ststistical MT (PB-SMT) system (Biçici and Dymetman, 2008). Simard and Isabelle (2009) reported a work on integration of PB-SMT with TM technology in a CAT environment in which the PBSMT system exploits the most similar matches by making use of TM-based feature functions. Koehn and Senellart (2010) reported another MT-TM integration strategy where TM is used to retrieve matching source seg3 System Description We demonstrate the functionalities and features of CATaLog in an English - Bengali translation task. The TM database consists of English sentences taken from BTEC3 (Basic Travel Expression Corpus) corpus and their Bengali translations4 . Unseen input or test segments are provided to the post-editing tool and the tool matches each of the input segments to the most similar segments contained in the TM. TM segments are then ranked according their the similarity to the test sentence us"
W15-5206,2008.amta-srw.4,0,0.397037,"e the translation workﬂow. In this paper, however, we do not yet explore the impact of our tool in the translation process, because the functionalities required for this kind of study are currently under development in CATaLog. Future work aims to investigate the impact of the new features we are proposing on the translator’s work. Related Work CAT tools have become very popular in the translation and localization industries in the last two decades. They are used by many language service providers, freelance translators to improve translation quality and to increase translator’s productivity (Lagoudaki, 2008). Although the work presented in this paper focuses on TM, it should also be noted that there were many studies on MT post-editing published in the last few years (Specia, 2011; Green et al., 2013; Green, 2014) and as mentioned in the last section, one of the recent trends is the development of hybrid systems that are able to combine MT with TM output. Therefore work on MT post-editing presents signiﬁcant overlap with state-of-the-art CAT tools and to what we propose in this paper. Substantial work have also been carried out on improving translation recommendation systems which recommends post"
W15-5206,N06-1014,0,0.0673427,"he top candidates by the TM. Figure 1 presents a snapshot of CATaLog. Color Coding Among the top 5 choices, post-editor selects one reference translation to do the post-editing task. To make that decision process easy, we color code the matched parts and unmatched parts in each reference translation. Green portion implies that they are matched fragments and red portion implies a mismatch. The alignments between the TM source sentences and their corresponding translations are generated using GIZA++ (Och and Ney, 2003) in the present work. However, any other word aligner, e.g., Berkley Aligner (Liang et al., 2006), could be used to produce this alignment. The alignment between the matched source segment and the corresponding translation, together with the TER alignment between the input sentence and the matched source segment, are used to generate the aforementioned color coding between selected source and target sentences. The GIZA++ alignment ﬁle is directly fed into the present TM tool. Given below is an example TM sentence pair along with the corresponding word alignment input to the TM. Input: you gave me wrong number . • English: we want to have a table near the window . Source Matches: 1. you ga"
W15-5206,J03-1002,0,0.00872292,"ssign a higher cost for insertion than deletion, and hence such sentences will not be shown as the top candidates by the TM. Figure 1 presents a snapshot of CATaLog. Color Coding Among the top 5 choices, post-editor selects one reference translation to do the post-editing task. To make that decision process easy, we color code the matched parts and unmatched parts in each reference translation. Green portion implies that they are matched fragments and red portion implies a mismatch. The alignments between the TM source sentences and their corresponding translations are generated using GIZA++ (Och and Ney, 2003) in the present work. However, any other word aligner, e.g., Berkley Aligner (Liang et al., 2006), could be used to produce this alignment. The alignment between the matched source segment and the corresponding translation, together with the TER alignment between the input sentence and the matched source segment, are used to generate the aforementioned color coding between selected source and target sentences. The GIZA++ alignment ﬁle is directly fed into the present TM tool. Given below is an example TM sentence pair along with the corresponding word alignment input to the TM. Input: you gave"
W15-5206,2012.amta-papers.26,0,0.0145925,". | D S S || S | | | we - would like a table by the window . For ﬁnding out the similar and dissimilar parts between the test segment and a matching TM segment, we use TER alignments. TER is an error metric and it gives an edit ratio (often referred to as edit rate or error rate) in terms of how much editing is required to convert a sentence into another with respect to the length of the ﬁrst sentence. Allowable edit operations include insert, delete, substitute and shift. We use the TER metric (using tercom-7.2515 ) to ﬁnd the edit rate between a test sentence and the TM reference sentences. Simard and Fujita (2012) ﬁrst proposed the use of MT evaluation metrics as similarity functions in implementing TM functionality. They experimented with several MT evaluation metrics, viz. BLEU, NIST, Meteor and TER, and studied their behaviors on TM performance. In the TM tool presented here we use TER as the similarity metric as it is very fast and lightweight and it directly mimics the human post-editing eﬀort. Moreover, the tercom-7.251 package also produces the alignments between the sentence pair from which it is very easy to identify which portions in the matching segment match with the input sentence and whic"
W15-5206,2015.eamt-1.6,1,\N,Missing
W16-2333,D11-1033,0,0.150111,", large amount of additional out-domain data may bias the resultant distribution towards the out-domain. In practice, 442 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 442–448, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2 3 Related Work System Description 3.1 Data selection Approach Among the different approaches proposed for data selection, the two most popular and successful methodologies are based on monolingual crossentropy difference (Moore and Lewis, 2010) and bilingual cross-entropy difference (Axelrod et al., 2011). The data selection approach taken in the present work is also motivated by the bilingual cross-entropy difference (Axelrod et al., 2011) based data selection. However, instead of using bilingual cross-entropy difference, we applied bilingual cross-perplexity difference to model our data selection process. The difference in crossentropy is computed on two language models (LM); the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm"
W16-2333,W08-0321,0,0.0258029,"sj − tj ] is calculated based on Equation 2. H(Plm ) = − Koehn et al. (2007) used multiple decoding paths for combining multiple domain-specific translation tables in the state-of-the-art PB-SMT decoder MOSES. Banerjee et al. (2013) combined an in-domain model (translation and reordering model) with an out-of-domain model into MOSES and they derived log-linear features to distinguish between phrases of multiple domains by applying the data-source indicator features and showed modest improvement in translation quality. score = |P Pinsl (sj ) − P Posl (sj )| + |P Pintl (tj ) − P Potl (tj ) |(2) Bach et al. (2008) suggested that sentences may be weighted by how much it matches with the target domain. A comparison among different domain adaptation methods for different subject matters in patent translation was carried out by (Ceaus¸fu et al., 2011) which led to a small gain over the baseline. Subsequently, sentence pairs [s − t] from the out-domain corpus (o) are ranked based on this score. 3.2 Interpolation Approach To combine multiple translation and language models, a common approach is to linearly interpolate them. The language model interpolation weights are automatically learnt by minimizing the p"
W16-2333,2011.mtsummit-papers.32,1,0.872589,"Missing"
W16-2333,N03-1017,0,0.00959327,"performance of the in-domain MT system. The following subsections describe the datasets used for the experiments, detailed experimental settings and systematic evaluation on both the development set and test set. 4.1 4.2 Experimental Settings We used the standard log-linear PB-SMT model for our experiments. All the experiments were carried out using a maximum phrase length of 7 for the translation model and 5-gram language models. The other experimental settings involved word alignment model between EN–DE trained with Berkeley Aligner (Liang et al., 2006). The phraseextraction heuristics of (Koehn et al., 2003) were used to build the phrase-based SMT systems. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) (Galley and Manning, 2008) method and conditioned on both the source and target languages. The 5-gram language models were built using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e., 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning w"
W16-2333,2013.mtsummit-papers.13,1,0.874847,"Missing"
W16-2333,E12-1045,0,0.0171699,"the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different directions, ranging from adapating language models and translation model"
W16-2333,W04-3250,0,0.0396345,"otivated by the bilingual cross-entropy difference (Axelrod et al., 2011) based data selection. However, instead of using bilingual cross-entropy difference, we applied bilingual cross-perplexity difference to model our data selection process. The difference in crossentropy is computed on two language models (LM); the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in"
W16-2333,2005.mtsummit-papers.11,0,0.0178143,"he bilingual cross-entropy difference (Axelrod et al., 2011) based data selection. However, instead of using bilingual cross-entropy difference, we applied bilingual cross-perplexity difference to model our data selection process. The difference in crossentropy is computed on two language models (LM); the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingu"
W16-2333,2011.eamt-1.5,0,0.0592153,"Missing"
W16-2333,W07-0734,0,0.0408698,"e performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the parameters were tuned, decoding was carried out on the held out development test set (Batch2 in Table 3) as well as test set released by the shared task organizers. We evaluated the systems using three well known automatic MT evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The evaluation results of our baseline systems trained on in-domain and out-domain data are reported in Table 3. Datasets In-domain Data: The detailed statistics of indomain data is reported in Table 1. We considered all the data provided by the WMT-2016 organizers for the IT translation task. We combined all data and performed cleaning in two steps: (i) Cleaning1: following the cleaning process described in (Pal et al., 2015), and (ii) Cleaning 2: using the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and"
W16-2333,N06-1014,0,0.0556417,"an them. We also use out of domain data to accelerate the performance of the in-domain MT system. The following subsections describe the datasets used for the experiments, detailed experimental settings and systematic evaluation on both the development set and test set. 4.1 4.2 Experimental Settings We used the standard log-linear PB-SMT model for our experiments. All the experiments were carried out using a maximum phrase length of 7 for the translation model and 5-gram language models. The other experimental settings involved word alignment model between EN–DE trained with Berkeley Aligner (Liang et al., 2006). The phraseextraction heuristics of (Koehn et al., 2003) were used to build the phrase-based SMT systems. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) (Galley and Manning, 2008) method and conditioned on both the source and target languages. The 5-gram language models were built using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e., 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing s"
W16-2333,2010.iwslt-papers.5,0,0.0226244,"is computed on two language models (LM); the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different directions, ranging f"
W16-2333,W07-0717,0,0.0383431,"erence in crossentropy is computed on two language models (LM); the domain-specific LM is estimated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different dir"
W16-2333,D07-1036,0,0.0628601,"Missing"
W16-2333,W06-1607,0,0.20109,"esearch Center for Artificial Intelligence (DFKI), Germany {pahari.koushik,alapan.cse}@gmail.com, sudip.naskar@jdvu.ac.in, sivaji cse ju@yahoo.com {santanu.pal, josef.vangenabith}@uni-saarland.de Abstract it is often difficult to obtain sufficient amount of in-domain parallel data to train a system which can provide good performance in a specific domain. The performance of an in-domain model can be improved by selecting a subset from the out-domain data which is very similar to the indomain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distributions (Foster et al., 2006; Sennrich et al., 2013) in favor of the in-domain data. This paper presents the JU-USAAR English–German domain adaptive machine translation (MT) system submitted to the IT domain translation task organized in WMT-2016 . Our system brings improvements over the in-domain baseline system by incorporating out-domain knowledge. We applied two methodologies to accelerate the performance of our in-domain MT system: (i) additional training material extraction from out-domain data using data selection method, and (ii) language model and translation model adaptation through interpolation. Our primary s"
W16-2333,D09-1074,0,0.060014,"Missing"
W16-2333,D08-1089,0,0.0115611,"set and test set. 4.1 4.2 Experimental Settings We used the standard log-linear PB-SMT model for our experiments. All the experiments were carried out using a maximum phrase length of 7 for the translation model and 5-gram language models. The other experimental settings involved word alignment model between EN–DE trained with Berkeley Aligner (Liang et al., 2006). The phraseextraction heuristics of (Koehn et al., 2003) were used to build the phrase-based SMT systems. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) (Galley and Manning, 2008) method and conditioned on both the source and target languages. The 5-gram language models were built using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e., 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the paramete"
W16-2333,P10-2041,0,0.539223,"ata, India 3 Universit¨at des Saarlandes, Saarbr¨ucken, Germany 4 German Research Center for Artificial Intelligence (DFKI), Germany {pahari.koushik,alapan.cse}@gmail.com, sudip.naskar@jdvu.ac.in, sivaji cse ju@yahoo.com {santanu.pal, josef.vangenabith}@uni-saarland.de Abstract it is often difficult to obtain sufficient amount of in-domain parallel data to train a system which can provide good performance in a specific domain. The performance of an in-domain model can be improved by selecting a subset from the out-domain data which is very similar to the indomain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distributions (Foster et al., 2006; Sennrich et al., 2013) in favor of the in-domain data. This paper presents the JU-USAAR English–German domain adaptive machine translation (MT) system submitted to the IT domain translation task organized in WMT-2016 . Our system brings improvements over the in-domain baseline system by incorporating out-domain knowledge. We applied two methodologies to accelerate the performance of our in-domain MT system: (i) additional training material extraction from out-domain data using data selection method, and (ii) language mode"
W16-2333,W12-3154,0,0.0151824,"s (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different directions, ranging from adapating language models and translation models to alignment adaptation approach to improve domainspecific wor"
W16-2333,P03-1021,0,0.00947555,"rdering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) (Galley and Manning, 2008) method and conditioned on both the source and target languages. The 5-gram language models were built using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e., 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the parameters were tuned, decoding was carried out on the held out development test set (Batch2 in Table 3) as well as test set released by the shared task organizers. We evaluated the systems using three well known automatic MT evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The evaluation results of our baseline systems trained on in-domain and out-domain data are reported in Table 3. Datasets In-domain D"
W16-2333,W11-2123,0,0.0263772,"rried out using a maximum phrase length of 7 for the translation model and 5-gram language models. The other experimental settings involved word alignment model between EN–DE trained with Berkeley Aligner (Liang et al., 2006). The phraseextraction heuristics of (Koehn et al., 2003) were used to build the phrase-based SMT systems. The reordering model was trained with the hierarchical, monotone, swap, left to right bidirectional (hier-mslr-bidirectional) (Galley and Manning, 2008) method and conditioned on both the source and target languages. The 5-gram language models were built using KenLM (Heafield, 2011). Phrase pairs that occur only once in the training data are assigned an unduly high probability mass (i.e., 1). To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the parameters were tuned, decoding was carried out on the held out development test set (Batch2 in Table 3) as well as test set released by th"
W16-2333,P02-1040,0,0.099321,"To alleviate this shortcoming, we performed smoothing of the phrase table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the parameters were tuned, decoding was carried out on the held out development test set (Batch2 in Table 3) as well as test set released by the shared task organizers. We evaluated the systems using three well known automatic MT evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The evaluation results of our baseline systems trained on in-domain and out-domain data are reported in Table 3. Datasets In-domain Data: The detailed statistics of indomain data is reported in Table 1. We considered all the data provided by the WMT-2016 organizers for the IT translation task. We combined all data and performed cleaning in two steps: (i) Cleaning1: following the cleaning process described in (Pal et al., 2015), and (ii) Cleaning 2: using the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and max"
W16-2333,P13-1082,0,0.0831355,"tificial Intelligence (DFKI), Germany {pahari.koushik,alapan.cse}@gmail.com, sudip.naskar@jdvu.ac.in, sivaji cse ju@yahoo.com {santanu.pal, josef.vangenabith}@uni-saarland.de Abstract it is often difficult to obtain sufficient amount of in-domain parallel data to train a system which can provide good performance in a specific domain. The performance of an in-domain model can be improved by selecting a subset from the out-domain data which is very similar to the indomain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distributions (Foster et al., 2006; Sennrich et al., 2013) in favor of the in-domain data. This paper presents the JU-USAAR English–German domain adaptive machine translation (MT) system submitted to the IT domain translation task organized in WMT-2016 . Our system brings improvements over the in-domain baseline system by incorporating out-domain knowledge. We applied two methodologies to accelerate the performance of our in-domain MT system: (i) additional training material extraction from out-domain data using data selection method, and (ii) language model and translation model adaptation through interpolation. Our primary submission obtained a BLE"
W16-2333,E12-1055,0,0.0841094,"timated from the entire in-domain corpus (lmin ) and the second LM (lmo ) is estimated from the out-domain corpus. Mathematically, the cross-entropy H(Plm ) of language model probability Plm is defined as in Equation 1 considering a k-gram language model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different directions, ranging from adapating language models and translation models to alignment a"
W16-2333,N06-2037,0,0.0324717,"age model. Koehn (2004; Koehn (2005) first proposed domain adaptation in SMT by integrating terminological lexicons in the translation model, as a result of which there was a significant reduction in word error rate (WER). Over the last decade, many researchers (Foster and Kuhn, 2007; Duh et al., 2010; Banerjee et al., 2011; Bisazza and Federico, 2012; Sennrich, 2012; Sennrich et al., 2013; Haddow and Koehn, 2012) investigated the problem of combining multi-domain datasets. To construct a good domain-specific language model, sentences which are similar to the target domain should be included (Sethy et al., 2006) in the monolingual target language corupus on which the language model is trained. L¨u et al. (2007) identified those sentences using the tf/idf method and they increased the count of such sentences. Domain adaptation in MT have been explored in many different directions, ranging from adapating language models and translation models to alignment adaptation approach to improve domainspecific word alignment. N 1 X log Plm (wi |wi−k+1 ...wi−1 ) N i=1 (1) We calculated perplexity (P P = 2H ) of individual sentences of out-domain with respect to indomain LM and out-domain LM for both source (sl) a"
W16-2333,2006.amta-papers.25,0,0.0417351,"e table using the Good-Turing smoothing technique (Foster et al., 2006). System tuning was carried out using Minimum Error Rate Training (MERT) (Och, 2003) on a held out development set (Batch1 in Table 3) of size 1,000 sentences provided by the WMT-2016 task organizers. After the parameters were tuned, decoding was carried out on the held out development test set (Batch2 in Table 3) as well as test set released by the shared task organizers. We evaluated the systems using three well known automatic MT evaluation metrics: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006). The evaluation results of our baseline systems trained on in-domain and out-domain data are reported in Table 3. Datasets In-domain Data: The detailed statistics of indomain data is reported in Table 1. We considered all the data provided by the WMT-2016 organizers for the IT translation task. We combined all data and performed cleaning in two steps: (i) Cleaning1: following the cleaning process described in (Pal et al., 2015), and (ii) Cleaning 2: using the Moses (Koehn et al., 2007) corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 80 respectively. Additionally"
W16-2333,W14-3323,1,0.898642,"Missing"
W16-2333,P07-2045,0,\N,Missing
W16-2333,W15-3017,1,\N,Missing
W16-6308,N06-1020,0,0.0628442,"am Features: We use the character bi-gram and tri-gram sequences extracted from a token as features. For example, for the token “upregulation”, the bigram features will be up pr re eg gu ul la at ti io on and tri-gram features will be upr pre reg egu gul ula lat ati tio ion . 6. Dependency Path Features: There are are some trigger words which ca not be detected using context features or b-gram or tri-gram features. So we depend on dependency relations inside sentence. Dependency features are extracted from dependency graph generated by dependency parser(David McClosky and Manning, 2011; David McClosky and Johnson, 2006) . Figure 3 shows the dependency graph for the sentence “BMP-6 inhibits growth of mature human B cells; in59 duction of Smad phosphorylation and upregulation of Id1”, generated by the CharniakMcCloskey parser (David McClosky and Johnson, 2006). In the graph, an edge label represents the dependency relation between two nodes. Each node in the graph is labelled by a number which represents a word appearing in that position (0-based index) of the sentence. For example, node labelled with number 0 indicates the word BMP-6 and node labelled with number 1 indicates the word inhibits. Figure 3: Depen"
W16-6308,W09-1401,0,0.417369,"Missing"
W16-6308,W11-1806,0,0.0210969,"5. Bi-gram and Tri-gram Features: We use the character bi-gram and tri-gram sequences extracted from a token as features. For example, for the token “upregulation”, the bigram features will be up pr re eg gu ul la at ti io on and tri-gram features will be upr pre reg egu gul ula lat ati tio ion . 6. Dependency Path Features: There are are some trigger words which ca not be detected using context features or b-gram or tri-gram features. So we depend on dependency relations inside sentence. Dependency features are extracted from dependency graph generated by dependency parser(David McClosky and Manning, 2011; David McClosky and Johnson, 2006) . Figure 3 shows the dependency graph for the sentence “BMP-6 inhibits growth of mature human B cells; in59 duction of Smad phosphorylation and upregulation of Id1”, generated by the CharniakMcCloskey parser (David McClosky and Johnson, 2006). In the graph, an edge label represents the dependency relation between two nodes. Each node in the graph is labelled by a number which represents a word appearing in that position (0-based index) of the sentence. For example, node labelled with number 0 indicates the word BMP-6 and node labelled with number 1 indicates"
W16-6308,W11-1801,0,0.353092,"Missing"
W16-6308,W13-2015,0,0.0321345,"Missing"
W16-6308,W11-1807,0,0.360884,"Missing"
W16-6308,W11-1808,0,0.263057,"5. Bi-gram and Tri-gram Features: We use the character bi-gram and tri-gram sequences extracted from a token as features. For example, for the token “upregulation”, the bigram features will be up pr re eg gu ul la at ti io on and tri-gram features will be upr pre reg egu gul ula lat ati tio ion . 6. Dependency Path Features: There are are some trigger words which ca not be detected using context features or b-gram or tri-gram features. So we depend on dependency relations inside sentence. Dependency features are extracted from dependency graph generated by dependency parser(David McClosky and Manning, 2011; David McClosky and Johnson, 2006) . Figure 3 shows the dependency graph for the sentence “BMP-6 inhibits growth of mature human B cells; in59 duction of Smad phosphorylation and upregulation of Id1”, generated by the CharniakMcCloskey parser (David McClosky and Johnson, 2006). In the graph, an edge label represents the dependency relation between two nodes. Each node in the graph is labelled by a number which represents a word appearing in that position (0-based index) of the sentence. For example, node labelled with number 0 indicates the word BMP-6 and node labelled with number 1 indicates"
W16-6308,W09-1313,0,0.0700811,"Missing"
W16-6624,W05-0909,0,0.0347111,"ms whose outputs on the Prodigy-METEO testset are also available in the Prodigy-METEO corpus. These ten NLG systems are PCFG-Greedy, PSCFG-Semantic, PSCFG-Unstructured, PCFG-Viterbii, PCFG2gram, PCFG-Roulette, PBSMT-Unstructured, Figure 4: An sample of input and outputs of different NLG system SumTime-Hybrid, PBSMT-Structured and PCFGRandom (Belz and Kow, 2009). Figure 4 shows a sample input and outputs of all the above mentioned systems including our system. 4.2.1 Automatic Evaluation For automatic evaluation, we used two automatic evaluation metrics; BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). Both BLEU and METEOR were originally proposed for evaluation of machine translation (MT) systems However, due to the similarity between the two tasks (i.e., MT and NLG) from the point of view of their working principles, most of the NLG systems are also evaluated using these two automatic MT evaluation metrics. Because of the relatively small size of the dataset, we took a five-fold cross validation policy which was predefined in the Prodigy-METEO corpus. Table 1 presents the evaluation results obtained with BLEU and METEOR on our system along with the ten other NLG systems. 149 System Corpu"
W16-6624,W09-0603,0,0.726589,"language model. Since its inception, statistical machine translation (Brown et al., 1993; Koehn, 2010) has gained immense popularity and it is the most prominent approach and represents the state-of-the-art in automatic machine translation. The task of NLG can be thought as a machine translation task because of the similarity between their end objectives - converting from one language to another. Langner and Black (2009) proposed an NLG system, Mountain, which modelled the task of NLG as statistical machine translation (SMT). They used the MOSES1 toolkit (Koehn et al., 2007) for this purpose. Belz and Kow (2009) proposed another SMT based NLG system which made use of the phrase-based SMT (PB-SMT) model (Koehn et al., 2003). The MOSES toolkit offers an efficient implementation of the PBSMT model. However, the linguistic quality and readability of PB-SMT based NLG systems were not as good as compared to other statistical NLG systems like Nitrogen, Oxygen, etc. (Belz and Kow, 1 http://www.statmt.org/moses 144 2009). Some semi-automatic NLG systems had also been proposed. The Probabilistic synchronous contextfree grammar (PSCFG) generator (Belz, 2008) represents this category of NLG systems which can be"
W16-6624,J93-2003,0,0.0709952,"stems have been proposed and some of them achieved quite good results in the generation task. Two such successful statistical NLG systems are Nitrogen (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998) and Oxygen (Habash, 2000). These two NLG systems are based on statistical sentence realizer. Similarly Halogen (Langkilde and Knight, 1998) represents another statistical language generator which is based on statistical n-gram language model. Oh and Rudnicky (2000) also proposed an NLG system based on statistical language model. Since its inception, statistical machine translation (Brown et al., 1993; Koehn, 2010) has gained immense popularity and it is the most prominent approach and represents the state-of-the-art in automatic machine translation. The task of NLG can be thought as a machine translation task because of the similarity between their end objectives - converting from one language to another. Langner and Black (2009) proposed an NLG system, Mountain, which modelled the task of NLG as statistical machine translation (SMT). They used the MOSES1 toolkit (Koehn et al., 2007) for this purpose. Belz and Kow (2009) proposed another SMT based NLG system which made use of the phrase-b"
W16-6624,P95-1034,0,0.346082,"and resources, our system is able to generate intelligible and readable text output. The remainder of the paper is organized as follows. Section 2 briefly presents relevant related work. The proposed NLG system is described in Section 3. Section 4 elaborates the experimental settings, dataset and the corresponding results. Section 5 concludes the paper. 2 Related works Till date, a number of knowledge-light approach based language generator systems have been proposed and some of them achieved quite good results in the generation task. Two such successful statistical NLG systems are Nitrogen (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998) and Oxygen (Habash, 2000). These two NLG systems are based on statistical sentence realizer. Similarly Halogen (Langkilde and Knight, 1998) represents another statistical language generator which is based on statistical n-gram language model. Oh and Rudnicky (2000) also proposed an NLG system based on statistical language model. Since its inception, statistical machine translation (Brown et al., 1993; Koehn, 2010) has gained immense popularity and it is the most prominent approach and represents the state-of-the-art in automatic machine translation. The task of NL"
W16-6624,N03-1017,0,0.0108914,"immense popularity and it is the most prominent approach and represents the state-of-the-art in automatic machine translation. The task of NLG can be thought as a machine translation task because of the similarity between their end objectives - converting from one language to another. Langner and Black (2009) proposed an NLG system, Mountain, which modelled the task of NLG as statistical machine translation (SMT). They used the MOSES1 toolkit (Koehn et al., 2007) for this purpose. Belz and Kow (2009) proposed another SMT based NLG system which made use of the phrase-based SMT (PB-SMT) model (Koehn et al., 2003). The MOSES toolkit offers an efficient implementation of the PBSMT model. However, the linguistic quality and readability of PB-SMT based NLG systems were not as good as compared to other statistical NLG systems like Nitrogen, Oxygen, etc. (Belz and Kow, 1 http://www.statmt.org/moses 144 2009). Some semi-automatic NLG systems had also been proposed. The Probabilistic synchronous contextfree grammar (PSCFG) generator (Belz, 2008) represents this category of NLG systems which can be created mostly automatically but requires manual help to certain extent. In synchronous contextfree grammar (SCFG"
W16-6624,P07-2045,0,0.00852566,"ed an NLG system based on statistical language model. Since its inception, statistical machine translation (Brown et al., 1993; Koehn, 2010) has gained immense popularity and it is the most prominent approach and represents the state-of-the-art in automatic machine translation. The task of NLG can be thought as a machine translation task because of the similarity between their end objectives - converting from one language to another. Langner and Black (2009) proposed an NLG system, Mountain, which modelled the task of NLG as statistical machine translation (SMT). They used the MOSES1 toolkit (Koehn et al., 2007) for this purpose. Belz and Kow (2009) proposed another SMT based NLG system which made use of the phrase-based SMT (PB-SMT) model (Koehn et al., 2003). The MOSES toolkit offers an efficient implementation of the PBSMT model. However, the linguistic quality and readability of PB-SMT based NLG systems were not as good as compared to other statistical NLG systems like Nitrogen, Oxygen, etc. (Belz and Kow, 1 http://www.statmt.org/moses 144 2009). Some semi-automatic NLG systems had also been proposed. The Probabilistic synchronous contextfree grammar (PSCFG) generator (Belz, 2008) represents this"
W16-6624,J10-4005,0,0.025254,"osed and some of them achieved quite good results in the generation task. Two such successful statistical NLG systems are Nitrogen (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998) and Oxygen (Habash, 2000). These two NLG systems are based on statistical sentence realizer. Similarly Halogen (Langkilde and Knight, 1998) represents another statistical language generator which is based on statistical n-gram language model. Oh and Rudnicky (2000) also proposed an NLG system based on statistical language model. Since its inception, statistical machine translation (Brown et al., 1993; Koehn, 2010) has gained immense popularity and it is the most prominent approach and represents the state-of-the-art in automatic machine translation. The task of NLG can be thought as a machine translation task because of the similarity between their end objectives - converting from one language to another. Langner and Black (2009) proposed an NLG system, Mountain, which modelled the task of NLG as statistical machine translation (SMT). They used the MOSES1 toolkit (Koehn et al., 2007) for this purpose. Belz and Kow (2009) proposed another SMT based NLG system which made use of the phrase-based SMT (PB-S"
W16-6624,P98-1116,0,0.440094,"c.in Abstract Knowledge-intensive generation approaches take significant human effort or expert advise for building an NLG system. Some examples of this type of NLG systems are SumTime system (Reiter et al., 2005), FoG system (Goldberg et al., 1994), PLANDOC system (McKeown et al., 1994), etc. On the other hand, knowledge-light NLG systems mostly use statistical methods to generate output text and take less human effort. Being automatic systems, knowledge-light systems mostly employ machine learning and data mining techniques. There are many types of knowledge-light systems; n-gram based NLG (Langkilde and Knight, 1998), neural network based NLG (Sutskever et al., 2011), case based NLG (Pan and Shaw, 2004), etc. However, it has been observed that knowledge-intensive systems typically perform better than knowledge-light systems as per human evaluation (Adeyanju, 2012). Most of the existing natural language generation (NLG) techniques employing statistical methods are typically resource and time intensive. On the other hand, handcrafted rulebased and template-based NLG systems typically require significant human/designer efforts. In this paper, we proposed a statistical NLG technique which does not require any"
W16-6624,A94-1002,0,0.33437,"Missing"
W16-6624,W00-0306,0,0.122681,"responding results. Section 5 concludes the paper. 2 Related works Till date, a number of knowledge-light approach based language generator systems have been proposed and some of them achieved quite good results in the generation task. Two such successful statistical NLG systems are Nitrogen (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998) and Oxygen (Habash, 2000). These two NLG systems are based on statistical sentence realizer. Similarly Halogen (Langkilde and Knight, 1998) represents another statistical language generator which is based on statistical n-gram language model. Oh and Rudnicky (2000) also proposed an NLG system based on statistical language model. Since its inception, statistical machine translation (Brown et al., 1993; Koehn, 2010) has gained immense popularity and it is the most prominent approach and represents the state-of-the-art in automatic machine translation. The task of NLG can be thought as a machine translation task because of the similarity between their end objectives - converting from one language to another. Langner and Black (2009) proposed an NLG system, Mountain, which modelled the task of NLG as statistical machine translation (SMT). They used the MOSE"
W16-6624,P02-1040,0,0.0986526,"system with ten existing NLG systems whose outputs on the Prodigy-METEO testset are also available in the Prodigy-METEO corpus. These ten NLG systems are PCFG-Greedy, PSCFG-Semantic, PSCFG-Unstructured, PCFG-Viterbii, PCFG2gram, PCFG-Roulette, PBSMT-Unstructured, Figure 4: An sample of input and outputs of different NLG system SumTime-Hybrid, PBSMT-Structured and PCFGRandom (Belz and Kow, 2009). Figure 4 shows a sample input and outputs of all the above mentioned systems including our system. 4.2.1 Automatic Evaluation For automatic evaluation, we used two automatic evaluation metrics; BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). Both BLEU and METEOR were originally proposed for evaluation of machine translation (MT) systems However, due to the similarity between the two tasks (i.e., MT and NLG) from the point of view of their working principles, most of the NLG systems are also evaluated using these two automatic MT evaluation metrics. Because of the relatively small size of the dataset, we took a five-fold cross validation policy which was predefined in the Prodigy-METEO corpus. Table 1 presents the evaluation results obtained with BLEU and METEOR on our system along with the t"
W16-6624,W15-4639,0,0.0122505,"es very little manual help and if the given prior dataset covers almost all types of input instances then CBR based systems perform better. Recently, some neural network based NLG systems have been proposed. With the advent of recurrent neural network (RNN) based language models (RNNLM) (Mikolov et al., 2010), some RNN based NLG systems have been proposed. An idea of generating text through recurrent neural network based approach with Hessian-free optimization was proposed by (Sutskever et al., 2011). However, this method takes a long training time. An RNN based NLG technique was proposed by (Wen et al., 2015) based on a joint recurrent and convolutional neural network structure. This system was able to train on dialogue act-utterance pairs without any semantic alignments or predefined grammar trees. Although rule based knowledge-intensive NLG systems take long time and expert knowledge and feedback to be developed, this type of systems most of the times are able to generate high quality natural language text output. For example, SumTime (Reiter et al., 2005) weather forecasting system is essentially a rule based NLG system, however, its output text quality was found to be quite better compared to"
W16-6624,C98-1112,0,\N,Missing
W17-7519,D14-1058,0,0.240568,"t and convert it into a program skeleton using ‘PERL’ programming language which is object oriented in nature. Following the “who does what” structure their system develops a program skeleton and generates the ‘PERL’ code for texts like “When customer orders a drink, the bartender makes it”. They developed a model which creates different classes like ‘Customer’, ‘Bartender’ and relevant methods like ‘order drink()’, ‘make drink()’ to support their actions. Our work is little relevant to their work. Alongside, many researchers proposed various methodologies to solve MWPs (Kushman et al., 2014; Hosseini et al., 2014; Walker and Kintsch, 1985; Fletcher, 1985; Roy and Roth, 2015; Shi et al., 2015; Mitra and Baral, 2016). The work presented in this paper differs from these works. 3 3.1 System Description Mapping Input Texts to The Concept Natural language texts representing some MWPs typically contain multiple factual sentences and a 148 ‘question sentence’ at the end (cf. the example given in Section 1). Each sentence may or may not have some mathematical meaning. Our objective is to identify the key players or entities and their state transitions from the first sentence they occur in and till the last sen"
W17-7519,N13-1103,0,0.031041,"s together with some analysis, followed by conclusions and avenues for further research in Section 5. 2 Related Work The research problem on generation of executable computer programs for solving MWPs has not been attempted so far to the best of our knowledge. However, formal language modeling from natural language text has been studied previously in various domain by researchers mainly in software engineering (Bryant et al., 2003; Lei et al., 2013), web interfaces of databases (Alexander et al., 2013), etc. Some researchers tried to represent natural language texts using regular expressions (Kushman and Barzilay, 2013). Ballard and Biermann (Ballard and Biermann, 1979) proposed a natural language computing (‘NLC’) prototype to process and evaluate small natural language text word problems based on matrix computation. They proposed a method to generate solution from a matrix entry and solve problems like “add five with the second positive entry in row 5”, “double the fifth entry and add that to the last entry of that row”, etc. Each of these assignments have some types of mathematical terminologies like ‘add’, ‘double’, etc., which clearly indicates the operation or operator. This research problem is not exa"
W17-7519,P14-1026,0,0.0246204,"representing any event and convert it into a program skeleton using ‘PERL’ programming language which is object oriented in nature. Following the “who does what” structure their system develops a program skeleton and generates the ‘PERL’ code for texts like “When customer orders a drink, the bartender makes it”. They developed a model which creates different classes like ‘Customer’, ‘Bartender’ and relevant methods like ‘order drink()’, ‘make drink()’ to support their actions. Our work is little relevant to their work. Alongside, many researchers proposed various methodologies to solve MWPs (Kushman et al., 2014; Hosseini et al., 2014; Walker and Kintsch, 1985; Fletcher, 1985; Roy and Roth, 2015; Shi et al., 2015; Mitra and Baral, 2016). The work presented in this paper differs from these works. 3 3.1 System Description Mapping Input Texts to The Concept Natural language texts representing some MWPs typically contain multiple factual sentences and a 148 ‘question sentence’ at the end (cf. the example given in Section 1). Each sentence may or may not have some mathematical meaning. Our objective is to identify the key players or entities and their state transitions from the first sentence they occur i"
W17-7519,P13-1127,0,0.0178995,"relevant related work. Section 3 provides a detailed discussion on the system components. Section 4 outlines the datasets, experiments and the corresponding results together with some analysis, followed by conclusions and avenues for further research in Section 5. 2 Related Work The research problem on generation of executable computer programs for solving MWPs has not been attempted so far to the best of our knowledge. However, formal language modeling from natural language text has been studied previously in various domain by researchers mainly in software engineering (Bryant et al., 2003; Lei et al., 2013), web interfaces of databases (Alexander et al., 2013), etc. Some researchers tried to represent natural language texts using regular expressions (Kushman and Barzilay, 2013). Ballard and Biermann (Ballard and Biermann, 1979) proposed a natural language computing (‘NLC’) prototype to process and evaluate small natural language text word problems based on matrix computation. They proposed a method to generate solution from a matrix entry and solve problems like “add five with the second positive entry in row 5”, “double the fifth entry and add that to the last entry of that row”, etc. Each of t"
W17-7519,P16-1202,0,0.0518046,"nature. Following the “who does what” structure their system develops a program skeleton and generates the ‘PERL’ code for texts like “When customer orders a drink, the bartender makes it”. They developed a model which creates different classes like ‘Customer’, ‘Bartender’ and relevant methods like ‘order drink()’, ‘make drink()’ to support their actions. Our work is little relevant to their work. Alongside, many researchers proposed various methodologies to solve MWPs (Kushman et al., 2014; Hosseini et al., 2014; Walker and Kintsch, 1985; Fletcher, 1985; Roy and Roth, 2015; Shi et al., 2015; Mitra and Baral, 2016). The work presented in this paper differs from these works. 3 3.1 System Description Mapping Input Texts to The Concept Natural language texts representing some MWPs typically contain multiple factual sentences and a 148 ‘question sentence’ at the end (cf. the example given in Section 1). Each sentence may or may not have some mathematical meaning. Our objective is to identify the key players or entities and their state transitions from the first sentence they occur in and till the last sentence. An MWP example containing multiple sentences is given below. “Harry has 15 blue and 10 green ball"
W17-7519,Q15-1032,0,0.0207407,"d’, ‘,’ (comma), ‘but’, ‘, and’ and ‘, but’ from compound sentences and break them into multiple simple sentences. The coreference mentions for pronouns like ‘he’, ‘she’, ‘his’, ‘her’ etc., are substituted with the corresponding referred expressions so that we can extract the owner entities directly and unambiguously. 3.3 Information Extraction based on Semantic Role Labelling (SRL) SRL techniques are mainly used to semantically process texts and to define role(s) of every words present in a text. For extracting information from text, we used the SRL tool – Mateplus4 (Roth and Woodsend, 2014; Roth and Lapata, 2015), which was developed for meaning representations based on the CMU SEMAFOR5 tool and frameNet6 . Table 1 shows the output of ‘Mateplus’ for the sample sentence “Sam gave Mary 23 green marbles.”. Depending on the type of the predicates and also ID 1 2 3 4 5 6 7 Form Sam gave Mary 23 green marbles ‘.’ POS NNP VBD NNP CD JJ NNS ‘.’ Dependency SUB ROOT OBJ NMOD AMOD OBJ P Predicate Giving - Args:Locating Donor Recipient Theme - Table 1: A sample SRL output the verb grouping from VerbNet7 , the verbs are manually categorized and respective equations are generated by the system (cf. Subsection 3.4)."
W17-7519,D14-1045,0,0.0301776,"ove conjunctions like ‘and’, ‘,’ (comma), ‘but’, ‘, and’ and ‘, but’ from compound sentences and break them into multiple simple sentences. The coreference mentions for pronouns like ‘he’, ‘she’, ‘his’, ‘her’ etc., are substituted with the corresponding referred expressions so that we can extract the owner entities directly and unambiguously. 3.3 Information Extraction based on Semantic Role Labelling (SRL) SRL techniques are mainly used to semantically process texts and to define role(s) of every words present in a text. For extracting information from text, we used the SRL tool – Mateplus4 (Roth and Woodsend, 2014; Roth and Lapata, 2015), which was developed for meaning representations based on the CMU SEMAFOR5 tool and frameNet6 . Table 1 shows the output of ‘Mateplus’ for the sample sentence “Sam gave Mary 23 green marbles.”. Depending on the type of the predicates and also ID 1 2 3 4 5 6 7 Form Sam gave Mary 23 green marbles ‘.’ POS NNP VBD NNP CD JJ NNS ‘.’ Dependency SUB ROOT OBJ NMOD AMOD OBJ P Predicate Giving - Args:Locating Donor Recipient Theme - Table 1: A sample SRL output the verb grouping from VerbNet7 , the verbs are manually categorized and respective equations are generated by the syst"
W17-7519,D15-1202,0,0.016934,"language which is object oriented in nature. Following the “who does what” structure their system develops a program skeleton and generates the ‘PERL’ code for texts like “When customer orders a drink, the bartender makes it”. They developed a model which creates different classes like ‘Customer’, ‘Bartender’ and relevant methods like ‘order drink()’, ‘make drink()’ to support their actions. Our work is little relevant to their work. Alongside, many researchers proposed various methodologies to solve MWPs (Kushman et al., 2014; Hosseini et al., 2014; Walker and Kintsch, 1985; Fletcher, 1985; Roy and Roth, 2015; Shi et al., 2015; Mitra and Baral, 2016). The work presented in this paper differs from these works. 3 3.1 System Description Mapping Input Texts to The Concept Natural language texts representing some MWPs typically contain multiple factual sentences and a 148 ‘question sentence’ at the end (cf. the example given in Section 1). Each sentence may or may not have some mathematical meaning. Our objective is to identify the key players or entities and their state transitions from the first sentence they occur in and till the last sentence. An MWP example containing multiple sentences is given b"
W17-7519,D15-1135,0,0.0204994,"bject oriented in nature. Following the “who does what” structure their system develops a program skeleton and generates the ‘PERL’ code for texts like “When customer orders a drink, the bartender makes it”. They developed a model which creates different classes like ‘Customer’, ‘Bartender’ and relevant methods like ‘order drink()’, ‘make drink()’ to support their actions. Our work is little relevant to their work. Alongside, many researchers proposed various methodologies to solve MWPs (Kushman et al., 2014; Hosseini et al., 2014; Walker and Kintsch, 1985; Fletcher, 1985; Roy and Roth, 2015; Shi et al., 2015; Mitra and Baral, 2016). The work presented in this paper differs from these works. 3 3.1 System Description Mapping Input Texts to The Concept Natural language texts representing some MWPs typically contain multiple factual sentences and a 148 ‘question sentence’ at the end (cf. the example given in Section 1). Each sentence may or may not have some mathematical meaning. Our objective is to identify the key players or entities and their state transitions from the first sentence they occur in and till the last sentence. An MWP example containing multiple sentences is given below. “Harry has 1"
W17-7537,J11-2002,0,0.0746273,"Missing"
W17-7537,W04-0109,0,0.0599002,"ich is a very popular ULM technique till date. Few researchers (Golcher, 2006; Hammarstr¨om, 2009) suggested morpheme segmentation using entropy. The second type uses grouping and abstracting techniques and they first group all similar morphological words into a particular cluster among many existing ones, then find unique pattern for each cluster of words in such a way that the patterns can reveal all morphemes corresponding to the clusters. his approach is also very common and has multiple implementation examples (Schone, 2001; Yarowsky and Wicentowski, 2000; Wicentowski and Yarowsky, 2002; Wicentowski, 2004; Majumder et al., 2007). The third ULM based approach (Mayfield and McNamee, 2003; De Pauw and Wagacha, 2007) is quite similar to basic machine learning based approaches. It first represents each word by multiple features and finally stems are separated from the affixes based on the feature values. The last type of ULM technique is quite similar to the first ULM technique, with a small exception that prior to the border separation, words are categorized based on their phoneme structure (Rodrigues and Cavar, 2007). This ULM technique is applicable for non-concatenative morphology 299 analysis,"
W17-7537,P00-1027,0,0.194258,"ULM based system (Harris, 1955) falls in this category of ULM which is a very popular ULM technique till date. Few researchers (Golcher, 2006; Hammarstr¨om, 2009) suggested morpheme segmentation using entropy. The second type uses grouping and abstracting techniques and they first group all similar morphological words into a particular cluster among many existing ones, then find unique pattern for each cluster of words in such a way that the patterns can reveal all morphemes corresponding to the clusters. his approach is also very common and has multiple implementation examples (Schone, 2001; Yarowsky and Wicentowski, 2000; Wicentowski and Yarowsky, 2002; Wicentowski, 2004; Majumder et al., 2007). The third ULM based approach (Mayfield and McNamee, 2003; De Pauw and Wagacha, 2007) is quite similar to basic machine learning based approaches. It first represents each word by multiple features and finally stems are separated from the affixes based on the feature values. The last type of ULM technique is quite similar to the first ULM technique, with a small exception that prior to the border separation, words are categorized based on their phoneme structure (Rodrigues and Cavar, 2007). This ULM technique is applic"
W17-7539,W15-4319,0,0.0561899,"Missing"
W17-7539,D14-1179,0,0.0211622,"Missing"
W17-7539,W11-0609,0,0.0222913,"Missing"
W17-7539,P13-1155,0,0.0239593,"proach for neural network based sequence to sequence modelling tasks2 . Our LSTM model was trained with attention mechanism (Bahdanau et al., 2014). 2 Related Work Text Normalization is a well known task in the field of NLP, particularly in the Social Media domain. Clark and Araki (2011) provides a detailed survey on the challenges and applications of text normalization in Social Media. Researchers have shown that text normalization is a major factor in improving performance of NLP intermediate tasks like part-of-speech tagging (Han et al., 2013) and NLP applications like machine translation (Hassan and Menezes, 2013). Research in text normalization started with spelling correction with noisy channel model (Kernighan et al., 1990; Mays et al., 1991). Since then several different approaches have been proposed by researchers. We report here a few of the most prominent works on text normalization, with a particular focus on social media. 2 (Ling et al., 2015) showed improvement on the machine translation task. Text normalization is conceptually very similar to and can be modelled as a machine translation task using noisy–clean parallel corpus.. S Bandyopadhyay, D S Sharma and R Sangal. Proc. of the 14th Intl."
W17-7539,C90-2036,0,0.393922,"hanism (Bahdanau et al., 2014). 2 Related Work Text Normalization is a well known task in the field of NLP, particularly in the Social Media domain. Clark and Araki (2011) provides a detailed survey on the challenges and applications of text normalization in Social Media. Researchers have shown that text normalization is a major factor in improving performance of NLP intermediate tasks like part-of-speech tagging (Han et al., 2013) and NLP applications like machine translation (Hassan and Menezes, 2013). Research in text normalization started with spelling correction with noisy channel model (Kernighan et al., 1990; Mays et al., 1991). Since then several different approaches have been proposed by researchers. We report here a few of the most prominent works on text normalization, with a particular focus on social media. 2 (Ling et al., 2015) showed improvement on the machine translation task. Text normalization is conceptually very similar to and can be modelled as a machine translation task using noisy–clean parallel corpus.. S Bandyopadhyay, D S Sharma and R Sangal. Proc. of the 14th Intl. Conference on Natural Language Processing, pages 312–321, c Kolkata, India. December 2017. 2016 NLP Association o"
W17-7539,W15-4323,0,0.0186736,"their system with a finite-state transducer (FST) filter to take care of mistakes made by the RNN based model. Deep Neural Network models suffer from the Out-Of-Vocabulary (OOV) problem (Luong et al., 2014), when text normalization is performed using word based approach. Xie et al. (2016) solved this problem by illustrating how character based neural networks are much better in normalizing noisy and user generated texts. They also showed that results can be improved by introducing synthesized errors in a datasets. They showed improvement using noisy text collected from English learner forum. Leeman-Munk et al. (2015) proposed a model for normalizing noisy text which uses two augmented feed forward networks (Glorot and Bengio, 2010), flagger to identify the word to be normalized and at last a normalizer which provides the correct output for one token at a time. Chollampatt et al. (2016) showed that neural machine translation models are better in correcting grammatical errors, a task closely related to text normalization, as compared to phrase based statistical machine translation models (Wang et al., 2014). Other noticeable works in text normalization include the use of adaptive parser-centric strategy (Zh"
W17-7539,P14-3012,0,0.0195724,"rd to be normalized and at last a normalizer which provides the correct output for one token at a time. Chollampatt et al. (2016) showed that neural machine translation models are better in correcting grammatical errors, a task closely related to text normalization, as compared to phrase based statistical machine translation models (Wang et al., 2014). Other noticeable works in text normalization include the use of adaptive parser-centric strategy (Zhang et al., 2013) to convert noisy texts into grammatically correct texts, unsupervised model using semantic similarity and Re-ranking strategy (Li and Liu, 2014). Toruno˘glu and Eryi˘git (2014) proposed a cascaded approach for normalizing Turkish text by dividing the main problem into sub problems and solving them one by one. Liu (2012) used character-block level SMT to normalize SMS and Twitter text. Pusateri et al. (2017) reports the use of bi-directional LSTM for the task of inverse text normalization, the objective of which is the exact opposite of (Sproat and Jaitly, 2016), i.e. to convert the spoken form token sequence produced by a speech recognizer into written form. 3 3.1 System Architecture Recurrent Neural Network The main motive behind usi"
W17-7539,C12-1097,0,0.0223938,"er in correcting grammatical errors, a task closely related to text normalization, as compared to phrase based statistical machine translation models (Wang et al., 2014). Other noticeable works in text normalization include the use of adaptive parser-centric strategy (Zhang et al., 2013) to convert noisy texts into grammatically correct texts, unsupervised model using semantic similarity and Re-ranking strategy (Li and Liu, 2014). Toruno˘glu and Eryi˘git (2014) proposed a cascaded approach for normalizing Turkish text by dividing the main problem into sub problems and solving them one by one. Liu (2012) used character-block level SMT to normalize SMS and Twitter text. Pusateri et al. (2017) reports the use of bi-directional LSTM for the task of inverse text normalization, the objective of which is the exact opposite of (Sproat and Jaitly, 2016), i.e. to convert the spoken form token sequence produced by a speech recognizer into written form. 3 3.1 System Architecture Recurrent Neural Network The main motive behind using RNN for text normalization is to utilize sequential content. Input to the RNNs is the current information they see as well as the previous information remembered by them at t"
W17-7539,W14-1308,0,0.0420689,"Missing"
W17-7539,W14-1711,0,0.0690927,"in a datasets. They showed improvement using noisy text collected from English learner forum. Leeman-Munk et al. (2015) proposed a model for normalizing noisy text which uses two augmented feed forward networks (Glorot and Bengio, 2010), flagger to identify the word to be normalized and at last a normalizer which provides the correct output for one token at a time. Chollampatt et al. (2016) showed that neural machine translation models are better in correcting grammatical errors, a task closely related to text normalization, as compared to phrase based statistical machine translation models (Wang et al., 2014). Other noticeable works in text normalization include the use of adaptive parser-centric strategy (Zhang et al., 2013) to convert noisy texts into grammatically correct texts, unsupervised model using semantic similarity and Re-ranking strategy (Li and Liu, 2014). Toruno˘glu and Eryi˘git (2014) proposed a cascaded approach for normalizing Turkish text by dividing the main problem into sub problems and solving them one by one. Liu (2012) used character-block level SMT to normalize SMS and Twitter text. Pusateri et al. (2017) reports the use of bi-directional LSTM for the task of inverse text n"
W17-7539,P13-1114,0,0.0175263,"5) proposed a model for normalizing noisy text which uses two augmented feed forward networks (Glorot and Bengio, 2010), flagger to identify the word to be normalized and at last a normalizer which provides the correct output for one token at a time. Chollampatt et al. (2016) showed that neural machine translation models are better in correcting grammatical errors, a task closely related to text normalization, as compared to phrase based statistical machine translation models (Wang et al., 2014). Other noticeable works in text normalization include the use of adaptive parser-centric strategy (Zhang et al., 2013) to convert noisy texts into grammatically correct texts, unsupervised model using semantic similarity and Re-ranking strategy (Li and Liu, 2014). Toruno˘glu and Eryi˘git (2014) proposed a cascaded approach for normalizing Turkish text by dividing the main problem into sub problems and solving them one by one. Liu (2012) used character-block level SMT to normalize SMS and Twitter text. Pusateri et al. (2017) reports the use of bi-directional LSTM for the task of inverse text normalization, the objective of which is the exact opposite of (Sproat and Jaitly, 2016), i.e. to convert the spoken for"
W18-6455,W16-2302,0,0.0287897,"Missing"
W18-6455,W16-2339,0,0.0353419,"Missing"
W18-6455,W16-2343,0,0.0269996,"Missing"
W18-6455,P02-1040,0,0.113719,"ly for those words in the hypothesis which find a stem match in the reference. Although TER outperforms WER, the normalization of the WER metric is the basis of our metric, i.e., Introduction There has been several efforts to introduce better automatic evaluation metrics that can help towards the growth of machine translation (MT) systems. Human evaluation is slow and expensive and thereby efficient automatic MT evaluation metrics are required which are faster and correlate strongly with human judgements. Over the years a number of automatic MT evaluation metrics have been proposed like BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), Translation Edit Rate (Snover et al., 2002), NIST (Doddington, 2002), etc., which are widely used in the MT research and development community. However, due to its due to its simplicity and easier interpretability, Translation Edit Rate, or Translation Error Rate (TER), is one of the most commonly used MT evaluation metrics and often it is used as a baseline evaluation metric by MT researchers. In this work, we propose a new MT Work Related Work done while at Jadavpur University. 746 Proceedings of the Third Conference on Machine Translation (WMT), Volume"
W18-6455,2006.amta-papers.25,0,0.136685,"es on the segment-level for various language pairs. Abstract The paper presents our participation in the WMT 2018 Metrics Shared Task. We propose an improved version of Translation Edit/Error Rate (TER). In addition to including the basic edit operations in TER, namely - insertion, deletion, substitution and shift, our metric also allows stem matching, optimizable edit costs and better normalization so as to correlate better with human judgement scores. The proposed metric shows much higher correlation with human judgments than TER. 1 2 The proposed metric is based on and an extension of TER (Snover et al., 2006), one of the most popular MT evaluation metrics. TER is an edit distance style error metric and it provides an edit ratio (often referred to as edit rate or error rate) in terms of how much editing is required to transform the MT output (also known as hypothesis) into a human translation (reference translation) with respect to the average length of the references. The term average is defined in case of multiple references, where normalization is done over the closest reference. The required editing is measured in terms of four edit operations - insertion, deletion, substitution and shifting. O"
W18-6455,W16-2342,0,0.202856,"t ratio (often referred to as edit rate or error rate) in terms of how much editing is required to transform the MT output (also known as hypothesis) into a human translation (reference translation) with respect to the average length of the references. The term average is defined in case of multiple references, where normalization is done over the closest reference. The required editing is measured in terms of four edit operations - insertion, deletion, substitution and shifting. Other related work relevant to our metric includes word error rate (WER) (Zechner and Waibel, 2000) and CharacTER (Wang et al., 2016). WER is the basis of TER and, unlike TER, it does not include the ‘shift’ operation. Both WER and TER consider word level edit operations. CharacTER is character level TER which calculates the edit distance at character level while performing the shift operations at word level. Our work is different from CharacTER since we allow edit operations at character level only for those words in the hypothesis which find a stem match in the reference. Although TER outperforms WER, the normalization of the WER metric is the basis of our metric, i.e., Introduction There has been several efforts to intro"
W18-6455,A00-2025,0,0.0111068,"style error metric and it provides an edit ratio (often referred to as edit rate or error rate) in terms of how much editing is required to transform the MT output (also known as hypothesis) into a human translation (reference translation) with respect to the average length of the references. The term average is defined in case of multiple references, where normalization is done over the closest reference. The required editing is measured in terms of four edit operations - insertion, deletion, substitution and shifting. Other related work relevant to our metric includes word error rate (WER) (Zechner and Waibel, 2000) and CharacTER (Wang et al., 2016). WER is the basis of TER and, unlike TER, it does not include the ‘shift’ operation. Both WER and TER consider word level edit operations. CharacTER is character level TER which calculates the edit distance at character level while performing the shift operations at word level. Our work is different from CharacTER since we allow edit operations at character level only for those words in the hypothesis which find a stem match in the reference. Although TER outperforms WER, the normalization of the WER metric is the basis of our metric, i.e., Introduction There"
W18-6457,C04-1046,0,0.101233,"cument-to-Vector (Doc2Vec) model. In the BoW model, we compute the cosine similarity while in the Doc2Vec model we consider the Doc2Vec similarity. By applying the Kneedle algorithm on the F1mult vs. similarity score plot, we derive the threshold based on which OK/BAD decisions are taken for the MT words. Experimental results revealed that the Doc2Vec model performs better than the BoW model on the word level QE task. 1 Introduction Evaluating and estimating quality of a machine translation (MT) system without referring the actual translation is now one of the key research areas in MT domain (Blatz et al., 2004; Specia et al., 2009). In a machine translated document quality estimation can be performed at various granularities like word level, phrase level or sentence level (Specia et al., 2010, 2013). Scarton et al. (2016) produced their task in WMT16 in document level quality estimation with winning result in two different models (Bojar et al., 2016). One model used discourse features and SVR and another model employed word embedding feature and Gaussian Process for quality estimation. (Bic¸ici, 2017) predicted translation performance with referential translation machines at word level, sentence le"
W18-6457,W17-4759,0,0.191822,"Missing"
W18-6457,W16-2391,0,0.0204848,"ore plot, we derive the threshold based on which OK/BAD decisions are taken for the MT words. Experimental results revealed that the Doc2Vec model performs better than the BoW model on the word level QE task. 1 Introduction Evaluating and estimating quality of a machine translation (MT) system without referring the actual translation is now one of the key research areas in MT domain (Blatz et al., 2004; Specia et al., 2009). In a machine translated document quality estimation can be performed at various granularities like word level, phrase level or sentence level (Specia et al., 2010, 2013). Scarton et al. (2016) produced their task in WMT16 in document level quality estimation with winning result in two different models (Bojar et al., 2016). One model used discourse features and SVR and another model employed word embedding feature and Gaussian Process for quality estimation. (Bic¸ici, 2017) predicted translation performance with referential translation machines at word level, sentence level 759 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 759–764 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistic"
W18-6457,2006.amta-papers.25,0,0.0789424,"l Si,k ∈ srci do Blist .add(Si,k ) end Bdict [Ti,j ].add(Blist ) end end end return Bdict end tion (QE task-2) on English–German (IT domain) SMT data. The proposed model has been developed in two ways - one using the standard Bagof-Words model and another using the Doc2Vec model. The motivation behind the use of Doc2Vec model is to achieve more accurate semantic similarity compared to the simple cosine similarity on Bag-of-Words model. The Doc2Vec model captures semantic similarity which the Bag-of-Words model can not. Our word level error estimation is mainly based on Translation Error Rate (Snover et al., 2006) between MT and PE. 2 Proposed Approach Our system highlights the retention of a word in MT translation and thus it helps human posteditors to increase their productivity with less effort. Our QE system is built over the Translation Error Rate (TER) (Snover et al., 2006) alignment between MT output and the corresponding PE output in the training data. TER alignment shows whether words from MT data (hypothesis in TER) will be continued, deleted or substituted with respect to the PE data (reference in TER). Based on the TER alignment, we build binary classification models that suggests OK for co"
W18-6457,P13-4014,0,0.0347432,"Missing"
W18-6457,2009.eamt-1.5,0,0.0382457,"c2Vec) model. In the BoW model, we compute the cosine similarity while in the Doc2Vec model we consider the Doc2Vec similarity. By applying the Kneedle algorithm on the F1mult vs. similarity score plot, we derive the threshold based on which OK/BAD decisions are taken for the MT words. Experimental results revealed that the Doc2Vec model performs better than the BoW model on the word level QE task. 1 Introduction Evaluating and estimating quality of a machine translation (MT) system without referring the actual translation is now one of the key research areas in MT domain (Blatz et al., 2004; Specia et al., 2009). In a machine translated document quality estimation can be performed at various granularities like word level, phrase level or sentence level (Specia et al., 2010, 2013). Scarton et al. (2016) produced their task in WMT16 in document level quality estimation with winning result in two different models (Bojar et al., 2016). One model used discourse features and SVR and another model employed word embedding feature and Gaussian Process for quality estimation. (Bic¸ici, 2017) predicted translation performance with referential translation machines at word level, sentence level 759 Proceedings of"
W18-6457,P10-1040,0,0.0542079,"anu.pal@uni-saarland.de, sudip.naskar@jdvu.ac.in Abstract and at phrase level. (Blain et al., 2017) submitted task on bi-lexical word embedding in WMT17 QE shared task, which produced promising results in sentence level Quality Estimation. Some studies (Fiederer and OBrien, 2009; Koehn, 2009; DePalma and Kelly, 2011; Zampieri and Vela, 2014) show that the quality of MT output along with PE can produce better result than human editor in certain situations. In our work we mainly focus on word level quality estimation. The distributional structure of words was first described by (Harris, 1954). (Turian et al., 2010) illustrated representations of words in semi-supervised learning. Bengio et al. (2003) proposed neural probabilistic language model by using a distributed representation of words. Collobert and Weston (2008), described how a convolutional neural network architecture could be used to make different language processing predictions, such as semantically similar words, etc. Mnih and Hinton (2008) proposed a fast hierarchical language model along with a feature based algorithm which automatically builds word trees from data. Mikolov et al. (2013b) proposed vector representation of words with the h"
W18-6457,W14-0314,0,0.0233394,"Prasenjit Basu1 , Santanu Pal2,3 , Sudip Kumar Naskar4 1 Future Institute of Engineering and Management, India 2 Saarland University, Germany, 3 German Research Center for Artificial Intelligence (DFKI), Saarbr¨ucken, Germany 4 Jadavpur University, India basuprasen@gmail.com, santanu.pal@uni-saarland.de, sudip.naskar@jdvu.ac.in Abstract and at phrase level. (Blain et al., 2017) submitted task on bi-lexical word embedding in WMT17 QE shared task, which produced promising results in sentence level Quality Estimation. Some studies (Fiederer and OBrien, 2009; Koehn, 2009; DePalma and Kelly, 2011; Zampieri and Vela, 2014) show that the quality of MT output along with PE can produce better result than human editor in certain situations. In our work we mainly focus on word level quality estimation. The distributional structure of words was first described by (Harris, 1954). (Turian et al., 2010) illustrated representations of words in semi-supervised learning. Bengio et al. (2003) proposed neural probabilistic language model by using a distributed representation of words. Collobert and Weston (2008), described how a convolutional neural network architecture could be used to make different language processing pre"
W19-5332,D18-1399,0,0.115166,"ta is not enough to train a neural system for such a low resource language pair. Therefore, preparation for large volume of parallel corpus is required which can be produced either by manual translation by professional translators or scraping parallel data from the internet. However, these processes are costly, tedious and sometimes inefficient (in case of scraping from internet). As the released data was insufficient, to generate more training data, we use back-translation. For back-translation we applied two methods, first, using unsupervised statistical machine translation as described in (Artetxe et al., 2018) and second, using Doc translation API1 (The API uses Google translator as of April 2019). We have explained the extraction of sentences and the corresponding results using the above methods in section 4.2. The synthetic dataset which we have generated can be found here.2 192,367 64,346 219,654 1,998 1,016 998 Table 1: Data Statistics of WMT 2019 English– Gujarati translation shared task. Russian. Sennrich et al. (Sennrich et al., 2016a) shown how back-translation of monolingual data can improve the NMT system. Ramesh et al. (Ramesh and Sankaranarayanan, 2018) demonstrated how an existing mode"
W19-5332,1983.tc-1.13,0,0.639264,"Missing"
W19-5332,2000.eamt-1.5,0,0.626376,"Missing"
W19-5332,N18-4016,0,0.0187063,"atistical machine translation as described in (Artetxe et al., 2018) and second, using Doc translation API1 (The API uses Google translator as of April 2019). We have explained the extraction of sentences and the corresponding results using the above methods in section 4.2. The synthetic dataset which we have generated can be found here.2 192,367 64,346 219,654 1,998 1,016 998 Table 1: Data Statistics of WMT 2019 English– Gujarati translation shared task. Russian. Sennrich et al. (Sennrich et al., 2016a) shown how back-translation of monolingual data can improve the NMT system. Ramesh et al. (Ramesh and Sankaranarayanan, 2018) demonstrated how an existing model like bidirectional recurrent neural network can be used to generate parallel sentences for non-English languages like English-Tamil and English-Hindi, which belong to low-resource language pair, to improve the SMT and the NMT systems. Choudhary et al. (Choudhary et al., 2018) has shown how to build NMT system for low resource parallel corpus language pair like English-Tamil using techniques like word embeddings and Byte-PairEncoding (Sennrich et al., 2016b) to handle OutOf-Vocabulary Words. 3 Data Preparation For our experiments we used both parallel and mon"
W19-5332,W18-6459,0,0.0163515,"n be found here.2 192,367 64,346 219,654 1,998 1,016 998 Table 1: Data Statistics of WMT 2019 English– Gujarati translation shared task. Russian. Sennrich et al. (Sennrich et al., 2016a) shown how back-translation of monolingual data can improve the NMT system. Ramesh et al. (Ramesh and Sankaranarayanan, 2018) demonstrated how an existing model like bidirectional recurrent neural network can be used to generate parallel sentences for non-English languages like English-Tamil and English-Hindi, which belong to low-resource language pair, to improve the SMT and the NMT systems. Choudhary et al. (Choudhary et al., 2018) has shown how to build NMT system for low resource parallel corpus language pair like English-Tamil using techniques like word embeddings and Byte-PairEncoding (Sennrich et al., 2016b) to handle OutOf-Vocabulary Words. 3 Data Preparation For our experiments we used both parallel and monolingual corpus released by the WMT 2019 Organizers. We back-translate the monolingual corpus and use it as additional synthetic parallel corpus to train our NMT system. The detailed statistics of the corpus is given in Table 1. We performed our experiments on two datasets, one using the parallel corpus provide"
W19-5332,P16-1009,0,0.200866,"training data, we use back-translation. For back-translation we applied two methods, first, using unsupervised statistical machine translation as described in (Artetxe et al., 2018) and second, using Doc translation API1 (The API uses Google translator as of April 2019). We have explained the extraction of sentences and the corresponding results using the above methods in section 4.2. The synthetic dataset which we have generated can be found here.2 192,367 64,346 219,654 1,998 1,016 998 Table 1: Data Statistics of WMT 2019 English– Gujarati translation shared task. Russian. Sennrich et al. (Sennrich et al., 2016a) shown how back-translation of monolingual data can improve the NMT system. Ramesh et al. (Ramesh and Sankaranarayanan, 2018) demonstrated how an existing model like bidirectional recurrent neural network can be used to generate parallel sentences for non-English languages like English-Tamil and English-Hindi, which belong to low-resource language pair, to improve the SMT and the NMT systems. Choudhary et al. (Choudhary et al., 2018) has shown how to build NMT system for low resource parallel corpus language pair like English-Tamil using techniques like word embeddings and Byte-PairEncoding"
W19-5332,W14-3308,0,0.0232331,"ne translation (MT) that uses artificial neural network to directly model the conditional probability p(y|x) of translating a source sentence (x1 ,x2 ,...,xn ) into a target sentence (y1 ,y2 ,...,ym ). NMT has consistently performed better than the phrase-based statistical MT (PB-SMT) approaches and has provided state-ofthe-art results in the last few years. However, one of the major constraints of using supervised NMT is that it is not suitable for low resource language pairs. Thus, to use supervised NMT, low resource pairs need to resort to other techniques 2 Related Works Dungarwal et al. (Dungarwal et al., 2014) developed a statistical method for machine translation, where phrase based method for Hindi-English and factored based method for English-Hindi SMT system was used. They had shown improvements to the existing SMT systems using pre-procesing and post-processing components that generated morphological inflections correctly. Imankulova et al. (Imankulova et al., 2017) showed how backtranslation and filtering from monolingual data can be used to build an effective translation system for a low-resourse language pair like Japanese∗ These three authors have contributed equally. 308 Proceedings of th"
W19-5332,P16-1162,0,0.189162,"training data, we use back-translation. For back-translation we applied two methods, first, using unsupervised statistical machine translation as described in (Artetxe et al., 2018) and second, using Doc translation API1 (The API uses Google translator as of April 2019). We have explained the extraction of sentences and the corresponding results using the above methods in section 4.2. The synthetic dataset which we have generated can be found here.2 192,367 64,346 219,654 1,998 1,016 998 Table 1: Data Statistics of WMT 2019 English– Gujarati translation shared task. Russian. Sennrich et al. (Sennrich et al., 2016a) shown how back-translation of monolingual data can improve the NMT system. Ramesh et al. (Ramesh and Sankaranarayanan, 2018) demonstrated how an existing model like bidirectional recurrent neural network can be used to generate parallel sentences for non-English languages like English-Tamil and English-Hindi, which belong to low-resource language pair, to improve the SMT and the NMT systems. Choudhary et al. (Choudhary et al., 2018) has shown how to build NMT system for low resource parallel corpus language pair like English-Tamil using techniques like word embeddings and Byte-PairEncoding"
W19-5332,W17-5704,0,0.0159843,"r, one of the major constraints of using supervised NMT is that it is not suitable for low resource language pairs. Thus, to use supervised NMT, low resource pairs need to resort to other techniques 2 Related Works Dungarwal et al. (Dungarwal et al., 2014) developed a statistical method for machine translation, where phrase based method for Hindi-English and factored based method for English-Hindi SMT system was used. They had shown improvements to the existing SMT systems using pre-procesing and post-processing components that generated morphological inflections correctly. Imankulova et al. (Imankulova et al., 2017) showed how backtranslation and filtering from monolingual data can be used to build an effective translation system for a low-resourse language pair like Japanese∗ These three authors have contributed equally. 308 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 308–313 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics Dataset Parallel Corpora Cleaned Parallel Corpora Back-translated Data Development Data Gujarati Test Data English Test Data Pairs is important in the splitting part too as it is impo"
W19-5332,2006.amta-papers.25,0,0.0451986,"ask for English–Gujarati and Gujarati–English. The released training data set is completely different in-domain compared to the development set and the size is not anywhere close to the sizable amount of training data which is typically required for the success of NMT systems. We use additional synthetic data produced through backtranslation from the monolingual corpus. This provides significant improvements in translation performance for both our English–Gujarati and Gujarati–English NMT systems. Our English– Gujarati system was ranked second in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) in the shared task. In this paper we describe our joint submission (JU-Saarland) from Jadavpur University and Saarland University in the WMT 2019 news translation shared task for English–Gujarati language pair within the translation task subtrack. Our baseline and primary submissions are built using a Recurrent neural network (RNN) based neural machine translation (NMT) system which follows attention mechanism followed by fine-tuning using in-domain data. Given the fact that the two languages belong to different language families and there is not enough parallel data for this language pair, b"
W19-5332,P17-4012,0,0.0223876,"s initially set to 1.0. Table 2 shows the hyper-parameter configurations for our Gujarati–English translation system. We initially trained our model with the cleaned parallel corpus provided by WMT 2019 up to 100K training steps. Thereafter, we fine-tune our generic model on domain specific corpus (containing 219K sentences back-translated using Doc Translator API) changing the learning rate to 0.5 and decay started from 130K training steps with a decay factor of 0.5 and keeping the other hyperparameters same as mentioned in Table 2. Data Postprocessing Postprocessing, such as detokenization (Klein et al., 2017), punctuation normalization4 (Koehn et al., 2007), was performed on our translated data (on the test set) to produce the final translated data. 4 Primary System description Experiment Setup We have explained our experimental setups in the next two sections. The first section contains the setup used for our final submission and the next section describes all the other supporting experimental setups. We use the OpenNMT toolkit (Klein et al., 2017) for our experiments. We performed several experiments where the parallel corpus is sent to the model as space separated character format, space separa"
W19-5332,P07-2045,0,0.00941137,"nslation pairs makes the model prone to overfitting and hence prevents it from recognizing new features. Thus, one of the sentence pair is kept while the other redundant pairs are removed. Some sentence pairs had combinations of both language pairs which were also identified as redundant. These pairs strictly need elimination as the vocabularies of the individual languages consist of alphanumeric characters of the other language which results in inconsistent encoding and decoding during encoderdecoder application steps on the considered language pair. We tokenize the English side using Moses (Koehn et al., 2007) tokenizer and for Gujarati, we use the Indic NLP library tokenization tool3 . Punctuation normalization was also done. 3.2 Value text fp32 2 8 500 256 160,000 50,000 50,000 warm-up+decay* softmax wordpiece LSTM Table 2: Hyper-parameter configurations for Gujarati– English translation using unidirectional RNN (Cho et al., 2014)), *learning-rate was initially set to 1.0. Table 2 shows the hyper-parameter configurations for our Gujarati–English translation system. We initially trained our model with the cleaned parallel corpus provided by WMT 2019 up to 100K training steps. Thereafter, we fine-t"
W19-5332,P02-1040,0,0.104065,"the WMT 2019 news translation task for English–Gujarati and Gujarati–English. The released training data set is completely different in-domain compared to the development set and the size is not anywhere close to the sizable amount of training data which is typically required for the success of NMT systems. We use additional synthetic data produced through backtranslation from the monolingual corpus. This provides significant improvements in translation performance for both our English–Gujarati and Gujarati–English NMT systems. Our English– Gujarati system was ranked second in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) in the shared task. In this paper we describe our joint submission (JU-Saarland) from Jadavpur University and Saarland University in the WMT 2019 news translation shared task for English–Gujarati language pair within the translation task subtrack. Our baseline and primary submissions are built using a Recurrent neural network (RNN) based neural machine translation (NMT) system which follows attention mechanism followed by fine-tuning using in-domain data. Given the fact that the two languages belong to different language families and there is not enough parallel"
W19-6702,C14-2028,0,0.137444,", CATaLog Online uses the Nutch2 information retrieval (IR) system. Nutch follows the standard IR model of Lucene3 with document parsing, document Indexing, TF-IDF calculation, query parsing and finally searching/document retrieval and document ranking. In our implementation, each document contains (a) a TM source segment, (b) its corresponding translation and (c) the word alignments. Machine Translation and Automatic Post Editing Along with TM matches, CATaLog Online provides MT output (Pal et al., 2015a) to the translator, an option provided by many state-of-the-art CAT tools (e.g. MateCat (Federico et al., 2014)). Besides the retrieved TM segment and the MT output CATaLog Online provides also a third option to the translator: the output of an automatic post-editing system meant to be post-edited as the MT output. The APE system is based in an OSM model (Pal et al., 2016b) and proved to deliver competitive performance in previous editions of the Automatic Post Editing (APE) shared task at WMT Bojar et al. (2016). 2 3 http://nutch.apache.org/ http://lucene.apache.org/ Proceedings of MT Summit XVII, volume 2 Editing Logs For a given input segment, CATaLog Online provides four different options: TM, MT,"
W19-6702,W15-4905,1,0.909403,"Missing"
W19-6702,W12-3123,0,0.598965,"Missing"
W19-6702,W15-5206,1,0.888157,"Missing"
W19-6702,W15-3017,1,0.90096,"Missing"
W19-6702,W15-3026,1,0.902711,"Missing"
W19-6702,L16-1095,1,0.855593,"Missing"
W19-6702,W16-2379,1,0.892057,"Missing"
W19-6702,2015.tc-1.15,0,0.150837,"Missing"
W19-6702,W14-0314,1,0.712543,"the users preferred using CATaLog Online over existing CAT tools in some respects, especially by selecting the output of the MT system and taking advantage of the color scheme for TM suggestions. 1 Introduction The use of computer software is an important part of the modern translation workflow (Zaretskaya et al., 2015; Schneider et al., 2019). A number of tools are widely used by professional translators, most notably CAT tools and terminology management software. These tools increase translators’ productivity, improve consistency in translation and, in turn, reduce the cost of translation (Zampieri and Vela, 2014). The most important compo© 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CC-BY-ND. Proceedings of MT Summit XVII, volume 2 nent in state-of-the-art CAT tools are translation memories (TM). The translators can either accept, reject or modify the suggestions received from the TM engine. As the process is done iteratively, every new translation increases the size of the translation memory making it more useful for future translations. The idea behind TMs is relatively simple, however, the process of matching and retrieval of so"
W19-6702,2015.eamt-1.6,1,\N,Missing
Y09-1019,W05-0909,0,0.059657,"Missing"
Y09-1019,P07-1020,0,0.209869,"Missing"
Y09-1019,J96-1002,0,0.0373506,"describe the features used in the experiments, and the pre-processing required. Section 7 presents the results obtained, and offers some analysis. In Section 8 we formulate our conclusions, and offer some avenues for further work. 2 Related Work Brown et al. (1991) were the first to propose the use of dedicated WSD models in word-based SMT systems. Results were limited to the case of binary disambiguation, i.e., deciding between only two possible translation candidates, and to a reduced set of common words. A significant improvement in translation was reported according to manual evaluation. Berger et al. (1996) suggested context-sensitive modeling of word translations in order to integrate local contextual information into their IBM translation models using a Maximum Entropy (MaxEnt) model, but the work is not supported by any significant evaluation results. García Varea et al. (2001) present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Using such a model on the German—English Verbmobil corpus, they obtained better alignment quality in terms of improved alignment error rate (AER)."
Y09-1019,J93-2003,0,0.0107764,"ng. In the second group, predictions are allowed to interact with other models (e.g., language, distortion, additional translation models etc.) during decoding time. The present work falls into the second type of interaction methods. 3 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation e1I = e1 . . . e I of a source sentence f1J = f1 . . . f J is chosen to maximize (1): arg max P(e1I |f1J ) = arg max P( f 1J |e1I ) P(e1I ) I ,e1I (1) I ,e1I where P( f 1J |e1I ) and P(e1I ) denote respectively the translation model and the target-language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( f 1I |e1J ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): M log P ( e1I |f 1 J ) = ∑λ I m h m ( f 1 J , e1I , s1K ) + λLM log P(e1 ) (2) m =1 where s1K = s1...sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (eˆ ,..., eˆ ) and ( fˆ ,..., fˆ ) such that (we set i0 = 0) (3): 1 k ∀1 ≤ k ≤ K , sk = (ik ; bk, jk), 1 k eˆk = eik−1 +1...eik , fˆk = f b ... f j k k ("
Y09-1019,H91-1025,0,0.219377,"79 170 The remainder of the paper is organized as follows. In Section 2 we discuss related work. Section 3 provides a brief overview of PB-SMT. In Section 4 we describe how we model dependency information as context-informed features in our baseline log-linear PB-SMT system. Section 5 describes the memory-based classification approach. In Section 6 we describe the features used in the experiments, and the pre-processing required. Section 7 presents the results obtained, and offers some analysis. In Section 8 we formulate our conclusions, and offer some avenues for further work. 2 Related Work Brown et al. (1991) were the first to propose the use of dedicated WSD models in word-based SMT systems. Results were limited to the case of binary disambiguation, i.e., deciding between only two possible translation candidates, and to a reduced set of common words. A significant improvement in translation was reported according to manual evaluation. Berger et al. (1996) suggested context-sensitive modeling of word translations in order to integrate local contextual information into their IBM translation models using a Maximum Entropy (MaxEnt) model, but the work is not supported by any significant evaluation re"
Y09-1019,I05-2021,0,0.158454,"-SMT systems, improved AER scores do not necessarily result in improved translation quality, as noted by a number of researchers. Vickrey et al. (2005) built classifiers inspired by those used in WSD to fill in any blanks in a partially completed translation. Giménez and Màrquez (2007) extended this work by considering the more general case of frequent phrases and moved to full translation rather than blank-filling on the target side. Attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches of integrating state-of-theart WSD methods into SMT to improve the overall translation quality have met with more success (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007, 2009). Recently, Bangalore et al. (2008) employed an SMT architecture based on stochastic finitestate transducers that addresses global lexical selection, i.e. dedicated word selection. Specia et al. (2008) use dedicated predictions for the re-ranking of n-best translations, limited to a small set of words from different grammatical categories. Significant BLEU improvement"
Y09-1019,2007.mtsummit-papers.11,0,0.220443,"ses. Approaches to include source context for proper selection of target phrases have been inspired by methods for word sense disambiguation (WSD), that employ rich context-sensitive features to determine the contextually most likely sense of a polysemous word. These contextual features may include lexical features of words appearing in the context and bearing sensediscriminatory information, position-specific neighbouring words (Giménez and Márquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features of the sentential context (Gimpel and Smith, 2008) and full sentential context (Carpuat and Wu, 2007). Most of the work on syntactic features has made use of part-of-speech taggers (Stroppa et al., 2007), supertaggers (Haque et al., 2009) and shallow and deep syntactic parsers (Gimpel and Smith, 2008). In the present work, we explore how the local sentential context information from a dependency parse can be modeled as source context features to be integrated into a PB-SMT model. Copyright 2009 by Rejwanul Haque, Sudip Kumar Naskar, Antal van den Bosch, and Andy Way 23rd Pacific Asia Conference on Language, Information and Computation, pages 170–179 170 The remainder of the paper is organized"
Y09-1019,P07-1005,0,0.234639,"to fill in any blanks in a partially completed translation. Giménez and Màrquez (2007) extended this work by considering the more general case of frequent phrases and moved to full translation rather than blank-filling on the target side. Attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches of integrating state-of-theart WSD methods into SMT to improve the overall translation quality have met with more success (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007, 2009). Recently, Bangalore et al. (2008) employed an SMT architecture based on stochastic finitestate transducers that addresses global lexical selection, i.e. dedicated word selection. Specia et al. (2008) use dedicated predictions for the re-ranking of n-best translations, limited to a small set of words from different grammatical categories. Significant BLEU improvements were reported in both approaches. Hasan et al. (2008) present target context modeling into SMT using a triplet lexicon model that captures long-distance (global) dependencies. Their approach is"
Y09-1019,W06-1628,0,0.185399,"Missing"
Y09-1019,P01-1027,0,0.430501,"Missing"
Y09-1019,W07-0719,0,0.377407,"p a refined context-dependent lexicon model. Using such a model on the German—English Verbmobil corpus, they obtained better alignment quality in terms of improved alignment error rate (AER). However, since alignment is not an end task in itself and is most often used as an intermediate task to generate phrase pairs for the t-tables in PB-SMT systems, improved AER scores do not necessarily result in improved translation quality, as noted by a number of researchers. Vickrey et al. (2005) built classifiers inspired by those used in WSD to fill in any blanks in a partially completed translation. Giménez and Màrquez (2007) extended this work by considering the more general case of frequent phrases and moved to full translation rather than blank-filling on the target side. Attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches of integrating state-of-theart WSD methods into SMT to improve the overall translation quality have met with more success (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007, 2009). Recently, Bangalore et al. (2008)"
Y09-1019,W08-0302,0,0.104114,"influence the weighting and selection of target phrases. Approaches to include source context for proper selection of target phrases have been inspired by methods for word sense disambiguation (WSD), that employ rich context-sensitive features to determine the contextually most likely sense of a polysemous word. These contextual features may include lexical features of words appearing in the context and bearing sensediscriminatory information, position-specific neighbouring words (Giménez and Márquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features of the sentential context (Gimpel and Smith, 2008) and full sentential context (Carpuat and Wu, 2007). Most of the work on syntactic features has made use of part-of-speech taggers (Stroppa et al., 2007), supertaggers (Haque et al., 2009) and shallow and deep syntactic parsers (Gimpel and Smith, 2008). In the present work, we explore how the local sentential context information from a dependency parse can be modeled as source context features to be integrated into a PB-SMT model. Copyright 2009 by Rejwanul Haque, Sudip Kumar Naskar, Antal van den Bosch, and Andy Way 23rd Pacific Asia Conference on Language, Information and Computation, pages"
Y09-1019,2009.eamt-1.32,1,0.872115,"n (WSD), that employ rich context-sensitive features to determine the contextually most likely sense of a polysemous word. These contextual features may include lexical features of words appearing in the context and bearing sensediscriminatory information, position-specific neighbouring words (Giménez and Márquez, 2007; Stroppa et al., 2007), shallow and deep syntactic features of the sentential context (Gimpel and Smith, 2008) and full sentential context (Carpuat and Wu, 2007). Most of the work on syntactic features has made use of part-of-speech taggers (Stroppa et al., 2007), supertaggers (Haque et al., 2009) and shallow and deep syntactic parsers (Gimpel and Smith, 2008). In the present work, we explore how the local sentential context information from a dependency parse can be modeled as source context features to be integrated into a PB-SMT model. Copyright 2009 by Rejwanul Haque, Sudip Kumar Naskar, Antal van den Bosch, and Andy Way 23rd Pacific Asia Conference on Language, Information and Computation, pages 170–179 170 The remainder of the paper is organized as follows. In Section 2 we discuss related work. Section 3 provides a brief overview of PB-SMT. In Section 4 we describe how we model d"
Y09-1019,D08-1039,0,0.0899696,"integrating state-of-theart WSD methods into SMT to improve the overall translation quality have met with more success (Carpuat and Wu, 2007; Chan et al., 2007; Giménez and Màrquez, 2007, 2009). Recently, Bangalore et al. (2008) employed an SMT architecture based on stochastic finitestate transducers that addresses global lexical selection, i.e. dedicated word selection. Specia et al. (2008) use dedicated predictions for the re-ranking of n-best translations, limited to a small set of words from different grammatical categories. Significant BLEU improvements were reported in both approaches. Hasan et al. (2008) present target context modeling into SMT using a triplet lexicon model that captures long-distance (global) dependencies. Their approach is evaluated in a re-ranking framework; slight improvements are observed over IBM model 1 in terms of BLEU and TER (Snover et al., 2006). Target-language models arguably play the most significant role in today’s PB-SMT systems. However, for some time now people have believed that some incorporation of source language information into SMT systems was bound to help. Stroppa et al. (2007) added source-side contextual features to a state-of-the-art log-linear PB"
Y09-1019,N03-1017,0,0.0255589,"Missing"
Y09-1019,P07-2045,0,0.00699313,"features from the head-words of the SMT phrases, identified from the dependency graph generated for the source sentence (as described earlier in Section 4). (ii) They filter out phrases from phrase table entries for which P( eˆk |fˆk ) < 0.0002. In contrast, we keep all phrase pairs for more discrimination. (iii) Their experimental data contains 95K English-to-French training pairs, while we trained our models on about three times as many (286K) Dutch-to-English translation pairs, a less explored direction. 6.2 Pre-processing As (Stroppa et al., 2007) point out, PB-SMT decoders such as Moses (Koehn et al., 2007) rely on a static phrase table, represented as a list of aligned phrases accompanied by several estimated metrics. Since these features do not express the context information in which those phrases occur, no dependency information is kept in the phrase table, and there is no way to recover this information from the phrase table. In order to take into account the dependency information features within such decoders, the test text to be translated is pre-processed. Each word appearing in the test set (and, during development, the development set) is assigned a unique identifier. First we prepare"
Y09-1019,P06-1096,0,0.0168677,"es learned using decision trees. They considered up to two words and/or POS tags on either side of the source focus word as contextual features. In order to overcome problems of estimation of such features, they used a decision-tree classifier (Daelemans et al., 2005) that implicitly smoothes the probability estimates. Significant improvements over a baseline state-of-the-art PB-SMT system were obtained on Italian— English and Chinese—English IWSLT tasks. Several proposals have recently been made to fully exploit the accuracy and the flexibility of discriminative learning (Cowan et al., 2006; Liang et al., 2006). Work of this type generally 171 requires a redefinition of the training procedure; in contrast, our approach introduces new features while retaining the strength of existing state-of-the-art systems. Like the work of (Max et al., 2008), the present work is directly motivated by and is an extension of the approach of (Stroppa et al., 2007). The work of both (Max et al., 2008) and (Gimpel and Smith, 2008) focuses on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for English-to-German, (Max et al., 2008) conduct experiments fro"
Y09-1019,2008.eamt-1.17,0,0.39666,"fier (Daelemans et al., 2005) that implicitly smoothes the probability estimates. Significant improvements over a baseline state-of-the-art PB-SMT system were obtained on Italian— English and Chinese—English IWSLT tasks. Several proposals have recently been made to fully exploit the accuracy and the flexibility of discriminative learning (Cowan et al., 2006; Liang et al., 2006). Work of this type generally 171 requires a redefinition of the training procedure; in contrast, our approach introduces new features while retaining the strength of existing state-of-the-art systems. Like the work of (Max et al., 2008), the present work is directly motivated by and is an extension of the approach of (Stroppa et al., 2007). The work of both (Max et al., 2008) and (Gimpel and Smith, 2008) focuses on language pairs where the target is not English. While (Gimpel and Smith, 2008) are unable to show any improvements for English-to-German, (Max et al., 2008) conduct experiments from English-to-French. Using the same sorts of local contextual features as (Stroppa et al., 2007), as well as using broader context in addition to grammatical dependency information, (Max et al., 2008) show modest gains over a PB-SMT base"
Y09-1019,P03-1021,0,0.0118444,"ized, these weights can be seen as the posterior probabilities of the target phrases eˆ k , which thus give access to P( eˆk |fˆk ,DI( fˆk )). Therefore, the expected feature is derived as in (7): hˆmbl = log P( eˆk |fˆk ,DI( fˆk )) (7) In addition to the above feature, we derived a simple binary feature hˆbest . The feature hˆbest is defined as in (8): hˆbest = 1 0 if eˆk maximizes P( eˆk |fˆk ,CI( fˆk )) otherwise, (8) We performed experiments by integrating these two features hˆmbl and hˆbest directly into the log-linear model. Their weights are optimized using minimum error-rate training (Och, 2003) on a held-out development set for each of the experiments. Our approach in terms of experimental set-up and classification of a source phrase along with contextual dependency features differs from Stroppa et al., (2009) and Haque et al., (2009) in the following respects: (i) Stroppa et al. (2007) and Haque et al., (2009) integrate local, position-specific contextual features into the log-linear framework. Here we integrate a feature encoding positionindependent dependency information; (ii) Haque et al. (2009) interpolate the context-dependent phrase translation probability with the forward ph"
Y09-1019,P02-1038,0,0.32297,"g time. The present work falls into the second type of interaction methods. 3 Log-Linear PB-SMT Translation is modelled in PB-SMT as a decision process, in which the translation e1I = e1 . . . e I of a source sentence f1J = f1 . . . f J is chosen to maximize (1): arg max P(e1I |f1J ) = arg max P( f 1J |e1I ) P(e1I ) I ,e1I (1) I ,e1I where P( f 1J |e1I ) and P(e1I ) denote respectively the translation model and the target-language model (Brown et al., 1993). In log-linear phrase-based SMT, the posterior probability P( f 1I |e1J ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (2): M log P ( e1I |f 1 J ) = ∑λ I m h m ( f 1 J , e1I , s1K ) + λLM log P(e1 ) (2) m =1 where s1K = s1...sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (eˆ ,..., eˆ ) and ( fˆ ,..., fˆ ) such that (we set i0 = 0) (3): 1 k ∀1 ≤ k ≤ K , sk = (ik ; bk, jk), 1 k eˆk = eik−1 +1...eik , fˆk = f b ... f j k k (3) The translational features depend only on pairs of source/target phrases and do not take into account any context of these phrases, i.e. each feature hm i"
Y09-1019,P02-1040,0,0.0773823,"Missing"
Y09-1019,2006.amta-papers.25,0,0.0152464,"astic finitestate transducers that addresses global lexical selection, i.e. dedicated word selection. Specia et al. (2008) use dedicated predictions for the re-ranking of n-best translations, limited to a small set of words from different grammatical categories. Significant BLEU improvements were reported in both approaches. Hasan et al. (2008) present target context modeling into SMT using a triplet lexicon model that captures long-distance (global) dependencies. Their approach is evaluated in a re-ranking framework; slight improvements are observed over IBM model 1 in terms of BLEU and TER (Snover et al., 2006). Target-language models arguably play the most significant role in today’s PB-SMT systems. However, for some time now people have believed that some incorporation of source language information into SMT systems was bound to help. Stroppa et al. (2007) added source-side contextual features to a state-of-the-art log-linear PB-SMT system by incorporating contextdependent phrasal translation probabilities learned using decision trees. They considered up to two words and/or POS tags on either side of the source focus word as contextual features. In order to overcome problems of estimation of such"
Y09-1019,2007.tmi-papers.28,1,0.904233,"Missing"
Y09-1019,tiedemann-nygaard-2004-opus,0,0.0233867,"l. Thus we create a dynamic phrase table. A lexicalized reordering model is used for all the experiments undertaken on development and test texts. The source phrase in the reordering table is replaced by the sequence of unique identifiers when the new phrase table is created. After replacing all words by their unique identifyers, we perform MERT using our new phrase table to optimize the feature weights. 7 Results and Analysis The experiments were carried out on the Dutch-to-English Open Subtitles corpus,2 which is collected as part of the Opus collection of freely available parallel corpora (Tiedemann and Nygaard, 2004). The corpus contains user-contributed translations of movie subtitles. The training text contains 286,160 sentences; the development set and test set each contain 1,000 sentences. Dutch sentences were parsed using Tadpole 3 , a morphosyntactic analyzer and dependency parser (Van den Bosch et al., 2007). 2 3 http://urd.let.rug.nl/tiedeman/OPUS/OpenSubtitles.php http://ilk.uvt.nl/tadpole/ 175 Table 1: Experiments with words and part-of-speech. Experiments Baseline Word±2 POS±2 POS±2* Word±2+POS±2 BLEU 32.39 32.48 33.07 33.29 32.59 NIST 6.11 6.11 6.13 6.17 6.09 METEOR 55.39 55.72 56.17 55.72 55."
Y09-1019,H05-1097,0,0.0687212,"present a MaxEnt approach to integrate contextual dependencies into the EM algorithm of the statistical alignment model to develop a refined context-dependent lexicon model. Using such a model on the German—English Verbmobil corpus, they obtained better alignment quality in terms of improved alignment error rate (AER). However, since alignment is not an end task in itself and is most often used as an intermediate task to generate phrase pairs for the t-tables in PB-SMT systems, improved AER scores do not necessarily result in improved translation quality, as noted by a number of researchers. Vickrey et al. (2005) built classifiers inspired by those used in WSD to fill in any blanks in a partially completed translation. Giménez and Màrquez (2007) extended this work by considering the more general case of frequent phrases and moved to full translation rather than blank-filling on the target side. Attempts to embed context-rich approaches from WSD methods into SMT systems to enhance lexical selection did not lead to any improvement in translation quality (Carpuat and Wu, 2005). However, more recent approaches of integrating state-of-theart WSD methods into SMT to improve the overall translation quality h"
Y09-1019,N04-1033,0,0.0677221,"his feature is denoted as PW (parent word). Together we refer to these dependency features as the grammatical dependency information (DI) of the focus phrase fˆk , DI ( fˆk ). They are expressed as the conditional probability of the target phrase given the source phrase fˆk and its grammatical dependency information DI ( fˆk ), as in (6): (6) hˆm ( fˆk , DI ( fˆk ), eˆk , sk) = log P ( eˆk |fˆk , DI( fˆk )) 5 Memory-Based Classification As (Stroppa et al., 2007) point out, directly estimating context-dependent phrase translation probabilities using relative frequencies is problematic. Indeed, Zens and Ney (2004) showed that the estimation of P( eˆk |fˆk ) using relative frequencies results in the overestimation of the probabilities of long phrases. In the case of grammatical dependency-informed features, which include the identity of the parent word of the focus phrase, this estimation problem can only become worse. As an alternative, in this work we make use of memory-based machine learning classifiers that are able to estimate P ( eˆk |fˆk , DI ( fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source—target phrase translations to a new source phrase to be translate"
Y09-2027,A94-1010,0,0.38398,"rder (used with emphasis and complex structures) language. Therefore, applying adaptation techniques on such a language pair could produce interesting findings. For adaptation purposes, previous research used similarity metrics to cluster heterogeneous corpus data into sub-corpora with homogeneous topics. In order to compute the distance Copyright 2009 by Rejwanul Haque, Sudip Kumar Naskar, Josef van Genabith, and Andy Way 23rd Pacific Asia Conference on Language, Information and Computation, pages 670–677 670 between a sentence and a cluster, different similarity metrics have been proposed. (Carter, 1994) introduced an entropy reduction based similarity metric to cluster a multi-domain monolingual corpus. A regular expression based similarity function has been defined to build class specific language models (Hasan and Ney, 2005). In our research, we explore a clustering technique based on an n-gram overlap metric to extract sentences similar to in-domain text from large outof-domain training data. We employ domain adaptation techniques to adapt an out-of-domain bilingual corpus to an in-domain SMT model using clustering to extract sentences similar to in-domain text from large out-of-domain tr"
Y09-2027,W07-0722,0,0.097568,"g the mixture model proposed by Iyer et al. (1999). The results look promising in terms of perplexity reduction, as well as error rates obtained for a translation task using an n-best list rescoring framework. Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007) extended this work to include the translation model. Yamamoto and Sumita (2007) used an unsupervised clustering technique on an unlabelled bilingual training corpus. Each cluster is regarded as a domain. Clusters are defined automatically (without human knowledge) and created by the entropy reduction based method (Carter, 1994). Civera and Juan (2007) introduce the mixture extension for HMM alignment models. This approach generates topic dependent viterbi alignments to feed a state-of-art phrase based SMT (PB-SMT). Koehn and Schroeder (2007) investigated domain adaptations by integrating in-domain and out-of-domain language models as log-linear features in an SMT model. They also used multiple decoding paths (Birch et al. 2007) for combining multiple domain translation tables in the state-of-the-art PB-SMT decoder Moses (Koehn et al., 2003). 671 Nakov (2008) combine an in-domain model (translation and reordering model) with an outof-domain"
Y09-2027,eck-etal-2004-language,0,0.312628,"mbination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses (Koehn et al., 2007) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data’s match with respect to domain, topic, and style (Eck et al., 2004). Recently, researchers incorporate out-ofdomain data through learning phrase templates (phrase generalisation) in order to improve translation quality (Lim and Kirchhoff, 2008). In the present work, we conduct experiments on the English—Hindi language pair. Like other Indian languages, Hindi is also a free phrase order (used with emphasis and complex structures) language. Therefore, applying adaptation techniques on such a language pair could produce interesting findings. For adaptation purposes, previous research used similarity metrics to cluster heterogeneous corpus data into sub-corpora w"
Y09-2027,W08-0334,0,0.0119442,"m on news stories. For the translation of each source text, a large monolingual data set in the target language is searched for documents that might be comparable to the source text. These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document. Experimental results show substantial gains. Lim and Kirchhoff (2008) proposed a method for incorporating out-of-domain data through phrase generalization in order to improve the Italian-English translation quality. They showed a noticeable improvement in translation quality. Finch and Sumita (2008) employed probabilistic mixture weights to combine two models for questions and declarative sentences with a general model. Foster and Kuhn (2007) used distance based weights in a mixture model. In contrast to their work, Finch and Sumita (2008) used a probabilistic classifier to determine a vector of probability representing classmembership. They performed experiments on a number of language pairs and experimental results showed the usefulness of their method. Domain adaptation techniques can be broadly divided into two categories: (i) adaptation techniques to improve word alignment models; s"
Y09-2027,W07-0717,0,0.165775,"ectly optimizing machine translation evaluation metrics such as BLEU score. An improvement 0.4 BLEU score was reported. Hasan and Ney (2005) cluster the training sentences into specific classes based on regular expressions to build class specific language models. They proposed a method of interpolating class specific and global language models following the mixture model proposed by Iyer et al. (1999). The results look promising in terms of perplexity reduction, as well as error rates obtained for a translation task using an n-best list rescoring framework. Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007) extended this work to include the translation model. Yamamoto and Sumita (2007) used an unsupervised clustering technique on an unlabelled bilingual training corpus. Each cluster is regarded as a domain. Clusters are defined automatically (without human knowledge) and created by the entropy reduction based method (Carter, 1994). Civera and Juan (2007) introduce the mixture extension for HMM alignment models. This approach generates topic dependent viterbi alignments to feed a state-of-art phrase based SMT (PB-SMT). Koehn and Schroeder (2007) investigated domain adaptations by integrating in-d"
Y09-2027,2005.eamt-1.17,0,0.514059,"etrics to cluster heterogeneous corpus data into sub-corpora with homogeneous topics. In order to compute the distance Copyright 2009 by Rejwanul Haque, Sudip Kumar Naskar, Josef van Genabith, and Andy Way 23rd Pacific Asia Conference on Language, Information and Computation, pages 670–677 670 between a sentence and a cluster, different similarity metrics have been proposed. (Carter, 1994) introduced an entropy reduction based similarity metric to cluster a multi-domain monolingual corpus. A regular expression based similarity function has been defined to build class specific language models (Hasan and Ney, 2005). In our research, we explore a clustering technique based on an n-gram overlap metric to extract sentences similar to in-domain text from large outof-domain training data. We employ domain adaptation techniques to adapt an out-of-domain bilingual corpus to an in-domain SMT model using clustering to extract sentences similar to in-domain text from large out-of-domain training data. We apply adaptation techniques to combine sub-corpora with indomain small-scale training data into a unified framework. The remainder of the paper is organized as follows. In section 2 we discuss related work. Secti"
Y09-2027,2005.eamt-1.19,0,0.0349785,"in SMT by integrating terminological lexicons in the translation model resulting in a significant reduction in word error rate (WER). Over the last years, many researchers have investigated the problem of combining multi-domain data. Wu and Wang (2004) and Wu et al. (2005) propose an alignment adaptation approach to improve domain-specific word alignment. Eck et al. (2004) present a language model (LM) adaptation technique in SMT applying information retrieval theory following the approach of Mahajan et al. (1999) in speech recognition. This approach was further refined by Zhao et al. (2004). Hildebrand et al. (2005) adapt the translation model by selecting similar sentences from the available training data applying the approach of Eck et al. (2004). The adapted models significantly improve the translation performance compared to baseline systems. More recently, Bulyko et al., (2007) studied language model adaptation for SMT. They explored discriminative estimation of language model weights by directly optimizing machine translation evaluation metrics such as BLEU score. An improvement 0.4 BLEU score was reported. Hasan and Ney (2005) cluster the training sentences into specific classes based on regular e"
Y09-2027,N03-1017,0,0.00855717,"cally (without human knowledge) and created by the entropy reduction based method (Carter, 1994). Civera and Juan (2007) introduce the mixture extension for HMM alignment models. This approach generates topic dependent viterbi alignments to feed a state-of-art phrase based SMT (PB-SMT). Koehn and Schroeder (2007) investigated domain adaptations by integrating in-domain and out-of-domain language models as log-linear features in an SMT model. They also used multiple decoding paths (Birch et al. 2007) for combining multiple domain translation tables in the state-of-the-art PB-SMT decoder Moses (Koehn et al., 2003). 671 Nakov (2008) combine an in-domain model (translation and reordering model) with an outof-domain model (translation and reordering) into Moses (Koehn et al., 2007). They derived log-linear features to distinguish between phrases of multiple domains by applying the datasource indicator features and showed modest improvement in translation quality. Munteanu and Marcu (2006) automatically extract in-domain bilingual sentence pairs from large comparable corpora to enlarge the in-domain bilingual corpus. They showed a modest gain over the baseline system. Ueffing et al. (2007) introduced trans"
Y09-2027,P07-2045,0,0.0154384,"o account the semantic context in which they appear. The semantic dependency problem could be overcome by learning topic-dependent translation models. There has been increased interest in incorporating data from domains with sufficient data in order to improve translation quality for small-data domains. Several approaches have been applied to domain adaptation such as using two phrase tables jointly with a data source indicator feature added to the log-linear combination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses (Koehn et al., 2007) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data’s match with respect to domain, topic, and style (Eck et al., 2004). Recently, researchers incorporate out-ofdomain data through learning phrase templates (phrase generalisation) in order to improve tran"
Y09-2027,W07-0733,0,0.509107,"e semantic dependency problem could be overcome by learning topic-dependent translation models. There has been increased interest in incorporating data from domains with sufficient data in order to improve translation quality for small-data domains. Several approaches have been applied to domain adaptation such as using two phrase tables jointly with a data source indicator feature added to the log-linear combination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses (Koehn et al., 2007) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data’s match with respect to domain, topic, and style (Eck et al., 2004). Recently, researchers incorporate out-ofdomain data through learning phrase templates (phrase generalisation) in order to improve translation quality (Lim and Kirchhoff, 2008). In the present work"
Y09-2027,W02-1405,0,0.709646,"section 2 we discuss related work. Section 3 describes experimental results using our baseline SMT model. In section 4 we describe the domain adaptation techniques which are employed to combine multiple models. Section 5 presents the results obtained, together with some analysis. Section 6 concludes, and provides avenues for further work. 2 Related Work Topic-dependent modeling was effectively applied in speech recognition to improve the quality of models (Carter, 1994). Adaptation technology has been widely used in language modeling in the same filed over the last decade (Iyer et al., 1997). Langlais (2002) was the first to introduce domain adaptation in SMT by integrating terminological lexicons in the translation model resulting in a significant reduction in word error rate (WER). Over the last years, many researchers have investigated the problem of combining multi-domain data. Wu and Wang (2004) and Wu et al. (2005) propose an alignment adaptation approach to improve domain-specific word alignment. Eck et al. (2004) present a language model (LM) adaptation technique in SMT applying information retrieval theory following the approach of Mahajan et al. (1999) in speech recognition. This approa"
Y09-2027,W08-0320,0,0.487175,"eneous topics. These topics usually define a set of terminological lexicons. Terminologies need to be translated taking into account the semantic context in which they appear. The semantic dependency problem could be overcome by learning topic-dependent translation models. There has been increased interest in incorporating data from domains with sufficient data in order to improve translation quality for small-data domains. Several approaches have been applied to domain adaptation such as using two phrase tables jointly with a data source indicator feature added to the log-linear combination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses (Koehn et al., 2007) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data’s match with respect to domain, topic, and style (Eck et al., 2004). Rece"
Y09-2027,P03-1021,0,0.0301251,"ated EILMT and TIDES language models and translation models using a log-linear combination. 4.1 Language Model Adaptation We used the language modeling toolkit SRILM (Stolke, 2002) to build two language models from the target side of the EILMT and TIDES training data. We performed log-linear interpolation of multi-domain translation models. This results in a straight-forward combination of in-domain and out-of-domain language models. Fortunately, the PB-SMT Moses decoder supports log-linear combinations of language models. Language model weights are optimized with minimum error rate training (Och, 2003). 4.2 Translation Model Adaptation In general, translation models are built separately for each of the domain specific corpora. These models are then combined using two techniques: (i) linear interpolation (ii) log-linear interpolation. We performed the log-linear interpolation of multi-domain translation models. There are two ways of performing log-linear interpolation: Multiple Decoding Paths: a recent feature of Moses is multiple decoding paths. This alternate decoding path model was developed by Birch et al. (2007). Here we use Moses’ capabilities to use different decoding paths for transl"
Y09-2027,D08-1090,0,0.0207945,"ted repeatedly and the generated translations are added to training data to improve the performance of the SMT system. They reported a significant improvement of BLEU over the baseline. Wu et al. (2008) proposed a method to perform domain adaptation for SMT, where indomain bilingual data do not exist. The transductive learning method (Ueffing et al. 2007) has been used to adapt the in-domain monolingual corpus. Wu et al. (2008) also showed that loglinear interpolation performs better than linear interpolation to combine in-domain and out-ofdomain language models as well as translation models. Snover et al. (2008) describes a novel domain adaptation method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories. For the translation of each source text, a large monolingual data set in the target language is searched for documents that might be comparable to the source text. These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document. Experimental results show substantial gains. Lim and Kirchhoff (2008) proposed a method for incorporating out-of-domain data t"
Y09-2027,P07-1004,0,0.0249606,"SMT decoder Moses (Koehn et al., 2003). 671 Nakov (2008) combine an in-domain model (translation and reordering model) with an outof-domain model (translation and reordering) into Moses (Koehn et al., 2007). They derived log-linear features to distinguish between phrases of multiple domains by applying the datasource indicator features and showed modest improvement in translation quality. Munteanu and Marcu (2006) automatically extract in-domain bilingual sentence pairs from large comparable corpora to enlarge the in-domain bilingual corpus. They showed a modest gain over the baseline system. Ueffing et al. (2007) introduced transductive semi-supervised learning for SMT, where source language corpora are used to train the models. The transductive learning can be seen as a means to adapt the SMT system to a new domain. Sentences from the devset or testset are translated repeatedly and the generated translations are added to training data to improve the performance of the SMT system. They reported a significant improvement of BLEU over the baseline. Wu et al. (2008) proposed a method to perform domain adaptation for SMT, where indomain bilingual data do not exist. The transductive learning method (Ueffin"
Y09-2027,P05-1058,0,0.27129,"prove translation quality for small-data domains. Several approaches have been applied to domain adaptation such as using two phrase tables jointly with a data source indicator feature added to the log-linear combination (Nakov, 2008), which has shown good results. Some researchers use multiple decoding paths of PB-SMT decoders such as Moses (Koehn et al., 2007) for multi domain model adaptation (Koehn and Schroeder, 2007). Adaptations on the alignment model have been investigated where word alignments learned from a large out-of-domain corpus are used to align words for a small-scale domain (Wu et al., 2005). Some researchers proposed a way to retrieve only those sentences which are most similar to the test data in order to improve the training data’s match with respect to domain, topic, and style (Eck et al., 2004). Recently, researchers incorporate out-ofdomain data through learning phrase templates (phrase generalisation) in order to improve translation quality (Lim and Kirchhoff, 2008). In the present work, we conduct experiments on the English—Hindi language pair. Like other Indian languages, Hindi is also a free phrase order (used with emphasis and complex structures) language. Therefore, a"
Y09-2027,C08-1125,0,0.0743408,"ence pairs from large comparable corpora to enlarge the in-domain bilingual corpus. They showed a modest gain over the baseline system. Ueffing et al. (2007) introduced transductive semi-supervised learning for SMT, where source language corpora are used to train the models. The transductive learning can be seen as a means to adapt the SMT system to a new domain. Sentences from the devset or testset are translated repeatedly and the generated translations are added to training data to improve the performance of the SMT system. They reported a significant improvement of BLEU over the baseline. Wu et al. (2008) proposed a method to perform domain adaptation for SMT, where indomain bilingual data do not exist. The transductive learning method (Ueffing et al. 2007) has been used to adapt the in-domain monolingual corpus. Wu et al. (2008) also showed that loglinear interpolation performs better than linear interpolation to combine in-domain and out-ofdomain language models as well as translation models. Snover et al. (2008) describes a novel domain adaptation method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories. For the tran"
Y09-2027,D07-1054,0,0.353807,"f language model weights by directly optimizing machine translation evaluation metrics such as BLEU score. An improvement 0.4 BLEU score was reported. Hasan and Ney (2005) cluster the training sentences into specific classes based on regular expressions to build class specific language models. They proposed a method of interpolating class specific and global language models following the mixture model proposed by Iyer et al. (1999). The results look promising in terms of perplexity reduction, as well as error rates obtained for a translation task using an n-best list rescoring framework. Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007) extended this work to include the translation model. Yamamoto and Sumita (2007) used an unsupervised clustering technique on an unlabelled bilingual training corpus. Each cluster is regarded as a domain. Clusters are defined automatically (without human knowledge) and created by the entropy reduction based method (Carter, 1994). Civera and Juan (2007) introduce the mixture extension for HMM alignment models. This approach generates topic dependent viterbi alignments to feed a state-of-art phrase based SMT (PB-SMT). Koehn and Schroeder (2007) investigated domain adap"
Y09-2027,C04-1059,0,0.0553937,"e domain adaptation in SMT by integrating terminological lexicons in the translation model resulting in a significant reduction in word error rate (WER). Over the last years, many researchers have investigated the problem of combining multi-domain data. Wu and Wang (2004) and Wu et al. (2005) propose an alignment adaptation approach to improve domain-specific word alignment. Eck et al. (2004) present a language model (LM) adaptation technique in SMT applying information retrieval theory following the approach of Mahajan et al. (1999) in speech recognition. This approach was further refined by Zhao et al. (2004). Hildebrand et al. (2005) adapt the translation model by selecting similar sentences from the available training data applying the approach of Eck et al. (2004). The adapted models significantly improve the translation performance compared to baseline systems. More recently, Bulyko et al., (2007) studied language model adaptation for SMT. They explored discriminative estimation of language model weights by directly optimizing machine translation evaluation metrics such as BLEU score. An improvement 0.4 BLEU score was reported. Hasan and Ney (2005) cluster the training sentences into specific"
Y09-2027,wu-wang-2004-improving-domain,0,\N,Missing
Y09-2027,J05-4003,0,\N,Missing
Y09-2027,P08-1000,0,\N,Missing
Y10-1041,2007.mtsummit-papers.19,0,0.527693,"single analogical equation which otherwise had only one solution in ALEPH, and is accordingly much slower. While Lepage and colleagues have had ∗ The research is supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of the Centre for Next Generation Localization (http://www.cngl.ie) at Dublin City University. Copyright 2010 by Sandipan Dandapat, Sara Morrissey, Sudip Kumar Naskar, and Harold Somers 365 366 Poster Papers modest success using PA for a full translation task, the idea is adapted to translating unknown words in the context of another approach to MT as reported by Denoual (2007), Langlais and Patry (2007), and Langlais et al. (2009). Denoual’s (2007) experiments attempt to translate all unknown words in a Japanese–English task and have reported that translation adequacy (NIST score) improves but fluency (BLEU score) remains stable or is decreased. Langlais and Patry (2007) had more success in handling unknown words while the language pairs are quite close in morphological structure. Langlais and Yvon (2008) use PA to supplement the words and phrases for standard SMT when a word to be translated is not covered by the statistical model. Finally, Langlais et al. (2009)"
Y10-1041,P08-1045,0,0.0732534,"Missing"
Y10-1041,D07-1092,0,0.659524,"1 Introduction In the EBMT workshop in Phuket, Thailand, Lepage and Denoual (2005a) presented “The ‘purest’ EBMT system ever built: no variable, no templates, no training, examples, just examples, only examples”. This purely data-driven approach to MT uses the notion of proportional analogy (PA, described below), a type of analogical learning, the very simplicity of which is its attraction. We have attempted to reimplement the method in order to verify the previous authors’ claims. It has been noted, that the PA-based system works well in the case of shorter sentences with similar structure (Langlais and Patry, 2007). Thus we have taken named entity (NE) transliteration as a case study because NEs are typically short. Furthermore, it was reported (Hermjakob et. al, 2008) that the state-of-the-art SMT system can’t handle NEs that are not found in the training parallel text. In this paper we describe an experiment in which we use the method to handle unknown named entities (NEs) in an English-Hindi transliteration task. The idea introduced in Lepage and Denoual (2005a) is explained in considerably more detail in Lepage and Denoual (2005b, c). ALEPH system is an implementation of the research described in th"
Y10-1041,C08-2013,0,0.537519,"Poster Papers modest success using PA for a full translation task, the idea is adapted to translating unknown words in the context of another approach to MT as reported by Denoual (2007), Langlais and Patry (2007), and Langlais et al. (2009). Denoual’s (2007) experiments attempt to translate all unknown words in a Japanese–English task and have reported that translation adequacy (NIST score) improves but fluency (BLEU score) remains stable or is decreased. Langlais and Patry (2007) had more success in handling unknown words while the language pairs are quite close in morphological structure. Langlais and Yvon (2008) use PA to supplement the words and phrases for standard SMT when a word to be translated is not covered by the statistical model. Finally, Langlais et al. (2009) applied the method to the translation of medical terms and showed little improvement on purely statistical approaches. Since no off-the-shelf implementation is available for solving analogies, we have implemented our own EBMT system from scratch using PA based on the description in Lepage (2005c). It is often the case that a PAbased system suffers from low recall. First we try to improve the PA-based system by introducing new heurist"
Y10-1041,E09-1056,0,0.669284,"only one solution in ALEPH, and is accordingly much slower. While Lepage and colleagues have had ∗ The research is supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of the Centre for Next Generation Localization (http://www.cngl.ie) at Dublin City University. Copyright 2010 by Sandipan Dandapat, Sara Morrissey, Sudip Kumar Naskar, and Harold Somers 365 366 Poster Papers modest success using PA for a full translation task, the idea is adapted to translating unknown words in the context of another approach to MT as reported by Denoual (2007), Langlais and Patry (2007), and Langlais et al. (2009). Denoual’s (2007) experiments attempt to translate all unknown words in a Japanese–English task and have reported that translation adequacy (NIST score) improves but fluency (BLEU score) remains stable or is decreased. Langlais and Patry (2007) had more success in handling unknown words while the language pairs are quite close in morphological structure. Langlais and Yvon (2008) use PA to supplement the words and phrases for standard SMT when a word to be translated is not covered by the statistical model. Finally, Langlais et al. (2009) applied the method to the translation of medical terms"
Y10-1041,P98-1120,0,0.867743,"logy PAs are global relationships between four objects - A : B :: C : D , read as “A is to B as C is to D”. The symbol ‘::’ is sometimes replaced with an equals sign (=) to denote an equation. This formulation as an equation can have zero, one, or more solutions if any of the objects (usually D) is considered as a variable. PAs are often seen as a way of knowledge representation in Artificial Intelligence due to their power to represent world knowledge and the lexical relation encoded in them. In NLP, the analogies are used as an instrument to explain inflectional and derivational morphology (Lepage, 1998). 2.2 EBMT using Analogy Lepage and Denoual (2005a,b,c) showed how an EBMT system can be built based on the algorithm proposed by Lepage (1998). Treating a sentence as a string of characters, they note that PAs can be handled as in (1). (1) They swam in the sea : They swam across the river :: It floated in the sea : It floated across the river To build up an EBMT system, we must assume a database of example pairs, where each pair is a source and target language translation equivalent. For the first three sentences in (1), the translation equivalents in Spanish are given in (2). (2) a. Nadarón"
Y10-1041,C04-1106,0,0.0684331,"Missing"
Y10-1041,2005.mtsummit-ebmt.11,0,0.870088,"for tackling English–Hindi Named Entity (NE) Transliteration. We have implemented our own EBMT system using proportional analogy and have found that the analogy-based system on its own has low precision but a high recall due to the fact that a large number of names are untransliterated with the approach. However, mitigating problems in analogy-based EBMT with SMT and vice-versa have shown considerable improvement over the individual approach. Keywords: Example Based Machine Translation, Proportional Analogy, Named Entity Transliteration 1 Introduction In the EBMT workshop in Phuket, Thailand, Lepage and Denoual (2005a) presented “The ‘purest’ EBMT system ever built: no variable, no templates, no training, examples, just examples, only examples”. This purely data-driven approach to MT uses the notion of proportional analogy (PA, described below), a type of analogical learning, the very simplicity of which is its attraction. We have attempted to reimplement the method in order to verify the previous authors’ claims. It has been noted, that the PA-based system works well in the case of shorter sentences with similar structure (Langlais and Patry, 2007). Thus we have taken named entity (NE) transliteration as"
Y10-1041,2005.iwslt-1.4,0,0.228595,"for tackling English–Hindi Named Entity (NE) Transliteration. We have implemented our own EBMT system using proportional analogy and have found that the analogy-based system on its own has low precision but a high recall due to the fact that a large number of names are untransliterated with the approach. However, mitigating problems in analogy-based EBMT with SMT and vice-versa have shown considerable improvement over the individual approach. Keywords: Example Based Machine Translation, Proportional Analogy, Named Entity Transliteration 1 Introduction In the EBMT workshop in Phuket, Thailand, Lepage and Denoual (2005a) presented “The ‘purest’ EBMT system ever built: no variable, no templates, no training, examples, just examples, only examples”. This purely data-driven approach to MT uses the notion of proportional analogy (PA, described below), a type of analogical learning, the very simplicity of which is its attraction. We have attempted to reimplement the method in order to verify the previous authors’ claims. It has been noted, that the PA-based system works well in the case of shorter sentences with similar structure (Langlais and Patry, 2007). Thus we have taken named entity (NE) transliteration as"
Y10-1041,2007.iwslt-1.7,0,0.0662448,"nsliteration task. The idea introduced in Lepage and Denoual (2005a) is explained in considerably more detail in Lepage and Denoual (2005b, c). ALEPH system is an implementation of the research described in their three 2005 papers, and was tested on a corpus of 160K English, Japanese and Chinese sentences, from the C-STAR project’s BTEC, a travel and tourism domain corpus. The system did very well on data from the IWSLT 2004 competition, coming a close second to the competition winner on all measures. The ALEPH system evolved into a new system, named GREYC, with some modification described in Lepage and Lardilleux (2007). The GREYC system also incorporated new heuristics and had an additional refinement of non-determinism to generate all possible solutions for a single analogical equation which otherwise had only one solution in ALEPH, and is accordingly much slower. While Lepage and colleagues have had ∗ The research is supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of the Centre for Next Generation Localization (http://www.cngl.ie) at Dublin City University. Copyright 2010 by Sandipan Dandapat, Sara Morrissey, Sudip Kumar Naskar, and Harold Somers 365 366 Poster Papers modest succes"
Y10-1041,2006.iwslt-evaluation.4,0,0.0168074,"s expected to work best with our current experimental setup. 4 Experiments We have tested our EBMT system using PA for a NE transliteration task from English to Hindi. Five different experiments were conducted based on our EBMT system using PA. We shall call PACLIC 24 Proceedings 369 these analogy-based EBMT (AEBMT).The five experiments deal with the five different heuristics described in 3.1. Each of these five experiments was also tested with a time bound of one second and three seconds to understand the effect of time while using analogy-based system. In parallel, we have also used MaTrEx (Stroppa and Way, 2006), an open source statistical MT (SMT) system in order to estimate the relative performance of the models. Furthermore, the SMT system can be trained at character-, syllable- and word-level using appropriate examplebases. We have found that there are cases where AEBMT correctly produces the transliteration but SMT fails and vice versa. In order to further improve the transliteration accuracy, we use a combination of AEBMT with SMT. We combine these two systems in two ways. We assume that the transliteration of a word w produced by AEBMT and SMT are respectively TAEBMT(w) and TSMT(w). First, we"
Y10-1041,C98-1116,0,\N,Missing
Y10-1041,W09-3502,0,\N,Missing
