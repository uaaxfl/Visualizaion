2011.mtsummit-papers.57,D08-1033,0,0.0376903,"Missing"
2011.mtsummit-papers.57,N03-1017,0,0.104029,"Missing"
2011.mtsummit-papers.57,W04-3250,0,0.278689,"Missing"
2011.mtsummit-papers.57,W02-1018,0,0.0552923,"Missing"
2011.mtsummit-papers.57,P03-1021,0,0.0240929,"Missing"
2011.mtsummit-papers.57,P00-1056,0,0.0992779,"Missing"
2011.mtsummit-papers.57,P02-1040,0,0.0794549,"Missing"
2020.acl-main.338,P19-1140,0,0.0276113,"present a simpliﬁed graph neural network model, called graph convolutional networks (GCN), which has been exported to several tasks such as scene recognition (Yuan et al., 2019), 3668 semi-supervised node classiﬁcation (Zhang et al., 2019b), text-to-SQL parsing (Bogin et al., 2019) and relation extraction (Sahu et al., 2019). On this basis, some other improved Graph-based Neural Networks are proposed. Morris et al. (2019) propose a generalization of Graph-based Neural Networks, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. Cao et al. (2019) propose a novel Multi-channel Graph Neural Network model to learn alignment-oriented knowledge graph embeddings by robustly encoding two knowledge graphs via multiple channels. More recently, there exist several studies also adopting graph-based neural networks to ASC. For instance, Hou et al. (2019) and Zhang et al. (2019a) build GCN over the dependency tree of a sentence to exploit syntactical information and word dependencies for learning better aspect-related sentence representation for ASC. Different from all the above studies, this paper proposes a novel Cooperative Graph Attention Netw"
2020.acl-main.338,D17-1047,0,0.0234547,"sting studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to using external information to improve the performance of ASC. He et al. (2018b) and Chen and Qian (2019) incorporate the knowledge from document-le"
2020.acl-main.338,P19-1052,0,0.0183307,"information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to using external information to improve the performance of ASC. He et al. (2018b) and Chen and Qian (2019) incorporate the knowledge from document-level sentiment classiﬁcation to improve the performance of ASC. Ma et al. (2018) propose an extension of LSTM to integrate the commonsense knowledge into the recurrent encoder for improving the performance of ASC. In addition, it is worthwhile to note that Hazarika et al. (2018) also investigate the inter-aspect sentiment dependency for ASC, but is limited to capture this information inside a single sentence. In summary, all the above studies ignore the document-level sentiment preference information, which can be leveraged to effectively mitigate the"
2020.acl-main.338,N19-1423,0,0.00741175,"es each new vertex vector h tex vi by considering neighboring vertices’ vectors {hj }Ij=1 with the following formulas: ˆ i = tanh( h I  αij W hj + b) j=1 exp(f (w [W hi ; W hj ])) αij = I t=1 exp(f (w  [W h (1) i ; W ht ])) where αij is the attention weight (i.e., the edge weight) between vertex vi and vertex vj . f (·) is a LeakyReLU activation function. [; ] denotes vector concatenation. W ∈ Rd×d and w ∈ R2d are the trainable parameters. In the following, we will illustrate the ﬁve main components of our CoGAN approach respectively. 3.2 Encoding Block As a text encoding mechanism, BERT (Devlin et al., 2019) can be ﬁne-tuned to create state-of-the-art models for a range of NLP tasks, e.g., text classiﬁcation and natural language inference. In our approach, we use BERT-base2 (uncased) model to encode both the aspect and the sentence as follows. • Aspect Encoding. Since an aspect ak consists of an entity eentity and an attribute eattribute (Pontiki et al., 2015), we process the entity-attribute pair (eentity , eattribute ) into the input pair format of BERT as: [CLS] eentity [SEP] eattribute [SEP] Then, we feed the entity-attribute pair into BERT and regard the mark “[CLS]” representation as the as"
2020.acl-main.338,C18-1096,0,0.0832994,"networks. Aspect Sentiment Classiﬁcation. The ASC task aims to predict the sentiment polarity for each aspect discussed inside a sentence. Existing studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to u"
2020.acl-main.338,P18-2092,0,0.0685403,"networks. Aspect Sentiment Classiﬁcation. The ASC task aims to predict the sentiment polarity for each aspect discussed inside a sentence. Existing studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information for performing ASC. Wang et al. (2019a) and Wang et al. (2019b) leverage reinforcement learning grounded approaches to select aspect-relevant words for ASC. Recently, a few studies have recognized the information deﬁciency problem in ASC and attempted to u"
2020.acl-main.338,P19-1048,0,0.0578973,"n task to a sentence pair classiﬁcation task. In our implementation, we regard the pair of sentence and its aspect as the input pair of BERT-base model (Devlin et al., 2018) for performing ASC. 8) CADMN. This approach employs attention model to attend on relevant aspects for enhancing the aspect representation. This is a state-of-the-art approach proposed by Song et al. (2019). 9) IMN. This approach is a multi-task learning approach, which employs a novel message passing mechanism to better exploit the correlation among the tasks related to ASC. This is a state-of-the-art approach proposed by He et al. (2019). 10) BERT-QA. This approach is an extension of the above BERT baseline proposed by Sun et al. (2019). In this study, we adopt BERTpair-QA-M in our implementation. This is another state-of-the-art approach for ASC. 11) Sentiue. This is the best-performed system in SemEval-2015 Task 12 (Saias, 2015), which achieves the best accuracy scores in both the laptop15 and restaurant15 domains. 12) XRCE. This is the best-performed system in SemEval-2016 Task 5 (Pontiki et al., 2016), which achieves the best accuracy score in the restaurant16 domain. 13) IIT-TUDA. This is also the best-performed system i"
2020.acl-main.338,S16-1174,0,0.0691356,"Missing"
2020.acl-main.338,P10-1043,1,0.697296,"ITY, polarity = positive - Category = FOOD#PRICES, polarity = positive S3: The lava cake dessert was incredible and I recommend it. - Category = FOOD#QUALITY, polarity = positive Figure 1: Two documents from SemEval 2016 (Pontiki et al. (2016)) datasets, where aspect category is deﬁned as the entity E and attribute A pair (i.e., E#A). Red lines denote the intra-aspect sentiment consistency and blue lines denote the inter-aspect sentiment tendency. Introduction Aspect Sentiment Classiﬁcation (ASC), a ﬁnegrained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang"
2020.acl-main.338,S16-1002,0,0.132215,"Missing"
2020.acl-main.338,S15-2082,0,0.248872,"Missing"
2020.acl-main.338,P19-1423,0,0.043128,"e document-level sentiment preference information, which can be leveraged to effectively mitigate the information deﬁciency problem in ASC. Graph-based Neural Networks. In recent years, graph-based neural networks have received more and more attentions. As a pioneer, Kipf and Welling (2017) present a simpliﬁed graph neural network model, called graph convolutional networks (GCN), which has been exported to several tasks such as scene recognition (Yuan et al., 2019), 3668 semi-supervised node classiﬁcation (Zhang et al., 2019b), text-to-SQL parsing (Bogin et al., 2019) and relation extraction (Sahu et al., 2019). On this basis, some other improved Graph-based Neural Networks are proposed. Morris et al. (2019) propose a generalization of Graph-based Neural Networks, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. Cao et al. (2019) propose a novel Multi-channel Graph Neural Network model to learn alignment-oriented knowledge graph embeddings by robustly encoding two knowledge graphs via multiple channels. More recently, there exist several studies also adopting graph-based neural networks to ASC. For instance, Hou et al. (2019) and Zh"
2020.acl-main.338,S15-2130,0,0.15077,"entation. This is a state-of-the-art approach proposed by Song et al. (2019). 9) IMN. This approach is a multi-task learning approach, which employs a novel message passing mechanism to better exploit the correlation among the tasks related to ASC. This is a state-of-the-art approach proposed by He et al. (2019). 10) BERT-QA. This approach is an extension of the above BERT baseline proposed by Sun et al. (2019). In this study, we adopt BERTpair-QA-M in our implementation. This is another state-of-the-art approach for ASC. 11) Sentiue. This is the best-performed system in SemEval-2015 Task 12 (Saias, 2015), which achieves the best accuracy scores in both the laptop15 and restaurant15 domains. 12) XRCE. This is the best-performed system in SemEval-2016 Task 5 (Pontiki et al., 2016), which achieves the best accuracy score in the restaurant16 domain. 13) IIT-TUDA. This is also the best-performed system in SemEval-2016 Task 5 (Pontiki et al., 2016), while achieving the best accuracy score in the laptop16 domain. 15) CoGAN w/o Intra-Aspect Consistency. Our approach only modeling Inter-Aspect Tendency. 16) CoGAN w/o Inter-Aspect Tendency. Our approach only modeling Intra-Aspect Consistency. 17) CoGAN"
2020.acl-main.338,N19-1035,0,0.360347,"S] Excellent food … [SEP] What do you … Encoding Block  ൌ ܹݎ  ܾ ݒො Layer L ܧ sentence vector ݒ ܵ ݒොଶ ݒොଵ Layer 2 … ܧሾௌாሿ … ܽ [SEP] … Shared BERT ܧଵ ୀଵ Layer 1 ܧሾௌሿ quality … ܧሾௌாሿ … ܧଵ ܵାଵ ௧௧௨௧ [SEP] ൌ ሺሺݒ  ߙ ሺ ߙ ܹ ݒ ሻሻ  ܾሻ ܵ Aspect Input: FOOD#QUALITY ௧௧௬ ூᇲ ሺ௧ሻ ݒො ூ ܵଷ ሺ௧ሻ ݒො ܵଶ ൌ ሺ ߙ ܹఈ ݒ  ܾఈ ሻ ୀଵ Inter-Aspect Tendency Modeling Block Interaction Block Figure 2: The overall framework of our proposed Cooperative Graph Attention Networks (CoGAN). • Sentence Encoding. We borrow the approach proposed by Sun et al. (2019) to generate the aspectrelated sentence representation, which has achieved promising performance for the ASC task. Following Sun et al. (2019), we ﬁrst process the sentence si and its corresponding aspect ak into the input pair format of BERT as: [CLS] si [SEP] question(ak ) [SEP] where question(·) denotes the construction of auxiliary question sentence for aspect ak proposed by Sun et al. (2019). For example, the auxiliary sentence for aspect FOOD#PRICE is constructed as “what do you think of the food and price?”. Then, we similarly feed the above pair into BERT (shared with aspect encoding)"
2020.acl-main.338,C16-1311,0,0.576477,"n Aspect Sentiment Classiﬁcation (ASC), a ﬁnegrained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang et al., 2019c) and Question Answering (Wang et al., 2019a). In the literature, given the ASC datasets (Pontiki et al. (2016)) where aspects (i.e., entity and attribute) are manually annotated comprehensively sentence by sentence, previous studies model the aspect sentiment independently sentence by sentence, which suffer from the problem of ignoring the document-level sentiment preference information. In this study, we argue that such documentlevel sentiment"
2020.acl-main.338,D16-1021,0,0.477432,"n Aspect Sentiment Classiﬁcation (ASC), a ﬁnegrained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang et al., 2019c) and Question Answering (Wang et al., 2019a). In the literature, given the ASC datasets (Pontiki et al. (2016)) where aspects (i.e., entity and attribute) are manually annotated comprehensively sentence by sentence, previous studies model the aspect sentiment independently sentence by sentence, which suffer from the problem of ignoring the document-level sentiment preference information. In this study, we argue that such documentlevel sentiment"
2020.acl-main.338,P19-1345,1,0.819933,"010), aims to identify the sentiment polarity (e.g., positive, negative or neutral) for each aspect discussed inside a sentence. For example, the sentence “The restaurant has quite low price but the food tastes not good” would be assigned with a positive polarity for the aspect price and with a negative ∗ Corresponding Author: Jingjing Wang. polarity for the aspect food. Over the past decade, the ASC task has been drawing more and more interests (Tang et al., 2016b; Wang et al., 2018) due to its wide applications, such as e-commerce customer service (Jing et al., 2015), public opinion mining (Wang et al., 2019c) and Question Answering (Wang et al., 2019a). In the literature, given the ASC datasets (Pontiki et al. (2016)) where aspects (i.e., entity and attribute) are manually annotated comprehensively sentence by sentence, previous studies model the aspect sentiment independently sentence by sentence, which suffer from the problem of ignoring the document-level sentiment preference information. In this study, we argue that such documentlevel sentiment preference information is crucial to 3667 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3667–3677 c"
2020.acl-main.338,D19-1560,1,0.883168,"Missing"
2020.acl-main.338,D16-1058,0,0.0694742,"hree top-performed systems from SemEval-2015 Task 12 and SemEval2016 Task 5 (Pontiki et al., 2015, 2016). 2 Related Work In this section, we ﬁrst review the Aspect Sentiment Classiﬁcation (ASC) task, and then introduce the related studies on graph-based neural networks. Aspect Sentiment Classiﬁcation. The ASC task aims to predict the sentiment polarity for each aspect discussed inside a sentence. Existing studies mainly focus on utilizing various approaches (e.g., attention mechanism and memory network) to align each aspect and the sentence for learning aspect-related sentence representation. Wang et al. (2016) propose an attention-based LSTM in order to explore the potential correlation of aspects and sentiment polarities in ASC. Wang et al. (2018) propose a hierarchical attention network to incorporate both words and clauses information for ASC. He et al. (2018a) propose an attention-based approach to incorporate the aspect-related syntactic information for ASC. Tang et al. (2016b) and Chen et al. (2017) design deep memory networks to align the aspect and sentence for ASC. Lin et al. (2019) propose a semantic and context-aware memory network to integrate aspect-related semantic parsing information"
2020.acl-main.338,D19-1464,0,0.0482997,"e this information inside a single sentence. In summary, all the above studies ignore the document-level sentiment preference information, which can be leveraged to effectively mitigate the information deﬁciency problem in ASC. Graph-based Neural Networks. In recent years, graph-based neural networks have received more and more attentions. As a pioneer, Kipf and Welling (2017) present a simpliﬁed graph neural network model, called graph convolutional networks (GCN), which has been exported to several tasks such as scene recognition (Yuan et al., 2019), 3668 semi-supervised node classiﬁcation (Zhang et al., 2019b), text-to-SQL parsing (Bogin et al., 2019) and relation extraction (Sahu et al., 2019). On this basis, some other improved Graph-based Neural Networks are proposed. Morris et al. (2019) propose a generalization of Graph-based Neural Networks, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. Cao et al. (2019) propose a novel Multi-channel Graph Neural Network model to learn alignment-oriented knowledge graph embeddings by robustly encoding two knowledge graphs via multiple channels. More recently, there exist several studies"
2020.acl-main.569,E17-1028,0,0.318957,", 7 EDUs are connected by 6 rhetorical relations, while in each non-terminal node, the rhetorical relation and the nuclearity type are labeled. Correspondingly, text-level DRS parsing consists of three components, i.e., bare DRS generation (hierarchical span determination), rhetorical nuclearity determination and rhetorical relation classification. During the past decade, text-level DRS parsing has been drawing more and more attention and achieved certain success (Hernault et al., 2010; Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Heilman and Sagae, 2015; Li et al., 2016; Braud et al., 2017; Yu et al., 2018). However, all the previous studies on text-level DRS parsing adopt bottom-up approaches. That is, adjacent EDUs are recursively combined into high-level larger text spans by rhetorical relations to form a final discourse tree in a bottom-up way. In this paper, we justify that compared with a bottom-up approach, a top-down approach may be more suitable for textlevel DRS parsing from two points-of-view, • From the computational view, only local information (i.e., the constructed DRS subtrees and their context) can be naturally employed to determine the upper layer structure in"
2020.acl-main.569,C16-1179,0,0.356122,"1 scores. Furthermore, to gain detailed comparison between the bottom-up and the top-down approaches, we also report the performance of relation classification and full discourse parsing. Relation 40.7 48.5 Full 39.6 41.7 various additional features. For example, both Ji and Eisenstein (2014) and Feng and Hirst (2014) employed many kinds of additional hand-crafted features including syntactic, contextual and so on, while Braud et al. (2017) resort to additional cross-lingual features and achieve the gain of 3.2, 7.3, 10.8 and 10.8 on the four evaluation metrics respectively in comparison with Braud et al. (2016). This indicates the great preference of top-down over bottom-up text-level DRS parsing. This also suggests the great potential of additional carefully designed features, which are worth exploring in the future work. • Braud et al. (2016), a sequence-to-sequence parser that is heuristically constrained to build trees with a hierarchical neural model. • For English, our top-down system achieves comparable performance with the state-of-the-art systems. It is worthwhile to note that, we focus on the effectiveness of our proposed top-down architecture in this paper. The performance of our top-down"
2020.acl-main.569,P16-1032,0,0.0607192,"Missing"
2020.acl-main.569,P14-1048,0,0.683193,"s combined into high level larger discourse units in a bottom-up fashion. In this example, 7 EDUs are connected by 6 rhetorical relations, while in each non-terminal node, the rhetorical relation and the nuclearity type are labeled. Correspondingly, text-level DRS parsing consists of three components, i.e., bare DRS generation (hierarchical span determination), rhetorical nuclearity determination and rhetorical relation classification. During the past decade, text-level DRS parsing has been drawing more and more attention and achieved certain success (Hernault et al., 2010; Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Heilman and Sagae, 2015; Li et al., 2016; Braud et al., 2017; Yu et al., 2018). However, all the previous studies on text-level DRS parsing adopt bottom-up approaches. That is, adjacent EDUs are recursively combined into high-level larger text spans by rhetorical relations to form a final discourse tree in a bottom-up way. In this paper, we justify that compared with a bottom-up approach, a top-down approach may be more suitable for textlevel DRS parsing from two points-of-view, • From the computational view, only local information (i.e., the constructed DRS subtrees"
2020.acl-main.569,W16-5903,0,0.0398394,"Missing"
2020.acl-main.569,P14-1002,0,0.836917,"evel larger discourse units in a bottom-up fashion. In this example, 7 EDUs are connected by 6 rhetorical relations, while in each non-terminal node, the rhetorical relation and the nuclearity type are labeled. Correspondingly, text-level DRS parsing consists of three components, i.e., bare DRS generation (hierarchical span determination), rhetorical nuclearity determination and rhetorical relation classification. During the past decade, text-level DRS parsing has been drawing more and more attention and achieved certain success (Hernault et al., 2010; Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Heilman and Sagae, 2015; Li et al., 2016; Braud et al., 2017; Yu et al., 2018). However, all the previous studies on text-level DRS parsing adopt bottom-up approaches. That is, adjacent EDUs are recursively combined into high-level larger text spans by rhetorical relations to form a final discourse tree in a bottom-up way. In this paper, we justify that compared with a bottom-up approach, a top-down approach may be more suitable for textlevel DRS parsing from two points-of-view, • From the computational view, only local information (i.e., the constructed DRS subtrees and their context) can b"
2020.acl-main.569,P17-1092,0,0.0827807,"Missing"
2020.acl-main.569,P13-1048,0,0.684754,"tational Linguistics combined into high level larger discourse units in a bottom-up fashion. In this example, 7 EDUs are connected by 6 rhetorical relations, while in each non-terminal node, the rhetorical relation and the nuclearity type are labeled. Correspondingly, text-level DRS parsing consists of three components, i.e., bare DRS generation (hierarchical span determination), rhetorical nuclearity determination and rhetorical relation classification. During the past decade, text-level DRS parsing has been drawing more and more attention and achieved certain success (Hernault et al., 2010; Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Heilman and Sagae, 2015; Li et al., 2016; Braud et al., 2017; Yu et al., 2018). However, all the previous studies on text-level DRS parsing adopt bottom-up approaches. That is, adjacent EDUs are recursively combined into high-level larger text spans by rhetorical relations to form a final discourse tree in a bottom-up way. In this paper, we justify that compared with a bottom-up approach, a top-down approach may be more suitable for textlevel DRS parsing from two points-of-view, • From the computational view, only local information (i.e., the co"
2020.acl-main.569,D14-1220,0,0.45585,"nstein, 2016), sentiment analysis (Choi et al., 2016), text categorization (Ji and Smith, 2017), pronoun Corresponding author Coordinating (NN) e2 Introduction ⇤ Overall-branch (SN) Purpose (SN) resolution (Sheng et al., 2017) and event temporal relation identification (Dai et al., 2019). According to Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), a text can be presented by a hierarchical tree structure known as a Discourse Tree(DT). Figure 1 illustrates an excerpt with its gold standard DRS from article chtb 0005 in the Chinese CDTB (Connectivedriven Discourse Treebank) corpus (Li et al., 2014c). We can find that, in the DT, each leaf node corresponds to an elementary discourse unit (EDU), and various EDUs are recursively 6386 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6386–6395 c July 5 - 10, 2020. 2020 Association for Computational Linguistics combined into high level larger discourse units in a bottom-up fashion. In this example, 7 EDUs are connected by 6 rhetorical relations, while in each non-terminal node, the rhetorical relation and the nuclearity type are labeled. Correspondingly, text-level DRS parsing consists of three c"
2020.acl-main.569,D16-1035,0,0.9121,". In this example, 7 EDUs are connected by 6 rhetorical relations, while in each non-terminal node, the rhetorical relation and the nuclearity type are labeled. Correspondingly, text-level DRS parsing consists of three components, i.e., bare DRS generation (hierarchical span determination), rhetorical nuclearity determination and rhetorical relation classification. During the past decade, text-level DRS parsing has been drawing more and more attention and achieved certain success (Hernault et al., 2010; Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Heilman and Sagae, 2015; Li et al., 2016; Braud et al., 2017; Yu et al., 2018). However, all the previous studies on text-level DRS parsing adopt bottom-up approaches. That is, adjacent EDUs are recursively combined into high-level larger text spans by rhetorical relations to form a final discourse tree in a bottom-up way. In this paper, we justify that compared with a bottom-up approach, a top-down approach may be more suitable for textlevel DRS parsing from two points-of-view, • From the computational view, only local information (i.e., the constructed DRS subtrees and their context) can be naturally employed to determine the uppe"
2020.acl-main.569,P14-1003,0,0.35024,"nstein, 2016), sentiment analysis (Choi et al., 2016), text categorization (Ji and Smith, 2017), pronoun Corresponding author Coordinating (NN) e2 Introduction ⇤ Overall-branch (SN) Purpose (SN) resolution (Sheng et al., 2017) and event temporal relation identification (Dai et al., 2019). According to Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), a text can be presented by a hierarchical tree structure known as a Discourse Tree(DT). Figure 1 illustrates an excerpt with its gold standard DRS from article chtb 0005 in the Chinese CDTB (Connectivedriven Discourse Treebank) corpus (Li et al., 2014c). We can find that, in the DT, each leaf node corresponds to an elementary discourse unit (EDU), and various EDUs are recursively 6386 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6386–6395 c July 5 - 10, 2020. 2020 Association for Computational Linguistics combined into high level larger discourse units in a bottom-up fashion. In this example, 7 EDUs are connected by 6 rhetorical relations, while in each non-terminal node, the rhetorical relation and the nuclearity type are labeled. Correspondingly, text-level DRS parsing consists of three c"
2020.acl-main.569,D14-1224,1,0.931395,"Missing"
2020.acl-main.569,P19-1410,0,0.439963,"d model is employed to parse a bare discourse tree. Then, an independent relation labeller is adopted to determine discourse relations. Braud et al. (2017) present two variants of transition-based discourse parsing using a feedforward neural network model. Yu et al. (2018) build a transition based RST parser with implicit syntactic features. In particular, the information of 6387 sentence boundaries and paragraph boundaries is embedded as additional features. It is worthwhile to emphasize that, all the above studies on text-level discourse parsing employ the bottom-up approaches. So far, only Lin et al. (2019) and Liu et al. (2019) make the preliminary explorations on constructing sentence-level DTs in a top-down fashion. Lin et al. (2019) proposed a framework for both the EDU segmenter and the sentence-level discourse parser uniformly. Following the work of Lin et al. (2019), Liu et al. (2019) proposed hierarchical pointer network for better dependency and sentence-level discourse parsing. However, both studies consider merely sentencelevel discourse parsing. While it is simple but effective to encode entire sentence sequentially, entire text-level discourse larger than sentence, such as paragraph"
2020.acl-main.569,J18-2001,0,0.0528363,"d ways to measure the performance: unlabeled (i.e., hierarchical spans) and labeled (i.e., nuclearity and relation) F-scores. Same as previous studies, we evaluate our system with gold EDU segmentation and binarize those non-binary subtrees with right-branching. We use the 18 fine-grained relations defined in (Carlson and Marcu, 2001) and the 16 fine-grained relations defined in (Li et al., 2014c) to evaluate the relation metric for English and Chinese respectively. In order to avoid the problem that the performance with RST-Parseval evaluation (Marcu, 2000) looks unreasonably high, we follow Morey et al. (2018), which adopts the standard Parseval procedure. For fair comparison, we report microaveraged F1 scores by default. 4.1.3 Hyper-parameters We use the word embedding representation based on the 300D vectors provided by Glove (2014)1 and Qiu(2018) for English and Chinese respectively, and do not update the weights of these vectors during training, while the POS embedding uses the random initialization method and is optimized with our model. We fine-tune the hyper-parameters on the development set as shown in Table 1. 4.2 Experimental Results 4.2.1 Overall Performance First, Table 2 compares the d"
2020.acl-main.569,D14-1162,0,0.0823391,"Missing"
2020.acl-main.569,prasad-etal-2008-penn,0,0.0526489,"1 45.7 47.7 Table 2: Performance Comparison.(Bare, bare DRS generation. Nuc, nuclearity determination. Rel, rhetorical relation classification. Full, full discourse parsing. The sign + means the systems with additional handcrafted features including syntactic, contextual and so on, ⇤ means with additional cross-lingual features.) Table 1: Experimental parameter settings. The Chinese CDTB corpus is motivated by taking both advantages of the English RST-DT corpus (e.g. the tree structure, the nuclearity representation) and the PDTB corpus (e.g., the connectivedriven predict-argument structure) (Prasad et al., 2008). In the Chinese CDTB corpus, each paragraph is marked as a Connective-driven Discourse Tree (CDT), where its leaf nodes are EDUs, its intermediate nodes represent (insertable) connectives (i.e., discourse relations), and EDUs connected by connectives can be combined into higher level discourse units. Currently, the Chinese CDTB corpus consists of 500 newswire articles, which are further divided into 2336 paragraphs with a CDT representation for one paragraph and 10650 EDUs in total. We divide the corpus into three parts, i.e., 425 training documents containing 2002 discourse trees and 6967 di"
2020.acl-main.569,P17-2029,0,0.513061,"Missing"
2020.acl-main.569,C18-1047,0,0.675213,"ed by 6 rhetorical relations, while in each non-terminal node, the rhetorical relation and the nuclearity type are labeled. Correspondingly, text-level DRS parsing consists of three components, i.e., bare DRS generation (hierarchical span determination), rhetorical nuclearity determination and rhetorical relation classification. During the past decade, text-level DRS parsing has been drawing more and more attention and achieved certain success (Hernault et al., 2010; Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Heilman and Sagae, 2015; Li et al., 2016; Braud et al., 2017; Yu et al., 2018). However, all the previous studies on text-level DRS parsing adopt bottom-up approaches. That is, adjacent EDUs are recursively combined into high-level larger text spans by rhetorical relations to form a final discourse tree in a bottom-up way. In this paper, we justify that compared with a bottom-up approach, a top-down approach may be more suitable for textlevel DRS parsing from two points-of-view, • From the computational view, only local information (i.e., the constructed DRS subtrees and their context) can be naturally employed to determine the upper layer structure in the bottom-up fas"
2020.acl-main.569,miltsakaki-etal-2004-penn,0,\N,Missing
2020.acl-main.569,D19-1093,0,\N,Missing
2020.ccl-1.22,J93-2003,0,0.141654,"Missing"
2020.ccl-1.22,P17-2025,0,0.060887,"Missing"
2020.ccl-1.22,N16-1014,0,0.0667337,"Missing"
2020.ccl-1.22,D17-1230,0,0.066836,"Missing"
2020.ccl-1.22,I17-1099,0,0.0289316,"Missing"
2020.ccl-1.22,C16-1316,0,0.0325835,"Missing"
2020.ccl-1.22,P02-1040,0,0.10603,"Missing"
2020.ccl-1.22,W12-2018,0,0.0501177,"Missing"
2020.ccl-1.22,D17-1235,0,0.0498423,"Missing"
2020.ccl-1.22,N15-1020,0,0.0668821,"Missing"
2020.ccl-1.22,D15-1199,0,0.0392959,"Missing"
2020.ccl-1.22,D19-1124,0,0.0419545,"Missing"
2020.ccl-1.22,D19-1186,0,0.0332404,"Missing"
2020.ccl-1.22,P18-1102,0,0.0318686,"Missing"
2020.coling-main.221,P18-1236,0,0.0859762,"al studies illustrate the importance of proposed sentiment forecasting task, and justify the effectiveness of our NSF model over several strong baselines. 1 Introduction Developing intelligent chatbots is of great appealing to both the industry and the academics. However it is challenging to build up such an intelligent chatbot which involves a series of high-level natural language processing techniques, such as sentiment analysis of utterances in dialog. Previous studies on sentiment classification focus on determining polarity (positive or negative) in a single document (Pang and Lee, 2008; Amplayo et al., 2018). In comparison, only few studies focus on determining polarity of utterances in dialog (Herzig et al., 2016; Majumder et al., 2018). However, all of these studies focus on determining the polarity of existing utterances. It may be more important to predict the polarity of next utterance yet to come. Given the example in Figure 1, although B expresses a positive sentiment in second utterance, A still shows a negative sentiment in his response. In this case, if B know that A would be very upset after his first utterance, he may revise his utterance to let A feel more comfortable. Hence, predict"
2020.coling-main.221,C18-1063,0,0.0192703,"Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sentimental text on the customer’s perception of the service agent. On the basic, Herzig et al. (2016) used SVM to classify sentiment in customer support dialogs by integrating both text based turn and dialog features. Recently, Cerisara et al. (2018) proposed a multi-task hierarchical recurrent network to classify sentiment and dialog act jointly. Majumder et al. (2018) proposed a recurrent neural networks to track of the individual states throughout the dialog and employed this information for sentiment classification. Different from previous studies which focus on detecting the polarity of existing utterances, we propose a novel and important task to forecast polarity of next utterance yet to come. 3 Neural Sentiment Forecasting Modeling As illustrated in Figure 1, given a existed utterance sequence {u1 , u2 , ..., un−1 } in a dialog d,"
2020.coling-main.221,D16-1171,0,0.0190581,"to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sentimental text on the customer’s perception of the service agent. On the basic, Herzig et al. (2016) used SVM to classify sentiment in customer support dialogs by integrating both text based turn and dialog features. Recently,"
2020.coling-main.221,D18-1280,0,0.0169534,"el to learn the representation of ui (1 ≤ i ≤ 3) (Section 3.1), and then employs the representation of ui to forecasting sentiment of next utterance un . • LSTMseq is a sequence based sentiment forecasting model, it employs a LSTM model to learn dialog representation d from the existed utterance sequence {u1 , u2 , u3 } (Eq. 2), and then employ the dialog representation d to forecast sentiment of un . • ICON takes one utterance with previous k utterances as input, and uses a GRU model for modeling inter-personal dependency in previous utterances and stores all history with one memory network (Hazarika et al., 2018). 1 There exist six categories of emotion in the dataset: joy, anger, disgust, fear, sadness, and surprise. Besides, many utterances do not express any emotion (i.e., neutral). 2453 Table 1: Comparison with baselines. Pos F1. Neg F1. Avg F1. LSTM1 0.545 0.285 0.415 2 LSTM 0.506 0.310 0.408 LSTM3 0.558 0.273 0.415 seq LSTM 0.563 0.342 0.453 ICON 0.529 0.293 0.411 DialogRNN 0.540 0.358 0.449 NSF 0.586 0.387 0.486 • DialogRNN employs recurrent neural networks to keep track of the individual states of utterances and uses this information for sentiment classification in dialog (Majumder et al., 201"
2020.coling-main.221,W16-3609,0,0.137295,"our NSF model over several strong baselines. 1 Introduction Developing intelligent chatbots is of great appealing to both the industry and the academics. However it is challenging to build up such an intelligent chatbot which involves a series of high-level natural language processing techniques, such as sentiment analysis of utterances in dialog. Previous studies on sentiment classification focus on determining polarity (positive or negative) in a single document (Pang and Lee, 2008; Amplayo et al., 2018). In comparison, only few studies focus on determining polarity of utterances in dialog (Herzig et al., 2016; Majumder et al., 2018). However, all of these studies focus on determining the polarity of existing utterances. It may be more important to predict the polarity of next utterance yet to come. Given the example in Figure 1, although B expresses a positive sentiment in second utterance, A still shows a negative sentiment in his response. In this case, if B know that A would be very upset after his first utterance, he may revise his utterance to let A feel more comfortable. Hence, predicting the polarity of the next utterance can help a speaker to improve their utterances, which is important in"
2020.coling-main.221,D18-1136,0,0.0213367,": Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sentimental text on the customer’s perception of the service agent. On the basic, Herzig et al. (2016) used SVM to classify sentiment in customer suppor"
2020.coling-main.221,D14-1181,0,0.00759751,"inguistics, pages 2448–2458 Barcelona, Spain (Online), December 8-13, 2020 A: John, I&apos;ve asked you not to smoke in here! I don&apos;t want to see you smoking in my office again. B: I&apos;m sorry, Ms. Fairbanks. I won&apos;t let it happen again. A: Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated tha"
2020.coling-main.221,I17-1099,0,0.0645772,"Missing"
2020.coling-main.221,D13-1170,0,0.0067668,"Spain (Online), December 8-13, 2020 A: John, I&apos;ve asked you not to smoke in here! I don&apos;t want to see you smoking in my office again. B: I&apos;m sorry, Ms. Fairbanks. I won&apos;t let it happen again. A: Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversation"
2020.coling-main.221,P15-1150,0,0.0123949,"oke in here! I don&apos;t want to see you smoking in my office again. B: I&apos;m sorry, Ms. Fairbanks. I won&apos;t let it happen again. A: Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sentimental text on th"
2020.coling-main.221,C16-1311,0,0.0168024,"banks. I won&apos;t let it happen again. A: Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sentimental text on the customer’s perception of the service agent. On the basic, Herzig et al. (2016) used S"
2020.coling-main.221,D16-1169,1,0.852207,"asked you not to smoke in here! I don&apos;t want to see you smoking in my office again. B: I&apos;m sorry, Ms. Fairbanks. I won&apos;t let it happen again. A: Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sen"
2020.coling-main.242,W15-4319,0,0.0418861,"Missing"
2020.coling-main.242,D18-1454,0,0.021553,"ar in the passages. 2688 Normalizer SPLT EXPN WDLK MISC Informal type Mixed tokens Abbreviation Misspelling word Other Example #InTheUnlikelyEvent (# In The Unlikely Event) u (you), convo (conversation), addr (address) preety (pretty), goverment (government) sh*t (·), :-) (·) Table 2: Taxonomy of informal texts and non-standard word normalizers. The revised texts are in parentheses; “(·)” denotes deletion. With the development of natural language generation technology, researchers focused on using generative models to solve reading comprehension problems. For example, McCann et al. (2018) and Bauer et al. (2018) used RNN-based pointer generation mechanisms to generate answers from a single document. Tan et al. (2018) adopted a pipeline method in multi-document reading comprehension. In this paper, we propose a multi-task learning based generative RC model that integrates the advantages of both the extractive and the generative RC models by share hidden state representations. Noisy User-generated Text-oriented NLP Recently, due to the increasing number of social media users, many research directions of NLP are required for processing the noisy user-generated texts in social media, such as tweets. The"
2020.coling-main.242,D18-1241,0,0.0151838,"ill et al., 2015; Seo et al., 2016) or multiple-choice (Richardson et al., 2013; Lai et al., 2017). However, these models are difficult to be directly utilized for real application scenarios. Recently, many large-scale RC datasets constructed by a crowdsourced way (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018) have been proposed and received widespread attentions (Wang et al., 2017; Min et al., 2019). Besides, to make RC models have the ability of conversation understanding like a human being, more challenging multi-round conversational RC datasets are proposed, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019). After Transformer (Vaswani et al., 2017) has been released, various pre-trained models (Devlin et al., 2019; Yang et al., 2019; Dong et al., 2019) have sprung up and achieved promising results on most of the RC datasets through purely fine-tuning. The parameters of our NUT-RC model are also initialized by BERT (Devlin et al., 2019). According to the original formats of the predicted answers, the existing RC model can be roughly divided into two major categories: extractive and generative. In an extractive RC model, the answers are limited to be a span of the giv"
2020.coling-main.242,W11-2107,0,0.050686,"Missing"
2020.coling-main.242,N19-1423,0,0.423501,"be directly utilized for real application scenarios. Recently, many large-scale RC datasets constructed by a crowdsourced way (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018) have been proposed and received widespread attentions (Wang et al., 2017; Min et al., 2019). Besides, to make RC models have the ability of conversation understanding like a human being, more challenging multi-round conversational RC datasets are proposed, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019). After Transformer (Vaswani et al., 2017) has been released, various pre-trained models (Devlin et al., 2019; Yang et al., 2019; Dong et al., 2019) have sprung up and achieved promising results on most of the RC datasets through purely fine-tuning. The parameters of our NUT-RC model are also initialized by BERT (Devlin et al., 2019). According to the original formats of the predicted answers, the existing RC model can be roughly divided into two major categories: extractive and generative. In an extractive RC model, the answers are limited to be a span of the given passages. Most of the mainstream RC models are the extractive RC model. In a generative RC model, the answers can be free-text and do no"
2020.coling-main.242,W17-4414,0,0.126078,"mainly concentrate on part-of-speech tagging and dependency parser in tweets. For example, Foster et al. (2011) annotated 7,630 PoS tags according to the Penn Treebank (Marcus et al., 1993); Kong et al. (2014) built a dependency parser for tweets based on the TWEEBANK, which is the first dataset annotated with syntactic information on tweets. Besides, the ACL-IJCNLP Workshop (Baldwin et al., 2015) provides a shared task on noisy user-generated text processing, including twitter lexical normalization and named entity recognition, which stimulated many studies on this topic (Godin et al., 2015; Flint et al., 2017; Liu et al., 2018). Recently, Xiong et al. (2019) released the first large-scale RC dataset over social media data, TweetQA, which gathered tweets used in news articles and encouraged human annotators to write questions and answers upon these tweets in their language. To the best of our knowledge, there are rare reading comprehension models over social media, and the proposed NUT-RC model is inspired by TweetQA. 3 Noisy User-Generated Text-oriented Reading Comprehension Figure 1 illustrates the architecture of the noisy user-generated text-oriented reading comprehension model (NUT-RC), which"
2020.coling-main.242,W15-4322,0,0.0480543,"Missing"
2020.coling-main.242,P17-1147,0,0.109537,"been proposed and have achieved considerable successes. According to answer prediction methods, the RC models can be roughly divided into two major categories: extractive and generative. For an extractive RC model, the predicted answer is limited to be a consecutive span in the passage. For a generative RC model, it allows the answer to be free-text which can include novel words and phrases not appeared in passages. However, most of the existing RC models are developed for formal text, such as news articles (Hermann et al., 2015; Trischler et al., 2017) and Wikipedia (Rajpurkar et al., 2016; Joshi et al., 2017), which severely limit their performances on Noisy User-generated Texts (NUT). Meanwhile, an increasing number of people are accustomed to getting real-time information via social media like Twitter. Recently, Xiong et al. (2019) propose a large-scale dataset for question answering over social media texts, TweetQA, which is constructed in a crowd-sourcing way, and recommended annotators to write answers in their own language. Table 1 shows an example from the TweetQA dataset. In order to answer the question “what is kerry thankful for?”, an RC model first needs to be positioned to “Thank u 4 o"
2020.coling-main.242,D18-1452,0,0.0588946,"Missing"
2020.coling-main.242,D14-1108,0,0.0407993,"Missing"
2020.coling-main.242,D17-1082,0,0.0185176,"iliary task to optimize the generative RC model by multi-task learning; and (3) empirical verification of the effectiveness of the model and achieving the state-of-the-art performance on TweetQA. 2 Related Work Reading Comprehension Reading comprehension (RC) aims to teach a machine answering questions by comprehending the context of given passages, which is one of the most important tasks in the NLP community automatically. The prior research on RC mainly focuses on either the cloze-style (Hermann et al., 2015; Hill et al., 2015; Seo et al., 2016) or multiple-choice (Richardson et al., 2013; Lai et al., 2017). However, these models are difficult to be directly utilized for real application scenarios. Recently, many large-scale RC datasets constructed by a crowdsourced way (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018) have been proposed and received widespread attentions (Wang et al., 2017; Min et al., 2019). Besides, to make RC models have the ability of conversation understanding like a human being, more challenging multi-round conversational RC datasets are proposed, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019). After Transformer (Vaswani et al., 2017) has bee"
2020.coling-main.242,P18-1161,0,0.0211775,"troduce a set of text normalizers to transform the noisy and informal texts to the formal ones. Then, we integrate the extractive and the generative RC model by a multi-task learning mechanism and an answer selection module. Experimental results on TweetQA demonstrate that our NUT-RC model significantly outperforms the state-of-the-art social media-oriented RC models. 1 Introduction Reading Comprehension (RC), which aims to answer questions by comprehending the contexts of given passages, is a frontier topic in natural language processing research. Recently, many RC models (Wang et al., 2018; Lin et al., 2018; Zhu et al., 2018; Joty et al., 2018; Weber et al., 2019) have been proposed and have achieved considerable successes. According to answer prediction methods, the RC models can be roughly divided into two major categories: extractive and generative. For an extractive RC model, the predicted answer is limited to be a consecutive span in the passage. For a generative RC model, it allows the answer to be free-text which can include novel words and phrases not appeared in passages. However, most of the existing RC models are developed for formal text, such as news articles (Hermann et al., 2015;"
2020.coling-main.242,W04-1013,0,0.0276995,"Missing"
2020.coling-main.242,N18-1088,0,0.0179535,"n part-of-speech tagging and dependency parser in tweets. For example, Foster et al. (2011) annotated 7,630 PoS tags according to the Penn Treebank (Marcus et al., 1993); Kong et al. (2014) built a dependency parser for tweets based on the TWEEBANK, which is the first dataset annotated with syntactic information on tweets. Besides, the ACL-IJCNLP Workshop (Baldwin et al., 2015) provides a shared task on noisy user-generated text processing, including twitter lexical normalization and named entity recognition, which stimulated many studies on this topic (Godin et al., 2015; Flint et al., 2017; Liu et al., 2018). Recently, Xiong et al. (2019) released the first large-scale RC dataset over social media data, TweetQA, which gathered tweets used in news articles and encouraged human annotators to write questions and answers upon these tweets in their language. To the best of our knowledge, there are rare reading comprehension models over social media, and the proposed NUT-RC model is inspired by TweetQA. 3 Noisy User-Generated Text-oriented Reading Comprehension Figure 1 illustrates the architecture of the noisy user-generated text-oriented reading comprehension model (NUT-RC), which comprises six basic"
2020.coling-main.242,J93-2004,0,0.0716815,"s paper, we propose a multi-task learning based generative RC model that integrates the advantages of both the extractive and the generative RC models by share hidden state representations. Noisy User-generated Text-oriented NLP Recently, due to the increasing number of social media users, many research directions of NLP are required for processing the noisy user-generated texts in social media, such as tweets. The prior works mainly concentrate on part-of-speech tagging and dependency parser in tweets. For example, Foster et al. (2011) annotated 7,630 PoS tags according to the Penn Treebank (Marcus et al., 1993); Kong et al. (2014) built a dependency parser for tweets based on the TWEEBANK, which is the first dataset annotated with syntactic information on tweets. Besides, the ACL-IJCNLP Workshop (Baldwin et al., 2015) provides a shared task on noisy user-generated text processing, including twitter lexical normalization and named entity recognition, which stimulated many studies on this topic (Godin et al., 2015; Flint et al., 2017; Liu et al., 2018). Recently, Xiong et al. (2019) released the first large-scale RC dataset over social media data, TweetQA, which gathered tweets used in news articles a"
2020.coling-main.242,P19-1613,0,0.0116933,"he context of given passages, which is one of the most important tasks in the NLP community automatically. The prior research on RC mainly focuses on either the cloze-style (Hermann et al., 2015; Hill et al., 2015; Seo et al., 2016) or multiple-choice (Richardson et al., 2013; Lai et al., 2017). However, these models are difficult to be directly utilized for real application scenarios. Recently, many large-scale RC datasets constructed by a crowdsourced way (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018) have been proposed and received widespread attentions (Wang et al., 2017; Min et al., 2019). Besides, to make RC models have the ability of conversation understanding like a human being, more challenging multi-round conversational RC datasets are proposed, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019). After Transformer (Vaswani et al., 2017) has been released, various pre-trained models (Devlin et al., 2019; Yang et al., 2019; Dong et al., 2019) have sprung up and achieved promising results on most of the RC datasets through purely fine-tuning. The parameters of our NUT-RC model are also initialized by BERT (Devlin et al., 2019). According to the original formats o"
2020.coling-main.242,P02-1040,0,0.106357,"Missing"
2020.coling-main.242,D16-1264,0,0.251542,"Weber et al., 2019) have been proposed and have achieved considerable successes. According to answer prediction methods, the RC models can be roughly divided into two major categories: extractive and generative. For an extractive RC model, the predicted answer is limited to be a consecutive span in the passage. For a generative RC model, it allows the answer to be free-text which can include novel words and phrases not appeared in passages. However, most of the existing RC models are developed for formal text, such as news articles (Hermann et al., 2015; Trischler et al., 2017) and Wikipedia (Rajpurkar et al., 2016; Joshi et al., 2017), which severely limit their performances on Noisy User-generated Texts (NUT). Meanwhile, an increasing number of people are accustomed to getting real-time information via social media like Twitter. Recently, Xiong et al. (2019) propose a large-scale dataset for question answering over social media texts, TweetQA, which is constructed in a crowd-sourcing way, and recommended annotators to write answers in their own language. Table 1 shows an example from the TweetQA dataset. In order to answer the question “what is kerry thankful for?”, an RC model first needs to be posit"
2020.coling-main.242,Q19-1016,0,0.0150036,"2016) or multiple-choice (Richardson et al., 2013; Lai et al., 2017). However, these models are difficult to be directly utilized for real application scenarios. Recently, many large-scale RC datasets constructed by a crowdsourced way (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018) have been proposed and received widespread attentions (Wang et al., 2017; Min et al., 2019). Besides, to make RC models have the ability of conversation understanding like a human being, more challenging multi-round conversational RC datasets are proposed, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019). After Transformer (Vaswani et al., 2017) has been released, various pre-trained models (Devlin et al., 2019; Yang et al., 2019; Dong et al., 2019) have sprung up and achieved promising results on most of the RC datasets through purely fine-tuning. The parameters of our NUT-RC model are also initialized by BERT (Devlin et al., 2019). According to the original formats of the predicted answers, the existing RC model can be roughly divided into two major categories: extractive and generative. In an extractive RC model, the answers are limited to be a span of the given passages. Most of the mains"
2020.coling-main.242,D13-1020,0,0.0198942,"e extractive RC as an auxiliary task to optimize the generative RC model by multi-task learning; and (3) empirical verification of the effectiveness of the model and achieving the state-of-the-art performance on TweetQA. 2 Related Work Reading Comprehension Reading comprehension (RC) aims to teach a machine answering questions by comprehending the context of given passages, which is one of the most important tasks in the NLP community automatically. The prior research on RC mainly focuses on either the cloze-style (Hermann et al., 2015; Hill et al., 2015; Seo et al., 2016) or multiple-choice (Richardson et al., 2013; Lai et al., 2017). However, these models are difficult to be directly utilized for real application scenarios. Recently, many large-scale RC datasets constructed by a crowdsourced way (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018) have been proposed and received widespread attentions (Wang et al., 2017; Min et al., 2019). Besides, to make RC models have the ability of conversation understanding like a human being, more challenging multi-round conversational RC datasets are proposed, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019). After Transformer (Vaswani et"
2020.coling-main.242,W17-2623,0,0.0186848,"; Zhu et al., 2018; Joty et al., 2018; Weber et al., 2019) have been proposed and have achieved considerable successes. According to answer prediction methods, the RC models can be roughly divided into two major categories: extractive and generative. For an extractive RC model, the predicted answer is limited to be a consecutive span in the passage. For a generative RC model, it allows the answer to be free-text which can include novel words and phrases not appeared in passages. However, most of the existing RC models are developed for formal text, such as news articles (Hermann et al., 2015; Trischler et al., 2017) and Wikipedia (Rajpurkar et al., 2016; Joshi et al., 2017), which severely limit their performances on Noisy User-generated Texts (NUT). Meanwhile, an increasing number of people are accustomed to getting real-time information via social media like Twitter. Recently, Xiong et al. (2019) propose a large-scale dataset for question answering over social media texts, TweetQA, which is constructed in a crowd-sourcing way, and recommended annotators to write answers in their own language. Table 1 shows an example from the TweetQA dataset. In order to answer the question “what is kerry thankful for?"
2020.coling-main.242,P17-1018,0,0.0254948,"by comprehending the context of given passages, which is one of the most important tasks in the NLP community automatically. The prior research on RC mainly focuses on either the cloze-style (Hermann et al., 2015; Hill et al., 2015; Seo et al., 2016) or multiple-choice (Richardson et al., 2013; Lai et al., 2017). However, these models are difficult to be directly utilized for real application scenarios. Recently, many large-scale RC datasets constructed by a crowdsourced way (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018) have been proposed and received widespread attentions (Wang et al., 2017; Min et al., 2019). Besides, to make RC models have the ability of conversation understanding like a human being, more challenging multi-round conversational RC datasets are proposed, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019). After Transformer (Vaswani et al., 2017) has been released, various pre-trained models (Devlin et al., 2019; Yang et al., 2019; Dong et al., 2019) have sprung up and achieved promising results on most of the RC datasets through purely fine-tuning. The parameters of our NUT-RC model are also initialized by BERT (Devlin et al., 2019). According to the"
2020.coling-main.242,P18-1158,0,0.0116626,"icular, we first introduce a set of text normalizers to transform the noisy and informal texts to the formal ones. Then, we integrate the extractive and the generative RC model by a multi-task learning mechanism and an answer selection module. Experimental results on TweetQA demonstrate that our NUT-RC model significantly outperforms the state-of-the-art social media-oriented RC models. 1 Introduction Reading Comprehension (RC), which aims to answer questions by comprehending the contexts of given passages, is a frontier topic in natural language processing research. Recently, many RC models (Wang et al., 2018; Lin et al., 2018; Zhu et al., 2018; Joty et al., 2018; Weber et al., 2019) have been proposed and have achieved considerable successes. According to answer prediction methods, the RC models can be roughly divided into two major categories: extractive and generative. For an extractive RC model, the predicted answer is limited to be a consecutive span in the passage. For a generative RC model, it allows the answer to be free-text which can include novel words and phrases not appeared in passages. However, most of the existing RC models are developed for formal text, such as news articles (Herm"
2020.coling-main.242,P19-1618,0,0.0226783,"Missing"
2020.coling-main.242,P19-1496,0,0.130926,"answer is limited to be a consecutive span in the passage. For a generative RC model, it allows the answer to be free-text which can include novel words and phrases not appeared in passages. However, most of the existing RC models are developed for formal text, such as news articles (Hermann et al., 2015; Trischler et al., 2017) and Wikipedia (Rajpurkar et al., 2016; Joshi et al., 2017), which severely limit their performances on Noisy User-generated Texts (NUT). Meanwhile, an increasing number of people are accustomed to getting real-time information via social media like Twitter. Recently, Xiong et al. (2019) propose a large-scale dataset for question answering over social media texts, TweetQA, which is constructed in a crowd-sourcing way, and recommended annotators to write answers in their own language. Table 1 shows an example from the TweetQA dataset. In order to answer the question “what is kerry thankful for?”, an RC model first needs to be positioned to “Thank u 4 opening this convo”, where “u, 4, convoy” is the abbreviation of “you, for, conversation”, respectively. Furthermore, “u” refers to “@InStyle”. Then the RC model can predict the answer “instyle opening the conversation”. Such an e"
2020.coling-main.242,D18-1259,0,0.0188694,"ion Reading comprehension (RC) aims to teach a machine answering questions by comprehending the context of given passages, which is one of the most important tasks in the NLP community automatically. The prior research on RC mainly focuses on either the cloze-style (Hermann et al., 2015; Hill et al., 2015; Seo et al., 2016) or multiple-choice (Richardson et al., 2013; Lai et al., 2017). However, these models are difficult to be directly utilized for real application scenarios. Recently, many large-scale RC datasets constructed by a crowdsourced way (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018) have been proposed and received widespread attentions (Wang et al., 2017; Min et al., 2019). Besides, to make RC models have the ability of conversation understanding like a human being, more challenging multi-round conversational RC datasets are proposed, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019). After Transformer (Vaswani et al., 2017) has been released, various pre-trained models (Devlin et al., 2019; Yang et al., 2019; Dong et al., 2019) have sprung up and achieved promising results on most of the RC datasets through purely fine-tuning. The parameters of our NUT-RC m"
2020.coling-main.282,C18-1048,0,0.108324,"2) in Figure 1 hold a causal relation, where the possible connective “because” is not given. Interactive attention [Arg1: Psyllium’s not a good crop.] [Arg2: You get a rain at the wrong time and the crop is ruined.] Self-attention Figure 1: An example of causally-related arguments. Since the time when IDRR was boiled down to a problem of discourse relation classification (Pitler et al., 2009; Lin et al., 2014), intense interest has been devoted to the study of argument representation and neural relation classification (Zhang et al., 2015; Liu et al., 2016; Chen et al., 2016; Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). Context-specific non-interactive attention mechanism (also referred to self-attention mechanism) (Lin et al., 2017) and companion-dependent interactive attention mechanism (Ma et al., 2017; Meng et al., 2016) have been used for enhancing the sentence-level embedding process. Both are proven effective in argument encoding (Guo et al., 2018; Liu and Li, 2016) as well as the perception of discourse relations. During encoding, the self-attention mechanism is able to highlight the latent information of attention-worthy words conditioned on local context (Note: we refer the a"
2020.coling-main.282,P16-1163,0,0.359836,"le, the arguments (i.e., Arg1 and Arg2) in Figure 1 hold a causal relation, where the possible connective “because” is not given. Interactive attention [Arg1: Psyllium’s not a good crop.] [Arg2: You get a rain at the wrong time and the crop is ruined.] Self-attention Figure 1: An example of causally-related arguments. Since the time when IDRR was boiled down to a problem of discourse relation classification (Pitler et al., 2009; Lin et al., 2014), intense interest has been devoted to the study of argument representation and neural relation classification (Zhang et al., 2015; Liu et al., 2016; Chen et al., 2016; Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). Context-specific non-interactive attention mechanism (also referred to self-attention mechanism) (Lin et al., 2017) and companion-dependent interactive attention mechanism (Ma et al., 2017; Meng et al., 2016) have been used for enhancing the sentence-level embedding process. Both are proven effective in argument encoding (Guo et al., 2018; Liu and Li, 2016) as well as the perception of discourse relations. During encoding, the self-attention mechanism is able to highlight the latent information of attention-worthy words conditioned"
2020.coling-main.282,N18-1013,0,0.0584646,"the P-value, the higher the significance (Dror et al., 2018). We calculate P-values by comparing the experimental results of IPAL and the updated version (Multihead self-attention+IPAL) with those of others. Similarly, we consider two scenarios in which the GloVE based BiLSTM and BERT respectively cooperate with IPAL. We show the results of significance tests in Table 3, where the P-values which are lower than the threshold are marked with the sign “∗”. 3172 Method Zhang et al. (2015) Chen et al. (2016) Qin et al. (2016) Liu et al. (2016) Liu and Li (2016) Qin et al. (2017) Lan et al. (2017) Dai and Huang (2018) Lei et al. (2018) Guo et al. (2018) Bai and Zhao (2018) Nguyen et al. (2019) He et al (2020) IPAL+Multihead (BERT) COM 33.22 40.17 41.55 37.91 36.70 40.87 40.73 46.79 43.24 40.35 47.85 48.44 47.98 46.75 CON 52.04 54.76 57.32 55.88 54.48 54.56 58.96 57.09 57.82 56.81 54.47 56.84 55.62 59.56 EXP 69.59 71.50 69.97 70.43 72.38 72.47 70.41 72.88 72.11 70.60 73.66 69.37 75.83 TEM 30.54 31.32 35.43 37.17 38.84 36.20 38.50 45.61 29.10 38.65 36.97 38.60 38.94 39.35 Table 4: Comparisons with the state of the art The P-values listed in Table 3 demonstrate that IPAL yields statistically significant impro"
2020.coling-main.282,N19-1423,0,0.172773,"state ht ∈ Rdh and a backward hidden state ht ∈ Rdh . → − ← − → − ← − We concatenate ht and ht to form the synthetic hidden state ht = [ ht , ht ]. Accordingly, the inattentive sentence-level embeddings of the arguments (i.e., inattentive representations) can be represented as follows, where L is the maximum length of an argument: ( Arg1 : H1 ∈ RL×2dh = (h11 , ..., h1L ) (1) Arg2 : H2 ∈ RL×2dh = (h21 , ..., h2L ) The entries of BiLSTM layer are constituted of pretrained GloVE word embeddings (Pennington et al., 2014). In our experiments, we additionally evaluate the effect of fine-tuned BERT (Devlin et al., 2019) which is deployed as the substitution of the GloVE based BiLSTM. 2.2 Classic Self-Attention Mechanism For an argument, we first compute the self-attention vector α ∈ RL merely using intrinsic information in the argument itself. Lin et al. (2017)’s self-attention mechanism is used: ˇ α tanh(Wα H&gt; )) α = softmax(W (2) ˇ α ∈ Rda are learnable parameters, while da is a hyperparameter need to be where Wα ∈ Rda ×2dh and W tuned heuristically. On the basis, the self-attentive argument representation is computed as follows: Hα = αH 2.3 (3) Interactive Attention Propagation We model the interaction be"
2020.coling-main.282,P18-1128,0,0.0130014,"version to form IPAL instead of the classic one. It can be observed that the updated IPAL achieves better performance (see the bottom row in Table 2). 3.4 Discussion 1: Statistical Significance Testing We follow Johnson (1999) to use the sampling-based P-values for examining the significance. Johnson (1999) suggests that the ideal threshold of P-value is 0.05. It indicates that a system achieves significant improvements over others only if P-values are less than 0.05, otherwise insignificant. More importantly, it has been demonstrated that the smaller the P-value, the higher the significance (Dror et al., 2018). We calculate P-values by comparing the experimental results of IPAL and the updated version (Multihead self-attention+IPAL) with those of others. Similarly, we consider two scenarios in which the GloVE based BiLSTM and BERT respectively cooperate with IPAL. We show the results of significance tests in Table 3, where the P-values which are lower than the threshold are marked with the sign “∗”. 3172 Method Zhang et al. (2015) Chen et al. (2016) Qin et al. (2016) Liu et al. (2016) Liu and Li (2016) Qin et al. (2017) Lan et al. (2017) Dai and Huang (2018) Lei et al. (2018) Guo et al. (2018) Bai"
2020.coling-main.282,C18-1046,0,0.0633123,"Missing"
2020.coling-main.282,2020.acl-main.14,0,0.0473335,"Missing"
2020.coling-main.282,D17-1134,0,0.197868,"i.e., Arg1 and Arg2) in Figure 1 hold a causal relation, where the possible connective “because” is not given. Interactive attention [Arg1: Psyllium’s not a good crop.] [Arg2: You get a rain at the wrong time and the crop is ruined.] Self-attention Figure 1: An example of causally-related arguments. Since the time when IDRR was boiled down to a problem of discourse relation classification (Pitler et al., 2009; Lin et al., 2014), intense interest has been devoted to the study of argument representation and neural relation classification (Zhang et al., 2015; Liu et al., 2016; Chen et al., 2016; Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). Context-specific non-interactive attention mechanism (also referred to self-attention mechanism) (Lin et al., 2017) and companion-dependent interactive attention mechanism (Ma et al., 2017; Meng et al., 2016) have been used for enhancing the sentence-level embedding process. Both are proven effective in argument encoding (Guo et al., 2018; Liu and Li, 2016) as well as the perception of discourse relations. During encoding, the self-attention mechanism is able to highlight the latent information of attention-worthy words conditioned on local context ("
2020.coling-main.282,D09-1036,0,0.0525221,"closed closed closed closed 783 783 783 783 27 27 27 27 ] ] ] ] Figure 5: Examples of attention weight assignment (Ground-truth attention-worthy words are marked by yellow background, while the predicted ones by blue. A darker color denotes a higher attention weight.) ings, as well as the exploitation of effective features. One of the most important feature engineering approaches uses interrelated word pairs as the reliable features (Marcu and Echihabi, 2002) since they imply semantic relationships. Hereafter, part-of-speech (POS) (Pitler et al., 2009), syntactic structures and dependencies (Lin et al., 2009) and semantic properties (Lei et al., 2018) were used as novel features. Recently, neural networks have been widely studied for argument representation learning (Zhang et al., 2015), which is admitted to be the crucial issue for discourse relation recognition. Due to the capacity of generating low-dimensional continuous representations for arguments, RNNs with Bi-LSTM are used during encoding. Chen et al (2016) couple Bi-LSTM with a gated relevance model. Liu and Li (2016) use multi-layer attention computation over the output of Bi-LSTM. Meanwhile, Liu et al (2016) build a multi-task learning"
2020.coling-main.282,D16-1130,0,0.223023,"al., 2009; Lin et al., 2014), intense interest has been devoted to the study of argument representation and neural relation classification (Zhang et al., 2015; Liu et al., 2016; Chen et al., 2016; Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). Context-specific non-interactive attention mechanism (also referred to self-attention mechanism) (Lin et al., 2017) and companion-dependent interactive attention mechanism (Ma et al., 2017; Meng et al., 2016) have been used for enhancing the sentence-level embedding process. Both are proven effective in argument encoding (Guo et al., 2018; Liu and Li, 2016) as well as the perception of discourse relations. During encoding, the self-attention mechanism is able to highlight the latent information of attention-worthy words conditioned on local context (Note: we refer the attention-worthy words to the ones which play the dominant role in signaling discourse relations). By contrast, the interactive attention mechanism introduces external evidence into the identification of attention-worthy words, and similarly, highlighting their latent information. So far, the two kinds of attention computations are performed separately. However, our survey shows th"
2020.coling-main.282,P02-1047,0,0.227096,"B.A.T B.A.T B.A.T rally rally rally rally spread European markets spread European markets spread European markets spread European markets ] ] ] ] ] ] ] ] ] ] ] ] closed closed closed closed 783 783 783 783 27 27 27 27 ] ] ] ] Figure 5: Examples of attention weight assignment (Ground-truth attention-worthy words are marked by yellow background, while the predicted ones by blue. A darker color denotes a higher attention weight.) ings, as well as the exploitation of effective features. One of the most important feature engineering approaches uses interrelated word pairs as the reliable features (Marcu and Echihabi, 2002) since they imply semantic relationships. Hereafter, part-of-speech (POS) (Pitler et al., 2009), syntactic structures and dependencies (Lin et al., 2009) and semantic properties (Lei et al., 2018) were used as novel features. Recently, neural networks have been widely studied for argument representation learning (Zhang et al., 2015), which is admitted to be the crucial issue for discourse relation recognition. Due to the capacity of generating low-dimensional continuous representations for arguments, RNNs with Bi-LSTM are used during encoding. Chen et al (2016) couple Bi-LSTM with a gated rele"
2020.coling-main.282,C16-1205,0,0.0151382,"igure 1: An example of causally-related arguments. Since the time when IDRR was boiled down to a problem of discourse relation classification (Pitler et al., 2009; Lin et al., 2014), intense interest has been devoted to the study of argument representation and neural relation classification (Zhang et al., 2015; Liu et al., 2016; Chen et al., 2016; Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). Context-specific non-interactive attention mechanism (also referred to self-attention mechanism) (Lin et al., 2017) and companion-dependent interactive attention mechanism (Ma et al., 2017; Meng et al., 2016) have been used for enhancing the sentence-level embedding process. Both are proven effective in argument encoding (Guo et al., 2018; Liu and Li, 2016) as well as the perception of discourse relations. During encoding, the self-attention mechanism is able to highlight the latent information of attention-worthy words conditioned on local context (Note: we refer the attention-worthy words to the ones which play the dominant role in signaling discourse relations). By contrast, the interactive attention mechanism introduces external evidence into the identification of attention-worthy words, and s"
2020.coling-main.282,P19-1411,0,0.765915,"Missing"
2020.coling-main.282,D14-1162,0,0.0890528,"n respectively. By BiLSTM, each word in an argu→ − ← − ment will be transformed into a forward hidden state ht ∈ Rdh and a backward hidden state ht ∈ Rdh . → − ← − → − ← − We concatenate ht and ht to form the synthetic hidden state ht = [ ht , ht ]. Accordingly, the inattentive sentence-level embeddings of the arguments (i.e., inattentive representations) can be represented as follows, where L is the maximum length of an argument: ( Arg1 : H1 ∈ RL×2dh = (h11 , ..., h1L ) (1) Arg2 : H2 ∈ RL×2dh = (h21 , ..., h2L ) The entries of BiLSTM layer are constituted of pretrained GloVE word embeddings (Pennington et al., 2014). In our experiments, we additionally evaluate the effect of fine-tuned BERT (Devlin et al., 2019) which is deployed as the substitution of the GloVE based BiLSTM. 2.2 Classic Self-Attention Mechanism For an argument, we first compute the self-attention vector α ∈ RL merely using intrinsic information in the argument itself. Lin et al. (2017)’s self-attention mechanism is used: ˇ α tanh(Wα H&gt; )) α = softmax(W (2) ˇ α ∈ Rda are learnable parameters, while da is a hyperparameter need to be where Wα ∈ Rda ×2dh and W tuned heuristically. On the basis, the self-attentive argument representation is"
2020.coling-main.282,P09-1077,0,0.0675507,"elationship between arguments, under the condition that there is lack of a connective signaling the relationship. An argument generally stands for a narrative sentence or clause. For example, the arguments (i.e., Arg1 and Arg2) in Figure 1 hold a causal relation, where the possible connective “because” is not given. Interactive attention [Arg1: Psyllium’s not a good crop.] [Arg2: You get a rain at the wrong time and the crop is ruined.] Self-attention Figure 1: An example of causally-related arguments. Since the time when IDRR was boiled down to a problem of discourse relation classification (Pitler et al., 2009; Lin et al., 2014), intense interest has been devoted to the study of argument representation and neural relation classification (Zhang et al., 2015; Liu et al., 2016; Chen et al., 2016; Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). Context-specific non-interactive attention mechanism (also referred to self-attention mechanism) (Lin et al., 2017) and companion-dependent interactive attention mechanism (Ma et al., 2017; Meng et al., 2016) have been used for enhancing the sentence-level embedding process. Both are proven effective in argument encoding (Guo et al., 2018; Liu and Li"
2020.coling-main.282,prasad-etal-2008-penn,0,0.108423,"Given a target relation type r, the set R comprises two (n=2) class labels— rM and rO — which respectively signal a positive sample (argument pair) which holds the target relation and a negative sample which doesn’t hold the relation. During training, we minimize the binary classification loss L. The cost function is the cross-entropy of yr and yˆr for both the class labels rM and rO , where yrM , yrO ∈ {0,1} denotes the ground truth: L = −yrM log(ˆ yrM ) − yrO log(ˆ yrO ) 3 3.1 (8) Experimentation Datasets and Evaluation Metric We follow the common practice to use section 02-20 of PDTB v2.0 (Prasad et al., 2008) as the training set, section 00-01 as the development set, and section 21-22 as the test set. Table 1 shows the statistics of instances in the sets. We use F1-score as the evaluation metric for binary discourse relation classification. Besides, in the discussion sections, P-value (Johnson, 1999) is taken as the evaluation metric for statistical significance, and NDCG@k (J¨arvelin and Kek¨al¨ainen, 2002) is employed for evaluating the integrity of attention-worthy words. 3.2 Hyperparameter Settings We set two groups of hyperparameters in total, which correspond to different word embedding lear"
2020.coling-main.282,D16-1246,0,0.573583,"tion learning, Liu et al (2016) developed a multilayer attention mechanism. Chen et al (2016) integrated both the linear and non-linear interactions. Guo et al (2018) incorporated sparse learning into the interactive attention mechanism. Bai and Zhao (2018) used a feed forward network to model interactive attention and captured the effects on multi-grain linguistic features. Nguyen et al (2019) followed Bai and Zhao (2018)’s framework and conducted knowledge transferring. • For model design, neural networks were mainly used, including the basic ones like CNN, RNN and LSTM (Zhang et al., 2015; Qin et al., 2016; Guo et al., 2018), and the variants such as CRN and CGNN (Chen et al., 2016; Qin et al., 2016), as well as adversarial (Qin et al., 2017) and multi-task learning models (Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). • For feature selection, the embeddings of character, subword, word, sentence and sentence-pair levels (Bai and Zhao, 2018; Nguyen et al., 2019), paragraph-level relation continuity (Dai and Huang, 2018) and topic continuity (Lei et al., 2018) have been successfully applied in this area. Table 4 shows the performance of the previous methods and ours. Compared to the"
2020.coling-main.282,P17-1093,0,0.455204,"teractions. Guo et al (2018) incorporated sparse learning into the interactive attention mechanism. Bai and Zhao (2018) used a feed forward network to model interactive attention and captured the effects on multi-grain linguistic features. Nguyen et al (2019) followed Bai and Zhao (2018)’s framework and conducted knowledge transferring. • For model design, neural networks were mainly used, including the basic ones like CNN, RNN and LSTM (Zhang et al., 2015; Qin et al., 2016; Guo et al., 2018), and the variants such as CRN and CGNN (Chen et al., 2016; Qin et al., 2016), as well as adversarial (Qin et al., 2017) and multi-task learning models (Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). • For feature selection, the embeddings of character, subword, word, sentence and sentence-pair levels (Bai and Zhao, 2018; Nguyen et al., 2019), paragraph-level relation continuity (Dai and Huang, 2018) and topic continuity (Lei et al., 2018) have been successfully applied in this area. Table 4 shows the performance of the previous methods and ours. Compared to the state-of-the-art methods mentioned above, our IPAL is puny as it is isolated from the highly sophisticated learning architectures. More se"
2020.coling-main.282,D15-1266,0,0.768531,"arrative sentence or clause. For example, the arguments (i.e., Arg1 and Arg2) in Figure 1 hold a causal relation, where the possible connective “because” is not given. Interactive attention [Arg1: Psyllium’s not a good crop.] [Arg2: You get a rain at the wrong time and the crop is ruined.] Self-attention Figure 1: An example of causally-related arguments. Since the time when IDRR was boiled down to a problem of discourse relation classification (Pitler et al., 2009; Lin et al., 2014), intense interest has been devoted to the study of argument representation and neural relation classification (Zhang et al., 2015; Liu et al., 2016; Chen et al., 2016; Lan et al., 2017; Bai and Zhao, 2018; Nguyen et al., 2019). Context-specific non-interactive attention mechanism (also referred to self-attention mechanism) (Lin et al., 2017) and companion-dependent interactive attention mechanism (Ma et al., 2017; Meng et al., 2016) have been used for enhancing the sentence-level embedding process. Both are proven effective in argument encoding (Guo et al., 2018; Liu and Li, 2016) as well as the perception of discourse relations. During encoding, the self-attention mechanism is able to highlight the latent information o"
2020.coling-main.94,P15-1077,0,0.0302088,"rivastava and Sutton (2017) propose the neural topic models (NTM) to mine the topic information inside texts. Gui et al. (2019a) propose a reinforcement learning based neural topic model to alleviate the limitations of traditional topic coherence measures. Wang et al. (2020) also propose a topicaware multi-task learning model to learn topic-enriched utterance representations in customer service, which is inspirational to our topic-enriched auxiliary learning framework. Unlike the above studies modeling topics under the assumption that the topic-word distribution is a multinomial distribution, Das et al. (2015) model topics with multivariate gaussian distribution over the word embedding space to deal with the new word issue, which is inspirational to our proposed modality-agnostic topic model. However, all the prior topic models rely on word vocabulary and thus are specially-designed for text modality, which cannot be directly adopted to capture the topic information inside images. Different from all the above studies, this paper proposes a new modality-agnostic topic model to mine the global topics from either the discrete textual signals or the continuous visual signals. On this basis, a MTAL appr"
2020.coling-main.94,N19-1423,0,0.0535922,"text and image along the timeline (see Figure 1) posted by a user, the primary task aims at modeling both the text sequence and the corresponding image sequence to perform depression prediction for this user. Figure 2 shows the illustration of the primary task. First of all, given n pairs 1080 Convolutions … ෝ ൌ ࣆ  ܃ ࣕ  ڄሺ܃ሻ) ा Pooling ࣆሺ܃ሻ ܐ (ሺ܃ሻ) Inference Network ࣂ … ܃ Input ۱ ܃ Generative Network Figure 3: The framework of the proposed modality-agnostic topic model. of text and image, each text and each image are encoded with a shared (i.e., parameter sharing) BERT (Devlin et al., 2019) model and a shared VGG (Simonyan and Zisserman, 2015) model respectively. Text Encoder. As a pre-trained text encoding mechanism, BERT can be ﬁne-tuned to create state-ofthe-art models for a range of NLP tasks, e.g., text classiﬁcation and natural language inference. In our approach, we use BERT-Base (uncased) model as the shared text encoder. Given the t-th text xtext = t text , ..., xtext } of each user, we adopt BERT to encode this text and use the mark “[CLS]” repre, x {xtext t1 t2 tn ˆ t ∈ R768 to compute the text vector htext ˆ h ). Here, ˆt + b sentation h ∈ Rd of xtext as htext = tanh"
2020.coling-main.94,W16-0307,0,0.0284064,"and image modalities) like Gui et al. (2019b) are much less and limited to neglect the topic information inside multiple modalities. In the following, we will ﬁrst review the depression detection task and then introduce the related studies on neural topic models. Depression Detection. The ubiquity of social media poses a great opportunity to perform depression detection. Prior studies mainly focus on identifying depressed persons by analyzing the generated textual information in social media. Speciﬁcally, Choudhury et al. (2013) focus on the differences in word usage for depression detection. Gkotsis et al. (2016) focus on the depth of syntax-parsing trees for depression detection. In recent years, researchers begin to use multimodal information (e.g., the text, speech and image) for depression detection. Speciﬁcally, Yin et al. (2019) propose a hierarchical RNN network to extract the features from the vision, speech and text for depression detection. Gui et al. (2019b) propose a reinforced GRU network to capture both the textual and visual information for depression detection. In addition, it is worthwhile to mention that, Resnik et al. (2015) and Shen et al. (2017) also investigate 1079 Text and Imag"
2020.coling-main.94,C18-1201,0,0.14871,"n shown in Eq.(4). For clarity, the losses for two auxiliary tasks, i.e., the textual topic modeling and visual topic modeling, are denoted as Ltext and Limage respectively. Finally, the joint loss L is deﬁned as follows: L = Lprimary + λ(Ltext + Limage ) (8) where λ is a weight and ﬁne-tuned to be 0.25 for balancing the losses for primary and auxiliary tasks. 1083 Approach H-LSTM (Wang et al., 2018) BERT + LSTM BERT + LSTM + Textual Topic Modeling VGG + LSTM VGG + LSTM + Visual Topic Modeling EF-LSTM (Zadeh et al., 2018) CoMemory (Xu et al., 2018) CoATT (Zhang et al., 2018) Hybrid Attention (Gu et al., 2018) CoMMA (Gui et al., 2019b) Primary Task MTAL Modality Text Image Text + Image Precision (P) 77.2 79.2 81.1 62.3 66.8 79.9 80.6 79.6 80.6 78.3 81.4 84.2 Recall (R) 77.1 79.2 81.2 61.7 66.7 79.9 80.4 80.3 80.6 79.4 80.9 84.2 F1 77.1 79.2 81.1 62.0 66.7 79.9 80.5 79.9 80.6 78.8 81.1 84.2 Acc. 77.1 79.2 82.3 61.7 66.7 79.9 80.4 80.4 80.6 79.2 80.9 84.2 Table 2: Performance comparison of various kinds of approaches with the single-modality (text or image) and the multimodality (text and image) for depression detection. 4 Experimentation To validate the effectiveness of our approach, we evaluate the"
2020.coling-main.94,D19-1350,0,0.291954,"demonstrates that the proposed MTAL approach can signiﬁcantly outperform several state-of-the-art baselines, including the representative textual depression detection approaches and the state-of-the-art multimodal-based approaches. 2 Related Work Depression detection is an interdisciplinary research task and has been drawing ever-more attention in NLP with a focus on extracting various types of features from text modality (Choudhury et al., 2013; Nambisan et al., 2015). Compared with the studies on the text modality, the studies on multimodality (e.g., both the text and image modalities) like Gui et al. (2019b) are much less and limited to neglect the topic information inside multiple modalities. In the following, we will ﬁrst review the depression detection task and then introduce the related studies on neural topic models. Depression Detection. The ubiquity of social media poses a great opportunity to perform depression detection. Prior studies mainly focus on identifying depressed persons by analyzing the generated textual information in social media. Speciﬁcally, Choudhury et al. (2013) focus on the differences in word usage for depression detection. Gkotsis et al. (2016) focus on the depth of"
2020.coling-main.94,W15-1212,0,0.0289221,"n the differences in word usage for depression detection. Gkotsis et al. (2016) focus on the depth of syntax-parsing trees for depression detection. In recent years, researchers begin to use multimodal information (e.g., the text, speech and image) for depression detection. Speciﬁcally, Yin et al. (2019) propose a hierarchical RNN network to extract the features from the vision, speech and text for depression detection. Gui et al. (2019b) propose a reinforced GRU network to capture both the textual and visual information for depression detection. In addition, it is worthwhile to mention that, Resnik et al. (2015) and Shen et al. (2017) also investigate 1079 Text and Image Encoding … ܂௧భ ܂௧ሾಽೄሿ Primary Task ܂௧ሾೄಶುሿ ܂௧ trm …Shared BERT trm trm … trm … ۳௧భ ۳௧ሾೄಶುሿ ۳௧ … [CLS] ܐ௧௫௧ ଶ ܐଷ LSTM LSTM LSTM … ܐ௧௫௧ ௧ … ۳௧ሾಽೄሿ ܐଶ ܐଵ௧௫௧ … trm trm ܐଵ LSTM Depressed? Yes or No … ܞଵ  ܃௧௫௧ ܐ [SEP] ܞଶ ܞଷ ܞ୬ ܘఏ =ሺ܅   ܊ ሻ Modality Fusion The t-th text ܚ 2ʹ4ൈ ʹ24 2 1ͳʹ ൈ ͳͳʹ ܙ௧௫௧ Shared VGG Modality-Agnostic Topic Model 5 ൈ ͷ6 2ͺ ൈ ʹ8 ܐ௧ ConvNet 1Ͷ ൈ ͳ4 Pool  ܐଵ 7ൈ  FC D=512 The t-th image D=4096  ܐଶ Textual Topic Modelling ܙ Modali"
2020.emnlp-main.196,W13-2322,0,0.230043,"Europe"" :name (n2 / name :op1 ""Europe"")))) (c) AMR Linearization ( consider-01 : ARG0 ( country : name ( name : op1 "" China "" ) ) : ARG1 ( partner-01 : ARG1 ( country : name ( name : op1 "" Germany "" ) ) : mod ( important : degree ( most ) ) : mod ( trade-01 ) : location ( continent : name ( name : op1 "" Europe "" ) ) ) ) Figure 1: An example of seq2seq-based AMR parsing. Introduction Abstract meaning representation (AMR) parsing aims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these appro"
2020.emnlp-main.196,D19-1393,0,0.671556,"p1 "" Europe "" ) ) ) ) Figure 1: An example of seq2seq-based AMR parsing. Introduction Abstract meaning representation (AMR) parsing aims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in implementation and the competitive performance. Similar to many NLP tasks, the performance of AMR parsing is much restricted by the size of human-curated da"
2020.emnlp-main.196,2020.acl-main.119,0,0.495375,"en the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in implementation and the competitive performance. Similar to many NLP tasks, the performance of AMR parsing is much restricted by the size of human-curated dataset. For example, even recent AMR 2.0 contains only 36.5K training AMRs. To alleviate the effect of such restriction, a previous attempt is to utilize large-scale unlabeled sentences with self-training (Konstas et al., 2017). Alternatively, a more recent feasible solution is to resor"
2020.emnlp-main.196,P13-2131,0,0.470685,"gap 6.9 between their corresponding baselines. • Finally, our approach achieves the best reported performance on AMR 1.0 and the performance on AMR 2.0 is higher than or close to that achieved by previous studies which use BERT. This is very encouraging taking into consideration the fact that our seq2seq model is much simper than the graph-based models proposed in related studies (Zhang et al., 2019a,b; Naseem et al., 2019; Cai and Lam, 2020). Evaluation Metrics For evaluation purpose, we use the AMR-evaluation toolkit to evaluate parsing performance in Smatch and other fine-grained metrics (Cai and Knight, 2013; Damonte et al., 2017). We report results of single models that are tuned on the development set. 4.2 Experimental Results Table 2 presents the comparison of our approach and related studies on the test sets of AMR 1.0 and AMR 2.0. From the results, we have the following observations: • Pre-trained models on a single task (i.e., from #2 to #6) significantly improve the performance of AMR parsing, indicating seq2seq pre-training is helpful for seq2seqbased AMR parsing. We also note that the pre-trained model of NMT achieves the best performance, followed by the pre-trained models on AMR parsin"
2020.emnlp-main.196,D16-1257,0,0.025459,"re-trained model. PTM-SynPar is a seq2seq constituent parsing model. Building such a model requires a training dataset which consists of sentences paired with constituency parse trees. To construct a silver treebank, we parse the English sentences in the bilingual data for MT by using an off-the-shelf parser. Then we linearize the automatic parse trees to get syntax sequences, as illustrated in Figure 2. Note that in the linearization, we let the output contain the words from the source sentence. The motivation here is to regard parsing as a language generation problem, similar to the idea in Choe and Charniak (2016). Intuitively, the above described single pre-trained models can capture linguistic features from different perspectives. One question is whether these models are complementary when they are properly used to initialize a seq2seq-based AMR parser. To empirically answer this question, we propose to build pre-trained models through jointly learning multiple pre-training tasks. Inspired by the zeroshot approach proposed for multi-lingual neural machine translation (Johnson et al., 2017), we add a unique preceding tag to the target side of training data to distinguish the task of each training inst"
2020.emnlp-main.196,E17-1051,0,0.436782,"2seq-based AMR parsing. Introduction Abstract meaning representation (AMR) parsing aims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in implementation and the competitive performance. Similar to many NLP tasks, the performance of AMR parsing is much restricted by the size of human-curated dataset. For example, even recent AMR 2.0 contains on"
2020.emnlp-main.196,N19-1423,0,0.136025,"ural Language Processing, pages 2501–2511, c November 16–20, 2020. 2020 Association for Computational Linguistics Task machine translation syntactic parsing AMR parsing Dataset gold silver silver Source sentence sentence sentence Target sentence tree sequence AMR sequence Table 1: Three seq2seq learning tasks explored in this paper to obtain pre-trained models. Here silver dataset indicates that the sequences in the target-side are generated automatically . incorporated into the training of an AMR parser. However, the widely used pre-trained models such as ELMO (Peters et al., 2017) and BERT (Devlin et al., 2019) may not work as expected for building a state-of-the-art seq2seq AMR parser. The reasons are two-fold. On the one hand, previous studies on both seq2seq-based AMR parsing and AMR-to-text generation demonstrate the necessity of a shared vocabulary for the source and target sides (Ge et al., 2019; Zhu et al., 2019). Using pretrained models like BERT as pre-trained encoders for AMR parsing, however, will violate the rule of sharing a vocabulary. On the other hand, pretrained models such as BERT are basically tuned for the purpose of representing sentences instead of generating target sequences."
2020.emnlp-main.196,N19-1409,0,0.0154628,"a universal model and then fine-tuning the model on a downstream task have recently become a popular strategy in the field of natural language processing. Previous works on pre-training can be roughly grouped into two categories. One category of approaches is to learn static word embeddings such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) while the other group builds dynamic pre-trained models that would also be used in downstream tasks. Representative examples in the latter group in2508 clude Dai and Le (2015), CoVe (McCann et al., 2017), ELMo (Peters et al., 2017; Edunov et al., 2019), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019). Besides the aforementioned encoder-only (e.g., BERT) or decoderonly (e.g., GPT) pre-training approaches, recent studies also propose approaches to pre-training seq2seq models, such as MASS (Song et al., 2019), PoDA (Wang et al., 2019), PEGASUS (Zhang et al., 2020), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020). AMR Parsing. As a semantic parsing task that translates texts into AMR graphs, AMR parsing has received much attention in recent years. Diverse approaches have been applied to the task. Flanigan et al. (2014)"
2020.emnlp-main.196,P14-1134,0,0.581194,"01 ) : location ( continent : name ( name : op1 "" Europe "" ) ) ) ) Figure 1: An example of seq2seq-based AMR parsing. Introduction Abstract meaning representation (AMR) parsing aims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in implementation and the competitive performance. Similar to many NLP tasks, the performance of AMR parsing is much"
2020.emnlp-main.196,P18-1170,0,0.102799,"Missing"
2020.emnlp-main.196,D18-1198,0,0.462111,". Introduction Abstract meaning representation (AMR) parsing aims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in implementation and the competitive performance. Similar to many NLP tasks, the performance of AMR parsing is much restricted by the size of human-curated dataset. For example, even recent AMR 2.0 contains only 36.5K training A"
2020.emnlp-main.196,P19-1356,0,0.0153463,"models, we merge all the source and target sides of the three pretraining tasks, and construct a shared vocabulary. Moreover, in all the models we share vocabulary embeddings for both the source and target sides. Transformer Encoder Task1: Task2: Task3: s1(1) … sn(1)1 s1(2) … sn(2) 2 s1(3) … sn(3)3 Transformer Decoder t1(1) … t m(1)1 T1 T2 t1(2) … t m(2)2 T3 t1(3) … t m(3)3 Figure 3: Illustration of the joint pre-training approach. PTM-MT is a seq2seq neural machine translation (NMT) model which is trained on a publicly available bilingual dataset. According to findings in Goldberg (2019) and Jawahar et al. (2019), the Transformer encoder is strong in capturing syntax and semantics from source sentences, which is helpful to AMR parsing. baseline system of AMR parsing to process the English sentences in the bilingual MT corpus. Then we adopt the linearization process illustrated in Figure 1 to obtain source-target pairs. Finally, we train a seq2seq-based AMR parsing model on the silver corpus that will be used as a pre-trained model. PTM-SynPar is a seq2seq constituent parsing model. Building such a model requires a training dataset which consists of sentences paired with constituency parse trees. To co"
2020.emnlp-main.196,Q17-1024,0,0.0607601,"Missing"
2020.emnlp-main.196,P17-4012,0,0.0224894,"ut 3.9M training sentence pairs after filtering out long and imbalanced pairs. To obtain syntactic parse trees for the source sentences, we utilize toolkit AllenNLP (Gardner et al., 2017) which is trained on Penn Treebank (Marcus et al., 1993). To obtain AMR graphs for the source sentences, we utilize our baseline AMR parsing system. Then we merge English/German sentences and linearized parse trees, and AMR graphs together and segment all the tokens into subwords by byte pair encoding (BPE) (Sennrich et al., 2016) with 20K operations. We implement above pre-trained models based on OpenNMT-py (Klein et al., 2017).3 For simplicity, we unify parameters of these models as the Transformer-base model in Vaswani et al. (2017). The number of layers in encoder and decoder is 6 while the number of heads is 8. Both the embedding size and the hidden size are 512 while the size of feedforward network is 2048. Moreover, we use Adam optimizer (Kingma and Ba, 2015) with β1 of 0.9 and β2 of 0.998. Warm up step, learning rate, dropout rate and label smoothing epsilon are 16000, 2.0, 0.1 and 0.1 respectively. In addition, we set the batch token-size to 8,192. We train the models for 300K steps and choose the model 2 ht"
2020.emnlp-main.196,P17-1014,0,0.475488,"irected and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in implementation and the competitive performance. Similar to many NLP tasks, the performance of AMR parsing is much restricted by the size of human-curated dataset. For example, even recent AMR 2.0 contains only 36.5K training AMRs. To alleviate the effect of such restriction, a previous attempt is to utilize large-scale unlabeled sent"
2020.emnlp-main.196,2020.acl-main.703,0,0.0612792,"2014) while the other group builds dynamic pre-trained models that would also be used in downstream tasks. Representative examples in the latter group in2508 clude Dai and Le (2015), CoVe (McCann et al., 2017), ELMo (Peters et al., 2017; Edunov et al., 2019), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019). Besides the aforementioned encoder-only (e.g., BERT) or decoderonly (e.g., GPT) pre-training approaches, recent studies also propose approaches to pre-training seq2seq models, such as MASS (Song et al., 2019), PoDA (Wang et al., 2019), PEGASUS (Zhang et al., 2020), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020). AMR Parsing. As a semantic parsing task that translates texts into AMR graphs, AMR parsing has received much attention in recent years. Diverse approaches have been applied to the task. Flanigan et al. (2014) pioneer the research work on AMR parsing by using a a two-stage approach: node identification followed by relation recognition. Werling et al. (2015) improve the first stage in the parser of Flanigan et al. (2014) by generating subgraph aligned to lexical items. To avoid conducting AMR parsing from scratch, Wang et al. (2015b) propose to obtain AMR graphs f"
2020.emnlp-main.196,P17-1064,1,0.825829,"y as the encoder, a more reasonable approach is to utilize BERT as an extra feature or view BERT as an extra encoder. See Section 5.1 for more detailed discussions on the effect of BERT on AMR parsing. In this paper, we propose to pre-train seq2seq models that aim to capture different linguistic knowledge from input sentences. To build such pre-trained models, we explore three different yet relevant seq2seq tasks, as listed in Table 1. Here, machine translation acts as the most representative seq2seq task which takes a bilingual dataset as the training data. According to Shi et al. (2016) and Li et al. (2017), a machine translation system with good performance requires the model to well derive linguistic information from input sentences. The other two tasks require auto-parsed syntactic parse trees and AMR graphs as the training data, respectively. It is worth noting that the pre-training task of AMR parsing is in the similar spirit of selftraining (Konstas et al., 2017). In order to investigate whether various seq2seq pre-trained models are complementary to each other in the sense that they can be learned jointly to achieve better performance, we further explore joint learning of several pre-trai"
2020.emnlp-main.196,P18-1037,0,0.321386,"ds, the BERT tokenizer segments it into a subword sequence x0 = (x01 , · · · , x0m ) with m 2505 # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 AMR 1.0 P. R. F1 None None 69.8 60.2 64.6 Vanilla 78.8 69.5 73.8 PTM-MT MTL 81.1 72.2 76.4 Vanilla 74.3 65.8 69.8 PTM-SynPar MTL 76.7 68.1 72.2 PTM-SemPar Vanilla 80.8 73.5 77.0 Vanilla 79.1 70.5 74.6 PTM-MT-SynPar MTL 81.2 74.0 77.5 Vanilla 82.3 75.4 78.7 PTM-MT-SemPar MTL 82.4 74.6 78.3 Vanilla 81.6 74.0 77.6 PTM-SynPar-SemPar MTL 81.8 74.0 77.7 Vanilla 82.4 75.4 78.7 PTM-MT-SynPar-SemPar MTL 82.6 75.9 79.1 Previous work without extra resources Graph Prediction(Lyu and Titov, 2018) Prediction(Guo and Lu, 2018) Prediction(Groschwitz et al., 2018) Seq2Seq(Ge et al., 2019) Seq2Seq(Cai and Lam, 2019) Graph(Cai and Lam, 2020) 71.2 Previous work with extra resources Seq2Graph(Zhang et al., 2019a)† 70.2 71.3 Seq2Graph(Zhang et al., 2019b)† RL(Naseem et al., 2019)† Seq2Seq(Ge et al., 2019)∗ Graph(Cai and Lam, 2020)† 75.4 Pre-trained Model Fine-Tune AMR 2.0 P. R. F1 75.8 67.7 71.5 80.0 74.3 77.1 81.3 77.1 79.1 76.2 71.5 73.8 78.0 72.8 75.3 80.8 75.2 77.9 79.5 75.0 77.1 81.5 77.6 79.5 82.4 77.3 79.7 82.3 78.0 80.1 81.1 76.3 78.6 81.3 76.8 79.0 82.1 77.6 79.8 82.3 78.3 80.2 74.4 6"
2020.emnlp-main.196,J93-2004,0,0.0824997,"M-MT-SynPar-SemPar, in MTL fine-tuning we only keep the pre-training tasks of MT and syntactic parsing. 4 Experimentation In this section, we report the performance of our seq2seq pre-training approach to AMR parsing. 4.1 Experimental Settings Pre-training Dataset and Pre-trained Models For pre-trained models, we use the WMT14 English-to-German dataset2 which consists of about 3.9M training sentence pairs after filtering out long and imbalanced pairs. To obtain syntactic parse trees for the source sentences, we utilize toolkit AllenNLP (Gardner et al., 2017) which is trained on Penn Treebank (Marcus et al., 1993). To obtain AMR graphs for the source sentences, we utilize our baseline AMR parsing system. Then we merge English/German sentences and linearized parse trees, and AMR graphs together and segment all the tokens into subwords by byte pair encoding (BPE) (Sennrich et al., 2016) with 20K operations. We implement above pre-trained models based on OpenNMT-py (Klein et al., 2017).3 For simplicity, we unify parameters of these models as the Transformer-base model in Vaswani et al. (2017). The number of layers in encoder and decoder is 6 while the number of heads is 8. Both the embedding size and the"
2020.emnlp-main.196,P19-1451,0,0.543462,"we move from single pre-training models to joint pre-training models. For example, based on PTM-MT-SynParSemPar, the performance gap is 1.1 in Smatch F1 scores, much less than the performance gap 6.9 between their corresponding baselines. • Finally, our approach achieves the best reported performance on AMR 1.0 and the performance on AMR 2.0 is higher than or close to that achieved by previous studies which use BERT. This is very encouraging taking into consideration the fact that our seq2seq model is much simper than the graph-based models proposed in related studies (Zhang et al., 2019a,b; Naseem et al., 2019; Cai and Lam, 2020). Evaluation Metrics For evaluation purpose, we use the AMR-evaluation toolkit to evaluate parsing performance in Smatch and other fine-grained metrics (Cai and Knight, 2013; Damonte et al., 2017). We report results of single models that are tuned on the development set. 4.2 Experimental Results Table 2 presents the comparison of our approach and related studies on the test sets of AMR 1.0 and AMR 2.0. From the results, we have the following observations: • Pre-trained models on a single task (i.e., from #2 to #6) significantly improve the performance of AMR parsing, indica"
2020.emnlp-main.196,E17-1035,0,0.0758254,"ims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in implementation and the competitive performance. Similar to many NLP tasks, the performance of AMR parsing is much restricted by the size of human-curated dataset. For example, even recent AMR 2.0 contains only 36.5K training AMRs. To alleviate the effect of such restriction, a previous a"
2020.emnlp-main.196,D14-1162,0,0.0822054,"Missing"
2020.emnlp-main.196,P16-1162,0,0.0160956,"rained Models For pre-trained models, we use the WMT14 English-to-German dataset2 which consists of about 3.9M training sentence pairs after filtering out long and imbalanced pairs. To obtain syntactic parse trees for the source sentences, we utilize toolkit AllenNLP (Gardner et al., 2017) which is trained on Penn Treebank (Marcus et al., 1993). To obtain AMR graphs for the source sentences, we utilize our baseline AMR parsing system. Then we merge English/German sentences and linearized parse trees, and AMR graphs together and segment all the tokens into subwords by byte pair encoding (BPE) (Sennrich et al., 2016) with 20K operations. We implement above pre-trained models based on OpenNMT-py (Klein et al., 2017).3 For simplicity, we unify parameters of these models as the Transformer-base model in Vaswani et al. (2017). The number of layers in encoder and decoder is 6 while the number of heads is 8. Both the embedding size and the hidden size are 512 while the size of feedforward network is 2048. Moreover, we use Adam optimizer (Kingma and Ba, 2015) with β1 of 0.9 and β2 of 0.998. Warm up step, learning rate, dropout rate and label smoothing epsilon are 16000, 2.0, 0.1 and 0.1 respectively. In addition"
2020.emnlp-main.196,D16-1159,0,0.0162119,"to using BERT directly as the encoder, a more reasonable approach is to utilize BERT as an extra feature or view BERT as an extra encoder. See Section 5.1 for more detailed discussions on the effect of BERT on AMR parsing. In this paper, we propose to pre-train seq2seq models that aim to capture different linguistic knowledge from input sentences. To build such pre-trained models, we explore three different yet relevant seq2seq tasks, as listed in Table 1. Here, machine translation acts as the most representative seq2seq task which takes a bilingual dataset as the training data. According to Shi et al. (2016) and Li et al. (2017), a machine translation system with good performance requires the model to well derive linguistic information from input sentences. The other two tasks require auto-parsed syntactic parse trees and AMR graphs as the training data, respectively. It is worth noting that the pre-training task of AMR parsing is in the similar spirit of selftraining (Konstas et al., 2017). In order to investigate whether various seq2seq pre-trained models are complementary to each other in the sense that they can be learned jointly to achieve better performance, we further explore joint learnin"
2020.emnlp-main.196,P15-2141,0,0.484496,": ARG1 ( country : name ( name : op1 "" Germany "" ) ) : mod ( important : degree ( most ) ) : mod ( trade-01 ) : location ( continent : name ( name : op1 "" Europe "" ) ) ) ) Figure 1: An example of seq2seq-based AMR parsing. Introduction Abstract meaning representation (AMR) parsing aims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in imple"
2020.emnlp-main.196,N15-1040,0,0.543204,": ARG1 ( country : name ( name : op1 "" Germany "" ) ) : mod ( important : degree ( most ) ) : mod ( trade-01 ) : location ( continent : name ( name : op1 "" Europe "" ) ) ) ) Figure 1: An example of seq2seq-based AMR parsing. Introduction Abstract meaning representation (AMR) parsing aims to translate a textual sentence into a directed and acyclic graph which consists of concept nodes and edges representing the semantic relations between the nodes (Banarescu et al., 2013). Previous studies focus on building diverse approaches to modeling the structure in AMR graphs, such as treebased approaches (Wang et al., 2015b; Groschwitz ∗ Corresponding Author: Junhui Li. et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Damonte et al., 2017; Guo and Lu, 2018), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019), and sequence-to-graph (seq2graph) approaches (Zhang et al., 2019a,b; Cai and Lam, 2020). Among these approaches, seq2seq-based approaches, which properly transform AMR graphs into sequences, have received much interest, due to the simplicity in imple"
2020.emnlp-main.196,D19-1412,0,0.0250793,"vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) while the other group builds dynamic pre-trained models that would also be used in downstream tasks. Representative examples in the latter group in2508 clude Dai and Le (2015), CoVe (McCann et al., 2017), ELMo (Peters et al., 2017; Edunov et al., 2019), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019). Besides the aforementioned encoder-only (e.g., BERT) or decoderonly (e.g., GPT) pre-training approaches, recent studies also propose approaches to pre-training seq2seq models, such as MASS (Song et al., 2019), PoDA (Wang et al., 2019), PEGASUS (Zhang et al., 2020), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020). AMR Parsing. As a semantic parsing task that translates texts into AMR graphs, AMR parsing has received much attention in recent years. Diverse approaches have been applied to the task. Flanigan et al. (2014) pioneer the research work on AMR parsing by using a a two-stage approach: node identification followed by relation recognition. Werling et al. (2015) improve the first stage in the parser of Flanigan et al. (2014) by generating subgraph aligned to lexical items. To avoid conducting AMR parsing from sc"
2020.emnlp-main.196,P15-1095,0,0.0984235,"Missing"
2020.emnlp-main.196,P19-1009,0,0.47722,"Missing"
2020.emnlp-main.196,D19-1392,0,0.545394,"Missing"
2020.emnlp-main.196,D19-1548,1,0.823865,"d in this paper to obtain pre-trained models. Here silver dataset indicates that the sequences in the target-side are generated automatically . incorporated into the training of an AMR parser. However, the widely used pre-trained models such as ELMO (Peters et al., 2017) and BERT (Devlin et al., 2019) may not work as expected for building a state-of-the-art seq2seq AMR parser. The reasons are two-fold. On the one hand, previous studies on both seq2seq-based AMR parsing and AMR-to-text generation demonstrate the necessity of a shared vocabulary for the source and target sides (Ge et al., 2019; Zhu et al., 2019). Using pretrained models like BERT as pre-trained encoders for AMR parsing, however, will violate the rule of sharing a vocabulary. On the other hand, pretrained models such as BERT are basically tuned for the purpose of representing sentences instead of generating target sequences. According to Zhu et al. (2020), by contrast to using BERT directly as the encoder, a more reasonable approach is to utilize BERT as an extra feature or view BERT as an extra encoder. See Section 5.1 for more detailed discussions on the effect of BERT on AMR parsing. In this paper, we propose to pre-train seq2seq m"
2020.emnlp-main.291,P17-1067,0,0.0252471,"multi-label emotion detection. The detailed evaluation demonstrates the effectiveness of our approach. 1 Visual Modality Acoustic Modality Emotions Emotion detection is to predict emotion categories, such as angry, happy, and surprise, expressed by an utterance of a speaker and has largely encompassed a variety of applications, such as online chatting (Galik and Rank, 2012; Zhang et al., c), news analysis (Li et al., 2015; Zhu et al., 2019) and dialogue systems (Ghosal et al., 2019; Zhang et al., d). Over the last few years, there has been a substantial body of research on emotion detection (Abdul-Mageed and Ungar, 2017; Zhou et al., 2019; Zhang et al., a), where a considerable amount of work has focused on multi-label emotion detection (Li et al., 2015; Yu et al., 2018; Ying et al., 2019). Basically, emotion detection is a multi-label classiﬁcation problem since one utterance naturally tends to involve more than one emotion category. However, classifying instances with multiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video seg"
2020.emnlp-main.291,P19-2045,0,0.0493231,"Missing"
2020.emnlp-main.291,D19-1566,0,0.0224143,"motion detection rely on special knowledge of emotion, such as context information (Li et al., 2015), cross-domain transferring (Yu et al., 2018) and external resource (Ying et al., 2019). In fact, when there is no special knowledge (Kim et al., 2018), it can be normally handled by multi-label text classiﬁcation approaches. In the multi-modal community, related studies normally focus on single-label emotion task and the studies for multi-label emotion task are much less and limited to be transformed to multiple binary classiﬁcation (Zadeh et al., 2018b; Wang et al., 2019; Akhtar et al., 2019; Chauhan et al., 2019). In the following, we give an overview of multi-label emotion/text classiﬁcation and multi-modal emotion detection. Multi-label Emotion/Text Classiﬁcation. Recent studies normally cast multi-label emotion detection task as a classiﬁcation problem and leverage the special knowledge as auxiliary information (Yu et al., 2018; Ying et al., 2019). These approaches may not be easily extended to those tasks without external knowledge. At this time, the multi-label text classiﬁcation approaches can be quickly applied to emotion detection. There have been a large number of representative studies for t"
2020.emnlp-main.291,S18-1019,0,0.0211402,"g, we give an overview of multi-label emotion/text classiﬁcation and multi-modal emotion detection. Multi-label Emotion/Text Classiﬁcation. Recent studies normally cast multi-label emotion detection task as a classiﬁcation problem and leverage the special knowledge as auxiliary information (Yu et al., 2018; Ying et al., 2019). These approaches may not be easily extended to those tasks without external knowledge. At this time, the multi-label text classiﬁcation approaches can be quickly applied to emotion detection. There have been a large number of representative studies for that. Kant et al. (2018) leverage the pre-trained BERT to perform multi-label emotion task and Kim et al. (2018) propose an attention-based classiﬁer that predicts multiple emotions of a given sentence. More recently, Yang et al. (2018) propose a sequence generation model and Yang et al. (2019) leverage a reinforced approach to ﬁnd a better sequence than a baseline sequence, but it still relies on the pretrained seq2seq model with a pre-deﬁned order of ground-truth. Different from above studies, we focus on multilabel emotion detection in a multi-modal scenario by considering the modality dependence besides the label"
2020.emnlp-main.291,P15-1101,1,0.814811,"Missing"
2020.emnlp-main.291,D14-1162,0,0.0827516,"Missing"
2020.emnlp-main.291,N19-1321,0,0.0449293,"Missing"
2020.emnlp-main.291,P19-1656,0,0.174944,"ultilabel emotion detection in a multi-modal scenario by considering the modality dependence besides the label dependence. To the best of our knowledge, this is the ﬁrst attempt to perform multi-label emotion detection in a multi-modal scenario. Multi-modal Emotion Detection. Recent studies on multi-modal emotion detection largely depend on multi-modal fusion framework to perform binary classiﬁcation within each emotion category. Recently, Wang et al. (2019) introduce a recurrent attended variation embedding network for multimodal language analysis with non-verbal shifted word representation. Tsai et al. (2019) employ the Transformer-based architecture to capture the long-range interactions inside and across different modalities. However, they still cast the multi-label emotion detection as multiple binary classiﬁcation problems. Different from above studies, we focus on multimodal emotion detection in a multi-label scenario by considering the label dependence besides the modality dependence. To the best of our knowledge, this is the ﬁrst attempt to perform multimodal emotion detection in a multi-label scenario. 3585 3 Output Probabilities Data Pre-processing Softmax We extract low-level handcrafted"
2020.emnlp-main.291,P18-1208,0,0.0263778,"Missing"
2020.emnlp-main.291,D19-1444,0,0.0275047,"parameters of β1 and β2 , 0.9 and 0.999 respectively. The beam size K is set to be 5 at both training and inference stages. To motivate future research, the code will be released via github 3 . Evaluation Metrics and Signiﬁcance Test. In our study, we employ three evaluation metrics to measure the performances of different approaches to multi-modal multi-label emotion detection, i.e., multi-label Accuracy (Acc), Hamming Loss (HL) and micro F1 measure (F1 ). These metrics have been popularly used in some multi-label classiﬁcation problems (Li et al., 2015; Yang et al., 2019; Aly et al., 2019; Wu et al., 2019). Note that smaller Hamming Loss corresponds to better classiﬁcation quality, while larger Accuracy 2 3 For a thorough comparison, we implement various baseline approaches in three groups: Multi-label Classiﬁcation Approaches. In this group, the baselines use different approaches to deal with the multi-label issue without considering the modality dependence issue. Speciﬁcally, in these approaches, the multi-modal inputs are early fused (simply concatenated) as a new input. (1) BR5 (Shen et al., 2004), which transforms the multi-label task into multiple single-label binary classiﬁcation problem"
2020.emnlp-main.291,D19-1044,0,0.0370485,"ultiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video segment. Introduction ∗ ĊĊ the hardest thing that we face is ĊĊ main challenge is how to model the label dependence in the classiﬁcation approach. For example, in the utterance as shown in Figure 1, both the Sad and Disgust emotions are more likely to exist, rather than the conﬂicting emotions of Sad and Happy. Recent studies, such as (Yang et al., 2019) and (Xiao et al., 2019), have begun to address this challenge. However, almost all existing studies in multilabel emotion detection focus on one modality (e.g., textual modality). Only very recently, the research community has become increasingly aware of the need on multi-modal emotion detection (Zadeh et al., 2018b) due to its wide potential applications, e.g., with the massively growing importance of analyzing conversations in speech (Gu et al., 2019) and video (Majumder et al., 2019). In this study, we aim to tackle multi-modal multi-label emotion detection. Compared with single modality, multimodal multi-label"
2020.emnlp-main.291,P19-1518,0,0.255733,"sifying instances with multiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video segment. Introduction ∗ ĊĊ the hardest thing that we face is ĊĊ main challenge is how to model the label dependence in the classiﬁcation approach. For example, in the utterance as shown in Figure 1, both the Sad and Disgust emotions are more likely to exist, rather than the conﬂicting emotions of Sad and Happy. Recent studies, such as (Yang et al., 2019) and (Xiao et al., 2019), have begun to address this challenge. However, almost all existing studies in multilabel emotion detection focus on one modality (e.g., textual modality). Only very recently, the research community has become increasingly aware of the need on multi-modal emotion detection (Zadeh et al., 2018b) due to its wide potential applications, e.g., with the massively growing importance of analyzing conversations in speech (Gu et al., 2019) and video (Majumder et al., 2019). In this study, we aim to tackle multi-modal multi-label emotion detection. Compared with single modality,"
2020.emnlp-main.291,C18-1330,0,0.116609,"ask as a classiﬁcation problem and leverage the special knowledge as auxiliary information (Yu et al., 2018; Ying et al., 2019). These approaches may not be easily extended to those tasks without external knowledge. At this time, the multi-label text classiﬁcation approaches can be quickly applied to emotion detection. There have been a large number of representative studies for that. Kant et al. (2018) leverage the pre-trained BERT to perform multi-label emotion task and Kim et al. (2018) propose an attention-based classiﬁer that predicts multiple emotions of a given sentence. More recently, Yang et al. (2018) propose a sequence generation model and Yang et al. (2019) leverage a reinforced approach to ﬁnd a better sequence than a baseline sequence, but it still relies on the pretrained seq2seq model with a pre-deﬁned order of ground-truth. Different from above studies, we focus on multilabel emotion detection in a multi-modal scenario by considering the modality dependence besides the label dependence. To the best of our knowledge, this is the ﬁrst attempt to perform multi-label emotion detection in a multi-modal scenario. Multi-modal Emotion Detection. Recent studies on multi-modal emotion detecti"
2020.emnlp-main.291,D19-1552,1,0.831171,". The detailed evaluation demonstrates the effectiveness of our approach. 1 Visual Modality Acoustic Modality Emotions Emotion detection is to predict emotion categories, such as angry, happy, and surprise, expressed by an utterance of a speaker and has largely encompassed a variety of applications, such as online chatting (Galik and Rank, 2012; Zhang et al., c), news analysis (Li et al., 2015; Zhu et al., 2019) and dialogue systems (Ghosal et al., 2019; Zhang et al., d). Over the last few years, there has been a substantial body of research on emotion detection (Abdul-Mageed and Ungar, 2017; Zhou et al., 2019; Zhang et al., a), where a considerable amount of work has focused on multi-label emotion detection (Li et al., 2015; Yu et al., 2018; Ying et al., 2019). Basically, emotion detection is a multi-label classiﬁcation problem since one utterance naturally tends to involve more than one emotion category. However, classifying instances with multiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video segment. Introduction"
2020.emnlp-main.291,P19-1045,1,0.763654,"and different modalities (modality dependence). Particularly, we propose a multi-modal sequence-to-set approach to effectively model both kinds of dependence in multi-modal multi-label emotion detection. The detailed evaluation demonstrates the effectiveness of our approach. 1 Visual Modality Acoustic Modality Emotions Emotion detection is to predict emotion categories, such as angry, happy, and surprise, expressed by an utterance of a speaker and has largely encompassed a variety of applications, such as online chatting (Galik and Rank, 2012; Zhang et al., c), news analysis (Li et al., 2015; Zhu et al., 2019) and dialogue systems (Ghosal et al., 2019; Zhang et al., d). Over the last few years, there has been a substantial body of research on emotion detection (Abdul-Mageed and Ungar, 2017; Zhou et al., 2019; Zhang et al., a), where a considerable amount of work has focused on multi-label emotion detection (Li et al., 2015; Yu et al., 2018; Ying et al., 2019). Basically, emotion detection is a multi-label classiﬁcation problem since one utterance naturally tends to involve more than one emotion category. However, classifying instances with multiple possible categories is sometimes much more difﬁcul"
2020.emnlp-main.291,D19-5541,0,0.142045,"otion categories, such as angry, happy, and surprise, expressed by an utterance of a speaker and has largely encompassed a variety of applications, such as online chatting (Galik and Rank, 2012; Zhang et al., c), news analysis (Li et al., 2015; Zhu et al., 2019) and dialogue systems (Ghosal et al., 2019; Zhang et al., d). Over the last few years, there has been a substantial body of research on emotion detection (Abdul-Mageed and Ungar, 2017; Zhou et al., 2019; Zhang et al., a), where a considerable amount of work has focused on multi-label emotion detection (Li et al., 2015; Yu et al., 2018; Ying et al., 2019). Basically, emotion detection is a multi-label classiﬁcation problem since one utterance naturally tends to involve more than one emotion category. However, classifying instances with multiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video segment. Introduction ∗ ĊĊ the hardest thing that we face is ĊĊ main challenge is how to model the label dependence in the classiﬁcation approach. For example, in the utterance"
2020.emnlp-main.291,D18-1137,0,0.0783276,"is to predict emotion categories, such as angry, happy, and surprise, expressed by an utterance of a speaker and has largely encompassed a variety of applications, such as online chatting (Galik and Rank, 2012; Zhang et al., c), news analysis (Li et al., 2015; Zhu et al., 2019) and dialogue systems (Ghosal et al., 2019; Zhang et al., d). Over the last few years, there has been a substantial body of research on emotion detection (Abdul-Mageed and Ungar, 2017; Zhou et al., 2019; Zhang et al., a), where a considerable amount of work has focused on multi-label emotion detection (Li et al., 2015; Yu et al., 2018; Ying et al., 2019). Basically, emotion detection is a multi-label classiﬁcation problem since one utterance naturally tends to involve more than one emotion category. However, classifying instances with multiple possible categories is sometimes much more difﬁcult than classifying instances with a single label. One Corresponding author Sad, Disgust Figure 1: An example of multi-modal instance with multi-label emotion categories in a video segment. Introduction ∗ ĊĊ the hardest thing that we face is ĊĊ main challenge is how to model the label dependence in the classiﬁcation approach. For examp"
2021.acl-long.222,N18-1118,0,0.0171027,"SR alone. Table 6: Performance (BLEU scores) on dev and test sets of ZH-EN translation with respect to different gap sentence ratios in pre-training task of document-level restoration. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model t"
2021.acl-long.222,2012.eamt-1.60,0,0.0139028,"parallel dataset and the monolingual document and segment words into sub-words by a BPE model with 30K (25K) operations (Sennrich et al., 2016). Fine-tuning data settings. For ZH-EN, we have one translation task on news domain. The document-level parallel corpus of training set include 41K documents with 780K sentence pairs.8 We use the NIST MT 2006 dataset as the development set, and combine the NIST MT 2002, 2003, 2004, 2005, 2008 datasets as test set.. For EN-DE, we test three translation tasks in domains of TED talks, News-Commentary and Europarl. • TED, which is from IWSLT 2017 MT track (Cettolo et al., 2012). We combine test2016 and test2017 as our test set while the rest as the development set. Experimentation To test the effect of our approach in leveraging sentence-level parallel dataset and monolingual documents, we carry out experiments on Chineseto-English (ZH-EN) and English-to-German (ENDE) translation. 4.1 Experimental Settings Pre-training data settings. The ZH-EN sentence-level parallel dataset contains 2.0M sentence pairs with 54.8M Chinese words and 60.8M English words.4 We use WMT14 EN-DE 4 It consists of LDC2002E18, LDC2003E07, LDC2003E14, news part of LDC2004T08, LDC2002T01, LDC20"
2021.acl-long.222,Q17-1024,0,0.049404,"Missing"
2021.acl-long.222,W19-5321,0,0.0420252,"Missing"
2021.acl-long.222,2020.emnlp-main.175,0,0.0462114,"ory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (2019) use source-side monolingual documents to explore multi-task training via the BERTobjective on the encoder. They simply concatenate sentences within a document int"
2021.acl-long.222,P17-4012,0,0.0297593,"rovement of averaged 1.36 BLEU and 1.72 Meteor. Moreover, when we use documents as translation units, our models (i.e., #3 Ours-doc) achieve further improvement by modeling document-level context. Compared to previous studies, it also shows that our approach surpasses all context-aware baselines on ZH-EN and EN-DE (TED) tasks and achieves the state-ofthe-art on average. In the scenario where both sentence-level parallel dataset and monolingual documents are used,12 similar performance trends also hold. For example, #5 Ours-sent significantly exceeds Transformer Model settings. We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it.11 For all translation models, the numbers of layers in the context encoder, sentence encoder and decoder (i.e., Ng , Ne , and Nd in Fig 3) are set to 6. The hidden size and the filter size are set to 512 and 2048, respectively. The number of heads in multi-head attention is 8 and the dropout rate is 0.1. In pre-training, we train the models for 500K steps on four V100 GPUs with batch-size 8192. We use Adam (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98 for optimization, and learning rate as 1, the warm-up step as 16K."
2021.acl-long.222,P07-2045,0,0.0126851,"Missing"
2021.acl-long.222,C18-1050,1,0.815965,"example, will be X respectively. Table 7 compares the performance when the pre-training task is of CA-MSR objective or combination of CA-GSR and CA-MSR.It Related Work We describe related studies in the following two perspectives. 6.1 5.4 Test 50.03 49.73 shows the combining objective achieves better performance than using CA-MSR alone. Table 6: Performance (BLEU scores) on dev and test sets of ZH-EN translation with respect to different gap sentence ratios in pre-training task of document-level restoration. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several atte"
2021.acl-long.222,W07-0734,0,0.0412222,"on test sets. Bi-sent/Mo-doc indicates if the models are pretrained on sentence-level parallel dataset or monolingual documents (7 for no and 3 for yes). Ours-sent/Ours-doc indicates that we use sentences or documents as input units, i.e., performing sentence-level NMT or context-aware NMT. Scores are obtained by running their source code with our model settings. • Europarl, which is extracted from the Europarl v7. The training, development and test sets are obtained through randomly splitting the corpus. Evaluation. For evaluation, we use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate translation quality. All above EN-DE document-level parallel datasets are downloaded from Maruf et al. (2019).10 Similar to fine-tuning datasets, the pre-processing steps consist of word segmentation, tokenization, long document split. Then we segment the words into subwords using the BPE models trained on pretraining datasets. See Appendix A for more statistics of the fine-tuning datasets. Main results. Table 1 shows the performance of our approach, where Ours-sent and Ours-doc indicate the performance achieved by our approach when we use sentences or documents as input units, re"
2021.acl-long.222,N19-1423,0,0.0188033,"context-aware NMT performance in the scenarios where the document-level parallel dataset is scale-limited, or even not available. On the one hand, sentence-level parallel dataset is a natural resource to use. For example, Zhang et al. (2018) propose a two-stage training strategy for context-aware NMT by pre-training the model on a sentencelevel parallel dataset. On the other hand, JunczysDowmunt (2019) leverage large-scale source-side monolingual documents, in which they simply concatenate sentences within a document into a long sequence and explore multi-task training via the BERT-objective (Devlin et al., 2019) on the encoder. Due to that different models are usually required to model sentences and documents, however, it is challenging to effectively take them both in a single model. In order to effectively and simultaneously model 2 We note that not all, but many context-aware NMT models contain a context encoder to extract global context information from the document. 2851 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2851–2861 August 1–6, 2021. ©2021 Association for Computat"
2021.acl-long.222,2020.acl-main.703,0,0.0431285,"tencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (2019) use source-side monolingual documents to explore multi-task training via the BERTobjective on the encoder. They simply concatenate sentences within a document into a long sequence, which is different from our approach. 6.2 Pre-training for Document-Level NMT While there are substantial studies on improving sentence-level NMT with pre-training, we limit ourselves here to pre-training for document-level (context-aware) NMT. BART (Lewis et al., 2020) is a denoising auto-encoder model which learns to reconstruct the original document from a noised version. Inspired by BART, mBART (Liu et al., 2858 2020) is a model trained on a mixed corpus containing monolingual documents of different languages. Both BART and mBART concatenate sentences in one document into a long sequence, and thus fall into a standard sequence-to-sequence (seq2seq) framework. This is very different from our cross-task pre-training, in which we combine both context-agnostic learning and context-aware learning in a single model. 7 Conclusion In order to leverage both large"
2021.acl-long.222,2020.wmt-1.71,0,0.0581681,"f ZH-EN translation with respect to different gap sentence ratios in pre-training task of document-level restoration. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (20"
2021.acl-long.222,2020.tacl-1.47,0,0.0324015,"Missing"
2021.acl-long.222,P18-1118,0,0.0816908,"cument-level perspectives. Experimental results on four translation tasks show that our approach significantly improves translation performance. One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents. 1 Introduction Document-level context-aware neural machine translation (NMT) aims to translate sentences in a document under the guidance of document-level context. Recent years have witnessed great improvement in context-aware NMT with extensive attempts at effectively leveraging document-level context ((Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Maruf et al., 2019), to name a few). However, the performance of contextaware NMT still suffers from the size of parallel document dataset. On the one hand, unlike ∗ Corresponding Author: Junhui Li. If not specified, monolingual documents are all for sourceside through this paper. 1 sentence-level translation models which could be well trained on large-scale sentence-level parallel datasets, the translation models of context-aware NMT may result in insufficient training. On the other hand, with only scale-limited source-side documents, the context encoders may fail to effectively extract use"
2021.acl-long.222,N19-1313,0,0.223252,". Experimental results on four translation tasks show that our approach significantly improves translation performance. One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents. 1 Introduction Document-level context-aware neural machine translation (NMT) aims to translate sentences in a document under the guidance of document-level context. Recent years have witnessed great improvement in context-aware NMT with extensive attempts at effectively leveraging document-level context ((Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Maruf et al., 2019), to name a few). However, the performance of contextaware NMT still suffers from the size of parallel document dataset. On the one hand, unlike ∗ Corresponding Author: Junhui Li. If not specified, monolingual documents are all for sourceside through this paper. 1 sentence-level translation models which could be well trained on large-scale sentence-level parallel datasets, the translation models of context-aware NMT may result in insufficient training. On the other hand, with only scale-limited source-side documents, the context encoders may fail to effectively extract useful context from the"
2021.acl-long.222,D18-1325,0,0.0263752,"Missing"
2021.acl-long.222,P02-1040,0,0.109336,"rformance (BLEU and Meteor scores) on test sets. Bi-sent/Mo-doc indicates if the models are pretrained on sentence-level parallel dataset or monolingual documents (7 for no and 3 for yes). Ours-sent/Ours-doc indicates that we use sentences or documents as input units, i.e., performing sentence-level NMT or context-aware NMT. Scores are obtained by running their source code with our model settings. • Europarl, which is extracted from the Europarl v7. The training, development and test sets are obtained through randomly splitting the corpus. Evaluation. For evaluation, we use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate translation quality. All above EN-DE document-level parallel datasets are downloaded from Maruf et al. (2019).10 Similar to fine-tuning datasets, the pre-processing steps consist of word segmentation, tokenization, long document split. Then we segment the words into subwords using the BPE models trained on pretraining datasets. See Appendix A for more statistics of the fine-tuning datasets. Main results. Table 1 shows the performance of our approach, where Ours-sent and Ours-doc indicate the performance achieved by our approach when we use sent"
2021.acl-long.222,P16-1162,0,0.127889,"Missing"
2021.acl-long.222,D19-1168,1,0.714836,"storation. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (2019) use source-side monolingual documents to explore multi-task training via the BERTobjective on the enco"
2021.acl-long.222,W17-4811,0,0.022607,"from both sentencelevel and document-level perspectives. Experimental results on four translation tasks show that our approach significantly improves translation performance. One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents. 1 Introduction Document-level context-aware neural machine translation (NMT) aims to translate sentences in a document under the guidance of document-level context. Recent years have witnessed great improvement in context-aware NMT with extensive attempts at effectively leveraging document-level context ((Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Maruf et al., 2019), to name a few). However, the performance of contextaware NMT still suffers from the size of parallel document dataset. On the one hand, unlike ∗ Corresponding Author: Junhui Li. If not specified, monolingual documents are all for sourceside through this paper. 1 sentence-level translation models which could be well trained on large-scale sentence-level parallel datasets, the translation models of context-aware NMT may result in insufficient training. On the other hand, with only scale-limited source-side documents, the context encoders may fail t"
2021.acl-long.222,W17-4802,0,0.0221713,"Missing"
2021.acl-long.222,D19-1164,0,0.0578752,"ed studies, we lowercase English sentences in ZHEN while truecase English and German sentences in EN-DE. 8 It consists of LDC2002T01, LDC2004T07, LDC2005T06, LDC2005T10, LDC2009T02, LDC2009T15, LDC2010T03. Note that they are also included in ZH-EN parallel dataset. 9 http://www.casmacat.eu/corpus/news-co mmentary.html 2855 Model # ZH-EN BLEU Meteor 7 40.32 27.93 7 40.83 28.19 7 41.01 28.37 7 7 40.92 28.25 7 39.64 27.56 7 40.73 27.97 7 41.27 28.46 3 46.30 32.91 3 49.58 35.97 3 50.03 36.50 Bi- Mosent doc DocT (Zhang et al., 2018) 7 HAN (Miculicich et al., 2018) 7 SAN (Maruf et al., 2019) 7 QCN (Yang et al., 2019) 7 MCN (Zheng et al., 2020) 7 #1 Transformer 7 7 #2 Ours-sent #3 Ours-doc 7 #4 Transformer 3 #5 Ours-sent 3 #6 Ours-doc 3 EN-DE (TED) BLEU Meteor 24.00 44.69 24.58 45.48 24.42 45.26 25.19 46.09 25.10 23.02 43.66 24.75 45.83 25.31 46.30 26.94 47.06 28.73 48.80 29.31 49.40 EN-DE (News) BLEU Meteor 23.08 42.40 25.03 44.02 24.84 44.17 22.37 41.88 24.91 22.03 41.37 24.19 43.96 24.70 44.38 26.80 46.99 28.41 48.52 29.01 48.83 EN-DE (Europarl) BLEU Meteor 29.32 46.72 28.60 46.09 29.75 47.22 29.82 47.86 30.40 28.65 45.83 29.10 47.55 30.07 47.93 29.90 47.50 30.61 48.29 31.52 49.02 Avg. BLEU Meteor 29.18"
2021.acl-long.222,2020.tacl-1.23,0,0.0232631,"g et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-level translations. Finally, JunczysDowmunt (2019) use source-side monolingual documents to explore multi-task training via the BERTobjective on the encoder. They simply concatenate sentences within a document into a long sequence, which is different from our approach. 6.2 Pre-training for Document-Level NMT While there are substantial studies on improving sentence-level NMT with pre-training, we limit ourselves here to pre-training for document-level (context-aware) NMT. BART (Lewis et al., 2020) is a denoising auto-encoder model which"
2021.acl-long.222,D18-1049,1,0.913664,"o break the corpus bottleneck for context-aware NMT by leveraging both largescale sentence-level parallel dataset and monolingual documents. Specifically, we aim to use the former to boost the performance of translation models while employ the latter to enhance the context encoders’ capability of capturing useful context information. There have been several attempts to boost context-aware NMT performance in the scenarios where the document-level parallel dataset is scale-limited, or even not available. On the one hand, sentence-level parallel dataset is a natural resource to use. For example, Zhang et al. (2018) propose a two-stage training strategy for context-aware NMT by pre-training the model on a sentencelevel parallel dataset. On the other hand, JunczysDowmunt (2019) leverage large-scale source-side monolingual documents, in which they simply concatenate sentences within a document into a long sequence and explore multi-task training via the BERT-objective (Devlin et al., 2019) on the encoder. Due to that different models are usually required to model sentences and documents, however, it is challenging to effectively take them both in a single model. In order to effectively and simultaneously m"
2021.acl-long.222,Q18-1029,0,0.0560714,"Missing"
2021.acl-long.222,D19-1081,0,0.0570404,"el translation as a fine-tuning task. Table 3 compares the performance with respect to different fine-tuning strategies and different input units in inferring. When we use documents as input units in inferring, the joint fine-tuning strategy provides no advantage. However, when the input units are sentences, the joint fine-tuning strategy outperforms the one not including sentence-level translation in fine-tuning. 5.2 Analysis of Discourse Phenomena We also want to examine whether the proposed approach actually learns to utilize document context to resolve discourse inconsistencies. Following Voita et al. (2019b) and Zheng et al. (2020), we use the same datasets to train model and contrastive test set for the evaluation of discourse phenomena for English-Russian by Voita et al. (2019b). There are four test sets in the suite regarding deixis, lexicon consistency, ellipsis (inflection and verb phrase). Each testset contains groups of contrastive examples consisting of a positive translation with correct discourse phenomenon and negative translations with incorrect phenomena. The goal is to figure out if a model is more likely to generate a cor2857 Model Trans. Ours Bi-sent Mo-doc 7 7 7 7 Dev 67.30 68."
2021.acl-long.222,P19-1116,0,0.0302677,"Missing"
2021.acl-long.222,P18-1117,0,0.0178409,"rformance (BLEU scores) on dev and test sets of ZH-EN translation with respect to different gap sentence ratios in pre-training task of document-level restoration. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monolingual document data. To make translations more coherent within a document, Voita et al. (2019a) propose DocRepair trained on monolingual target language documents to correct the inconsistencies in sentence-level translation while Yu et al. (2020) train a context-aware language model to rerank sentence-le"
2021.acl-long.222,D17-1301,0,0.0138548,"ares the performance when the pre-training task is of CA-MSR objective or combination of CA-GSR and CA-MSR.It Related Work We describe related studies in the following two perspectives. 6.1 5.4 Test 50.03 49.73 shows the combining objective achieves better performance than using CA-MSR alone. Table 6: Performance (BLEU scores) on dev and test sets of ZH-EN translation with respect to different gap sentence ratios in pre-training task of document-level restoration. 5.3 Dev 50.90 50.61 Context-Aware NMT Cache/Memory-based approaches (Tu et al., 2018; Kuang et al., 2018; Maruf and Haffari, 2018; Wang et al., 2017) store word/sentence translation in previous sentences for future sentence translation. Various approaches with an extra context encoders are proposed to model either local context, e.g., previous sentences (Jean et al., 2017; Wang et al., 2017; Zhang et al., 2018; Bawden et al., 2018; Voita et al., 2018, 2019b; Yang et al., 2019; Huo et al., 2020), or entire document (Maruf and Haffari, 2018; Mace and Servan, 2019; Maruf et al., 2019; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020; Kang et al., 2020). Besides, there have been several attempts to improve context-aware NMT with monoli"
2021.acl-long.305,E17-1028,0,0.615376,"the release of various discourse corpora, text-level DSR parsing has been drawing more and more attention in the last decade. However, since the corpus annotation is usually time-consuming, existing DRS corpora are much limited in size. For example, the English RST-DT (Carlson et al., 2001) corpus only contains 385 WSJ articles, and the Chinese CDTB (Li et al., 2014b) corpus only contains 500 newswire articles. In this situation, previous studies usually rely on multifarious handengineered features (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Braud et al., 2017). And all these systems perform DRS parsing in a bottom-up fashion. Until recently, some researchers turn to top-down DRS parsing (Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) to explore the potential capabilities of data-driven models. Nevertheless, text-level DRS parsing is still challenging and worthy of in-depth exploration. Theoretically, in supervised learning, annotated 3946 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3946–3957 August 1–6, 2021."
2021.acl-long.305,C16-1179,0,0.0439678,"Missing"
2021.acl-long.305,W01-1605,0,0.862132,"394 Introduction ∗ e2 Elaboration (NS) nucleus is considered more important than the satellite. Since the RST structure can well describe the organization of an article, it has been playing a central role in various down-stream tasks like summarization (Xu et al., 2020), text categorization (Ji and Smith, 2017), and so on. With the release of various discourse corpora, text-level DSR parsing has been drawing more and more attention in the last decade. However, since the corpus annotation is usually time-consuming, existing DRS corpora are much limited in size. For example, the English RST-DT (Carlson et al., 2001) corpus only contains 385 WSJ articles, and the Chinese CDTB (Li et al., 2014b) corpus only contains 500 newswire articles. In this situation, previous studies usually rely on multifarious handengineered features (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Braud et al., 2017). And all these systems perform DRS parsing in a bottom-up fashion. Until recently, some researchers turn to top-down DRS parsing (Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) to explore the potential capabilities of data-driven models. Nevertheless, text-"
2021.acl-long.305,P19-1211,0,0.0281919,"models. Among them, (Lin et al., 2019; Liu et al., 2019) have achieved certain success in sentencelevel DRS parsing. Nevertheless, due to the longdistance dependency over the discourse, text-level DRS parsing remains challenging. To alleviate this problem, Zhang et al. (2020) proposed a top-down architecture tailored for text-level DRS parsing. Kobayashi et al. (2020) used contextualized word representation and proposed to parse a document in three granularity levels for good performance. In the past decade, GANs have achieved great progress in NLP (Wu et al., 2019; Elazar and Goldberg, 2018; Chen and Chen, 2019; Zou et al., 2020). However, to our knowledge, there is still no research on adversarial learning in DRS parsing so far. In this work, we explore to adversarially train a discriminator to estimate the quality of the entire DRS tree for global optimization. Notably, we propose to transform each DRS tree into a continuous tree diagram, and thus our adversarial method does not suffer from the “discrete data” problem. 3 Related Work In the literature, previous studies on RST-style DRS parsing mainly consist of two categories, i.e., Baseline Top-Down Architecture In this section, we give a brief i"
2021.acl-long.305,2020.findings-emnlp.58,0,0.0201889,"own DRS parsers in the table show similar results on “R”, we speculate that the Chinese rhetorical relation prediction has encountered a bottleneck to some extent, which requires more effort to be invested. Performances based on the SOTA language models. Recently, more and more researchers (Shi et al., 2020; Koto et al., 2021) propose to improve DRS parsing performance through powerful language models (LMs) like Bert (Devlin et al., 2019) and XLNet (Yang et al., 2019). Following these studies, in this work, we perform additional experiments on the XLNet-base models in (Yang et al., 2019) and (Cui et al., 2020) for the RST-DT and CDTB corpus, respectively. For better model integration, we slightly adjust the previously described model architecture5 , more specifically, the EDU encoder. We first use a pre-trained LM to encode each entire discourse where each EDU is attached with the [SEP] and [CLS] tokens and then take the LM outputs corresponding to [CLS] as our EDU representation. Moreover, we segment each document according to the maximum length of 768 tokens and encode these text segments one by one to avoid the problem of memory overflow. For the RST-DT corpus, we report the results of the recen"
2021.acl-long.305,N19-1423,0,0.0361809,"due to the joint prediction of nuclearity and relation categories. This suggests the robustness of our simplified parser with only two classifiers. Moreover, since the two top-down DRS parsers in the table show similar results on “R”, we speculate that the Chinese rhetorical relation prediction has encountered a bottleneck to some extent, which requires more effort to be invested. Performances based on the SOTA language models. Recently, more and more researchers (Shi et al., 2020; Koto et al., 2021) propose to improve DRS parsing performance through powerful language models (LMs) like Bert (Devlin et al., 2019) and XLNet (Yang et al., 2019). Following these studies, in this work, we perform additional experiments on the XLNet-base models in (Yang et al., 2019) and (Cui et al., 2020) for the RST-DT and CDTB corpus, respectively. For better model integration, we slightly adjust the previously described model architecture5 , more specifically, the EDU encoder. We first use a pre-trained LM to encode each entire discourse where each EDU is attached with the [SEP] and [CLS] tokens and then take the LM outputs corresponding to [CLS] as our EDU representation. Moreover, we segment each document according t"
2021.acl-long.305,D18-1002,0,0.0251816,"apabilities of data-driven models. Among them, (Lin et al., 2019; Liu et al., 2019) have achieved certain success in sentencelevel DRS parsing. Nevertheless, due to the longdistance dependency over the discourse, text-level DRS parsing remains challenging. To alleviate this problem, Zhang et al. (2020) proposed a top-down architecture tailored for text-level DRS parsing. Kobayashi et al. (2020) used contextualized word representation and proposed to parse a document in three granularity levels for good performance. In the past decade, GANs have achieved great progress in NLP (Wu et al., 2019; Elazar and Goldberg, 2018; Chen and Chen, 2019; Zou et al., 2020). However, to our knowledge, there is still no research on adversarial learning in DRS parsing so far. In this work, we explore to adversarially train a discriminator to estimate the quality of the entire DRS tree for global optimization. Notably, we propose to transform each DRS tree into a continuous tree diagram, and thus our adversarial method does not suffer from the “discrete data” problem. 3 Related Work In the literature, previous studies on RST-style DRS parsing mainly consist of two categories, i.e., Baseline Top-Down Architecture In this secti"
2021.acl-long.305,P14-1048,0,0.403871,"t al., 2020), text categorization (Ji and Smith, 2017), and so on. With the release of various discourse corpora, text-level DSR parsing has been drawing more and more attention in the last decade. However, since the corpus annotation is usually time-consuming, existing DRS corpora are much limited in size. For example, the English RST-DT (Carlson et al., 2001) corpus only contains 385 WSJ articles, and the Chinese CDTB (Li et al., 2014b) corpus only contains 500 newswire articles. In this situation, previous studies usually rely on multifarious handengineered features (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Braud et al., 2017). And all these systems perform DRS parsing in a bottom-up fashion. Until recently, some researchers turn to top-down DRS parsing (Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) to explore the potential capabilities of data-driven models. Nevertheless, text-level DRS parsing is still challenging and worthy of in-depth exploration. Theoretically, in supervised learning, annotated 3946 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confere"
2021.acl-long.305,P14-1002,0,0.444362,"egorization (Ji and Smith, 2017), and so on. With the release of various discourse corpora, text-level DSR parsing has been drawing more and more attention in the last decade. However, since the corpus annotation is usually time-consuming, existing DRS corpora are much limited in size. For example, the English RST-DT (Carlson et al., 2001) corpus only contains 385 WSJ articles, and the Chinese CDTB (Li et al., 2014b) corpus only contains 500 newswire articles. In this situation, previous studies usually rely on multifarious handengineered features (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Braud et al., 2017). And all these systems perform DRS parsing in a bottom-up fashion. Until recently, some researchers turn to top-down DRS parsing (Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) to explore the potential capabilities of data-driven models. Nevertheless, text-level DRS parsing is still challenging and worthy of in-depth exploration. Theoretically, in supervised learning, annotated 3946 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language P"
2021.acl-long.305,P17-1092,0,0.0152681,"lite (S)) tags to form high-layer discourse units (DUs), where the Corresponding author e4 [e 1 : In fact,] [e 2 : Budget indicated] [e 3 : it saw some benefit] [e4: to staying involved in these programs,] [e5: in which renters earn frequent-flier miles] [e6: and fliers can get car-rental discounts.] wsj_2394 Introduction ∗ e2 Elaboration (NS) nucleus is considered more important than the satellite. Since the RST structure can well describe the organization of an article, it has been playing a central role in various down-stream tasks like summarization (Xu et al., 2020), text categorization (Ji and Smith, 2017), and so on. With the release of various discourse corpora, text-level DSR parsing has been drawing more and more attention in the last decade. However, since the corpus annotation is usually time-consuming, existing DRS corpora are much limited in size. For example, the English RST-DT (Carlson et al., 2001) corpus only contains 385 WSJ articles, and the Chinese CDTB (Li et al., 2014b) corpus only contains 500 newswire articles. In this situation, previous studies usually rely on multifarious handengineered features (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et a"
2021.acl-long.305,P13-1048,0,0.0359983,"ovements to it. For global optimization, we first utilize a novel strategy to transform both gold standard and predicted DRS trees into tree diagrams with two color channels. After that, an LSGAN-based adversarial bot is structured between gold and fake tree diagrams as an examiner for global estimation and optimization. Experimental results on the RST-DT and CDTB corpora show that our approaches are effective. 2 bottom-up and top-down frameworks. For the first category, early studies on DRS parsing heavily relied on hand-crafted features and linguistic characteristics (Hernault et al., 2010; Joty et al., 2013; Feng and Hirst, 2014). During the past decade, more and more researchers turned to data-driven approaches, and some effective strategies were proposed to adapt to the small-scale data corpora. Among these studies, (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Mabona et al., 2019) used some trivial features as auxiliaries in their data-driven systems; Braud et al. (2016; 2017) harnessed task supervision from related tasks, alternative views on discourse structures, and crosslingual data to alleviate the data insufficiency problem; Wang et al. (2017) introduced a two-stage parser to first"
2021.acl-long.305,2021.eacl-main.60,0,0.0748888,"ucture labeled with both nuclearity and relation categories. However, since current performances on S, N and R are imbalanced, the performance on F is much limited by relation prediction. In other words, the Full score may underestimate the performance in span and nuclearity prediction. In this work, we combine nuclearity and rhetorical relation tags for joint N-R prediction aiming to reduce the uncertainty of the Full measure. Moreover, since RST-Parseval (Marcu, 2000) overestimates the DRS parsing performance to a certain extent, (Morey et al., 2017; Mabona et al., 2019; Zhang et al., 2020; Koto et al., 2021) adopt the original Parseval to reveal the actual performance level of DRS parsing. Following these studies, we also use the original Parseval for evaluation and report the micro-averaged F1 scores by default. Hyper-Parameter Setting. For word representation, we employed the 300D vectors of GloVe (Pennington et al., 2014) and the 1024D vectors of ELMo (Peters et al., 2018) for RST-DT and the 300D vectors of Qiu et al. (2018) (Qiu-W2V) for CDTB, and we did not update these vectors during training. The English POS tags were obtained through the Stanford CoreNLP toolkit (Manning et al., 2014), th"
2021.acl-long.305,D14-1220,0,0.372386,"e satellite. Since the RST structure can well describe the organization of an article, it has been playing a central role in various down-stream tasks like summarization (Xu et al., 2020), text categorization (Ji and Smith, 2017), and so on. With the release of various discourse corpora, text-level DSR parsing has been drawing more and more attention in the last decade. However, since the corpus annotation is usually time-consuming, existing DRS corpora are much limited in size. For example, the English RST-DT (Carlson et al., 2001) corpus only contains 385 WSJ articles, and the Chinese CDTB (Li et al., 2014b) corpus only contains 500 newswire articles. In this situation, previous studies usually rely on multifarious handengineered features (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Braud et al., 2017). And all these systems perform DRS parsing in a bottom-up fashion. Until recently, some researchers turn to top-down DRS parsing (Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) to explore the potential capabilities of data-driven models. Nevertheless, text-level DRS parsing is still challenging and worthy of in-depth exploration. Th"
2021.acl-long.305,D16-1035,0,0.643552,"necessity of joint N-R prediction. Considering the above results, in the following, we separately use two sets of model settings for different languages. For English, we build our final model based on the simplified architecture without dummy split points. For Chinese, we build our final model based on the architecture of Zhang et al. (2020). For both systems, we only use two classifiers for DRS parsing. 3951 Systems Final EN - Advers. bot Final CN - Advers. bot S 71.8 70.7 84.9 83.2 N 59.5 58.3 58.4 57.8 R 47.0 46.5 54.5 52.7 F 45.9 45.2 50.3 49.0 Systems JE2017-updated Feng and Hirst (2014) Li et al. (2016) Braud et al. (2016) EN Braud et al. (2017) Mabona et al. (2019) Zhang et al. (2020) Ours (GloVe) Ours (ELMo) Zhang et al. (2020) CN Zhang et al. (2020)* Ours (Qiu-W2V) Table 3: Comparison on the adversarial bot. Comparison on the adversarial bot. Here, we perform experiments to explore the effects of the adversarial learning approach, and the experimental results are presented in Table 3. For the RST-DT corpus, the results show that our adversarial model setting can improve the performance on all the four indicators, especially in structure and nuclearity prediction. Similarly, the results on"
2021.acl-long.305,D14-1224,1,0.928989,"e satellite. Since the RST structure can well describe the organization of an article, it has been playing a central role in various down-stream tasks like summarization (Xu et al., 2020), text categorization (Ji and Smith, 2017), and so on. With the release of various discourse corpora, text-level DSR parsing has been drawing more and more attention in the last decade. However, since the corpus annotation is usually time-consuming, existing DRS corpora are much limited in size. For example, the English RST-DT (Carlson et al., 2001) corpus only contains 385 WSJ articles, and the Chinese CDTB (Li et al., 2014b) corpus only contains 500 newswire articles. In this situation, previous studies usually rely on multifarious handengineered features (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Braud et al., 2017). And all these systems perform DRS parsing in a bottom-up fashion. Until recently, some researchers turn to top-down DRS parsing (Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) to explore the potential capabilities of data-driven models. Nevertheless, text-level DRS parsing is still challenging and worthy of in-depth exploration. Th"
2021.acl-long.305,P19-1410,0,0.0596112,"us annotation is usually time-consuming, existing DRS corpora are much limited in size. For example, the English RST-DT (Carlson et al., 2001) corpus only contains 385 WSJ articles, and the Chinese CDTB (Li et al., 2014b) corpus only contains 500 newswire articles. In this situation, previous studies usually rely on multifarious handengineered features (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Braud et al., 2017). And all these systems perform DRS parsing in a bottom-up fashion. Until recently, some researchers turn to top-down DRS parsing (Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) to explore the potential capabilities of data-driven models. Nevertheless, text-level DRS parsing is still challenging and worthy of in-depth exploration. Theoretically, in supervised learning, annotated 3946 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3946–3957 August 1–6, 2021. ©2021 Association for Computational Linguistics n2 n2 Local OPT or Global OPT n1 e1 n1 e3 e2 (a) e1 e3 e2 (b) Figure 2: Local and global optimizati"
2021.acl-long.305,D19-1233,0,0.232998,"g show that • Compared with bottom-up DRS parsers, recent top-down frameworks can better leverage global document context and have achieved promising results in text-level DRS parsing (Zhang et al., 2020; Kobayashi et al., 2020). • All previous studies produce their DRS parsers with local decisions made at each time step for either bottom-up node composition or top-down split point selection (Figure 2 (a)), and no global decisions are made for the entire DRS structure (Figure 2 (b)). Therefore, it is difficult for them to achieve global optimization. Although some studies (Braud et al., 2017; Mabona et al., 2019) leverage “beam-search” to traverse the solution space to find the optimal parsing route, the algorithms are time-consuming to some extent. Considering the above-mentioned status quo, in this work, we study a global optimization method based on the well-performing top-down parsers. For model structure, we take the top-down parser of Zhang et al. (2020) as our baseline system and make some improvements to it. For global optimization, we first utilize a novel strategy to transform both gold standard and predicted DRS trees into tree diagrams with two color channels. After that, an LSGAN-based ad"
2021.acl-long.305,P14-5010,0,0.00265875,"2020; Koto et al., 2021) adopt the original Parseval to reveal the actual performance level of DRS parsing. Following these studies, we also use the original Parseval for evaluation and report the micro-averaged F1 scores by default. Hyper-Parameter Setting. For word representation, we employed the 300D vectors of GloVe (Pennington et al., 2014) and the 1024D vectors of ELMo (Peters et al., 2018) for RST-DT and the 300D vectors of Qiu et al. (2018) (Qiu-W2V) for CDTB, and we did not update these vectors during training. The English POS tags were obtained through the Stanford CoreNLP toolkit (Manning et al., 2014), the Chinese tags were borrowed from Chinese PTB, and all the POS embeddings were optimized during training. For model learning, we used the development set to fine-tune the parameters in Table 1, and the number of parameter search trials was around 20. All the experiments based on the above-mentioned settings were conducted on GeForce RTX 2080Ti GPU, and the codes will be published at https://github.com/ NLP-Discourse-SoochowU/GAN_DP. 5.2 Experimental Results Comparison between different system settings. As stated before, we explore to make possible improvements to the top-down architecture"
2021.acl-long.305,W05-1513,0,0.113776,"2020), we utilize both the English RST Discourse Treebank (RST-DT) (Carlson et al., 2001) and the Chinese Connective-driven Discourse TreeBank (CDTB) (Li et al., 2014b) as the benchmark corpora for experimentation. Here, we give a brief introduction to the two corpora: • The RST-DT corpus contains 385 news articles (347 for training and 38 for testing) from the Wall Street Journal (WSJ). Following previous work, we randomly select 34 documents from the training corpus as the development corpus for parameter tuning. And we also binarize those non-binary subtrees in RST-DT with right-branching (Sagae and Lavie, 2005) for preprocessing. • The Chinese CDTB corpus is motivated by taking advantages of both the English RST-DT corpus and the PDTB corpus (Prasad et al., 2008). The CDTB corpus annotates each paragraph as a Connective-driven Discourse Tree (CDT). The corpus consists of 500 newswire articles which are further segmented into 2336 paragraphs and 10650 EDUs. The corpus is divided into three parts with 425 articles (2002 CDT trees) for training, 25 articles (105 CDT trees) for validation, and 50 articles (229 CDT trees) for testing. 3950 Metrics. Following previous studies, we measure the performance o"
2021.acl-long.305,P17-2029,0,0.083009,"characteristics (Hernault et al., 2010; Joty et al., 2013; Feng and Hirst, 2014). During the past decade, more and more researchers turned to data-driven approaches, and some effective strategies were proposed to adapt to the small-scale data corpora. Among these studies, (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Mabona et al., 2019) used some trivial features as auxiliaries in their data-driven systems; Braud et al. (2016; 2017) harnessed task supervision from related tasks, alternative views on discourse structures, and crosslingual data to alleviate the data insufficiency problem; Wang et al. (2017) introduced a two-stage parser to first parse a naked tree structure and then determine rhetorical relations for different discourse levels to mitigate data sparsity; Yu et al. (2018) employed both syntax information and discourse boundaries in their transition-based system and achieved good performance. For the second category, some researchers (Lin et al., 2019; Liu et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) turned to top-down frameworks to tap the potential capabilities of data-driven models. Among them, (Lin et al., 2019; Liu et al., 2019) have achieved certain success in se"
2021.acl-long.305,P19-1375,0,0.0206867,"p the potential capabilities of data-driven models. Among them, (Lin et al., 2019; Liu et al., 2019) have achieved certain success in sentencelevel DRS parsing. Nevertheless, due to the longdistance dependency over the discourse, text-level DRS parsing remains challenging. To alleviate this problem, Zhang et al. (2020) proposed a top-down architecture tailored for text-level DRS parsing. Kobayashi et al. (2020) used contextualized word representation and proposed to parse a document in three granularity levels for good performance. In the past decade, GANs have achieved great progress in NLP (Wu et al., 2019; Elazar and Goldberg, 2018; Chen and Chen, 2019; Zou et al., 2020). However, to our knowledge, there is still no research on adversarial learning in DRS parsing so far. In this work, we explore to adversarially train a discriminator to estimate the quality of the entire DRS tree for global optimization. Notably, we propose to transform each DRS tree into a continuous tree diagram, and thus our adversarial method does not suffer from the “discrete data” problem. 3 Related Work In the literature, previous studies on RST-style DRS parsing mainly consist of two categories, i.e., Baseline Top-Down"
2021.acl-long.305,2020.acl-main.451,0,0.0370349,"on and nuclearity (nucleus (N) or satellite (S)) tags to form high-layer discourse units (DUs), where the Corresponding author e4 [e 1 : In fact,] [e 2 : Budget indicated] [e 3 : it saw some benefit] [e4: to staying involved in these programs,] [e5: in which renters earn frequent-flier miles] [e6: and fliers can get car-rental discounts.] wsj_2394 Introduction ∗ e2 Elaboration (NS) nucleus is considered more important than the satellite. Since the RST structure can well describe the organization of an article, it has been playing a central role in various down-stream tasks like summarization (Xu et al., 2020), text categorization (Ji and Smith, 2017), and so on. With the release of various discourse corpora, text-level DSR parsing has been drawing more and more attention in the last decade. However, since the corpus annotation is usually time-consuming, existing DRS corpora are much limited in size. For example, the English RST-DT (Carlson et al., 2001) corpus only contains 385 WSJ articles, and the Chinese CDTB (Li et al., 2014b) corpus only contains 500 newswire articles. In this situation, previous studies usually rely on multifarious handengineered features (Hernault et al., 2010; Feng and Hir"
2021.acl-long.305,D17-1136,0,0.0664385,"ntly, the Full (F) indicator is used to estimate the tree structure labeled with both nuclearity and relation categories. However, since current performances on S, N and R are imbalanced, the performance on F is much limited by relation prediction. In other words, the Full score may underestimate the performance in span and nuclearity prediction. In this work, we combine nuclearity and rhetorical relation tags for joint N-R prediction aiming to reduce the uncertainty of the Full measure. Moreover, since RST-Parseval (Marcu, 2000) overestimates the DRS parsing performance to a certain extent, (Morey et al., 2017; Mabona et al., 2019; Zhang et al., 2020; Koto et al., 2021) adopt the original Parseval to reveal the actual performance level of DRS parsing. Following these studies, we also use the original Parseval for evaluation and report the micro-averaged F1 scores by default. Hyper-Parameter Setting. For word representation, we employed the 300D vectors of GloVe (Pennington et al., 2014) and the 1024D vectors of ELMo (Peters et al., 2018) for RST-DT and the 300D vectors of Qiu et al. (2018) (Qiu-W2V) for CDTB, and we did not update these vectors during training. The English POS tags were obtained th"
2021.acl-long.305,D14-1162,0,0.0922317,"earity and rhetorical relation tags for joint N-R prediction aiming to reduce the uncertainty of the Full measure. Moreover, since RST-Parseval (Marcu, 2000) overestimates the DRS parsing performance to a certain extent, (Morey et al., 2017; Mabona et al., 2019; Zhang et al., 2020; Koto et al., 2021) adopt the original Parseval to reveal the actual performance level of DRS parsing. Following these studies, we also use the original Parseval for evaluation and report the micro-averaged F1 scores by default. Hyper-Parameter Setting. For word representation, we employed the 300D vectors of GloVe (Pennington et al., 2014) and the 1024D vectors of ELMo (Peters et al., 2018) for RST-DT and the 300D vectors of Qiu et al. (2018) (Qiu-W2V) for CDTB, and we did not update these vectors during training. The English POS tags were obtained through the Stanford CoreNLP toolkit (Manning et al., 2014), the Chinese tags were borrowed from Chinese PTB, and all the POS embeddings were optimized during training. For model learning, we used the development set to fine-tune the parameters in Table 1, and the number of parameter search trials was around 20. All the experiments based on the above-mentioned settings were conducted"
2021.acl-long.305,N18-1202,0,0.0153934,"tion aiming to reduce the uncertainty of the Full measure. Moreover, since RST-Parseval (Marcu, 2000) overestimates the DRS parsing performance to a certain extent, (Morey et al., 2017; Mabona et al., 2019; Zhang et al., 2020; Koto et al., 2021) adopt the original Parseval to reveal the actual performance level of DRS parsing. Following these studies, we also use the original Parseval for evaluation and report the micro-averaged F1 scores by default. Hyper-Parameter Setting. For word representation, we employed the 300D vectors of GloVe (Pennington et al., 2014) and the 1024D vectors of ELMo (Peters et al., 2018) for RST-DT and the 300D vectors of Qiu et al. (2018) (Qiu-W2V) for CDTB, and we did not update these vectors during training. The English POS tags were obtained through the Stanford CoreNLP toolkit (Manning et al., 2014), the Chinese tags were borrowed from Chinese PTB, and all the POS embeddings were optimized during training. For model learning, we used the development set to fine-tune the parameters in Table 1, and the number of parameter search trials was around 20. All the experiments based on the above-mentioned settings were conducted on GeForce RTX 2080Ti GPU, and the codes will be pu"
2021.acl-long.305,prasad-etal-2008-penn,0,0.0135846,"al., 2014b) as the benchmark corpora for experimentation. Here, we give a brief introduction to the two corpora: • The RST-DT corpus contains 385 news articles (347 for training and 38 for testing) from the Wall Street Journal (WSJ). Following previous work, we randomly select 34 documents from the training corpus as the development corpus for parameter tuning. And we also binarize those non-binary subtrees in RST-DT with right-branching (Sagae and Lavie, 2005) for preprocessing. • The Chinese CDTB corpus is motivated by taking advantages of both the English RST-DT corpus and the PDTB corpus (Prasad et al., 2008). The CDTB corpus annotates each paragraph as a Connective-driven Discourse Tree (CDT). The corpus consists of 500 newswire articles which are further segmented into 2336 paragraphs and 10650 EDUs. The corpus is divided into three parts with 425 articles (2002 CDT trees) for training, 25 articles (105 CDT trees) for validation, and 50 articles (229 CDT trees) for testing. 3950 Metrics. Following previous studies, we measure the performance of bare tree structure (S), tree structure labeled with nuclearity (N), and tree structure labeled with rhetorical relation (R). Recently, the Full (F) indi"
2021.acl-long.305,C18-1047,0,0.17981,"strategies were proposed to adapt to the small-scale data corpora. Among these studies, (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Mabona et al., 2019) used some trivial features as auxiliaries in their data-driven systems; Braud et al. (2016; 2017) harnessed task supervision from related tasks, alternative views on discourse structures, and crosslingual data to alleviate the data insufficiency problem; Wang et al. (2017) introduced a two-stage parser to first parse a naked tree structure and then determine rhetorical relations for different discourse levels to mitigate data sparsity; Yu et al. (2018) employed both syntax information and discourse boundaries in their transition-based system and achieved good performance. For the second category, some researchers (Lin et al., 2019; Liu et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) turned to top-down frameworks to tap the potential capabilities of data-driven models. Among them, (Lin et al., 2019; Liu et al., 2019) have achieved certain success in sentencelevel DRS parsing. Nevertheless, due to the longdistance dependency over the discourse, text-level DRS parsing remains challenging. To alleviate this problem, Zhang et al. (2020"
2021.acl-long.305,2020.acl-main.569,1,0.080566,"sually time-consuming, existing DRS corpora are much limited in size. For example, the English RST-DT (Carlson et al., 2001) corpus only contains 385 WSJ articles, and the Chinese CDTB (Li et al., 2014b) corpus only contains 500 newswire articles. In this situation, previous studies usually rely on multifarious handengineered features (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Braud et al., 2017). And all these systems perform DRS parsing in a bottom-up fashion. Until recently, some researchers turn to top-down DRS parsing (Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020) to explore the potential capabilities of data-driven models. Nevertheless, text-level DRS parsing is still challenging and worthy of in-depth exploration. Theoretically, in supervised learning, annotated 3946 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3946–3957 August 1–6, 2021. ©2021 Association for Computational Linguistics n2 n2 Local OPT or Global OPT n1 e1 n1 e3 e2 (a) e1 e3 e2 (b) Figure 2: Local and global optimization of DRS trees. dat"
2021.acl-long.305,2020.acl-main.319,0,0.0381467,"Lin et al., 2019; Liu et al., 2019) have achieved certain success in sentencelevel DRS parsing. Nevertheless, due to the longdistance dependency over the discourse, text-level DRS parsing remains challenging. To alleviate this problem, Zhang et al. (2020) proposed a top-down architecture tailored for text-level DRS parsing. Kobayashi et al. (2020) used contextualized word representation and proposed to parse a document in three granularity levels for good performance. In the past decade, GANs have achieved great progress in NLP (Wu et al., 2019; Elazar and Goldberg, 2018; Chen and Chen, 2019; Zou et al., 2020). However, to our knowledge, there is still no research on adversarial learning in DRS parsing so far. In this work, we explore to adversarially train a discriminator to estimate the quality of the entire DRS tree for global optimization. Notably, we propose to transform each DRS tree into a continuous tree diagram, and thus our adversarial method does not suffer from the “discrete data” problem. 3 Related Work In the literature, previous studies on RST-style DRS parsing mainly consist of two categories, i.e., Baseline Top-Down Architecture In this section, we give a brief introduction to our"
2021.acl-long.73,W13-2322,0,0.107559,"ur knowledge, this is the first study that utilizes such a pre-training approach in cross-lingual AMR research. We also explore and compare four different finetuning methods to answer the question that whether combining AMR parsing and AMR-to-text generation tasks in fine-tuning stage will achieve better performance. Moreover, inspired by the teacherstudent mechanism (Kim and Rush, 2016; Chen et al., 2017), we extend the fine-tuning method to improve a target fine-tuning task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; X"
2021.acl-long.73,2020.emnlp-main.195,0,0.507377,"on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et al., 2014). Till recently, Damonte and Cohen (2018) demonstrate that AMR annotated for English can be used as cross-lingual semantic representations, and propose to conduct cross-lingual AMR parsing via annotation projection and machine translation. Blloshmi et al. (2020) follow the same line and create large-scale silver data to boost the performance of cross-lingual AMR parsing. Fan and Gardent (2020) focus on multilingual AMR-to-text generation for twenty one different languages. The aforementioned studies consider AMR parsing and AMR-to-text generation separately. In this paper, we formalize both AMR parsing and AMR-to-text generation as sequence-tosequence (seq2seq) learning and propose a novel and effective approach to cross-lingual AMR, which is illustrated in Figure 1. Upon the availability of the English AMR dataset and English-toX parallel datasets ("
2021.acl-long.73,D19-1393,0,0.0119068,"red by the teacherstudent mechanism (Kim and Rush, 2016; Chen et al., 2017), we extend the fine-tuning method to improve a target fine-tuning task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on gra"
2021.acl-long.73,2020.acl-main.119,0,0.0208192,"ouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into t"
2021.acl-long.73,2020.acl-main.640,0,0.0447769,"ouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into t"
2021.acl-long.73,P13-2131,0,0.268815,"m at every 10K (1K) steps. Finally, we obtain final pre-trained (finetuned) models by averaging the last 10 checkpoints. Evaluation. We evaluate on LDC2020T07 (Damonte and Cohen, 2018), a corpus containing human translations of the test portion of 1371 sentences from the AMR 2.0, in German, Spanish, Italian, and Chinese. This data is designed for use in cross-lingual AMR research. Following Fan and Gardent (2020), we only evaluate on languages of German, Spanish and Italian where we have training data from EUROPARL. For AMR parsing evaluation, we utilize Smatch and other fine-grained metrics (Cai and Knight, 2013; Damonte et al., 2017). For AMR-to-text generation, we report performance in BLEU (Papineni et al., 2002). 5.2 We compare the performance of our approach against two baseline systems. Baselinescratch . To build this baseline system, we directly train models from scratch on the finetuning datasets. Taking German AMR parsing as example, we trainthe model on its fine-tuning dataset F DE , F AMR to get Baselinescratch . Fine-Tuning Datasets. We use English AMR2.0 which contains 36,521, 1,368, and 1,371 EnglishAMR pairs for training, development, and testing, respectively. We translate the Englis"
2021.acl-long.73,2020.coling-main.218,0,0.0313791,"Missing"
2021.acl-long.73,N19-1223,0,0.0155329,"t al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models,"
2021.acl-long.73,P17-1176,0,0.136857,"ned while learning for English AMR parsing and text generation could be helpful to the X-language counterparts, and machine translation tasks could act as a good regularizer (Xu et al., 2020). To the best of our knowledge, this is the first study that utilizes such a pre-training approach in cross-lingual AMR research. We also explore and compare four different finetuning methods to answer the question that whether combining AMR parsing and AMR-to-text generation tasks in fine-tuning stage will achieve better performance. Moreover, inspired by the teacherstudent mechanism (Kim and Rush, 2016; Chen et al., 2017), we extend the fine-tuning method to improve a target fine-tuning task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte"
2021.acl-long.73,D16-1139,0,0.180998,"e that knowledge gained while learning for English AMR parsing and text generation could be helpful to the X-language counterparts, and machine translation tasks could act as a good regularizer (Xu et al., 2020). To the best of our knowledge, this is the first study that utilizes such a pre-training approach in cross-lingual AMR research. We also explore and compare four different finetuning methods to answer the question that whether combining AMR parsing and AMR-to-text generation tasks in fine-tuning stage will achieve better performance. Moreover, inspired by the teacherstudent mechanism (Kim and Rush, 2016; Chen et al., 2017), we extend the fine-tuning method to improve a target fine-tuning task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou e"
2021.acl-long.73,N18-1104,0,0.3748,"nverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et al., 2014). Till recently, Damonte and Cohen (2018) demonstrate that AMR annotated for English can be used as cross-lingual semantic representations, and propose to conduct cross-lingual AMR parsing via annotation projection and machine translation. Blloshmi et al. (2020) follow the same line and create large-scale silver data to boost the performance of cross-lingual AMR parsing. Fan and Gardent (2020) focus on multilingual AMR-to-text generation for twenty one different languages. The aforementioned studies consider AMR parsing and AMR-to-text generation separately. In this paper, we formalize both AMR parsing and AMR-to-text generation as s"
2021.acl-long.73,N19-1366,0,0.0176982,"(Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) . Cross-Lingual AMR. All above related studies focus on English AMR research. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether AMR can act as an interlingua is an open ques"
2021.acl-long.73,P17-4012,0,0.0108398,"entences into German, Spanish, and Italian, respectively. We segment all the tokens into subwords by using the BPE model trained on pre-training datasets. Baselinepre-trained . Rather than training models from scratch, we pre-train the models on largescale silver datasets. Taking German AMR parsing as example, we first pre-train the model on the pre DE AMR training dataset, i.e., T , T , then we finetune the pre-trained model on the corresponding  fine-tuning dataset, i.e., F DE , F AMR . Pre-Training and Fine-Tuning Model Settings. We implement above pre-trained models based on OpenNMT-py (Klein et al., 2017). 7 For simplicity, we use the same hyperparameter settings to train all the models in both pre-training and fine-tuning 5 https://www.statmt.org/wmt14/ translation-task.html 6 https://www.statmt.org/europarl/index. html 7 https://github.com/OpenNMT/OpenNMT-py Baseline Systems 5.3 Main Results Table 2 shows the performance of AMR parsing and AMR-to-text generation for German (DE), Spanish (ES), and Italian (IT). 901 AMR Parsing AMR-to-Text DE ES IT DE ES IT Baselinescratch 58.10 60.65 58.67 13.11 17.83 13.59 Baselinepre-trained 64.90 68.05 66.54 19.32 27.17 24.13 XLPT-AMRnone 48.97 59.52 58.13"
2021.acl-long.73,E17-1051,0,0.112665,", 2017), we extend the fine-tuning method to improve a target fine-tuning task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More"
2021.acl-long.73,2020.emnlp-main.231,0,0.432859,"chiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et al., 2014). Till recently, Damonte and Cohen (2018) demonstrate that AMR annotated for English can be used as cross-lingual semantic representations, and propose to conduct cross-lingual AMR parsing via annotation projection and machine translation. Blloshmi et al. (2020) follow the same line and create large-scale silver data to boost the performance of cross-lingual AMR parsing. Fan and Gardent (2020) focus on multilingual AMR-to-text generation for twenty one different languages. The aforementioned studies consider AMR parsing and AMR-to-text generation separately. In this paper, we formalize both AMR parsing and AMR-to-text generation as sequence-tosequence (seq2seq) learning and propose a novel and effective approach to cross-lingual AMR, which is illustrated in Figure 1. Upon the availability of the English AMR dataset and English-toX parallel datasets (X ∈ {German, Spanish, Italian} in this paper), our purpose is to boost the performance of zero-shot AMR parsing and text generation in"
2021.acl-long.73,W17-3501,0,0.0171375,"al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advant"
2021.acl-long.73,P17-1014,0,0.262024,"/xdqkid/XLPT-AMR. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a widely used formalism that represents the semantics of a sentence with a directed and acyclic graph. Figure 1 (b) shows an example AMR graph where the nodes such as ∗ Corresponding Author: Junhui Li. “doctor” and “give-01” represent concepts, and the edges such as “:ARG0” and “:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et a"
2021.acl-long.73,P18-1037,0,0.0142329,"on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs"
2021.acl-long.73,N16-1087,0,0.14842,"represents the semantics of a sentence with a directed and acyclic graph. Figure 1 (b) shows an example AMR graph where the nodes such as ∗ Corresponding Author: Junhui Li. “doctor” and “give-01” represent concepts, and the edges such as “:ARG0” and “:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et al., 2014). Till recently, Damonte and Cohen (2018) demonstrate that AMR annotated for English can be used as cross-lingual seman"
2021.acl-long.73,P14-1134,0,0.0923677,"ode available on github https:// github.com/xdqkid/XLPT-AMR. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a widely used formalism that represents the semantics of a sentence with a directed and acyclic graph. Figure 1 (b) shows an example AMR graph where the nodes such as ∗ Corresponding Author: Junhui Li. “doctor” and “give-01” represent concepts, and the edges such as “:ARG0” and “:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annota"
2021.acl-long.73,P18-1170,0,0.0393304,"Missing"
2021.acl-long.73,P02-1040,0,0.109557,"10 checkpoints. Evaluation. We evaluate on LDC2020T07 (Damonte and Cohen, 2018), a corpus containing human translations of the test portion of 1371 sentences from the AMR 2.0, in German, Spanish, Italian, and Chinese. This data is designed for use in cross-lingual AMR research. Following Fan and Gardent (2020), we only evaluate on languages of German, Spanish and Italian where we have training data from EUROPARL. For AMR parsing evaluation, we utilize Smatch and other fine-grained metrics (Cai and Knight, 2013; Damonte et al., 2017). For AMR-to-text generation, we report performance in BLEU (Papineni et al., 2002). 5.2 We compare the performance of our approach against two baseline systems. Baselinescratch . To build this baseline system, we directly train models from scratch on the finetuning datasets. Taking German AMR parsing as example, we trainthe model on its fine-tuning dataset F DE , F AMR to get Baselinescratch . Fine-Tuning Datasets. We use English AMR2.0 which contains 36,521, 1,368, and 1,371 EnglishAMR pairs for training, development, and testing, respectively. We translate the English sentences into German, Spanish, and Italian, respectively. We segment all the tokens into subwords by us"
2021.acl-long.73,D18-1198,0,0.0128812,"arget fine-tuning task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generat"
2021.acl-long.73,E17-1035,0,0.0184233,"hat translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira"
2021.acl-long.73,Q19-1019,0,0.0140853,"ong et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) . Cross-Lingual AMR. All above related studies focus on English AMR research. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether AMR can act as an interlingua is an open question (Xue et al.,"
2021.acl-long.73,W16-6603,0,0.0201628,"q2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al.,"
2021.acl-long.73,D19-1314,0,0.0124211,"More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) . Cross-Lingual AMR. All above related studies focus on English AMR research. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether AMR can act as an interlingua is an open question (Xue et al., 2014; Hajiˇ c et al.,"
2021.acl-long.73,P16-1162,0,0.00918509,"MT14 English-German translation dataset 5 which consists of 3.9M sentence pairs after preprocessing. For Spanish and Italian, we use Europarl parallel datasets,6 which consist of 1.9M English-Spanish and 1.9M English-Italian sentence pairs, respectively. The English sentences of all the datasets are all parsed into AMR graphs via an English AMR parser trained on AMR 2.0 (LDC2017T10) (Appendix A provides more details on the English AMR parser). We merge English, German (Spanish/Italian) sentences and linearized AMRs together and segment all the tokens into subwords by byte pair encoding (BPE) (Sennrich et al., 2016) with 40K (or 30K for both Spanish and Italian) operations. In addition, we also train NMT models to translate English into German, Spanish, and Italian on above parallel datasets with Transformer-big settings (Vaswani et al., 2017). These NMT models will be used in preparing fine-tuning datasets (Appendix B provides more implementation details on the NMT models). by just following the settings for the Transformerbase model in Vaswani et al. (2017). The number of layers in encoder and decoder is 6 while the number of heads is 8. Both the embedding size and the hidden state size are 512 while t"
2021.acl-long.73,2021.eacl-main.30,0,0.39898,"rch. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether AMR can act as an interlingua is an open question (Xue et al., 2014; Hajiˇ c et al., 2014). Till lately , Damonte and Cohen (2018) demonstrate that a simplified AMR can be used across languages and for the first time they study crosslingual AMR parsing for languages rather than English. Blloshmi et al. (2020) employ large-scale silver parallel AMR data to bridge the gap between different languages and greatly advance the performance of cross-lingual AMR parsing. Sheth et al. (2021) explore annotation projection to leverage existing English AMR and overcome resource shortage in the target language. Furthermore, Fan and Gardent (2020) explore cross-lingual AMR-to-text based on pre-trained cross-lingual language model (XLM) (Lample and Conneau, 2019). In this paper we build strong cross-lingual pre-trained models for both AMR parsing and AMR-to-text generation. Moreover, a nice property of our approach is that for AMR parsing, unlike related studies (Damonte and Cohen, 2018; Blloshmi et al., 2020), we do not need to perform lemmatization, POS tagging, NER, or re-categoriza"
2021.acl-long.73,D19-6313,0,0.017455,"“:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et al., 2014). Till recently, Damonte and Cohen (2018) demonstrate that AMR annotated for English can be used as cross-lingual semantic representations, and propose to conduct cross-lingual AMR parsing via annotation projection and machine translation. Blloshmi et al. (2020) follow the same line and create large-scale silver data to boost the performance of cross-lingual AMR parsi"
2021.acl-long.73,P17-2002,0,0.0896182,"cs of a sentence with a directed and acyclic graph. Figure 1 (b) shows an example AMR graph where the nodes such as ∗ Corresponding Author: Junhui Li. “doctor” and “give-01” represent concepts, and the edges such as “:ARG0” and “:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et al., 2014). Till recently, Damonte and Cohen (2018) demonstrate that AMR annotated for English can be used as cross-lingual semantic representations"
2021.acl-long.73,2020.acl-main.712,0,0.023251,"lation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) . Cross-Lingual AMR. All above related studies focus on English AMR research. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether AMR can act as an interlingua is an open question (Xue et al., 2014; Hajiˇ c et al., 2014). Till lately , Damonte and Cohen (2018) demonstrate that a simplified"
2021.acl-long.73,P18-1150,0,0.0149225,"task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) . Cross-Lingual AMR. All above related studies focus on English AMR research. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether A"
2021.acl-long.73,2020.findings-emnlp.199,0,0.0227891,"sly. The results also show that our XLPT-AMRT-S models greatly advance the state of art. For example, our XLPT-AMRT-S models outperform Sheth et al. (2021) by 3.4∼7.8 Smatch F1 on AMR parsing of the three languages while surpass Fan and Gardent (2020) by around 10 BLEU on AMR-to-text generation. 902 Table 3 compares the performance of finegrained metrics for AMR parsing. It shows that our XLPT-AMRT-S models achieve the best performance on all the metrics with the only exception of Concepts for Italian AMR parsing. It shows that like English AMR parsing, all models predict Reentrancies poorly (Szubert et al., 2020). It also demonstrates that Negations is another metric which is hard to predict. In future work, we will pay particular attention to the two metrics. Metric Smatch Unlabeled No WSD Concepts Named Ent. Negations Wikification Reentrancies SRL Blloshmi et al. (2020) DE ES IT 53.0 58.0 58.1 57.7 63.0 63.4 53.2 58.4 58.4 58.0 65.9 64.7 66.0 65.9 64.7 11.7 23.4 29.2 60.9 63.1 67.0 39.9 46.6 46.1 47.9 55.2 54.7 Baselinepre-trained DE ES IT 64.90 68.05 66.54 69.53 72.49 71.16 65.16 68.40 66.78 68.79 73.06 78.21 79.12 81.34 68.42 42.69 51.93 48.57 67.40 69.40 71.05 42.40 46.20 44.10 60.50 65.20 63.80"
2021.acl-long.73,N18-2040,0,0.507917,"the edges such as “:ARG0” and “:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et al., 2014; Hajiˇ c et al., 2014). Till recently, Damonte and Cohen (2018) demonstrate that AMR annotated for English can be used as cross-lingual semantic representations, and propose to conduct cross-lingual AMR parsing via annotation projection and machine translation. Blloshmi et al. (2020) follow the same line and create large-scale silver data to boost the perf"
2021.acl-long.73,P15-2141,0,0.113751,"https:// github.com/xdqkid/XLPT-AMR. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a widely used formalism that represents the semantics of a sentence with a directed and acyclic graph. Figure 1 (b) shows an example AMR graph where the nodes such as ∗ Corresponding Author: Junhui Li. “doctor” and “give-01” represent concepts, and the edges such as “:ARG0” and “:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et"
2021.acl-long.73,N15-1040,0,0.48219,"https:// github.com/xdqkid/XLPT-AMR. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a widely used formalism that represents the semantics of a sentence with a directed and acyclic graph. Figure 1 (b) shows an example AMR graph where the nodes such as ∗ Corresponding Author: Junhui Li. “doctor” and “give-01” represent concepts, and the edges such as “:ARG0” and “:ARG1” stand for semantic relations between two connected concepts. Recent studies on AMR mainly fall in two directions: AMR parsing which converts a sentence into an AMR graph (Flanigan et al., 2014; Wang et al., 2015a; Konstas et al., 2017, to name a few) and its inverse, i.e., AMR-to-text generation that produces a sentence from an AMR graph (Flanigan et al., 2016; Song et al., 2017, 2018, to name a few). Restricted by the availability of annotated corpora, most of previous studies on AMR focus on English while very few studies are for Chinese and Portuguese (Wang et al., 2018; Sobrevilla Cabezudo et al., 2019; Anchiˆeta and Pardo, 2020). Cross-lingual AMR research, however, has received relatively less attention. In fact, crosslingual AMR has mainly been studied in the scope of annotation works (Xue et"
2021.acl-long.73,P15-1095,0,0.0555699,"Missing"
2021.acl-long.73,2020.emnlp-main.196,1,0.922309,"-01 she much medication :ARG1 :quant :ARG1 :ARG2 doctor • We evaluate our approach in three zero-shot languages of AMR and our approach greatly advances the state of the art. make-02 2 good-02 :degree Related Work We describe related studies on AMR from three perspectives: English AMR parsing, English AMRto-text generation, and cross-lingual AMR. more Figure 1: Illustration of cross-lingual AMR parsing and AMR-to-text generation: (a) sentences in different languages sharing the same meaning; (b) AMR graph of the sentences. X-language. To this end, we borrow the idea of joint pre-training from Xu et al. (2020) and explore three types of relevant tasks, including machine translation tasks, AMR parsing and AMR-to-text generation tasks. We conjecture that knowledge gained while learning for English AMR parsing and text generation could be helpful to the X-language counterparts, and machine translation tasks could act as a good regularizer (Xu et al., 2020). To the best of our knowledge, this is the first study that utilizes such a pre-training approach in cross-lingual AMR research. We also explore and compare four different finetuning methods to answer the question that whether combining AMR parsing"
2021.acl-long.73,P19-1009,0,0.0288182,"Missing"
2021.acl-long.73,D19-1392,0,0.0281734,"Missing"
2021.acl-long.73,2020.acl-main.67,0,0.024457,"as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) . Cross-Lingual AMR. All above related studies focus on English AMR research. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether AMR can act as an interlingua is an open question (Xue et al., 2014; Hajiˇ c et al., 2014). Till lately , Damonte and Cohen (2018) demonstrate"
2021.acl-long.73,2021.naacl-main.443,0,0.0233173,"task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; Song et al., 2017). More recent studies propose to regard AMR-totext generation as a machine tra"
2021.acl-long.73,D16-1065,0,0.360797,", 2016; Chen et al., 2017), we extend the fine-tuning method to improve a target fine-tuning task with the help English AMR Parsing. AMR parsing is a task that translates a sentence into a directed and acyclic graph (Banarescu et al., 2013). According to the approaches to modeling the structure in AMR graphs, previous studies on AMR Parsing for English can be broadly grouped into several categories, which are tree-based approaches (Wang et al., 2015b; Groschwitz et al., 2018), graph-based approaches (Flanigan et al., 2014; Werling et al., 2015; Cai and Lam, 2019), transition-based approaches (Zhou et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Guo and Lu, 2018; Zhou et al., 2021), sequence-to-sequence (seq2seq) approaches (Peng et al., 2017; van Noord and Bos, 2017; Konstas et al., 2017; Ge et al., 2019; Xu et al., 2020; Bevilacqua et al., 2021), and sequence-to-graph (seq2graph) approaches (Lyu and Titov, 2018; Zhang et al., 2019a,b; Cai and Lam, 2020a). 897 English AMR-to-Text Generation. As an inverse task of AMR parsing, AMR-to-text generation aims to write a sentence from an AMR graph. Early studies on this task rely on grammar-based approaches (Flanigan et al., 2016; So"
2021.acl-long.73,D19-1548,1,0.884741,"ropose to regard AMR-totext generation as a machine translation or seq2seq task (Pourdamghani et al., 2016; Ferreira et al., 2017; Konstas et al., 2017; Cao and Clark, 2019). However, seq2seq approaches tend to lose structural information in AMR graphs since they simply linearize AMR graphs into sequences before feeding them into the models. To prevent information loss caused by linearization, a variety of graph-tosequence approaches have been proposed to better model structural information (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020b; Zhao et al., 2020; Song et al., 2020; Yao et al., 2020; Bai et al., 2020). By taking advantages of strong pre-trained language models, recent studies achieve new state of the art (Mager et al., 2020; Harkous et al., 2020; Ribeiro et al., 2020; Bevilacqua et al., 2021) . Cross-Lingual AMR. All above related studies focus on English AMR research. Relatively limited efforts have been put on other languages due to the lack of language-specific AMR corpora. Actually, whether AMR can act as an interlingua is an open question (Xue et al., 2014; Hajiˇ c et al., 2014). Till lately"
2021.acl-short.70,2020.emnlp-main.457,0,0.023036,"ng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, which can deal with multi-modal scenarios and offers an alternative solution to robustly enhan"
2021.acl-short.70,D18-1529,0,0.0242889,"Missing"
2021.acl-short.70,I13-1181,0,0.0120815,"xperimental results demonstrate that our approach performs signiﬁcantly better than the single-modal state-of-the-art and the multi-modal approaches with early fused features of CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies l"
2021.acl-short.70,2020.coling-main.183,0,0.021417,"Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, which can deal with multi-modal scenarios and offers an alternative solution to robustly enhancing neural CWS models. 3 Data Collection and Annotation We collect the multi-modal data for CWS from a Chinese news reporting platform “Xuexi”2 . We mainly focus on the audios equipped with machine transcription text. In total, we crawl 120 short videos and segment them into about 2000 sentences. To avoid the contextual inﬂuence and augment the robust of designed computing model, we randomly 2 https://www.xuexi.cn/ Size 250 50.5"
2021.acl-short.70,P14-1028,0,0.0182098,"CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020"
2021.acl-short.70,N19-1276,0,0.01271,"r CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, which can deal with multi-modal scenarios and offers an alternative solution to robustly enhancing neural CWS models. 3 Data Collection and Annotation We collect the multi-modal data for CWS from a Chinese news reporting platform “Xuexi”2 . We mainly focus on the audios equipped with machine transcription text. In total, we crawl 120 short videos and segment them into about 2000 sentences. To avoid the contextual inﬂuence and augment the robust of designed computing model, we randomly 2 https://www.xue"
2021.acl-short.70,C04-1081,0,0.235078,"aracters Total Words Total Time(s) Finally, we leverage the CRF to perform sequence labeling on the basis of the above character representation. We evaluate our approach on the newly annotated small-scale dataset with different size of training sets. The experimental results demonstrate that our approach performs signiﬁcantly better than the single-modal state-of-the-art and the multi-modal approaches with early fused features of CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020;"
2021.acl-short.70,2020.findings-emnlp.260,0,0.0593949,"Missing"
2021.acl-short.70,D11-1090,0,0.0337871,"aining sets. The experimental results demonstrate that our approach performs signiﬁcantly better than the single-modal state-of-the-art and the multi-modal approaches with early fused features of CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models,"
2021.acl-short.70,2020.acl-main.735,0,0.048718,"Missing"
2021.acl-short.70,2020.acl-main.734,0,0.0661243,"ard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, which can deal with multi-modal scenarios and offers an alternative solution to robustly enhancing neural CWS models. 3 Data Collect"
2021.acl-short.70,P19-1656,0,0.0280817,"timedependent uni-modal interaction, time-dependent multi-modal interaction and CRF labeling. Figure 2 shows the overall architecture of our TMIN. 4.1 Time-dependent Uni-modal Interaction To better capture the temporal correspondences between different modalities (Zhang et al., 2019; Ju et al., 2020), we ﬁrst align two modalities by extracting the exact time stamp of each phoneme and character using Montreal Forced Aligner (McAuliffe et al., 2017). For machines to understand human utterance, they must be ﬁrst able to understand the intramodal dynamics (Zadeh et al., 2018; Wang et al., 2019b; Tsai et al., 2019) in each modality, such as the word order and grammar in text, breathe and 551 Multi-attention Gating B h I  αK ĊĊ CRF Layer  α2 CRF Labeling  α1 ^ha O ĊĊ Multi-modal Hybrid Representation h1 ^hx LSTHMG Gating LSTM h1a h2 Gating LSTM h1 x ĊĊ Gating LSTM LSTM LSTM hn ĊĊ Time-depedent Multi-modal Interaction LSTM ĊĊ Transformerbased Unit Transformerbased Unit Transformerbased Unit Transformerbased Unit ᗵ 享 i=1 Transformerbased Unit ĊĊ ĊĊ Transformerbased Unit Time-dependent Uni-modal Text Interaction ભ ĊĊ i=2 Audio i=n ĊĊ Figure 2: The overview of our proposed TMIN. tone in audio. Textual Mod"
2021.acl-short.70,I17-1017,0,0.0125628,"ture engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, which can deal with multi-modal scenarios and offers an alternative solution to robustly enhancing neural CWS models. 3 Data Collection and Annotation We collect the multi-modal data for CWS from a Chinese news reporting platform “Xuexi”2 . We mainly focus on the audios equipped with machine transcription text. In total, we crawl 120 short videos and segment them into about 2000 sentences. To avoid the contextual inﬂuence and augment the robust of designed computing model, we"
2021.acl-short.70,O03-4002,0,0.261908,"Sentences Avg. Length (Character) Avg. Length (Word) Avg. Length (Time)(s) Max Length (Character) Max Length (Word) Max Length (Time)(s) Total Characters Total Words Total Time(s) Finally, we leverage the CRF to perform sequence labeling on the basis of the above character representation. We evaluate our approach on the newly annotated small-scale dataset with different size of training sets. The experimental results demonstrate that our approach performs signiﬁcantly better than the single-modal state-of-the-art and the multi-modal approaches with early fused features of CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al.,"
2021.acl-short.70,P17-1078,0,0.0125572,"k, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with multi-modality, whi"
2021.acl-short.70,2020.emnlp-main.291,1,0.714341,"be represented as: X = (x1 , x2 , · · · , xn ) ∈ Rn×d1 . Acoustic Modality. We use a famous audio processing tool, i.e., OpenSMILE (Eyben et al., 2010), to extract the MFCC, LP-coefﬁcients, pure FFT spectrum, etc. from dual channels (Jayram et al., 2002; Sakran et al., 2017), and leverage multiple Transformer layers (Vaswani et al., 2017) to perform intra-modal interactions. Then, each character-level audio feature can be represented as: A = (a1 , a2 , · · · , an ) ∈ Rn×d2 . 4.2 Time-dependent Multi-modal Interaction To better capture the cross-modal semantic correspondences (Wu et al., 2020; Zhang et al., 2020), we design a long- and short-term hybrid memory gating (LSTHMG) block, which is a extension of standard LSTM. We ﬁrst obtain the current memory of each character-level representation for both modalities. ˆ x , cx = LSTMx (xi , hx , cx ) h i i i i−1 i−1 a a a ˆ , c = LSTM (ai , ha , ca ) h i i i i−1 i−1 (1) (2) where LSTM denotes the standard LSTM (Graves et al., 2013). After current updating, we employ multiattention to control the different contributions of each hidden state. ˆ i + MA(h ˆ x, h ˆ a) hi = h i i ˆi + = h L  l=0 (softmax( (3) Ql (K l ) √ d )V l ) (4) where MA denotes the multi"
2021.acl-short.70,D13-1031,0,0.0153164,"emonstrate that our approach performs signiﬁcantly better than the single-modal state-of-the-art and the multi-modal approaches with early fused features of CWS. 2 Related Work Xu (2003) ﬁrst formalize CWS as a sequence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external info"
2021.acl-short.70,D13-1061,0,0.0394832,"Missing"
2021.acl-short.70,D17-1079,0,0.0120733,"quence labeling task, considering CWS as a supervised learning from annotated corpus with human segmentation. Peng et al. (2004) further adopt standard sequence labeling tool CRFs for CWS modeling, achieving a best performance in their same period. Then, a large amount of approaches based on above settings are proposed for CWS (Li and Sun, 2009; Sun and Xu, 2011; Mansur et al., 2013; Zhang et al., 2013). Recently, deep neural approaches have been widely proposed to minimize the efforts in feature engineering for CWS (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Zhou et al., 2017; Yang et al., 2017; Zhang et al., 2017; Ma et al., 2018; Li et al., 2019; Wang et al., 2019a; Fu et al., 2020; Ding et al., 2020; Tian et al., 2020a; Zhao et al., 2020). Among these studies, most of them follow the character-based paradigm to predict segmentation labels for each character in an input sentence. To enhance CWS with neural models, there were studies leverage external information, such as vocabularies from auto-segmented external corpus and weakly labeled data (Wang and Xu, 2017; Higashiyama et al., 2019; Gong et al., 2020). To our best knowledge, we are ﬁrst to perform CWS with"
2021.acl-short.70,P19-2029,0,0.0283995,"Missing"
2021.alvr-1.1,W14-3348,0,0.0172384,"on the ambiguous COCO which contains 461 examples (Elliott et al., 2017). Due to the inclusion of ambiguous verbs, the examples in ambiguous COCO can be used for the evaluation of visual sense disambiguation in a MNMT scenario. 6.2 6.3 Comparison to the Baseline We carry out 5 independent experiments (5 runs) for each of the proposed MNMT variants. In each run, any of the variants is retrained and redeveloped under cold-start conditions using a set of randomlyselected seeds by MultEval4 . Eventually, the resultant models are evaluated on the test set with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). For each variant, we report not only the comprehensive performance (denoted as ensemble) which is obtained using ensemble learning (Garmash and Monz, 2016) but that without ensemble learning. In the latter case, the average performance (µ) and deviations (σ) in the 5 runs are reported. Training and Hyperparameter Settings For preprocessing, we apply Byte-Pair Encoding (BPE) (Sennrich et al., 2015) for tokenizing all the captions and translations in Multi30k and COCO, and use the open-source toolkit3 of Moses (Koehn et al., 2007) for lowercasing and punctuation n"
2021.alvr-1.1,P15-1166,0,0.0279985,"007 ±0.001 Table 5: Test results in Elliott’s utility test. RELATED WORK We have mentioned the previous work of MNMT in section 1, where the research interest has been classified into image encoding, encoder-decoder NMT construction and cross-modality learning. Besides, we present the methods of Caglayan et al. (2017a) and Calixto et al. (2017a) in the section 4.4.2, along with the systematic analysis. Besides, many scholars within the research community have made great efforts upon the development of sophisticated NMT architectures, including multi-source (Zoph and Knight, 2016), multi-task (Dong et al., 2015) and multi-way (Firat et al., 2016) NMT, as well as those equipped with attention mechanisms (Sennrich et al., 2015). The research activities are particularly crucial since they broaden the range of cross-modality learning strategies. Current research interest has concentrated on the incorporation of visual features into NMT (Lala et al., 2018), by means of visual-linguistic context vector concatenation (Libovick`y et al., 2016), doubly-attentive decoding (Calixto et al., 2017a), hierarchical attention combination (Libovick`y and Helcl, 2017), cross-attention network (Helcl et al., 2018), gate"
2021.alvr-1.1,D18-1329,0,0.0328594,"Missing"
2021.alvr-1.1,W17-4718,0,0.0160185,"e they contain not only English (En) image captions but their translations in German (De), French (Fr) and Czech (Cz). Hereinafter, we specify an example in Multi30k as an image which is accompanied by three En→De, En→Fr and En→Cz caption-translation pairs. Each of Multi30k-16 and Multi30k-17 contains about 31K examples. We experiment on the corpora separately, and as usual divide each of them into training, validation and test sets, at the scale of 29K, 1,014 and 1K examples, respectively. In addition, we carry out a complementary experiment on the ambiguous COCO which contains 461 examples (Elliott et al., 2017). Due to the inclusion of ambiguous verbs, the examples in ambiguous COCO can be used for the evaluation of visual sense disambiguation in a MNMT scenario. 6.2 6.3 Comparison to the Baseline We carry out 5 independent experiments (5 runs) for each of the proposed MNMT variants. In each run, any of the variants is retrained and redeveloped under cold-start conditions using a set of randomlyselected seeds by MultEval4 . Eventually, the resultant models are evaluated on the test set with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). For each varia"
2021.alvr-1.1,W16-3210,0,0.0365524,"Missing"
2021.alvr-1.1,W17-4746,0,0.0390873,"Missing"
2021.alvr-1.1,N16-1101,0,0.0266909,"n Elliott’s utility test. RELATED WORK We have mentioned the previous work of MNMT in section 1, where the research interest has been classified into image encoding, encoder-decoder NMT construction and cross-modality learning. Besides, we present the methods of Caglayan et al. (2017a) and Calixto et al. (2017a) in the section 4.4.2, along with the systematic analysis. Besides, many scholars within the research community have made great efforts upon the development of sophisticated NMT architectures, including multi-source (Zoph and Knight, 2016), multi-task (Dong et al., 2015) and multi-way (Firat et al., 2016) NMT, as well as those equipped with attention mechanisms (Sennrich et al., 2015). The research activities are particularly crucial since they broaden the range of cross-modality learning strategies. Current research interest has concentrated on the incorporation of visual features into NMT (Lala et al., 2018), by means of visual-linguistic context vector concatenation (Libovick`y et al., 2016), doubly-attentive decoding (Calixto et al., 2017a), hierarchical attention combination (Libovick`y and Helcl, 2017), cross-attention network (Helcl et al., 2018), gated attention network (Zhang et al.,"
2021.alvr-1.1,C16-1133,0,0.0226747,"se disambiguation in a MNMT scenario. 6.2 6.3 Comparison to the Baseline We carry out 5 independent experiments (5 runs) for each of the proposed MNMT variants. In each run, any of the variants is retrained and redeveloped under cold-start conditions using a set of randomlyselected seeds by MultEval4 . Eventually, the resultant models are evaluated on the test set with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). For each variant, we report not only the comprehensive performance (denoted as ensemble) which is obtained using ensemble learning (Garmash and Monz, 2016) but that without ensemble learning. In the latter case, the average performance (µ) and deviations (σ) in the 5 runs are reported. Training and Hyperparameter Settings For preprocessing, we apply Byte-Pair Encoding (BPE) (Sennrich et al., 2015) for tokenizing all the captions and translations in Multi30k and COCO, and use the open-source toolkit3 of Moses (Koehn et al., 2007) for lowercasing and punctuation normalization. It reproduces the neural network architecture of Anderson et al (Anderson et al., 2018)’s top-down attentive CAP. The only difference is that it merely utilizes ResNet-101 i"
2021.alvr-1.1,W16-2358,0,0.0346661,"Missing"
2021.alvr-1.1,P17-1175,0,0.260886,") which serves both to encode a sourcelanguage caption and to generate the targetlanguage caption by decoding, where the latent information that flows through the network is referred to linguistic feature. • Multimodal learning network which uses visual features to enhance the encoding of linguistic semantics (Ngiam et al., 2011). Besides of the concatenation and combination of linguistic and visual features, vision-to-language attention mechanisms serve as the essential operations for cross-modality learning. Nowadays, they are implemented with single-layer attentive (Caglayan et al., 2017a; Calixto et al., 2017b), doubly-attentive (Calixto et al., 2017a), interpolated (Hitschler et al., 2016) and multi-task (Zhou et al., 2018) neural networks, respectively. Introduction Caption translation is required to translate a sourcelanguage caption into target-language, where a caption refers to the sentence-level text annotation of an image. As defined in the shared multimodal translation task1 in WMT, caption translation can be conducted over both visual features in images and linguistic features of the accompanying captions. The question of how to opportunely utilize images for caption translation motivate"
2021.alvr-1.1,W18-6441,0,0.0299803,"Missing"
2021.alvr-1.1,D17-1105,0,0.244486,") which serves both to encode a sourcelanguage caption and to generate the targetlanguage caption by decoding, where the latent information that flows through the network is referred to linguistic feature. • Multimodal learning network which uses visual features to enhance the encoding of linguistic semantics (Ngiam et al., 2011). Besides of the concatenation and combination of linguistic and visual features, vision-to-language attention mechanisms serve as the essential operations for cross-modality learning. Nowadays, they are implemented with single-layer attentive (Caglayan et al., 2017a; Calixto et al., 2017b), doubly-attentive (Calixto et al., 2017a), interpolated (Hitschler et al., 2016) and multi-task (Zhou et al., 2018) neural networks, respectively. Introduction Caption translation is required to translate a sourcelanguage caption into target-language, where a caption refers to the sentence-level text annotation of an image. As defined in the shared multimodal translation task1 in WMT, caption translation can be conducted over both visual features in images and linguistic features of the accompanying captions. The question of how to opportunely utilize images for caption translation motivate"
2021.alvr-1.1,P16-1227,0,0.0478942,"tlanguage caption by decoding, where the latent information that flows through the network is referred to linguistic feature. • Multimodal learning network which uses visual features to enhance the encoding of linguistic semantics (Ngiam et al., 2011). Besides of the concatenation and combination of linguistic and visual features, vision-to-language attention mechanisms serve as the essential operations for cross-modality learning. Nowadays, they are implemented with single-layer attentive (Caglayan et al., 2017a; Calixto et al., 2017b), doubly-attentive (Calixto et al., 2017a), interpolated (Hitschler et al., 2016) and multi-task (Zhou et al., 2018) neural networks, respectively. Introduction Caption translation is required to translate a sourcelanguage caption into target-language, where a caption refers to the sentence-level text annotation of an image. As defined in the shared multimodal translation task1 in WMT, caption translation can be conducted over both visual features in images and linguistic features of the accompanying captions. The question of how to opportunely utilize images for caption translation motivates the study of multimodality, including not only the extraction of visual features"
2021.alvr-1.1,D18-1340,0,0.0491066,"Missing"
2021.alvr-1.1,W16-2360,0,0.0489366,"Missing"
2021.alvr-1.1,P19-1653,0,0.0317986,"Missing"
2021.alvr-1.1,P11-2031,0,0.0483506,"Missing"
2021.alvr-1.1,D13-1176,0,0.0599399,"by the attention mechanism over the global encoder hidden states S: ct = αt S, where αt denotes the attention weight at the t-th time step. Eventually, the prediction of each target-language word is carried out as follows (where, Wh , Wc , Wy , bo and by are trainable parameters): with a BiGRU encoder and a Conditional GRU (CGRU) decoder (Firat and Cho, 2016; Caglayan et al., 2017a). Attention mechanism is used between BiGRU and CGRU. The diagram at the right side of Figure 3 shows the baseline framework. For a source-language caption, we represent it with a sequence of randomly-initialized (Kalchbrenner and Blunsom, 2013) word embeddings X=(x1 , ..., xN ), where each xt is uniformly specified as a k-dimensional word embedding. Conditioned on the embeddings, Chung et al (2014)’s BiGRU is used to compute the bidirectional hidden states S=(s1 , ..., sN ), where each st is obtained by combining the t-th hidden state of forward −−−→ GRU and that of backward GRU: st =[GRU e (xt ), ←−−−e GRU (xt )]. Padding (Libovick`y and Helcl, 2018) and dynamic stabilization (Ba et al., 2016) are used. Firat and Cho (2016)’s CGRU is utilized for decoding, which comprises two forward GRU units, −−−→ −−−→ −−−→ i.e., GRU d1 and GRU d"
2021.alvr-1.1,2006.amta-papers.25,0,0.0547957,"461 examples (Elliott et al., 2017). Due to the inclusion of ambiguous verbs, the examples in ambiguous COCO can be used for the evaluation of visual sense disambiguation in a MNMT scenario. 6.2 6.3 Comparison to the Baseline We carry out 5 independent experiments (5 runs) for each of the proposed MNMT variants. In each run, any of the variants is retrained and redeveloped under cold-start conditions using a set of randomlyselected seeds by MultEval4 . Eventually, the resultant models are evaluated on the test set with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). For each variant, we report not only the comprehensive performance (denoted as ensemble) which is obtained using ensemble learning (Garmash and Monz, 2016) but that without ensemble learning. In the latter case, the average performance (µ) and deviations (σ) in the 5 runs are reported. Training and Hyperparameter Settings For preprocessing, we apply Byte-Pair Encoding (BPE) (Sennrich et al., 2015) for tokenizing all the captions and translations in Multi30k and COCO, and use the open-source toolkit3 of Moses (Koehn et al., 2007) for lowercasing and punctuation normalization. It reproduces th"
2021.alvr-1.1,P07-2045,0,0.00752123,"eni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). For each variant, we report not only the comprehensive performance (denoted as ensemble) which is obtained using ensemble learning (Garmash and Monz, 2016) but that without ensemble learning. In the latter case, the average performance (µ) and deviations (σ) in the 5 runs are reported. Training and Hyperparameter Settings For preprocessing, we apply Byte-Pair Encoding (BPE) (Sennrich et al., 2015) for tokenizing all the captions and translations in Multi30k and COCO, and use the open-source toolkit3 of Moses (Koehn et al., 2007) for lowercasing and punctuation normalization. It reproduces the neural network architecture of Anderson et al (Anderson et al., 2018)’s top-down attentive CAP. The only difference is that it merely utilizes ResNet-101 in generating the input set of visual features V , without the use of Faster R-CNN (Ren et al., 2015). This CAP has 6.3.1 Performance on Multi30k Tables 1 and 2 respectively show the performance of our models on Multi30k-16 and Multi30k-17 for the translation scenarios of En→De, En→Fr and En→Cz. Each of our MNMT models in the tables is denoted with a symbol &quot;+&quot;, which indicates"
2021.alvr-1.1,W16-2346,0,0.0444291,"Missing"
2021.alvr-1.1,W18-6442,0,0.0142515,") in the section 4.4.2, along with the systematic analysis. Besides, many scholars within the research community have made great efforts upon the development of sophisticated NMT architectures, including multi-source (Zoph and Knight, 2016), multi-task (Dong et al., 2015) and multi-way (Firat et al., 2016) NMT, as well as those equipped with attention mechanisms (Sennrich et al., 2015). The research activities are particularly crucial since they broaden the range of cross-modality learning strategies. Current research interest has concentrated on the incorporation of visual features into NMT (Lala et al., 2018), by means of visual-linguistic context vector concatenation (Libovick`y et al., 2016), doubly-attentive decoding (Calixto et al., 2017a), hierarchical attention combination (Libovick`y and Helcl, 2017), cross-attention network (Helcl et al., 2018), gated attention network (Zhang et al., 2019), joint (Zhou et al., 2018) and ensemble (Zheng et al., 2018) learning . In addition, image attention optimization (Delbrouck and Dupont, 2017) and monolingual data expansion (Hitschler et al., 2016) have been proven effective in this field. Ive et al. (2019) use an off-shelf object detector and an additi"
2021.alvr-1.1,P17-2031,0,0.0331937,"Missing"
2021.alvr-1.1,Q14-1006,0,0.0418453,"es and output states with the probabilities of (0.3, 0.5, 0.5) for En→De MT, (0.2, 0.4, 0.4) for En→Fr and (0.1, 0.3, 0.3) for En→Cz. In order to avoid overfitting, we apply a L2 regularization term with a factor of 1e-5. We specify the dimension as 128 for all token embeddings (k = 128) and 256 for hidden states. Experimentation Resource and Experimental Datasets We perform experiments on Multi30k-16 and Multi30k-172 , which are provided by WMT for the shared tasks of multilingual captioning and multimodal MT (Elliott et al., 2016). The corpora are used as the extended versions of Flichr30k (Young et al., 2014), since they contain not only English (En) image captions but their translations in German (De), French (Fr) and Czech (Cz). Hereinafter, we specify an example in Multi30k as an image which is accompanied by three En→De, En→Fr and En→Cz caption-translation pairs. Each of Multi30k-16 and Multi30k-17 contains about 31K examples. We experiment on the corpora separately, and as usual divide each of them into training, validation and test sets, at the scale of 29K, 1,014 and 1K examples, respectively. In addition, we carry out a complementary experiment on the ambiguous COCO which contains 461 exam"
2021.alvr-1.1,W16-2361,0,0.0428808,"Missing"
2021.alvr-1.1,W18-6443,0,0.0155363,"mechanisms (Sennrich et al., 2015). The research activities are particularly crucial since they broaden the range of cross-modality learning strategies. Current research interest has concentrated on the incorporation of visual features into NMT (Lala et al., 2018), by means of visual-linguistic context vector concatenation (Libovick`y et al., 2016), doubly-attentive decoding (Calixto et al., 2017a), hierarchical attention combination (Libovick`y and Helcl, 2017), cross-attention network (Helcl et al., 2018), gated attention network (Zhang et al., 2019), joint (Zhou et al., 2018) and ensemble (Zheng et al., 2018) learning . In addition, image attention optimization (Delbrouck and Dupont, 2017) and monolingual data expansion (Hitschler et al., 2016) have been proven effective in this field. Ive et al. (2019) use an off-shelf object detector and an additional image dataset (Kuznetsova et al., 2018) to form a bag of category-level object embeddings. Conditioned on the embeddings, Ive et al. (2019) develop a sophisticated MNMT model which integrates selfattention and cross-attention mechanisms into the encoder-decoder based deliberation architecture. This paper also touches on the research area of image c"
2021.alvr-1.1,D18-1400,0,0.0668645,"latent information that flows through the network is referred to linguistic feature. • Multimodal learning network which uses visual features to enhance the encoding of linguistic semantics (Ngiam et al., 2011). Besides of the concatenation and combination of linguistic and visual features, vision-to-language attention mechanisms serve as the essential operations for cross-modality learning. Nowadays, they are implemented with single-layer attentive (Caglayan et al., 2017a; Calixto et al., 2017b), doubly-attentive (Calixto et al., 2017a), interpolated (Hitschler et al., 2016) and multi-task (Zhou et al., 2018) neural networks, respectively. Introduction Caption translation is required to translate a sourcelanguage caption into target-language, where a caption refers to the sentence-level text annotation of an image. As defined in the shared multimodal translation task1 in WMT, caption translation can be conducted over both visual features in images and linguistic features of the accompanying captions. The question of how to opportunely utilize images for caption translation motivates the study of multimodality, including not only the extraction of visual features but the cooperation between visual"
2021.alvr-1.1,N16-1004,0,0.0193207,"1±0.1 -0.002 ±0.001 34.6 33.8±0.1 0.007 ±0.001 Table 5: Test results in Elliott’s utility test. RELATED WORK We have mentioned the previous work of MNMT in section 1, where the research interest has been classified into image encoding, encoder-decoder NMT construction and cross-modality learning. Besides, we present the methods of Caglayan et al. (2017a) and Calixto et al. (2017a) in the section 4.4.2, along with the systematic analysis. Besides, many scholars within the research community have made great efforts upon the development of sophisticated NMT architectures, including multi-source (Zoph and Knight, 2016), multi-task (Dong et al., 2015) and multi-way (Firat et al., 2016) NMT, as well as those equipped with attention mechanisms (Sennrich et al., 2015). The research activities are particularly crucial since they broaden the range of cross-modality learning strategies. Current research interest has concentrated on the incorporation of visual features into NMT (Lala et al., 2018), by means of visual-linguistic context vector concatenation (Libovick`y et al., 2016), doubly-attentive decoding (Calixto et al., 2017a), hierarchical attention combination (Libovick`y and Helcl, 2017), cross-attention ne"
2021.alvr-1.1,P02-1040,0,0.109315,"out a complementary experiment on the ambiguous COCO which contains 461 examples (Elliott et al., 2017). Due to the inclusion of ambiguous verbs, the examples in ambiguous COCO can be used for the evaluation of visual sense disambiguation in a MNMT scenario. 6.2 6.3 Comparison to the Baseline We carry out 5 independent experiments (5 runs) for each of the proposed MNMT variants. In each run, any of the variants is retrained and redeveloped under cold-start conditions using a set of randomlyselected seeds by MultEval4 . Eventually, the resultant models are evaluated on the test set with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). For each variant, we report not only the comprehensive performance (denoted as ensemble) which is obtained using ensemble learning (Garmash and Monz, 2016) but that without ensemble learning. In the latter case, the average performance (µ) and deviations (σ) in the 5 runs are reported. Training and Hyperparameter Settings For preprocessing, we apply Byte-Pair Encoding (BPE) (Sennrich et al., 2015) for tokenizing all the captions and translations in Multi30k and COCO, and use the open-source toolkit3 of Moses (Koehn et al., 200"
2021.emnlp-main.197,D19-1294,0,0.0131621,"recovered ZP representation and the supervision of specific tasks are well shared between the two processes for high-quality model integration. To comprehensively investigate our proposed method, we conduct experiments1 on three D-NLG tasks: document-level neural machine translation (NMT), question answering (QA), and summarization. Experimental results show that our approach can significantly improve the performance on these tasks due to the effective combination of ZP recovering and document context modeling. Furthermore, we use both APT (Miculicich Werlen and Popescu-Belis, 2017) and CRC (Jwalapuram et al., 2019) to evaluate our model performance on pronoun generation, and the results show that our approach can achieve remarkable performance. 2 Related Work As a fundamental research in natural language processing, ZP resolution aims at detecting pronoun chains and resolving missing pronouns to their antecedents. In the literature, previous work mainly resolved ZPs in three steps: zero pronoun detection, anaphoricity determination, and coreference linking. On this basis, varied traditional rule-based or machine-learning methods were used for ZP resolution (Converse, 2006; Zhao and Ng, 2007; Kong and Zh"
2021.emnlp-main.197,2020.emnlp-main.175,0,0.0151635,"me consuming and the automatic annotation is limited to specific tasks like machine translation, it remains challenging when facing new corpus domains or tasks. Moreover, although some two-stage methods were proposed to use pre-trained ZP resolution systems for preprocessing (Taira et al., 2012; Xiang et al., 2013), these methods are known to face notorious error propagation problems. Second, some recent studies explored context-aware architectures for better document cohesion modeling (Zhang et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Tan et al., 2019; Kang et al., 2020). For instance, Tan et al. (2019) proposed a hierarchical model to capture global context, which can significantly improve pronoun translation in document-level NMT. Although the above studies can well capture document-level cohesion to some extent, the context information is too broad to solve ZP problems in a targeted manner. The difference between our method and previous ones is two-fold: First, our two-stage method is a combination of the above categories which can mitigate both corpora limitation and error propagation issues. Second, compared with previous ZP recovery methods, we focus on"
2021.emnlp-main.197,P17-4012,0,0.0233168,"n sentence-level NMT. Then we employ the Ted talks corpus4 to train our model on document-level NMT and use dev2010 (8 documents with 879 sentence pairs) as the development corpus, tst2012-2015 (62 documents with 5566 sentence pairs) as the test corpus. For the QA and summarization tasks, we use the Maternal and Infant Dataset (Xu et al., 2020) for experimentation. The training corpus, validation corpus, and test corpus contain 0.75M, 0.21M, and 0.11M articles, respectively. Model Settings. For document-level NMT, we apply our proposed approach to the Transformer model implemented by OpenNMT (Klein et al., 2017). For fair comparison, we keep our system settings the same as previous work (Tan et al., 2019), and the detailed model configurations are shown in Appendix. Following previous work, we also use the multi-bleu.perl script to compute caseinsensitive BLEU score for evaluation. For the QA and summarization tasks, we apply our proposed approach to the sequence-to-sequence model of MTF-S2S (the single task version) (Xu et al., 2020)5 , and keep the system settings consistent with (Xu et al., 2020). Concretely, we use the beam search algorithm during decoding, and we set the hidden size of encoders"
2021.emnlp-main.197,D10-1086,1,0.699333,"al., 2019) to evaluate our model performance on pronoun generation, and the results show that our approach can achieve remarkable performance. 2 Related Work As a fundamental research in natural language processing, ZP resolution aims at detecting pronoun chains and resolving missing pronouns to their antecedents. In the literature, previous work mainly resolved ZPs in three steps: zero pronoun detection, anaphoricity determination, and coreference linking. On this basis, varied traditional rule-based or machine-learning methods were used for ZP resolution (Converse, 2006; Zhao and Ng, 2007; Kong and Zhou, 2010). Recently, some neural approaches (Liu et al., 2017; Yin et al., 2018; Zhang et al., 2019a,b; Song et al., 2020) were proposed and have achieved certain success due to their better objective representation and powerful neural architectures. As a common language phenomenon in prodrop languages, zero pronoun could result in poor discourse-level cohesion and thus seriously impact the performance of document-level NLG. To date, two types of researches have been conducted to alleviate the discourse-level cohesion deficiency: (i) Recovering dropped pronouns in specific NLG corpora for downstream ta"
2021.emnlp-main.197,2012.eamt-1.60,0,0.0085091,"ults of related systems on the two tasks for reference, and the results of these systems are directly borrowed from (Xu et al., 2020). QA. For question answering, in addition to (Xu et al., 2020), we also compare with a retrievalbased baseline by fine-tuning BERT-base (Devlin et al., 2019) for question matching on an external dataset and two character-based generation baselines (Sutskever et al., 2014; Luong et al., 2015). The overall results are shown in Table 2. From the 4 Including 1906 documents with 226K sentence pairs results, lines 4 and 5 show that our model outperfrom the IWSLT 2017 (Cettolo et al., 2012) evaluation camforms the baseline (Xu et al., 2020) on all the three paigns https://wit3.fbk.eu. 5 https://github.com/WHUIR/MATINF. indicators. And the last two lines show that the 2535 Model Mihalcea and Tarau (2004) Erkan and Radev (2004) Sutskever et al. (2014) Luong et al. (2015) Ma et al. (2018) Lin et al. (2018) Liu and Lapata (2019)† Baseline (Xu et al., 2020) Ours - GAN R-1 R-2 R-L 35.53 33.08 23.05 43.05 34.63 49.28 57.31 43.02 50.12 49.82 25.78 23.31 11.44 28.03 22.56 34.14 44.05 28.05 33.79 33.34 36.84 34.96 19.55 38.58 28.92 47.64 55.93 38.55 44.00 43.44 APT CRC Transformer (2017)"
2021.emnlp-main.197,D14-1179,0,0.0199546,"Missing"
2021.emnlp-main.197,N19-1423,0,0.0128169,"ly useful, although the performance improvement is not so significant. 4.3 Results on QA & Summarization Tasks For QA and summarization, we directly borrow the systems of Xu et al. (2020) as our baselines, and we perform experiments on their single version systems for clarity. Similar to (Xu et al., 2020), we also report the results of related systems on the two tasks for reference, and the results of these systems are directly borrowed from (Xu et al., 2020). QA. For question answering, in addition to (Xu et al., 2020), we also compare with a retrievalbased baseline by fine-tuning BERT-base (Devlin et al., 2019) for question matching on an external dataset and two character-based generation baselines (Sutskever et al., 2014; Luong et al., 2015). The overall results are shown in Table 2. From the 4 Including 1906 documents with 226K sentence pairs results, lines 4 and 5 show that our model outperfrom the IWSLT 2017 (Cettolo et al., 2012) evaluation camforms the baseline (Xu et al., 2020) on all the three paigns https://wit3.fbk.eu. 5 https://github.com/WHUIR/MATINF. indicators. And the last two lines show that the 2535 Model Mihalcea and Tarau (2004) Erkan and Radev (2004) Sutskever et al. (2014) Luon"
2021.emnlp-main.197,N03-1020,0,0.12141,"Appendix. Following previous work, we also use the multi-bleu.perl script to compute caseinsensitive BLEU score for evaluation. For the QA and summarization tasks, we apply our proposed approach to the sequence-to-sequence model of MTF-S2S (the single task version) (Xu et al., 2020)5 , and keep the system settings consistent with (Xu et al., 2020). Concretely, we use the beam search algorithm during decoding, and we set the hidden size of encoders and decoders to 200 and the batch size to 64. We also use Adam as our optimizer with the learning rate set to 0.001. Similarly, we also use ROUGE (Lin and Hovy, 2003) to estimate the quality of the generated texts for performance evaluation. 4.2 Results on Document-level NMT For document-level NMT, we compare our system with two recent context-aware systems (Zhang et al., 2018; Tan et al., 2019). Among them, Zhang et al. (2018) propose to model partial document context from previous sentences for better performance. And Tan et al. (2019) put their insight on global context modeling and have demonstrated the usefulness of global context in document-level NMT. Besides, we also present the results of Transformer (Vaswani et al., 2017) for comparison. From the"
2021.emnlp-main.197,P18-2027,0,0.0417335,"Missing"
2021.emnlp-main.197,P17-1010,0,0.0226364,"generation, and the results show that our approach can achieve remarkable performance. 2 Related Work As a fundamental research in natural language processing, ZP resolution aims at detecting pronoun chains and resolving missing pronouns to their antecedents. In the literature, previous work mainly resolved ZPs in three steps: zero pronoun detection, anaphoricity determination, and coreference linking. On this basis, varied traditional rule-based or machine-learning methods were used for ZP resolution (Converse, 2006; Zhao and Ng, 2007; Kong and Zhou, 2010). Recently, some neural approaches (Liu et al., 2017; Yin et al., 2018; Zhang et al., 2019a,b; Song et al., 2020) were proposed and have achieved certain success due to their better objective representation and powerful neural architectures. As a common language phenomenon in prodrop languages, zero pronoun could result in poor discourse-level cohesion and thus seriously impact the performance of document-level NLG. To date, two types of researches have been conducted to alleviate the discourse-level cohesion deficiency: (i) Recovering dropped pronouns in specific NLG corpora for downstream tasks; (ii) Using well-designed context-aware architec"
2021.emnlp-main.197,D19-1387,0,0.0539049,"Missing"
2021.emnlp-main.197,D15-1166,0,0.0359673,"we directly borrow the systems of Xu et al. (2020) as our baselines, and we perform experiments on their single version systems for clarity. Similar to (Xu et al., 2020), we also report the results of related systems on the two tasks for reference, and the results of these systems are directly borrowed from (Xu et al., 2020). QA. For question answering, in addition to (Xu et al., 2020), we also compare with a retrievalbased baseline by fine-tuning BERT-base (Devlin et al., 2019) for question matching on an external dataset and two character-based generation baselines (Sutskever et al., 2014; Luong et al., 2015). The overall results are shown in Table 2. From the 4 Including 1906 documents with 226K sentence pairs results, lines 4 and 5 show that our model outperfrom the IWSLT 2017 (Cettolo et al., 2012) evaluation camforms the baseline (Xu et al., 2020) on all the three paigns https://wit3.fbk.eu. 5 https://github.com/WHUIR/MATINF. indicators. And the last two lines show that the 2535 Model Mihalcea and Tarau (2004) Erkan and Radev (2004) Sutskever et al. (2014) Luong et al. (2015) Ma et al. (2018) Lin et al. (2018) Liu and Lapata (2019)† Baseline (Xu et al., 2020) Ours - GAN R-1 R-2 R-L 35.53 33.08"
2021.emnlp-main.197,N18-1018,0,0.04637,"Missing"
2021.emnlp-main.197,D19-1168,1,0.0659804,"ocess, thus our tated corpora (Wang et al., 2016, 2018a,b, 2019; method does not require NLG corpora annoZhang et al., 2019c) or pre-trained ZP resolution tated with ZPs. For system enhancement, we systems (Taira et al., 2012; Xiang et al., 2013). Anlearn an adversarial bot to adjust our model outother line indirectly deals with the ZP problem by puts to alleviate the error propagation caused producing better discourse cohesion through docby mis-recovered ZPs. Experiments on three ument context modeling (Zhang et al., 2018; Midocument-level NLG tasks, i.e., machine transculicich et al., 2018; Tan et al., 2019; Wong et al., lation, question answering, and summarization, 2020). Although the above studies have made great show that our approach can improve the performance to a great extent, and the improvement progresses, ZP resolution in NLG still faces the on pronoun translation is very impressive. following possible bottlenecks: (i) ZP-annotated corpora tailored for NLG tasks are scarcity, and 1 Introduction existing ZP corpora are limited to certain domains For a long time, natural language generation (NLG) and tasks. (ii) Using pre-trained ZP resolution syshas attracted a lot of attention for its"
2021.emnlp-main.197,P18-1118,0,0.0479959,"Missing"
2021.emnlp-main.197,N19-1313,0,0.106472,"s, the manual annotation is usually time consuming and the automatic annotation is limited to specific tasks like machine translation, it remains challenging when facing new corpus domains or tasks. Moreover, although some two-stage methods were proposed to use pre-trained ZP resolution systems for preprocessing (Taira et al., 2012; Xiang et al., 2013), these methods are known to face notorious error propagation problems. Second, some recent studies explored context-aware architectures for better document cohesion modeling (Zhang et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Tan et al., 2019; Kang et al., 2020). For instance, Tan et al. (2019) proposed a hierarchical model to capture global context, which can significantly improve pronoun translation in document-level NMT. Although the above studies can well capture document-level cohesion to some extent, the context information is too broad to solve ZP problems in a targeted manner. The difference between our method and previous ones is two-fold: First, our two-stage method is a combination of the above categories which can mitigate both corpora limitation and error propagation issues. Second, compared with pre"
2021.emnlp-main.197,D18-1325,0,0.0208934,"a) annotated ZP corpora for NLG tasks. Nevertheless, the manual annotation is usually time consuming and the automatic annotation is limited to specific tasks like machine translation, it remains challenging when facing new corpus domains or tasks. Moreover, although some two-stage methods were proposed to use pre-trained ZP resolution systems for preprocessing (Taira et al., 2012; Xiang et al., 2013), these methods are known to face notorious error propagation problems. Second, some recent studies explored context-aware architectures for better document cohesion modeling (Zhang et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Tan et al., 2019; Kang et al., 2020). For instance, Tan et al. (2019) proposed a hierarchical model to capture global context, which can significantly improve pronoun translation in document-level NMT. Although the above studies can well capture document-level cohesion to some extent, the context information is too broad to solve ZP problems in a targeted manner. The difference between our method and previous ones is two-fold: First, our two-stage method is a combination of the above categories which can mitigate both corpora limitation and error"
2021.emnlp-main.197,W17-4802,0,0.0144809,"G task learning in turn. In this way, the recovered ZP representation and the supervision of specific tasks are well shared between the two processes for high-quality model integration. To comprehensively investigate our proposed method, we conduct experiments1 on three D-NLG tasks: document-level neural machine translation (NMT), question answering (QA), and summarization. Experimental results show that our approach can significantly improve the performance on these tasks due to the effective combination of ZP recovering and document context modeling. Furthermore, we use both APT (Miculicich Werlen and Popescu-Belis, 2017) and CRC (Jwalapuram et al., 2019) to evaluate our model performance on pronoun generation, and the results show that our approach can achieve remarkable performance. 2 Related Work As a fundamental research in natural language processing, ZP resolution aims at detecting pronoun chains and resolving missing pronouns to their antecedents. In the literature, previous work mainly resolved ZPs in three steps: zero pronoun detection, anaphoricity determination, and coreference linking. On this basis, varied traditional rule-based or machine-learning methods were used for ZP resolution (Converse, 20"
2021.emnlp-main.197,W04-3252,0,0.549182,"Missing"
2021.emnlp-main.197,N15-1052,0,0.023678,"al., 2019a; Song et al., 2020), and these studies Natural language generation (NLG) tasks on have achieved certain success. Nevertheless, due to pro-drop languages are known to suffer from the lack of NLG corpora annotated with ZPs, the zero pronoun (ZP) problems, and the probproblem of zero pronoun remains challenging in lems remain challenging due to the scarcity of ZP-annotated NLG corpora. In this case, we document-level NLG tasks. propose a highly adaptive two-stage approach Recently, more and more researchers turn to ZP to couple context modeling with ZP recoverresolution in NLG tasks (Rao et al., 2015; Wang ing to mitigate the ZP problem in NLG tasks. et al., 2016, 2018a,b, 2019). Among these studies, Notably, we frame the recovery process in a one line explicitly deals with the ZP problem by task-supervised fashion where the ZP reprerecovering dropped pronouns through either annosentation recovering capability is learned during the NLG task learning process, thus our tated corpora (Wang et al., 2016, 2018a,b, 2019; method does not require NLG corpora annoZhang et al., 2019c) or pre-trained ZP resolution tated with ZPs. For system enhancement, we systems (Taira et al., 2012; Xiang et al.,"
2021.emnlp-main.197,D19-1085,0,0.0331153,"Missing"
2021.emnlp-main.197,D18-1333,0,0.0182364,"re effective due to the great capability of our approach in extracting more concise and effective features from the global context from a specific perspective (i.e., zero pronoun). Moreover, the last two lines show that distributing global context to word units can further improve the performance. This indicates that the global context still contains some other effective information worthy of further mining. 5.3 tor by considering both the left and right sides of each word as candidate ZP positions. To investigate the effect of our approach, we perform experiments on the cleaned tvsub corpus (Wang et al., 2018a) with the sentences without ZPs filtered out9 . For performance evaluation, we follow Wang et al. (2016) to utilize the micro-averaged F1 -score to measure our model performance. The results in Table 6 show that the improved ZP position detector does achieve results better than the method of (Wang et al., 2018a), which suggests the necessity of taking both sides of each word into consideration for ZP position detection. It is worth mentioning that since this work directly harnesses ZP representation in subsequent tasks aiming to alleviate error propagation, it does not depend on ZP-annotated"
2021.emnlp-main.197,N16-1113,0,0.144674,"case, we document-level NLG tasks. propose a highly adaptive two-stage approach Recently, more and more researchers turn to ZP to couple context modeling with ZP recoverresolution in NLG tasks (Rao et al., 2015; Wang ing to mitigate the ZP problem in NLG tasks. et al., 2016, 2018a,b, 2019). Among these studies, Notably, we frame the recovery process in a one line explicitly deals with the ZP problem by task-supervised fashion where the ZP reprerecovering dropped pronouns through either annosentation recovering capability is learned during the NLG task learning process, thus our tated corpora (Wang et al., 2016, 2018a,b, 2019; method does not require NLG corpora annoZhang et al., 2019c) or pre-trained ZP resolution tated with ZPs. For system enhancement, we systems (Taira et al., 2012; Xiang et al., 2013). Anlearn an adversarial bot to adjust our model outother line indirectly deals with the ZP problem by puts to alleviate the error propagation caused producing better discourse cohesion through docby mis-recovered ZPs. Experiments on three ument context modeling (Zhang et al., 2018; Midocument-level NLG tasks, i.e., machine transculicich et al., 2018; Tan et al., 2019; Wong et al., lation, question"
2021.emnlp-main.197,2020.acl-main.530,0,0.0294841,"Missing"
2021.emnlp-main.197,2020.acl-main.482,0,0.0130717,"ieve remarkable performance. 2 Related Work As a fundamental research in natural language processing, ZP resolution aims at detecting pronoun chains and resolving missing pronouns to their antecedents. In the literature, previous work mainly resolved ZPs in three steps: zero pronoun detection, anaphoricity determination, and coreference linking. On this basis, varied traditional rule-based or machine-learning methods were used for ZP resolution (Converse, 2006; Zhao and Ng, 2007; Kong and Zhou, 2010). Recently, some neural approaches (Liu et al., 2017; Yin et al., 2018; Zhang et al., 2019a,b; Song et al., 2020) were proposed and have achieved certain success due to their better objective representation and powerful neural architectures. As a common language phenomenon in prodrop languages, zero pronoun could result in poor discourse-level cohesion and thus seriously impact the performance of document-level NLG. To date, two types of researches have been conducted to alleviate the discourse-level cohesion deficiency: (i) Recovering dropped pronouns in specific NLG corpora for downstream tasks; (ii) Using well-designed context-aware architectures for document-level cohesion modeling. First, some studi"
2021.emnlp-main.197,P13-1081,0,0.18252,"o et al., 2015; Wang ing to mitigate the ZP problem in NLG tasks. et al., 2016, 2018a,b, 2019). Among these studies, Notably, we frame the recovery process in a one line explicitly deals with the ZP problem by task-supervised fashion where the ZP reprerecovering dropped pronouns through either annosentation recovering capability is learned during the NLG task learning process, thus our tated corpora (Wang et al., 2016, 2018a,b, 2019; method does not require NLG corpora annoZhang et al., 2019c) or pre-trained ZP resolution tated with ZPs. For system enhancement, we systems (Taira et al., 2012; Xiang et al., 2013). Anlearn an adversarial bot to adjust our model outother line indirectly deals with the ZP problem by puts to alleviate the error propagation caused producing better discourse cohesion through docby mis-recovered ZPs. Experiments on three ument context modeling (Zhang et al., 2018; Midocument-level NLG tasks, i.e., machine transculicich et al., 2018; Tan et al., 2019; Wong et al., lation, question answering, and summarization, 2020). Although the above studies have made great show that our approach can improve the performance to a great extent, and the improvement progresses, ZP resolution in"
2021.emnlp-main.197,2020.acl-main.330,0,0.0748875,"d fmri technology functional magnetic decoding phase of each subsequent task for the resonance imaging technology to image the brain learning of NLG tasks. .” Obviously, the dropped pronoun “他们 (They)” 2 The pronoun vectors are randomly initialized 512D vecis not explicitly translated, according to the lan- tors that represent the 31 pronoun labels (including the ε label); guage habits of the target side. This indicates that the pronoun labels are detailed in Appendix. 2533 In detail, we apply our method to three recent NLG systems: document-level NMT (Tan et al., 2019), QA and summarization (Xu et al., 2020). Among them, Tan et al. (2019) introduced a hierarchical structure to model global context from all sentences of an article and have demonstrated the effectiveness of global context in machine translation. Xu et al. (2020) presented a straightforward yet effective model on summarization and QA based on their proposed MATINF dataset3 . We incorporate our ZP-focused context information into the three systems as following: • For document-level NMT, we first apply our ZP position detector to NMT corpora for preprocessing. Then, based on the Transformer-based system of Tan et al. (2019), we input"
2021.emnlp-main.197,W12-4213,0,0.199254,"ion in NLG tasks (Rao et al., 2015; Wang ing to mitigate the ZP problem in NLG tasks. et al., 2016, 2018a,b, 2019). Among these studies, Notably, we frame the recovery process in a one line explicitly deals with the ZP problem by task-supervised fashion where the ZP reprerecovering dropped pronouns through either annosentation recovering capability is learned during the NLG task learning process, thus our tated corpora (Wang et al., 2016, 2018a,b, 2019; method does not require NLG corpora annoZhang et al., 2019c) or pre-trained ZP resolution tated with ZPs. For system enhancement, we systems (Taira et al., 2012; Xiang et al., 2013). Anlearn an adversarial bot to adjust our model outother line indirectly deals with the ZP problem by puts to alleviate the error propagation caused producing better discourse cohesion through docby mis-recovered ZPs. Experiments on three ument context modeling (Zhang et al., 2018; Midocument-level NLG tasks, i.e., machine transculicich et al., 2018; Tan et al., 2019; Wong et al., lation, question answering, and summarization, 2020). Although the above studies have made great show that our approach can improve the performance to a great extent, and the improvement progres"
2021.emnlp-main.197,2020.findings-emnlp.13,0,0.0980895,"Missing"
2021.emnlp-main.197,P15-2051,0,0.124139,"ieved certain success due to their better objective representation and powerful neural architectures. As a common language phenomenon in prodrop languages, zero pronoun could result in poor discourse-level cohesion and thus seriously impact the performance of document-level NLG. To date, two types of researches have been conducted to alleviate the discourse-level cohesion deficiency: (i) Recovering dropped pronouns in specific NLG corpora for downstream tasks; (ii) Using well-designed context-aware architectures for document-level cohesion modeling. First, some studies directly used manually (Yang et al., 2015; Zhang et al., 2019c; Yang et al., 2020) or automatically (Wang et al., 2016, 2018a) annotated ZP corpora for NLG tasks. Nevertheless, the manual annotation is usually time consuming and the automatic annotation is limited to specific tasks like machine translation, it remains challenging when facing new corpus domains or tasks. Moreover, although some two-stage methods were proposed to use pre-trained ZP resolution systems for preprocessing (Taira et al., 2012; Xiang et al., 2013), these methods are known to face notorious error propagation problems. Second, some recent studies explored cont"
2021.emnlp-main.197,P18-1053,0,0.0204186,"he results show that our approach can achieve remarkable performance. 2 Related Work As a fundamental research in natural language processing, ZP resolution aims at detecting pronoun chains and resolving missing pronouns to their antecedents. In the literature, previous work mainly resolved ZPs in three steps: zero pronoun detection, anaphoricity determination, and coreference linking. On this basis, varied traditional rule-based or machine-learning methods were used for ZP resolution (Converse, 2006; Zhao and Ng, 2007; Kong and Zhou, 2010). Recently, some neural approaches (Liu et al., 2017; Yin et al., 2018; Zhang et al., 2019a,b; Song et al., 2020) were proposed and have achieved certain success due to their better objective representation and powerful neural architectures. As a common language phenomenon in prodrop languages, zero pronoun could result in poor discourse-level cohesion and thus seriously impact the performance of document-level NLG. To date, two types of researches have been conducted to alleviate the discourse-level cohesion deficiency: (i) Recovering dropped pronouns in specific NLG corpora for downstream tasks; (ii) Using well-designed context-aware architectures for document"
2021.emnlp-main.197,N19-1093,0,0.0632795,"Missing"
2021.emnlp-main.197,P19-1083,0,0.138009,"roach Recently, more and more researchers turn to ZP to couple context modeling with ZP recoverresolution in NLG tasks (Rao et al., 2015; Wang ing to mitigate the ZP problem in NLG tasks. et al., 2016, 2018a,b, 2019). Among these studies, Notably, we frame the recovery process in a one line explicitly deals with the ZP problem by task-supervised fashion where the ZP reprerecovering dropped pronouns through either annosentation recovering capability is learned during the NLG task learning process, thus our tated corpora (Wang et al., 2016, 2018a,b, 2019; method does not require NLG corpora annoZhang et al., 2019c) or pre-trained ZP resolution tated with ZPs. For system enhancement, we systems (Taira et al., 2012; Xiang et al., 2013). Anlearn an adversarial bot to adjust our model outother line indirectly deals with the ZP problem by puts to alleviate the error propagation caused producing better discourse cohesion through docby mis-recovered ZPs. Experiments on three ument context modeling (Zhang et al., 2018; Midocument-level NLG tasks, i.e., machine transculicich et al., 2018; Tan et al., 2019; Wong et al., lation, question answering, and summarization, 2020). Although the above studies have made g"
2021.emnlp-main.197,D18-1049,0,0.0452414,"Missing"
2021.emnlp-main.197,D07-1057,0,0.335238,"in pro-drop languages like that couples context modeling with ZP recovering. Chinese, Spanish, etc. Taking the Chinese TED Specifically, our approach mainly consists of two corpus as an example, according to our statistics, phases: First, we pre-train a fault-tolerant ZP poin sentences with an average length of 18, each sition detector for downstream tasks’ data corpora sentence will omit around 0.5 pronouns. Facing to automatically detect ZP positions. Second, we this problem, lots of attention has been paid to ZP perform document context modeling for both taskresolution in the past decade (Zhao and Ng, 2007; supervised ZP recovering and ZP-focused NLG ∗ Corresponding author task learning. Notably, the ZP recovering process 2530 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2530–2540 c November 7–11, 2021. 2021 Association for Computational Linguistics and the NLG task learning process depend on and promote with each other harmoniously. On the one hand, instead of recovering specific pronoun labels, we learn ZP representation through the supervision of NLG tasks and thus our method does not require large-scale ZP-annotated data. One the other, the a"
2021.emnlp-main.360,2020.coling-main.24,0,0.481817,"extraction besides the corresponding sentiment classification in a multi-modal scenario. Note that we also propose a multi-modal joint learning approach to improve the performance of both MATE and MASC. Text-based Joint Aspect Terms Extraction and Sentiment Classification. Some studies (Zhang et al., 2020a) have attempted to solve both sub-tasks in a more integrated way, by jointly extracting aspect terms and predicting their sentiment polarities. The most recent and representative are a span-based extract-then-classify approach (Hu et al., 2019) and a directed GCN with syntactic information (Chen et al., 2020). However, all the above studies can not model the visual guidance for both sub-tasks. Different from them, we propose a multi-modal joint framework to handle both MATE and MASC. 3 Joint Multi-modal Aspect-Sentiment Analysis 3.1 Cross-modal Relation Detection Unlike traditional approaches, which take visual information into consideration completely and ignore whether image can bring benefits to text, we incorporate the image-text relation into the model and only retain the auxiliary visual information towards the text. Therefore, we build a relation module by pre-training to properly exploit v"
2021.emnlp-main.360,P19-1052,0,0.0263826,"ent Classification (MASC). Different from text-based aspect sentiment classification (Sundararaman et al., 2020; Ji et al., 2020; Liang et al., 2020b,a), it is challenging to effectively fuse the textual and visual information. As a pioneer, Xu et al. (2019) collect a benchmark Chinese dataset from a digital product review platform for multi-modal aspect-level sentiment analysis and propose a multi-interactive memory network to iteratively fuse the textual and visual representations. In the past five years, text-based aspect-level sentiment analysis has drawn much attention (Luo et al., 2019; Chen and Qian, 2019; Zhang and Qian, Recently, Yu and Jiang (2019) annotate two 2020; Zheng et al., 2020; Tulkens and van Cranen- datasets in Twitter for multi-modal target-oriented burgh, 2020; Akhtar et al., 2020). While, multi- (aka aspect-level) sentiment classification and levermodal target-oriented sentiment analysis has be- age BERT as backbone to effectively combine both come more and more vital because of its urgent textual and visual modalities. In the same period, 4396 Yu et al. (2020a) propose a target-sensitive attention and fusion network to address both text-based and multi-modal target-oriented s"
2021.emnlp-main.360,2020.emnlp-main.164,0,0.118715,"(a) RT @ funnytwittingg : [OBAMA]Neg TO [ISRAEL]Neu ? [OBAMA]Neg TO [UKRAINE] ? [OBAMA]Neg TO [USA]Neu ? (b) Figure 1: Two examples for joint multi-modal aspectsentiment analysis. aspect terms from a free text with its accompanying image (Wu et al., 2020a). Second, MASC aims to classify the sentiment polarity of a multi-modal post towards a given aspect in textual modality (Yu and Jiang, 2019). To better satisfy the practical applications, the aspect term-polarity co-extraction, which solves ATE and ASC simultaneously, receives much attention recently in a textual scenario (Wan et al., 2020; Chen and Qian, 2020b; Ying et al., 2020). However, to our best knowledge, in the multi-modal scenario, the joint MATE and MASC, i.e., joint multi-modal aspect-sentiment analysis (JMASA), have never been investigated so far. For this joint multi-modal task, we believe that there exist the following challenges at least. 1 Introduction On the one hand, visual modality may provide no clues for one of sub-tasks. For example, in FigMulti-modal aspect-level (aka target-oriented) senure 1(a), since the image shows most of the content timent analysis (MALSA) is an important and finedescribed in the text, and we can’t inf"
2021.emnlp-main.360,2020.acl-main.340,0,0.174693,"(a) RT @ funnytwittingg : [OBAMA]Neg TO [ISRAEL]Neu ? [OBAMA]Neg TO [UKRAINE] ? [OBAMA]Neg TO [USA]Neu ? (b) Figure 1: Two examples for joint multi-modal aspectsentiment analysis. aspect terms from a free text with its accompanying image (Wu et al., 2020a). Second, MASC aims to classify the sentiment polarity of a multi-modal post towards a given aspect in textual modality (Yu and Jiang, 2019). To better satisfy the practical applications, the aspect term-polarity co-extraction, which solves ATE and ASC simultaneously, receives much attention recently in a textual scenario (Wan et al., 2020; Chen and Qian, 2020b; Ying et al., 2020). However, to our best knowledge, in the multi-modal scenario, the joint MATE and MASC, i.e., joint multi-modal aspect-sentiment analysis (JMASA), have never been investigated so far. For this joint multi-modal task, we believe that there exist the following challenges at least. 1 Introduction On the one hand, visual modality may provide no clues for one of sub-tasks. For example, in FigMulti-modal aspect-level (aka target-oriented) senure 1(a), since the image shows most of the content timent analysis (MALSA) is an important and finedescribed in the text, and we can’t inf"
2021.emnlp-main.360,N19-1423,0,0.135672,"k and a softn=1 be the set of data samples. Given a word sequence X = {x1 , x2 , · · · , xk } max activation function as follows: with length k and an image I, the joint task is to extract a aspect terms list A = {a1 , a2 , · · · , am } pr = softmax(W2 tanh(W1 H)) (5) and classify the aspect sentiment list S = {s1 , s2 , · · · , sm } simultaneously, where m denotes where W1 ∈ R4∗dm ×dm and W2 ∈ Rdm ×2 are the number of aspects. Note that the word embed- two trainable parameter matrices. H means the dings are obtained by pre-processing via BERT concatenation of Ho , Hx , Ho→x and Hx→o . Since (Devlin et al., 2019) due to its excellent ability the relation score can also be binary: 0 or 1, we of textual representation, meanwhile the image re- calculated by equation similarly to equation 5,but gion embeddings are obtained by pre-processing score pr < 0.5 = 0, p < 0.5. Then we try both via ResNet (He et al., 2016) due to its excellent soft and hard relations to guide our multi-modal ability of visual representation. joint tasks. 4397 BERT Embedding Crossmodal Attenion ... Multi-aspect Extraction H ak ... T3  T2  T1  H a3 H o Tk Trm Trm ... Trm Trm Trm ... Trm E1 E2 ... Ek H u1  H s2  u1,1 H s1  u1,"
2021.emnlp-main.360,P19-1051,0,0.30161,"Different from them, we propose to jointly perform aspect terms extraction besides the corresponding sentiment classification in a multi-modal scenario. Note that we also propose a multi-modal joint learning approach to improve the performance of both MATE and MASC. Text-based Joint Aspect Terms Extraction and Sentiment Classification. Some studies (Zhang et al., 2020a) have attempted to solve both sub-tasks in a more integrated way, by jointly extracting aspect terms and predicting their sentiment polarities. The most recent and representative are a span-based extract-then-classify approach (Hu et al., 2019) and a directed GCN with syntactic information (Chen et al., 2020). However, all the above studies can not model the visual guidance for both sub-tasks. Different from them, we propose a multi-modal joint framework to handle both MATE and MASC. 3 Joint Multi-modal Aspect-Sentiment Analysis 3.1 Cross-modal Relation Detection Unlike traditional approaches, which take visual information into consideration completely and ignore whether image can bring benefits to text, we incorporate the image-text relation into the model and only retain the auxiliary visual information towards the text. Therefore"
2021.emnlp-main.360,2020.emnlp-main.570,0,0.0372153,"2020a,b) However, all the above studies completely ignore the sentiment polarity analysis dependent on the detected target, which has great facilitates in practical applications, such as e-commerce. Different from them, we propose to jointly perform the corresponding sentiment classification besides aspect terms extraction in a multi-modal scenario. Note that we propose a multi-modal joint learning approach to improve the performance of both MATE and MASC. Multi-modal Aspect Sentiment Classification (MASC). Different from text-based aspect sentiment classification (Sundararaman et al., 2020; Ji et al., 2020; Liang et al., 2020b,a), it is challenging to effectively fuse the textual and visual information. As a pioneer, Xu et al. (2019) collect a benchmark Chinese dataset from a digital product review platform for multi-modal aspect-level sentiment analysis and propose a multi-interactive memory network to iteratively fuse the textual and visual representations. In the past five years, text-based aspect-level sentiment analysis has drawn much attention (Luo et al., 2019; Chen and Qian, 2019; Zhang and Qian, Recently, Yu and Jiang (2019) annotate two 2020; Zheng et al., 2020; Tulkens and van Cranen"
2021.emnlp-main.360,D19-1468,0,0.0233119,"to the industry recently (Akhtar et al., 2019; Zadeh et al., 2020; Sun et al., 2021a; Tang et al., 2019; Zhang et al., 2020b, 2021a). In the following, we mainly overview the limited studies of multi-modal aspect terms extraction and multi-modal aspect sentiment classification on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the gap between text and image. Several related studies with focus on named entity recognition propose to leverage the whole image information by ResNet encoding to augment each word representation, such as (Moon et al., 2018; Zhang et al., 2018) upon RNN, (Yu et al., 2020b) upon Transformer and (Zhang et al., 2021b) on GNN. Besides, several related studies propose to leveraging the fine-grained visual information by object detection, such as (Wu et al., 2020a,b) However, all the above studies completely ignore the sentiment polarity analysis dependent on the"
2021.emnlp-main.360,P19-1056,0,0.0191113,"odal Aspect Sentiment Classification (MASC). Different from text-based aspect sentiment classification (Sundararaman et al., 2020; Ji et al., 2020; Liang et al., 2020b,a), it is challenging to effectively fuse the textual and visual information. As a pioneer, Xu et al. (2019) collect a benchmark Chinese dataset from a digital product review platform for multi-modal aspect-level sentiment analysis and propose a multi-interactive memory network to iteratively fuse the textual and visual representations. In the past five years, text-based aspect-level sentiment analysis has drawn much attention (Luo et al., 2019; Chen and Qian, 2019; Zhang and Qian, Recently, Yu and Jiang (2019) annotate two 2020; Zheng et al., 2020; Tulkens and van Cranen- datasets in Twitter for multi-modal target-oriented burgh, 2020; Akhtar et al., 2020). While, multi- (aka aspect-level) sentiment classification and levermodal target-oriented sentiment analysis has be- age BERT as backbone to effectively combine both come more and more vital because of its urgent textual and visual modalities. In the same period, 4396 Yu et al. (2020a) propose a target-sensitive attention and fusion network to address both text-based and multi-mo"
2021.emnlp-main.360,P19-1344,0,0.0248744,"hes. 2 Related Work need to be applied to the industry recently (Akhtar et al., 2019; Zadeh et al., 2020; Sun et al., 2021a; Tang et al., 2019; Zhang et al., 2020b, 2021a). In the following, we mainly overview the limited studies of multi-modal aspect terms extraction and multi-modal aspect sentiment classification on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the gap between text and image. Several related studies with focus on named entity recognition propose to leverage the whole image information by ResNet encoding to augment each word representation, such as (Moon et al., 2018; Zhang et al., 2018) upon RNN, (Yu et al., 2020b) upon Transformer and (Zhang et al., 2021b) on GNN. Besides, several related studies propose to leveraging the fine-grained visual information by object detection, such as (Wu et al., 2020a,b) However, all the above studies completely ignor"
2021.emnlp-main.360,N18-1078,0,0.0282201,"fication on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the gap between text and image. Several related studies with focus on named entity recognition propose to leverage the whole image information by ResNet encoding to augment each word representation, such as (Moon et al., 2018; Zhang et al., 2018) upon RNN, (Yu et al., 2020b) upon Transformer and (Zhang et al., 2021b) on GNN. Besides, several related studies propose to leveraging the fine-grained visual information by object detection, such as (Wu et al., 2020a,b) However, all the above studies completely ignore the sentiment polarity analysis dependent on the detected target, which has great facilitates in practical applications, such as e-commerce. Different from them, we propose to jointly perform the corresponding sentiment classification besides aspect terms extraction in a multi-modal scenario. Note that we p"
2021.emnlp-main.360,2020.aacl-main.33,0,0.0301429,"ection, such as (Wu et al., 2020a,b) However, all the above studies completely ignore the sentiment polarity analysis dependent on the detected target, which has great facilitates in practical applications, such as e-commerce. Different from them, we propose to jointly perform the corresponding sentiment classification besides aspect terms extraction in a multi-modal scenario. Note that we propose a multi-modal joint learning approach to improve the performance of both MATE and MASC. Multi-modal Aspect Sentiment Classification (MASC). Different from text-based aspect sentiment classification (Sundararaman et al., 2020; Ji et al., 2020; Liang et al., 2020b,a), it is challenging to effectively fuse the textual and visual information. As a pioneer, Xu et al. (2019) collect a benchmark Chinese dataset from a digital product review platform for multi-modal aspect-level sentiment analysis and propose a multi-interactive memory network to iteratively fuse the textual and visual representations. In the past five years, text-based aspect-level sentiment analysis has drawn much attention (Luo et al., 2019; Chen and Qian, 2019; Zhang and Qian, Recently, Yu and Jiang (2019) annotate two 2020; Zheng et al., 2020; Tulke"
2021.emnlp-main.360,P19-1053,0,0.0572799,"Missing"
2021.emnlp-main.360,2020.acl-main.290,0,0.0610892,"Missing"
2021.emnlp-main.360,2020.coling-main.13,0,0.0357248,", all the above studies completely ignore the sentiment polarity analysis dependent on the detected target, which has great facilitates in practical applications, such as e-commerce. Different from them, we propose to jointly perform the corresponding sentiment classification besides aspect terms extraction in a multi-modal scenario. Note that we propose a multi-modal joint learning approach to improve the performance of both MATE and MASC. Multi-modal Aspect Sentiment Classification (MASC). Different from text-based aspect sentiment classification (Sundararaman et al., 2020; Ji et al., 2020; Liang et al., 2020b,a), it is challenging to effectively fuse the textual and visual information. As a pioneer, Xu et al. (2019) collect a benchmark Chinese dataset from a digital product review platform for multi-modal aspect-level sentiment analysis and propose a multi-interactive memory network to iteratively fuse the textual and visual representations. In the past five years, text-based aspect-level sentiment analysis has drawn much attention (Luo et al., 2019; Chen and Qian, 2019; Zhang and Qian, Recently, Yu and Jiang (2019) annotate two 2020; Zheng et al., 2020; Tulkens and van Cranen- datasets in Twitte"
2021.emnlp-main.360,P19-1272,0,0.0608412,"Missing"
2021.emnlp-main.360,2020.emnlp-main.286,0,0.0998868,"Missing"
2021.emnlp-main.360,2020.acl-main.306,0,0.0900057,"Missing"
2021.emnlp-main.360,2020.emnlp-main.141,0,0.0335747,"s-modal relation detection to control whether the image adds to the text meaning. Second, we leverage the joint hierarchical framework to separately attend to the effective visual information for each sub-task instead of collapsed tagging framework. Finally, we can obtain all potential aspect term-polarity pairs. Extensive experiments and analysis on two multi-modal datasets in Twitter show that our approach performs significantly better than text-based joint approaches and collapsed multi-modal joint approaches. 2 Related Work need to be applied to the industry recently (Akhtar et al., 2019; Zadeh et al., 2020; Sun et al., 2021a; Tang et al., 2019; Zhang et al., 2020b, 2021a). In the following, we mainly overview the limited studies of multi-modal aspect terms extraction and multi-modal aspect sentiment classification on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the"
2021.emnlp-main.360,2020.findings-emnlp.72,0,0.202055,"ds to the text meaning. Second, we leverage the joint hierarchical framework to separately attend to the effective visual information for each sub-task instead of collapsed tagging framework. Finally, we can obtain all potential aspect term-polarity pairs. Extensive experiments and analysis on two multi-modal datasets in Twitter show that our approach performs significantly better than text-based joint approaches and collapsed multi-modal joint approaches. 2 Related Work need to be applied to the industry recently (Akhtar et al., 2019; Zadeh et al., 2020; Sun et al., 2021a; Tang et al., 2019; Zhang et al., 2020b, 2021a). In the following, we mainly overview the limited studies of multi-modal aspect terms extraction and multi-modal aspect sentiment classification on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the gap between text and image. Several related studies with f"
2021.emnlp-main.360,2020.emnlp-main.291,1,0.842073,"ds to the text meaning. Second, we leverage the joint hierarchical framework to separately attend to the effective visual information for each sub-task instead of collapsed tagging framework. Finally, we can obtain all potential aspect term-polarity pairs. Extensive experiments and analysis on two multi-modal datasets in Twitter show that our approach performs significantly better than text-based joint approaches and collapsed multi-modal joint approaches. 2 Related Work need to be applied to the industry recently (Akhtar et al., 2019; Zadeh et al., 2020; Sun et al., 2021a; Tang et al., 2019; Zhang et al., 2020b, 2021a). In the following, we mainly overview the limited studies of multi-modal aspect terms extraction and multi-modal aspect sentiment classification on text and image modalities. Besides, we also introduce some representative studies for text-based joint aspect terms extraction and sentiment polarity classification. Multi-modal Aspect Terms Extraction (MATE). Sequence labeling approaches are typically employed for this sub-task(Ma et al., 2019; Chen and Qian, 2020a; Karamanolakis et al., 2019). But it is challenging to bridge the gap between text and image. Several related studies with f"
2021.findings-emnlp.110,P19-1411,0,0.0335856,"Missing"
2021.findings-emnlp.110,P19-1442,0,0.0394084,"Missing"
2021.findings-emnlp.110,prasad-etal-2008-penn,0,0.75262,"E for X (b). Cause of Erroneous Sampling Bo da un Bo Sample Space (c). CVAE-based Re-anchoring r da un Others Others y ry SE (M Z ) Erroneous Sampling (mean) Anchor X Anchor X Strongly-associated sampling area Weakly-associated sampling area Relation R ry da un Bo R ry Strongly-associated sampling area Weakly-associated sampling area da un Bo Relation Migrating Anchor Strongly-associated sampling area Weakly-associated sampling area Figure 2: The schematic diagrams regarding sampling area, erroneous sampling and re-anchoring. et al., 2015) is used for re-anchoring. We experiment on PDTB v2.0 (Prasad et al., 2008). Experimental results (Section 6) show that re-anchoring yields significant performance advantages. More importantly, it is proven that the cooperation between re-anchoring and other auxiliary strategies (transfer learning and interactive attention mechanism) yields further improvements. 2 Variational AutoEncoder (VAE) CNN (Zhang and Wallace, 2016) to build VAE, which serves to predict µ and σ 2 conditioned on the input X. The decoder is a BiLSTM unit. 3 Three-stage Encoder Community As shown in Figure 1, we carry out a three-stage encoding process for arguments Arg 1 and Arg2. First, the arg"
2021.findings-emnlp.110,D16-1246,0,0.0374216,"Missing"
2021.findings-emnlp.110,P17-1093,0,0.0130932,"addition, we prove that re-anchoring can cooperate with other auxiliary strategies (transfer learning and interactive attention mechanism) to further improve the classification performance. Self-supervised training X R RoBERTa VAE MLP Condition Figure 1: The three-stage encoder community. et al., 2016; Shi et al., 2017, 2018). Others dug deeper into the existing data (instead of expanding it) to squeeze out additional salient features, where implicit connectives are speculated and annotated, and predicting them by machine is used as a supplementary task in a multi-task learning architecture (Qin et al., 2017; Shi and Demberg, 2019). In this paper, we attempt to enhance representation learning without using any external resources or artificially-created implicit connectives. We couple VAE (Kingma and Welling, 2014) with RoBERTa (Liu et al., 2019) and MLP to build a 1 Introduction three-stage encoder community (Figure 1). It is Implicit discourse relation classification is a task inspired by the ability of VAE in generating variof determining relationships between arguments ants (Section 2), and more importantly, the variants without connectives. We provide an example in cover a wider range of ling"
2021.findings-emnlp.110,2020.coling-main.282,1,0.842377,"(including [CLS], [Sep] and all words in Ar- sembles RoBERTa and MLP is taken as the baseline. We improve the baseline by the following guments, one token per timestep). VAE outputs auxiliary strategies: 1) coupling it with VAE (i.e., 256 768-dimensional vectors, which is used as a 256∗768 matrix. Such a matrix is fed into CNN, forming the three-stage encoder community); 2) conducting re-anchoring when applying VAE dura network comprising two groups of filters in the ing test; 3) additionally equipping VAE with insize of 2∗768 and 4∗ 768 respectively. Each group teractive attention mechanism (Ruan et al., 2020); contains 128 filters. Using CNN along with 2 linear 4) retraining RoBERTa by transfer learning (Nie FC layers, we convolute the input matrix into a pair et al., 2019) upon the explicit discourse relation of 128∗ 768 matrices (where, padding and dropout dataset (Prasad et al., 2008), and 5) employing all operations are used while pooling is not used), and the above strategies to form a cooperation model. concatenate them to form a 256∗ 768 matrix. We split the matrix into two 256∗384 submatrices. One We show the test results in Table 1. It can be obof them is used to represent the independent"
2021.findings-emnlp.110,N15-1081,0,0.0566181,"Missing"
2021.findings-emnlp.110,W19-0416,0,0.025252,"e that re-anchoring can cooperate with other auxiliary strategies (transfer learning and interactive attention mechanism) to further improve the classification performance. Self-supervised training X R RoBERTa VAE MLP Condition Figure 1: The three-stage encoder community. et al., 2016; Shi et al., 2017, 2018). Others dug deeper into the existing data (instead of expanding it) to squeeze out additional salient features, where implicit connectives are speculated and annotated, and predicting them by machine is used as a supplementary task in a multi-task learning architecture (Qin et al., 2017; Shi and Demberg, 2019). In this paper, we attempt to enhance representation learning without using any external resources or artificially-created implicit connectives. We couple VAE (Kingma and Welling, 2014) with RoBERTa (Liu et al., 2019) and MLP to build a 1 Introduction three-stage encoder community (Figure 1). It is Implicit discourse relation classification is a task inspired by the ability of VAE in generating variof determining relationships between arguments ants (Section 2), and more importantly, the variants without connectives. We provide an example in cover a wider range of linguistic phenomena. SpeApp"
2021.findings-emnlp.110,I17-1049,0,0.0125481,"AE) to estimate the risk of erroneous sampling. Moreover, we develop a re-anchoring method which migrates the anchor of sampling area of VAE to reduce the risk. The experiments on PDTB v2.0 demonstrate that, compared to the RoBERTabased baseline, re-anchoring yields substantial improvements. In addition, we prove that re-anchoring can cooperate with other auxiliary strategies (transfer learning and interactive attention mechanism) to further improve the classification performance. Self-supervised training X R RoBERTa VAE MLP Condition Figure 1: The three-stage encoder community. et al., 2016; Shi et al., 2017, 2018). Others dug deeper into the existing data (instead of expanding it) to squeeze out additional salient features, where implicit connectives are speculated and annotated, and predicting them by machine is used as a supplementary task in a multi-task learning architecture (Qin et al., 2017; Shi and Demberg, 2019). In this paper, we attempt to enhance representation learning without using any external resources or artificially-created implicit connectives. We couple VAE (Kingma and Welling, 2014) with RoBERTa (Liu et al., 2019) and MLP to build a 1 Introduction three-stage encoder communit"
2021.findings-emnlp.110,D16-1253,0,0.0389539,"Missing"
2021.findings-emnlp.110,D18-1079,1,0.82713,"e than et al., 2020). However, there is lack of labeled the less-sophisticated model that simply couples data for learning. To overcome the bottleneck, pre- RoBERTa with MLP. Data analysis shows that the vious studies have explored two classes of meth- main drawback is caused by erroneous sampling. ods. Some of them conducted data expansion us- The errors occur when VAE tends to produce quite ing PDTB explicit samples (Marcu and Echihabi, unusual variants (Section 4). To address the issue, 2002; Braud and Denis, 2014; Rutherford and Xue, we propose a re-anchoring strategy (Section 5) to 2015; Xu et al., 2018) and parallel corpora (Wu migrate potential variants from high-risk sampling ∗ Corresponding author areas to low-risk. Instead of VAE, CVAE (Sohn 1275 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1275–1283 November 7–11, 2021. ©2021 Association for Computational Linguistics (a). Sampling Area of VAE for X (b). Cause of Erroneous Sampling Bo da un Bo Sample Space (c). CVAE-based Re-anchoring r da un Others Others y ry SE (M Z ) Erroneous Sampling (mean) Anchor X Anchor X Strongly-associated sampling area Weakly-associated sampling area Relation R ry da un Bo R ry"
2021.findings-emnlp.110,D15-1266,0,0.0316119,"Missing"
2021.findings-emnlp.110,C10-2172,0,0.0480272,"ources or artificially-created implicit connectives. We couple VAE (Kingma and Welling, 2014) with RoBERTa (Liu et al., 2019) and MLP to build a 1 Introduction three-stage encoder community (Figure 1). It is Implicit discourse relation classification is a task inspired by the ability of VAE in generating variof determining relationships between arguments ants (Section 2), and more importantly, the variants without connectives. We provide an example in cover a wider range of linguistic phenomena. SpeAppendix A. Due to the omission of connectives cially, we utilize VAE to generate numerous vari(Zhou et al., 2010), classifying relations heavily re- ants for initial representations of arguments, and lies on recognizable representations of arguments. use them to challenge both RoBERTa and MLP Learning richer and diverse linguistic phenom- (Section 3). Ideally, this helps to make the encoder ena from a large number of samples (relation-aware community generalize well. argument pairs) helps to enhance encoding, proHowever, the use of VAE is proven ineffective ducing more recognizable representations (Ruan in our experiments. It performs much worse than et al., 2020). However, there is lack of labeled the l"
2021.findings-emnlp.113,2020.findings-emnlp.58,0,0.0170358,"compare three groups of manual annotations on DTUs with each other for kappa value calculation and report the average score. The data and codes are published at https://github. com/NLP-Discourse-SoochowU/DTCP. 3 Baseline Recent years have witnessed the great effects of pre-trained language models (Devlin et al., 2019; 1307 Corpus Train Dev. Test Doc. 313 34 38 Sent. 6352 740 870 Link 3260 403 459 Chain 1410 164 183 Method Bert-base Bert-large Human-level Link 89.5 91.7 94.2 Chain 78.9 82.1 89.1 Table 2: Statistic results for the datasets. Table 3: Baseline performance (F1). Yang et al., 2019; Cui et al., 2020) on natural language understanding. Following previous work, we introduce a Bert-based (Devlin et al., 2019) method in our baseline system. Given a discourse with k-1 DTUs, we use the pretrained Bert3 model to encode the entire discourse where each DTU is surrounded by the [CLS] and [SEP] tokens. And we take the Bert output corresponding to [CLS] as our DTU representation. Following previous work, we also fine-tuned the pre-trained language model parameters during the training process. For the convenience of calculation, a zero-initialized vector uz is added at the end of the DTU sequence for"
2021.findings-emnlp.113,N19-1423,0,0.00508734,"ins have less than 5 topic arcs. For supervised learning, we have divided the dataset into three parts (the test corpus is consist with that of RST-DT), as shown in Table 2. Based on the test corpus, we calculate the annotation consistency with an averaged Cohen’s kappa value of 0.72. Concretely, we compare three groups of manual annotations on DTUs with each other for kappa value calculation and report the average score. The data and codes are published at https://github. com/NLP-Discourse-SoochowU/DTCP. 3 Baseline Recent years have witnessed the great effects of pre-trained language models (Devlin et al., 2019; 1307 Corpus Train Dev. Test Doc. 313 34 38 Sent. 6352 740 870 Link 3260 403 459 Chain 1410 164 183 Method Bert-base Bert-large Human-level Link 89.5 91.7 94.2 Chain 78.9 82.1 89.1 Table 2: Statistic results for the datasets. Table 3: Baseline performance (F1). Yang et al., 2019; Cui et al., 2020) on natural language understanding. Following previous work, we introduce a Bert-based (Devlin et al., 2019) method in our baseline system. Given a discourse with k-1 DTUs, we use the pretrained Bert3 model to encode the entire discourse where each DTU is surrounded by the [CLS] and [SEP] tokens. And"
2021.findings-emnlp.113,W19-4103,0,0.0179654,"Since the DTC ing. In this research, we present our insight structure can provide relatively rich and low-noise on discourse-level topic chain (DTC) parsing information about certain topic aspects of articles, which aims at discovering new topics and investigating how these topics evolve over time it is meaningful for various NLP tasks like summawithin an article. To address the lack of data, rization (Perez-Beltrachini et al., 2019), document we contribute a new discourse corpus with similarity measuring (Gong et al., 2018), and reDTC-style dependency graphs annotated upon sponse generation (Dziri et al., 2019). news articles. In particular, we ensure the high In the literature, topic detection and tracking reliability of the corpus by utilizing a two-step (TDT) (Allan, 2002) is a research area most simiannotation strategy to build the data and filtering out the annotations with low confidence lar to DTC parsing which aims at identifying new scores. Based on the annotated corpus, we events and tracking how they change over time. introduce a simple yet robust system for auHowever, the events in the TDT task refer to haptomatic discourse-level topic chain parsing. penings at certain places and times w"
2021.findings-emnlp.113,P18-1218,0,0.0145192,"as discoursebe fundamental in natural language processlevel topic chain (DTC) parsing. Since the DTC ing. In this research, we present our insight structure can provide relatively rich and low-noise on discourse-level topic chain (DTC) parsing information about certain topic aspects of articles, which aims at discovering new topics and investigating how these topics evolve over time it is meaningful for various NLP tasks like summawithin an article. To address the lack of data, rization (Perez-Beltrachini et al., 2019), document we contribute a new discourse corpus with similarity measuring (Gong et al., 2018), and reDTC-style dependency graphs annotated upon sponse generation (Dziri et al., 2019). news articles. In particular, we ensure the high In the literature, topic detection and tracking reliability of the corpus by utilizing a two-step (TDT) (Allan, 2002) is a research area most simiannotation strategy to build the data and filtering out the annotations with low confidence lar to DTC parsing which aims at identifying new scores. Based on the annotated corpus, we events and tracking how they change over time. introduce a simple yet robust system for auHowever, the events in the TDT task refer"
2021.findings-emnlp.113,J97-1003,0,0.747861,"y on Sahlgren, 2020). In the literature, previous studies English DTC parsing usually uses unsupervised on topic modeling usually extract topics by intro- methods (Kim and Oh, 2011) to explore the strucducing latent variables for tokens for topic assign- ture and trends of important topics hidden within ing (Hofmann, 1999; Blei et al., 2003; Yishu et al., news articles. Obviously, one intractable problem 2017). Similarly, researches on text-tilling achieve facing DTC parsing is the lack of data. topic segments through lexical cohesion modelThis research is primarily motivated by (Polanyi ing (Hearst, 1997; Purver et al., 2006). Instead of and Scha, 1984; Kim and Oh, 2011) on the topic lexical cohesion measuring, Rahimi et al. (2015) chain concept, (Xi and Zhou, 2017) on DTC corput their attention on evaluating the organization pus construction, and (Reimers et al., 2019) on and cohesion of pieces of evidence and build topic topic-dependent argument linking. And our conchains on related text units. Besides, recent studies tributions mainly include two aspects: (i) building on argument mining explore to build links or clus- an English corpus of discourse-level topic chain ters for topic-dependen"
2021.findings-emnlp.113,2021.naacl-main.127,0,0.0242384,"uild links or clus- an English corpus of discourse-level topic chain ters for topic-dependent arguments (Wachsmuth (EDTC) through a two-step annotation method and et al., 2018; Shnarch et al., 2018; Reimers et al., (ii) lunching a simple but robust Bert-based base2019). Obviously, more and more researches show line system for automatic DTC parsing. Moreover, that there are certain structures among topic seg- as implied in recent researches on discourse rhetorments that deserve deeper exploration. ical structure (DRS) parsing (Zhang et al., 2020; In this work, we aim to explore the cohesion of Kobayashi et al., 2021; Zhang et al., 2021), distopic-related text segments. Different from Rahimi course parsing remains challenging due to the lack ∗ Corresponding author of data. Under this circumstance, we annotate the 1304 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1304–1312 November 7–11, 2021. ©2021 Association for Computational Linguistics U3 U4 U2 U5 Ă U-k U1 coronavirus international response Ă u1. The novel [coronavirus], now called [COVID-19], had not previously detected before the outbreak was reported in Wuhan, China in December 2019. u2. [It] has killed around 800 p"
2021.findings-emnlp.113,P19-1110,0,0.0165014,"tifying new scores. Based on the annotated corpus, we events and tracking how they change over time. introduce a simple yet robust system for auHowever, the events in the TDT task refer to haptomatic discourse-level topic chain parsing. penings at certain places and times which only compose a small subset of general topics. Recently, 1 Introduction Xi and Zhou (2017) manually annotate the first Topic information as a crucial auxiliary for text Chinese DTC corpus based on the theme-rheme understanding has drawn great attention in re- theory (Halliday and Matthiessen, 2004). By concent decades (Wu et al., 2019; Wang et al., 2020; trast, due to the lack of corpus, previous study on Sahlgren, 2020). In the literature, previous studies English DTC parsing usually uses unsupervised on topic modeling usually extract topics by intro- methods (Kim and Oh, 2011) to explore the strucducing latent variables for tokens for topic assign- ture and trends of important topics hidden within ing (Hofmann, 1999; Blei et al., 2003; Yishu et al., news articles. Obviously, one intractable problem 2017). Similarly, researches on text-tilling achieve facing DTC parsing is the lack of data. topic segments through lexical"
C04-1004,W97-0307,0,0.0329416,"el where a sequence of observations is generated in addition to the Markov state sequence. It is a latent variable model in the sense that only the observation sequence is known while the state sequence remains “hidden”. In recent years, HMMs have enjoyed great success in many tagging applications, most notably part-of-speech (POS) tagging (Church 1988; Weischedel et al 1993; Merialdo 1994) and named entity recognition (Bikel et al 1999; Zhou et al 2002). Moreover, there have been also efforts to extend the use of HMMs to word sense disambiguation (Segond et al 1997) and shallow/full parsing (Brants et al 1997; Skut et al 1998; Zhou et al 2000). Traditionally, a HMM segments and labels sequential data in a generative way, assigning a joint probability to paired observation and state sequences. More formally, a generative (first-order) HMM (GHMM) is given by a finite set of states S including an designated initial state and an designated final state, a set of possible observation O , two conditional probability distributions: a state transition model from s ' to s , p ( s |s ' ) for s ' , s ∈ S and an output model, p (o |s ) for o ∈ O, s ∈ S . A sequence of observations is generated by starting from"
C04-1004,J94-2001,0,0.0234949,"s GHMMs but also much outperforms other DHMMs. This suggests that the LSD-DHMM can effectively capture the long context dependence to segment and label sequential data. 1. Introduction A Hidden Markov Model (HMM) is a model where a sequence of observations is generated in addition to the Markov state sequence. It is a latent variable model in the sense that only the observation sequence is known while the state sequence remains “hidden”. In recent years, HMMs have enjoyed great success in many tagging applications, most notably part-of-speech (POS) tagging (Church 1988; Weischedel et al 1993; Merialdo 1994) and named entity recognition (Bikel et al 1999; Zhou et al 2002). Moreover, there have been also efforts to extend the use of HMMs to word sense disambiguation (Segond et al 1997) and shallow/full parsing (Brants et al 1997; Skut et al 1998; Zhou et al 2000). Traditionally, a HMM segments and labels sequential data in a generative way, assigning a joint probability to paired observation and state sequences. More formally, a generative (first-order) HMM (GHMM) is given by a finite set of states S including an designated initial state and an designated final state, a set of possible observation"
C04-1004,W97-0811,0,0.0122816,"oduction A Hidden Markov Model (HMM) is a model where a sequence of observations is generated in addition to the Markov state sequence. It is a latent variable model in the sense that only the observation sequence is known while the state sequence remains “hidden”. In recent years, HMMs have enjoyed great success in many tagging applications, most notably part-of-speech (POS) tagging (Church 1988; Weischedel et al 1993; Merialdo 1994) and named entity recognition (Bikel et al 1999; Zhou et al 2002). Moreover, there have been also efforts to extend the use of HMMs to word sense disambiguation (Segond et al 1997) and shallow/full parsing (Brants et al 1997; Skut et al 1998; Zhou et al 2000). Traditionally, a HMM segments and labels sequential data in a generative way, assigning a joint probability to paired observation and state sequences. More formally, a generative (first-order) HMM (GHMM) is given by a finite set of states S including an designated initial state and an designated final state, a set of possible observation O , two conditional probability distributions: a state transition model from s ' to s , p ( s |s ' ) for s ' , s ∈ S and an output model, p (o |s ) for o ∈ O, s ∈ S . A sequence o"
C04-1004,W00-1309,1,0.887287,"Missing"
C04-1004,P02-1060,1,0.9033,"Missing"
C04-1004,J93-2004,0,\N,Missing
C04-1004,J93-2006,0,\N,Missing
C04-1004,A88-1019,0,\N,Missing
C04-1014,J92-4003,0,0.0680619,"Missing"
C04-1014,H90-1056,0,0.162874,"Missing"
C04-1014,J93-1005,0,0.0141327,"Missing"
C04-1014,P98-2239,1,0.877472,"Missing"
C04-1014,C98-2234,1,\N,Missing
C04-1033,P95-1017,0,0.602569,"d coreferential clusters. Compared with individual NPs, coreferential clusters could provide richer information of the entities for better rules learning and reference determination. The evaluation done on MEDLINE data set shows that our approach outperforms the baseline NP-NP based approach in both recall and precision. 1 Introduction Coreference resolution is the process of linking as a cluster1 multiple expressions which refer to the same entities in a document. In recent years, supervised machine learning approaches have been applied to this problem and achieved considerable success (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002b)). The main idea of most supervised learning approaches is to recast this task as a binary classification problem. Specifically, a classifier is learned and then used to determine whether or not two NPs in a document are co-referring. Clusters are formed by linking coreferential NP pairs according to a certain selection strategy. In this way, the identification of coreferential clusters in text is reduced to the identification of coreferential NP pairs. One problem of such reduction, however, is that the individual NP usua"
C04-1033,P98-1012,0,0.0814212,"9) have proposed an unsupervised approach which also incorporates cluster information into consideration. Their approach uses hard constraints to preclude the link of an NP to a cluster mismatching the number, gender or semantic agreements, while our approach takes these agreements together with other features (e.g. cluster-length, string-matching degree,etc) as preference factors for cluster selection. Besides, the idea of clustering can be seen in the research of cross-document coreference, where NPs with high context similarity would be chained together based on certain clustering methods (Bagga and Biermann, 1998; Gooi and Allan, 2004). 6 Conclusion In this paper we have proposed a supervised learning-based approach to coreference resolution. Rather than mining the coreferential relationship between NP pairs as in conventional approaches, our approach does resolution by exploring the relationships between an NP and the coreferential clusters. Compared to individual NPs, coreferential clusters provide more information for rules learning and reference determination. In the paper, we first introduced the conventional NP-NP based approach and analyzed its limitation. Then we described in details the frame"
C04-1033,W99-0611,0,0.0941913,"ll and precision. 2. Cluster StrSim (f23 ) is the most effective as it contributes most to the system performance. Simply using this feature boosts 5 Related work To our knowledge, our work is the first supervised-learning based attempt to do coreference resolution by exploring the relationship between an NP and coreferential clusters. In the heuristic salience-based algorithm for pronoun resolution, Lappin and Leass (1994) introduce a procedure for identifying anaphorically linked NP as a cluster for which a global salience value is computed as the sum of the salience values of its elements. Cardie and Wagstaff (1999) have proposed an unsupervised approach which also incorporates cluster information into consideration. Their approach uses hard constraints to preclude the link of an NP to a cluster mismatching the number, gender or semantic agreements, while our approach takes these agreements together with other features (e.g. cluster-length, string-matching degree,etc) as preference factors for cluster selection. Besides, the idea of clustering can be seen in the research of cross-document coreference, where NPs with high context similarity would be chained together based on certain clustering methods (Ba"
C04-1033,N04-1002,0,0.0128346,"rvised approach which also incorporates cluster information into consideration. Their approach uses hard constraints to preclude the link of an NP to a cluster mismatching the number, gender or semantic agreements, while our approach takes these agreements together with other features (e.g. cluster-length, string-matching degree,etc) as preference factors for cluster selection. Besides, the idea of clustering can be seen in the research of cross-document coreference, where NPs with high context similarity would be chained together based on certain clustering methods (Bagga and Biermann, 1998; Gooi and Allan, 2004). 6 Conclusion In this paper we have proposed a supervised learning-based approach to coreference resolution. Rather than mining the coreferential relationship between NP pairs as in conventional approaches, our approach does resolution by exploring the relationships between an NP and the coreferential clusters. Compared to individual NPs, coreferential clusters provide more information for rules learning and reference determination. In the paper, we first introduced the conventional NP-NP based approach and analyzed its limitation. Then we described in details the framework of our NP-Cluster"
C04-1033,N01-1008,0,0.0893674,"Missing"
C04-1033,J94-4002,0,0.569485,"t, the Best-First strategy was applied. As illustrated in the table, we could observe that: 1. Without the three features, the system is equivalent to the baseline system in terms of the same recall and precision. 2. Cluster StrSim (f23 ) is the most effective as it contributes most to the system performance. Simply using this feature boosts 5 Related work To our knowledge, our work is the first supervised-learning based attempt to do coreference resolution by exploring the relationship between an NP and coreferential clusters. In the heuristic salience-based algorithm for pronoun resolution, Lappin and Leass (1994) introduce a procedure for identifying anaphorically linked NP as a cluster for which a global salience value is computed as the sum of the salience values of its elements. Cardie and Wagstaff (1999) have proposed an unsupervised approach which also incorporates cluster information into consideration. Their approach uses hard constraints to preclude the link of an NP to a cluster mismatching the number, gender or semantic agreements, while our approach takes these agreements together with other features (e.g. cluster-length, string-matching degree,etc) as preference factors for cluster selecti"
C04-1033,W02-1008,0,0.0936705,"ters could provide richer information of the entities for better rules learning and reference determination. The evaluation done on MEDLINE data set shows that our approach outperforms the baseline NP-NP based approach in both recall and precision. 1 Introduction Coreference resolution is the process of linking as a cluster1 multiple expressions which refer to the same entities in a document. In recent years, supervised machine learning approaches have been applied to this problem and achieved considerable success (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002b)). The main idea of most supervised learning approaches is to recast this task as a binary classification problem. Specifically, a classifier is learned and then used to determine whether or not two NPs in a document are co-referring. Clusters are formed by linking coreferential NP pairs according to a certain selection strategy. In this way, the identification of coreferential clusters in text is reduced to the identification of coreferential NP pairs. One problem of such reduction, however, is that the individual NP usually lacks adequate descriptive information of its referred entity. Con"
C04-1033,P02-1014,0,0.267893,"ters could provide richer information of the entities for better rules learning and reference determination. The evaluation done on MEDLINE data set shows that our approach outperforms the baseline NP-NP based approach in both recall and precision. 1 Introduction Coreference resolution is the process of linking as a cluster1 multiple expressions which refer to the same entities in a document. In recent years, supervised machine learning approaches have been applied to this problem and achieved considerable success (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002b)). The main idea of most supervised learning approaches is to recast this task as a binary classification problem. Specifically, a classifier is learned and then used to determine whether or not two NPs in a document are co-referring. Clusters are formed by linking coreferential NP pairs according to a certain selection strategy. In this way, the identification of coreferential clusters in text is reduced to the identification of coreferential NP pairs. One problem of such reduction, however, is that the individual NP usually lacks adequate descriptive information of its referred entity. Con"
C04-1033,W03-1307,1,0.587032,"Missing"
C04-1033,J01-4004,0,0.870843,", coreferential clusters could provide richer information of the entities for better rules learning and reference determination. The evaluation done on MEDLINE data set shows that our approach outperforms the baseline NP-NP based approach in both recall and precision. 1 Introduction Coreference resolution is the process of linking as a cluster1 multiple expressions which refer to the same entities in a document. In recent years, supervised machine learning approaches have been applied to this problem and achieved considerable success (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002b)). The main idea of most supervised learning approaches is to recast this task as a binary classification problem. Specifically, a classifier is learned and then used to determine whether or not two NPs in a document are co-referring. Clusters are formed by linking coreferential NP pairs according to a certain selection strategy. In this way, the identification of coreferential clusters in text is reduced to the identification of coreferential NP pairs. One problem of such reduction, however, is that the individual NP usually lacks adequate descriptive information of its"
C04-1033,W02-1040,0,0.120755,"ion 4 reports and discusses the experimental results. Section 5 describes related research work. Finally, conclusion is given in Section 6. 2 Baseline: the NP-NP based approach 2.1 Framework description We built a baseline coreference resolution system, which adopts the common NP-NP based learning framework as employed in (Soon et al., 2001). Each instance in this approach takes the form of i {NPj , NPi }, which is associated with a feature vector consisting of 18 features (f1 ∼ f18 ) as described in Table 2. Most of the features come from Soon et al. (2001)’s system. Inspired by the work of (Strube et al., 2002) and (Yang et al., 2004), we use two features, StrSim1 (f17 ) and StrSim2 (f18 ), to measure the string-matching degree of NPj and NPi . Given the following similarity function: Str Simlarity(Str1 , Str2 ) = 100 × |Str1 ∩ Str2 | Str1 StrSim1 and StrSim2 are computed using Str Similarity(SN Pj , SN Pi ) and Str Similarity(SN Pi , SN Pj ), respectively. Here SN P is the token list of NP, which is obtained by applying word stemming, stopword removal and acronym expansion to the original string as described in Yang et al. (2004)’s work. During training, for each anaphor NPj in a given text, a posi"
C04-1033,M95-1005,0,0.468812,"Missing"
C04-1033,P02-1060,1,0.804783,"Missing"
C04-1033,C98-1012,0,\N,Missing
C04-1075,P87-1022,0,0.660741,"Missing"
C04-1075,C88-1021,0,0.40473,"nformation extraction and question answering. In particular, information extraction systems like those built in the DARPA Message Understanding Conferences (MUC) have revealed that coreference resolution is such a crucial component of an information extraction system that a separate coreference task has been defined and evaluated in MUC-6 (1995) and MUC-7 (1998). There is a long tradition of work on coreference resolution within computational linguistics. Many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988). However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpusbased NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology). Approaches to coreference resolution usually rely on a set of factors which include gender and number agreements, c-command co"
C04-1075,C90-3063,0,0.0314584,"e revealed that coreference resolution is such a crucial component of an information extraction system that a separate coreference task has been defined and evaluated in MUC-6 (1995) and MUC-7 (1998). There is a long tradition of work on coreference resolution within computational linguistics. Many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988). However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpusbased NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology). Approaches to coreference resolution usually rely on a set of factors which include gender and number agreements, c-command constraints, semantic consistency, syntactic parallelism, semantic parallelism, salience, proximity, etc. These factors can be either “constraints” which discard"
C04-1075,J86-3001,0,0.0235999,"nts, c-command constraints, semantic consistency), or “preferences” which gives more preference to certain candidates and less to others (such as syntactic parallelism, semantic parallelism, salience, proximity). While a number of approaches use a similar set of factors, the computational strategies (the way antecedents are determined, i.e. the algorithm and formula for assigning antecedents) may differ, i.e. from simple co-occurrence rules (Dagan and Itai 1990) to decision trees (Soon, Ng and Lim 2001; Ng and Cardie 2002) to pattern induced rules (Ng and Cardie 2002) to centering algorithms (Grosz and Sidner 1986; Brennan, Friedman and Pollard 1987; Strube 1998; Tetreault 2001). This paper proposes a simple constraint-based multi-agent system to coreference resolution of general noun phrases in unrestricted English text. For a given anaphor and all the preceding referring expressions as the antecedent candidates, a common constraint agent is first presented to filter out invalid antecedent candidates using various kinds of general knowledge. Then, according to the type of the anaphor, a special constraint agent is proposed to filter out more invalid antecedent candidates using constraints which are de"
C04-1075,P98-2143,0,0.0616325,"ch a crucial component of an information extraction system that a separate coreference task has been defined and evaluated in MUC-6 (1995) and MUC-7 (1998). There is a long tradition of work on coreference resolution within computational linguistics. Many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988). However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpusbased NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology). Approaches to coreference resolution usually rely on a set of factors which include gender and number agreements, c-command constraints, semantic consistency, syntactic parallelism, semantic parallelism, salience, proximity, etc. These factors can be either “constraints” which discard invalid ones from the set of possib"
C04-1075,P02-1014,0,0.718957,"tion extraction system that a separate coreference task has been defined and evaluated in MUC-6 (1995) and MUC-7 (1998). There is a long tradition of work on coreference resolution within computational linguistics. Many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988). However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpusbased NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology). Approaches to coreference resolution usually rely on a set of factors which include gender and number agreements, c-command constraints, semantic consistency, syntactic parallelism, semantic parallelism, salience, proximity, etc. These factors can be either “constraints” which discard invalid ones from the set of possible candidates (such as gender and number agr"
C04-1075,A88-1003,0,0.301296,"n, text summarization, information extraction and question answering. In particular, information extraction systems like those built in the DARPA Message Understanding Conferences (MUC) have revealed that coreference resolution is such a crucial component of an information extraction system that a separate coreference task has been defined and evaluated in MUC-6 (1995) and MUC-7 (1998). There is a long tradition of work on coreference resolution within computational linguistics. Many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988). However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpusbased NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology). Approaches to coreference resolution usually rely on a set of factors which include gender and numb"
C04-1075,J01-4004,0,0.540354,"Missing"
C04-1075,P98-2204,0,0.0232425,"erences” which gives more preference to certain candidates and less to others (such as syntactic parallelism, semantic parallelism, salience, proximity). While a number of approaches use a similar set of factors, the computational strategies (the way antecedents are determined, i.e. the algorithm and formula for assigning antecedents) may differ, i.e. from simple co-occurrence rules (Dagan and Itai 1990) to decision trees (Soon, Ng and Lim 2001; Ng and Cardie 2002) to pattern induced rules (Ng and Cardie 2002) to centering algorithms (Grosz and Sidner 1986; Brennan, Friedman and Pollard 1987; Strube 1998; Tetreault 2001). This paper proposes a simple constraint-based multi-agent system to coreference resolution of general noun phrases in unrestricted English text. For a given anaphor and all the preceding referring expressions as the antecedent candidates, a common constraint agent is first presented to filter out invalid antecedent candidates using various kinds of general knowledge. Then, according to the type of the anaphor, a special constraint agent is proposed to filter out more invalid antecedent candidates using constraints which are derived from various kinds of special knowledge. Fi"
C04-1075,J01-4003,0,0.224416,"h gives more preference to certain candidates and less to others (such as syntactic parallelism, semantic parallelism, salience, proximity). While a number of approaches use a similar set of factors, the computational strategies (the way antecedents are determined, i.e. the algorithm and formula for assigning antecedents) may differ, i.e. from simple co-occurrence rules (Dagan and Itai 1990) to decision trees (Soon, Ng and Lim 2001; Ng and Cardie 2002) to pattern induced rules (Ng and Cardie 2002) to centering algorithms (Grosz and Sidner 1986; Brennan, Friedman and Pollard 1987; Strube 1998; Tetreault 2001). This paper proposes a simple constraint-based multi-agent system to coreference resolution of general noun phrases in unrestricted English text. For a given anaphor and all the preceding referring expressions as the antecedent candidates, a common constraint agent is first presented to filter out invalid antecedent candidates using various kinds of general knowledge. Then, according to the type of the anaphor, a special constraint agent is proposed to filter out more invalid antecedent candidates using constraints which are derived from various kinds of special knowledge. Finally, a simple p"
C04-1075,W00-1309,1,0.873037,"Missing"
C04-1075,P02-1060,1,0.838301,"Missing"
C04-1075,J94-4002,0,\N,Missing
C04-1075,C98-2138,0,\N,Missing
C04-1075,C98-2199,0,\N,Missing
C08-1088,P04-1054,0,0.295638,"rmance and competitive efficiency by transforming a relation example into a set of syntactic and semantic features, such as lexical knowledge, entity-related information, syntactic parse trees and deep semantic information. However, detailed research (Zhou et al., 2005) shows that it’s difficult to extract new effective features to further improve the extraction accuracy. Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction. The key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances. While kernel methods using the dependency tree (Culotta and Sorensen, 2004) and the shortest dependency path (Bunescu and Mooney, 2005) suffer from low recall performance, convolution tree kernels (Zhang et al., 2006; Zhou et al., 2007) over syntactic parse trees achieve"
C08-1088,P04-1043,0,0.0215852,"ty nodes, or directly combined with the entity nodes as in Figure 1. However, detailed evaluation (Qian et al., 2007) indicates that the UPST achieves the best performance when the feature nodes are attached under the top node. Hence, we also attach three kinds of entity-related semantic trees (i.e. BOF, FPT and EPT) under the top node of the DSPT right after its original children. Thereafter, we employ the standard CTK (Collins and Duffy, 2001) to compute the similarity between two UPSTs, since this CTK and its variations are successfully applied in syntactic parsing, semantic role labeling (Moschitti, 2004) and relation extraction (Zhang et al., 2006; Zhou et al., 2007) as well. 5 Experimentation This section will evaluate the effectiveness of the DSPT and the contribution of entity-related semantic information through experiments. 5.1 Experimental Setting For evaluation, we use the ACE RDC 2004 corpus as the benchmark data. This data set contains 451 documents and 5702 relation instances. It defines 7 entity types, 7 major relation types and 23 subtypes. For comparison with previous work, evaluation is done on 347 (nwire/bnews) documents and 4307 relation instances using 5-fold cross-validation"
C08-1088,Y07-1043,1,0.375439,"stituents outside the linking path should be removed) and CS-CSPT (Zhou et al., 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. 4 Entity-related Semantic Tree Entity semantic features, such as entity headword, entity type and subtype etc., impose a strong constraint on relation types in terms of relation definition by the ACE RDC task. Experiments by Zhang et al. (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. Qian et al. (2007) further indicates that among these entity features, entity type, subtype, and mention type, as well as the base form of predicate verb, contribute most while the contribution of other features, such as entity class, headword and GPE role, can be ignored. In order to effectively capture entity-related semantic features, and their combined features as well, especially bi-gram or tri-gram features, we build an Entity-related Semantic Tree (EST) in three ways as illustrated in Figure 2. In the example sentence “they ’re here”, which is excerpted from the ACE RDC 2004 corpus, there exists a relati"
C08-1088,J03-4003,0,0.042424,"ancestor of the two entities under consideration) as the representation of each relation instance, along the path connecting two entities, the head child of every node is found according to various constituent dependencies. Then the path nodes and their head children are kept while any other nodes are removed from the tree. Eventually we arrive at a tree called Dynamic Syntactic Parse Tree (DSPT), which is dynamically determined by constituent dependencies and only contains necessary information as expected. There exist a considerable number of constituent dependencies in CFG as described by Collins (2003). However, since our task is to extract the relationship between two named entities, our focus is on how to condense Noun-Phrases (NPs) and other useful constituents for relation extraction. Therefore constituent dependencies can be classified according to constituent types of the CFG rules: 699 entities. In the contrast, our DSPT can avoid this problem by keeping the constituent “’s” and the headword “plants”. (2) Modification to NPs: except base-NPs, other modification to NPs can be classified into this type. Usually these NPs are recursive, meaning that they contain another NP as their chil"
C08-1088,P06-1104,1,0.709373,"set of syntactic and semantic features, such as lexical knowledge, entity-related information, syntactic parse trees and deep semantic information. However, detailed research (Zhou et al., 2005) shows that it’s difficult to extract new effective features to further improve the extraction accuracy. Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction. The key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances. While kernel methods using the dependency tree (Culotta and Sorensen, 2004) and the shortest dependency path (Bunescu and Mooney, 2005) suffer from low recall performance, convolution tree kernels (Zhang et al., 2006; Zhou et al., 2007) over syntactic parse trees achieve comparable or even better performance than feature-based methods."
C08-1088,P05-1052,0,0.0469553,"lation extraction. Zhou et al. (2007) point out that both SPT and the convolution tree kernel are context-free. They expand SPT to CS-SPT by dynamically including necessary predicate-linked path information and extending the standard CTK to contextsensitive CTK, obtaining the F-measure of 73.2 on the 7 relation types of the ACE RDC 2004 corpus. However, the CS-SPT only recovers part of contextual information and may contain noisy information as much as SPT. In order to fully utilize the advantages of feature-based methods and kernel-based methods, researchers turn to composite kernel methods. Zhao and Grishman (2005) define several feature-based composite kernels to capture diverse linguistic knowledge and achieve the F-measure of 70.4 on the 7 relation types in the ACE RDC 2004 corpus. Zhang et al. (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. Zhou et al. (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. It achieves the so far best F-measure of 75.8 on the 7 relation types in the ACE RDC 2004 corpus. In this paper, we"
C08-1088,P05-1053,1,0.901875,"accuracy in state-of-the-art syntactic and semantic parsing, reliably extracting semantic relationships between named entities in natural language documents is still a difficult, unresolved problem. In the literature, feature-based methods have dominated the research in semantic relation extraction. Featured-based methods achieve promising performance and competitive efficiency by transforming a relation example into a set of syntactic and semantic features, such as lexical knowledge, entity-related information, syntactic parse trees and deep semantic information. However, detailed research (Zhou et al., 2005) shows that it’s difficult to extract new effective features to further improve the extraction accuracy. Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction. The key problem for kernel methods on relation extraction is how to repres"
C08-1088,D07-1076,1,0.7458,"d semantic features, such as lexical knowledge, entity-related information, syntactic parse trees and deep semantic information. However, detailed research (Zhou et al., 2005) shows that it’s difficult to extract new effective features to further improve the extraction accuracy. Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly. From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction. The key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances. While kernel methods using the dependency tree (Culotta and Sorensen, 2004) and the shortest dependency path (Bunescu and Mooney, 2005) suffer from low recall performance, convolution tree kernels (Zhang et al., 2006; Zhou et al., 2007) over syntactic parse trees achieve comparable or even better performance than feature-based methods. However, there still"
C08-1088,P01-1017,0,\N,Missing
C08-1088,H05-1091,0,\N,Missing
C10-1068,P09-1074,0,0.0807521,"r D-DSPT-based anaphoricity determination module on coreference resolution by including 605 phoricity determination, with D-DSPT-based anaphoricity determination and. with golden anaphoricity determination. Table 3 shows that: 1) There is a performance gap of 6.4, 6.1 and 7.0 in F1-measure on the NWIRE, NPAPER and BNEWS domain, respectively, between the coreference resolution system with golden anaphoricity determination and the baseline system without anaphoricity determination. This suggests the usefulness of proper anaphoricity determination in coreference resolution. This also agrees with Stoyanov et al. (2009) which measured the impact of golden anaphoricity determination on coreference resolution using only the annotated anaphors in both training and testing. 2) Compared to the baseline system without anaphoricity determination, the D-DSPT-based anaphoricity determination module improves the performance by 4.1(***), 3.9(***) and 5.0(***) to 63.2, 67.4 and 61.6 in F1-measure on the NWIRE, NPAPER and BNEWS domains, respectively, due to a large gain in precision and a much smaller drop in recall. In addition, D-DSPT-based anaphoricity determination can not only much improve the performance of corefer"
C10-1068,P03-1023,1,0.861855,"roduction Coreference resolution aims to identify which noun phrases (NPs, or mentions) refer to the same real-world entity in a text. According to Webber (1979), coreference resolution can be decomposed into two complementary sub-tasks: (1) anaphoricity determination, determining whether a given NP is anaphoric or not; and (2) anaphor resolution, linking together multiple mentions of a given entity in the world. Although machine learning approaches have performed reasonably well in coreference resolution without explicit anaphoricity determination (e.g. Soon et al. 2001; Ng and Cardie 2002b; Yang et al. 2003, 2008; Kong et al. 2009), knowledge of NP anaphoricity is expected to much improve the performance of a coreference resolution system, since a * non-anaphoric NP does not have an antecedent and therefore does not need to be resolved. Recently, anaphoricity determination has been drawing more and more attention. One common approach involves the design of some heuristic rules to identify specific types of non-anaphoric NPs, such as pleonastic it (e.g. Paice and Husk 1987; Lappin and Leass 1994, Kennedy and Boguraev 1996; Denber 1998) and definite descriptions (e.g. Vieira and Poesio 2000). Alte"
C10-1068,P05-1052,0,0.0192837,"anning 2008; Zhou and Kong 2009). As a representative, Zhou and Kong (2009) directly employ a tree kernel-based method to automatically mine the non-anaphoric information embedded in the syntactic parse tree. One main advantage of the kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structured information in various NLP applications like syntactic parsing (Collins and Duffy, 2001; Moschitti, 2004), semantic relation extraction (Zelenko et al., 2003; Zhao and Grishman, 2005; Zhou et al. 2007; Qian et al., 2008), semantic role labeling (Moschitti, 2004); coreference resolution (Yang et al., 2006; Zhou et al., 2008). One of the key problems for the kernel-based methods is how to effectively capture the structured information according to the nature of the structured object in the specific task. This paper advances the state-of-the-art performance in anaphoricity determination by efCorresponding author 599 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 599–607, Beijing, August 2010 fectively capturing the structur"
C10-1068,D07-1076,1,0.932713,"g 2009). As a representative, Zhou and Kong (2009) directly employ a tree kernel-based method to automatically mine the non-anaphoric information embedded in the syntactic parse tree. One main advantage of the kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structured information in various NLP applications like syntactic parsing (Collins and Duffy, 2001; Moschitti, 2004), semantic relation extraction (Zelenko et al., 2003; Zhao and Grishman, 2005; Zhou et al. 2007; Qian et al., 2008), semantic role labeling (Moschitti, 2004); coreference resolution (Yang et al., 2006; Zhou et al., 2008). One of the key problems for the kernel-based methods is how to effectively capture the structured information according to the nature of the structured object in the specific task. This paper advances the state-of-the-art performance in anaphoricity determination by efCorresponding author 599 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 599–607, Beijing, August 2010 fectively capturing the structured syntactic infor"
C10-1068,P99-1048,0,0.275587,"ove the performance of a coreference resolution system, since a * non-anaphoric NP does not have an antecedent and therefore does not need to be resolved. Recently, anaphoricity determination has been drawing more and more attention. One common approach involves the design of some heuristic rules to identify specific types of non-anaphoric NPs, such as pleonastic it (e.g. Paice and Husk 1987; Lappin and Leass 1994, Kennedy and Boguraev 1996; Denber 1998) and definite descriptions (e.g. Vieira and Poesio 2000). Alternatively, some studies focus on using statistics to tackle this problem (e.g., Bean and Riloff 1999; Bergsma et al. 2008) and others apply machine learning approaches (e.g. Evans 2001;Ng and Cardie 2002a, 2004,2009; Yang et al. 2005; Denis and Balbridge 2007; Luo 2007; Finkel and Manning 2008; Zhou and Kong 2009). As a representative, Zhou and Kong (2009) directly employ a tree kernel-based method to automatically mine the non-anaphoric information embedded in the syntactic parse tree. One main advantage of the kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully app"
C10-1068,W05-0612,0,0.135509,"related tasks. Section 3 presents our dependency-driven scheme to determine the syntactic parse tree structure. Section 4 reports the experimental results. Finally, we conclude our work in Section 5. 2 Related Work This section briefly overviews the related work on both anaphoricity determination and exploring syntactic parse tree structures. 2.1 Anaphoricity Determination Previous work on anaphoricity determination can be broadly divided into three categories: heuristic rule-based (e.g. Paice and Husk 1987;Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998; Vieira and Poesio 2000; Cherry and Bergsma 2005), statistics-based (e.g. Bean and Riloff 1999; Cherry and Bergsma 2005; Bergsma et al. 2008) and learning-based methods (e.g. Evans 2001; Ng and Cardie 2002a; Ng 2004; Yang et al. 2005; Denis and Balbridge 2007; Luo 2007; Finkel and Manning 2008; Zhou and Kong 2009; Ng 2009). The heuristic rule-based methods focus on designing some heuristic rules to identify specific types of non-anaphoric NPs. Representative work includes: Paice and Husk (1987), Lappin and Leass (1994) and Kennedy and Boguraev (1996). For example, Kennedy and Boguraev (1996) looked for modal adjectives (e.g. “necessary”) or"
C10-1068,N07-1030,0,0.104068,"Missing"
C10-1068,P04-1020,0,0.0697443,"rk in Section 5. 2 Related Work This section briefly overviews the related work on both anaphoricity determination and exploring syntactic parse tree structures. 2.1 Anaphoricity Determination Previous work on anaphoricity determination can be broadly divided into three categories: heuristic rule-based (e.g. Paice and Husk 1987;Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998; Vieira and Poesio 2000; Cherry and Bergsma 2005), statistics-based (e.g. Bean and Riloff 1999; Cherry and Bergsma 2005; Bergsma et al. 2008) and learning-based methods (e.g. Evans 2001; Ng and Cardie 2002a; Ng 2004; Yang et al. 2005; Denis and Balbridge 2007; Luo 2007; Finkel and Manning 2008; Zhou and Kong 2009; Ng 2009). The heuristic rule-based methods focus on designing some heuristic rules to identify specific types of non-anaphoric NPs. Representative work includes: Paice and Husk (1987), Lappin and Leass (1994) and Kennedy and Boguraev (1996). For example, Kennedy and Boguraev (1996) looked for modal adjectives (e.g. “necessary”) or cognitive verbs (e.g. “It is 600 thought that…” in a set of patterned constructions) in identifying pleonastic it. Among the statistics-based methods, Bean and Riloff"
C10-1068,N07-1010,0,\N,Missing
C10-1068,N09-1065,0,\N,Missing
C10-1068,D09-1102,1,\N,Missing
C10-1068,D09-1103,1,\N,Missing
C10-1068,C02-1139,0,\N,Missing
C10-1068,J08-3002,0,\N,Missing
C10-1068,D09-1133,1,\N,Missing
C10-1068,P06-1006,0,\N,Missing
C10-1068,P02-1060,1,\N,Missing
C10-1068,J94-4002,0,\N,Missing
C10-1068,I05-1063,0,\N,Missing
C10-1068,P08-1002,0,\N,Missing
C10-1068,P02-1014,0,\N,Missing
C10-1068,J01-4004,0,\N,Missing
C10-1068,C08-1088,1,\N,Missing
C10-1068,I08-1004,1,\N,Missing
C10-1068,P06-1104,1,\N,Missing
C10-1072,P07-1056,0,0.455354,"Missing"
C10-1072,C04-1200,0,0.209598,"selection method is presented to automatically generate the labeled training data for polarity shifting detection of sentences. The remainder of this paper is organized as follows. Section 2 introduces the related work of sentiment classification. Section 3 presents our approach in details. Experimental results are presented and analyzed in Section 4. Finally, 636 Section 5 draws the conclusion and outlines the future work. 2 Related Work Generally, sentiment classification can be performed at four different levels: word level (Wiebe, 2000), phrase level (Wilson et al., 2009), sentence level (Kim and Hovy, 2004; Liu et al., 2005), and document level (Turney, 2002; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006). This paper focuses on document-level sentiment classification. In the literature, there are mainly two kinds of approaches on document-level sentiment classification: term-counting approaches (lexicon-based) and machine learning approaches (corpus-based). Term-counting approaches usually involve deriving a sentiment measure by calculating the total number of negative and positive terms (Turney, 2002; Kim and Hovy, 2004; Kennedy and Inkpen, 2006). Machine learning approaches recas"
C10-1072,P08-2065,1,0.901692,"Missing"
C10-1072,W02-1011,0,0.0411334,"shifting detection of sentences. Then, by using the obtained binary classifier, each document in the original polarity classification training data is split into two partitions, polarity-shifted and polarity-unshifted, which are used to train two base classifiers respectively for further classifier combination. The experimental results across four different domains demonstrate the effectiveness of our approach. 1 Introduction Sentiment classification is a special task of text classification whose objective is to classify a text according to the sentimental polarities of opinions it contains (Pang et al., 2002), e.g., favorable or unfavorable, positive or negative. This task has received considerable interests in the computational linguistic community due to its potential applications. In the literature, machine learning approaches have dominated the research in sentiment classification and achieved the state-of-the-art performance (e.g., Kennedy and Inkpen, 2006; ‡ Natural Language Processing Lab School of Computer Science and Technology Soochow University gdzhou@suda.edu.cn Pang et al., 2002). In a typical machine learning approach, a document (text) is modeled as a bag-of-words, i.e. a set of con"
C10-1072,P02-1053,0,0.02264,"e sentimental orientation of the whole text depends on the sum of the sentimental polarities of content words. Although this assumption is reasonable and has led to initial success, it is linguistically unsound since many function words and constructions can shift the sentimental polarities of a text. For example, in the sentence ‘The chair is not comfortable’, the polarity of the word ‘comfortable’ is positive while the polarity of the whole sentence is reversed because of the negation word ‘not’. Therefore, the overall sentiment of a document is not necessarily the sum of the content parts (Turney, 2002). This phenomenon is one main reason why machine learning approaches fail under some circumstances. As a typical case of polarity shifting, negation has been paid close attention and widely studied in the literature (Na et al., 2004; Wilson et al., 2009; Kennedy and Inkpen, 2006). Generally, there are two steps to incorporate negation information into a system: negation detection and negation classification. For negation detection, some negation trigger words, such as ‘no’, ‘not’, and ‘never’, are usually applied to recognize negation phrases or sentences. As for negation classification, one w"
C10-1072,P09-1027,0,0.204745,"Missing"
C10-1072,J09-3003,0,0.511671,"uctions can shift the sentimental polarities of a text. For example, in the sentence ‘The chair is not comfortable’, the polarity of the word ‘comfortable’ is positive while the polarity of the whole sentence is reversed because of the negation word ‘not’. Therefore, the overall sentiment of a document is not necessarily the sum of the content parts (Turney, 2002). This phenomenon is one main reason why machine learning approaches fail under some circumstances. As a typical case of polarity shifting, negation has been paid close attention and widely studied in the literature (Na et al., 2004; Wilson et al., 2009; Kennedy and Inkpen, 2006). Generally, there are two steps to incorporate negation information into a system: negation detection and negation classification. For negation detection, some negation trigger words, such as ‘no’, ‘not’, and ‘never’, are usually applied to recognize negation phrases or sentences. As for negation classification, one way to import negation information is to directly reverse the polarity of the words which contain negation trigger words as far as term-counting approaches are considered (Kennedy and Inkpen, 2006). An alternative way is to add some negation features (e."
C10-1072,P04-1035,0,\N,Missing
C10-1072,W06-1652,0,\N,Missing
C10-1072,P09-1078,1,\N,Missing
C10-1072,P09-1079,0,\N,Missing
C10-1072,I08-1039,0,\N,Missing
C10-1076,W05-0630,0,0.037794,"Missing"
C10-1076,W05-0620,0,0.0191159,"Missing"
C10-1076,W08-0606,0,0.299169,"Missing"
C10-1076,I05-2038,0,0.0167196,"Missing"
C10-1076,E99-1043,0,0.0851759,"wordn. That is to say, X’s parent constituent must cross-bracket or include the scope of wordm, …, wordn. <sentence id=""S26.8"">These findings <xcope id=""X26.8.2""><cue type=""speculation"" ref=""X26.8.2"">indicate that</cue> <xcope id=""X26.8.1"">corticosteroid resistance in bronchial asthma <cue type=""negation"" ref=""X26.8.1"">can not</cue> be explained by abnormalities in corticosteroid receptor characteristics</xcope></xcope>.</sentence> Figure 1: An annotated sentence in the BioScope corpus. The Bioscope corpus consists of three subcorpora: the full papers and the abstracts from the GENIA corpus (Collier et al., 1999), and clinical (radiology) reports. Among them, the full papers subcorpus and the abstracts subcorpus come from the same genre, and thus share some common characteristics in statistics, such as the number of words in the negation scope to the right (or left) of the negation signal and the average scope length. In comparison, the clinical reports subcorpus consists of clinical radiology reports with short sentences. For detailed statistics about the three subcorpora, please see Morante and Daelemans (2009). 1 Negation Scope Finding via Shallow Semantic Parsing 2 http://code.google.com/p/berkele"
C10-1076,P05-1073,0,0.0576361,"Missing"
C10-1076,P02-1031,0,0.0202022,"Missing"
C10-1076,P02-1053,0,0.00295295,"Missing"
C10-1076,W06-1617,0,0.0115245,"eatures To capture more useful information in the negation signal-scope structures, we also explore various kinds of additional features. Table 2 shows the features in better capturing the details regarding the argument candidate and the negation signal. In particular, we categorize the additional features into three groups according to their relationship with the argument candidate (AC, in short) and the given negation signal (NS, in short). Some features proposed above may not be effective in argument identification. Therefore, we adopt the greedy feature selection algorithm as described in Jiang and Ng (2006) to pick up positive features incrementally according to their contributions on the development data. The algorithm repeatedly selects one feature each time which contributes most, and stops when adding any of the remaining features fails to improve the performance. As far as the negation scope finding task concerned, the whole feature selection process could be done by first running the selection algorithm with the basic features (b1-b4) and then incrementally picking up effective features from (ac1-ac6, AC1-AC2, 4.4 Post-Processing Although a negation signal in the BioScope corpus always has"
C10-1076,W04-3212,0,0.0242302,"finding does not involve semantic label classification and thus could be divided into three consequent phases: argument pruning, argument identification and post-processing. 674 4.2 Argument Pruning Similar to the predicate-argument structures in common shallow semantic parsing, the negation signal-scope structures in negation scope finding can be also classified into several certain types and argument pruning can be done by employing several heuristic rules to filter out constituents, which are most likely non-arguments of a negation signal. Similar to the heuristic algorithm as proposed in Xue and Palmer (2004) for argument pruning in common shallow semantic parsing, the argument pruning algorithm adopted here starts from designating the negation signal as the current node and collects its siblings. It then iteratively moves one level up to the parent of the current node and collects its siblings. The algorithm ends when it reaches the root of the parse tree. To sum up, except the negation signal and its ancestral constituents, any constituent in the parse tree whose parent covers the given negation signal will be collected as argument candidates. Taking the negation signal node “RB7,7” in Figure 2"
C10-1076,J08-2004,0,0.0124589,". (NP<S>VP>RB) nsac2 whether AC and NS are adjacent in position. “yes” or “no”. (no) combined features (NSAC1-NSAC7) b1&b2, b1&b3, b1&nsac1, b3&NS1, b3&NS2, b4&NS1, b4&NS2 Table 2: Additional features and their instantiations for argument identification in negation scope finding, with NP4,5 as the focus constituent (i.e., the argument candidate) and “can not” as the given negation signal, regarding Figure 2. Basic Features Table 1 lists the basic features for argument identification. These features are also widely used in common shallow semantic parsing for both verbal and nominal predicates (Xue, 2008; Li et al., 2009). Feature Remarks b1 Negation: the stem of the negation signal, e.g., not, rather_than. (can_not) b2 Phrase Type: the syntactic category of the argument candidate. (NP) b3 Path: the syntactic path from the argument candidate to the negation signal. (NP<S>VP>RB) b4 Position: the positional relationship of the argument candidate with the negation signal. “left” or “right”. (left) Table 1: Basic features and their instantiations for argument identification in negation scope finding, with NP4,5 as the focus constituent (i.e., the argument candidate) and “can not” as the given neg"
C10-1076,D08-1075,0,0.171304,"his paper is organized as follows. Section 2 reviews related work. Section 3 introduces the Bioscope corpus on which our approach is evaluated. Section 4 describes our parsing approach by formulating negation scope finding as a simplified shallow semantic parsing problem. Section 5 presents the experimental results. Finally, Section 6 concludes the work. 672 2 Related Work While there is a certain amount of literature within the NLP community on negated terms finding (Chapman et al., 2001; Huang and Lowe, 2007; Goldin and Chapman, 2003), there are only a few studies on negation scope finding (Morante et al., 2008; Morante and Daelemans, 2009). Negated terms finding Rule-based methods dominated the initial research on negated terms finding. As a representative, Chapman et al. (2001) developed a simple regular expression-based algorithm to detect negation signals and identify medical terms which fall within the negation scope. They found that their simple regular expression-based algorithm can effectively identify a large portion of the pertinent negative statements from discharge summaries on determining whether a finding or disease is absent. Besides, Huang and Lowe (2007) first proposed some heuristi"
C10-1076,W09-1105,0,0.453568,"as follows. Section 2 reviews related work. Section 3 introduces the Bioscope corpus on which our approach is evaluated. Section 4 describes our parsing approach by formulating negation scope finding as a simplified shallow semantic parsing problem. Section 5 presents the experimental results. Finally, Section 6 concludes the work. 672 2 Related Work While there is a certain amount of literature within the NLP community on negated terms finding (Chapman et al., 2001; Huang and Lowe, 2007; Goldin and Chapman, 2003), there are only a few studies on negation scope finding (Morante et al., 2008; Morante and Daelemans, 2009). Negated terms finding Rule-based methods dominated the initial research on negated terms finding. As a representative, Chapman et al. (2001) developed a simple regular expression-based algorithm to detect negation signals and identify medical terms which fall within the negation scope. They found that their simple regular expression-based algorithm can effectively identify a large portion of the pertinent negative statements from discharge summaries on determining whether a finding or disease is absent. Besides, Huang and Lowe (2007) first proposed some heuristic rules from a parse tree pers"
C10-1076,D09-1145,0,0.0181298,"8) pioneered the research on negation scope finding, largely due to the availability of a large-scale annotated corpus, the Bioscope corpus. They approached the negation scope finding task as a chunking problem which predicts whether a word in the sentence is inside or outside of the negation scope, with proper post-processing to ensure consecutiveness of the negation scope. Morante and Daelemans (2009) further improved the performance by combing several classifiers. Similar to SoN learning, there are some efforts in the NLP community on learning the scope of speculation. As a representative, Özgür and Radev (2009) divided speculation learning into two subtasks: speculation signal finding and speculation scope finding. In particular, they formulated speculation signal finding as a classification problem while employing some heuristic rules from the parse tree perspective on speculation scope finding. 3 For preprocessing, all the sentences in the Bioscope corpus are tokenized and then parsed using the Berkeley parser 2 (Petrov and Klein, 2007) trained on the GENIA TreeBank (GTB) 1.0 (Tateisi et al., 2005) 3 , which is a bracketed corpus in (almost) PTB style. 10-fold cross-validation on GTB1.0 shows that"
C10-1076,N07-1051,0,0.0744542,"Missing"
C10-1076,D09-1133,1,\N,Missing
C10-2087,P04-1054,0,0.0798595,"p between two proteins in a syntactic representation. With the wide application of kernel-based methods to many NLP tasks, various kernels such as subsequence kernels (Bunescu and Mooney, 2005) and tree kernels (Li et al., 2008), are also applied to PPI detection.. Particularly, dependency-based kernels such as edit distance kernels (Erkan et al., 2007) and graph kernels (Airola et al., 2008; Kim et al., 2010) show some promising results for PPI extraction. This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories (Culotta and Sorensen, 2004). In order to appreciate the advantages of both feature-based methods and kernel-based methods, composite kernels (Miyao et al., 2008; Miwa et al., 2009a; Miwa et al., 2009b) are further employed to combine structural syntactic information with flat word features and significantly improve the performance of PPI extraction. However, one critical challenge for kernel-based methods is their computation complexity, which prevents them from being widely deployed in real-world applications regarding the large amount of biomedical literature being archived everyday. Considering the potential of depen"
C10-2087,P08-1006,0,0.176675,"h as subsequence kernels (Bunescu and Mooney, 2005) and tree kernels (Li et al., 2008), are also applied to PPI detection.. Particularly, dependency-based kernels such as edit distance kernels (Erkan et al., 2007) and graph kernels (Airola et al., 2008; Kim et al., 2010) show some promising results for PPI extraction. This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories (Culotta and Sorensen, 2004). In order to appreciate the advantages of both feature-based methods and kernel-based methods, composite kernels (Miyao et al., 2008; Miwa et al., 2009a; Miwa et al., 2009b) are further employed to combine structural syntactic information with flat word features and significantly improve the performance of PPI extraction. However, one critical challenge for kernel-based methods is their computation complexity, which prevents them from being widely deployed in real-world applications regarding the large amount of biomedical literature being archived everyday. Considering the potential of dependency information for PPI extraction and the challenge of computation complexity of kernel-based methods, one may naturally ask the q"
C10-2087,W06-1634,0,0.0320787,"Missing"
C10-2087,D07-1024,0,0.197868,"n of the entire biological process. However, manual collection of relevant Protein-Protein Interaction (PPI) information from thousands of research papers published every day is so time-consuming that automatic extraction approaches with the help of Natural Language Processing (NLP) techniques become necessary. Various machine learning approaches for relation extraction have been applied to the biomedical domain, which can be classified into two categories: feature-based methods (Mitsumori et al., 2006; Giuliano et al., 2006; Sætre et al., 2007) and kernel-based methods (Bunescu et al., 2005; Erkan et al., 2007; Airola et al., 2008; Kim et al., 2010). Provided a large-scale manually annotated corpus, the task of PPI extraction can be formulated as a classification problem. Typically, for featured-based learning each protein pair is represented as a vector whose features are extracted from the sentence involving two protein names. Early studies identify the existence of protein interactions by using “bag-of-words” features (usually uni-gram or bi-gram) around the protein names as well as various kinds of shallow linguistic information, such as POS tag, lemma and orthographical features. However, thes"
C10-2087,P05-1052,0,0.0608613,"learning each protein pair is represented as a vector whose features are extracted from the sentence involving two protein names. Early studies identify the existence of protein interactions by using “bag-of-words” features (usually uni-gram or bi-gram) around the protein names as well as various kinds of shallow linguistic information, such as POS tag, lemma and orthographical features. However, these systems do not achieve promising results since they disregard any syntactic or semantic information altogether, which are very useful for the task of relation extraction in the newswire domain (Zhao and Grishman, 2005; Zhou et al., 2005). Furthermore, feature-based methods fail to effectively capture the structural information, which is essential to Abstract Recent kernel-based PPI extraction systems achieve promising performance because of their capability to capture structural syntactic information, but at the expense of computational complexity. This paper incorporates dependency information as well as other lexical and syntactic knowledge in a feature-based framework. Our motivation is that, considering the large amount of biomedical literature being archived daily, feature-based methods with comparabl"
C10-2087,E06-1051,0,0.590702,"ersity Email: liubingnlp@gmail.com {qianlonghua,redleaf,gdzhou}@suda.edu.cn the organization of the entire biological process. However, manual collection of relevant Protein-Protein Interaction (PPI) information from thousands of research papers published every day is so time-consuming that automatic extraction approaches with the help of Natural Language Processing (NLP) techniques become necessary. Various machine learning approaches for relation extraction have been applied to the biomedical domain, which can be classified into two categories: feature-based methods (Mitsumori et al., 2006; Giuliano et al., 2006; Sætre et al., 2007) and kernel-based methods (Bunescu et al., 2005; Erkan et al., 2007; Airola et al., 2008; Kim et al., 2010). Provided a large-scale manually annotated corpus, the task of PPI extraction can be formulated as a classification problem. Typically, for featured-based learning each protein pair is represented as a vector whose features are extracted from the sentence involving two protein names. Early studies identify the existence of protein interactions by using “bag-of-words” features (usually uni-gram or bi-gram) around the protein names as well as various kinds of shallow l"
C10-2087,P05-1053,1,0.950324,"r is represented as a vector whose features are extracted from the sentence involving two protein names. Early studies identify the existence of protein interactions by using “bag-of-words” features (usually uni-gram or bi-gram) around the protein names as well as various kinds of shallow linguistic information, such as POS tag, lemma and orthographical features. However, these systems do not achieve promising results since they disregard any syntactic or semantic information altogether, which are very useful for the task of relation extraction in the newswire domain (Zhao and Grishman, 2005; Zhou et al., 2005). Furthermore, feature-based methods fail to effectively capture the structural information, which is essential to Abstract Recent kernel-based PPI extraction systems achieve promising performance because of their capability to capture structural syntactic information, but at the expense of computational complexity. This paper incorporates dependency information as well as other lexical and syntactic knowledge in a feature-based framework. Our motivation is that, considering the large amount of biomedical literature being archived daily, feature-based methods with comparable performance are mo"
C10-2087,D09-1013,0,0.395439,"nels (Bunescu and Mooney, 2005) and tree kernels (Li et al., 2008), are also applied to PPI detection.. Particularly, dependency-based kernels such as edit distance kernels (Erkan et al., 2007) and graph kernels (Airola et al., 2008; Kim et al., 2010) show some promising results for PPI extraction. This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories (Culotta and Sorensen, 2004). In order to appreciate the advantages of both feature-based methods and kernel-based methods, composite kernels (Miyao et al., 2008; Miwa et al., 2009a; Miwa et al., 2009b) are further employed to combine structural syntactic information with flat word features and significantly improve the performance of PPI extraction. However, one critical challenge for kernel-based methods is their computation complexity, which prevents them from being widely deployed in real-world applications regarding the large amount of biomedical literature being archived everyday. Considering the potential of dependency information for PPI extraction and the challenge of computation complexity of kernel-based methods, one may naturally ask the question: “Can the e"
C12-1090,P02-1011,0,0.25969,"alysis, anaphora resolution aims to resolve a given mention to its referred expression in a text and has been a focus of research in Natural Language Processing (NLP) for decades. According to the nature of the referred expression, anaphora resolution can be categorized into entity anaphora resolution and event anaphora resolution. While most studies focus on entity anaphora resolution and have achieved much success recently (e.g. Soon et al. 2001; Ng and Cardie 2002; Ng 2007, 2009; Yang et al.2004, 2006, 2008; Kong et al. 2009,2010), there are only a few studies on event anaphora resolution (Byron, 2002; Pradhan et al., 2007; Chen et al. 2010a, 2010b; Kong and Zhou, 2011). In this paper, we address event pronoun resolution, the most difficult type of event anaphora resolution due to the least discriminative information that an event pronoun can provide. Here, an event pronoun is a pronoun whose antecedent refers to an event. In particular, we focus on improving event pronoun resolution using both local and global semantic information. For local semantic information, we employ a shallow semantic parser to extract the predicate-argument structure in a sentence to represent an involved event. I"
C12-1090,C10-1022,0,0.174555,"resolve a given mention to its referred expression in a text and has been a focus of research in Natural Language Processing (NLP) for decades. According to the nature of the referred expression, anaphora resolution can be categorized into entity anaphora resolution and event anaphora resolution. While most studies focus on entity anaphora resolution and have achieved much success recently (e.g. Soon et al. 2001; Ng and Cardie 2002; Ng 2007, 2009; Yang et al.2004, 2006, 2008; Kong et al. 2009,2010), there are only a few studies on event anaphora resolution (Byron, 2002; Pradhan et al., 2007; Chen et al. 2010a, 2010b; Kong and Zhou, 2011). In this paper, we address event pronoun resolution, the most difficult type of event anaphora resolution due to the least discriminative information that an event pronoun can provide. Here, an event pronoun is a pronoun whose antecedent refers to an event. In particular, we focus on improving event pronoun resolution using both local and global semantic information. For local semantic information, we employ a shallow semantic parser to extract the predicate-argument structure in a sentence to represent an involved event. In order to complement the locality of th"
C12-1090,N09-1065,0,0.039144,"Missing"
C12-1090,J05-1004,0,0.106488,"Missing"
C12-1090,P04-1017,1,0.813256,"ent anaphora resolution first and then explore its possible integration (e.g. joint learning) with entity anaphora resolution. In this paper, we focus on improving event pronoun resolution by exploring both local and global semantic information. 4 Baseline System Our event pronoun resolution framework adopts the common learning-based one for entity anaphora resolution, as described by Soon et al. (2001). Specially, the way generating instances during training and testing procedures of event pronoun resolution is similar to Kong and Zhou (2011). 4.1 Flat Features For entity pronoun resolution, Yang et al. (2004, 2005, 2006) explored various kinds of syntactic and semantic features to describe the information related with the antecedent candidate and the anaphor from their own and the relationship between them. However, few of these features can be adopted in event pronoun resolution. On one hand, since the antecedent candidate is an event trigger and the anaphor is a pronoun, both carry little obvious information about their own. On the other hand, the event anaphor and candidate pair in event pronoun resolution consists of a predicate and a pronoun. The difference in syntactic categories introduces"
C12-1090,D09-1102,1,\N,Missing
C12-1090,D09-1103,1,\N,Missing
C12-1090,J08-3002,0,\N,Missing
C12-1090,P04-1043,0,\N,Missing
C12-1090,D09-1133,1,\N,Missing
C12-1090,P06-1006,0,\N,Missing
C12-1090,C10-1068,1,\N,Missing
C12-1090,D10-1086,1,\N,Missing
C12-1090,P07-1103,0,\N,Missing
C12-1090,P02-1014,0,\N,Missing
C12-1090,J01-4004,0,\N,Missing
C12-1090,D10-1085,0,\N,Missing
C12-1090,I08-1004,1,\N,Missing
C12-1099,W06-0901,0,0.369137,"Missing"
C12-1099,W09-2209,0,0.275307,"Missing"
C12-1099,N09-2053,0,0.421502,"Missing"
C12-1099,P05-1045,0,0.0317298,"Missing"
C12-1099,P09-2093,0,0.0845261,"Missing"
C12-1099,W09-1704,0,0.16505,"Missing"
C12-1099,P10-1113,1,0.900633,"Missing"
C12-1099,D12-1092,1,0.891326,". Take following two sentences as samples: (E1) 我们将承担所有本公司的费用。 ( We will bear all the expenses for our company.) (E2) 他将在IBM从事科学研究工作。 (He will engage in scientific research in IBM.) Although “承担” (bear) and “从事” (engage) are two synonyms of “担任”, they do not trigger the Start-position event but any other events. The second one is to expand the triggers using the compositional semantics inside Ch inese words. The intuition is that if a Chinese word contains more than one character, and its meaning can be often inferred fro m the mean ings of its component characters (Yuan, 1998). Fo r example, Li et al. (2012) infer the semantics of a verb (most triggers in Chinese events are verbs) fro m its basic single-character verb (BV) and significantly imp rove the F1-measure, largely due to the dramatic increase in the recall. The problem with Li et al. (2012) is that they extract all single-character verbs contained in triggers as BVs (e.g., “担” (undertake, verb) and “任” (serve as, verb) are treated as two BVs for “担任” (serve as)). Therefore, pseudo triggers are much introduced. This severely harms the precision. Take the following sentence as a sample: (E3) 所有的公司员工信任他们的董事长。（All employees trust their chair"
C12-1099,P10-1081,0,0.692738,"Missing"
C12-1099,I11-1080,0,0.021464,"Missing"
C12-1099,P12-1088,0,0.0626125,"Missing"
C12-1099,D07-1075,0,0.153259,"s morphological structures in Ch inese words and proposes a mechanis m for determining the morphological structure and head morpheme in a Chinese trigger. Section 4 proposes an algorithm to infer unknown triggers on their morphological structures and sememes. Section 5 presents the experimental results. Finally, we conclude the paper with future work. 2 Related work In the literature, most of existing studies on event extraction concern English and can be classified into either pattern-based (e.g., Riloff, 1996; Yangarber et al., 2000; Stevenson and Greenwood, 2005; Shinyama and Sekine, 2006; Patwardhan and Riloff, 2007; Chambers and 1621 Jurafsky, 2011) or classifier-based (e.g., Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006; Maslennikov and Chua, 2007; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; & 2011; Hong et al., 2011; Lu and Roth, 2012; Llorens et al., 2012). In particular, while earlier studies focus on sentence-level extract ion, later ones turn to employ global info rmation. Co mpared with tremendous work on English event extraction, there are only a few studies on Chinese event extract ion with focus on either feature engineering or trigger expansion, under t"
C12-1099,D09-1016,0,0.572011,"Missing"
C12-1099,P05-1047,0,0.0931335,"2 overviews the related work. Sect ion 3 describes various morphological structures in Ch inese words and proposes a mechanis m for determining the morphological structure and head morpheme in a Chinese trigger. Section 4 proposes an algorithm to infer unknown triggers on their morphological structures and sememes. Section 5 presents the experimental results. Finally, we conclude the paper with future work. 2 Related work In the literature, most of existing studies on event extraction concern English and can be classified into either pattern-based (e.g., Riloff, 1996; Yangarber et al., 2000; Stevenson and Greenwood, 2005; Shinyama and Sekine, 2006; Patwardhan and Riloff, 2007; Chambers and 1621 Jurafsky, 2011) or classifier-based (e.g., Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006; Maslennikov and Chua, 2007; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; & 2011; Hong et al., 2011; Lu and Roth, 2012; Llorens et al., 2012). In particular, while earlier studies focus on sentence-level extract ion, later ones turn to employ global info rmation. Co mpared with tremendous work on English event extraction, there are only a few studies on Chinese event extract ion with focus on"
C12-1099,C00-2136,0,\N,Missing
C12-1099,P11-1098,0,\N,Missing
C12-1099,P08-1030,0,\N,Missing
C12-1099,N06-1039,0,\N,Missing
C12-1100,N06-1046,0,0.467183,"Missing"
C12-1100,W09-2209,0,0.0926762,"10) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et al. (2010) applied a feature weighting algorithm to re-weight various features extracted for trigger identification and event type determination. Chen and Ji (2009b) applied various kinds of lexical, syntactic and semantic features to address the special issues in Chinese. They also constructed a global errata table to record the inconsistency in the training set and used it to correct the inconsistency in the test set. The other studies focused on automatic expansion of event triggers to improve the recall. Chen and Ji (2009a) proposed a bootstrapping framework, which exploited extra information captured by an English event extraction system. Ji (2009) first extracts some cross-lingual predicate clusters using bilingual parallel corpora and a cross-lin"
C12-1100,N09-2053,0,0.156331,"10) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et al. (2010) applied a feature weighting algorithm to re-weight various features extracted for trigger identification and event type determination. Chen and Ji (2009b) applied various kinds of lexical, syntactic and semantic features to address the special issues in Chinese. They also constructed a global errata table to record the inconsistency in the training set and used it to correct the inconsistency in the test set. The other studies focused on automatic expansion of event triggers to improve the recall. Chen and Ji (2009a) proposed a bootstrapping framework, which exploited extra information captured by an English event extraction system. Ji (2009) first extracts some cross-lingual predicate clusters using bilingual parallel corpora and a cross-lin"
C12-1100,N07-1030,0,0.157608,"del may suffer from the errors propagated from upstream tasks, a joint model can benefit from the close interaction between two or more tasks: it not only allows the uncertainty about one task to be carried forward to next ones but also allows useful information from one task to be carried backward to previous ones. Recently, joint modeling has been widely attempted in various NLP tasks, such as joint named entity recognition and syntactic parsing (Finkel and Manning, 2009), joint syntactic parsing and semantic role labeling (Li et al., 2010), joint anaphoricity and coreference determination (Denis and Baldridge, 2007; Iida and Poesio, 2011). In the event extraction task, only a few studies are concerned with joint modeling, mostly in the bio-molecular domain. Riedel et al. (2009) used Markov Logic as a general purpose framework for jointly modeling the complete bio-molecular event structure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and McCallum (2011) presented three joint models for bio-molecular event extraction. While the first model jointly predicts triggers and their arguments"
C12-1100,D12-1062,0,0.124274,"ure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and McCallum (2011) presented three joint models for bio-molecular event extraction. While the first model jointly predicts triggers and their arguments and the second model enforces additional constraints that ensure the consistency between events in hierarchical regulation structures, the third model integrates the first one and the second one in explicitly capturing the interaction of various arguments in the same event. Do et al. (2012) constructed a timeline of events mentioned in a given text which proposed a joint inference module that enforced global coherency constraints on the final outputs of the two pairwise classifiers, one between event mentions and time intervals, and one between event mentions themselves. Our joint model is inspired by both Roth and Yih (2004) on joint named entity recognition and relation extraction and Denis and Baldridge (2007) on joint anaphoricity determination and coreference resolution. However, as far as we know, there are no successful models for jointly solving Chinese trigger identific"
C12-1100,P05-1045,0,0.0257747,"ing an event can be recast as identifying a corresponding trigger. Trigger mention: a reference to a trigger word. Trigger type/Event type: the type of an event. Argument: the entity mentions involved in an event. Argument role: the relation of an argument to an event where it participates. In the literature, almost all the existing studies on event extraction are concerned with English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Gupta and Ji, 2009; Liao and Grishman, 2010) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et al. (2010) applied a"
C12-1100,D09-1015,0,0.0174881,"its in F1-measure on the ACE 2005 Chinese corpus, ignoring the post-processing – discourse consistency. 1638 2.2 Joint modeling While a pipeline model may suffer from the errors propagated from upstream tasks, a joint model can benefit from the close interaction between two or more tasks: it not only allows the uncertainty about one task to be carried forward to next ones but also allows useful information from one task to be carried backward to previous ones. Recently, joint modeling has been widely attempted in various NLP tasks, such as joint named entity recognition and syntactic parsing (Finkel and Manning, 2009), joint syntactic parsing and semantic role labeling (Li et al., 2010), joint anaphoricity and coreference determination (Denis and Baldridge, 2007; Iida and Poesio, 2011). In the event extraction task, only a few studies are concerned with joint modeling, mostly in the bio-molecular domain. Riedel et al. (2009) used Markov Logic as a general purpose framework for jointly modeling the complete bio-molecular event structure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and M"
C12-1100,P09-2093,0,0.0310181,"gger word. Trigger type/Event type: the type of an event. Argument: the entity mentions involved in an event. Argument role: the relation of an argument to an event where it participates. In the literature, almost all the existing studies on event extraction are concerned with English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Gupta and Ji, 2009; Liao and Grishman, 2010) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et al. (2010) applied a feature weighting algorithm to re-weight various features extracted for trigger identification and eve"
C12-1100,P11-1113,1,0.924792,"redefined event type, and their participants and attributes. It can be typically divided into four components: trigger identification, event type determination, argument identification and argument role determination. Due to the central role of the contained events in a text, it is critical to mine their semantics in order to understand a text. Unfortunately, event extraction has been proven its performance is still very low. In the literature, most studies focus on English event extraction and have achieved certain success (e.g., Grishman et al., 2005; Ahn, 2006; Patwardhan and Riloff, 2009; Hong et al., 2011; Lu and Roth, 2012; Llorens et al., 2012). However, there are few successful stories regarding Chinese event extraction due to the special characteristics in Chinese trigger identification. Besides unknown triggers1 and word segmentation errors (Li et al., 2012), the low quality of annotated corpora and the high ratio of pseudo trigger mentions to true ones are also blamed for the low performance of Chinese event extraction. To examine the low quality of annotated corpora in Chinese event extraction, we take the ACE (Automatic Content Extraction) 2005 Chinese corpus (with 8 types and 33 subty"
C12-1100,P11-1081,0,0.0964903,"ors propagated from upstream tasks, a joint model can benefit from the close interaction between two or more tasks: it not only allows the uncertainty about one task to be carried forward to next ones but also allows useful information from one task to be carried backward to previous ones. Recently, joint modeling has been widely attempted in various NLP tasks, such as joint named entity recognition and syntactic parsing (Finkel and Manning, 2009), joint syntactic parsing and semantic role labeling (Li et al., 2010), joint anaphoricity and coreference determination (Denis and Baldridge, 2007; Iida and Poesio, 2011). In the event extraction task, only a few studies are concerned with joint modeling, mostly in the bio-molecular domain. Riedel et al. (2009) used Markov Logic as a general purpose framework for jointly modeling the complete bio-molecular event structure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and McCallum (2011) presented three joint models for bio-molecular event extraction. While the first model jointly predicts triggers and their arguments and the second model en"
C12-1100,W09-1704,0,0.0193835,"Trigger type/Event type: the type of an event. Argument: the entity mentions involved in an event. Argument role: the relation of an argument to an event where it participates. In the literature, almost all the existing studies on event extraction are concerned with English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Gupta and Ji, 2009; Liao and Grishman, 2010) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et al. (2010) applied a feature weighting algorithm to re-weight various features extracted for trigger identification and eve"
C12-1100,P08-1030,0,0.0745871,"Trigger mention: a reference to a trigger word. Trigger type/Event type: the type of an event. Argument: the entity mentions involved in an event. Argument role: the relation of an argument to an event where it participates. In the literature, almost all the existing studies on event extraction are concerned with English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Gupta and Ji, 2009; Liao and Grishman, 2010) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et al. (2010) applied a feature weighting algorithm to re-weight various features extracted f"
C12-1100,W09-1419,0,0.0199403,"e class distribution imbalance by discarding harmful or superfluous instances. In the literature, instance filtering has been widely employed in various NLP tasks. As for event extraction, there are also a few relevant studies. Patwardhan and Riloff (2009) first applied a selftrained relevant sentence classifier to identify relevant regions and split all candidate sentences into two sets: relevant and irrelevant sentences. Then, they used a pattern-based classifier to recognize events from those relevant sentences and a SVM-based classifier to recognize events from those irrelevant sentences. Landeghem et al. (2009) provided a negative-instances filter to check whether the length of the sub-sentence spanned by a candidate event does not exceed a certain value. Landeghem et al. (2010) further designed a false-positive filter using specific categories of relations to serve as negative indicators in Bio-NLP. Liao and Grishman (2010) applied a pseudo co-testing algorithm based on various criteria, such as informativeness, representativeness and diversity of the sentence, to filter out those pseudo samples to reduce annotation labour in event corpus annotation. 1639 3 Joint modeling of trigger identification"
C12-1100,W10-1921,0,0.0188036,"event extraction, there are also a few relevant studies. Patwardhan and Riloff (2009) first applied a selftrained relevant sentence classifier to identify relevant regions and split all candidate sentences into two sets: relevant and irrelevant sentences. Then, they used a pattern-based classifier to recognize events from those relevant sentences and a SVM-based classifier to recognize events from those irrelevant sentences. Landeghem et al. (2009) provided a negative-instances filter to check whether the length of the sub-sentence spanned by a candidate event does not exceed a certain value. Landeghem et al. (2010) further designed a false-positive filter using specific categories of relations to serve as negative indicators in Bio-NLP. Liao and Grishman (2010) applied a pseudo co-testing algorithm based on various criteria, such as informativeness, representativeness and diversity of the sentence, to filter out those pseudo samples to reduce annotation labour in event corpus annotation. 1639 3 Joint modeling of trigger identification and event type determination In this section, an ILP (Integer Logic Programming) -based inference framework is proposed to jointly model trigger identification and event t"
C12-1100,P10-1113,1,0.748089,"– discourse consistency. 1638 2.2 Joint modeling While a pipeline model may suffer from the errors propagated from upstream tasks, a joint model can benefit from the close interaction between two or more tasks: it not only allows the uncertainty about one task to be carried forward to next ones but also allows useful information from one task to be carried backward to previous ones. Recently, joint modeling has been widely attempted in various NLP tasks, such as joint named entity recognition and syntactic parsing (Finkel and Manning, 2009), joint syntactic parsing and semantic role labeling (Li et al., 2010), joint anaphoricity and coreference determination (Denis and Baldridge, 2007; Iida and Poesio, 2011). In the event extraction task, only a few studies are concerned with joint modeling, mostly in the bio-molecular domain. Riedel et al. (2009) used Markov Logic as a general purpose framework for jointly modeling the complete bio-molecular event structure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and McCallum (2011) presented three joint models for bio-molecular event ex"
C12-1100,D12-1092,1,0.881305,"he high ratio of pseudo trigger mentions to true ones, Table 3 shows top 5 imbalanced triggers from the training set of the ACE 2005 Chinese corpus and justifies the difficulty for a classifier to identify a true trigger mention, especially for those of a particular event type, which appears only a few times in the training set. Trigger2 #True trigger mentions #Pseudo trigger mentions 投资 (invest) 1 67 建设 (set up) 1 66 取得 (obtain) 1 52 发 (provide) 1 36 给 (give) 2 64 TABLE 3 – Top 5 triggers with the highest ratios of pseudo trigger mentions to true ones in the ACE 2005 Chinese corpus Recently, Li et al. (2012) justified that trigger identification was most critical for the performance of Chinese event extraction. In this paper, we also focus on trigger identification and its impact on overall Chinese event extraction. In order to address the above-mentioned two critical issues in Chinese event extraction, this paper proposes a joint model of trigger identification and event type determination to improve the performance of trigger identification and overall Chinese event extraction. Besides, several trigger filtering schemas are introduced to filter out those pseudo trigger mentions as many as possi"
C12-1100,P10-1081,0,0.257082,"ype/Event type: the type of an event. Argument: the entity mentions involved in an event. Argument role: the relation of an argument to an event where it participates. In the literature, almost all the existing studies on event extraction are concerned with English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Gupta and Ji, 2009; Liao and Grishman, 2010) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et al. (2010) applied a feature weighting algorithm to re-weight various features extracted for trigger identification and event type determination. Che"
C12-1100,I11-1080,0,0.0568574,"Missing"
C12-1100,P12-1088,0,0.141156,"Missing"
C12-1100,P07-1075,0,0.072223,"nce of an event, so recognizing an event can be recast as identifying a corresponding trigger. Trigger mention: a reference to a trigger word. Trigger type/Event type: the type of an event. Argument: the entity mentions involved in an event. Argument role: the relation of an argument to an event where it participates. In the literature, almost all the existing studies on event extraction are concerned with English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Gupta and Ji, 2009; Liao and Grishman, 2010) and cross-entity (Hong et al., 2011) information. 2.1 Chinese event extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Some studies focused on feature selection. Tan et al. (2008) used a local feature selection method to ensure the performance of trigger classification and applied multiple levels of patterns to improve their coverage in argument classification. Fu et"
C12-1100,D07-1075,0,0.3712,"Missing"
C12-1100,D09-1016,0,0.643633,"ntify trigger mentions of a predefined event type, and their participants and attributes. It can be typically divided into four components: trigger identification, event type determination, argument identification and argument role determination. Due to the central role of the contained events in a text, it is critical to mine their semantics in order to understand a text. Unfortunately, event extraction has been proven its performance is still very low. In the literature, most studies focus on English event extraction and have achieved certain success (e.g., Grishman et al., 2005; Ahn, 2006; Patwardhan and Riloff, 2009; Hong et al., 2011; Lu and Roth, 2012; Llorens et al., 2012). However, there are few successful stories regarding Chinese event extraction due to the special characteristics in Chinese trigger identification. Besides unknown triggers1 and word segmentation errors (Li et al., 2012), the low quality of annotated corpora and the high ratio of pseudo trigger mentions to true ones are also blamed for the low performance of Chinese event extraction. To examine the low quality of annotated corpora in Chinese event extraction, we take the ACE (Automatic Content Extraction) 2005 Chinese corpus (with 8"
C12-1100,N10-1123,0,0.144878,"joint modeling has been widely attempted in various NLP tasks, such as joint named entity recognition and syntactic parsing (Finkel and Manning, 2009), joint syntactic parsing and semantic role labeling (Li et al., 2010), joint anaphoricity and coreference determination (Denis and Baldridge, 2007; Iida and Poesio, 2011). In the event extraction task, only a few studies are concerned with joint modeling, mostly in the bio-molecular domain. Riedel et al. (2009) used Markov Logic as a general purpose framework for jointly modeling the complete bio-molecular event structure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and McCallum (2011) presented three joint models for bio-molecular event extraction. While the first model jointly predicts triggers and their arguments and the second model enforces additional constraints that ensure the consistency between events in hierarchical regulation structures, the third model integrates the first one and the second one in explicitly capturing the interaction of various arguments in the same event. Do et al. (2012) constructed a timeline of events men"
C12-1100,W09-1406,0,0.0300702,"tainty about one task to be carried forward to next ones but also allows useful information from one task to be carried backward to previous ones. Recently, joint modeling has been widely attempted in various NLP tasks, such as joint named entity recognition and syntactic parsing (Finkel and Manning, 2009), joint syntactic parsing and semantic role labeling (Li et al., 2010), joint anaphoricity and coreference determination (Denis and Baldridge, 2007; Iida and Poesio, 2011). In the event extraction task, only a few studies are concerned with joint modeling, mostly in the bio-molecular domain. Riedel et al. (2009) used Markov Logic as a general purpose framework for jointly modeling the complete bio-molecular event structure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and McCallum (2011) presented three joint models for bio-molecular event extraction. While the first model jointly predicts triggers and their arguments and the second model enforces additional constraints that ensure the consistency between events in hierarchical regulation structures, the third model integrates the"
C12-1100,D11-1001,0,0.0214976,"nning, 2009), joint syntactic parsing and semantic role labeling (Li et al., 2010), joint anaphoricity and coreference determination (Denis and Baldridge, 2007; Iida and Poesio, 2011). In the event extraction task, only a few studies are concerned with joint modeling, mostly in the bio-molecular domain. Riedel et al. (2009) used Markov Logic as a general purpose framework for jointly modeling the complete bio-molecular event structure for a given sentence. Poon and Vanderwende (2010) also adopted Markov Logic for bio-molecular event extraction in jointly predicting events and their arguments. Riedel and McCallum (2011) presented three joint models for bio-molecular event extraction. While the first model jointly predicts triggers and their arguments and the second model enforces additional constraints that ensure the consistency between events in hierarchical regulation structures, the third model integrates the first one and the second one in explicitly capturing the interaction of various arguments in the same event. Do et al. (2012) constructed a timeline of events mentioned in a given text which proposed a joint inference module that enforced global coherency constraints on the final outputs of the two"
C12-1100,W04-2401,0,0.163796,"model enforces additional constraints that ensure the consistency between events in hierarchical regulation structures, the third model integrates the first one and the second one in explicitly capturing the interaction of various arguments in the same event. Do et al. (2012) constructed a timeline of events mentioned in a given text which proposed a joint inference module that enforced global coherency constraints on the final outputs of the two pairwise classifiers, one between event mentions and time intervals, and one between event mentions themselves. Our joint model is inspired by both Roth and Yih (2004) on joint named entity recognition and relation extraction and Denis and Baldridge (2007) on joint anaphoricity determination and coreference resolution. However, as far as we know, there are no successful models for jointly solving Chinese trigger identification and event type determination. 2.3 Trigger filtering With the high ratio of pseudo trigger mentions to true ones, it is natural to filter out those unlikely trigger mentions in a preprocessing step. Basically, the general purpose for instance filtering is to reduce the class distribution imbalance by discarding harmful or superfluous i"
C12-1100,W06-0901,0,\N,Missing
C12-1139,P02-1034,0,0.0190685,". That is, different mappings should have different weights to exploit such difference. Thus, Formula (5) can be recast as follows: |Ψ| SimT ( ws , wt ) = α 0 × SimDW ( ws , wt ) + ∑ α i SimiDM ( ws , wt ) i =1 (5’) In this paper, we propose a simple perceptron algorithm to optimize those weights for different mappings using the development lexicon. Generally, the perceptron algorithm is guaranteed to find a hyper-plane that classifies all training points, if the data is separable. Even the data is nonseparable as in most practical cases, the variants of perceptron (Freund and Schapire, 1999; Collins and Duffy, 2002), such as averaged perceptron (AP) or voted perceptron (VP), can generalize well. Input: training examples (si,ti) Output: w Initialize: w= w0 Steps: 1. for i = 1…T 2. for j =1…N 3. Calculate tˆj = arg max t∈GEN ( s ) Φ(s j , t ) × w j 4. 5. 6. 7. 8. If tˆj ! = t j then w = w + Φ( s j , t j ) − Φ( s j , tˆj ) ui , j = w end for end for output w = ∑ ui , j / NT i, j Figure 4 – Perceptron algorithm for weight learning 2283 Fig. 4 shows our averaged perceptron algorithm, where  d, the number of features in a vector, is set to the order of the mapping set plus 1 for the dependency word similarity"
C12-1139,R11-1018,0,0.0313754,"Missing"
C12-1139,W95-0114,0,0.19051,"y between word contexts via an online dictionary. Particularly she analyzed the impact of polysemous words, Chinese tokenization and English morphological information. Zhang et al. (2006) built a Chinese-English financial lexicon from a comparable corpus with focus on the impact of seed lexicon selection. Haghighi et al. (2008) proposed a generative model to construct lexicons for multiple language pairs, including EnglishChinese, via canonical correlation analysis, which effectively explores monolingual lexicons in terms of latent matching. In particular for BLC without any external lexicon, Fung (1995) focused on context heterogeneity in Chinese and English languages, which measures how productive the context of a word is, instead of its absolute occurrence frequency. She suggested that bilingual translation words tend to share similar context heterogeneity in non-parallel corpora. Specifically, she calculated the similarity between two bilingual words using the ratios of unique words in the right and left contexts. Yu and Tsujii (2009) proposed the notion of dependency heterogeneity, which assumes that a word and its translation should share similar modifiers and heads in comparable corpor"
C12-1139,W09-1117,0,0.257847,", 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languages tend to occur in similar contexts, even in non-parallel corpora. Rapp (1999) and Fung (2001) proposed a bilingual context vector mapping strategy to explore word co-occurrence information. Both studies rely on a large, one-to-one mapping seed lexicon between the source and target languages. Koehn and Knight (2002) investigated various clues such as cognates, similar context, preservation of word similarity and word frequency. Garera et al. (2009) proposed a dependency-based context model and achieved better performance than previous word-based context models. Recent studies concentrate on automatic augmentation of the seed lexicon either by extracting identical words between two closely related languages (Ficšer and Ljubešić, 2011) or by aligning translation pairs from parallel sentences, which is mined in advance from a comparable corpus (Morin and Prochasson, 2011). The problem with above method is that they only consider the words involved in the contexts and ignore other rich information therein, such as syntactic relationships, t"
C12-1139,D11-1084,1,0.799809,"上下文更为可靠的信息。我们还进 一步展示了在没有人工干预的情况下可以产生和利用这种双语依存关系。从英文到中文的 双语词表构建实验表明，通过在计算双语词语相似度时同时映射词语及其依存关系，同目 前性能最好的系统相比，我们的方法显著提高了精度。对于经常出现的名词，精度提高了 14个百分点；对于较大频率范围内的名词和动词，性能也提高了，尽管程度较小。这说明 了依存映射模型对双语词表构建的有效性。 KEYWORDS: Bilingual Lexicon Construction, Comparable Corpora, Dependency Mapping KEYWORDS IN CHINESE: 双语词表构建, 可比较语料库, 依存映射 Corresponding author * Proceedings of COLING 2012: Technical Papers, pages 2275–2290, COLING 2012, Mumbai, December 2012. 2275 1 Introduction Bilingual lexicons play an important role in many natural language processing tasks, such as machine translation (MT) (Och and Ney, 2003; Gong et al., 2011) and cross-language information retrieval (CLIR) (Grefenstette, 1998). Traditionally, bilingual lexicons are built manually with tremendous efforts. With the availability of large-scale parallel corpora, researchers turn to automatic construction from parallel corpora and achieve certain success (Wu and Xia, 1994). However, large-scale parallel corpora do not always exist for most language pairs. Therefore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, vario"
C12-1139,1998.amta-tutorials.5,0,0.0982495,"过在计算双语词语相似度时同时映射词语及其依存关系，同目 前性能最好的系统相比，我们的方法显著提高了精度。对于经常出现的名词，精度提高了 14个百分点；对于较大频率范围内的名词和动词，性能也提高了，尽管程度较小。这说明 了依存映射模型对双语词表构建的有效性。 KEYWORDS: Bilingual Lexicon Construction, Comparable Corpora, Dependency Mapping KEYWORDS IN CHINESE: 双语词表构建, 可比较语料库, 依存映射 Corresponding author * Proceedings of COLING 2012: Technical Papers, pages 2275–2290, COLING 2012, Mumbai, December 2012. 2275 1 Introduction Bilingual lexicons play an important role in many natural language processing tasks, such as machine translation (MT) (Och and Ney, 2003; Gong et al., 2011) and cross-language information retrieval (CLIR) (Grefenstette, 1998). Traditionally, bilingual lexicons are built manually with tremendous efforts. With the availability of large-scale parallel corpora, researchers turn to automatic construction from parallel corpora and achieve certain success (Wu and Xia, 1994). However, large-scale parallel corpora do not always exist for most language pairs. Therefore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, various studies have been done to take advantage of multiple paths (Mann a"
C12-1139,kaji-etal-2008-automatic,0,0.0215735,"refore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, various studies have been done to take advantage of multiple paths (Mann and Yarowsky, 2001) and even multiple pivot languages (Mausam et al., 2009) between the source and target languages. Since such automatically constructed lexicons usually contain noisy and polysemous entries, corpus-based occurrence information has been widely used to help rank the candidate target words (Schafer and Yarowsky, 2002; Kaji et al., 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languages tend to occur in similar contexts, even in non-parallel corpora. Rapp (1999) and Fung (2001) proposed a bilingual context vector mapping strategy to explore word co-occurrence information. Both studies rely on a large, one-to-one mapping seed lexicon between the source and target languages. Koehn and Knight (2002) investigated various clues such as cognates, similar context, preservation of word similarity and word frequency. Garera"
C12-1139,W02-0902,0,0.470016,"corpus-based occurrence information has been widely used to help rank the candidate target words (Schafer and Yarowsky, 2002; Kaji et al., 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languages tend to occur in similar contexts, even in non-parallel corpora. Rapp (1999) and Fung (2001) proposed a bilingual context vector mapping strategy to explore word co-occurrence information. Both studies rely on a large, one-to-one mapping seed lexicon between the source and target languages. Koehn and Knight (2002) investigated various clues such as cognates, similar context, preservation of word similarity and word frequency. Garera et al. (2009) proposed a dependency-based context model and achieved better performance than previous word-based context models. Recent studies concentrate on automatic augmentation of the seed lexicon either by extracting identical words between two closely related languages (Ficšer and Ljubešić, 2011) or by aligning translation pairs from parallel sentences, which is mined in advance from a comparable corpus (Morin and Prochasson, 2011). The problem with above method is t"
C12-1139,N01-1020,0,0.105907,"Missing"
C12-1139,de-marneffe-etal-2006-generating,0,0.00571831,"7 3.1 Comparable corpus In this paper, we generate a comparable corpus from the parallel Chinese-English Foreign Broadcast Information Service (FBIS) corpus, gathered from the news domain. This bilingual corpus contains about 240k sentences, 6.9 million words in Chinese and 8.9 million words in English. Similar to the way adopted in (Garera et al., 2009; Haghighi et al., 2008), we couple the first half of Chinese corpus and the second half on the English side as our comparable corpus. For corpus pre-processing, we use the Stanford POS tagger (Toutanova and Manning, 2000) and syntactic parser (Marneffe et al., 2006) to generate the POS and dependency information for each sentence in both Chinese and English corpora. Particularly, English words are transformed to their respective lemmas using the TreeTagger package (Helmut, 1994). 3.2 Bilingual lexicons for evaluation: seed, test and development In the literature, different scales of bilingual seed lexicons have been used. For example, Rapp (1999) and Fung (2000) used large-scale dictionaries of 10-20k word pairs while other studies (Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009) used only small dictionaries of about 100-1000 word pai"
C12-1139,P09-1030,0,0.0248923,"with tremendous efforts. With the availability of large-scale parallel corpora, researchers turn to automatic construction from parallel corpora and achieve certain success (Wu and Xia, 1994). However, large-scale parallel corpora do not always exist for most language pairs. Therefore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, various studies have been done to take advantage of multiple paths (Mann and Yarowsky, 2001) and even multiple pivot languages (Mausam et al., 2009) between the source and target languages. Since such automatically constructed lexicons usually contain noisy and polysemous entries, corpus-based occurrence information has been widely used to help rank the candidate target words (Schafer and Yarowsky, 2002; Kaji et al., 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languages tend to occur in similar contexts, even in non-parallel corpora. Rapp (1999) and Fung (2001) proposed a bilingual context vector mapping strategy to explore wo"
C12-1139,W11-1205,0,0.0252602,"etween the source and target languages. Koehn and Knight (2002) investigated various clues such as cognates, similar context, preservation of word similarity and word frequency. Garera et al. (2009) proposed a dependency-based context model and achieved better performance than previous word-based context models. Recent studies concentrate on automatic augmentation of the seed lexicon either by extracting identical words between two closely related languages (Ficšer and Ljubešić, 2011) or by aligning translation pairs from parallel sentences, which is mined in advance from a comparable corpus (Morin and Prochasson, 2011). The problem with above method is that they only consider the words involved in the contexts and ignore other rich information therein, such as syntactic relationships, thus usually suffering from low performance especially when they are applied to two distinct languages such as English and Chinese. For example, our preliminary experiment with the dependency-based model (Garera et al., 2009) shows that English source word “profit” matches wrongly with Chinese target word “企业” (enterprise), instead of the correct one “利润”, due to the higher similarity score with the former than that with the l"
C12-1139,P00-1056,0,0.053703,"lish corpora. Particularly, English words are transformed to their respective lemmas using the TreeTagger package (Helmut, 1994). 3.2 Bilingual lexicons for evaluation: seed, test and development In the literature, different scales of bilingual seed lexicons have been used. For example, Rapp (1999) and Fung (2000) used large-scale dictionaries of 10-20k word pairs while other studies (Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009) used only small dictionaries of about 100-1000 word pairs. In this paper, we adopt a small scale one. In particular, we use the GIZA++ package (Och and Ney, 2000) to extract the most frequently occurring 1000 word pairs as the bilingual seed lexicon (denoted as Ls) and the subsequent 500 noun pairs (denoted as LNt) as the primary bilingual test lexicon. This way of generating the test lexicon for nouns has been commonly used in previous studies (Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). Besides, we frame a secondary test lexicon (denoted as LAt) including 200 nouns, verbs and adjectives respectively, which spread evenly in the four ranges of 1001-2000, 2001-3000, 3001-4000 and 4001-5000. The goal of LAt is to evaluate the ada"
C12-1139,J03-1002,0,0.00281751,"合方法为双语词表构建提供了比单一的词语上下文更为可靠的信息。我们还进 一步展示了在没有人工干预的情况下可以产生和利用这种双语依存关系。从英文到中文的 双语词表构建实验表明，通过在计算双语词语相似度时同时映射词语及其依存关系，同目 前性能最好的系统相比，我们的方法显著提高了精度。对于经常出现的名词，精度提高了 14个百分点；对于较大频率范围内的名词和动词，性能也提高了，尽管程度较小。这说明 了依存映射模型对双语词表构建的有效性。 KEYWORDS: Bilingual Lexicon Construction, Comparable Corpora, Dependency Mapping KEYWORDS IN CHINESE: 双语词表构建, 可比较语料库, 依存映射 Corresponding author * Proceedings of COLING 2012: Technical Papers, pages 2275–2290, COLING 2012, Mumbai, December 2012. 2275 1 Introduction Bilingual lexicons play an important role in many natural language processing tasks, such as machine translation (MT) (Och and Ney, 2003; Gong et al., 2011) and cross-language information retrieval (CLIR) (Grefenstette, 1998). Traditionally, bilingual lexicons are built manually with tremendous efforts. With the availability of large-scale parallel corpora, researchers turn to automatic construction from parallel corpora and achieve certain success (Wu and Xia, 1994). However, large-scale parallel corpora do not always exist for most language pairs. Therefore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994"
C12-1139,P99-1067,0,0.479653,"ltiple paths (Mann and Yarowsky, 2001) and even multiple pivot languages (Mausam et al., 2009) between the source and target languages. Since such automatically constructed lexicons usually contain noisy and polysemous entries, corpus-based occurrence information has been widely used to help rank the candidate target words (Schafer and Yarowsky, 2002; Kaji et al., 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languages tend to occur in similar contexts, even in non-parallel corpora. Rapp (1999) and Fung (2001) proposed a bilingual context vector mapping strategy to explore word co-occurrence information. Both studies rely on a large, one-to-one mapping seed lexicon between the source and target languages. Koehn and Knight (2002) investigated various clues such as cognates, similar context, preservation of word similarity and word frequency. Garera et al. (2009) proposed a dependency-based context model and achieved better performance than previous word-based context models. Recent studies concentrate on automatic augmentation of the seed lexicon either by extracting identical words"
C12-1139,W02-2026,0,0.0302353,"for most language pairs. Therefore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, various studies have been done to take advantage of multiple paths (Mann and Yarowsky, 2001) and even multiple pivot languages (Mausam et al., 2009) between the source and target languages. Since such automatically constructed lexicons usually contain noisy and polysemous entries, corpus-based occurrence information has been widely used to help rank the candidate target words (Schafer and Yarowsky, 2002; Kaji et al., 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languages tend to occur in similar contexts, even in non-parallel corpora. Rapp (1999) and Fung (2001) proposed a bilingual context vector mapping strategy to explore word co-occurrence information. Both studies rely on a large, one-to-one mapping seed lexicon between the source and target languages. Koehn and Knight (2002) investigated various clues such as cognates, similar context, preservation of word similarity and wor"
C12-1139,P10-1011,0,0.0138969,"turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, various studies have been done to take advantage of multiple paths (Mann and Yarowsky, 2001) and even multiple pivot languages (Mausam et al., 2009) between the source and target languages. Since such automatically constructed lexicons usually contain noisy and polysemous entries, corpus-based occurrence information has been widely used to help rank the candidate target words (Schafer and Yarowsky, 2002; Kaji et al., 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languages tend to occur in similar contexts, even in non-parallel corpora. Rapp (1999) and Fung (2001) proposed a bilingual context vector mapping strategy to explore word co-occurrence information. Both studies rely on a large, one-to-one mapping seed lexicon between the source and target languages. Koehn and Knight (2002) investigated various clues such as cognates, similar context, preservation of word similarity and word frequency. Garera et al. (2009) proposed a dep"
C12-1139,C94-1048,0,0.224167,"(MT) (Och and Ney, 2003; Gong et al., 2011) and cross-language information retrieval (CLIR) (Grefenstette, 1998). Traditionally, bilingual lexicons are built manually with tremendous efforts. With the availability of large-scale parallel corpora, researchers turn to automatic construction from parallel corpora and achieve certain success (Wu and Xia, 1994). However, large-scale parallel corpora do not always exist for most language pairs. Therefore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, various studies have been done to take advantage of multiple paths (Mann and Yarowsky, 2001) and even multiple pivot languages (Mausam et al., 2009) between the source and target languages. Since such automatically constructed lexicons usually contain noisy and polysemous entries, corpus-based occurrence information has been widely used to help rank the candidate target words (Schafer and Yarowsky, 2002; Kaji et al., 2008; Shezaf and Rappoport, 2010). Alternatively, extracting bilingual lexicons from comparable corpora assumes that words with similar meanings in different languag"
C12-1139,W00-1308,0,0.0815763,"as well as a state-of-the-art baseline for BLC. 2277 3.1 Comparable corpus In this paper, we generate a comparable corpus from the parallel Chinese-English Foreign Broadcast Information Service (FBIS) corpus, gathered from the news domain. This bilingual corpus contains about 240k sentences, 6.9 million words in Chinese and 8.9 million words in English. Similar to the way adopted in (Garera et al., 2009; Haghighi et al., 2008), we couple the first half of Chinese corpus and the second half on the English side as our comparable corpus. For corpus pre-processing, we use the Stanford POS tagger (Toutanova and Manning, 2000) and syntactic parser (Marneffe et al., 2006) to generate the POS and dependency information for each sentence in both Chinese and English corpora. Particularly, English words are transformed to their respective lemmas using the TreeTagger package (Helmut, 1994). 3.2 Bilingual lexicons for evaluation: seed, test and development In the literature, different scales of bilingual seed lexicons have been used. For example, Rapp (1999) and Fung (2000) used large-scale dictionaries of 10-20k word pairs while other studies (Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009) used only"
C12-1139,1994.amta-1.26,0,0.0960705,"存映射 Corresponding author * Proceedings of COLING 2012: Technical Papers, pages 2275–2290, COLING 2012, Mumbai, December 2012. 2275 1 Introduction Bilingual lexicons play an important role in many natural language processing tasks, such as machine translation (MT) (Och and Ney, 2003; Gong et al., 2011) and cross-language information retrieval (CLIR) (Grefenstette, 1998). Traditionally, bilingual lexicons are built manually with tremendous efforts. With the availability of large-scale parallel corpora, researchers turn to automatic construction from parallel corpora and achieve certain success (Wu and Xia, 1994). However, large-scale parallel corpora do not always exist for most language pairs. Therefore, researchers turn their attention to either pivot languages or non-parallel but comparable corpora. Using pivot languages in BLC was pioneered by Tanaka and Umemura (1994). Thereafter, various studies have been done to take advantage of multiple paths (Mann and Yarowsky, 2001) and even multiple pivot languages (Mausam et al., 2009) between the source and target languages. Since such automatically constructed lexicons usually contain noisy and polysemous entries, corpus-based occurrence information ha"
C12-1139,N09-2031,0,0.237284,"via canonical correlation analysis, which effectively explores monolingual lexicons in terms of latent matching. In particular for BLC without any external lexicon, Fung (1995) focused on context heterogeneity in Chinese and English languages, which measures how productive the context of a word is, instead of its absolute occurrence frequency. She suggested that bilingual translation words tend to share similar context heterogeneity in non-parallel corpora. Specifically, she calculated the similarity between two bilingual words using the ratios of unique words in the right and left contexts. Yu and Tsujii (2009) proposed the notion of dependency heterogeneity, which assumes that a word and its translation should share similar modifiers and heads in comparable corpora, no matter whether they occur in similar contexts or not. In this sense, our approach is similar to theirs. However, while their distance measure of dependency heterogeneity is limited to three easily-mapping common relationships between two languages, namely SUB, OBJ and NMOD, we further generalize to automatic mappings of any bilingual dependency relationships. Another difference is that our method considers dependent words and their r"
C12-1139,C02-1144,0,\N,Missing
C12-1139,P08-1088,0,\N,Missing
C12-2041,W05-0909,0,0.0336729,"Missing"
C12-2041,P92-1033,0,0.285973,"owever, there is a big difference in the usage of tenses in different languages. In inflectional languages like English, tense is often expressed by verb inflections and thus can be easily recognized. Whereas, some of the major Eastern Asian languages such as Chinese, Vietnamese and Thai, do not have the grammatical category of tense, and their tense is indicated by content words such as adverbs of time. So mapping a correct tense from source-side into target-side in this case is difficult and thus it poses challenges on current machine translation tasks. In some Interlingua-based MT systems (Dorr, 1992; Olsen et al., 2001; Wang and Seneff, 2006), tense information of the source language can be firstly transformed into an abstract languageindependent representation and passed to the target language. However, such research works have not been thoroughly studied since the definition of an Interlingua is already very difficult esp. for a wider domain. With the popularity of SMT, corpus-based methods of addressing tense problems for MT have been introduced (Ye and Zhang, 2005; Liu et al., 2011; Lee, 2011). However, these works just resolve tense recognition and do not address how to integrate th"
C12-2041,D12-1026,1,0.443298,"firstly transformed into an abstract languageindependent representation and passed to the target language. However, such research works have not been thoroughly studied since the definition of an Interlingua is already very difficult esp. for a wider domain. With the popularity of SMT, corpus-based methods of addressing tense problems for MT have been introduced (Ye and Zhang, 2005; Liu et al., 2011; Lee, 2011). However, these works just resolve tense recognition and do not address how to integrate their works into a realistic SMT system. There is little work on resolving tense error for SMT. Gong et al. (2012) propose target N-grambased tense models to improve translation performance. However, it is not reliable enough since they only consider the target-side tense information and SMT systems often generate abnormal outputs. For the source language with weak inflections or even without inflections, its contexts can provide valuable cues about tense. For example, some Chinese words, such as “~(JinChang, often)” and “…U(ZuoTian, yesterday)”, can obviously indicate present tense and past tense respectively. Our proposed SMT system, both source-side and target-side tense cues are employed. Aiming at t"
C12-2041,N03-1017,0,0.125254,"al. (2011), our predication is special in two aspects: (1) we only consider the major tense of source-side sentence and thus our classification accuracy is high enough; (2) we exploit more useful features for this task. Furthermore, we integrate our tense model into a popular SMT system and improve the translation results. 2.2 The system framework The work described in this paper is based on a modified Moses, a state-of-the-art phrase-based SMT system. The major modified parts for Moses are input and output modules in order to translate using document-level information. Our SMT system follows Koehn et al. (2003) and adopts similar six groups of features. Besides, the log-linear model(Och and Ney, 2000) is employed to linearly interpolate these features according to formula(1): e best = arg max e M X m=1 λm hm (e, f ) (1) where hm (e, f ) is a feature function, and λm is the weight of hm (e, f ) optimized by a discriminative training method on a held-out development data (Och, 2003). Classifier-based tense model can be easily integrated into the formula(1) by the following special features: ¨ 1 i f (t g = t s ) F2 = P(Ts_i |f ) F1 = 0 else F1 is a binary feature which encourages the decoder to prefer"
C12-2041,I11-1125,0,0.116943,"cult and thus it poses challenges on current machine translation tasks. In some Interlingua-based MT systems (Dorr, 1992; Olsen et al., 2001; Wang and Seneff, 2006), tense information of the source language can be firstly transformed into an abstract languageindependent representation and passed to the target language. However, such research works have not been thoroughly studied since the definition of an Interlingua is already very difficult esp. for a wider domain. With the popularity of SMT, corpus-based methods of addressing tense problems for MT have been introduced (Ye and Zhang, 2005; Liu et al., 2011; Lee, 2011). However, these works just resolve tense recognition and do not address how to integrate their works into a realistic SMT system. There is little work on resolving tense error for SMT. Gong et al. (2012) propose target N-grambased tense models to improve translation performance. However, it is not reliable enough since they only consider the target-side tense information and SMT systems often generate abnormal outputs. For the source language with weak inflections or even without inflections, its contexts can provide valuable cues about tense. For example, some Chinese words, such"
C12-2041,P03-1021,0,0.00601257,"based on a modified Moses, a state-of-the-art phrase-based SMT system. The major modified parts for Moses are input and output modules in order to translate using document-level information. Our SMT system follows Koehn et al. (2003) and adopts similar six groups of features. Besides, the log-linear model(Och and Ney, 2000) is employed to linearly interpolate these features according to formula(1): e best = arg max e M X m=1 λm hm (e, f ) (1) where hm (e, f ) is a feature function, and λm is the weight of hm (e, f ) optimized by a discriminative training method on a held-out development data (Och, 2003). Classifier-based tense model can be easily integrated into the formula(1) by the following special features: ¨ 1 i f (t g = t s ) F2 = P(Ts_i |f ) F1 = 0 else F1 is a binary feature which encourages the decoder to prefer hypothesis translations which have tense form conforming to the source-side contexts. Since tense of source-side sentence sometimes is not reliable enough, F2 is introduced to inform the decoder to what extent it can trust F1 , it indicates the confidence of the source-side tense classifier. It is worth noting that we don’t introduce the similar feature for the target side s"
C12-2041,N04-1021,0,0.0554589,"Missing"
C12-2041,P00-1056,0,0.504692,"f source-side sentence and thus our classification accuracy is high enough; (2) we exploit more useful features for this task. Furthermore, we integrate our tense model into a popular SMT system and improve the translation results. 2.2 The system framework The work described in this paper is based on a modified Moses, a state-of-the-art phrase-based SMT system. The major modified parts for Moses are input and output modules in order to translate using document-level information. Our SMT system follows Koehn et al. (2003) and adopts similar six groups of features. Besides, the log-linear model(Och and Ney, 2000) is employed to linearly interpolate these features according to formula(1): e best = arg max e M X m=1 λm hm (e, f ) (1) where hm (e, f ) is a feature function, and λm is the weight of hm (e, f ) optimized by a discriminative training method on a held-out development data (Och, 2003). Classifier-based tense model can be easily integrated into the formula(1) by the following special features: ¨ 1 i f (t g = t s ) F2 = P(Ts_i |f ) F1 = 0 else F1 is a binary feature which encourages the decoder to prefer hypothesis translations which have tense form conforming to the source-side contexts. Since"
C12-2041,2001.mtsummit-papers.47,0,0.0462034,"Missing"
C12-2041,P02-1040,0,0.085781,"Missing"
C12-2041,2006.amta-papers.25,0,0.0529701,"Missing"
C12-2041,vilar-etal-2006-error,0,0.0462898,"Missing"
C12-2041,I05-1077,0,0.0171145,"n this case is difficult and thus it poses challenges on current machine translation tasks. In some Interlingua-based MT systems (Dorr, 1992; Olsen et al., 2001; Wang and Seneff, 2006), tense information of the source language can be firstly transformed into an abstract languageindependent representation and passed to the target language. However, such research works have not been thoroughly studied since the definition of an Interlingua is already very difficult esp. for a wider domain. With the popularity of SMT, corpus-based methods of addressing tense problems for MT have been introduced (Ye and Zhang, 2005; Liu et al., 2011; Lee, 2011). However, these works just resolve tense recognition and do not address how to integrate their works into a realistic SMT system. There is little work on resolving tense error for SMT. Gong et al. (2012) propose target N-grambased tense models to improve translation performance. However, it is not reliable enough since they only consider the target-side tense information and SMT systems often generate abnormal outputs. For the source language with weak inflections or even without inflections, its contexts can provide valuable cues about tense. For example, some C"
C12-2065,P04-1079,0,0.0604812,"Missing"
C12-2065,W05-0909,0,0.472396,"ncorporated easily. Experiments also show the metric, compared with some popular metrics, achieves comparable correlation with human judgements at segmentlevel and significant higher correlation at document-level. 2 Related works In recent years, numerous ngram-based metrics have been proposed. BLEU (Papineni et al., 2002) as the most famous evaluation metric calculates an overall score via geometric mean of precisions on different ngrams. NIST (Doddington, 2002) improves BLEU with arithmetic mean and weight for different ngrams. However both BLEU and NIST do not consider synonyms. In METEOR (Banerjee and Lavie, 2005), three modules, “exact”, “porter stem” and “WN synonymy”, are used to create word-alignment successively. And a penalty for word-order is integrated into the final score. MAXSIM (Chan and Ng, 2008) constructs a bipartite graph for unmatched ngrams. And Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) is used to find a maximum weighted matching. However, synonyms in METEOR and MAXSIM are viewed as equivalent completely. Furthermore, nonsense ngrams are still used in these metrics. By contrast, in our metric, phrases are considered as the unit of evaluation and a fine similarity 1 2 http://en"
C12-2065,C04-1046,0,0.0399305,"ituations which will be illustrated in section 7.3. Take Figure 1 as an example. KM will choose Figure 1(a), because in both maps w r,1 has the same correspondence w t,1 while w r,2 has the correspondence w t,2 in Figure 1(a) and w t,4 in Figure 1(b). 7 Experiments We conduct experiments on MTC Part 2 (LDC2003T17) which contains 100 source documents (878 segments in total) in Chinese and 4 English references for each segment. Translations of three systems were assessed by human judges on each segments in terms of adequacy (Adq) and fluency (Flu). We normalize the human raw scores according to Blatz et al. (2004) and average scores for segments. Document score is the average of scores of its segments. Before evaluation, translations are tokenized and lower-cased. In our default metric PBE (or PBEngram ), α is set to 0.2, both β and γ are 0.5, θ is 1 and phrase weight function is ngram. In this paper, only NP and VP are taken into consideration since they contain more information and give a stable evaluation in our preliminary experiments. Pearson correlation coefficient is used to measure correlation between automatic evaluation and human judgements. 668 7.1 Performance of default metric According to"
C12-2065,P08-1007,0,0.0916815,"t-level. 2 Related works In recent years, numerous ngram-based metrics have been proposed. BLEU (Papineni et al., 2002) as the most famous evaluation metric calculates an overall score via geometric mean of precisions on different ngrams. NIST (Doddington, 2002) improves BLEU with arithmetic mean and weight for different ngrams. However both BLEU and NIST do not consider synonyms. In METEOR (Banerjee and Lavie, 2005), three modules, “exact”, “porter stem” and “WN synonymy”, are used to create word-alignment successively. And a penalty for word-order is integrated into the final score. MAXSIM (Chan and Ng, 2008) constructs a bipartite graph for unmatched ngrams. And Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) is used to find a maximum weighted matching. However, synonyms in METEOR and MAXSIM are viewed as equivalent completely. Furthermore, nonsense ngrams are still used in these metrics. By contrast, in our metric, phrases are considered as the unit of evaluation and a fine similarity 1 2 http://en.wikipedia.org/wiki/Phrase http://jtextpro.sourceforge.net/ 664 function is defined. And context information contributes to our metric as well. Phrase information has been used in several evaluation"
C12-2065,W11-2107,0,0.0539753,"Missing"
C12-2065,P10-1012,0,0.0150271,"similarity 1 2 http://en.wikipedia.org/wiki/Phrase http://jtextpro.sourceforge.net/ 664 function is defined. And context information contributes to our metric as well. Phrase information has been used in several evaluation methods. In the work of Giménez and Màrquez (2007), overlapping is calculated on the set of words within a same phrase type, and sequences of phrase types are used in the metric of NIST to score phrase-order. However, this work does not distinguish different phrases with the same type and ignores the fact that different types of phrases can be established a correspondence. Echizen-ya and Araki (2010) propose to establish correspondence of phrases for which mutual similarity score is highest. But this method just takes NP into consideration since similarity based on PER (Su et al., 1992) cannot determine the correspondence of VP correctly. Zhou et al. (2008) diagnoses translations based on check-points where each phrase can be scored by ngram matching. However, it ignores the order of phrases in a translation and phrase correspondence relies on word-alignment trained on parallel corpus. Different with these works, this paper treats a phrase as a single unit and integrates explicit measurem"
C12-2065,W07-0738,0,0.0273435,"raph for unmatched ngrams. And Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) is used to find a maximum weighted matching. However, synonyms in METEOR and MAXSIM are viewed as equivalent completely. Furthermore, nonsense ngrams are still used in these metrics. By contrast, in our metric, phrases are considered as the unit of evaluation and a fine similarity 1 2 http://en.wikipedia.org/wiki/Phrase http://jtextpro.sourceforge.net/ 664 function is defined. And context information contributes to our metric as well. Phrase information has been used in several evaluation methods. In the work of Giménez and Màrquez (2007), overlapping is calculated on the set of words within a same phrase type, and sequences of phrase types are used in the metric of NIST to score phrase-order. However, this work does not distinguish different phrases with the same type and ignores the fact that different types of phrases can be established a correspondence. Echizen-ya and Araki (2010) propose to establish correspondence of phrases for which mutual similarity score is highest. But this method just takes NP into consideration since similarity based on PER (Su et al., 1992) cannot determine the correspondence of VP correctly. Zho"
C12-2065,D11-1084,1,0.89731,"Missing"
C12-2065,P02-1040,0,0.0964189,"o the last part, we address how to establish a maximum similarity map between phrases of candidates and references and further analyze its working mechanism by experiments. It is worth to mention that our metric has a great flexibility such that any other similarity and weight functions could be incorporated easily. Experiments also show the metric, compared with some popular metrics, achieves comparable correlation with human judgements at segmentlevel and significant higher correlation at document-level. 2 Related works In recent years, numerous ngram-based metrics have been proposed. BLEU (Papineni et al., 2002) as the most famous evaluation metric calculates an overall score via geometric mean of precisions on different ngrams. NIST (Doddington, 2002) improves BLEU with arithmetic mean and weight for different ngrams. However both BLEU and NIST do not consider synonyms. In METEOR (Banerjee and Lavie, 2005), three modules, “exact”, “porter stem” and “WN synonymy”, are used to create word-alignment successively. And a penalty for word-order is integrated into the final score. MAXSIM (Chan and Ng, 2008) constructs a bipartite graph for unmatched ngrams. And Kuhn-Munkres algorithm (Kuhn, 1955; Munkres,"
C12-2065,C92-2067,0,0.0620893,"in several evaluation methods. In the work of Giménez and Màrquez (2007), overlapping is calculated on the set of words within a same phrase type, and sequences of phrase types are used in the metric of NIST to score phrase-order. However, this work does not distinguish different phrases with the same type and ignores the fact that different types of phrases can be established a correspondence. Echizen-ya and Araki (2010) propose to establish correspondence of phrases for which mutual similarity score is highest. But this method just takes NP into consideration since similarity based on PER (Su et al., 1992) cannot determine the correspondence of VP correctly. Zhou et al. (2008) diagnoses translations based on check-points where each phrase can be scored by ngram matching. However, it ignores the order of phrases in a translation and phrase correspondence relies on word-alignment trained on parallel corpus. Different with these works, this paper treats a phrase as a single unit and integrates explicit measurement of phrase-order into metric and correspondence is established by fine similarities between phrases. 3 Phrase-based evaluation metric Phrase-based evaluation (PBE) metric proposed by this"
C12-2065,C08-1141,0,0.0161333,"07), overlapping is calculated on the set of words within a same phrase type, and sequences of phrase types are used in the metric of NIST to score phrase-order. However, this work does not distinguish different phrases with the same type and ignores the fact that different types of phrases can be established a correspondence. Echizen-ya and Araki (2010) propose to establish correspondence of phrases for which mutual similarity score is highest. But this method just takes NP into consideration since similarity based on PER (Su et al., 1992) cannot determine the correspondence of VP correctly. Zhou et al. (2008) diagnoses translations based on check-points where each phrase can be scored by ngram matching. However, it ignores the order of phrases in a translation and phrase correspondence relies on word-alignment trained on parallel corpus. Different with these works, this paper treats a phrase as a single unit and integrates explicit measurement of phrase-order into metric and correspondence is established by fine similarities between phrases. 3 Phrase-based evaluation metric Phrase-based evaluation (PBE) metric proposed by this paper compares a pair of candidatereference translation by identifying"
C12-2067,W06-1655,0,0.0242448,"ce labelling algorithms (Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both invocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling approach, many related studies make efforts to improve the performance by various means, such as using more tags and features (Tang et al., 2009; Zhao et al., 2006), employing word-based tagging without tagging (Zhang and Clark, 2007), employing some joint models that combines a generative model and a discriminative model (Wang et al., 2010; Wang et al., 2011) or Markov and semi-Markov CRF (Andrew, 2006), and integrating unsupervised segmentation features (Zhao and Kit, 2011). Although there are various studies CWS individually, there are few studies of active learning on CWS. One related work is about active learning on Japanese word segmentation via Support Vector Machines (SVM) (Sassano, 2002). However, both the two challenging problems mentioned above are unsolved. Specifically, that study annotates the whole sentence as a basic unit, which means much more annotation effort than our model. Furthermore, our corpus scale is much larger than the one in Sassano (2002). This makes SVM impracti"
C12-2067,P07-1007,0,0.0489837,"Missing"
C12-2067,N06-1016,0,0.0567906,"Missing"
C12-2067,I05-3017,0,0.10462,"Missing"
C12-2067,J98-4002,0,0.0700262,"Missing"
C12-2067,P07-2018,1,0.755901,"selected for oracle labelling; Uncertainty-Diversity sampling: In each iteration, all the instances in the unlabeled data U are ranked according to their uncertainty-diversity values and top instances are selected for oracle labeling. 4 4.1 Experimentation Experimental Setting The SIGHAN Bakeoff 2 dataset consists of four different corpora: PKU, MSR, CityU, and AS. But we only report the performance on three of the corpora except AS due to its significant large scale in causing the out-of-memory error. The basic segmenter in the active learning process is trained with a 2-tag labelling model (Huang et al., 2007; Huang and Xue, 2012) and implemented with a public tool for CRF implementation, i.e. CRF++ (Kudo, 2005). For the feature template, we adopt the one by Li and Huang (2009). In all experiments, we use the standard F1 score as our main performance measurement. Besides, the out-of-vocabulary (OOV) recall is used to evaluate the OOV issue. 4.2 Experimental Results In this experiment, we compare the random selection strategy and the two sampling strategies as illustrated in Section 3.3: uncertainty sampling and uncertainty-diversity sampling. To fairly compare the performances of different samplin"
C12-2067,P08-1102,0,0.0582361,"Missing"
C12-2067,Y09-2034,1,0.89236,"sity values and top instances are selected for oracle labeling. 4 4.1 Experimentation Experimental Setting The SIGHAN Bakeoff 2 dataset consists of four different corpora: PKU, MSR, CityU, and AS. But we only report the performance on three of the corpora except AS due to its significant large scale in causing the out-of-memory error. The basic segmenter in the active learning process is trained with a 2-tag labelling model (Huang et al., 2007; Huang and Xue, 2012) and implemented with a public tool for CRF implementation, i.e. CRF++ (Kudo, 2005). For the feature template, we adopt the one by Li and Huang (2009). In all experiments, we use the standard F1 score as our main performance measurement. Besides, the out-of-vocabulary (OOV) recall is used to evaluate the OOV issue. 4.2 Experimental Results In this experiment, we compare the random selection strategy and the two sampling strategies as illustrated in Section 3.3: uncertainty sampling and uncertainty-diversity sampling. To fairly compare the performances of different sampling strategies, we make sure that the number of annotated boundaries in either uncertainty sampling or uncertainty-diversity sampling is the same as random selection. Figure"
C12-2067,D12-1013,1,0.8313,"Missing"
C12-2067,W04-3236,0,0.0922903,"Missing"
C12-2067,P02-1064,0,0.529185,"ive learning on CWS. First, the state-of-the-art methods treat CWS as a sequence labelling task (Jiang et al., 2008; Ng and Low, 2004; Tseng et al., 2005; Zhang et al., 2006), i.e. labelling characters with tags from a pre-defined tag set, representing the position of a character in a word. Different from traditional classification tasks, each character is tagged sequentially according to its corresponding context. Under this circumstance, a character cannot be determined as a single unit to query in active learning. One possible solution is to select one sentence as a unit for annotation, as Sassano (2002) does for Japanese word segmentation. However, such solution is expensive for annotation and since one sentence might contain some words which can be easily segmented correctly by existing models with high confidence, annotating them becomes a waste of time and manual effort. Second, the number of the characters in a CWS corpus is normally extremely huge. For example, among the four corpora in SIGHAN Bakeoff 2 (Emerson, 2005), even the smallest corpus contains more than 1,800,000 characters while others are much larger in the order of tens of millions of characters. Compared to other tasks lik"
C12-2067,D08-1112,0,0.0916365,"ount to avoid duplicate annotation. For example, in the example E-A above, both the words ‘索拉纳 ’ and ‘波 兰 ’ are unknown words for the initial segmenter learned by the initial labelled set L with the boundaries of I A1 , I A 2 , I A9 , I B1 , I B 2 , and I B 9 , among the top uncertain instances. Obviously, some boundaries share the same segmentation information, e.g., I A1 and I B1 . Therefore, labelling both of them is a waste. One straightforward way to handle such duplicate annotation is to compute the similarity between every two instances and then pick those with the highest diversities (Settles and Craven, 2008). This method, however, requires O(N2) in computational complexity where N is the number of all boundaries. When N is huge (e.g. N&gt;1,800,000 in our experiments), the high computational burden is simply unacceptable. Fortunately, we find that the similarity between two boundaries is highly related to their surrounding character N-grams (in particular bigrams) and we can better evaluate the diversity with the help of the surrounding character bigrams. This is done in this paper by recording the frequencies of all surrounding bigrams in a set Scc , where f ci ci +1 ∈ S cc indicates the frequency"
C12-2067,P04-1075,1,0.646546,"Missing"
C12-2067,P98-2206,0,0.0867883,"For the second challenge, we propose 684 a diversity measurement among the instances to avoid duplicate annotation, so as to further reduce the annotation efforts. 2 Related Work Research on CWS has a long history and various methods have been proposed in the literature. Basically, these methods are mainly focus on two categories: unsupervised and supervised. Unsupervised methods aim to build a segmentation system without any lexicon or labelled data. They often start from an empirical definition of a word and then use some statistical measures, e.g. mutual information (Sproat and Shih, 1990; Sun et al., 1998), to learn words from a large unlabelled data resource. Although these unsupervised methods can capture many strong words, their performance is often not high enough for the practical use. Supervised methods, such as HMM tagging (Xue, 2003), character-based classification (Wang et al., 2008) and morpheme-based lexical chunking (Fu et al., 2008), attempt to acquire a model based on a dictionary or a labelled data set. Among them, character-based classification has drawn most attention recently and been further implemented with sequence labelling algorithms (Tseng et al., 2005), e.g., conditiona"
C12-2067,I05-3027,0,0.152497,"ttles and Craven, 2008). Although active learning has been widely employed to many NLP tasks, such as word sense disambiguation (Chan and Ng, 2007; Chen et al., 2006; Fujii et al., 1998), text categorization (Lewis and Gale, 1994; Liere and Tadepalli, 1997; McCallum and Nigam, 1998; Li et al., 2012), and named entity recognition (Shen et al., 2004), there are few studies of active learning on CWS, probably due to the strong challenges inherent in performing active learning on CWS. First, the state-of-the-art methods treat CWS as a sequence labelling task (Jiang et al., 2008; Ng and Low, 2004; Tseng et al., 2005; Zhang et al., 2006), i.e. labelling characters with tags from a pre-defined tag set, representing the position of a character in a word. Different from traditional classification tasks, each character is tagged sequentially according to its corresponding context. Under this circumstance, a character cannot be determined as a single unit to query in active learning. One possible solution is to select one sentence as a unit for annotation, as Sassano (2002) does for Japanese word segmentation. However, such solution is expensive for annotation and since one sentence might contain some words wh"
C12-2067,C10-1132,0,0.014142,"rawn most attention recently and been further implemented with sequence labelling algorithms (Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both invocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling approach, many related studies make efforts to improve the performance by various means, such as using more tags and features (Tang et al., 2009; Zhao et al., 2006), employing word-based tagging without tagging (Zhang and Clark, 2007), employing some joint models that combines a generative model and a discriminative model (Wang et al., 2010; Wang et al., 2011) or Markov and semi-Markov CRF (Andrew, 2006), and integrating unsupervised segmentation features (Zhao and Kit, 2011). Although there are various studies CWS individually, there are few studies of active learning on CWS. One related work is about active learning on Japanese word segmentation via Support Vector Machines (SVM) (Sassano, 2002). However, both the two challenging problems mentioned above are unsolved. Specifically, that study annotates the whole sentence as a basic unit, which means much more annotation effort than our model. Furthermore, our corpus scale is mu"
C12-2067,O03-4002,0,0.155392,"posed in the literature. Basically, these methods are mainly focus on two categories: unsupervised and supervised. Unsupervised methods aim to build a segmentation system without any lexicon or labelled data. They often start from an empirical definition of a word and then use some statistical measures, e.g. mutual information (Sproat and Shih, 1990; Sun et al., 1998), to learn words from a large unlabelled data resource. Although these unsupervised methods can capture many strong words, their performance is often not high enough for the practical use. Supervised methods, such as HMM tagging (Xue, 2003), character-based classification (Wang et al., 2008) and morpheme-based lexical chunking (Fu et al., 2008), attempt to acquire a model based on a dictionary or a labelled data set. Among them, character-based classification has drawn most attention recently and been further implemented with sequence labelling algorithms (Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both invocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling approach, many related studies make efforts to improve the performance by various means, such as"
C12-2067,W03-1730,0,0.0549875,"Missing"
C12-2067,P06-2123,0,0.046661,"Missing"
C12-2067,P07-1106,0,0.0245741,"to acquire a model based on a dictionary or a labelled data set. Among them, character-based classification has drawn most attention recently and been further implemented with sequence labelling algorithms (Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both invocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling approach, many related studies make efforts to improve the performance by various means, such as using more tags and features (Tang et al., 2009; Zhao et al., 2006), employing word-based tagging without tagging (Zhang and Clark, 2007), employing some joint models that combines a generative model and a discriminative model (Wang et al., 2010; Wang et al., 2011) or Markov and semi-Markov CRF (Andrew, 2006), and integrating unsupervised segmentation features (Zhao and Kit, 2011). Although there are various studies CWS individually, there are few studies of active learning on CWS. One related work is about active learning on Japanese word segmentation via Support Vector Machines (SVM) (Sassano, 2002). However, both the two challenging problems mentioned above are unsolved. Specifically, that study annotates the whole sentence"
C12-2067,I08-4017,0,0.204369,"Missing"
C12-2067,Y06-1012,0,0.0603917,"8) and morpheme-based lexical chunking (Fu et al., 2008), attempt to acquire a model based on a dictionary or a labelled data set. Among them, character-based classification has drawn most attention recently and been further implemented with sequence labelling algorithms (Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both invocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling approach, many related studies make efforts to improve the performance by various means, such as using more tags and features (Tang et al., 2009; Zhao et al., 2006), employing word-based tagging without tagging (Zhang and Clark, 2007), employing some joint models that combines a generative model and a discriminative model (Wang et al., 2010; Wang et al., 2011) or Markov and semi-Markov CRF (Andrew, 2006), and integrating unsupervised segmentation features (Zhao and Kit, 2011). Although there are various studies CWS individually, there are few studies of active learning on CWS. One related work is about active learning on Japanese word segmentation via Support Vector Machines (SVM) (Sassano, 2002). However, both the two challenging problems mentioned abov"
C12-2067,C08-1059,0,\N,Missing
C12-2067,I08-4015,0,\N,Missing
C12-2130,W05-0305,0,0.0873018,"In Section 3 the PDTB corpus is briefly introduced. Section 4 describes the methodology used for exact argument identification. In Section 5 the results of the research experiment are presented. Finally, some conclusions are drawn. 2 Related Work Related work on PDTB-style discourse parsing and shallow semantic parsing is presented in this section. PDTB-style discourse parsing consists of two major sub-tasks: Discourse Argument Identification (DAI) and Discourse Relation Identification (DRI). Related work for PDTB-style DAI can be mainly classified into three categories: rule-based approach, Dinesh et al. (2005); Prasad et al., (2010), classification-based method, Wellner et al. (2007); Elwell et al. (2008); Lin et al. (2010) and chunking-based approach, Ghosh et al. (2011a) (2011b) (2012). To be more specific, Dinesh et al. (2005) proposed a tree subtraction method for restricted subordinating connectives. Prasad et al. (2010) provided a set of scope-based filters for argument identification. Wellner et al. (2007) and Elwell et al. (2008) investigated the matching of head-words located in 1332 the argument. However, a potential issue of their work is that no golden head-words were annotated in the P"
C12-2130,I11-1120,0,0.386259,"rch experiment are presented. Finally, some conclusions are drawn. 2 Related Work Related work on PDTB-style discourse parsing and shallow semantic parsing is presented in this section. PDTB-style discourse parsing consists of two major sub-tasks: Discourse Argument Identification (DAI) and Discourse Relation Identification (DRI). Related work for PDTB-style DAI can be mainly classified into three categories: rule-based approach, Dinesh et al. (2005); Prasad et al., (2010), classification-based method, Wellner et al. (2007); Elwell et al. (2008); Lin et al. (2010) and chunking-based approach, Ghosh et al. (2011a) (2011b) (2012). To be more specific, Dinesh et al. (2005) proposed a tree subtraction method for restricted subordinating connectives. Prasad et al. (2010) provided a set of scope-based filters for argument identification. Wellner et al. (2007) and Elwell et al. (2008) investigated the matching of head-words located in 1332 the argument. However, a potential issue of their work is that no golden head-words were annotated in the PDTB. Lin et al. (2010) proposed a token-level argument node identifier, which determined whether each internal node was an Arg1, Arg2 or Non-argument, and then cond"
C12-2130,W12-1622,0,0.0651841,"e subtraction method for restricted subordinating connectives. Prasad et al. (2010) provided a set of scope-based filters for argument identification. Wellner et al. (2007) and Elwell et al. (2008) investigated the matching of head-words located in 1332 the argument. However, a potential issue of their work is that no golden head-words were annotated in the PDTB. Lin et al. (2010) proposed a token-level argument node identifier, which determined whether each internal node was an Arg1, Arg2 or Non-argument, and then conducted a tree subtraction algorithm to extract the argument of connectives. Ghosh et al. (2012) which integrated the n-best result of the previous token-level approach (Ghosh et al, 2011a) into their global sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and"
C12-2130,W11-4616,0,0.0388588,"Missing"
C12-2130,W10-2910,0,0.0381901,"Missing"
C12-2130,P09-1078,0,0.0295215,"‘and ensure that’, after the connective ‘and’, sometimes denotes abstract objects located in the previous sentence. Based on this observation, we add 8 new features: next1, next1 POS, next1+C, next1 POS+C POS, next2, next2 POS, next2+C, next2 POS+C POS. It is hard to decide which feature-set is more effective for Arg1 position identification, even if we use the Hill-climbing (greedy) feature selection technique, Caruana and Freitag (1994), due to the combination of a large number of different features. Therefore, we adopt the Information Gain (IG), which is widely used in text classification, Li et al. (2009), to calculate the efficacy of features and select an approximate optimal feature-set. After Arg1’s position is identified, we handle the DAI according to intra-sentence and intersentence cases methodologies, as follows. 4.1 Formulating Intra-sentence DAI as a Simplified Semantic Parsing Problem Given a parse tree and a predicate, shallow semantic parsing detects and classifies each of the constituents in the sentence into either their corresponding semantic argument (role) for the predicate, or as a non-argument. Similarly, the discourse connective can be taken as the predicate, while its sco"
C12-2130,P10-1113,1,0.848622,"lobal sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al. (2001), chunking-based, Morante et al. (2009) and shallow semantic parsing-based, Li et al. (2010b) and Zhu et al. (2010). 3 Penn Discourse Treebank (PDTB): an Introduction Currently, PDTB is the largest available discourse corpus. It has annotated 40,600 discourse relations, presented as five relation types: Explicit, Implicit, Alternative Lexicalization (AltLex), Entity-based coherence Relatio"
C12-2130,C10-1076,1,0.723365,"lobal sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al. (2001), chunking-based, Morante et al. (2009) and shallow semantic parsing-based, Li et al. (2010b) and Zhu et al. (2010). 3 Penn Discourse Treebank (PDTB): an Introduction Currently, PDTB is the largest available discourse corpus. It has annotated 40,600 discourse relations, presented as five relation types: Explicit, Implicit, Alternative Lexicalization (AltLex), Entity-based coherence Relatio"
C12-2130,D09-1036,0,0.0232391,"notated in the PDTB. Lin et al. (2010) proposed a token-level argument node identifier, which determined whether each internal node was an Arg1, Arg2 or Non-argument, and then conducted a tree subtraction algorithm to extract the argument of connectives. Ghosh et al. (2012) which integrated the n-best result of the previous token-level approach (Ghosh et al, 2011a) into their global sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al. (2001), chunking-bas"
C12-2130,P11-3009,0,0.0281253,"Missing"
C12-2130,W09-1105,0,0.0648834,"Missing"
C12-2130,P04-1043,0,0.0519013,", 2011a) into their global sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al. (2001), chunking-based, Morante et al. (2009) and shallow semantic parsing-based, Li et al. (2010b) and Zhu et al. (2010). 3 Penn Discourse Treebank (PDTB): an Introduction Currently, PDTB is the largest available discourse corpus. It has annotated 40,600 discourse relations, presented as five relation types: Explicit, Implicit, Alternative Lexicalization (AltLex), Entity-bas"
C12-2130,P09-1077,0,0.0437391,"lden head-words were annotated in the PDTB. Lin et al. (2010) proposed a token-level argument node identifier, which determined whether each internal node was an Arg1, Arg2 or Non-argument, and then conducted a tree subtraction algorithm to extract the argument of connectives. Ghosh et al. (2012) which integrated the n-best result of the previous token-level approach (Ghosh et al, 2011a) into their global sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al."
C12-2130,P09-2004,0,0.104768,"Missing"
C12-2130,prasad-etal-2008-penn,0,0.0608383,"t identification. In Section 5 the results of the research experiment are presented. Finally, some conclusions are drawn. 2 Related Work Related work on PDTB-style discourse parsing and shallow semantic parsing is presented in this section. PDTB-style discourse parsing consists of two major sub-tasks: Discourse Argument Identification (DAI) and Discourse Relation Identification (DRI). Related work for PDTB-style DAI can be mainly classified into three categories: rule-based approach, Dinesh et al. (2005); Prasad et al., (2010), classification-based method, Wellner et al. (2007); Elwell et al. (2008); Lin et al. (2010) and chunking-based approach, Ghosh et al. (2011a) (2011b) (2012). To be more specific, Dinesh et al. (2005) proposed a tree subtraction method for restricted subordinating connectives. Prasad et al. (2010) provided a set of scope-based filters for argument identification. Wellner et al. (2007) and Elwell et al. (2008) investigated the matching of head-words located in 1332 the argument. However, a potential issue of their work is that no golden head-words were annotated in the PDTB. Lin et al. (2010) proposed a token-level argument node identifier, which determined whether"
C12-2130,prasad-etal-2010-exploiting,0,0.0616553,"corpus is briefly introduced. Section 4 describes the methodology used for exact argument identification. In Section 5 the results of the research experiment are presented. Finally, some conclusions are drawn. 2 Related Work Related work on PDTB-style discourse parsing and shallow semantic parsing is presented in this section. PDTB-style discourse parsing consists of two major sub-tasks: Discourse Argument Identification (DAI) and Discourse Relation Identification (DRI). Related work for PDTB-style DAI can be mainly classified into three categories: rule-based approach, Dinesh et al. (2005); Prasad et al., (2010), classification-based method, Wellner et al. (2007); Elwell et al. (2008); Lin et al. (2010) and chunking-based approach, Ghosh et al. (2011a) (2011b) (2012). To be more specific, Dinesh et al. (2005) proposed a tree subtraction method for restricted subordinating connectives. Prasad et al. (2010) provided a set of scope-based filters for argument identification. Wellner et al. (2007) and Elwell et al. (2008) investigated the matching of head-words located in 1332 the argument. However, a potential issue of their work is that no golden head-words were annotated in the PDTB. Lin et al. (2010)"
C12-2130,D09-1018,0,0.0740607,"Missing"
C12-2130,P10-1073,0,0.0119903,". Lin et al. (2010) proposed a token-level argument node identifier, which determined whether each internal node was an Arg1, Arg2 or Non-argument, and then conducted a tree subtraction algorithm to extract the argument of connectives. Ghosh et al. (2012) which integrated the n-best result of the previous token-level approach (Ghosh et al, 2011a) into their global sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al. (2001), chunking-based, Morante et al. ("
C12-2130,D07-1010,0,0.37298,"Missing"
C12-2130,W04-3212,0,0.0970137,"Missing"
C12-2130,C10-2172,0,0.0124451,"proposed a token-level argument node identifier, which determined whether each internal node was an Arg1, Arg2 or Non-argument, and then conducted a tree subtraction algorithm to extract the argument of connectives. Ghosh et al. (2012) which integrated the n-best result of the previous token-level approach (Ghosh et al, 2011a) into their global sentence-level method, significantly improved the method’s DAI performance. Compared with DAI, explicit and implicit discourse relation identification has been studied more recently, such as Pitler et al. (2009a); Lin et al. (2009); Wang et al. (2010); Zhou et al. (2010); Hong et al. (2012). However, due to inherent difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al. (2001), chunking-based, Morante et al. (2009) and shallow se"
C12-2130,D10-1070,1,0.724788,"nt difficulties within the implicit discourse relation, its performance is still very low. Shallow semantic parsing, used to answer ‘the five Ws’ (Who, What, When, Where and Why) questions in a sentence, has been extensively studied in recent years, such as Moschitti (2004) and Li et al. (2010a). Scope learning, a specific shallow semantic parsing problem is also related to DAI. Most existing research on scope learning can be further classified by methodology into rule-based, Chapman et al. (2001), chunking-based, Morante et al. (2009) and shallow semantic parsing-based, Li et al. (2010b) and Zhu et al. (2010). 3 Penn Discourse Treebank (PDTB): an Introduction Currently, PDTB is the largest available discourse corpus. It has annotated 40,600 discourse relations, presented as five relation types: Explicit, Implicit, Alternative Lexicalization (AltLex), Entity-based coherence Relation(EntRel) and No Relation (NoRel). PDTB regards connectives as the discourse predicate, taking two text spans as two arguments, Arg1 and Arg2, which describe the events, facts and/or propositions. Of the two arguments Arg2 is syntactically bound to the connective, while Arg 1 is not. In addition, 3-layered hierarchy, sema"
C12-2130,J03-4003,0,\N,Missing
C14-1050,P13-1024,0,0.0234788,"searches, our study is forced on skill inference instead of traditional tag suggestion. Basically, the social connections in skill inference are much different from those in social tagging. In our study, we use co-major, co-title and other academic and business relationships to build the social connections. Meanwhile, there are also few researches concern to propose a joint model to leverage both personal and skill connections. 2.3 Factor Graph Model Among various approaches investigated in social networks in the last several years (Leskovec et al., 2010; Lu et al., 2010; Lampos et al., 2013; Guo et al., 2013), Factor Graph Model (FGM) becomes an effective way to represent and optimize the relationship in social networks (Dong et al., 2012; Yang et al., 2012b) via a graph structure. Tang et al. (2011a) and Zhuang et al. (2012) formalized the problem of social relationship learning as a semi-supervised framework, and proposed Partially-labeled Pairwise Factor Graph Model (PLP-FGM) for inferring the types of social ties. Tang et al. (2013) further proposed a factor graph based distributed learning method to construct a conformity influence model and formalize the effects of social conformity in a pro"
C14-1050,P13-1098,0,0.0205867,"fferent from above researches, our study is forced on skill inference instead of traditional tag suggestion. Basically, the social connections in skill inference are much different from those in social tagging. In our study, we use co-major, co-title and other academic and business relationships to build the social connections. Meanwhile, there are also few researches concern to propose a joint model to leverage both personal and skill connections. 2.3 Factor Graph Model Among various approaches investigated in social networks in the last several years (Leskovec et al., 2010; Lu et al., 2010; Lampos et al., 2013; Guo et al., 2013), Factor Graph Model (FGM) becomes an effective way to represent and optimize the relationship in social networks (Dong et al., 2012; Yang et al., 2012b) via a graph structure. Tang et al. (2011a) and Zhuang et al. (2012) formalized the problem of social relationship learning as a semi-supervised framework, and proposed Partially-labeled Pairwise Factor Graph Model (PLP-FGM) for inferring the types of social ties. Tang et al. (2013) further proposed a factor graph based distributed learning method to construct a conformity influence model and formalize the effects of social"
C14-1050,C12-2064,0,0.0132631,"heir information in an unconstrained manner (Ohkura et al., 2006; Si et al., 2010). Ohkura et al. (2006) created a multi-tagger to determine whether a particular tag from a candidate tag list should be attached to a weblog. Lappas et al. (2011) proposed a social endorsement-based approach to generate social tags from Twitter.com and Flickr.com where various kinds of information in recommendations and comments are used. Liu et al. (2012) propose a probabilistic model to connect the semantic relations between words and tags of microblog, and takes the social network structure as regularization. Li et al., (2012) propose to model context-aware relations of tags for suggestion by regarding resource content as context of tags. 521 Different from above researches, our study is forced on skill inference instead of traditional tag suggestion. Basically, the social connections in skill inference are much different from those in social tagging. In our study, we use co-major, co-title and other academic and business relationships to build the social connections. Meanwhile, there are also few researches concern to propose a joint model to leverage both personal and skill connections. 2.3 Factor Graph Model Amo"
C14-1050,D11-1146,0,0.0313099,"Missing"
C14-1050,C12-2074,0,0.012552,"ese skills is fully leveraged in the inference. 2.2 Social Tag Suggestion Social tag suggestion aims to extract proper tags from social media and can thus help people organize their information in an unconstrained manner (Ohkura et al., 2006; Si et al., 2010). Ohkura et al. (2006) created a multi-tagger to determine whether a particular tag from a candidate tag list should be attached to a weblog. Lappas et al. (2011) proposed a social endorsement-based approach to generate social tags from Twitter.com and Flickr.com where various kinds of information in recommendations and comments are used. Liu et al. (2012) propose a probabilistic model to connect the semantic relations between words and tags of microblog, and takes the social network structure as regularization. Li et al., (2012) propose to model context-aware relations of tags for suggestion by regarding resource content as context of tags. 521 Different from above researches, our study is forced on skill inference instead of traditional tag suggestion. Basically, the social connections in skill inference are much different from those in social tagging. In our study, we use co-major, co-title and other academic and business relationships to bu"
C14-1050,C10-1114,0,0.0165272,"relationship between persons). Expert finding is in nature different from skill inference. Our study predicts various skills attachable to a person collectively with both personal and skill connections among people. One distinguishing characteristics of our study is that several skills from a person are simultaneously modeled and the relationship among these skills is fully leveraged in the inference. 2.2 Social Tag Suggestion Social tag suggestion aims to extract proper tags from social media and can thus help people organize their information in an unconstrained manner (Ohkura et al., 2006; Si et al., 2010). Ohkura et al. (2006) created a multi-tagger to determine whether a particular tag from a candidate tag list should be attached to a weblog. Lappas et al. (2011) proposed a social endorsement-based approach to generate social tags from Twitter.com and Flickr.com where various kinds of information in recommendations and comments are used. Liu et al. (2012) propose a probabilistic model to connect the semantic relations between words and tags of microblog, and takes the social network structure as regularization. Li et al., (2012) propose to model context-aware relations of tags for suggestion"
C14-1050,P02-1053,0,0.00901377,"Missing"
C14-1204,D13-1178,0,0.037849,"Missing"
C14-1204,P08-1090,0,0.0926641,"Missing"
C14-1204,P09-1068,0,0.0867911,"Missing"
C14-1204,P11-1098,0,0.0373346,"Missing"
C14-1204,W09-2209,0,0.0625834,"Missing"
C14-1204,N09-2053,0,0.0634474,"Missing"
C14-1204,E12-1029,0,0.314158,"from free texts is a higher-level Information Extraction (IE) task, which is still a challenge due to the complexity of natural language and the domain-specific nature, especially in Chinese for its specific characteristics. In particular, most of previous studies have focused on English event extraction, while only a few concern Chinese. Currently, supervised learning models have dominated event extraction. To reduce the labeled data required, a few semi-supervised models have been applied to English event extraction (e.g., Riloff 1996; Yangarber et al., 2000; Stevenson and Greenwood, 2005; Huang and Riloff, 2012). Since classifier-based model needs dozens of annotated documents to train model, most of previous semisupervised models focused on pattern-based approach, which only needed a few seed (event) patterns. In those pattern-based approaches, frequent event patterns, which occur in many documents, were chosen as relevant patterns to match event mentions in unlabeled texts. However, the order of words in a Chinese sentence is rather agile for its open and flexible structure, and different orders might express the same meaning due to the semantics-driven nature of the Chinese language. This results"
C14-1204,P08-1030,0,0.034344,"Missing"
C14-1204,C12-1099,1,0.904356,"o single-morpheme word to generate two candidate trigger mentions when the following three conditions are satisfied: 1) verb  POS( m1 )  verb∈ POS( m2 ) 2) Max ( Wsim ( m1 , s1 ))  1∧ Max ( Wsim ( m2 , s2 ))  1∧ Etype( s1 ) ≠ Etype( s2 ) s1 ∈seeds s2∈seeds 3) Morph(m1 m2)= Coordination where POS(m) returns all possible parts of speech of morpheme m in Hownet and Etype(s) is to obtain the event type of seed trigger s; WSim(m,s) is defined in Subsection 3.3 and returns 1 when one word m is the synonym of the other word s; Morph(w) is to obtain the morphological structure of word w following Li and Zhou (2012). Since there is a strong trigger consistency in those two-morpheme words of Coordination structure which refers to two distinct events, we propose an event inference mechanism as follows. Compositional semantics: For each two-morpheme word identified by the above three conditions, if one of its morphemes has been extracted as an trigger mention of a specific event type, the other morpheme in the same word will refer to an a relevant event type. 4.3 Event Inference on Coreference Events To mine more event mentions, we use the simple trigger-entity pair to represent event pattern in this paper."
C14-1204,P13-1145,1,0.884039,"Missing"
C14-1204,C10-1077,0,0.581209,"ct the event mentions with infrequent patterns. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 2161 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2161–2171, Dublin, Ireland, August 23-29 2014. In this paper, we first implement a pattern-based semi-supervised model for Chinese event extraction as a baseline, following the state-of-the-art system as described in (Liao and Grishman, 2010a) and then refine this model to suit Chinese event extraction. Moreover, we propose various kinds of novel linguistic knowledge-driven event inference mechanisms to address the above issue and recover missing event mentions. These event inference mechanisms can capture the linguistic knowledge from semantics of argument role, compositional semantics of trigger, consistency on coreference events and relevant events. Evaluation on the ACE 2005 Chinese corpus shows that our event inference mechanisms dramatically outperform the baseline. The rest of this paper is organized as follows. Section 2"
C14-1204,P10-1081,0,0.362818,"ct the event mentions with infrequent patterns. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 2161 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2161–2171, Dublin, Ireland, August 23-29 2014. In this paper, we first implement a pattern-based semi-supervised model for Chinese event extraction as a baseline, following the state-of-the-art system as described in (Liao and Grishman, 2010a) and then refine this model to suit Chinese event extraction. Moreover, we propose various kinds of novel linguistic knowledge-driven event inference mechanisms to address the above issue and recover missing event mentions. These event inference mechanisms can capture the linguistic knowledge from semantics of argument role, compositional semantics of trigger, consistency on coreference events and relevant events. Evaluation on the ACE 2005 Chinese corpus shows that our event inference mechanisms dramatically outperform the baseline. The rest of this paper is organized as follows. Section 2"
C14-1204,P11-2045,0,0.498629,"Missing"
C14-1204,O02-2003,0,0.0610631,") = Max( WSim( t p ,t s ) ×ESim( e p ,es ) ×DSim( d p , d s ) s∈P (3) where t, e and d represent the trigger, entity type and dependency path in candidate pattern p(tp, ep, dp) or seed pattern s(ts, es, ds) in the set of extracted patterns P, respectively; ESim identifies whether two entities have the same type, and assigned 1 if two entities have the same entity type and otherwise a small number 0.1; DSim calculates the similarity between two dependency paths in edit distance. Finally, WSim is to obtain the trigger similarity in lexical semantics, using Hownet (Dong and Dong, 2006) following Liu and Li (2002): WSim (t p , t s )   Dis(t p , t s )   (4) where Dis(tp,ts) is the distance between the sememes of triggers tp and ts, in HowNet’s sememe hierarchical architecture, with parameter ϕ assigned 0.75 following Liu and Li (2002). 4 Event Inference The pattern-based semi-supervised model cannot extract those event mentions matching infrequent patterns or without matching patterns. The knowledge from linguistic aspect (e.g., definition of events, compositional semantics of Chinese words, coreference events and relevant events, etc.) is helpful to further recover missing event mentions or filter"
C14-1204,E12-1030,0,0.0300677,"Missing"
C14-1204,P07-1075,0,0.0352049,"Missing"
C14-1204,D09-1016,0,0.0441251,"Missing"
C14-1204,P05-1047,0,0.0347413,"Automatically extracting events from free texts is a higher-level Information Extraction (IE) task, which is still a challenge due to the complexity of natural language and the domain-specific nature, especially in Chinese for its specific characteristics. In particular, most of previous studies have focused on English event extraction, while only a few concern Chinese. Currently, supervised learning models have dominated event extraction. To reduce the labeled data required, a few semi-supervised models have been applied to English event extraction (e.g., Riloff 1996; Yangarber et al., 2000; Stevenson and Greenwood, 2005; Huang and Riloff, 2012). Since classifier-based model needs dozens of annotated documents to train model, most of previous semisupervised models focused on pattern-based approach, which only needed a few seed (event) patterns. In those pattern-based approaches, frequent event patterns, which occur in many documents, were chosen as relevant patterns to match event mentions in unlabeled texts. However, the order of words in a Chinese sentence is rather agile for its open and flexible structure, and different orders might express the same meaning due to the semantics-driven nature of the Chines"
C14-1204,H01-1009,0,0.0679108,"Missing"
C14-1204,P03-1029,0,0.113623,"Missing"
C14-1204,P03-1044,0,0.117077,"Missing"
C14-1204,C00-2136,0,\N,Missing
C14-1204,P11-1113,1,\N,Missing
C14-1204,P03-1028,0,\N,Missing
C16-1137,W06-1623,0,0.138231,"action. In comparison, few studies concern temporal relation extraction from Chinese text. Chen et al. (2008) used verbal attributes to identify temporal relations of verbs. Li et al. (2004) presented a classifier-based collaborative bootstrapping approach to analyze temporal relations in a small Chinese corpus. While above studies focus on local information, a few studies sort to global inference, with focus on exploiting global information via various kinds of temporal logic reflexivity and transitivity constraints, using frameworks like Integer Linear Programming and Markov Logic Networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). However, their gains are rather small, largely due to the common disconnectedness in the sparsely annotated corpora (Chambers et al., 2014). To overcome this problem, Denis and Muller (2011) decomposed temporal entities into sub-graphs and enforced the coherence only within these substructures, while Do et al. (2012) proposed a joint eventevent and event-time classification model to enforce various coreference constraints. Different from previous studies, we build an event-driven fully-annotated Chinese corpus and propose 1452 a discourse"
C16-1137,P14-2082,0,0.428167,"lems with the TimeBank corpus are that it only annotates a small subset of easily-identified event mention pairs and that it largely ignores those temporal relations between event mentions in nonadjacent sentences. These lead to fragmented relations and much limit its applications. To overcome above problems, Do et al. (2012) produced an event-driven corpus on the ACE 2005 English corpus. However, “the annotator was not required to annotate all pairs of event mentions, but as many as possible”, as stated in their paper. This makes the annotation inconsistent and difficult to follow. Recently, Cassidy et al. (2014) enriched the TimeBank-Dense corpus, on the top of TimeBank. Specifically, they approximated the completeness by labeling locally complete graphs over neighboring sentences. In comparison, there are few corpora for Chinese temporal relation extraction. Li et al. (2004) annotated a Chinese corpus including 700 sentences. The TempEval-2 competition (Verhagen et al., 2010) provided 780 instances of Chinese temporal event relations. Obviously, both corpora are rather small and largely impede the research in Chinese temporal relation extraction. For example, no team participated in the TempEval-2 c"
C16-1137,P07-2044,0,0.156417,"on. For example, no team participated in the TempEval-2 competition on Chinese temporal relation extraction. 2.2 Inference Mechanism Due to the corpus limitation, previous studies on temporal relation extraction focus on inferring temporal relations between event mentions in the same sentence or neighboring sentences from English text, dominated by feature-based approaches. Mani et al. (2006) applied the temporal transitivity rule to greatly expand the corpus. Lapata and Lascarides (2006) introduced various kinds of syntactic and clause-ordering features to classify the temporal relationship. Chambers et al. (2007) used previously learned event attributes to classify the temporal relationship. Laokulrat et al. (2013), the best performing one in the TempEval-3 competition, applied various predicate-argument structure features from a deep syntactic parser to enhance their classifier. Mirza and Tonelli (2014) illustrated that simple features resulted in a better performance than sophisticated features. Chambers et al. (2014) proposed a sievebased architecture to joint those different tasks of temporal relation extraction. In comparison, few studies concern temporal relation extraction from Chinese text. Ch"
C16-1137,D08-1073,0,0.119331,"few studies concern temporal relation extraction from Chinese text. Chen et al. (2008) used verbal attributes to identify temporal relations of verbs. Li et al. (2004) presented a classifier-based collaborative bootstrapping approach to analyze temporal relations in a small Chinese corpus. While above studies focus on local information, a few studies sort to global inference, with focus on exploiting global information via various kinds of temporal logic reflexivity and transitivity constraints, using frameworks like Integer Linear Programming and Markov Logic Networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). However, their gains are rather small, largely due to the common disconnectedness in the sparsely annotated corpora (Chambers et al., 2014). To overcome this problem, Denis and Muller (2011) decomposed temporal entities into sub-graphs and enforced the coherence only within these substructures, while Do et al. (2012) proposed a joint eventevent and event-time classification model to enforce various coreference constraints. Different from previous studies, we build an event-driven fully-annotated Chinese corpus and propose 1452 a discourse-level global inference model"
C16-1137,Q14-1022,0,0.0143842,"l transitivity rule to greatly expand the corpus. Lapata and Lascarides (2006) introduced various kinds of syntactic and clause-ordering features to classify the temporal relationship. Chambers et al. (2007) used previously learned event attributes to classify the temporal relationship. Laokulrat et al. (2013), the best performing one in the TempEval-3 competition, applied various predicate-argument structure features from a deep syntactic parser to enhance their classifier. Mirza and Tonelli (2014) illustrated that simple features resulted in a better performance than sophisticated features. Chambers et al. (2014) proposed a sievebased architecture to joint those different tasks of temporal relation extraction. In comparison, few studies concern temporal relation extraction from Chinese text. Chen et al. (2008) used verbal attributes to identify temporal relations of verbs. Li et al. (2004) presented a classifier-based collaborative bootstrapping approach to analyze temporal relations in a small Chinese corpus. While above studies focus on local information, a few studies sort to global inference, with focus on exploiting global information via various kinds of temporal logic reflexivity and transitivi"
C16-1137,I08-4005,0,0.042478,"Missing"
C16-1137,N13-1112,0,0.0404303,"Missing"
C16-1137,D12-1062,0,0.353459,"al relation extraction, the TimeBank corpus (Pustejovsky et al., 2003) has been adopted in a series of TempEval competitions (Verhagen et al., 2007; Verhagen et al., 2010; Uz-Zaman et al., 2013), facilitating the development and evaluation of temporal relation extraction systems. The problems with the TimeBank corpus are that it only annotates a small subset of easily-identified event mention pairs and that it largely ignores those temporal relations between event mentions in nonadjacent sentences. These lead to fragmented relations and much limit its applications. To overcome above problems, Do et al. (2012) produced an event-driven corpus on the ACE 2005 English corpus. However, “the annotator was not required to annotate all pairs of event mentions, but as many as possible”, as stated in their paper. This makes the annotation inconsistent and difficult to follow. Recently, Cassidy et al. (2014) enriched the TimeBank-Dense corpus, on the top of TimeBank. Specifically, they approximated the completeness by labeling locally complete graphs over neighboring sentences. In comparison, there are few corpora for Chinese temporal relation extraction. Li et al. (2004) annotated a Chinese corpus including"
C16-1137,S13-2015,0,0.0180355,"tion. 2.2 Inference Mechanism Due to the corpus limitation, previous studies on temporal relation extraction focus on inferring temporal relations between event mentions in the same sentence or neighboring sentences from English text, dominated by feature-based approaches. Mani et al. (2006) applied the temporal transitivity rule to greatly expand the corpus. Lapata and Lascarides (2006) introduced various kinds of syntactic and clause-ordering features to classify the temporal relationship. Chambers et al. (2007) used previously learned event attributes to classify the temporal relationship. Laokulrat et al. (2013), the best performing one in the TempEval-3 competition, applied various predicate-argument structure features from a deep syntactic parser to enhance their classifier. Mirza and Tonelli (2014) illustrated that simple features resulted in a better performance than sophisticated features. Chambers et al. (2014) proposed a sievebased architecture to joint those different tasks of temporal relation extraction. In comparison, few studies concern temporal relation extraction from Chinese text. Chen et al. (2008) used verbal attributes to identify temporal relations of verbs. Li et al. (2004) presen"
C16-1137,P06-1095,0,0.318732,"emporal event relations. Obviously, both corpora are rather small and largely impede the research in Chinese temporal relation extraction. For example, no team participated in the TempEval-2 competition on Chinese temporal relation extraction. 2.2 Inference Mechanism Due to the corpus limitation, previous studies on temporal relation extraction focus on inferring temporal relations between event mentions in the same sentence or neighboring sentences from English text, dominated by feature-based approaches. Mani et al. (2006) applied the temporal transitivity rule to greatly expand the corpus. Lapata and Lascarides (2006) introduced various kinds of syntactic and clause-ordering features to classify the temporal relationship. Chambers et al. (2007) used previously learned event attributes to classify the temporal relationship. Laokulrat et al. (2013), the best performing one in the TempEval-3 competition, applied various predicate-argument structure features from a deep syntactic parser to enhance their classifier. Mirza and Tonelli (2014) illustrated that simple features resulted in a better performance than sophisticated features. Chambers et al. (2014) proposed a sievebased architecture to joint those diffe"
C16-1137,P04-1074,0,0.241398,"cations. To overcome above problems, Do et al. (2012) produced an event-driven corpus on the ACE 2005 English corpus. However, “the annotator was not required to annotate all pairs of event mentions, but as many as possible”, as stated in their paper. This makes the annotation inconsistent and difficult to follow. Recently, Cassidy et al. (2014) enriched the TimeBank-Dense corpus, on the top of TimeBank. Specifically, they approximated the completeness by labeling locally complete graphs over neighboring sentences. In comparison, there are few corpora for Chinese temporal relation extraction. Li et al. (2004) annotated a Chinese corpus including 700 sentences. The TempEval-2 competition (Verhagen et al., 2010) provided 780 instances of Chinese temporal event relations. Obviously, both corpora are rather small and largely impede the research in Chinese temporal relation extraction. For example, no team participated in the TempEval-2 competition on Chinese temporal relation extraction. 2.2 Inference Mechanism Due to the corpus limitation, previous studies on temporal relation extraction focus on inferring temporal relations between event mentions in the same sentence or neighboring sentences from En"
C16-1137,E14-1033,0,0.0201211,"r neighboring sentences from English text, dominated by feature-based approaches. Mani et al. (2006) applied the temporal transitivity rule to greatly expand the corpus. Lapata and Lascarides (2006) introduced various kinds of syntactic and clause-ordering features to classify the temporal relationship. Chambers et al. (2007) used previously learned event attributes to classify the temporal relationship. Laokulrat et al. (2013), the best performing one in the TempEval-3 competition, applied various predicate-argument structure features from a deep syntactic parser to enhance their classifier. Mirza and Tonelli (2014) illustrated that simple features resulted in a better performance than sophisticated features. Chambers et al. (2014) proposed a sievebased architecture to joint those different tasks of temporal relation extraction. In comparison, few studies concern temporal relation extraction from Chinese text. Chen et al. (2008) used verbal attributes to identify temporal relations of verbs. Li et al. (2004) presented a classifier-based collaborative bootstrapping approach to analyze temporal relations in a small Chinese corpus. While above studies focus on local information, a few studies sort to global"
C16-1137,W04-2401,0,0.07367,"ne the interaction among events in a document, we optimize the predicted temporal graph, formed by prediction from CE-E, with various kinds of discourse-level constraints derived from event semantics. Let E={e1,e2,…,en} denote the set of event mentions in a document, ε={(ei,ej)∈E×E|ei,ej∈E, i≠j} the set of event mention pairs, and R={ B, A, O,U } the set of temporal relations. Besides, let P<i,j,r> denote the prediction probability of (ei,ej) with relation r (r∈R), given by the event-event classifier CE-E, and x<i,j,r> the binary indicator on the existence of relation r for (ei,ej). Following Roth and Yih (2004) and Li et al. (2013) in information extraction, we define the following log costs: ci , j ,r    log( Pi , j ,r  ) (2) ci , j ,r   log(1  Pi , j ,r ) (3) Specifically, ILP (Integer Logical Programming), a global inference is employed to achieve global optimization with the following objective function to maximize over a document as follows: arg min x   (c ( ei ,e j ) rR i , j ,r   xi , j ,r   (1  xi , j ,r  )  c i , j ,r  ) (4) s.t. xi , j ,r  {0,1} (5) x (6) rR i , j ,r  1 while binary constraint (5) ensures that x<i,j,r> is binary value and equality cons"
C16-1137,S13-2001,0,0.164888,"Missing"
C16-1137,S07-1014,0,0.358501,"Missing"
C16-1137,P09-1046,0,0.0235071,"relation extraction from Chinese text. Chen et al. (2008) used verbal attributes to identify temporal relations of verbs. Li et al. (2004) presented a classifier-based collaborative bootstrapping approach to analyze temporal relations in a small Chinese corpus. While above studies focus on local information, a few studies sort to global inference, with focus on exploiting global information via various kinds of temporal logic reflexivity and transitivity constraints, using frameworks like Integer Linear Programming and Markov Logic Networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). However, their gains are rather small, largely due to the common disconnectedness in the sparsely annotated corpora (Chambers et al., 2014). To overcome this problem, Denis and Muller (2011) decomposed temporal entities into sub-graphs and enforced the coherence only within these substructures, while Do et al. (2012) proposed a joint eventevent and event-time classification model to enforce various coreference constraints. Different from previous studies, we build an event-driven fully-annotated Chinese corpus and propose 1452 a discourse-level global inference model to extract temporal rela"
C16-1153,P13-2037,0,0.0144394,"or-based document representation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open domains. Most of the previous studies focused on monolingual text, while our proposed bilingual attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from monolingual and bilingual contexts with attention mechanisms. 2.2 Research on Code-switching and Bilingual Text Code-switched documents have received considerable attention in the NLP community (Adel et al., 2013; Garrette et al., 2015). Several studies have focused on code-switching identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting code-switched points (Solorio and Liu, 2008), identifying code-switched tokens (Lignos and Marcus, 2013), adding code-switched support to language models (Li and Fung, 2012), and POS tagging for code-switching text (Jamatia et al., 2015). There is relatively little work focus on predicting emotion in code-switching text. Wang et al. (2015) proposed a machine translation based approach to predict emotion in"
C16-1153,D08-1092,0,0.024572,"g monolingual and bilingual information in both lexical and document level with neural network model and attention mechanism, while previous research only focused on lexicallevel bilingual information. In addition, we do not use any external resource, such as bilingual and sentiment dictionary, to train our model. More remotely connected, multilingual natural language processing has attracted increasing attention in the computational linguistic community due to its broad real-world applications. Relevant studies have been reported in various natural language processing tasks, such as parsing (Burkett and Klein, 2008), information retrieval (Gao et al., 2009), text classification (Amini et al., 2010), and so on. There are a number of studies on predicting sentiment polarity through multilingual text. Wan (2009) incorporated unlabeled data in the target language into classifier with co-training to improve classification performance. Wei and Pal (2010) regarded cross-lingual sentiment classification as a domain adaptation task and applied structural correspondence learning (SCL) to tackle this problem. Their approach achieves a better performance than the co-training algorithm. More recently, Meng et al. (20"
C16-1153,C14-1008,0,0.00907215,"-specific lexicon. For emotion classification, Liu et al. (2013) used a co-training framework to infer the news from readers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and emotion analysis (Vo and Zhang, 2015; Zhang et al., 2015). dos Santos and Gatti (2014) proposed a character-based deep convolutional neural network to predict sentiment of short text. Tang et al. (2015) proposed a neural network model to learn vector-based document representation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open domains. Most of the previous studies focused on monolingual text, while our proposed bilingual attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from monolingual and bilingual context"
C16-1153,P09-1121,0,0.0223953,"exical and document level with neural network model and attention mechanism, while previous research only focused on lexicallevel bilingual information. In addition, we do not use any external resource, such as bilingual and sentiment dictionary, to train our model. More remotely connected, multilingual natural language processing has attracted increasing attention in the computational linguistic community due to its broad real-world applications. Relevant studies have been reported in various natural language processing tasks, such as parsing (Burkett and Klein, 2008), information retrieval (Gao et al., 2009), text classification (Amini et al., 2010), and so on. There are a number of studies on predicting sentiment polarity through multilingual text. Wan (2009) incorporated unlabeled data in the target language into classifier with co-training to improve classification performance. Wei and Pal (2010) regarded cross-lingual sentiment classification as a domain adaptation task and applied structural correspondence learning (SCL) to tackle this problem. Their approach achieves a better performance than the co-training algorithm. More recently, Meng et al. (2012) employed the parallel corpus for cross"
C16-1153,N15-1109,0,0.0140355,"epresentation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open domains. Most of the previous studies focused on monolingual text, while our proposed bilingual attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from monolingual and bilingual contexts with attention mechanisms. 2.2 Research on Code-switching and Bilingual Text Code-switched documents have received considerable attention in the NLP community (Adel et al., 2013; Garrette et al., 2015). Several studies have focused on code-switching identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting code-switched points (Solorio and Liu, 2008), identifying code-switched tokens (Lignos and Marcus, 2013), adding code-switched support to language models (Li and Fung, 2012), and POS tagging for code-switching text (Jamatia et al., 2015). There is relatively little work focus on predicting emotion in code-switching text. Wang et al. (2015) proposed a machine translation based approach to predict emotion in codeswitching text with"
C16-1153,R15-1033,0,0.0212779,"Missing"
C16-1153,lee-etal-2014-annotating,1,0.895524,"Missing"
C16-1153,C12-1102,0,0.00831582,"tecture to capture informative words from monolingual and bilingual contexts with attention mechanisms. 2.2 Research on Code-switching and Bilingual Text Code-switched documents have received considerable attention in the NLP community (Adel et al., 2013; Garrette et al., 2015). Several studies have focused on code-switching identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting code-switched points (Solorio and Liu, 2008), identifying code-switched tokens (Lignos and Marcus, 2013), adding code-switched support to language models (Li and Fung, 2012), and POS tagging for code-switching text (Jamatia et al., 2015). There is relatively little work focus on predicting emotion in code-switching text. Wang et al. (2015) proposed a machine translation based approach to predict emotion in codeswitching text with various external resources. Our approach departs from the previous work that we model the task by considering monolingual and bilingual information in both lexical and document level with neural network model and attention mechanism, while previous research only focused on lexicallevel bilingual information. In addition, we do not use an"
C16-1153,P15-1101,1,0.208181,"focused on analyzing emotions in monolingual text. Some of these studies emotion lexicon building, for example, Rao et al. (2012) automatically built a word-emotion mapping dictionary for social emotion detection, Yang et al. (2014) proposed a novel emotion-aware topic model to build a domain-specific lexicon. For emotion classification, Liu et al. (2013) used a co-training framework to infer the news from readers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and emotion analysis (Vo and Zhang, 2015; Zhang et al., 2015). dos Santos and Gatti (2014) proposed a character-based deep convolutional neural network to predict sentiment of short text. Tang et al. (2015) proposed a neural network model to learn vector-based document representation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open dom"
C16-1153,P13-1018,0,0.0560854,"proposed model. Visualization of the attention layers illustrates that the model selects informative words qualitatively. 1 Introduction Microblogs such as Twitter and Facebook have gained tremendous popularity in the past decade, they often contain extremely current, even breaking, information about world events. However, the writing style of microblogs tends to be quite colloquial and nonstandard, unlike the style found in more traditional, edited genres (Li et al., 2015; Vo and Zhang, 2015). In addition, authors from multi-lingual communities tend to write code-switching posts frequently (Ling et al., 2013; Wang et al., 2015). These pose challenges for automatic emotion prediction tasks. There has been some previous research focusing on both emotion analysis (Pang et al., 2002; Lee et al., 2014) and code-switching text analysis (Solorio and Liu, 2008; Ling et al., 2013; Jamatia et al., 2015). However, little research has focused on predicting emotion in code-switching text. Different from monolingual emotion prediction, the emotion in code-switching posts can be expressed in either monolingual or bilingual forms. In this study, we focus on Chinese and English mixed code-switching text from Chin"
C16-1153,P13-2091,1,0.280309,"ows the effectiveness of our proposed BAN model with both monolingual and bilingual information. 2 2.1 Related Works Emotion Analysis Over the last decade, there has been much work exploring various aspects of emotion analysis (Wiebe et al., 2005). While most focused on analyzing emotions in monolingual text. Some of these studies emotion lexicon building, for example, Rao et al. (2012) automatically built a word-emotion mapping dictionary for social emotion detection, Yang et al. (2014) proposed a novel emotion-aware topic model to build a domain-specific lexicon. For emotion classification, Liu et al. (2013) used a co-training framework to infer the news from readers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and emotion analysis (Vo and Zhang, 2015; Zhang et al., 2015). dos Santos and Gatti (2014) proposed a character-based deep convolut"
C16-1153,P12-1060,0,0.0429676,"nd Klein, 2008), information retrieval (Gao et al., 2009), text classification (Amini et al., 2010), and so on. There are a number of studies on predicting sentiment polarity through multilingual text. Wan (2009) incorporated unlabeled data in the target language into classifier with co-training to improve classification performance. Wei and Pal (2010) regarded cross-lingual sentiment classification as a domain adaptation task and applied structural correspondence learning (SCL) to tackle this problem. Their approach achieves a better performance than the co-training algorithm. More recently, Meng et al. (2012) employed the parallel corpus for cross-lingual sentiment classification. They explored the case when no labeled data is available in the parallel corpus. However, such multi-lingual models do not explicitly consider code-switching, since their data sets are always parallel corpus. As the two languages are mixed in the code-switching text without parallel, code-switching corpus is more difficult to process. 3 Bilingual Attention Network Given a post X with T words (X =< w1 , w2 , ..., wT >), where each word wt is represented with a Kdimensional embedding (Mikolov et al., 2013), our goal is to"
C16-1153,W02-1011,0,0.0389416,"Missing"
C16-1153,D08-1102,0,0.0135616,"attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from monolingual and bilingual contexts with attention mechanisms. 2.2 Research on Code-switching and Bilingual Text Code-switched documents have received considerable attention in the NLP community (Adel et al., 2013; Garrette et al., 2015). Several studies have focused on code-switching identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting code-switched points (Solorio and Liu, 2008), identifying code-switched tokens (Lignos and Marcus, 2013), adding code-switched support to language models (Li and Fung, 2012), and POS tagging for code-switching text (Jamatia et al., 2015). There is relatively little work focus on predicting emotion in code-switching text. Wang et al. (2015) proposed a machine translation based approach to predict emotion in codeswitching text with various external resources. Our approach departs from the previous work that we model the task by considering monolingual and bilingual information in both lexical and document level with neural network model a"
C16-1153,D15-1167,0,0.00344898,"ers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and emotion analysis (Vo and Zhang, 2015; Zhang et al., 2015). dos Santos and Gatti (2014) proposed a character-based deep convolutional neural network to predict sentiment of short text. Tang et al. (2015) proposed a neural network model to learn vector-based document representation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open domains. Most of the previous studies focused on monolingual text, while our proposed bilingual attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from monolingual and bilingual contexts with attention mechanisms. 2.2 Research on Code-switching and Bilingual Text Code-switched documents have received"
C16-1153,P09-1027,0,0.0105089,"on, we do not use any external resource, such as bilingual and sentiment dictionary, to train our model. More remotely connected, multilingual natural language processing has attracted increasing attention in the computational linguistic community due to its broad real-world applications. Relevant studies have been reported in various natural language processing tasks, such as parsing (Burkett and Klein, 2008), information retrieval (Gao et al., 2009), text classification (Amini et al., 2010), and so on. There are a number of studies on predicting sentiment polarity through multilingual text. Wan (2009) incorporated unlabeled data in the target language into classifier with co-training to improve classification performance. Wei and Pal (2010) regarded cross-lingual sentiment classification as a domain adaptation task and applied structural correspondence learning (SCL) to tackle this problem. Their approach achieves a better performance than the co-training algorithm. More recently, Meng et al. (2012) employed the parallel corpus for cross-lingual sentiment classification. They explored the case when no labeled data is available in the parallel corpus. However, such multi-lingual models do n"
C16-1153,P15-2125,1,0.914845,"documents have received considerable attention in the NLP community (Adel et al., 2013; Garrette et al., 2015). Several studies have focused on code-switching identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting code-switched points (Solorio and Liu, 2008), identifying code-switched tokens (Lignos and Marcus, 2013), adding code-switched support to language models (Li and Fung, 2012), and POS tagging for code-switching text (Jamatia et al., 2015). There is relatively little work focus on predicting emotion in code-switching text. Wang et al. (2015) proposed a machine translation based approach to predict emotion in codeswitching text with various external resources. Our approach departs from the previous work that we model the task by considering monolingual and bilingual information in both lexical and document level with neural network model and attention mechanism, while previous research only focused on lexicallevel bilingual information. In addition, we do not use any external resource, such as bilingual and sentiment dictionary, to train our model. More remotely connected, multilingual natural language processing has attracted inc"
C16-1153,P10-2048,0,0.013418,"ilingual natural language processing has attracted increasing attention in the computational linguistic community due to its broad real-world applications. Relevant studies have been reported in various natural language processing tasks, such as parsing (Burkett and Klein, 2008), information retrieval (Gao et al., 2009), text classification (Amini et al., 2010), and so on. There are a number of studies on predicting sentiment polarity through multilingual text. Wan (2009) incorporated unlabeled data in the target language into classifier with co-training to improve classification performance. Wei and Pal (2010) regarded cross-lingual sentiment classification as a domain adaptation task and applied structural correspondence learning (SCL) to tackle this problem. Their approach achieves a better performance than the co-training algorithm. More recently, Meng et al. (2012) employed the parallel corpus for cross-lingual sentiment classification. They explored the case when no labeled data is available in the parallel corpus. However, such multi-lingual models do not explicitly consider code-switching, since their data sets are always parallel corpus. As the two languages are mixed in the code-switching"
C16-1153,P14-2069,0,0.0236212,"hen a sequence of tokens is relevant rather than simply filtering for sequences of tokens, taken out of context. Evaluation shows the effectiveness of our proposed BAN model with both monolingual and bilingual information. 2 2.1 Related Works Emotion Analysis Over the last decade, there has been much work exploring various aspects of emotion analysis (Wiebe et al., 2005). While most focused on analyzing emotions in monolingual text. Some of these studies emotion lexicon building, for example, Rao et al. (2012) automatically built a word-emotion mapping dictionary for social emotion detection, Yang et al. (2014) proposed a novel emotion-aware topic model to build a domain-specific lexicon. For emotion classification, Liu et al. (2013) used a co-training framework to infer the news from readers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and em"
C16-1153,N16-1174,0,0.0377432,"iations to the LSTM model, and we choose one for which the hidden state ht for each time-step t is given by: it = σ(W (i) xt + U (i) hi−1 + b(i) ) (1) ft = σ(W (f ) xt + U (f ) ht−1 + b(f ) ) (2) ot = σ(W (o) xt + U (o) ht−1 + b(o) ) (3) ut = tanh(W (u) + U (u) ht−1 + b(u) ) (4) ct = it ut + ft ct−1 (5) ht = ot tanh(ct ) (6) where σ denotes the sigmoid function. After the LSTM process, we obtain an annotation ht for a given word wt . 3.2 Attention Mechanism Not all words contribute equally to the representation of the meaning. Hence, we introduce an attention mechanism (Bahdanau et al., 2014; Yang et al., 2016) to extract the words that are important to the meaning of the post, and aggregate the representation of those informative words to form a vector. Since emotion can be expressed in either one or two languages in code-switching text, we build the vectors from monolingual and bilingual contexts respectively. For the monolingual case, we build two vectors v (cn) and v (en) to capture informative information from the Chinese and English contexts separately. For the bilingual case, we construct a vector v (bi) to capture the salient words from the mixed text. Bilingual Attention. We use an attentio"
C16-1153,D15-1073,1,0.85283,"c model to build a domain-specific lexicon. For emotion classification, Liu et al. (2013) used a co-training framework to infer the news from readers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and emotion analysis (Vo and Zhang, 2015; Zhang et al., 2015). dos Santos and Gatti (2014) proposed a character-based deep convolutional neural network to predict sentiment of short text. Tang et al. (2015) proposed a neural network model to learn vector-based document representation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open domains. Most of the previous studies focused on monolingual text, while our proposed bilingual attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from mono"
C16-1197,D11-1120,0,0.0891532,"Missing"
C16-1197,D13-1114,0,0.456469,"n is a fundamental task with regard to infer user’s gender from the user-generated data. Recently, this task is getting increasingly more attention in some prevailing research fields, such as social network analysis and natural language processing. Applications developed from gender classification have enormous commercial value in personalization, marketing and judicial investigation (Mukherjee and Liu, 2010; Burger et al., 2001; Volkova et al., 2013). In social media, conventional methods handle gender classification as a supervised learning problem over the past decade (Corney et al., 2002; Ciot et al., 2013). In supervised learning approaches, both user-generated textual and user social link features are verified to be effective for gender classification. For instance, in Figure 1, it is easy to infer User c to be a female through analyzing her saying “I&apos;m gonna be a mom!! ” Meanwhile, it is also possible to infer User c is more likely to be a female through analyzing her social link since she follows a cosmetic-selling User “Dior”. Although supervised methods have achieved remarkable success for gender classification, their good performances always depend on a large amount of labeled data, which"
C16-1197,D12-1135,0,0.0191123,"ction 3 introduces data collection and analysis. Section 4 describes our TSFG approach to gender classification. Section 5 presents the experimental results. Finally, Section 6 gives the conclusion and future work. 2 Related Work In the last decade, gender classification has been studied in two main aspects: supervised learning and semi-supervised learning. As for supervised learning, gender classification has been extensively studied in several textual styles, such as Blog (Nowson and Oberlander, 2006; Peersman et al., 2011; Gianfortoni et al., 2011), E-mail (Mohanmad et al., 2011), YouTube (Filippova, 2012) and Micro-blog (Rao et al., 2010; Liu et al., 2013). These studies mainly focus on employing various kinds of textual features such as character, word, POS features and their n-gram features to train the classifier. More recently, some studies focus on some specific application scenarios on supervised gender classification, such as multi-lingual gender classification (Ciot et al., 2013; Alowibdi et al., 2013) and interactive gender classification (Li et al., 2015). As for semi-supervised learning, gender classification has been studied with much less previous studies. Ikeda et al. (2008) prop"
C16-1197,W11-2606,0,0.378867,"llows. Section 2 overviews related work on gender classification. Section 3 introduces data collection and analysis. Section 4 describes our TSFG approach to gender classification. Section 5 presents the experimental results. Finally, Section 6 gives the conclusion and future work. 2 Related Work In the last decade, gender classification has been studied in two main aspects: supervised learning and semi-supervised learning. As for supervised learning, gender classification has been extensively studied in several textual styles, such as Blog (Nowson and Oberlander, 2006; Peersman et al., 2011; Gianfortoni et al., 2011), E-mail (Mohanmad et al., 2011), YouTube (Filippova, 2012) and Micro-blog (Rao et al., 2010; Liu et al., 2013). These studies mainly focus on employing various kinds of textual features such as character, word, POS features and their n-gram features to train the classifier. More recently, some studies focus on some specific application scenarios on supervised gender classification, such as multi-lingual gender classification (Ciot et al., 2013; Alowibdi et al., 2013) and interactive gender classification (Li et al., 2015). As for semi-supervised learning, gender classification has been studie"
C16-1197,D10-1021,0,0.379759,"el both the textual and the “same-interest” link information. Empirical studies demonstrate the effectiveness of the proposed approach to semi-supervised gender classification. 1 Introduction Gender classification is a fundamental task with regard to infer user’s gender from the user-generated data. Recently, this task is getting increasingly more attention in some prevailing research fields, such as social network analysis and natural language processing. Applications developed from gender classification have enormous commercial value in personalization, marketing and judicial investigation (Mukherjee and Liu, 2010; Burger et al., 2001; Volkova et al., 2013). In social media, conventional methods handle gender classification as a supervised learning problem over the past decade (Corney et al., 2002; Ciot et al., 2013). In supervised learning approaches, both user-generated textual and user social link features are verified to be effective for gender classification. For instance, in Figure 1, it is easy to infer User c to be a female through analyzing her saying “I&apos;m gonna be a mom!! ” Meanwhile, it is also possible to infer User c is more likely to be a female through analyzing her social link since she"
C16-1197,W11-1709,0,0.0283685,"Missing"
C16-1197,P15-1169,0,0.0878251,"Missing"
C16-1197,P11-1077,0,0.0453406,"Missing"
C16-1197,D13-1187,0,0.0660849,"nk information. Empirical studies demonstrate the effectiveness of the proposed approach to semi-supervised gender classification. 1 Introduction Gender classification is a fundamental task with regard to infer user’s gender from the user-generated data. Recently, this task is getting increasingly more attention in some prevailing research fields, such as social network analysis and natural language processing. Applications developed from gender classification have enormous commercial value in personalization, marketing and judicial investigation (Mukherjee and Liu, 2010; Burger et al., 2001; Volkova et al., 2013). In social media, conventional methods handle gender classification as a supervised learning problem over the past decade (Corney et al., 2002; Ciot et al., 2013). In supervised learning approaches, both user-generated textual and user social link features are verified to be effective for gender classification. For instance, in Figure 1, it is easy to infer User c to be a female through analyzing her saying “I&apos;m gonna be a mom!! ” Meanwhile, it is also possible to infer User c is more likely to be a female through analyzing her social link since she follows a cosmetic-selling User “Dior”. Alt"
C16-1199,D15-1141,0,0.0338907,"es demonstrate that our approach performs much better than many strong baseline approaches. Note that the motivation of employing LSTM as our single-perspective learning approach is that LSTM equips with a special gating mechanism that controls access to memory cells and it is powerful and effective at capturing long-term dependencies (Bengio et al., 1994). This advantage is helpful for modeling text and thus this approach has been successfully applied to a variety of NLP tasks, such as machine translation (Bahdanau et al., 2015), sentiment analysis (Tang et al., 2015), and sequence labeling (Chen et al., 2015). The remainder of this paper is organized as follows. Section 2 overviews related work on user classification. Section 3 introduces data collection. Section 4 proposes our multi-perspective ensemble LSTM approach with multiple textual perspectives for user classification. Section 5 evaluates our approach with a benchmark dataset. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, many previous studies have been devoted to the research on user classification with multiple attributes, such as user gender and user age. User gender classification has bee"
C16-1199,D13-1114,0,0.0509071,"ection 6 gives the conclusion and future work. 2 Related Work Over the last decade, many previous studies have been devoted to the research on user classification with multiple attributes, such as user gender and user age. User gender classification has been extensively studied in several domains, such as Blog (Peersman et al., 2011; Gianfortoni et al., 2011), E-mail (Mohanmad et al., 2011), YouTube (Filippova, 2012) and Micro-blog (Liu et al., 2013). More recently, some studies focus on some specific application scenarios on gender classification, such as multi-lingual gender classification (Ciot et al., 2013; Alowibdi et al., 2013), inferring gender by crowd (Nguyen et al., 2014) and interactive gender classification (Li et al., 2015). User age classification has been studied in two main domains, i.e., blog (Burger and Hender son, 2006) and social media (Machinnon and Warren, 2006). In the blog domain, Schler et al. (2006) focus on textual features extracted from the blog text, such as word context features and POS stylistic features. Burger and Henderson (2006) explore some social features, such as location, time, and friend features, related to blogger age. Other studies, such as Rosenthal and"
C16-1199,D12-1135,0,0.228565,"n. Section 4 proposes our multi-perspective ensemble LSTM approach with multiple textual perspectives for user classification. Section 5 evaluates our approach with a benchmark dataset. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, many previous studies have been devoted to the research on user classification with multiple attributes, such as user gender and user age. User gender classification has been extensively studied in several domains, such as Blog (Peersman et al., 2011; Gianfortoni et al., 2011), E-mail (Mohanmad et al., 2011), YouTube (Filippova, 2012) and Micro-blog (Liu et al., 2013). More recently, some studies focus on some specific application scenarios on gender classification, such as multi-lingual gender classification (Ciot et al., 2013; Alowibdi et al., 2013), inferring gender by crowd (Nguyen et al., 2014) and interactive gender classification (Li et al., 2015). User age classification has been studied in two main domains, i.e., blog (Burger and Hender son, 2006) and social media (Machinnon and Warren, 2006). In the blog domain, Schler et al. (2006) focus on textual features extracted from the blog text, such as word context feat"
C16-1199,W11-2606,0,0.248406,"ted work on user classification. Section 3 introduces data collection. Section 4 proposes our multi-perspective ensemble LSTM approach with multiple textual perspectives for user classification. Section 5 evaluates our approach with a benchmark dataset. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, many previous studies have been devoted to the research on user classification with multiple attributes, such as user gender and user age. User gender classification has been extensively studied in several domains, such as Blog (Peersman et al., 2011; Gianfortoni et al., 2011), E-mail (Mohanmad et al., 2011), YouTube (Filippova, 2012) and Micro-blog (Liu et al., 2013). More recently, some studies focus on some specific application scenarios on gender classification, such as multi-lingual gender classification (Ciot et al., 2013; Alowibdi et al., 2013), inferring gender by crowd (Nguyen et al., 2014) and interactive gender classification (Li et al., 2015). User age classification has been studied in two main domains, i.e., blog (Burger and Hender son, 2006) and social media (Machinnon and Warren, 2006). In the blog domain, Schler et al. (2006) focus on textual featu"
C16-1199,P09-1078,1,0.803869,"see that our ensemble LSTM approach performs best and it is also performs better than other strong ensemble strategies with multiple textual perspectives, such as Voting LSTM and Weighted_Sum LSTM. Significance test shows that our ensemble LSTM approach significantly outperforms other approaches when multiple textual perspectives are used (p-value<0.05). 5.3 Effectiveness Analysis and Case Study In order to further illustrate the superiority of our approach, we give a case study as following. Table 8 shows the selected features sorted by the feature selection method of information gain (IG) (Li et al., 2009) when the task of gender classification is considered. We extract the features from the original message text and the retweeted message text separately. This table shows the top-10 IG features from the original message text and their ranks in the retweeted message text. N denotes the sequence number of the feature in the selected features. Ff denotes the feature frequency in all samples of female. Fm denotes the feature frequency in all samples of male. For instance, the sequence number of emoticon “rabbit” in original message is the first, the feature frequency in all samples of female is 587"
C16-1199,W11-1709,0,0.0673424,"Missing"
C16-1199,D10-1021,0,0.115059,"Missing"
C16-1199,C14-1184,0,0.203088,"Missing"
C16-1199,P11-1077,0,0.0182223,"t et al., 2013; Alowibdi et al., 2013), inferring gender by crowd (Nguyen et al., 2014) and interactive gender classification (Li et al., 2015). User age classification has been studied in two main domains, i.e., blog (Burger and Hender son, 2006) and social media (Machinnon and Warren, 2006). In the blog domain, Schler et al. (2006) focus on textual features extracted from the blog text, such as word context features and POS stylistic features. Burger and Henderson (2006) explore some social features, such as location, time, and friend features, related to blogger age. Other studies, such as Rosenthal and McKeown (2011) and Goswami et al. (2009) explore both the textual and social features in automatic age classification. In the social media domain, Mackinnon and Warren (2006) explore some kind of social features, i.e., the relationship between users to predict a user’s age and country of residence in a social network. Peersman et al. (2011) apply a text categorization approach to age classification with textual features only, i.e., word unigrams and bigrams. More recently, Marquardt et al. (2014) propose a multi-label classification approach to predict both the gender and age of authors from texts. Specific"
C16-1199,D14-1121,0,0.320716,"Missing"
C16-1199,D15-1167,0,0.0176011,"fuse all textual knowledge. Empirical studies demonstrate that our approach performs much better than many strong baseline approaches. Note that the motivation of employing LSTM as our single-perspective learning approach is that LSTM equips with a special gating mechanism that controls access to memory cells and it is powerful and effective at capturing long-term dependencies (Bengio et al., 1994). This advantage is helpful for modeling text and thus this approach has been successfully applied to a variety of NLP tasks, such as machine translation (Bahdanau et al., 2015), sentiment analysis (Tang et al., 2015), and sequence labeling (Chen et al., 2015). The remainder of this paper is organized as follows. Section 2 overviews related work on user classification. Section 3 introduces data collection. Section 4 proposes our multi-perspective ensemble LSTM approach with multiple textual perspectives for user classification. Section 5 evaluates our approach with a benchmark dataset. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, many previous studies have been devoted to the research on user classification with multiple attributes, such as user gender and u"
C16-1249,H05-1073,0,0.640009,"sponse text samples. The remainder of this paper is organized as follows. Section 2 overviews related work on emotion classification. Section 3 introduces the baseline approach to semi-supervised reader emotion classification with single-view label propagation. Section 4 presents our two-view label propagation approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-text data Ur Unlab"
C16-1249,C10-1021,1,0.714715,"ta Lr Labeled response-text data Us Unlabeled source-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among the source text data The transition probability matrix among the response text data The transition probability matrix among the source and response-text data Mr M s ,r Table 1: Symbol definition LP Figure 2: The framework of single-view label propagation approach to semi-supervised learning on reader emotion classification 2005; Chen et al., 2010; Purver and Battersby, 2012; Hasegawa et al., 2013; Qadir and Riloff, 2014), mainly from the supervised learning paradigm. Compared with above studies on writer emotion classification, studies on reader emotion classification are much limited. Lin et al. (2007) first describe the task of reader emotion classification on news articles with some standard machine learning approaches. Lin et al. (2008) further exploit more features to improve the performance. More recently, Liu et al. (2013) propose a co-training approach to semi-supervised learning on reader emotion classification by considering"
C16-1249,P09-2038,0,0.0160824,"to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among the source text data The transition probability matrix among the response text data The transition probability matrix among the source"
C16-1249,P13-1095,0,0.0255958,"ource-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among the source text data The transition probability matrix among the response text data The transition probability matrix among the source and response-text data Mr M s ,r Table 1: Symbol definition LP Figure 2: The framework of single-view label propagation approach to semi-supervised learning on reader emotion classification 2005; Chen et al., 2010; Purver and Battersby, 2012; Hasegawa et al., 2013; Qadir and Riloff, 2014), mainly from the supervised learning paradigm. Compared with above studies on writer emotion classification, studies on reader emotion classification are much limited. Lin et al. (2007) first describe the task of reader emotion classification on news articles with some standard machine learning approaches. Lin et al. (2008) further exploit more features to improve the performance. More recently, Liu et al. (2013) propose a co-training approach to semi-supervised learning on reader emotion classification by considering the message text and the comment text as two views"
C16-1249,P13-2091,1,0.759792,"classification due to its importance in more and more real-life applications, such as content recommendation and online advertisement. Conventional approaches to reader emotion classification conceptualize the task as a supervised learning problem and rely on a large-scale human-annotated data for model learning. Although such supervised approaches deliver reasonably good performance, the reliance on labeled data, which is normally difficult and highly expensive to obtain, presents a major obstacle to the widespread application of reader emotion classification. To alleviate the problem above, Liu et al. (2013) originally propose a semi-supervised learning approach to reader emotion classification to improve the performance by enlarging the labeled data with automatically inferred annotations of unlabeled instances. Their basic idea mainly lies on unique characteristics in reader emotion analysis, different from the case in writer emotion analysis. That is, apart from the source text (e.g., news text), another type of text, the response text (e.g., comment text) written by the reader as a response to the source text, is available to help determine the reader emotion of the source text. For example,"
C16-1249,W02-1011,0,0.0223043,"ability between the source and response text samples. The remainder of this paper is organized as follows. Section 2 overviews related work on emotion classification. Section 3 introduces the baseline approach to semi-supervised reader emotion classification with single-view label propagation. Section 4 presents our two-view label propagation approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unl"
C16-1249,E12-1049,0,0.0178318,"nse-text data Us Unlabeled source-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among the source text data The transition probability matrix among the response text data The transition probability matrix among the source and response-text data Mr M s ,r Table 1: Symbol definition LP Figure 2: The framework of single-view label propagation approach to semi-supervised learning on reader emotion classification 2005; Chen et al., 2010; Purver and Battersby, 2012; Hasegawa et al., 2013; Qadir and Riloff, 2014), mainly from the supervised learning paradigm. Compared with above studies on writer emotion classification, studies on reader emotion classification are much limited. Lin et al. (2007) first describe the task of reader emotion classification on news articles with some standard machine learning approaches. Lin et al. (2008) further exploit more features to improve the performance. More recently, Liu et al. (2013) propose a co-training approach to semi-supervised learning on reader emotion classification by considering the message text and the co"
C16-1249,D14-1127,0,0.0137017,"beled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among the source text data The transition probability matrix among the response text data The transition probability matrix among the source and response-text data Mr M s ,r Table 1: Symbol definition LP Figure 2: The framework of single-view label propagation approach to semi-supervised learning on reader emotion classification 2005; Chen et al., 2010; Purver and Battersby, 2012; Hasegawa et al., 2013; Qadir and Riloff, 2014), mainly from the supervised learning paradigm. Compared with above studies on writer emotion classification, studies on reader emotion classification are much limited. Lin et al. (2007) first describe the task of reader emotion classification on news articles with some standard machine learning approaches. Lin et al. (2008) further exploit more features to improve the performance. More recently, Liu et al. (2013) propose a co-training approach to semi-supervised learning on reader emotion classification by considering the message text and the comment text as two views. However, their success"
C16-1249,D09-1150,0,0.111693,"on approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among the source text data The transition probability matrix among the response text data The transition proba"
C16-1249,P14-2070,0,0.0266015,"ith single-view label propagation. Section 4 presents our two-view label propagation approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among the source text data The tr"
C16-1249,P02-1053,0,0.0181175,"source and response text samples. The remainder of this paper is organized as follows. Section 2 overviews related work on emotion classification. Section 3 introduces the baseline approach to semi-supervised reader emotion classification with single-view label propagation. Section 4 presents our two-view label propagation approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-"
C16-1249,E12-1031,0,0.0194119,"ed reader emotion classification with single-view label propagation. Section 4 presents our two-view label propagation approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition probability matrix among"
C16-1249,J09-3003,0,0.019718,"s. The remainder of this paper is organized as follows. Section 2 overviews related work on emotion classification. Section 3 introduces the baseline approach to semi-supervised reader emotion classification with single-view label propagation. Section 4 presents our two-view label propagation approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-text data Ur Unlabeled response-text dat"
C16-1249,C10-1136,0,0.0258978,"to semi-supervised reader emotion classification with single-view label propagation. Section 4 presents our two-view label propagation approach to semi-supervised reader emotion classification. Section 5 empirically evaluates our approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Among the large number of studies in sentiment analysis over the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009), only a small portion focus on emotion classification. Besides those on emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012; Staiano and Guerini, 2014) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009), most of previous studies on emotion classification are devoted to designing novel classification approaches to emotion classification (Alm et al., 2648 Symbol Ls Definition Labeled source-text data Lr Labeled response-text data Us Unlabeled source-text data Ur Unlabeled response-text data Gs Graph of the source-text data Gr Graph of the response-text data Gs , r Joint graph of both the resource and response text data Ms The transition pr"
C16-1310,C10-1021,1,0.82193,"is proposed to refine the classification results. Empirical studies show the effectiveness of the proposed approach to corpus fusion for emotion classification. 1 Introduction Emotion classification aims to recognize human emotions, such as joy, anger or surprise in a given text. Emotion classification has a variety of applications including online chatting (Galik and Rank, 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). In recent years, emotion classification in social media has been greatly popular in the Natural Language Processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012; Li et al., 2015). Because of the popularity of social media today, the analysis of short text on social media becomes more important (Kiritchenko et al., 2014; Wen and Wan, 2014; Wang et al., 2015). Users express their feelings and emotions on various social media platforms. Existing emotion classification approaches are based on corpus classification methods where humanannotated emotion corpora are leveraged to train a machine learning-based emotion classification models. Recently, several different emotion corpora have been proposed by different researchers, suc"
C16-1310,P04-1059,0,0.0359125,"re LDA model to build a domainspecific lexicon. Xu et al. (2010) use language resource, such as synonym dictionary, semantic dictionary, and labeled and unlabeled corpus to construct the similarity matrix between words and seed words. They build an emotion lexicon with five emotion classes using graph-based rules. As a special expression of words, emoticons play an important role in emotion analysis due to the explosion in social media. Tang et al. (2013) annotate data from microblog posts with the help of emoticons. There are several studies to address corpus adaptation problem in NLP field. Gao et al. (2004) do a pioneer work by describing a transformation-based converter to transfer a certain word segmentation result to another annotation guideline. Jiang et al. (2009) investigate the automatic integration of word segmentation knowledge in different annotated corpora. Similar approaches are applied to constituency parsing (Zhu et al., 2011) and word segmentation (Sun and Wan, 2012) Unlike all above studies, we propose a corpus fusion approach to emotion classification in order to address the corpus fusion problem to combine two corpora with different emotion taxonomies and annotation guidelines."
C16-1310,P09-1059,0,0.0257711,"corpus to construct the similarity matrix between words and seed words. They build an emotion lexicon with five emotion classes using graph-based rules. As a special expression of words, emoticons play an important role in emotion analysis due to the explosion in social media. Tang et al. (2013) annotate data from microblog posts with the help of emoticons. There are several studies to address corpus adaptation problem in NLP field. Gao et al. (2004) do a pioneer work by describing a transformation-based converter to transfer a certain word segmentation result to another annotation guideline. Jiang et al. (2009) investigate the automatic integration of word segmentation knowledge in different annotated corpora. Similar approaches are applied to constituency parsing (Zhu et al., 2011) and word segmentation (Sun and Wan, 2012) Unlike all above studies, we propose a corpus fusion approach to emotion classification in order to address the corpus fusion problem to combine two corpora with different emotion taxonomies and annotation guidelines. To the best of our knowledge, this is the first attempt to address this task in emotion analysis. 3 Corpus Two emotion corpora we used are respectively constructed"
C16-1310,P15-1101,1,0.814442,"ts. Empirical studies show the effectiveness of the proposed approach to corpus fusion for emotion classification. 1 Introduction Emotion classification aims to recognize human emotions, such as joy, anger or surprise in a given text. Emotion classification has a variety of applications including online chatting (Galik and Rank, 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). In recent years, emotion classification in social media has been greatly popular in the Natural Language Processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012; Li et al., 2015). Because of the popularity of social media today, the analysis of short text on social media becomes more important (Kiritchenko et al., 2014; Wen and Wan, 2014; Wang et al., 2015). Users express their feelings and emotions on various social media platforms. Existing emotion classification approaches are based on corpus classification methods where humanannotated emotion corpora are leveraged to train a machine learning-based emotion classification models. Recently, several different emotion corpora have been proposed by different researchers, such as Yao et al. (2014) and Huang et al. (2015)"
C16-1310,P13-2091,1,0.849112,"taxonomies. The objective of this approach is to utilize the annotated data from one corpus to help the emotion classification on another corpus. An Integer Linear Programming (ILP) optimization is proposed to refine the classification results. Empirical studies show the effectiveness of the proposed approach to corpus fusion for emotion classification. 1 Introduction Emotion classification aims to recognize human emotions, such as joy, anger or surprise in a given text. Emotion classification has a variety of applications including online chatting (Galik and Rank, 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). In recent years, emotion classification in social media has been greatly popular in the Natural Language Processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012; Li et al., 2015). Because of the popularity of social media today, the analysis of short text on social media becomes more important (Kiritchenko et al., 2014; Wen and Wan, 2014; Wang et al., 2015). Users express their feelings and emotions on various social media platforms. Existing emotion classification approaches are based on corpus classification methods where humanann"
C16-1310,W10-0204,0,0.0385772,"ruct an emotion from TENCENT microblog including both simple and complex emotion annotation. According to the text granularity, emotion analysis works can be generally divided into three levels: document-level, sentence-level, and word-level. Gilad (2005) uses SVM to model a document-level 3288 emotion classifier with blog articles. Yang et al. (2007) identify the emotion types of blog articles based on SVM and CRF with sentiment lexicon. Lin et al. (2007) use the articles on Yahoo! News to analysis the news readers’ emotion. Sentence-level emotion analysis is mainly based on emotion lexicon. Mohammad and Turney (2010) study the effect of word level emotion lexicons for sentence level emotion analysis. They use word level emotion lexicons based on Word Net and NRC-10 to predict the emotion in sentences with Logistic Regression and SVM. Das and Bandyopadhyay (2010) categorize the emotions on Bengali blog. They first identify the emotion of words in a sentence, then judge the emotion of this sentence according to the words’ emotion. Aman and Szpakowicz (2007) implement a knowledge-based sentence level emotion recognition method. Word-level emotion analysis aims to construct emotion lexicon, which plays an imp"
C16-1310,pak-paroubek-2010-twitter,0,0.0477381,"on for emotion classification. Section 5 illustrates the experiments to evaluate the proposed approach. Section 6 gives the conclusion and future work. 2 Related Work In last decade, mainstream approaches for emotion analysis are corpus-based machine learning methods. Several studies construct emotion corpus from social media platform such as blog, microblog and news portal. Gilad (2005) collects blog texts from LiveJournal to construct an emotion corpus with 815,494 blog articles. Quan and Ren (2009) build an emotion corpus from blogs with eight types of emotions on three granularity levels. Pak and Paroubek (2010) establish an emotion corpus by capturing tweets on Twitter. Yao et al. (2014) build an emotion corpus with seven emotion types from SINA microblog. Huang et al. (2015) construct an emotion from TENCENT microblog including both simple and complex emotion annotation. According to the text granularity, emotion analysis works can be generally divided into three levels: document-level, sentence-level, and word-level. Gilad (2005) uses SVM to model a document-level 3288 emotion classifier with blog articles. Yang et al. (2007) identify the emotion types of blog articles based on SVM and CRF with se"
C16-1310,E12-1049,0,0.0226259,"ine the classification results. Empirical studies show the effectiveness of the proposed approach to corpus fusion for emotion classification. 1 Introduction Emotion classification aims to recognize human emotions, such as joy, anger or surprise in a given text. Emotion classification has a variety of applications including online chatting (Galik and Rank, 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). In recent years, emotion classification in social media has been greatly popular in the Natural Language Processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012; Li et al., 2015). Because of the popularity of social media today, the analysis of short text on social media becomes more important (Kiritchenko et al., 2014; Wen and Wan, 2014; Wang et al., 2015). Users express their feelings and emotions on various social media platforms. Existing emotion classification approaches are based on corpus classification methods where humanannotated emotion corpora are leveraged to train a machine learning-based emotion classification models. Recently, several different emotion corpora have been proposed by different researchers, such as Yao et al. (2014) and H"
C16-1310,D09-1150,0,0.0296881,"ated studies. Section 3 introduces two corpora used in this paper. Section 4 proposes the approach to corpus fusion for emotion classification. Section 5 illustrates the experiments to evaluate the proposed approach. Section 6 gives the conclusion and future work. 2 Related Work In last decade, mainstream approaches for emotion analysis are corpus-based machine learning methods. Several studies construct emotion corpus from social media platform such as blog, microblog and news portal. Gilad (2005) collects blog texts from LiveJournal to construct an emotion corpus with 815,494 blog articles. Quan and Ren (2009) build an emotion corpus from blogs with eight types of emotions on three granularity levels. Pak and Paroubek (2010) establish an emotion corpus by capturing tweets on Twitter. Yao et al. (2014) build an emotion corpus with seven emotion types from SINA microblog. Huang et al. (2015) construct an emotion from TENCENT microblog including both simple and complex emotion annotation. According to the text granularity, emotion analysis works can be generally divided into three levels: document-level, sentence-level, and word-level. Gilad (2005) uses SVM to model a document-level 3288 emotion class"
C16-1310,W04-2401,0,0.00977023,"p(rYi = eY2 ) ...p(rYi = eY7 )}, PHi = {p(rHi = eH1 ), p(rHi = eH2 ) ...p(rHi = eH7 )} (2) where p(rYi = eY1 ) denotes the probability of ti belonging to happiness under the emotion taxonomy of YAO (2014), and p(rHi = eH1 ) denotes the probability of ti belonging to joy under the emotion taxonomy of HUANG (2015). The rest can be done in the same manner. 2 http://t.qq.com/ 3290 4.2 Global Optimization with ILP ILP optimization aims to refine the label result given the probability result. We design objective function and constraints to exploit the similarity between two emotion taxonomies. Like Roth and Yih (2004), we firstly define following assignment costs: cYi = −log(p(rYi = eYi )) + log(1 − p(rYi = eYi )), cHi = −log(p(rHi = eHi )) + log(1 − p(rHi = eHi )), (3) 1 ≤i ≤ 7 where cYi is the cost of ti belonging to the ith emotion class under the taxonomy of YAO (2014), and cHi is the cost of ti belonging to the ith emotion class under the taxonomy of HUANG (2015). For each sample ti in testing set there can be two cost vectors CY and CH , and two label vectors LY and LH used on storing the refined labels of ti : CY = [c Y1 c Y2 ... c Y7 ]T , LY = [y1 y2 ... y7 ], CH = [c H1 c H2 ... c H7 ]T LH = [z1 z"
C16-1310,P12-1025,0,0.0164327,"e in emotion analysis due to the explosion in social media. Tang et al. (2013) annotate data from microblog posts with the help of emoticons. There are several studies to address corpus adaptation problem in NLP field. Gao et al. (2004) do a pioneer work by describing a transformation-based converter to transfer a certain word segmentation result to another annotation guideline. Jiang et al. (2009) investigate the automatic integration of word segmentation knowledge in different annotated corpora. Similar approaches are applied to constituency parsing (Zhu et al., 2011) and word segmentation (Sun and Wan, 2012) Unlike all above studies, we propose a corpus fusion approach to emotion classification in order to address the corpus fusion problem to combine two corpora with different emotion taxonomies and annotation guidelines. To the best of our knowledge, this is the first attempt to address this task in emotion analysis. 3 Corpus Two emotion corpora we used are respectively constructed by Yao et al. (2014) and Huang et al. (2015). We simply denote the two corpora as YAO (2014) and HUANG (2015) in the rest of this paper for convenience. YAO (2014) is constructed from SINA microblog1 . It categorizes"
C16-1310,P15-2125,1,0.837377,"otions, such as joy, anger or surprise in a given text. Emotion classification has a variety of applications including online chatting (Galik and Rank, 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). In recent years, emotion classification in social media has been greatly popular in the Natural Language Processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012; Li et al., 2015). Because of the popularity of social media today, the analysis of short text on social media becomes more important (Kiritchenko et al., 2014; Wen and Wan, 2014; Wang et al., 2015). Users express their feelings and emotions on various social media platforms. Existing emotion classification approaches are based on corpus classification methods where humanannotated emotion corpora are leveraged to train a machine learning-based emotion classification models. Recently, several different emotion corpora have been proposed by different researchers, such as Yao et al. (2014) and Huang et al. (2015). However, the size of each existing labeled corpus might be rather limited due to the high cost of data annotation, which results low performance in traditional supervised emotion"
C16-1310,C10-1136,0,0.0331506,"on Word Net and NRC-10 to predict the emotion in sentences with Logistic Regression and SVM. Das and Bandyopadhyay (2010) categorize the emotions on Bengali blog. They first identify the emotion of words in a sentence, then judge the emotion of this sentence according to the words’ emotion. Aman and Szpakowicz (2007) implement a knowledge-based sentence level emotion recognition method. Word-level emotion analysis aims to construct emotion lexicon, which plays an important auxiliary role in emotion analysis. Yang et al. (2014) propose Emotion-aware LDA model to build a domainspecific lexicon. Xu et al. (2010) use language resource, such as synonym dictionary, semantic dictionary, and labeled and unlabeled corpus to construct the similarity matrix between words and seed words. They build an emotion lexicon with five emotion classes using graph-based rules. As a special expression of words, emoticons play an important role in emotion analysis due to the explosion in social media. Tang et al. (2013) annotate data from microblog posts with the help of emoticons. There are several studies to address corpus adaptation problem in NLP field. Gao et al. (2004) do a pioneer work by describing a transformati"
C16-1310,P14-2069,0,0.025824,"icons for sentence level emotion analysis. They use word level emotion lexicons based on Word Net and NRC-10 to predict the emotion in sentences with Logistic Regression and SVM. Das and Bandyopadhyay (2010) categorize the emotions on Bengali blog. They first identify the emotion of words in a sentence, then judge the emotion of this sentence according to the words’ emotion. Aman and Szpakowicz (2007) implement a knowledge-based sentence level emotion recognition method. Word-level emotion analysis aims to construct emotion lexicon, which plays an important auxiliary role in emotion analysis. Yang et al. (2014) propose Emotion-aware LDA model to build a domainspecific lexicon. Xu et al. (2010) use language resource, such as synonym dictionary, semantic dictionary, and labeled and unlabeled corpus to construct the similarity matrix between words and seed words. They build an emotion lexicon with five emotion classes using graph-based rules. As a special expression of words, emoticons play an important role in emotion analysis due to the explosion in social media. Tang et al. (2013) annotate data from microblog posts with the help of emoticons. There are several studies to address corpus adaptation pr"
C16-1310,P11-2126,0,0.0253018,"of words, emoticons play an important role in emotion analysis due to the explosion in social media. Tang et al. (2013) annotate data from microblog posts with the help of emoticons. There are several studies to address corpus adaptation problem in NLP field. Gao et al. (2004) do a pioneer work by describing a transformation-based converter to transfer a certain word segmentation result to another annotation guideline. Jiang et al. (2009) investigate the automatic integration of word segmentation knowledge in different annotated corpora. Similar approaches are applied to constituency parsing (Zhu et al., 2011) and word segmentation (Sun and Wan, 2012) Unlike all above studies, we propose a corpus fusion approach to emotion classification in order to address the corpus fusion problem to combine two corpora with different emotion taxonomies and annotation guidelines. To the best of our knowledge, this is the first attempt to address this task in emotion analysis. 3 Corpus Two emotion corpora we used are respectively constructed by Yao et al. (2014) and Huang et al. (2015). We simply denote the two corpora as YAO (2014) and HUANG (2015) in the rest of this paper for convenience. YAO (2014) is construc"
C18-1015,I08-1065,0,0.0288267,"in section 8. 2 2.1 Related Work Causal and Temporal Event Relation Identification So far, the study of event relation parsing mainly concentrates on identifying two kinds of relationships, causality and temporality respectively. The early work on causality identification can be traced back to 178 the use of lexical-syntactic patterns (Girju et al., 2002; Girju, 2003). Soon thereafter, Chang and Choi (2004) revise Girju et al. (2002)’s patterns using lexical pairs (LP) and cue phrases (e.g., due to). Besides they generalize the model for binary relation classification using the Bayes theorem. Abe et al. (2008) further use co-occurrence probability as the novel feature. Recently, scholars have made a great effort to model fine-grained causal relations (Inui et al., 2005), exploit the features for classification (Blanco et al., 2008), refine the patterns by syntactic and discourse parsing (Ittoo and Bouma, 2011; Do et al., 2011) and predict relations by semantic network (Radinsky et al., 2012). Mani et al. (2006) and Lapata and Lascarides (2006) presented the first study on the temporal relation parsing. Both focus on the machine learning approaches. In the past decade, the SemEval (Verhagen et al.,"
C18-1015,S07-1025,0,0.0421137,"05), exploit the features for classification (Blanco et al., 2008), refine the patterns by syntactic and discourse parsing (Ittoo and Bouma, 2011; Do et al., 2011) and predict relations by semantic network (Radinsky et al., 2012). Mani et al. (2006) and Lapata and Lascarides (2006) presented the first study on the temporal relation parsing. Both focus on the machine learning approaches. In the past decade, the SemEval (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009) has promoted a great deal of experimental study, including on the grammatical, syntactic, semantic and ordering features (Bethard and Martin, 2007; Hepple et al., 2007; Pus¸cas¸u, 2007; Bethard, 2013; Mirza and Minard, 2015; Caselli et al., 2015; Hashimoto et al., 2015). Meanwhile, temporal relation modeling has been implemented in different ways, such as sequence labeling, Markov logic networks and hybrid systems (Cheng et al., 2007; Min et al., 2007; Yoshikawa et al., 2009; UzZaman and Allen, 2010; Llorens et al., 2010; Velupillai et al., 2015). 2.2 Multi-class Discourse Relation Classification In April 2006 (Prasad et al., 2007), the Penn Discourse Tree Bank (PDTB) was released as a corpus of discourse-level arguments as well as anno"
C18-1015,S13-2002,0,0.013211,"8), refine the patterns by syntactic and discourse parsing (Ittoo and Bouma, 2011; Do et al., 2011) and predict relations by semantic network (Radinsky et al., 2012). Mani et al. (2006) and Lapata and Lascarides (2006) presented the first study on the temporal relation parsing. Both focus on the machine learning approaches. In the past decade, the SemEval (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009) has promoted a great deal of experimental study, including on the grammatical, syntactic, semantic and ordering features (Bethard and Martin, 2007; Hepple et al., 2007; Pus¸cas¸u, 2007; Bethard, 2013; Mirza and Minard, 2015; Caselli et al., 2015; Hashimoto et al., 2015). Meanwhile, temporal relation modeling has been implemented in different ways, such as sequence labeling, Markov logic networks and hybrid systems (Cheng et al., 2007; Min et al., 2007; Yoshikawa et al., 2009; UzZaman and Allen, 2010; Llorens et al., 2010; Velupillai et al., 2015). 2.2 Multi-class Discourse Relation Classification In April 2006 (Prasad et al., 2007), the Penn Discourse Tree Bank (PDTB) was released as a corpus of discourse-level arguments as well as annotations of explicit and implicit relations. The PDTB"
C18-1015,C10-3009,0,0.0312492,"Missing"
C18-1015,blanco-etal-2008-causal,0,0.0393653,"ively. The early work on causality identification can be traced back to 178 the use of lexical-syntactic patterns (Girju et al., 2002; Girju, 2003). Soon thereafter, Chang and Choi (2004) revise Girju et al. (2002)’s patterns using lexical pairs (LP) and cue phrases (e.g., due to). Besides they generalize the model for binary relation classification using the Bayes theorem. Abe et al. (2008) further use co-occurrence probability as the novel feature. Recently, scholars have made a great effort to model fine-grained causal relations (Inui et al., 2005), exploit the features for classification (Blanco et al., 2008), refine the patterns by syntactic and discourse parsing (Ittoo and Bouma, 2011; Do et al., 2011) and predict relations by semantic network (Radinsky et al., 2012). Mani et al. (2006) and Lapata and Lascarides (2006) presented the first study on the temporal relation parsing. Both focus on the machine learning approaches. In the past decade, the SemEval (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009) has promoted a great deal of experimental study, including on the grammatical, syntactic, semantic and ordering features (Bethard and Martin, 2007; Hepple et al., 2007; Pus¸cas¸u, 2007; Be"
C18-1015,D16-1020,0,0.0141561,"ural Network (SCNN) model, which is used by Ponti and Korhonen (2017) to identify contingent relation. Qin et al (2016) utilize CNN with a collaborative gated neural network (CGNN) for recognizing implicit relations. Chen et al (2016) develop a Gated Relevance Network (GRN) based on Bidirectional Long-Short Term Memory (Bi-LSTM) framework, combining bilinear model and single layer network for relevance measurement between arguments. Besides, the current work improves the classification performance of implicit relations by expanding the training data with connectives (Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017) or combining multiple corpora via multi-task learning (Liu et al., 2016). 3 Relation Inference Engine In our statistical inference process, TERB is an indispensable knowledge base and needed to be build first. But in this section, we suppose that TERB has been established successfully, and focus on presenting the inference approach. TERB building will be treated as a separate work and presented in section 6. 3.1 Framework and Terminology The input of the relation inference engine is a pair of query events, while the output is a relation type tag. The inference engine is cons"
C18-1015,S15-2133,0,0.0149258,"d discourse parsing (Ittoo and Bouma, 2011; Do et al., 2011) and predict relations by semantic network (Radinsky et al., 2012). Mani et al. (2006) and Lapata and Lascarides (2006) presented the first study on the temporal relation parsing. Both focus on the machine learning approaches. In the past decade, the SemEval (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009) has promoted a great deal of experimental study, including on the grammatical, syntactic, semantic and ordering features (Bethard and Martin, 2007; Hepple et al., 2007; Pus¸cas¸u, 2007; Bethard, 2013; Mirza and Minard, 2015; Caselli et al., 2015; Hashimoto et al., 2015). Meanwhile, temporal relation modeling has been implemented in different ways, such as sequence labeling, Markov logic networks and hybrid systems (Cheng et al., 2007; Min et al., 2007; Yoshikawa et al., 2009; UzZaman and Allen, 2010; Llorens et al., 2010; Velupillai et al., 2015). 2.2 Multi-class Discourse Relation Classification In April 2006 (Prasad et al., 2007), the Penn Discourse Tree Bank (PDTB) was released as a corpus of discourse-level arguments as well as annotations of explicit and implicit relations. The PDTB relation scheme consists of 4 main relation cl"
C18-1015,P16-1163,0,0.228407,"ance 4 – Mask wearing is in fashion &lt;Result&gt; Relation – Causality &lt;Previously known&gt; In our experiments, images are taken as the visual scenes. On the basis, we introduce image captions into cross-media semantic matching, with the purpose of mining possible visual scenes by similarity measurement between query events and image captions. In addition, the Convolutional Neural Network (CNN) based image representation is utilized in visual scene matching. Over the ACE-R2, we compare our model with two fully-supervised discourse relation classification models, including Qin et al. (2016)’s CNN and Chen et al. (2016)’s Long-Short Term Memory (LSTM) based Recurrent Neural Network. Experimental results show that our minimally-supervised model slightly outperforms LSTM and obtain comparable performance with CNN. In the rest of the paper, we overview the related work in section 2. And then we present the methodological framework in section 3, the caption based cross-media semantic matching in section 4 and image matching in section 5. Section 6 will give the TERB establishment method. In section 7, we evaluate the proposed method and analyze the experimental results. We conclude the paper in section 8. 2 2.1"
C18-1015,P17-1038,0,0.0319469,"with a high level of confidence. This allows us to acquire numerous explicitly-related events from texts using a few carefullyselected connectives and simple patterns. Accordingly, we build a large-scale Textual Event Relation Bank (TERB) to support the relation inference. We focus on the second issue in this paper, measuring the Query Event 1 Event Instance 3 similarity between events. Recently, neural network has been successfully used in event-oriented semantic encoding to some extent (Nguyen and Grishman, 2016; Ghaeini et al., 2016; Feng et al., 2016; Peng et al., 2016; Liu et al., 2017; Chen et al., 2017), yielding substantial performance gains in the reEvent Instance 4 Query Event 2 lated information extraction tasks, such as event extraction (Doddington et al., 2004; Ahn, 2006) and nugget detection (Ellis et al., 2015). Semantic encoding enables the generation of a high-dimensional distributed representation for characterizing an event. So that it prompts semantic learning, computing and understanding at a very deep level. Undoubtedly, this Figure 1: Similar visual scenes can facilitate the acquisition of the “semantically-similar but pragmatically-different” event instances for the query ev"
C18-1015,S07-1052,0,0.0405727,"first study on the temporal relation parsing. Both focus on the machine learning approaches. In the past decade, the SemEval (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009) has promoted a great deal of experimental study, including on the grammatical, syntactic, semantic and ordering features (Bethard and Martin, 2007; Hepple et al., 2007; Pus¸cas¸u, 2007; Bethard, 2013; Mirza and Minard, 2015; Caselli et al., 2015; Hashimoto et al., 2015). Meanwhile, temporal relation modeling has been implemented in different ways, such as sequence labeling, Markov logic networks and hybrid systems (Cheng et al., 2007; Min et al., 2007; Yoshikawa et al., 2009; UzZaman and Allen, 2010; Llorens et al., 2010; Velupillai et al., 2015). 2.2 Multi-class Discourse Relation Classification In April 2006 (Prasad et al., 2007), the Penn Discourse Tree Bank (PDTB) was released as a corpus of discourse-level arguments as well as annotations of explicit and implicit relations. The PDTB relation scheme consists of 4 main relation classes and 16 sub-classes. Since it is released together with the corpus, a great deal of research has focused on the methodologies for multi-class relation classification. Motivated by Marcu a"
C18-1015,D11-1027,0,0.0199911,"tic patterns (Girju et al., 2002; Girju, 2003). Soon thereafter, Chang and Choi (2004) revise Girju et al. (2002)’s patterns using lexical pairs (LP) and cue phrases (e.g., due to). Besides they generalize the model for binary relation classification using the Bayes theorem. Abe et al. (2008) further use co-occurrence probability as the novel feature. Recently, scholars have made a great effort to model fine-grained causal relations (Inui et al., 2005), exploit the features for classification (Blanco et al., 2008), refine the patterns by syntactic and discourse parsing (Ittoo and Bouma, 2011; Do et al., 2011) and predict relations by semantic network (Radinsky et al., 2012). Mani et al. (2006) and Lapata and Lascarides (2006) presented the first study on the temporal relation parsing. Both focus on the machine learning approaches. In the past decade, the SemEval (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009) has promoted a great deal of experimental study, including on the grammatical, syntactic, semantic and ordering features (Bethard and Martin, 2007; Hepple et al., 2007; Pus¸cas¸u, 2007; Bethard, 2013; Mirza and Minard, 2015; Caselli et al., 2015; Hashimoto et al., 2015). Meanwhile, te"
C18-1015,doddington-etal-2004-automatic,0,0.182842,"tterns. Accordingly, we build a large-scale Textual Event Relation Bank (TERB) to support the relation inference. We focus on the second issue in this paper, measuring the Query Event 1 Event Instance 3 similarity between events. Recently, neural network has been successfully used in event-oriented semantic encoding to some extent (Nguyen and Grishman, 2016; Ghaeini et al., 2016; Feng et al., 2016; Peng et al., 2016; Liu et al., 2017; Chen et al., 2017), yielding substantial performance gains in the reEvent Instance 4 Query Event 2 lated information extraction tasks, such as event extraction (Doddington et al., 2004; Ahn, 2006) and nugget detection (Ellis et al., 2015). Semantic encoding enables the generation of a high-dimensional distributed representation for characterizing an event. So that it prompts semantic learning, computing and understanding at a very deep level. Undoubtedly, this Figure 1: Similar visual scenes can facilitate the acquisition of the “semantically-similar but pragmatically-different” event instances for the query events, such as those in (2). However, in our recent research on utilizing semantic similarity calculation, we fail to pass through a bottleneck that, for some query ev"
C18-1015,C14-1008,0,0.0110439,"e to use a relatively simple model because we are more willing to verify the validity of the methodological framework. The configuration of the employed CNN is presented as below: in the input layer, a short text (caption or mention) is represented as a fixed-size sequence of real numbers, involving 30 256-dimensional word embeddings. Zero padding is performed when the text length is smaller than 40, otherwise tail clipping. We follow Mikolov et al. (2013) to use skip-gram based word2vec to compute embeddings, and conduct training on the English articles in the latest 2015 Wikipedia dump (Dos Santos and Gatti, 2014). In the hidden layer, there are 128 (3×256) filters used for the convolutional computation with a stride of 1. This yields 28 128-dimensional feature vectors. Max pooling is then used to produce a lower dimensional (1×128) vector. As usual, we apply a dense layer to slightly increase the depth. And further, a dropout layer (rate=0.5) is used to produce a 64-dimensional vector. The vector produced in this way is specified as the semantic representation of the text. In the output layer, a fully-connected (FC) layer is used, followed by a 18-way softmax classification layer. We train the model o"
C18-1015,P16-2011,0,0.025376,"ve “because” can be determined to have a causal relation with a high level of confidence. This allows us to acquire numerous explicitly-related events from texts using a few carefullyselected connectives and simple patterns. Accordingly, we build a large-scale Textual Event Relation Bank (TERB) to support the relation inference. We focus on the second issue in this paper, measuring the Query Event 1 Event Instance 3 similarity between events. Recently, neural network has been successfully used in event-oriented semantic encoding to some extent (Nguyen and Grishman, 2016; Ghaeini et al., 2016; Feng et al., 2016; Peng et al., 2016; Liu et al., 2017; Chen et al., 2017), yielding substantial performance gains in the reEvent Instance 4 Query Event 2 lated information extraction tasks, such as event extraction (Doddington et al., 2004; Ahn, 2006) and nugget detection (Ellis et al., 2015). Semantic encoding enables the generation of a high-dimensional distributed representation for characterizing an event. So that it prompts semantic learning, computing and understanding at a very deep level. Undoubtedly, this Figure 1: Similar visual scenes can facilitate the acquisition of the “semantically-similar but"
C18-1015,P16-2060,0,0.0297084,"nected by the connective “because” can be determined to have a causal relation with a high level of confidence. This allows us to acquire numerous explicitly-related events from texts using a few carefullyselected connectives and simple patterns. Accordingly, we build a large-scale Textual Event Relation Bank (TERB) to support the relation inference. We focus on the second issue in this paper, measuring the Query Event 1 Event Instance 3 similarity between events. Recently, neural network has been successfully used in event-oriented semantic encoding to some extent (Nguyen and Grishman, 2016; Ghaeini et al., 2016; Feng et al., 2016; Peng et al., 2016; Liu et al., 2017; Chen et al., 2017), yielding substantial performance gains in the reEvent Instance 4 Query Event 2 lated information extraction tasks, such as event extraction (Doddington et al., 2004; Ahn, 2006) and nugget detection (Ellis et al., 2015). Semantic encoding enables the generation of a high-dimensional distributed representation for characterizing an event. So that it prompts semantic learning, computing and understanding at a very deep level. Undoubtedly, this Figure 1: Similar visual scenes can facilitate the acquisition of the “semant"
C18-1015,W03-1210,0,0.0943956,"on based cross-media semantic matching in section 4 and image matching in section 5. Section 6 will give the TERB establishment method. In section 7, we evaluate the proposed method and analyze the experimental results. We conclude the paper in section 8. 2 2.1 Related Work Causal and Temporal Event Relation Identification So far, the study of event relation parsing mainly concentrates on identifying two kinds of relationships, causality and temporality respectively. The early work on causality identification can be traced back to 178 the use of lexical-syntactic patterns (Girju et al., 2002; Girju, 2003). Soon thereafter, Chang and Choi (2004) revise Girju et al. (2002)’s patterns using lexical pairs (LP) and cue phrases (e.g., due to). Besides they generalize the model for binary relation classification using the Bayes theorem. Abe et al. (2008) further use co-occurrence probability as the novel feature. Recently, scholars have made a great effort to model fine-grained causal relations (Inui et al., 2005), exploit the features for classification (Blanco et al., 2008), refine the patterns by syntactic and discourse parsing (Ittoo and Bouma, 2011; Do et al., 2011) and predict relations by sema"
C18-1015,S07-1098,0,0.0169668,"for classification (Blanco et al., 2008), refine the patterns by syntactic and discourse parsing (Ittoo and Bouma, 2011; Do et al., 2011) and predict relations by semantic network (Radinsky et al., 2012). Mani et al. (2006) and Lapata and Lascarides (2006) presented the first study on the temporal relation parsing. Both focus on the machine learning approaches. In the past decade, the SemEval (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009) has promoted a great deal of experimental study, including on the grammatical, syntactic, semantic and ordering features (Bethard and Martin, 2007; Hepple et al., 2007; Pus¸cas¸u, 2007; Bethard, 2013; Mirza and Minard, 2015; Caselli et al., 2015; Hashimoto et al., 2015). Meanwhile, temporal relation modeling has been implemented in different ways, such as sequence labeling, Markov logic networks and hybrid systems (Cheng et al., 2007; Min et al., 2007; Yoshikawa et al., 2009; UzZaman and Allen, 2010; Llorens et al., 2010; Velupillai et al., 2015). 2.2 Multi-class Discourse Relation Classification In April 2006 (Prasad et al., 2007), the Penn Discourse Tree Bank (PDTB) was released as a corpus of discourse-level arguments as well as annotations of explicit a"
C18-1015,W16-1701,1,0.918883,"st our method on the ACE-R2 corpus and compare it with the fully-supervised neural network models. Experimental results show that we achieve a comparable performance to CNN while slightly better than LSTM. 1 Introduction Event relation recognition aims to predict the relationship between the query events. Here, an event is defined as a text span (sentence or clause) which describes the occurrence of a genuine event, such as “The 2nd industrial revolution”. An event-oriented relation recogntion system is required to assign a relation type tag to a pair of query events, such as those defined in Hong et al (2016)’s natural event relation scheme, including causality, temporality, conditionality, etc. Listed below are a pair of related query events, along with the relation need to be predicted: (1) Event Instance 1 – The 2nd industrial revolution &lt;Cause&gt; Event Instance 2 – The killer fog that blanketed London &lt;Result&gt; Relation – Causality &lt;Need to be predicted&gt; Recognizing event relations in an automatic way is a challenging task. It is because the query events are selected from different paragraphs in a document or even different documents, so that there is lack of explicit clue and shared context can"
C18-1015,P06-1095,0,0.0555205,"Missing"
C18-1015,D09-1036,0,0.0274487,"l 2006 (Prasad et al., 2007), the Penn Discourse Tree Bank (PDTB) was released as a corpus of discourse-level arguments as well as annotations of explicit and implicit relations. The PDTB relation scheme consists of 4 main relation classes and 16 sub-classes. Since it is released together with the corpus, a great deal of research has focused on the methodologies for multi-class relation classification. Motivated by Marcu and Echihabi (2002)’s work, in the earlier study, both feature engineering and sophisticated machine learning dominated the field in a large region (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, it becomes increasingly popular to use neural networks for learning to classify discourse relations (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). Typically, Zhang et al (2015) propose a Shallow Convolutional Neural Network (SCNN) model, which is used by Ponti and Korhonen (2017) to identify contingent relation. Qin et al (2016) utilize CNN with a collaborative gated neural network (CGNN) for recognizing implicit relations. Chen et al (2016) develop a Gated Relevanc"
C18-1015,D16-1130,0,0.0127719,"leased together with the corpus, a great deal of research has focused on the methodologies for multi-class relation classification. Motivated by Marcu and Echihabi (2002)’s work, in the earlier study, both feature engineering and sophisticated machine learning dominated the field in a large region (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, it becomes increasingly popular to use neural networks for learning to classify discourse relations (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). Typically, Zhang et al (2015) propose a Shallow Convolutional Neural Network (SCNN) model, which is used by Ponti and Korhonen (2017) to identify contingent relation. Qin et al (2016) utilize CNN with a collaborative gated neural network (CGNN) for recognizing implicit relations. Chen et al (2016) develop a Gated Relevance Network (GRN) based on Bidirectional Long-Short Term Memory (Bi-LSTM) framework, combining bilinear model and single layer network for relevance measurement between arguments. Besides, the current work improves the classification performance of implicit relations by expand"
C18-1015,P17-1164,0,0.0211371,"a causal relation with a high level of confidence. This allows us to acquire numerous explicitly-related events from texts using a few carefullyselected connectives and simple patterns. Accordingly, we build a large-scale Textual Event Relation Bank (TERB) to support the relation inference. We focus on the second issue in this paper, measuring the Query Event 1 Event Instance 3 similarity between events. Recently, neural network has been successfully used in event-oriented semantic encoding to some extent (Nguyen and Grishman, 2016; Ghaeini et al., 2016; Feng et al., 2016; Peng et al., 2016; Liu et al., 2017; Chen et al., 2017), yielding substantial performance gains in the reEvent Instance 4 Query Event 2 lated information extraction tasks, such as event extraction (Doddington et al., 2004; Ahn, 2006) and nugget detection (Ellis et al., 2015). Semantic encoding enables the generation of a high-dimensional distributed representation for characterizing an event. So that it prompts semantic learning, computing and understanding at a very deep level. Undoubtedly, this Figure 1: Similar visual scenes can facilitate the acquisition of the “semantically-similar but pragmatically-different” event instan"
C18-1015,S10-1063,0,0.016748,"ches. In the past decade, the SemEval (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009) has promoted a great deal of experimental study, including on the grammatical, syntactic, semantic and ordering features (Bethard and Martin, 2007; Hepple et al., 2007; Pus¸cas¸u, 2007; Bethard, 2013; Mirza and Minard, 2015; Caselli et al., 2015; Hashimoto et al., 2015). Meanwhile, temporal relation modeling has been implemented in different ways, such as sequence labeling, Markov logic networks and hybrid systems (Cheng et al., 2007; Min et al., 2007; Yoshikawa et al., 2009; UzZaman and Allen, 2010; Llorens et al., 2010; Velupillai et al., 2015). 2.2 Multi-class Discourse Relation Classification In April 2006 (Prasad et al., 2007), the Penn Discourse Tree Bank (PDTB) was released as a corpus of discourse-level arguments as well as annotations of explicit and implicit relations. The PDTB relation scheme consists of 4 main relation classes and 16 sub-classes. Since it is released together with the corpus, a great deal of research has focused on the methodologies for multi-class relation classification. Motivated by Marcu and Echihabi (2002)’s work, in the earlier study, both feature engineering and sophisticat"
C18-1015,W10-4310,0,0.0156199,"al., 2007), the Penn Discourse Tree Bank (PDTB) was released as a corpus of discourse-level arguments as well as annotations of explicit and implicit relations. The PDTB relation scheme consists of 4 main relation classes and 16 sub-classes. Since it is released together with the corpus, a great deal of research has focused on the methodologies for multi-class relation classification. Motivated by Marcu and Echihabi (2002)’s work, in the earlier study, both feature engineering and sophisticated machine learning dominated the field in a large region (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, it becomes increasingly popular to use neural networks for learning to classify discourse relations (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). Typically, Zhang et al (2015) propose a Shallow Convolutional Neural Network (SCNN) model, which is used by Ponti and Korhonen (2017) to identify contingent relation. Qin et al (2016) utilize CNN with a collaborative gated neural network (CGNN) for recognizing implicit relations. Chen et al (2016) develop a Gated Relevance Network (GRN) base"
C18-1015,P02-1047,0,0.177781,"., 2007; Min et al., 2007; Yoshikawa et al., 2009; UzZaman and Allen, 2010; Llorens et al., 2010; Velupillai et al., 2015). 2.2 Multi-class Discourse Relation Classification In April 2006 (Prasad et al., 2007), the Penn Discourse Tree Bank (PDTB) was released as a corpus of discourse-level arguments as well as annotations of explicit and implicit relations. The PDTB relation scheme consists of 4 main relation classes and 16 sub-classes. Since it is released together with the corpus, a great deal of research has focused on the methodologies for multi-class relation classification. Motivated by Marcu and Echihabi (2002)’s work, in the earlier study, both feature engineering and sophisticated machine learning dominated the field in a large region (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, it becomes increasingly popular to use neural networks for learning to classify discourse relations (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). Typically, Zhang et al (2015) propose a Shallow Convolutional Neural Network (SCNN) model, which is used by Ponti and Korhonen (2017) to identify conti"
C18-1015,S07-1046,0,0.0313683,"emporal relation parsing. Both focus on the machine learning approaches. In the past decade, the SemEval (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009) has promoted a great deal of experimental study, including on the grammatical, syntactic, semantic and ordering features (Bethard and Martin, 2007; Hepple et al., 2007; Pus¸cas¸u, 2007; Bethard, 2013; Mirza and Minard, 2015; Caselli et al., 2015; Hashimoto et al., 2015). Meanwhile, temporal relation modeling has been implemented in different ways, such as sequence labeling, Markov logic networks and hybrid systems (Cheng et al., 2007; Min et al., 2007; Yoshikawa et al., 2009; UzZaman and Allen, 2010; Llorens et al., 2010; Velupillai et al., 2015). 2.2 Multi-class Discourse Relation Classification In April 2006 (Prasad et al., 2007), the Penn Discourse Tree Bank (PDTB) was released as a corpus of discourse-level arguments as well as annotations of explicit and implicit relations. The PDTB relation scheme consists of 4 main relation classes and 16 sub-classes. Since it is released together with the corpus, a great deal of research has focused on the methodologies for multi-class relation classification. Motivated by Marcu and Echihabi (2002)"
C18-1015,S15-2135,0,0.0190845,"patterns by syntactic and discourse parsing (Ittoo and Bouma, 2011; Do et al., 2011) and predict relations by semantic network (Radinsky et al., 2012). Mani et al. (2006) and Lapata and Lascarides (2006) presented the first study on the temporal relation parsing. Both focus on the machine learning approaches. In the past decade, the SemEval (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009) has promoted a great deal of experimental study, including on the grammatical, syntactic, semantic and ordering features (Bethard and Martin, 2007; Hepple et al., 2007; Pus¸cas¸u, 2007; Bethard, 2013; Mirza and Minard, 2015; Caselli et al., 2015; Hashimoto et al., 2015). Meanwhile, temporal relation modeling has been implemented in different ways, such as sequence labeling, Markov logic networks and hybrid systems (Cheng et al., 2007; Min et al., 2007; Yoshikawa et al., 2009; UzZaman and Allen, 2010; Llorens et al., 2010; Velupillai et al., 2015). 2.2 Multi-class Discourse Relation Classification In April 2006 (Prasad et al., 2007), the Penn Discourse Tree Bank (PDTB) was released as a corpus of discourse-level arguments as well as annotations of explicit and implicit relations. The PDTB relation scheme consists"
C18-1015,D16-1085,0,0.0319934,"mple, a pair of clauses connected by the connective “because” can be determined to have a causal relation with a high level of confidence. This allows us to acquire numerous explicitly-related events from texts using a few carefullyselected connectives and simple patterns. Accordingly, we build a large-scale Textual Event Relation Bank (TERB) to support the relation inference. We focus on the second issue in this paper, measuring the Query Event 1 Event Instance 3 similarity between events. Recently, neural network has been successfully used in event-oriented semantic encoding to some extent (Nguyen and Grishman, 2016; Ghaeini et al., 2016; Feng et al., 2016; Peng et al., 2016; Liu et al., 2017; Chen et al., 2017), yielding substantial performance gains in the reEvent Instance 4 Query Event 2 lated information extraction tasks, such as event extraction (Doddington et al., 2004; Ahn, 2006) and nugget detection (Ellis et al., 2015). Semantic encoding enables the generation of a high-dimensional distributed representation for characterizing an event. So that it prompts semantic learning, computing and understanding at a very deep level. Undoubtedly, this Figure 1: Similar visual scenes can facilitate the acqu"
C18-1015,W12-1614,0,0.0147954,"Discourse Tree Bank (PDTB) was released as a corpus of discourse-level arguments as well as annotations of explicit and implicit relations. The PDTB relation scheme consists of 4 main relation classes and 16 sub-classes. Since it is released together with the corpus, a great deal of research has focused on the methodologies for multi-class relation classification. Motivated by Marcu and Echihabi (2002)’s work, in the earlier study, both feature engineering and sophisticated machine learning dominated the field in a large region (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, it becomes increasingly popular to use neural networks for learning to classify discourse relations (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). Typically, Zhang et al (2015) propose a Shallow Convolutional Neural Network (SCNN) model, which is used by Ponti and Korhonen (2017) to identify contingent relation. Qin et al (2016) utilize CNN with a collaborative gated neural network (CGNN) for recognizing implicit relations. Chen et al (2016) develop a Gated Relevance Network (GRN) based on Bidirectional Long"
C18-1015,D16-1038,0,0.0254941,"determined to have a causal relation with a high level of confidence. This allows us to acquire numerous explicitly-related events from texts using a few carefullyselected connectives and simple patterns. Accordingly, we build a large-scale Textual Event Relation Bank (TERB) to support the relation inference. We focus on the second issue in this paper, measuring the Query Event 1 Event Instance 3 similarity between events. Recently, neural network has been successfully used in event-oriented semantic encoding to some extent (Nguyen and Grishman, 2016; Ghaeini et al., 2016; Feng et al., 2016; Peng et al., 2016; Liu et al., 2017; Chen et al., 2017), yielding substantial performance gains in the reEvent Instance 4 Query Event 2 lated information extraction tasks, such as event extraction (Doddington et al., 2004; Ahn, 2006) and nugget detection (Ellis et al., 2015). Semantic encoding enables the generation of a high-dimensional distributed representation for characterizing an event. So that it prompts semantic learning, computing and understanding at a very deep level. Undoubtedly, this Figure 1: Similar visual scenes can facilitate the acquisition of the “semantically-similar but pragmatically-diffe"
C18-1015,P09-2004,0,0.00974967,"ion Classification In April 2006 (Prasad et al., 2007), the Penn Discourse Tree Bank (PDTB) was released as a corpus of discourse-level arguments as well as annotations of explicit and implicit relations. The PDTB relation scheme consists of 4 main relation classes and 16 sub-classes. Since it is released together with the corpus, a great deal of research has focused on the methodologies for multi-class relation classification. Motivated by Marcu and Echihabi (2002)’s work, in the earlier study, both feature engineering and sophisticated machine learning dominated the field in a large region (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, it becomes increasingly popular to use neural networks for learning to classify discourse relations (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). Typically, Zhang et al (2015) propose a Shallow Convolutional Neural Network (SCNN) model, which is used by Ponti and Korhonen (2017) to identify contingent relation. Qin et al (2016) utilize CNN with a collaborative gated neural network (CGNN) for recognizing implicit relations. Chen et al (2016) develo"
C18-1015,W17-0903,0,0.0208373,"ion. Motivated by Marcu and Echihabi (2002)’s work, in the earlier study, both feature engineering and sophisticated machine learning dominated the field in a large region (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, it becomes increasingly popular to use neural networks for learning to classify discourse relations (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). Typically, Zhang et al (2015) propose a Shallow Convolutional Neural Network (SCNN) model, which is used by Ponti and Korhonen (2017) to identify contingent relation. Qin et al (2016) utilize CNN with a collaborative gated neural network (CGNN) for recognizing implicit relations. Chen et al (2016) develop a Gated Relevance Network (GRN) based on Bidirectional Long-Short Term Memory (Bi-LSTM) framework, combining bilinear model and single layer network for relevance measurement between arguments. Besides, the current work improves the classification performance of implicit relations by expanding the training data with connectives (Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017) or combining multiple corpora"
C18-1015,S07-1108,0,0.0852605,"Missing"
C18-1015,W09-2418,0,0.02249,"er use co-occurrence probability as the novel feature. Recently, scholars have made a great effort to model fine-grained causal relations (Inui et al., 2005), exploit the features for classification (Blanco et al., 2008), refine the patterns by syntactic and discourse parsing (Ittoo and Bouma, 2011; Do et al., 2011) and predict relations by semantic network (Radinsky et al., 2012). Mani et al. (2006) and Lapata and Lascarides (2006) presented the first study on the temporal relation parsing. Both focus on the machine learning approaches. In the past decade, the SemEval (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009) has promoted a great deal of experimental study, including on the grammatical, syntactic, semantic and ordering features (Bethard and Martin, 2007; Hepple et al., 2007; Pus¸cas¸u, 2007; Bethard, 2013; Mirza and Minard, 2015; Caselli et al., 2015; Hashimoto et al., 2015). Meanwhile, temporal relation modeling has been implemented in different ways, such as sequence labeling, Markov logic networks and hybrid systems (Cheng et al., 2007; Min et al., 2007; Yoshikawa et al., 2009; UzZaman and Allen, 2010; Llorens et al., 2010; Velupillai et al., 2015). 2.2 Multi-class Discourse Relation Classifica"
C18-1015,D16-1246,0,0.191405,"buildings &lt;Cause&gt; Event Instance 4 – Mask wearing is in fashion &lt;Result&gt; Relation – Causality &lt;Previously known&gt; In our experiments, images are taken as the visual scenes. On the basis, we introduce image captions into cross-media semantic matching, with the purpose of mining possible visual scenes by similarity measurement between query events and image captions. In addition, the Convolutional Neural Network (CNN) based image representation is utilized in visual scene matching. Over the ACE-R2, we compare our model with two fully-supervised discourse relation classification models, including Qin et al. (2016)’s CNN and Chen et al. (2016)’s Long-Short Term Memory (LSTM) based Recurrent Neural Network. Experimental results show that our minimally-supervised model slightly outperforms LSTM and obtain comparable performance with CNN. In the rest of the paper, we overview the related work in section 2. And then we present the methodological framework in section 3, the caption based cross-media semantic matching in section 4 and image matching in section 5. Section 6 will give the TERB establishment method. In section 7, we evaluate the proposed method and analyze the experimental results. We conclude t"
C18-1015,P17-1093,0,0.0159947,"es. Since it is released together with the corpus, a great deal of research has focused on the methodologies for multi-class relation classification. Motivated by Marcu and Echihabi (2002)’s work, in the earlier study, both feature engineering and sophisticated machine learning dominated the field in a large region (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, it becomes increasingly popular to use neural networks for learning to classify discourse relations (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). Typically, Zhang et al (2015) propose a Shallow Convolutional Neural Network (SCNN) model, which is used by Ponti and Korhonen (2017) to identify contingent relation. Qin et al (2016) utilize CNN with a collaborative gated neural network (CGNN) for recognizing implicit relations. Chen et al (2016) develop a Gated Relevance Network (GRN) based on Bidirectional Long-Short Term Memory (Bi-LSTM) framework, combining bilinear model and single layer network for relevance measurement between arguments. Besides, the current work improves the classification performance of implicit"
C18-1015,E14-1068,0,0.0253689,"DTB) was released as a corpus of discourse-level arguments as well as annotations of explicit and implicit relations. The PDTB relation scheme consists of 4 main relation classes and 16 sub-classes. Since it is released together with the corpus, a great deal of research has focused on the methodologies for multi-class relation classification. Motivated by Marcu and Echihabi (2002)’s work, in the earlier study, both feature engineering and sophisticated machine learning dominated the field in a large region (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, it becomes increasingly popular to use neural networks for learning to classify discourse relations (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). Typically, Zhang et al (2015) propose a Shallow Convolutional Neural Network (SCNN) model, which is used by Ponti and Korhonen (2017) to identify contingent relation. Qin et al (2016) utilize CNN with a collaborative gated neural network (CGNN) for recognizing implicit relations. Chen et al (2016) develop a Gated Relevance Network (GRN) based on Bidirectional Long-Short Term Memory (Bi-LSTM"
C18-1015,N15-1081,0,0.0171389,"a Shallow Convolutional Neural Network (SCNN) model, which is used by Ponti and Korhonen (2017) to identify contingent relation. Qin et al (2016) utilize CNN with a collaborative gated neural network (CGNN) for recognizing implicit relations. Chen et al (2016) develop a Gated Relevance Network (GRN) based on Bidirectional Long-Short Term Memory (Bi-LSTM) framework, combining bilinear model and single layer network for relevance measurement between arguments. Besides, the current work improves the classification performance of implicit relations by expanding the training data with connectives (Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017) or combining multiple corpora via multi-task learning (Liu et al., 2016). 3 Relation Inference Engine In our statistical inference process, TERB is an indispensable knowledge base and needed to be build first. But in this section, we suppose that TERB has been established successfully, and focus on presenting the inference approach. TERB building will be treated as a separate work and presented in section 6. 3.1 Framework and Terminology The input of the relation inference engine is a pair of query events, while the output is a relation type tag. The i"
C18-1015,S10-1062,0,0.023974,"e machine learning approaches. In the past decade, the SemEval (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009) has promoted a great deal of experimental study, including on the grammatical, syntactic, semantic and ordering features (Bethard and Martin, 2007; Hepple et al., 2007; Pus¸cas¸u, 2007; Bethard, 2013; Mirza and Minard, 2015; Caselli et al., 2015; Hashimoto et al., 2015). Meanwhile, temporal relation modeling has been implemented in different ways, such as sequence labeling, Markov logic networks and hybrid systems (Cheng et al., 2007; Min et al., 2007; Yoshikawa et al., 2009; UzZaman and Allen, 2010; Llorens et al., 2010; Velupillai et al., 2015). 2.2 Multi-class Discourse Relation Classification In April 2006 (Prasad et al., 2007), the Penn Discourse Tree Bank (PDTB) was released as a corpus of discourse-level arguments as well as annotations of explicit and implicit relations. The PDTB relation scheme consists of 4 main relation classes and 16 sub-classes. Since it is released together with the corpus, a great deal of research has focused on the methodologies for multi-class relation classification. Motivated by Marcu and Echihabi (2002)’s work, in the earlier study, both feature engin"
C18-1015,S15-2137,0,0.0718043,"Missing"
C18-1015,S07-1014,0,0.030405,"Missing"
C18-1015,P17-2042,0,0.0539171,"el, which is used by Ponti and Korhonen (2017) to identify contingent relation. Qin et al (2016) utilize CNN with a collaborative gated neural network (CGNN) for recognizing implicit relations. Chen et al (2016) develop a Gated Relevance Network (GRN) based on Bidirectional Long-Short Term Memory (Bi-LSTM) framework, combining bilinear model and single layer network for relevance measurement between arguments. Besides, the current work improves the classification performance of implicit relations by expanding the training data with connectives (Rutherford and Xue, 2015; Braud and Denis, 2016; Wu et al., 2017) or combining multiple corpora via multi-task learning (Liu et al., 2016). 3 Relation Inference Engine In our statistical inference process, TERB is an indispensable knowledge base and needed to be build first. But in this section, we suppose that TERB has been established successfully, and focus on presenting the inference approach. TERB building will be treated as a separate work and presented in section 6. 3.1 Framework and Terminology The input of the relation inference engine is a pair of query events, while the output is a relation type tag. The inference engine is constituted of three c"
C18-1015,P09-1046,0,0.027819,"arsing. Both focus on the machine learning approaches. In the past decade, the SemEval (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009) has promoted a great deal of experimental study, including on the grammatical, syntactic, semantic and ordering features (Bethard and Martin, 2007; Hepple et al., 2007; Pus¸cas¸u, 2007; Bethard, 2013; Mirza and Minard, 2015; Caselli et al., 2015; Hashimoto et al., 2015). Meanwhile, temporal relation modeling has been implemented in different ways, such as sequence labeling, Markov logic networks and hybrid systems (Cheng et al., 2007; Min et al., 2007; Yoshikawa et al., 2009; UzZaman and Allen, 2010; Llorens et al., 2010; Velupillai et al., 2015). 2.2 Multi-class Discourse Relation Classification In April 2006 (Prasad et al., 2007), the Penn Discourse Tree Bank (PDTB) was released as a corpus of discourse-level arguments as well as annotations of explicit and implicit relations. The PDTB relation scheme consists of 4 main relation classes and 16 sub-classes. Since it is released together with the corpus, a great deal of research has focused on the methodologies for multi-class relation classification. Motivated by Marcu and Echihabi (2002)’s work, in the earlier"
C18-1015,D15-1266,0,0.014538,"heme consists of 4 main relation classes and 16 sub-classes. Since it is released together with the corpus, a great deal of research has focused on the methodologies for multi-class relation classification. Motivated by Marcu and Echihabi (2002)’s work, in the earlier study, both feature engineering and sophisticated machine learning dominated the field in a large region (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, it becomes increasingly popular to use neural networks for learning to classify discourse relations (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). Typically, Zhang et al (2015) propose a Shallow Convolutional Neural Network (SCNN) model, which is used by Ponti and Korhonen (2017) to identify contingent relation. Qin et al (2016) utilize CNN with a collaborative gated neural network (CGNN) for recognizing implicit relations. Chen et al (2016) develop a Gated Relevance Network (GRN) based on Bidirectional Long-Short Term Memory (Bi-LSTM) framework, combining bilinear model and single layer network for relevance measurement between arguments. Besides, the current wo"
C18-1015,P16-2034,0,0.0280655,"evel embedding (Sen2vec). We calcualte the cosine similarity of the Sen2vecs and use it as the caption-mention approximation. CNN is utilized for generating the Sen2vecs. As usual, it produces the convolutional features on the concatenated word embeddings of the words in the input short text. There is only 1 hidden layer included in the network. Thus it fails to possess very deep-level perceptions of semantics. Undoubtedly, it can be enhanced by either adding more hidden layers to the network, or instead, using other state-of-the-art network models, such as attention based bidirectional LSTM (Zhou et al., 2016) or gated recurrent unit (Vadapalli and Gangashetty, 2016). In this paper, we choose to use a relatively simple model because we are more willing to verify the validity of the methodological framework. The configuration of the employed CNN is presented as below: in the input layer, a short text (caption or mention) is represented as a fixed-size sequence of real numbers, involving 30 256-dimensional word embeddings. Zero padding is performed when the text length is smaller than 40, otherwise tail clipping. We follow Mikolov et al. (2013) to use skip-gram based word2vec to compute embeddings, a"
C18-1037,P15-1061,0,0.166912,"sets in English, such as the SemEval’10 Task-8 dataset (Hendrickx et al., 2010) and the SemEval’18 task-7 dataset (G´abor et al., 2018), annotated datasets in other languages are few. Traditional studies for relation classification usually perform supervised machine learning models trained on mono-lingual labeled datasets, which either rely on a set of linguistic or semantic features (Kambhatla, 2004; Suchanek et al., 2006), or apply tree kernel-based features to represent the input sentences (Bunescu and Mooney, 2005; Qian et al., 2008). Recently, deep neural networks (Zeng et al., 2015; dos Santos et al., 2015) and attention mechanism (Wang et al., 2016) show the effectiveness in relation classification. However, the training of neural network relies on large-scale labeled instances. This makes it difficult to re-construct such classification models for the poorly-labeled language. Most of existing studies have attempted to leverage parallel data or a knowledge-based system to transfer effective information from the richly-labeled language to the poorly-labeled language. Qian et al. (2014) proposed a bilingual active learning paradigm for Chinese and English relation classification with pseudo paral"
C18-1037,N15-1151,0,0.172756,"ed instances. This makes it difficult to re-construct such classification models for the poorly-labeled language. Most of existing studies have attempted to leverage parallel data or a knowledge-based system to transfer effective information from the richly-labeled language to the poorly-labeled language. Qian et al. (2014) proposed a bilingual active learning paradigm for Chinese and English relation classification with pseudo parallel corpora and entity alignment. Kim et al., (2014) proposed a cross-lingual annotation projection strategy by employing parallel corpora for relation detection. Faruqui and Kumar (2015) also present a cross-lingual annotation projection method by using machine translation results, rather than parallel data. Verga et al. (2016) performs multi-lingual relation classification by a knowledge base. Min et al. (2017) drive a classifier to learn discriminative representations by joint supervision of classification (softmax) loss and ideal representation loss. Instead of exploiting external resources and manually selecting a closeness metric, we come up with an adversarial mechanism to provide an adaptive metric for feature adaptation from the richly-labeled language to the poorly-l"
C18-1037,S10-1006,0,0.0536,"the related work. In Section 3, we introduce details of the proposed adversarial feature adaptation approach. We show our experimental results and discussions in Section 4. Finally, we conclude the paper in Section 5. 2 Related Work In this section, we briefly review the recent progress in cross-lingual relation classification and existing studies on adversarial adaptation. 2.1 Cross-lingual Relation Classification Labeled data on relation classification are not evenly distributed among languages. While there are various of annotated datasets in English, such as the SemEval’10 Task-8 dataset (Hendrickx et al., 2010) and the SemEval’18 task-7 dataset (G´abor et al., 2018), annotated datasets in other languages are few. Traditional studies for relation classification usually perform supervised machine learning models trained on mono-lingual labeled datasets, which either rely on a set of linguistic or semantic features (Kambhatla, 2004; Suchanek et al., 2006), or apply tree kernel-based features to represent the input sentences (Bunescu and Mooney, 2005; Qian et al., 2008). Recently, deep neural networks (Zeng et al., 2015; dos Santos et al., 2015) and attention mechanism (Wang et al., 2016) show the effec"
C18-1037,I17-1068,0,0.298273,"ation from the richly-labeled language to the poorly-labeled language. Qian et al. (2014) proposed a bilingual active learning paradigm for Chinese and English relation classification with pseudo parallel corpora and entity alignment. Kim et al., (2014) proposed a cross-lingual annotation projection strategy by employing parallel corpora for relation detection. Faruqui and Kumar (2015) also present a cross-lingual annotation projection method by using machine translation results, rather than parallel data. Verga et al. (2016) performs multi-lingual relation classification by a knowledge base. Min et al. (2017) drive a classifier to learn discriminative representations by joint supervision of classification (softmax) loss and ideal representation loss. Instead of exploiting external resources and manually selecting a closeness metric, we come up with an adversarial mechanism to provide an adaptive metric for feature adaptation from the richly-labeled language to the poorly-labeled language. 2.2 Adversarial Adaptation Recently, the generative adversarial networks (GAN) have become increasingly popular, especially in the area of deep generative unsupervised modeling (Goodfellow et al., 2014; Makhzani"
C18-1037,P14-1055,1,0.801624,"ntences (Bunescu and Mooney, 2005; Qian et al., 2008). Recently, deep neural networks (Zeng et al., 2015; dos Santos et al., 2015) and attention mechanism (Wang et al., 2016) show the effectiveness in relation classification. However, the training of neural network relies on large-scale labeled instances. This makes it difficult to re-construct such classification models for the poorly-labeled language. Most of existing studies have attempted to leverage parallel data or a knowledge-based system to transfer effective information from the richly-labeled language to the poorly-labeled language. Qian et al. (2014) proposed a bilingual active learning paradigm for Chinese and English relation classification with pseudo parallel corpora and entity alignment. Kim et al., (2014) proposed a cross-lingual annotation projection strategy by employing parallel corpora for relation detection. Faruqui and Kumar (2015) also present a cross-lingual annotation projection method by using machine translation results, rather than parallel data. Verga et al. (2016) performs multi-lingual relation classification by a knowledge base. Min et al. (2017) drive a classifier to learn discriminative representations by joint sup"
C18-1037,C08-1088,1,0.736058,"enly distributed among languages. While there are various of annotated datasets in English, such as the SemEval’10 Task-8 dataset (Hendrickx et al., 2010) and the SemEval’18 task-7 dataset (G´abor et al., 2018), annotated datasets in other languages are few. Traditional studies for relation classification usually perform supervised machine learning models trained on mono-lingual labeled datasets, which either rely on a set of linguistic or semantic features (Kambhatla, 2004; Suchanek et al., 2006), or apply tree kernel-based features to represent the input sentences (Bunescu and Mooney, 2005; Qian et al., 2008). Recently, deep neural networks (Zeng et al., 2015; dos Santos et al., 2015) and attention mechanism (Wang et al., 2016) show the effectiveness in relation classification. However, the training of neural network relies on large-scale labeled instances. This makes it difficult to re-construct such classification models for the poorly-labeled language. Most of existing studies have attempted to leverage parallel data or a knowledge-based system to transfer effective information from the richly-labeled language to the poorly-labeled language. Qian et al. (2014) proposed a bilingual active learni"
C18-1037,P17-1093,0,0.0590337,"target domain. Different from their study, our approach aims to find sharable languageindependent latent feature representations for cross-lingual relation classification. There have been some previous work applying adversarial adaptation technique to NLP tasks, such as that for sentiment analysis (Chen et al., 2016) and parsing (Sato et al., 2017). These studies learn the domain-invariant or domain-specific features by a shared network. Our work differs from them, since we force a second network with identical structure to learn the latent feature representations from the supervised network. Qin et al. (2017) propose a feature imitation approach. An adversarial mechanism is used between explicit and implicit discourse relation samples. Different from their study, we migrates the feature representations from one language to another (non-parallel). To the best of our knowledge, this is the first work to employ the adversarial feature adaptation for cross-lingual relation classification. 3 Adversarial Feature Adaptation for Cross-lingual Relation Classification The common semantic information between different language motivates our adversarial feature adaptation approach. In this section, let us tak"
C18-1037,K17-3007,0,0.0427001,"arial neural networks (DANN) to 438 Figure 1: Architecture of the adversarial feature adaptation framework for cross-lingual relation classification. learn discriminative but domain-invariant representations, transferring the information from the source domain to the target domain. Different from their study, our approach aims to find sharable languageindependent latent feature representations for cross-lingual relation classification. There have been some previous work applying adversarial adaptation technique to NLP tasks, such as that for sentiment analysis (Chen et al., 2016) and parsing (Sato et al., 2017). These studies learn the domain-invariant or domain-specific features by a shared network. Our work differs from them, since we force a second network with identical structure to learn the latent feature representations from the supervised network. Qin et al. (2017) propose a feature imitation approach. An adversarial mechanism is used between explicit and implicit discourse relation samples. Different from their study, we migrates the feature representations from one language to another (non-parallel). To the best of our knowledge, this is the first work to employ the adversarial feature ada"
C18-1037,P15-2093,0,0.178553,"e other networks, e.g., a long short-term memory network (LSTM)1 . Embedding Layer Following Zeng et al. (2014)’s work, we build an embedding layer to encode words, word positions, and entity types by real-valued vectors. Given an input sentence S = (w1 , w2 , ..., wn ), we first transform every word into a real-valued vector of dimension dw using a word embedding matrix WE ∈ Rdw ×|V |, where V is the input vocabulary. Since the structures of RSE and PSE are the same, it is necessary if the word representations for both languages have a shared vocabulary. Therefore, bilingual word embeddings (Shi et al., 2015) are employed to map words from different languages into the same feature space. To capture the informative features of the relationship between words and the entity mentions, we map the relative distances to entity mentions of each word to two real-valued vectors of dimension dp using a position embedding matrix PE ∈ Rdp ×|D |, where D is the set of relative distances which are mapped to a vector initialized randomly (dos Santos et al., 2015). For each word, we obtain two position vectors with respect to the two entity mentions. For each word, we also incorporate its entity type embedding to"
C18-1037,N16-1103,0,0.0320582,"mpted to leverage parallel data or a knowledge-based system to transfer effective information from the richly-labeled language to the poorly-labeled language. Qian et al. (2014) proposed a bilingual active learning paradigm for Chinese and English relation classification with pseudo parallel corpora and entity alignment. Kim et al., (2014) proposed a cross-lingual annotation projection strategy by employing parallel corpora for relation detection. Faruqui and Kumar (2015) also present a cross-lingual annotation projection method by using machine translation results, rather than parallel data. Verga et al. (2016) performs multi-lingual relation classification by a knowledge base. Min et al. (2017) drive a classifier to learn discriminative representations by joint supervision of classification (softmax) loss and ideal representation loss. Instead of exploiting external resources and manually selecting a closeness metric, we come up with an adversarial mechanism to provide an adaptive metric for feature adaptation from the richly-labeled language to the poorly-labeled language. 2.2 Adversarial Adaptation Recently, the generative adversarial networks (GAN) have become increasingly popular, especially in"
C18-1037,P16-1123,0,0.0338378,"Missing"
C18-1037,D15-1203,0,0.0418535,"rious of annotated datasets in English, such as the SemEval’10 Task-8 dataset (Hendrickx et al., 2010) and the SemEval’18 task-7 dataset (G´abor et al., 2018), annotated datasets in other languages are few. Traditional studies for relation classification usually perform supervised machine learning models trained on mono-lingual labeled datasets, which either rely on a set of linguistic or semantic features (Kambhatla, 2004; Suchanek et al., 2006), or apply tree kernel-based features to represent the input sentences (Bunescu and Mooney, 2005; Qian et al., 2008). Recently, deep neural networks (Zeng et al., 2015; dos Santos et al., 2015) and attention mechanism (Wang et al., 2016) show the effectiveness in relation classification. However, the training of neural network relies on large-scale labeled instances. This makes it difficult to re-construct such classification models for the poorly-labeled language. Most of existing studies have attempted to leverage parallel data or a knowledge-based system to transfer effective information from the richly-labeled language to the poorly-labeled language. Qian et al. (2014) proposed a bilingual active learning paradigm for Chinese and English relation classi"
C18-1037,C14-1220,0,0.0943683,"representations extracted by the two sentence encoders (Hp or Hr ). In test step, we utilize the PSE to encode the input sentences in target language, and apply the same classifier to predict to relation labels. 439 3.1 Components As a common neural network model that yields good performance for monolingual relation classification, we employ CNN to transform a sentence with pairs of entity mentions into a distributed representation H. Note that, this plug-in architecture can also be implemented with the other networks, e.g., a long short-term memory network (LSTM)1 . Embedding Layer Following Zeng et al. (2014)’s work, we build an embedding layer to encode words, word positions, and entity types by real-valued vectors. Given an input sentence S = (w1 , w2 , ..., wn ), we first transform every word into a real-valued vector of dimension dw using a word embedding matrix WE ∈ Rdw ×|V |, where V is the input vocabulary. Since the structures of RSE and PSE are the same, it is necessary if the word representations for both languages have a shared vocabulary. Therefore, bilingual word embeddings (Shi et al., 2015) are employed to map words from different languages into the same feature space. To capture th"
C18-1037,Y15-1009,0,0.0151127,"urce, all the instances in the Chinese test set (the “Target-test” Column in Table 1) are directly translated into English (source language) by Google Translator. A CNN model with the same structure of RSE is trained by leveraging the English training set. • BiLSTM-MT-Source and BiLSTM-MT-Target They are similar to the settings of the CNN-MTSource and the CNN-MT-Target, respectively. For further examining the robustness of our adversarial feature adaptation framework, we replace the sentence encoder networks (CNNs) with the BiLSTM networks for both RSE and PSE. The BiLSTM model is proposed by Zhang et al. (2015), which is one of the state-of-the-art mono-lingual relation classification system. For fair comparison, we only retain the word embeddings, the position embeddings, and the entity type embeddings. • BI-AL This model is proposed by Qian et al. (2014), which is a bilingual active learning system for Chinese and English relation classification with pseudo parallel corpora and entity alignment. 4.2 Experimental Results Adversarial Feature Adaptation Table 3 shows a performance comparison of our adversarial feature adaptation models (*-GAN) with baselines. With the CNN-GAN system, we achieve a mic"
C18-1044,P16-1163,0,0.0219228,"ations of two discourse units can be used as an effective feature to determine their nuclearity. We also calculated the cosine similarities ???(?1 , ?? ) and ???(?2 , ?? ) between the discourse unit and the paragraph to measure the similarity between the discourse unit and the topic of the paragraph. These two similarities are helpful for identifying the mononuclear relations. Bilinear is a simple way to incorporate the linear interactions between two vectors and is defined as follows: ?(?1 , ?2 ) = ?1⊤ ??2 , (4) ?×? where ? ∈ ℝ is the parameter matrix. Usually, when using the Bilinear model (Chen et al., 2016; Wan et al., 2016; Wu et al., 2017), the Bilinear value ?(??? , ??? ) = ???? ???? is calculated for any two words in the two word sequences {?1 , ?2 , … , ?? } and {?1 , ?2 , … , ?? } to obtain a matching matrix, where ??? , ??? are semantic vectors that correspond to word ?? and ?? . A discourse unit or a paragraph could contain a larger number of words, and it will lead to generating an enormous matching matrix. However, the number of training samples that can be used in our model is relatively small, which results in great difficulty with training the parameter ?. Therefore, we simplified"
C18-1044,P14-1048,0,0.403779,"e., a in Figure 1 (中国机电产品进出口贸易继续增加 The import and export trade of China’s mechanical and electronic products continues to increase), which can be used to represent a summary of this paragraph. Although there are many studies on discourse parsing due to its vital role in NLP, only a few address nuclearity recognition. Among them only three studies (Li et al., 2015; Chu et al., 2015; Kong and Zhou, 2017) explore nuclearity recognition in Chinese due to the lack of annotated corpus and the abstract nature of Chinese itself. In addition, those studies heavily relied on manual feature engineering (Feng and Hirst, 2014; Heilman and Sagae, 2015; Wang et al., 2017). Only a few studies (Li et al., 2014; Li et al., 2016) used deep neural networks to explore automatic representation learning. One of the disadvantages of previous studies is that they lack deep semantic information extracted from discourse units due to the ineffectiveness of classifier-based models and simple neural network models. Even worse, different from those hypotactic languages such as English, Chinese is a paratactic (discourse-driven and pro-drop) language with a wide spread of ellipsis and open flexible sentence structures. Therefore, th"
C18-1044,C14-1045,0,0.0562207,"Missing"
C18-1044,P14-1002,0,0.0240827,"Section 3 gives the details of our model TMN, Section 4 reports the experimental results and Section 5 gives the conclusions. 2 Related Work Previous studies on nuclearity recognition mainly focused on English, with RST Discourse Treebank (RST-DT) (Carlson et al., 2003) being the most popular corpus. However, most of them only regard 526 nuclearity recognition as a trivial component of overall discourse parsing, and they ignore its specific characteristics and critical importance. The algorithms of nuclearity recognition published on RST-DT can mainly be categorised as shiftreduce algorithms (Ji and Eisenstein, 2014; Heilman and Sagae, 2015; Wang et al., 2017), probabilistic CKY-like algorithms (Joty et al., 2013; Li et al., 2014; Li et al., 2016) and greedy bottom-up algorithms (Feng and Hirst, 2014). Li et al. (2016) applied different classifiers to three discourse parsing subtasks separately, but they share the high level representation of discourse units by the same network structures. Wang et al. (2017) used a transition-based system to build discourse trees with nuclearity labels and then used Support Vector Machines (SVMs) to determine the discourse relations at different text levels. Most of the"
C18-1044,P13-1048,0,0.0201529,"ves the conclusions. 2 Related Work Previous studies on nuclearity recognition mainly focused on English, with RST Discourse Treebank (RST-DT) (Carlson et al., 2003) being the most popular corpus. However, most of them only regard 526 nuclearity recognition as a trivial component of overall discourse parsing, and they ignore its specific characteristics and critical importance. The algorithms of nuclearity recognition published on RST-DT can mainly be categorised as shiftreduce algorithms (Ji and Eisenstein, 2014; Heilman and Sagae, 2015; Wang et al., 2017), probabilistic CKY-like algorithms (Joty et al., 2013; Li et al., 2014; Li et al., 2016) and greedy bottom-up algorithms (Feng and Hirst, 2014). Li et al. (2016) applied different classifiers to three discourse parsing subtasks separately, but they share the high level representation of discourse units by the same network structures. Wang et al. (2017) used a transition-based system to build discourse trees with nuclearity labels and then used Support Vector Machines (SVMs) to determine the discourse relations at different text levels. Most of the previous studies used SVMs and variants of Conditional Random Fields (CRFs); only Li et al. (2014)"
C18-1044,D14-1220,0,0.258813,"d electronic products continues to increase), which can be used to represent a summary of this paragraph. Although there are many studies on discourse parsing due to its vital role in NLP, only a few address nuclearity recognition. Among them only three studies (Li et al., 2015; Chu et al., 2015; Kong and Zhou, 2017) explore nuclearity recognition in Chinese due to the lack of annotated corpus and the abstract nature of Chinese itself. In addition, those studies heavily relied on manual feature engineering (Feng and Hirst, 2014; Heilman and Sagae, 2015; Wang et al., 2017). Only a few studies (Li et al., 2014; Li et al., 2016) used deep neural networks to explore automatic representation learning. One of the disadvantages of previous studies is that they lack deep semantic information extracted from discourse units due to the ineffectiveness of classifier-based models and simple neural network models. Even worse, different from those hypotactic languages such as English, Chinese is a paratactic (discourse-driven and pro-drop) language with a wide spread of ellipsis and open flexible sentence structures. Therefore, the shallow semantic features (syntactic features), which are widely used in English"
C18-1044,D16-1035,0,0.142759,"ucts continues to increase), which can be used to represent a summary of this paragraph. Although there are many studies on discourse parsing due to its vital role in NLP, only a few address nuclearity recognition. Among them only three studies (Li et al., 2015; Chu et al., 2015; Kong and Zhou, 2017) explore nuclearity recognition in Chinese due to the lack of annotated corpus and the abstract nature of Chinese itself. In addition, those studies heavily relied on manual feature engineering (Feng and Hirst, 2014; Heilman and Sagae, 2015; Wang et al., 2017). Only a few studies (Li et al., 2014; Li et al., 2016) used deep neural networks to explore automatic representation learning. One of the disadvantages of previous studies is that they lack deep semantic information extracted from discourse units due to the ineffectiveness of classifier-based models and simple neural network models. Even worse, different from those hypotactic languages such as English, Chinese is a paratactic (discourse-driven and pro-drop) language with a wide spread of ellipsis and open flexible sentence structures. Therefore, the shallow semantic features (syntactic features), which are widely used in English, might not be eff"
C18-1044,D14-1224,1,0.66149,"d electronic products continues to increase), which can be used to represent a summary of this paragraph. Although there are many studies on discourse parsing due to its vital role in NLP, only a few address nuclearity recognition. Among them only three studies (Li et al., 2015; Chu et al., 2015; Kong and Zhou, 2017) explore nuclearity recognition in Chinese due to the lack of annotated corpus and the abstract nature of Chinese itself. In addition, those studies heavily relied on manual feature engineering (Feng and Hirst, 2014; Heilman and Sagae, 2015; Wang et al., 2017). Only a few studies (Li et al., 2014; Li et al., 2016) used deep neural networks to explore automatic representation learning. One of the disadvantages of previous studies is that they lack deep semantic information extracted from discourse units due to the ineffectiveness of classifier-based models and simple neural network models. Even worse, different from those hypotactic languages such as English, Chinese is a paratactic (discourse-driven and pro-drop) language with a wide spread of ellipsis and open flexible sentence structures. Therefore, the shallow semantic features (syntactic features), which are widely used in English"
C18-1044,W10-4327,0,0.0325169,"ion about the nucleus (Stede, 2008), while a multinuclear relation holds two or more discourse units, which are all nuclei. Therefore, three types of nuclearity exist: Nucleus-Satellite if the left subtree is the nucleus and the right subtree is the satellite, Satellite-Nucleus if the order of the satellite and nucleus is inverted, and Nucleus-Nucleus for multinuclear relations. Nuclearity recognition is helpful in detecting discourse relations (Iruskieta et al., 2014) and extracting the main content of a document, and it is widely used in various NLP tasks, including automatic summarisation (Louis et al., 2010; Marcu, 2000), question answering (Verberne et al., 2007) and information extraction (Zou et al., 2014). Consider the following document as an example: Example 1: 中国机电产品进出口贸易继续增加 a，占总进出口的比重继续上升 b。其中，出口 五十七点九亿美元 c，占总出口的百分之三十二点五 d；进口八十五点二亿美元 e，占总进口的 百分之四十六点四 f，均比去年同期有所上升 g。The import and export trade of China’s mechanical and electronic products continues to increase a, and its proportion of the total imports and exports also continues to rise b. Among them, the exports amounted to 5.79 billion dollars c, accounting for 32.5 percent of the total exports d; and the imports of 8.52 billion dollar"
C18-1044,prasad-etal-2008-penn,0,0.0632604,"Missing"
C18-1044,P16-1044,0,0.033033,"a mononuclear relation, the nucleus unit is usually semantically closer to the topic of the paragraph. Therefore, our TMN model makes the semantic match between not only the different discourse units but also the discourse unit and paragraph, by three similarity metrics, namely, the Cosine, Bilinear and Single Layer Network, which can capture the features that are related to nuclearity recognition adequately. 3.1 Text Encoding Our Text Encoding module combines Bi-LSTM and CNN to encode the discourse unit ??? and the paragraph ????, which is the modification of the Convolutional-pooling LSTM (Tan et al., 2016) in question answering. Its input is a sequence of words (?1 , ?2 , … , ?? ) in a discourse unit ??? or a paragraph ???? where ? is the number of words in the discourse unit or paragraph. Each word ?? in the sequence is represented as the combination of its word embedding ?? and POS (Part-Of-Speech) tag embedding ?? as follows: (1) ?? = [?? , ?? ]. LSTM models successfully keep the useful information from long-range dependency, but they focus more on the words that are behind. Due to the need for our model to treat each word equally, Bi-LSTM is introduced to the Text Encoding module. At each p"
C18-1044,P17-2029,0,0.0557675,"nd export trade of China’s mechanical and electronic products continues to increase), which can be used to represent a summary of this paragraph. Although there are many studies on discourse parsing due to its vital role in NLP, only a few address nuclearity recognition. Among them only three studies (Li et al., 2015; Chu et al., 2015; Kong and Zhou, 2017) explore nuclearity recognition in Chinese due to the lack of annotated corpus and the abstract nature of Chinese itself. In addition, those studies heavily relied on manual feature engineering (Feng and Hirst, 2014; Heilman and Sagae, 2015; Wang et al., 2017). Only a few studies (Li et al., 2014; Li et al., 2016) used deep neural networks to explore automatic representation learning. One of the disadvantages of previous studies is that they lack deep semantic information extracted from discourse units due to the ineffectiveness of classifier-based models and simple neural network models. Even worse, different from those hypotactic languages such as English, Chinese is a paratactic (discourse-driven and pro-drop) language with a wide spread of ellipsis and open flexible sentence structures. Therefore, the shallow semantic features (syntactic featur"
C18-1044,P17-1046,0,0.0313515,"used as an effective feature to determine their nuclearity. We also calculated the cosine similarities ???(?1 , ?? ) and ???(?2 , ?? ) between the discourse unit and the paragraph to measure the similarity between the discourse unit and the topic of the paragraph. These two similarities are helpful for identifying the mononuclear relations. Bilinear is a simple way to incorporate the linear interactions between two vectors and is defined as follows: ?(?1 , ?2 ) = ?1⊤ ??2 , (4) ?×? where ? ∈ ℝ is the parameter matrix. Usually, when using the Bilinear model (Chen et al., 2016; Wan et al., 2016; Wu et al., 2017), the Bilinear value ?(??? , ??? ) = ???? ???? is calculated for any two words in the two word sequences {?1 , ?2 , … , ?? } and {?1 , ?2 , … , ?? } to obtain a matching matrix, where ??? , ??? are semantic vectors that correspond to word ?? and ?? . A discourse unit or a paragraph could contain a larger number of words, and it will lead to generating an enormous matching matrix. However, the number of training samples that can be used in our model is relatively small, which results in great difficulty with training the parameter ?. Therefore, we simplified this process to calculate the Biline"
C18-1044,P14-1049,1,0.82053,"ch are all nuclei. Therefore, three types of nuclearity exist: Nucleus-Satellite if the left subtree is the nucleus and the right subtree is the satellite, Satellite-Nucleus if the order of the satellite and nucleus is inverted, and Nucleus-Nucleus for multinuclear relations. Nuclearity recognition is helpful in detecting discourse relations (Iruskieta et al., 2014) and extracting the main content of a document, and it is widely used in various NLP tasks, including automatic summarisation (Louis et al., 2010; Marcu, 2000), question answering (Verberne et al., 2007) and information extraction (Zou et al., 2014). Consider the following document as an example: Example 1: 中国机电产品进出口贸易继续增加 a，占总进出口的比重继续上升 b。其中，出口 五十七点九亿美元 c，占总出口的百分之三十二点五 d；进口八十五点二亿美元 e，占总进口的 百分之四十六点四 f，均比去年同期有所上升 g。The import and export trade of China’s mechanical and electronic products continues to increase a, and its proportion of the total imports and exports also continues to rise b. Among them, the exports amounted to 5.79 billion dollars c, accounting for 32.5 percent of the total exports d; and the imports of 8.52 billion dollars e, accounting for 46.4 percent of the total imports f; all of them were higher than those in the same"
C18-1045,P12-1007,0,0.135105,"to binary relations. There are two shortcomings of these approach, on the one hand, the number of non-original samples is added, and on the other hand, it is difficult to automatically build a complete real structure tree. We build the local models of structure identification and nuclearity recognition respectively. Our local models are implemented using CRFs. In this way, we are able to take into account the sequential information from contextual discourse units, which cannot be naturally represented with Support Vector Machine (SVM) or Maximum Entropy (ME) as local classifiers. As shown by Feng and Hirst (2012; 2014), for a pair of discourse units of interest, the sequential information from contextual units is crucial for determining structures. Therefore, it is well motivated to use CRF, which is a discriminative probabilistic graphical model, to make predictions for a sequence of units surrounding the pair of interest. Figure 3 shows our structure identification model Mstruct implemented with conditional random field algorithm. The first layer of the chain is composed of discourse units Uj ’s, and the second layer is composed of nodes of Sj ’s to indicate the probability of merging adjacent disc"
C18-1045,P14-1048,0,0.429441,"classifier is applied to assign the type of discourse relation between the chosen pair of constituents. Joty et al. (2013) approach the document-level discourse parsing using a model trained by Conditional Random Fields (CRF). They decomposed the problem of document-level discourse parsing into two stage: intra-sentential and multi-sentential parsing. Specifically, they employed two separate models for intra- and multi-sentential parsing. They jointly modeled the structure and the relation for a given pair of discourse units, such that information from each aspect can interact with the other. Feng and Hirst (2014) develop a much faster model whose time complexity is linear in the number of sentences. Their model adopts a greedy bottom-up approach, with two linear-chain CRFs applied as local classifiers. An approach of post-editing is performed, which modified a fully-built tree by considering information from upper-levels, to improve the accuracy. There is no relevant research on document-level discourse parsing in Chinese so far. For micro level discourse structure analysis, Li (2015) proposes a Connective-driven Dependency Tree (CDT) schema to represent the discourse rhetorical structure in Chinese l"
C18-1045,P14-1065,0,0.0504555,"Missing"
C18-1045,D12-1083,0,0.113864,"work. 2 Related Work Discourse parsing is the task of discovering the presence and type of the discourse relations between discourse units. The existing discourse parsing researches are mainly based on Rhetorical Structure Theory Discourse Treebank (RST-DT). The RST-DT (Carlson et al., 2003) is built in the framework of Rhetorical Structure Theory, consisting of 385 Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993) and representing over 176,000 words of text. While recent advances in sentence-level discourse parsing have attained accuracies close to human performance (Joty et al., 2012), discourse parsing at the document-level still poses challenges. The HILDA discourse parser (Hernault et al., 2010) is the first attempt at document-level discourse parsing on RST-DT. It adopts a pipeline framework, and greedily builds the discourse tree from the bottom-up. In particular, at each step of the tree-building, a binary Support Vector Machine (SVM) classifier is applied to determine which pair of adjacent discourse constituents should be merged to form a larger span, and then another multi-class SVM classifier is applied to assign the type of discourse relation between the chosen"
C18-1045,P13-1048,0,0.506931,"the document-level still poses challenges. The HILDA discourse parser (Hernault et al., 2010) is the first attempt at document-level discourse parsing on RST-DT. It adopts a pipeline framework, and greedily builds the discourse tree from the bottom-up. In particular, at each step of the tree-building, a binary Support Vector Machine (SVM) classifier is applied to determine which pair of adjacent discourse constituents should be merged to form a larger span, and then another multi-class SVM classifier is applied to assign the type of discourse relation between the chosen pair of constituents. Joty et al. (2013) approach the document-level discourse parsing using a model trained by Conditional Random Fields (CRF). They decomposed the problem of document-level discourse parsing into two stage: intra-sentential and multi-sentential parsing. Specifically, they employed two separate models for intra- and multi-sentential parsing. They jointly modeled the structure and the relation for a given pair of discourse units, such that information from each aspect can interact with the other. Feng and Hirst (2014) develop a much faster model whose time complexity is linear in the number of sentences. Their model"
C18-1045,P10-1113,1,0.75867,"etween Uj+1 and the topic of the discourse. Whether the similarity between Uj and the topic is greater than the similarity between Uj+1 and the topic. Table 2: Features used in local models. 541 Used in SI Used in NR Y Y Y N Y Y N Y Y Y Y Y Y Y Y Y Y Y N Y 6 Joint Learning with Integer Linear Programing While a pipeline model may suffer from the errors propagated from upstream tasks, a joint model can benefit from the close interaction between two or more tasks. Recently, joint modeling has been widely attempted in various NLP tasks, such as joint syntactic parsing and semantic role labeling (Li et al., 2010), joint argument identification and role determination (Li et al., 2013), joint structure identification and relation recognition (Joty et al., 2012), etc. In our joint model, an ILP (Integer Logic Programming) -based inference framework is introduced to integrate two CRF-based local models, the structure identifier and the nuclearity recognizer. In this section, we propose a joint model of structure identification and nuclearity recognition with some intrainstance and contextual constraints. We assume pSI (s&lt;i,j&gt; |seqi ) the probability of Mstruct identifying s&lt;i,j&gt; as a structure of an seque"
C18-1045,D14-1224,1,0.832482,"formance has not yet achieve the level for application. However, the macro level research still stays in the theoretical research, and there is no available corpus resource, nor a corresponding computational model. For the two reasons mentioned above, in this paper, we take the macro discourse structure as the main research object, that is different from the previous research. We explore a macro discourse structure presentation schema to present the macro level discourse structure, and propose a Macro Chinese Discourse Treebank (MCDTB) on the top of existing Chinese Discourse Treebank (CDTB) (Li et al., 2014). On the basis of the presentation schema and annotated corpus, we divide the macro level discourse structure analysis into four tasks, including structure identification, nuclearity recognition, relation classification and discourse tree building. There are certain differences from the analysis of the micro level discourse structure. For example, macro discourse structure analysis takes paragraphs as the elementary discourse units, and the relations between the units are fairly loose, so it brings difficulties to the task of structure identification. Furthermore, there are virtually no connec"
C18-1045,J93-2004,0,0.0611808,"ibes the local models, and the joint approach we used with ILP is introduced in Section 6. Section 7 presents the experimental results. Section 8 gives the conclusion and future work. 2 Related Work Discourse parsing is the task of discovering the presence and type of the discourse relations between discourse units. The existing discourse parsing researches are mainly based on Rhetorical Structure Theory Discourse Treebank (RST-DT). The RST-DT (Carlson et al., 2003) is built in the framework of Rhetorical Structure Theory, consisting of 385 Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993) and representing over 176,000 words of text. While recent advances in sentence-level discourse parsing have attained accuracies close to human performance (Joty et al., 2012), discourse parsing at the document-level still poses challenges. The HILDA discourse parser (Hernault et al., 2010) is the first attempt at document-level discourse parsing on RST-DT. It adopts a pipeline framework, and greedily builds the discourse tree from the bottom-up. In particular, at each step of the tree-building, a binary Support Vector Machine (SVM) classifier is applied to determine which pair of adjacent dis"
C18-1045,C04-1007,0,0.743973,"For macro level discourse analysis, the particles of the N-gram model are too small to represent the information of a paragraph, so the lexical features are not used in our tasks. 2) Syntactic information and dominance set features are very useful for micro level discourse analysis. However, the elementary units of the macro level are paragraphs, and these features are not applicable. 3) Since there is no tense in Chinese, we cannot use temporal features in macro level structure analysis. Due to the appearance of word vector representation (Mikolov et al., 2013), the methods of cooccurrence (Sporleder and Lascarides, 2004) and word pairs (Feng and Hirst, 2012) are not necessary when the semantic similarity is calculated. We use the word2vec model to train word vector representation on the CTB 8.0, and use the method proposed by Jiang et al. (2018) to calculate the semantic similarity (including the semantic similarity between adjacent discourse units and the similarity between the discourse unit and the topic). In particular, in order to prevent the sparsity of the features, we discretize the semantic similarity into 10 levels. Features Organization features The beginning and end location of Uj . Distances of U"
C18-1045,C02-1145,0,0.074799,"relations. Specifically, the edges with arrows point to the “Nucleus” units, and the edges without arrows point to the “Satellite” units. 3.2 Corpus Annotating Guided by the macro discourse structure framework defined in Section 3.1, we have carried out annotating work of macro Chinese discourse structure, which we call Macro Chinese Discourse Treebank (MCDTB)1 . In the process of annotating, the structure definition and annotating criteria are modified iteratively. After nearly a year of annotation, 720 news wire articles are annotated, the source of which is Chinese Treebank 8.0 (CTB 8.0). (Xue et al., 2002; Xue et al., 2013) Because the discourse units are not isolated from the overall discourse, it’s difficult to judge whether the discourse units are important or not and what relations are between the discourse units simply from the units themselves. It is necessary to have a comprehensive understanding of the overall article before the annotating. We divide the annotating work into three main stages. The first stage lasted four months, with three annotators participating. We selected the first 50 news articles from CTB 8.0, and annotated them together. After a lot of discussions, a preliminar"
C18-1050,2013.mtsummit-papers.5,0,0.0180382,"opose a dynamic adaptive translation model using cache-based implementation for interactive machine translation, and develop a monolingual dynamic adaptive model and a bilingual dynamic adaptive model. Tiedemann (2010) propose a cache-based translation model, filling the cache with bilingual phrase pairs from the best translation hypotheses of previous sentences in a document. Gong et al. (2011) further propose a cache-based approach 597 to document-level translation, which includes three caches, a dynamic cache, a static cache and a topic cache, to capture various document-level information. Bertoldi et al. (2013) describe a cache mechanism to implement online learning in phrase-based SMT and use a repetition rate measure to predict the utility of cached items expected to be useful for the current translation. Our caches are similar to those used by Gong et al. (2011) who incorporate these caches into statistical machine translation. We adapt them to neural machine translation with a neural cache model. It is worthwhile to emphasize that such adaptation is nontrivial as shown below because the two translation philosophies and frameworks are significantly different. 3 Attention-based NMT In this section"
C18-1050,H92-1020,0,0.67311,"eline. 2 Related Work In the literature, several cache-based translation models have been proposed for conventional statistical machine translation, besides traditional n-gram language models and neural language models. In this section, we will first introduce related work in cache-based language models and then in translation models. For traditional n-gram language models, Kuhn and De Mori (1990) propose a cache-based language model, which mixes a large global language model with a small local model estimated from recent items in the history of the input stream for speech recongnition. Della Pietra et al. (1992) introduce a MaxEntbased cache model by integrating a cache into a smoothed trigram language model, reporting reduction in both perplexity and word error rates. Chueh and Chien (2010) present a new topic cache model for speech recongnition based on latent Dirichlet language model by incorporating a large-span topic cache into the generation of topic mixtures. For neural language models, Huang et al. (2014) propose a cache-based RNN inference scheme, which avoids repeated computation of identical LM calls and caches previously computed scores and useful intermediate results and thus reduce the"
C18-1050,D11-1084,1,0.926259,"nslated in a coherent way. In the literature, such informative constraints have been occasionally investigated in statistical machine translation and achieved certain success via a variety of document-level models, such as cache-based ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 596 Proceedings of the 27th International Conference on Computational Linguistics, pages 596–606 Santa Fe, New Mexico, USA, August 20-26, 2018. language and translation models (Tiedemann, 2010; Gong et al., 2011; Nepveu et al., 2004) for the consistency constraint, topic-based coherence model (Xiong and Zhang, 2013; Tam et al., 2007) for the coherence constraint. By contrast, in neural machine translation, to the best of our knowledge, such constraints have not been explored so far. Partially inspired by the success of cache models in SMT, we propose a cache-based approach to capturing coherence for neural machine translation. Particularly, we incorporate two types of caches into NMT: a static topic cache and a dynamic cache being updated on the fly. For the topic cache, we first use a projection-bas"
C18-1050,P15-1001,0,0.0288145,"o the final word prediction probabilities via a gating mechanism. Finally, the proposed cache-based neural model is trained jointly with NMT system in an end-to-end manner. Experiments and analysis presented in this paper demonstrate that the proposed cache-based model achieves substantial improvements over several state-of-the-art SMT and NMT baselines. 1 Introduction Neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) as an emerging machine translation approach, quickly achieves the state-of-the-art translation performance on many language pairs, e.g., English-French (Jean et al., 2015; Luong et al., 2015b), English-German (Shen et al., 2015; Luong et al., 2015a) and so on. In principle, NMT is established on an encoder-decoder framework, where the encoder reads a source sentence and encodes it into a fixed-length semantic vector, and the decoder generates a translation according to this vector. In spite of its current success, NMT translates sentences of a text independently, ignoring documentlevel information during translation. This largely limits its success since document-level information imposes constraints on the translations of individual sentences of a text. And s"
C18-1050,2005.iwslt-1.8,0,0.0349865,", and NIST02, NIST04, NIST06 (878, 1788, 1664 sentence pairs. respectively) as our test sets. We compared our proposed model against the following two systems: • Moses (Koehn et al., 2007): an off-the-shelf phrase-based translation system with its default setting. • RNNSearch*: our in-house attention-based NMT system which adopts the feedback attention as described in Section 3 . For Moses, we used the full training data to train the model. We ran GIZA++ (Och and Ney, 2000) on the training data in both directions, and merged alignments in two directions with “grow-diag-final” refinement rule (Koehn et al., 2005) to obtain final word alignments. We trained a 5-gram language model on the Xinhua portion of GIGA-WORD corpus using SRILM Toolkit with a modified KneserNey smoothing. For RNNSearch, we used the parallel corpus to train the attention-based NMT model. The encoder of RNNSearch consists of a forward and backward recurrent neural network. The word embedding dimension is 620 and the size of a hidden layer is 1000. The maximum length of sentences that we used to train RNNSearch in our experiments was set to 50 on both Chinese and English side. We used the most 601 Model Moses RNNSearch* + Cd + Cd, C"
C18-1050,P07-2045,0,0.00723363,"Setting We selected corpora LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and a portion of data from the corpus LDC2004T08 (Hong Kong Hansards/Laws/News) as our bilingual training data, where document boundaries are explicitly kept. In total, our training data contain 103,236 documents and 2.80M sentences. On average, each document consists of 28.4 sentences. We chose NIST05 dataset (1082 sentence pairs) as our development set, and NIST02, NIST04, NIST06 (878, 1788, 1664 sentence pairs. respectively) as our test sets. We compared our proposed model against the following two systems: • Moses (Koehn et al., 2007): an off-the-shelf phrase-based translation system with its default setting. • RNNSearch*: our in-house attention-based NMT system which adopts the feedback attention as described in Section 3 . For Moses, we used the full training data to train the model. We ran GIZA++ (Och and Ney, 2000) on the training data in both directions, and merged alignments in two directions with “grow-diag-final” refinement rule (Koehn et al., 2005) to obtain final word alignments. We trained a 5-gram language model on the Xinhua portion of GIGA-WORD corpus using SRILM Toolkit with a modified KneserNey smoothing. F"
C18-1050,D15-1166,0,0.0840203,"ediction probabilities via a gating mechanism. Finally, the proposed cache-based neural model is trained jointly with NMT system in an end-to-end manner. Experiments and analysis presented in this paper demonstrate that the proposed cache-based model achieves substantial improvements over several state-of-the-art SMT and NMT baselines. 1 Introduction Neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) as an emerging machine translation approach, quickly achieves the state-of-the-art translation performance on many language pairs, e.g., English-French (Jean et al., 2015; Luong et al., 2015b), English-German (Shen et al., 2015; Luong et al., 2015a) and so on. In principle, NMT is established on an encoder-decoder framework, where the encoder reads a source sentence and encodes it into a fixed-length semantic vector, and the decoder generates a translation according to this vector. In spite of its current success, NMT translates sentences of a text independently, ignoring documentlevel information during translation. This largely limits its success since document-level information imposes constraints on the translations of individual sentences of a text. And such document-level c"
C18-1050,P15-1002,0,0.0616669,"ediction probabilities via a gating mechanism. Finally, the proposed cache-based neural model is trained jointly with NMT system in an end-to-end manner. Experiments and analysis presented in this paper demonstrate that the proposed cache-based model achieves substantial improvements over several state-of-the-art SMT and NMT baselines. 1 Introduction Neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) as an emerging machine translation approach, quickly achieves the state-of-the-art translation performance on many language pairs, e.g., English-French (Jean et al., 2015; Luong et al., 2015b), English-German (Shen et al., 2015; Luong et al., 2015a) and so on. In principle, NMT is established on an encoder-decoder framework, where the encoder reads a source sentence and encodes it into a fixed-length semantic vector, and the decoder generates a translation according to this vector. In spite of its current success, NMT translates sentences of a text independently, ignoring documentlevel information during translation. This largely limits its success since document-level information imposes constraints on the translations of individual sentences of a text. And such document-level c"
C18-1050,W04-3225,0,0.393575,"nt way. In the literature, such informative constraints have been occasionally investigated in statistical machine translation and achieved certain success via a variety of document-level models, such as cache-based ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 596 Proceedings of the 27th International Conference on Computational Linguistics, pages 596–606 Santa Fe, New Mexico, USA, August 20-26, 2018. language and translation models (Tiedemann, 2010; Gong et al., 2011; Nepveu et al., 2004) for the consistency constraint, topic-based coherence model (Xiong and Zhang, 2013; Tam et al., 2007) for the coherence constraint. By contrast, in neural machine translation, to the best of our knowledge, such constraints have not been explored so far. Partially inspired by the success of cache models in SMT, we propose a cache-based approach to capturing coherence for neural machine translation. Particularly, we incorporate two types of caches into NMT: a static topic cache and a dynamic cache being updated on the fly. For the topic cache, we first use a projection-based bilingual topic lea"
C18-1050,P00-1056,0,0.187369,"2.80M sentences. On average, each document consists of 28.4 sentences. We chose NIST05 dataset (1082 sentence pairs) as our development set, and NIST02, NIST04, NIST06 (878, 1788, 1664 sentence pairs. respectively) as our test sets. We compared our proposed model against the following two systems: • Moses (Koehn et al., 2007): an off-the-shelf phrase-based translation system with its default setting. • RNNSearch*: our in-house attention-based NMT system which adopts the feedback attention as described in Section 3 . For Moses, we used the full training data to train the model. We ran GIZA++ (Och and Ney, 2000) on the training data in both directions, and merged alignments in two directions with “grow-diag-final” refinement rule (Koehn et al., 2005) to obtain final word alignments. We trained a 5-gram language model on the Xinhua portion of GIGA-WORD corpus using SRILM Toolkit with a modified KneserNey smoothing. For RNNSearch, we used the parallel corpus to train the attention-based NMT model. The encoder of RNNSearch consists of a forward and backward recurrent neural network. The word embedding dimension is 620 and the size of a hidden layer is 1000. The maximum length of sentences that we used t"
C18-1050,W10-2602,0,0.497666,"ual sentences translated in a coherent way. In the literature, such informative constraints have been occasionally investigated in statistical machine translation and achieved certain success via a variety of document-level models, such as cache-based ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 596 Proceedings of the 27th International Conference on Computational Linguistics, pages 596–606 Santa Fe, New Mexico, USA, August 20-26, 2018. language and translation models (Tiedemann, 2010; Gong et al., 2011; Nepveu et al., 2004) for the consistency constraint, topic-based coherence model (Xiong and Zhang, 2013; Tam et al., 2007) for the coherence constraint. By contrast, in neural machine translation, to the best of our knowledge, such constraints have not been explored so far. Partially inspired by the success of cache models in SMT, we propose a cache-based approach to capturing coherence for neural machine translation. Particularly, we incorporate two types of caches into NMT: a static topic cache and a dynamic cache being updated on the fly. For the topic cache, we first u"
C18-1050,P16-1125,0,0.0284115,"which stores recent hidden activations to be used as contextual representations. Our caches significantly differ from these two caches in that we store linguistic items in the cache rather than scores or activations. For neural machine translation, Wang et al. (2017) propose a cross-sentence context-aware approach and employ a hierarchy of Recurrent Neural Networks (RNNs) to summarize the cross-sentence context from source-side previous sentences. Jean et al. (2017) propose a novel larger-context neural machine translation model based on the recent works on larger-context language modelling (Wang and Cho, 2016) and employ the method to model the surrounding text in addition to the source sentence. For cache-based translation models, Nepveu et al. (2004) propose a dynamic adaptive translation model using cache-based implementation for interactive machine translation, and develop a monolingual dynamic adaptive model and a bilingual dynamic adaptive model. Tiedemann (2010) propose a cache-based translation model, filling the cache with bilingual phrase pairs from the best translation hypotheses of previous sentences in a document. Gong et al. (2011) further propose a cache-based approach 597 to documen"
C18-1050,D17-1301,0,0.0714475,"ral language models, Huang et al. (2014) propose a cache-based RNN inference scheme, which avoids repeated computation of identical LM calls and caches previously computed scores and useful intermediate results and thus reduce the computational expense of RNNLM. Grave et al. (2016) extend the neural network language model with a neural cache model, which stores recent hidden activations to be used as contextual representations. Our caches significantly differ from these two caches in that we store linguistic items in the cache rather than scores or activations. For neural machine translation, Wang et al. (2017) propose a cross-sentence context-aware approach and employ a hierarchy of Recurrent Neural Networks (RNNs) to summarize the cross-sentence context from source-side previous sentences. Jean et al. (2017) propose a novel larger-context neural machine translation model based on the recent works on larger-context language modelling (Wang and Cho, 2016) and employ the method to model the surrounding text in addition to the source sentence. For cache-based translation models, Nepveu et al. (2004) propose a dynamic adaptive translation model using cache-based implementation for interactive machine t"
C18-1119,P07-1056,0,0.0228053,"cting the user-pivotal word network is beneficial for learning a better user embedding for cross-media user profiling. 4.3 Performance Comparison For thorough comparison, several approaches are implemented for cross-media user profiling: • Baseline-SVM(BOW): a SVM classifier (or SVR regressor) trained with only labeled data in the source media and the representation model is BOW. This approach is proposed by Li et al. (2015). • Baseline-SCL(BOW): a famous textual domain adaptation approach named SCL which bridges the knowledge between the source and target domains using some pivotal features (Blitzer et al., 2007) and the representation model is BOW. We consider the source media as the source domain and the target media as the target domain. The number of the pivotal features is set to be 200. • Baseline-SDA(BOW): another famous textual domain adaptation approach with stacked denoising auto-encoder to extract meaningful representation (Glorot et al., 2011) and the representation model is BOW. Similar to SCL, we consider the source media as the source domain and the target media as the target domain. • Baseline-LSTM(Word Embeddings): a LSTM classification (or regression) model using Word Embeddings as i"
C18-1119,D13-1114,0,0.123047,"Wang et al., 2017; Li et al., 2016; Li et al., 2015) and age (Marquardt et al., 2014) identification. Recently, along with the boom of social media, user profiling has been drawing more and more attention in various social applications, such as personality analysis (Li et al., 2016; Sarawgi et al., 2011; O’Connor et al., 2010), intelligent marking (Preotiuc-Pietro et al., 2015) and online advertising (Zhang et al., 2016; Volkova et al., 2013). In the literature, conventional approaches normally recast user profiling as a supervised learning problem (Marquardt et al., 2014; Zhang et al., 2016; Ciot et al., 2013; Corney et al., 2002) by exploring various user generated textual features and user social connection features. However, the performance largely depends on a large amount of labeled data and the performance normally degrades dramatically when the model is tested on a different social media. Even worse, many real scenarios may involve several social media with some of them lacking sufficient labeled data to train a well-performed model in each social media. For example, Facebook.com is a social media site where many users publicize their age attribute in their homepages, making the collection"
C18-1119,W11-2606,0,0.0258669,"Missing"
C18-1119,C16-1197,1,0.748289,"ormation. Then, we learn user embedding by jointly learning the heterogeneous network composed of above two networks. Finally, we train a classification (or regression) model with the obtained user embeddings as input to perform user profiling. Empirical studies demonstrate the effectiveness of the proposed approach to two cross-media user profiling tasks, i.e., cross-media gender classification and cross-media age regression. 1 Introduction User profiling is a task which leverages user generated content (UGC) to automatically identify the data about a user, such as gender (Wang et al., 2017; Li et al., 2016; Li et al., 2015) and age (Marquardt et al., 2014) identification. Recently, along with the boom of social media, user profiling has been drawing more and more attention in various social applications, such as personality analysis (Li et al., 2016; Sarawgi et al., 2011; O’Connor et al., 2010), intelligent marking (Preotiuc-Pietro et al., 2015) and online advertising (Zhang et al., 2016; Volkova et al., 2013). In the literature, conventional approaches normally recast user profiling as a supervised learning problem (Marquardt et al., 2014; Zhang et al., 2016; Ciot et al., 2013; Corney et al.,"
C18-1119,D10-1021,0,0.0372759,"der identification and age identification) in several research communities, such as natural language processing and social network analysis. For the gender identification task, Mohammad and Yang (2013) show that there are marked differences across genders in how they use emotion words in work-place email. Ciot et al. (2013) conduct the first assessment of latent attribute inference in various languages beyond English, focusing on gender inference of Twitter users. Li et al. (2015) aim to identify the genders of two interactive users on the basis of micro-blog text. Some other studies, such as Mukherjee and Liu (2010), Peersman et al. (2011) and Gianfortoni et al. (2011) focus on exploring more effective features to improve the performance. Wang et al. (2017) propose a joint learning approach in order to leverage the relationship features among relevant user attributes. For the age identification task, most studies are devoted to explore efficient features in blog and social media. Schler et al. (2006) focus on textual features extracted from the blog text, such as word context features and POS stylistic features. Peersman et al. (2011) apply a text categorization approach to age classification with textua"
C18-1119,P15-1169,0,0.033224,"edia user profiling tasks, i.e., cross-media gender classification and cross-media age regression. 1 Introduction User profiling is a task which leverages user generated content (UGC) to automatically identify the data about a user, such as gender (Wang et al., 2017; Li et al., 2016; Li et al., 2015) and age (Marquardt et al., 2014) identification. Recently, along with the boom of social media, user profiling has been drawing more and more attention in various social applications, such as personality analysis (Li et al., 2016; Sarawgi et al., 2011; O’Connor et al., 2010), intelligent marking (Preotiuc-Pietro et al., 2015) and online advertising (Zhang et al., 2016; Volkova et al., 2013). In the literature, conventional approaches normally recast user profiling as a supervised learning problem (Marquardt et al., 2014; Zhang et al., 2016; Ciot et al., 2013; Corney et al., 2002) by exploring various user generated textual features and user social connection features. However, the performance largely depends on a large amount of labeled data and the performance normally degrades dramatically when the model is tested on a different social media. Even worse, many real scenarios may involve several social media with"
C18-1119,D14-1121,0,0.0138118,"man et al. (2011) apply a text categorization approach to age classification with textual features extracted from the text in social media. More recently, Marquardt et al. (2014) propose a multi-label classification approach to predict both the gender and age of authors from texts adopting some sentiment and emotion features. Differently, in this paper, we cast age identification task as a regression problem instead of a classification problem. Different from all above studies, we focus on the cross-media user profiling task. To the best of our knowledge, there are only two related papers by (Sap et al., 2014; Pardo et al., 2016) which mentions the cross-media user profiling issue. In their paper, only textual information is used in cross-media user profiling. Unlike their study, this paper firstly employs both textual and social information in cross-media user profiling. 3 3.1 Our Approach Framework Overview Our approach consists of two main phases, i.e., the representation learning phase and the classification (or regression) phase, which has been illustrated in Figure 4. In the representation phase, we first construct two different types of cross-media networks, i.e., crossmedia user-word netwo"
C18-1119,W11-0310,0,0.0238514,"monstrate the effectiveness of the proposed approach to two cross-media user profiling tasks, i.e., cross-media gender classification and cross-media age regression. 1 Introduction User profiling is a task which leverages user generated content (UGC) to automatically identify the data about a user, such as gender (Wang et al., 2017; Li et al., 2016; Li et al., 2015) and age (Marquardt et al., 2014) identification. Recently, along with the boom of social media, user profiling has been drawing more and more attention in various social applications, such as personality analysis (Li et al., 2016; Sarawgi et al., 2011; O’Connor et al., 2010), intelligent marking (Preotiuc-Pietro et al., 2015) and online advertising (Zhang et al., 2016; Volkova et al., 2013). In the literature, conventional approaches normally recast user profiling as a supervised learning problem (Marquardt et al., 2014; Zhang et al., 2016; Ciot et al., 2013; Corney et al., 2002) by exploring various user generated textual features and user social connection features. However, the performance largely depends on a large amount of labeled data and the performance normally degrades dramatically when the model is tested on a different social m"
C18-1119,D13-1187,0,0.140753,"ss-media age regression. 1 Introduction User profiling is a task which leverages user generated content (UGC) to automatically identify the data about a user, such as gender (Wang et al., 2017; Li et al., 2016; Li et al., 2015) and age (Marquardt et al., 2014) identification. Recently, along with the boom of social media, user profiling has been drawing more and more attention in various social applications, such as personality analysis (Li et al., 2016; Sarawgi et al., 2011; O’Connor et al., 2010), intelligent marking (Preotiuc-Pietro et al., 2015) and online advertising (Zhang et al., 2016; Volkova et al., 2013). In the literature, conventional approaches normally recast user profiling as a supervised learning problem (Marquardt et al., 2014; Zhang et al., 2016; Ciot et al., 2013; Corney et al., 2002) by exploring various user generated textual features and user social connection features. However, the performance largely depends on a large amount of labeled data and the performance normally degrades dramatically when the model is tested on a different social media. Even worse, many real scenarios may involve several social media with some of them lacking sufficient labeled data to train a well-perfo"
C18-1119,C16-1199,1,0.835775,"assification and cross-media age regression. 1 Introduction User profiling is a task which leverages user generated content (UGC) to automatically identify the data about a user, such as gender (Wang et al., 2017; Li et al., 2016; Li et al., 2015) and age (Marquardt et al., 2014) identification. Recently, along with the boom of social media, user profiling has been drawing more and more attention in various social applications, such as personality analysis (Li et al., 2016; Sarawgi et al., 2011; O’Connor et al., 2010), intelligent marking (Preotiuc-Pietro et al., 2015) and online advertising (Zhang et al., 2016; Volkova et al., 2013). In the literature, conventional approaches normally recast user profiling as a supervised learning problem (Marquardt et al., 2014; Zhang et al., 2016; Ciot et al., 2013; Corney et al., 2002) by exploring various user generated textual features and user social connection features. However, the performance largely depends on a large amount of labeled data and the performance normally degrades dramatically when the model is tested on a different social media. Even worse, many real scenarios may involve several social media with some of them lacking sufficient labeled dat"
C18-1203,W11-1701,0,0.376929,"on of documents and the corresponding feature sets. • We make a systematic comparison of LSTM with different linguistic features using the same benchmarks, and we explore influence of different linguistic features on neural models for stance detection. • Experimental results on two open datasets demonstrate the effectiveness of the proposed approach. 2 Related Works Previous works on stance detection have focused on congressional debates (Thomas et al., 2006; Burfoot et al., 2011), company-internal discussions (Agrawal et al., 2003), and debates in online forums (Somasundaran and Wiebe, 2010; Anand et al., 2011). Recently, there is a growing interest in performing stance detection on microblogs (Mohammad et al., 2016; Du et al., 2017). Most of existing works on stance detection can be separated into three categories: using linguistic features, using structure features, and using neural models. In the following subsections, we discuss the works on these three categories one by one. 2.1 Linguistic Features Mosts of previous works employed various kinds of linguistic features for stance detection. For example, Somasundaran and Wiebe (2010) developed a baseline for stance detection by modeling verbs and"
C18-1203,D16-1084,0,0.368152,"Missing"
C18-1203,P11-1151,0,0.193774,"erarchical attention based neural model tailored to stance detection task, which considers the mutual influence between the representation of documents and the corresponding feature sets. • We make a systematic comparison of LSTM with different linguistic features using the same benchmarks, and we explore influence of different linguistic features on neural models for stance detection. • Experimental results on two open datasets demonstrate the effectiveness of the proposed approach. 2 Related Works Previous works on stance detection have focused on congressional debates (Thomas et al., 2006; Burfoot et al., 2011), company-internal discussions (Agrawal et al., 2003), and debates in online forums (Somasundaran and Wiebe, 2010; Anand et al., 2011). Recently, there is a growing interest in performing stance detection on microblogs (Mohammad et al., 2016; Du et al., 2017). Most of existing works on stance detection can be separated into three categories: using linguistic features, using structure features, and using neural models. In the following subsections, we discuss the works on these three categories one by one. 2.1 Linguistic Features Mosts of previous works employed various kinds of linguistic feat"
C18-1203,C16-1154,0,0.301653,"t and are of practical interest to non-profits, governmental organizations, and companies. Previously, some of the related researches used feature engineering to extract either linguistic (Somasundaran and Wiebe, 2010) or structure (Sridhar et al., 2015) features manually. Recently, neural models have achieved high success and obtained the best results on stance detection (Zarrella and Marsh, 2016; Du et al., 2017). A key issue of the neural model is document representation, and most of previous works learn document representation from word sequence using Convolutional Neural Networks (CNNs) (Chen and Ku, 2016; Vijayaraghavan et al., 2016) or Recurrent Neural Networks (RNNs) (Augenstein et al., 2016; Du et al., 2017). nsubj dobj E1: I understand that homosexuals can't have kids. They can adopt children and thus, support the advancement of the human race. (Target: Gay Rights) dobj nmod:of Arguments pairs: (adopt,they),(adopt,children),(support,advancement),(advancement,race) However, besides the word sequence, stance is correlated with some explicit and implicit linguistic factors. Take E1 for example, both sentimental words (i.e., understand, support) and argument sentence (i.e., They can adopt ..."
C18-1203,P15-1033,0,0.0132835,"is employed to adjust the weight of different feature sets. In the following subsections, we firstly show the representation of a document, and then show the representation of different linguistic features. Finally, we illustrate hierarchical attention model, and training process. 3.1 Document Representation We use a standard Long Short-Term Memory (LSTM) model (Hochreiter and Schmidhuber, 1997) to learn the document representation. LSTM has been proven to be effective in many natural language processing (NLP) tasks such as machine translation (Sutskever et al., 2014) and dependency parsing (Dyer et al., 2015), and it is adept in harnessing long sentences. Let X = (w1 , w2 , ..., wn ) be a document, where n is the document length and wi is the i-th token. We transform each token wi into a real-valued vector xi using the word embedding vector of wi , obtained by looking up a pre-trained word embedding 2401 table D. We use the skip-gram algorithm to train embeddings (Mikolov et al., 2013). The LSTM is used over X to generate a hidden vector sequence (h1 , h2 , ..., hn ). At each step t, the hidden vector ht of LSTM model is computed based on the current vector xt and the previous vector ht−1 , and ht"
C18-1203,D14-1083,0,0.242867,"tection can be separated into three categories: using linguistic features, using structure features, and using neural models. In the following subsections, we discuss the works on these three categories one by one. 2.1 Linguistic Features Mosts of previous works employed various kinds of linguistic features for stance detection. For example, Somasundaran and Wiebe (2010) developed a baseline for stance detection by modeling verbs and sentiments. Anand et al. (2011) augmented the n-gram features with lexicon-based and dependency-based features. Except the above transitional linguistic factors, Hasan and Ng (2014) considered the importance of argument for stance detection, and explored the relations between stance and argument, and proposed a pipeline system to predict the stance and extract argument sentence from documents jointly. 2.2 Structure Features Although linguistic features show effectiveness for stance detection in many works, stance detection has been newly posed as collective classification task with extra structure information. For example, citation structure (Burfoot et al., 2011) or rebuttal links (Walker et al., 2012), was used as extra information to model agreements or disagreements"
C18-1203,S16-1003,0,0.530544,"ent linguistic features using the same benchmarks, and we explore influence of different linguistic features on neural models for stance detection. • Experimental results on two open datasets demonstrate the effectiveness of the proposed approach. 2 Related Works Previous works on stance detection have focused on congressional debates (Thomas et al., 2006; Burfoot et al., 2011), company-internal discussions (Agrawal et al., 2003), and debates in online forums (Somasundaran and Wiebe, 2010; Anand et al., 2011). Recently, there is a growing interest in performing stance detection on microblogs (Mohammad et al., 2016; Du et al., 2017). Most of existing works on stance detection can be separated into three categories: using linguistic features, using structure features, and using neural models. In the following subsections, we discuss the works on these three categories one by one. 2.1 Linguistic Features Mosts of previous works employed various kinds of linguistic features for stance detection. For example, Somasundaran and Wiebe (2010) developed a baseline for stance detection by modeling verbs and sentiments. Anand et al. (2011) augmented the n-gram features with lexicon-based and dependency-based featu"
C18-1203,C10-2100,0,0.0887726,"ence from documents jointly. 2.2 Structure Features Although linguistic features show effectiveness for stance detection in many works, stance detection has been newly posed as collective classification task with extra structure information. For example, citation structure (Burfoot et al., 2011) or rebuttal links (Walker et al., 2012), was used as extra information to model agreements or disagreements in debate posts and to infer their labels. Moreover, since similar users should express a similar opinion toward the same topic, user information is always used in stance detection. For example, Murakami and Raymond (2010) proposed a maximum cut method to aggregate stances in multiple posts to infer a user’s stance on the target. Rajadesingan and Liu (2014) used a retweet-based label propagation method, which started from a set of opinionated users and labeled tweets by the people who are in the retweet network. Sridhar et al. (2015) utilized Probabilistic Soft Logic (PSL) to collectively classify the stance of users and stance in posts. 2400 q Hyper Attention q2 q1 Attention H q3 Attention H(sent) X X(sent) Document Representation Sentiment Representation H(argument) H(dep) LSTM LSTM LSTM Attention X(dep) Depe"
C18-1203,P16-1205,0,0.0134273,"but use attention mechanism to measure the correlation between document representation and sentiment information. Particularly, we simply extract the sentimental word sequence X (sent) = {xs1 , xs2 , ..., xsm } of each document from sentiment lexicon, where xsi means sentiment word. We then learn the representation of sentiment information through this sentimental word sequence using LSTM model, and use H (sent) = LST M (xsm , hm−1 ) as sentiment representation. Dependency Representation The dependency-based features can be utilized to capture the inter-word relationships (Anand et al., 2011; Persing and Ng, 2016). The dependency structure involves some stance-taking text related to the given target. As in E4, we notice that the important words that express a stance in the sentence are “murder”, “never” and “necessity”. By analyzing the dependency structure in this and other prompt parts, we can extract the stance-taking dependency information we identify as “murder-never-necessity”. nsubj cop neg det E4: Murder is never a necessity. NN VBZ RB DT NN Hence, we extract the pair of arguments sequence X (dep) = {xd1 , xd2 , ..., xdm } = {x1 ⊕ x3 , ..., xi ⊕ xj } involved in each dependency relation from a"
C18-1203,W10-0214,0,0.595108,"e polarity expressed in a customer review (Pang et al., 2008; Liu, 2012), researchers have begun exploring new opinion mining tasks in recent years. One such task is stance detection: given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for and against) its author is taking. Stance detection system can potentially have a positive social impact and are of practical interest to non-profits, governmental organizations, and companies. Previously, some of the related researches used feature engineering to extract either linguistic (Somasundaran and Wiebe, 2010) or structure (Sridhar et al., 2015) features manually. Recently, neural models have achieved high success and obtained the best results on stance detection (Zarrella and Marsh, 2016; Du et al., 2017). A key issue of the neural model is document representation, and most of previous works learn document representation from word sequence using Convolutional Neural Networks (CNNs) (Chen and Ku, 2016; Vijayaraghavan et al., 2016) or Recurrent Neural Networks (RNNs) (Augenstein et al., 2016; Du et al., 2017). nsubj dobj E1: I understand that homosexuals can't have kids. They can adopt children and"
C18-1203,P15-1012,0,0.487386,"ang et al., 2008; Liu, 2012), researchers have begun exploring new opinion mining tasks in recent years. One such task is stance detection: given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for and against) its author is taking. Stance detection system can potentially have a positive social impact and are of practical interest to non-profits, governmental organizations, and companies. Previously, some of the related researches used feature engineering to extract either linguistic (Somasundaran and Wiebe, 2010) or structure (Sridhar et al., 2015) features manually. Recently, neural models have achieved high success and obtained the best results on stance detection (Zarrella and Marsh, 2016; Du et al., 2017). A key issue of the neural model is document representation, and most of previous works learn document representation from word sequence using Convolutional Neural Networks (CNNs) (Chen and Ku, 2016; Vijayaraghavan et al., 2016) or Recurrent Neural Networks (RNNs) (Augenstein et al., 2016; Du et al., 2017). nsubj dobj E1: I understand that homosexuals can't have kids. They can adopt children and thus, support the advancement of the"
C18-1203,W06-1639,0,0.309099,"We present a novel hierarchical attention based neural model tailored to stance detection task, which considers the mutual influence between the representation of documents and the corresponding feature sets. • We make a systematic comparison of LSTM with different linguistic features using the same benchmarks, and we explore influence of different linguistic features on neural models for stance detection. • Experimental results on two open datasets demonstrate the effectiveness of the proposed approach. 2 Related Works Previous works on stance detection have focused on congressional debates (Thomas et al., 2006; Burfoot et al., 2011), company-internal discussions (Agrawal et al., 2003), and debates in online forums (Somasundaran and Wiebe, 2010; Anand et al., 2011). Recently, there is a growing interest in performing stance detection on microblogs (Mohammad et al., 2016; Du et al., 2017). Most of existing works on stance detection can be separated into three categories: using linguistic features, using structure features, and using neural models. In the following subsections, we discuss the works on these three categories one by one. 2.1 Linguistic Features Mosts of previous works employed various k"
C18-1203,N12-1072,0,0.208958,"ncy-based features. Except the above transitional linguistic factors, Hasan and Ng (2014) considered the importance of argument for stance detection, and explored the relations between stance and argument, and proposed a pipeline system to predict the stance and extract argument sentence from documents jointly. 2.2 Structure Features Although linguistic features show effectiveness for stance detection in many works, stance detection has been newly posed as collective classification task with extra structure information. For example, citation structure (Burfoot et al., 2011) or rebuttal links (Walker et al., 2012), was used as extra information to model agreements or disagreements in debate posts and to infer their labels. Moreover, since similar users should express a similar opinion toward the same topic, user information is always used in stance detection. For example, Murakami and Raymond (2010) proposed a maximum cut method to aggregate stances in multiple posts to infer a user’s stance on the target. Rajadesingan and Liu (2014) used a retweet-based label propagation method, which started from a set of opinionated users and labeled tweets by the people who are in the retweet network. Sridhar et al"
C18-1203,W05-0308,0,0.0108991,"xd2 , ..., xdm } = {x1 ⊕ x3 , ..., xi ⊕ xj } involved in each dependency relation from a dependency parser as a feature, where xdi = xj ⊕ xk is arguments pair from dependency relation. We then learn the representation of dependency information through the dependency sequence X (dep) using LSTM model, and we use H (dep) = LST M (xdm , hm−1 ) as dependency representation. Argument Representation In general, there are not only the stance of the author toward the target in a post, but also personal beliefs about what is true or what action should be taken. This personal belief is called argument (Wilson and Wiebe, 2005). There exist some arguments behind the stance people express on a specified target. If we can extract the argument sentence from the post, then it will be beneficial to detect the author’s stance. 2402 As in E5, with the help of argument sentence with bold, it will be easy to detect that the author support the legalization of marijuana. E5: Most issues like this, such as sex between minors and alcohol, come down to one thing: it’s your choice. If you want to ruin your life, be my guest. It isn’t the government’s job to control that. (Target: Legalization of marijuana) In order to utilize argu"
C18-1203,S16-1074,0,0.306704,"en a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for and against) its author is taking. Stance detection system can potentially have a positive social impact and are of practical interest to non-profits, governmental organizations, and companies. Previously, some of the related researches used feature engineering to extract either linguistic (Somasundaran and Wiebe, 2010) or structure (Sridhar et al., 2015) features manually. Recently, neural models have achieved high success and obtained the best results on stance detection (Zarrella and Marsh, 2016; Du et al., 2017). A key issue of the neural model is document representation, and most of previous works learn document representation from word sequence using Convolutional Neural Networks (CNNs) (Chen and Ku, 2016; Vijayaraghavan et al., 2016) or Recurrent Neural Networks (RNNs) (Augenstein et al., 2016; Du et al., 2017). nsubj dobj E1: I understand that homosexuals can't have kids. They can adopt children and thus, support the advancement of the human race. (Target: Gay Rights) dobj nmod:of Arguments pairs: (adopt,they),(adopt,children),(support,advancement),(advancement,race) However, be"
C18-1215,D15-1075,0,0.185054,"c resources (Yih et al., 2013), tree edit distance (Yao and Durme, 2013) and named entities (Severyn and Moschitti, 2013). 1 https://www.taobao.com/ 2541 2 In deep learning methods, some neural network algorithms are employed to train learning models. Briefly, these methods could be categorized into three categories, i.e., siamense networks, attentive networks and compare-aggregate networks. In siamense networks, related studies use classic neural networks, such as LSTM and CNN, to get the representations separately and then concatenate them to classify. (Feng et al., 2015; Yang et al., 2015; Bowman et al., 2015). In attentive networks, instead of using the final time step of LSTM to represent a sentence, related studies use the attention strategy to get the weight of overall time steps and then use the weight to represent the sentence. (Tan et al., 2016; Hermann et al., 2015, Yin et al., 2015). In compare-aggregate networks, related studies use different matching strategy to get relationships within words. (He and Lin, 2016; Wang et al., 2017; Wang and Jiang, 2016; Trischler et al., 2016; Parikn et al., 2016.). However, all above approaches are similar to our One vs. One Matching model which deals wi"
C18-1215,N16-1108,0,0.0177049,"studies use classic neural networks, such as LSTM and CNN, to get the representations separately and then concatenate them to classify. (Feng et al., 2015; Yang et al., 2015; Bowman et al., 2015). In attentive networks, instead of using the final time step of LSTM to represent a sentence, related studies use the attention strategy to get the weight of overall time steps and then use the weight to represent the sentence. (Tan et al., 2016; Hermann et al., 2015, Yin et al., 2015). In compare-aggregate networks, related studies use different matching strategy to get relationships within words. (He and Lin, 2016; Wang et al., 2017; Wang and Jiang, 2016; Trischler et al., 2016; Parikn et al., 2016.). However, all above approaches are similar to our One vs. One Matching model which deals with the matching measurement between one sentence (or one piece of text) and another sentence (or another piece of text). In contrast, our approach is a One vs. Many Matching model which deals with the matching measurement between one sentence (or one piece of text) and multiple sentences (or multiple pieces of text). 3 Data Collection and Annotation We collect 4,060 question-answer pairs from “Asking All” in Taobao,"
C18-1215,D16-1244,0,0.0724571,"Missing"
C18-1215,D13-1044,0,0.0493654,"Missing"
C18-1215,P16-1044,0,0.189583,"mine whether an answer is answering a given question. For instance, in Figure 1, the question “Where is dear john filmed at?” in E1 has two candidate answers “The movie was filmed in 2009 in Charleston.” and “The file was released on May 25, 2010 on DVD.” The first answer is determined with the “Matching” category since it answers the question while the second answer is “Non-matching” since it could not answer the question. The past five years have witnessed a huge exploding interest in the research on QA matching, due to its widely applications, such as question answering (Yang et al., 2015; Tan et al., 2016; Wang et al., 2017) and reading comprehension (Trischler et al., 2016; Dhingra et al., 2017). However, all existing QA matching studies only focus on formal text. In real applications, there exists many scenarios where the QA text is informal. For instance, E2 is a question-answer pair extE1: Two QA pairs in formal text Q1: Where is dear john filmed at? Label: Matching Q2:Where is dear john filmed at? Label: Non-matching E2: Three QA pairs in informal text Q: Will the response time slow after updating os? What about the battery? What about the screen? A1: The movie was filmed in 2009 in Charl"
C18-1215,P16-1041,0,0.0278233,"ce, in Figure 1, the question “Where is dear john filmed at?” in E1 has two candidate answers “The movie was filmed in 2009 in Charleston.” and “The file was released on May 25, 2010 on DVD.” The first answer is determined with the “Matching” category since it answers the question while the second answer is “Non-matching” since it could not answer the question. The past five years have witnessed a huge exploding interest in the research on QA matching, due to its widely applications, such as question answering (Yang et al., 2015; Tan et al., 2016; Wang et al., 2017) and reading comprehension (Trischler et al., 2016; Dhingra et al., 2017). However, all existing QA matching studies only focus on formal text. In real applications, there exists many scenarios where the QA text is informal. For instance, E2 is a question-answer pair extE1: Two QA pairs in formal text Q1: Where is dear john filmed at? Label: Matching Q2:Where is dear john filmed at? Label: Non-matching E2: Three QA pairs in informal text Q: Will the response time slow after updating os? What about the battery? What about the screen? A1: The movie was filmed in 2009 in Charleston. A2: The film was released on May 25, 2010 on DVD. A: The respon"
C18-1215,C10-1131,0,0.083737,"Missing"
C18-1215,P16-1122,0,0.0269575,"Missing"
C18-1215,D15-1237,0,0.283339,"is a task to determine whether an answer is answering a given question. For instance, in Figure 1, the question “Where is dear john filmed at?” in E1 has two candidate answers “The movie was filmed in 2009 in Charleston.” and “The file was released on May 25, 2010 on DVD.” The first answer is determined with the “Matching” category since it answers the question while the second answer is “Non-matching” since it could not answer the question. The past five years have witnessed a huge exploding interest in the research on QA matching, due to its widely applications, such as question answering (Yang et al., 2015; Tan et al., 2016; Wang et al., 2017) and reading comprehension (Trischler et al., 2016; Dhingra et al., 2017). However, all existing QA matching studies only focus on formal text. In real applications, there exists many scenarios where the QA text is informal. For instance, E2 is a question-answer pair extE1: Two QA pairs in formal text Q1: Where is dear john filmed at? Label: Matching Q2:Where is dear john filmed at? Label: Non-matching E2: Three QA pairs in informal text Q: Will the response time slow after updating os? What about the battery? What about the screen? A1: The movie was filme"
C18-1215,N13-1106,0,0.0393979,"Missing"
C18-1215,P13-1171,0,0.02644,"erent from the above corpora, the question-answer pairs in our corpus are informal text. 2.2 Matching methods Generally speaking, QA matching methods could be split into two categories: shallow learning methods and deep learning methods. In shallow learning methods, some shallow learning algorithms, such as CRF, SVM and MaxEnt, are employed to train the learning models (Wang et al., 2010). Besides the learning algorithms, the related studies on shallow learning methods mainly focus on feature engineering, using linguistic tools and using external resources, such as lexical semantic resources (Yih et al., 2013), tree edit distance (Yao and Durme, 2013) and named entities (Severyn and Moschitti, 2013). 1 https://www.taobao.com/ 2541 2 In deep learning methods, some neural network algorithms are employed to train learning models. Briefly, these methods could be categorized into three categories, i.e., siamense networks, attentive networks and compare-aggregate networks. In siamense networks, related studies use classic neural networks, such as LSTM and CNN, to get the representations separately and then concatenate them to classify. (Feng et al., 2015; Yang et al., 2015; Bowman et al., 2015). In atten"
C18-1296,D15-1045,0,0.271598,"Missing"
C18-1296,H91-1060,0,0.133876,"f corpus annotation. For evaluating the consistency of the discourse structure tree, tree consistency can more objectively reflect the quality of annotation. CDTB and PDTB only evaluated the consistency of certain indicators, such as discourse relation, and did not evaluate the tree consistency. RST-DT proposed a method of evaluating the complete consistency of a discourse tree, but it also had several shortcomings. The evaluation objects of RST-DT are the discourse structure, nuclearity and relationship annotated on the children nodes, while the evaluation object of the standard syntax tree (Black et al., 1991) is the annotations on the parent node. In this way, RST-DT will be at least double the standard syntax tree evaluation of the sample, and 3498 because of the mutual constraint relationship between children nodes, the consistency of corpus annotation of RST-DT is objectively higher. In the annotation of a macro discourse relationship, the leaf nodes are natural paragraphs and do not need to be manually divided. Therefore, unlike the assessment method of the RST-DT agreement, we do not use the leaf nodes as an example of a consistent assessment. In addition, we adopted the standard syntax tree"
C18-1296,P12-1007,0,0.0292046,"most critical step in the related tasks of discourse analysis. We use a Conditional Random Field (CRF) model to experiment with the parameter C of 4, the feature window of 3, and the rest of the parameters as a default value. There were 8,863 samples in total, including 3,261 positive samples and 5,602 negative samples. Because of the language differences between English and Chinese, as well as the microscopic and macroscopic differences in features, such as the absence of syntax trees and the absence of tense features, we select only several of the structural features of the previous study (Feng and Hirst, 2012) as our Feature Set 1 as follows:  The position of the beginning and end of a discourse unit;  The number of sentences and the number of paragraphs contained in a discourse unit;  The comparison of the number of sentences in the discourse unit to the previous unit;  The comparison of the number of paragraphs in the discourse unit to the previous unit. 3501 Considering the process of constructing discourse structure trees, we may need some procedural characteristics. We regard whether the discourse unit is a leaf node or whether the discourse unit is merged in the previous round as organiza"
C18-1296,P14-1048,0,0.0603068,"Missing"
C18-1296,D14-1224,1,0.78642,"style to annotate 164 Chinese documents from CTB (Xue, 2005; Xue et al., 2005). The experiment demonstrated the transferability of the English discourse analysis theory in Chinese language. Lv et al. (2015) constructed a Chinese corpus of 496 news articles from People&apos;s Daily under the framework of the Chinese Framing Network (CFN) (Hao et al., 2007). Each news article annotated the discourse frame, structure and relationship. However, the discourse units are also smaller with a minimum of one sentence and a maximum of five sentences, and their research object is primarily at the word level. Li et al. (2014) integrated RST-DT and PDTB and used the representation method based on the connection dependence tree to annotate the Chinese Dependency Treebank (CDTB) containing 500 Chinese Xinhua news articles. However, this method only considered the discourse relationship annotation inside the paragraph and did not annotate the discourse relationship between paragraphs and the macro discourse information of the whole text. Compared with the well-studied micro discourse analysis, macro discourse analysis is in the trial stage both in English and Chinese. In English, Sporleder and Lascarides (2004) attemp"
C18-1296,D13-1070,0,0.423657,"Missing"
C18-1296,W99-0307,0,0.233266,"ded into micro and macro discourse analysis. The theory of micro discourse primarily includes Hobbs model (Hobbs, 1985), rhetorical structure theory (RST) (Mann and Thompson, 1987; Mann et al., 1992), sentence group theory (Wu and Tian, 2000) and complex sentence theory (Fu, 2001), and the macro discourse theory has macro hyper-theme theory (Martin and Rose, 3494 2003) and macro structure theory (Van Dijk, 1980). Under the guidance of these theories, various corpora have been developed accordingly. In micro discourse analysis, the most popular corpora are RST-DT and PDTB. Based on RST, RSTDT (Marcu et al., 1999; Carlson et al., 2003) contained 385 articles from the Wall Street Journal annotated by 16 categories and 78 classes&apos; rhetorical relations, to represent the relationship between two or more discourse units. Most of the elements of discourse units in RST-DT are phrases including partial clauses. To ensure the universality of the corpus, PDTB (Parsad et al., 2008) uses the LTAG (Lexicalized Tree-Adjoining Grammars) theory, and the annotation is entirely based on lexicalization. This approach primarily annotated the argument structure, including the conjunction and the semantic distinction infor"
C18-1296,prasad-etal-2008-penn,0,0.218524,"Missing"
C18-1296,C04-1007,0,0.632026,"ly at the word level. Li et al. (2014) integrated RST-DT and PDTB and used the representation method based on the connection dependence tree to annotate the Chinese Dependency Treebank (CDTB) containing 500 Chinese Xinhua news articles. However, this method only considered the discourse relationship annotation inside the paragraph and did not annotate the discourse relationship between paragraphs and the macro discourse information of the whole text. Compared with the well-studied micro discourse analysis, macro discourse analysis is in the trial stage both in English and Chinese. In English, Sporleder and Lascarides (2004) attempted to perform an experiment on macro discourse analysis on RST-DT. However, RST-DT focuses on the micro discourse level, which is only tagging the relations on the sentence level and ignoring the relations on the paragraph level. Thus, it leads to some tailoring and correction when Sporleder carries out macro discourse analysis on the paragraph level. In Chinese, Chu et al. (2017) carried out relevant research on the nuclearity of macro discourse and made corresponding attempts to construct a macro discourse corpus. In the field of Chinese discourse annotation, the number of relevant c"
C18-1296,P17-2029,0,0.0505954,"Missing"
C18-1296,W05-0312,0,0.025408,"e corpus, PDTB (Parsad et al., 2008) uses the LTAG (Lexicalized Tree-Adjoining Grammars) theory, and the annotation is entirely based on lexicalization. This approach primarily annotated the argument structure, including the conjunction and the semantic distinction information. The basic unit of argument is the event or state, and the specific form is the clause or sentence. The corpus is relatively large with 2,159 articles and approximately 1 million words. Regarding Chinese micro discourse corpus, Zhou et al. (2015) used the PDTB annotation style to annotate 164 Chinese documents from CTB (Xue, 2005; Xue et al., 2005). The experiment demonstrated the transferability of the English discourse analysis theory in Chinese language. Lv et al. (2015) constructed a Chinese corpus of 496 news articles from People&apos;s Daily under the framework of the Chinese Framing Network (CFN) (Hao et al., 2007). Each news article annotated the discourse frame, structure and relationship. However, the discourse units are also smaller with a minimum of one sentence and a maximum of five sentences, and their research object is primarily at the word level. Li et al. (2014) integrated RST-DT and PDTB and used the rep"
C18-1296,P14-1049,1,0.900527,"Missing"
C98-2234,J93-2002,0,0.0268082,"Missing"
C98-2234,C90-3010,0,0.0621194,"Missing"
C98-2234,J90-1003,0,0.0513027,"Missing"
C98-2234,J93-1005,0,0.307229,"Missing"
C98-2234,C94-2139,0,0.0397909,"Missing"
C98-2234,O90-1010,0,\N,Missing
D07-1076,P05-1053,1,0.924919,"” conveys the ACE-style relation “EMPLOYMENT.exec” between the entities “Bill Gates” (person name) and “Microsoft Corporation” (organization name). Extraction of semantic relations between entities can be very useful in many applic ations such as question answering, e.g. to answer the query “Who is the president of the United States?”, and information retrieval, e.g. to expand the query “George W. Bush”with “the president of the United States”via his relationship with “the United States”. Many researches have been done in relation extraction. Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entityrelated information to syntactic parse trees, dependency trees and semantic information. However, it is difficult for them to effectively capture structured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction. As an alternative to feature-based methods, tree kernel-based methods provide an elegant solution to explore implicitly structured features by directly computing the similarity between two trees. Alth"
D07-1076,P06-1016,1,0.776606,"Missing"
D07-1076,H05-1091,0,0.866971,"Missing"
D07-1076,P04-1054,0,\N,Missing
D07-1076,P05-1052,0,\N,Missing
D07-1076,P06-1104,1,\N,Missing
D07-1076,P01-1017,0,\N,Missing
D09-1102,P99-1048,0,0.692936,"1; Ng and Cardie 2002b; Strube and Muller 2003; Yang et al 2003, 2008), anaphoricity determination has been studied fairly extensively in the literature, given the potential usefulness of NP anaphoricity in coreference resolution. One common approach involves the design of heuristic rules to identify specific types of nonanaphoric NPs, such as pleonastic pronouns (e.g. Paice and Husk 1987; Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998) and existential definite descriptions (e.g., Vieira and Poesio 2000). More recently, the problem has been tackled using statistics-based (e.g., Bean and Riloff 1999; Bergsma et al 2008) and learning-based (e.g. Evans 2001; Ng and Cardie 2002a; Ng 2004; Yang et al 2005; Denis and Balbridge 2007) methods. Although there is empirical evidence (e.g. Ng and Cardie 2002a, 2004) that coreference resolution might be further improved with proper anaphoricity information, its contribution is still somewhat disappointing and lacks systematic evaluation. This paper employs a label propagation (LP) algorithm for global learning of NP anaphoricity. Given the labeled data and the unlabeled data, the LP algorithm first represents labeled and unlabeled instances as verti"
D09-1102,P01-1017,0,0.0599152,"tructure to cover syntactic structured information well in the tree kernel computation. Generally, the more a parse tree structure includes, the more syntactic structured information would be provided, at the expense of more noisy/unnecessary information. In this paper, we limit the window size to 5 chunks (either NPs or non-NPs), including previous two chunks, current chunk (i.e. current NP) and following two chunks, and prune out the substructures outside the window. Figure 2 shows the full parse tree for the sentence “Mary said the woman in the room hit her too”, using the Charniak parser (Charniak 2001), and the chunk sequence derived from the parse tree using the Perl script2 written by Sabine Buchholz from Tilburg University. Here, we explore four parse tree structures in NP anaphoricity determination: the common tree (CT), the shortest path-enclosed tree (SPT), the minimum tree (MT) and the dynamically extended tree (DET), motivated by Yang et al (2006) and Zhou et al (2008). Following are the examples of the four parse tree structures, corresponding to the full parse tree and the chunk sequence, as shown in Figure 2, with the NP chunk “(NP (DT the) (NN woman))” in anaphoricity determinat"
D09-1102,W05-0612,0,0.515212,"Missing"
D09-1102,N07-1030,0,0.627381,"Missing"
D09-1102,C96-1021,0,0.344032,"outcomes. Although many existing machine learning approaches to coreference resolution have performed reasonably well without explicit anaphoricity determination (e.g., Soon et al 2001; Ng and Cardie 2002b; Strube and Muller 2003; Yang et al 2003, 2008), anaphoricity determination has been studied fairly extensively in the literature, given the potential usefulness of NP anaphoricity in coreference resolution. One common approach involves the design of heuristic rules to identify specific types of nonanaphoric NPs, such as pleonastic pronouns (e.g. Paice and Husk 1987; Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998) and existential definite descriptions (e.g., Vieira and Poesio 2000). More recently, the problem has been tackled using statistics-based (e.g., Bean and Riloff 1999; Bergsma et al 2008) and learning-based (e.g. Evans 2001; Ng and Cardie 2002a; Ng 2004; Yang et al 2005; Denis and Balbridge 2007) methods. Although there is empirical evidence (e.g. Ng and Cardie 2002a, 2004) that coreference resolution might be further improved with proper anaphoricity information, its contribution is still somewhat disappointing and lacks systematic evaluation. This paper employs a label propagati"
D09-1102,J94-4002,0,0.763337,"end up with inaccurate outcomes. Although many existing machine learning approaches to coreference resolution have performed reasonably well without explicit anaphoricity determination (e.g., Soon et al 2001; Ng and Cardie 2002b; Strube and Muller 2003; Yang et al 2003, 2008), anaphoricity determination has been studied fairly extensively in the literature, given the potential usefulness of NP anaphoricity in coreference resolution. One common approach involves the design of heuristic rules to identify specific types of nonanaphoric NPs, such as pleonastic pronouns (e.g. Paice and Husk 1987; Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998) and existential definite descriptions (e.g., Vieira and Poesio 2000). More recently, the problem has been tackled using statistics-based (e.g., Bean and Riloff 1999; Bergsma et al 2008) and learning-based (e.g. Evans 2001; Ng and Cardie 2002a; Ng 2004; Yang et al 2005; Denis and Balbridge 2007) methods. Although there is empirical evidence (e.g. Ng and Cardie 2002a, 2004) that coreference resolution might be further improved with proper anaphoricity information, its contribution is still somewhat disappointing and lacks systematic evaluation. This pape"
D09-1102,C02-1139,0,0.735269,"when two NPs can co-refer remains an active area of research in the community. One significant constraint on coreference, the anaphoricity constraint, specifies that a nonanaphoric NP cannot be coreferent with any of its preceding NPs in a given text. Therefore, it is useful to skip over these non-anaphoric NPs rather than attempt an unnecessary search for an antecedent for them, only to end up with inaccurate outcomes. Although many existing machine learning approaches to coreference resolution have performed reasonably well without explicit anaphoricity determination (e.g., Soon et al 2001; Ng and Cardie 2002b; Strube and Muller 2003; Yang et al 2003, 2008), anaphoricity determination has been studied fairly extensively in the literature, given the potential usefulness of NP anaphoricity in coreference resolution. One common approach involves the design of heuristic rules to identify specific types of nonanaphoric NPs, such as pleonastic pronouns (e.g. Paice and Husk 1987; Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998) and existential definite descriptions (e.g., Vieira and Poesio 2000). More recently, the problem has been tackled using statistics-based (e.g., Bean and Riloff 1999;"
D09-1102,P02-1014,0,0.410195,"when two NPs can co-refer remains an active area of research in the community. One significant constraint on coreference, the anaphoricity constraint, specifies that a nonanaphoric NP cannot be coreferent with any of its preceding NPs in a given text. Therefore, it is useful to skip over these non-anaphoric NPs rather than attempt an unnecessary search for an antecedent for them, only to end up with inaccurate outcomes. Although many existing machine learning approaches to coreference resolution have performed reasonably well without explicit anaphoricity determination (e.g., Soon et al 2001; Ng and Cardie 2002b; Strube and Muller 2003; Yang et al 2003, 2008), anaphoricity determination has been studied fairly extensively in the literature, given the potential usefulness of NP anaphoricity in coreference resolution. One common approach involves the design of heuristic rules to identify specific types of nonanaphoric NPs, such as pleonastic pronouns (e.g. Paice and Husk 1987; Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998) and existential definite descriptions (e.g., Vieira and Poesio 2000). More recently, the problem has been tackled using statistics-based (e.g., Bean and Riloff 1999;"
D09-1102,P04-1020,0,0.690488,"as been studied fairly extensively in the literature, given the potential usefulness of NP anaphoricity in coreference resolution. One common approach involves the design of heuristic rules to identify specific types of nonanaphoric NPs, such as pleonastic pronouns (e.g. Paice and Husk 1987; Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998) and existential definite descriptions (e.g., Vieira and Poesio 2000). More recently, the problem has been tackled using statistics-based (e.g., Bean and Riloff 1999; Bergsma et al 2008) and learning-based (e.g. Evans 2001; Ng and Cardie 2002a; Ng 2004; Yang et al 2005; Denis and Balbridge 2007) methods. Although there is empirical evidence (e.g. Ng and Cardie 2002a, 2004) that coreference resolution might be further improved with proper anaphoricity information, its contribution is still somewhat disappointing and lacks systematic evaluation. This paper employs a label propagation (LP) algorithm for global learning of NP anaphoricity. Given the labeled data and the unlabeled data, the LP algorithm first represents labeled and unlabeled instances as vertices in a connected graph, then propagates the label information from any vertex to near"
D09-1102,P03-1022,0,0.0147053,"efer remains an active area of research in the community. One significant constraint on coreference, the anaphoricity constraint, specifies that a nonanaphoric NP cannot be coreferent with any of its preceding NPs in a given text. Therefore, it is useful to skip over these non-anaphoric NPs rather than attempt an unnecessary search for an antecedent for them, only to end up with inaccurate outcomes. Although many existing machine learning approaches to coreference resolution have performed reasonably well without explicit anaphoricity determination (e.g., Soon et al 2001; Ng and Cardie 2002b; Strube and Muller 2003; Yang et al 2003, 2008), anaphoricity determination has been studied fairly extensively in the literature, given the potential usefulness of NP anaphoricity in coreference resolution. One common approach involves the design of heuristic rules to identify specific types of nonanaphoric NPs, such as pleonastic pronouns (e.g. Paice and Husk 1987; Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998) and existential definite descriptions (e.g., Vieira and Poesio 2000). More recently, the problem has been tackled using statistics-based (e.g., Bean and Riloff 1999; Bergsma et al 2008) and"
D09-1102,J00-4003,0,0.108352,"on have performed reasonably well without explicit anaphoricity determination (e.g., Soon et al 2001; Ng and Cardie 2002b; Strube and Muller 2003; Yang et al 2003, 2008), anaphoricity determination has been studied fairly extensively in the literature, given the potential usefulness of NP anaphoricity in coreference resolution. One common approach involves the design of heuristic rules to identify specific types of nonanaphoric NPs, such as pleonastic pronouns (e.g. Paice and Husk 1987; Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998) and existential definite descriptions (e.g., Vieira and Poesio 2000). More recently, the problem has been tackled using statistics-based (e.g., Bean and Riloff 1999; Bergsma et al 2008) and learning-based (e.g. Evans 2001; Ng and Cardie 2002a; Ng 2004; Yang et al 2005; Denis and Balbridge 2007) methods. Although there is empirical evidence (e.g. Ng and Cardie 2002a, 2004) that coreference resolution might be further improved with proper anaphoricity information, its contribution is still somewhat disappointing and lacks systematic evaluation. This paper employs a label propagation (LP) algorithm for global learning of NP anaphoricity. Given the labeled data an"
D09-1102,M95-1005,0,0.408443,"t candidates. In particular, the test corpus is resolved in document-level, i.e. one document by one document. For anaphoricity determination, we report the performance in Acc+ and Acc-, which measure the accuracies of identifying anaphoric NPs and non-anaphoric NPs, respectively. Obviously, higher Acc+ means that more anaphoric NPs would be identified correctly, while higher Accmeans that more non-anaphoric NPs would be filtered out. For coreference resolution, we report the performance in terms of recall, precision, and F1-measure using the commonly-used model theoretic MUC scoring program (Vilain et al., 1995). For separate scoring of different NP types, a recognized reference is considered correct if the reconized antecedent is in the coreferential chain of the anaphor. To see whether an improvement is significant, we conduct significance testing using paired t-test. In this paper, ‘>>>’, ‘>>’and ‘>’denote p-values of an improvement smaller than 0.01, inbetween (0.01, 0,05] and bigger than 0.05, which mean significantly better, moderately better and slightly better, respectively. 5.2 Experimental Results Table 2 shows the performance of LP-based anaphoricity determination using the feature-based R"
D09-1102,P06-1006,0,0.120409,"hunks, current chunk (i.e. current NP) and following two chunks, and prune out the substructures outside the window. Figure 2 shows the full parse tree for the sentence “Mary said the woman in the room hit her too”, using the Charniak parser (Charniak 2001), and the chunk sequence derived from the parse tree using the Perl script2 written by Sabine Buchholz from Tilburg University. Here, we explore four parse tree structures in NP anaphoricity determination: the common tree (CT), the shortest path-enclosed tree (SPT), the minimum tree (MT) and the dynamically extended tree (DET), motivated by Yang et al (2006) and Zhou et al (2008). Following are the examples of the four parse tree structures, corresponding to the full parse tree and the chunk sequence, as shown in Figure 2, with the NP chunk “(NP (DT the) (NN woman))” in anaphoricity determination. last chunk “(NP (DT the) (NN room))” of the five-chunk window. Shortest Path-enclosed Tree (SPT) As shown in Figure 3(b), SPT is the smallest common sub-tree enclosed by the shortest path between the first chunk “(NP (NNP Mary))”and the last chunk “(NP (DT the) (NN room))”of the five-chunk window. (a) the full parse tree (NP (NNP Mary)) (VP (VBD said))"
D09-1102,P08-1096,0,0.144378,"Missing"
D09-1102,W00-1309,1,0.752244,"Missing"
D09-1102,I08-1004,1,0.857867,"i.e. current NP) and following two chunks, and prune out the substructures outside the window. Figure 2 shows the full parse tree for the sentence “Mary said the woman in the room hit her too”, using the Charniak parser (Charniak 2001), and the chunk sequence derived from the parse tree using the Perl script2 written by Sabine Buchholz from Tilburg University. Here, we explore four parse tree structures in NP anaphoricity determination: the common tree (CT), the shortest path-enclosed tree (SPT), the minimum tree (MT) and the dynamically extended tree (DET), motivated by Yang et al (2006) and Zhou et al (2008). Following are the examples of the four parse tree structures, corresponding to the full parse tree and the chunk sequence, as shown in Figure 2, with the NP chunk “(NP (DT the) (NN woman))” in anaphoricity determination. last chunk “(NP (DT the) (NN room))” of the five-chunk window. Shortest Path-enclosed Tree (SPT) As shown in Figure 3(b), SPT is the smallest common sub-tree enclosed by the shortest path between the first chunk “(NP (NNP Mary))”and the last chunk “(NP (DT the) (NN room))”of the five-chunk window. (a) the full parse tree (NP (NNP Mary)) (VP (VBD said)) (NP-E (DT the) (NN wom"
D09-1102,P02-1060,1,\N,Missing
D09-1102,I05-1063,0,\N,Missing
D09-1102,P08-1002,0,\N,Missing
D09-1102,J01-4004,0,\N,Missing
D09-1102,P03-1023,1,\N,Missing
D09-1103,P95-1017,0,0.301061,"Missing"
D09-1103,W99-0629,0,0.047712,"Missing"
D09-1103,C88-1021,0,0.134457,"nce resolution is to determine the antecedent for each referring expression in a text. The ability of linking referring expressions both within a sentence and across the sentences in a text is critical * to discourse and language understanding in general. For example, coreference resolution is a key task in information extraction, machine translation, text summarization, and question answering. There is a long tradition of research on coreference resolution within computational linguistics. While earlier knowledge-lean approaches heavily depend on domain and linguistic knowledge (Carter 1987; Carbonell and Brown 1988) and have significantly influenced the research, the later approaches usually rely on diverse lexical, syntactic and semantic properties of referring expressions (Soon et al., 2001;Ng and Cardie, 2002; Zhou et al., 2004). Current research has been focusing on exploiting semantic information in coreference resolution. For example, Yang et al (2005) proposed a template-based statistical approach to compute the semantic compatibility between a pronominal anaphor and an antecedent candidate, and Yang and Su (2007) explored semantic relatedness information from automatically discovered patterns, wh"
D09-1103,J95-2003,0,0.493741,"(in a left-to-right order) by mapping each pronoun into 5 levels: a) rank 1 for pronouns with semantic role ARG0/agent of the main predicate; b) rank 2 for pronouns with semantic role ARG1/patient of the main predicate; c) rank 3 for pronouns with semantic role ARG0/agent of other predicates; d) rank 4 for pronouns with semantic role ARG1/patient of other predicates; e) rank 5 for remaining pronouns. Furthermore, for those pronouns with the same ranking level, they are ordered according to their surface positions in a left-toright order, motivated by previous research on the centering theory (Grosz et al. 1995). 3) Detailed pronominal subcategory features. Given a pronominal expression, its detailed pronominal subcategory features, such as whether it is a first person pronoun, second person pronoun, third person pronoun, neuter pronoun or others, are explored to enhance the discriminative power of both the semantic role features and the relative pronominal ranking feature, considering the predominant importance of pronouns in tracking the local focus structure in discourse. 4.3 Comparison with Previous Work As a representative in explicitly employing semantic role labeling in coreference resolution,"
D09-1103,P98-2143,0,0.0606747,"Missing"
D09-1103,P02-1014,0,0.333918,"discourse and language understanding in general. For example, coreference resolution is a key task in information extraction, machine translation, text summarization, and question answering. There is a long tradition of research on coreference resolution within computational linguistics. While earlier knowledge-lean approaches heavily depend on domain and linguistic knowledge (Carter 1987; Carbonell and Brown 1988) and have significantly influenced the research, the later approaches usually rely on diverse lexical, syntactic and semantic properties of referring expressions (Soon et al., 2001;Ng and Cardie, 2002; Zhou et al., 2004). Current research has been focusing on exploiting semantic information in coreference resolution. For example, Yang et al (2005) proposed a template-based statistical approach to compute the semantic compatibility between a pronominal anaphor and an antecedent candidate, and Yang and Su (2007) explored semantic relatedness information from automatically discovered patterns, while Ng (2007) automatically induced semantic class knowledge from a treebank and explored its application in coreference resolution. Particularly, this paper focuses on the centering theory (Sidner,19"
D09-1103,J05-1004,0,0.0929658,"assigns them appropriate semantic roles. During the last few years, there has been growing interest in SRL. For example, CoNLL 2004 and 2005 have made this problem a well-known shared task. However, there is still little consensus in the linguistic and NLP communities about what set of semantic role labels are most appropriate. Typical semantic roles include core roles, such as agent, patient, instrument, and adjunct roles (such as locative, temporal, manner, and cause). For core roles, only agent and patient are consistently defined across different predicates, e.g. in the popular PropBank (Palmer et al. 2005) and the derived version evaluated in the CoNLL 2004 and 2005 shared tasks, as ARG0 and ARG1. In this paper, we extend the centering theory from the grammatical level to the semantic level for its better application in pronoun resolution via proper semantic role features due to three reasons: Sentence Grammatical Role Semantic Role Bob opened the Bob: Bob: door with a key. SUBJECT AGENT The key opened The key: The key : the door. SUBJECT INSTRUMENT The door opened. The door: The door: SUBJECT PATIENT Table 3: Relationship between grammatical roles and semantic roles: an example 1) Semantic rol"
D09-1103,E06-2015,0,0.209078,"3) Detailed pronominal subcategory features. Given a pronominal expression, its detailed pronominal subcategory features, such as whether it is a first person pronoun, second person pronoun, third person pronoun, neuter pronoun or others, are explored to enhance the discriminative power of both the semantic role features and the relative pronominal ranking feature, considering the predominant importance of pronouns in tracking the local focus structure in discourse. 4.3 Comparison with Previous Work As a representative in explicitly employing semantic role labeling in coreference resolution, Ponzetto and Strube (2006) explored two semantic role features to capture the predicateargument structure information to benefit coreference resolution: I_SEMROLE, the predicate-argument pairs of one referring expression, and J_SEMROLE, the predicate-argument pairs of another referring expression. Their experiments on the ACE 2003 corpus shows that, while the two semantic role features much improve the performance of common noun resolution by 3.8 and 2.7 in F-measure on the BNEWS and NWIRE domains respectively, they only 1 According to the centering theory, the backward-looking center Cb is preferentially realized by a"
D09-1103,A88-1003,0,0.376098,"Missing"
D09-1103,D07-1002,0,0.0750626,"Missing"
D09-1103,J81-4001,0,0.389356,"Missing"
D09-1103,P04-1017,1,0.920801,"Missing"
D09-1103,P05-1021,0,0.262157,"Missing"
D09-1103,P07-1067,0,0.0948564,"an approaches heavily depend on domain and linguistic knowledge (Carter 1987; Carbonell and Brown 1988) and have significantly influenced the research, the later approaches usually rely on diverse lexical, syntactic and semantic properties of referring expressions (Soon et al., 2001;Ng and Cardie, 2002; Zhou et al., 2004). Current research has been focusing on exploiting semantic information in coreference resolution. For example, Yang et al (2005) proposed a template-based statistical approach to compute the semantic compatibility between a pronominal anaphor and an antecedent candidate, and Yang and Su (2007) explored semantic relatedness information from automatically discovered patterns, while Ng (2007) automatically induced semantic class knowledge from a treebank and explored its application in coreference resolution. Particularly, this paper focuses on the centering theory (Sidner,1981;Grosz et al.,1995; Tetreault,2001), which reveals the significant impact of the local focus on referring expressions in that the antecedent of a referring expression usually depends on the center of attention throughout the local discourse segment (Mitkov,1998). Although the centering theory has been considered"
D09-1103,C04-1075,1,0.920198,"Missing"
D09-1103,A00-1020,0,\N,Missing
D09-1103,P99-1079,0,\N,Missing
D09-1103,P08-2029,0,\N,Missing
D09-1103,J01-4003,0,\N,Missing
D09-1103,P87-1022,0,\N,Missing
D09-1103,J01-4004,0,\N,Missing
D09-1103,P86-1031,0,\N,Missing
D09-1103,C98-2138,0,\N,Missing
D09-1103,P07-1068,0,\N,Missing
D09-1103,J01-4007,0,\N,Missing
D09-1133,P98-1013,0,0.0167375,"n and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse t"
D09-1133,W04-2412,0,0.0205686,"lution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and"
D09-1133,W05-0620,0,0.21267,"Missing"
D09-1133,D08-1034,0,0.0979658,"Missing"
D09-1133,W06-1617,0,0.63749,"L could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (X"
D09-1133,P07-1027,0,0.0832518,"Missing"
D09-1133,meyers-etal-2004-annotating,0,0.0395649,"s and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small priv"
D09-1133,P04-1043,0,0.12312,"arsing, we employ a Chinese word segmenter, similar to Ng and Low (2004), to obtain the best automatic segmentation result for a given sentence, which is then fed into Berkeley parser for further syntactic parsing. Both the word segmenter and Berkeley parser are developed with the same training and development datasets as our SRL experiments. The word segmenter achieves the performance of 96.1 in F1-measure while the Berkeley parser gives a performance of 82.5 and 85.5 in F1measure on golden and automatic word segmentation, respectively 2 . In addition, SVMLight with the tree kernel function (Moschitti, 2004) 3 is selected as our classifier. In order to handle multi-classification 1 Berkeley Parser. http://code.google.com/p/berkeleyparser/ POSs are not counted in evaluating the performance of word-based syntactic parser, but they are counted in evaluating the performance of character-based parser. Therefore the F1-measure for the later is higher than that for the former. 3 SVM-LIGHT-TK. http://dit.unitn.it/~moschitt/ 2 1285 problem in argument classification, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others. For argument identification an"
D09-1133,C04-1100,0,0.0221351,"nominal SRL system much outperforms the state-of-the-art ones. 1. Introduction Semantic parsing maps a natural language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most of previous work focuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate in a sentence. In particular, the well-defined semantic role labeling (SRL) task has been drawing more and more attention in recent years due to its importance in deep NLP applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), P"
D09-1133,W04-3236,0,0.0211059,"les (chtb_001 to 040.fid and chtb_900 to 931.fid) are held out as the test data, and 40 files (chtb_041 to 080.fid) as the development data, with 8642, 1124, and 731 propositions, respectively. As Chinese words are not naturally segmented in raw sentences, two Chinese automatic parsers are constructed: word-based parser (assuming golden word segmentation) and character-based parser (with automatic word segmentation). Here, Berkeley parser (Petrov and Klein, 2007) 1 is chosen as the Chinese automatic parser. With regard to character-based parsing, we employ a Chinese word segmenter, similar to Ng and Low (2004), to obtain the best automatic segmentation result for a given sentence, which is then fed into Berkeley parser for further syntactic parsing. Both the word segmenter and Berkeley parser are developed with the same training and development datasets as our SRL experiments. The word segmenter achieves the performance of 96.1 in F1-measure while the Berkeley parser gives a performance of 82.5 and 85.5 in F1measure on golden and automatic word segmentation, respectively 2 . In addition, SVMLight with the tree kernel function (Moschitti, 2004) 3 is selected as our classifier. In order to handle mul"
D09-1133,J05-1004,0,0.108985,"tion extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden"
D09-1133,N07-1051,0,0.0314267,"ese Penn TreeBank 5.1. Following the experimental setting in Xue (2008), 648 files (chtb_081 to 899.fid) are selected as the training data, 72 files (chtb_001 to 040.fid and chtb_900 to 931.fid) are held out as the test data, and 40 files (chtb_041 to 080.fid) as the development data, with 8642, 1124, and 731 propositions, respectively. As Chinese words are not naturally segmented in raw sentences, two Chinese automatic parsers are constructed: word-based parser (assuming golden word segmentation) and character-based parser (with automatic word segmentation). Here, Berkeley parser (Petrov and Klein, 2007) 1 is chosen as the Chinese automatic parser. With regard to character-based parsing, we employ a Chinese word segmenter, similar to Ng and Low (2004), to obtain the best automatic segmentation result for a given sentence, which is then fed into Berkeley parser for further syntactic parsing. Both the word segmenter and Berkeley parser are developed with the same training and development datasets as our SRL experiments. The word segmenter achieves the performance of 96.1 in F1-measure while the Berkeley parser gives a performance of 82.5 and 85.5 in F1measure on golden and automatic word segmen"
D09-1133,E06-2015,0,0.310819,"vided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (X"
D09-1133,N04-4036,0,0.0189779,"& 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended the"
D09-1133,N04-1032,0,0.0110768,"(Carreras and Màrquez, 2004 & 2005) in English language. As a complement to PropBank on verbal predicates, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively"
D09-1133,P03-1002,0,0.0353596,"ones. 1. Introduction Semantic parsing maps a natural language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most of previous work focuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate in a sentence. In particular, the well-defined semantic role labeling (SRL) task has been drawing more and more attention in recent years due to its importance in deep NLP applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). Given a sentence and a predicate (either a verb or a noun) in it, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) ∗ of the predicate. According to the predicate types, SRL could be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecuti"
D09-1133,W03-1707,0,0.0158673,"similar semantic framework as PropBank. As a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply inCorresponding author 1280 Proceedings of the 2009 Conference on Empirical Methods"
D09-1133,xue-2006-annotating,0,0.493591,"a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply inCorresponding author 1280 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,"
D09-1133,N06-1055,0,0.353081,"a representative, Jiang and Ng (2006) pioneered the exploration of various nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply inCorresponding author 1280 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,"
D09-1133,J08-2004,0,0.609138,"arious nominal SRL-specific features besides the traditional verbal SRL-related features on NomBank. They achieved the performance of 72.73 and 69.14 in F1-measure on golden and automatic syntactic parse trees, respectively, given golden nominal predicates. For SRL in Chinese, Sun and Jurafsky (2004) and Pradhan et al. (2004) pioneered the research on Chinese verbal and nominal SRLs, respectively, on small private datasets. Taking the advantage of recent release of Chinese PropBank (Xue and Palmer, 2003) and Chinese NomBank (Xue, 2006a), Xue and his colleagues (Xue and Palmer 2005; Xue 2006b; Xue, 2008) pioneered the exploration of Chinese verbal and nominal SRLs, given golden predicates. Among them, Xue and Palmer (2005) studied Chinese verbal SRL using Chinese PropBank and achieved the performance of 91.3 and 61.3 in F1-measure on golden and automatic syntactic parse trees, respectively. Xue (2006b) extended their study on Chinese nominal SRL and attempted to improve the performance of nominal SRL by simply inCorresponding author 1280 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280–1288, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP por"
D09-1133,W08-2121,0,\N,Missing
D09-1133,C98-1013,0,\N,Missing
D09-1149,J03-4003,0,0.0199954,"tances using 5-fold cross-validation. That is, these relation instances are first divided into 5 sets, then, one of them (about 860 instances) is used as the test data set, while the others are regarded as the training data set, from which the initial seed set is sampled. In the ACE 2003 corpus, the training set consists of 674 documents and 9683 positive relation instances while the test data consists of 97 documents and 1386 positive relation instances. The ACE RDC 2003 task defines 5 relation types and 24 subtypes between 5 entity types. The corpora are first parsed using Collins’s parser (Collins, 2003) with the boundaries of all the entity mentions kept. Then, the parse trees are converted into chunklink format using chunklink.pl 1. Finally, various useful lexical and syntactic features, as described in Subsection 3.1, are extracted and computed accordingly. For the purpose of comparison, we define our task as the classification of the 5 or 7 major relation types in the ACE RDC 2003 and 2004 corpora. For LIBSVM parameters, we adopted the polynomial kernel, and c is set to 10, g is set to 0.15. Under this setting, we achieved the best classification performance. 5.2 Experimental Results In t"
D09-1149,P06-1017,0,0.549966,"IST Automatic Content Extraction (ACE) program (ACE, 20002007), current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005). Current work on semantic relation extraction task mainly uses supervised learning methods, since it achieves relatively better performance. However this method requires a large amount of manually labeled relation instances, which is both time-consuming and laborious. In the contrast, unsupervised methods do not need definitions of relation types and hand-tagged data, but it is difficult to evaluate their performance since there are no criteria for evaluation. Therefore, semi-supervised learning has rece"
D09-1149,P04-1054,0,0.0956219,"Missing"
D09-1149,P04-1053,0,0.848952,"ity between patterns and relations to augment the target relation starting from a small sample. However, it only extracts simple relations such as (author, title) pairs from the WWW. Snowball (Agichtein and Gravano, 2000) is another bootstrappingbased system that extracts relations from unstructured text. Snowball shares much in common with DIPRE, including the use of both the bootstrapping framework and the pattern matching approach to extract new unlabeled instances. Due to pattern matching techniques, their systems are hard to be adapted to the general problem of relation extraction. Zhang (2004) approaches the relation classification problem with bootstrapping on top of SVM. He uses various lexical and syntactic features in the BootProject algorithm based on random feature projection to extract top-level relation types in the ACE corpus. Evaluation shows that bootstrapping can alleviate the burden of hand annotations for supervised learning methods to a certain extent. Chen et al. (2006) investigate a semisupervised learning algorithm based on label propagation for relation extraction, where labeled and unlabeled examples and their distances are represented as the nodes and the weigh"
D09-1149,A00-2030,0,0.0443339,"E) is such a technology that IE systems are expected to identify relevant information (usually of pre-defined types) from text documents in a certain domain and put them in a structured format. According to the scope of the NIST Automatic Content Extraction (ACE) program (ACE, 20002007), current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005). Current work on semantic relation extraction task mainly uses supervised learning methods, since it achieves relatively better performance. However this method requires a large amount of manually labeled relation instances, which is both time-consuming and laborious. In the contrast,"
D09-1149,C08-1088,1,0.938606,"7 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1437–1445, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP 100 instances), the imbalance of relation types or manifold structure (cluster structure) in it will severely weaken the strength of bootstrapping. Therefore, it is critical for a bootstrapping approach to select the most appropriate initial seed set. However, current systems (Zhang, 2004; Chen et al., 2006) use a randomly sampling strategy, which fails to explore the affinity nature among the training instances. Alternatively, Zhou et al. (2008) bootstrap a set of weighted support vectors from both labeled and unlabeled data using SVM. Nevertheless, the initial labeled data is still randomly generated only to ensure that there are at least 5 instances for every relation subtype. This paper presents a new approach to selecting the initial seed set based on stratified sampling strategy in the bootstrapping procedure for semi-supervised semantic relation classification. The motivation behind the stratified sampling is that every relation type should be as much as possible represented in the initial seed set, thus leading to more instanc"
D09-1149,P95-1026,0,0.10208,"ere are no criteria for evaluation. Therefore, semi-supervised learning has received more and more attention, as it can balance the advantages and disadvantages between supervised and unsupervised methods. With the plenitude of unlabeled natural language data at hand, semi-supervised learning can significantly reduce the need for labeled data with only limited sacrifice in performance. Specifically, a bootstrapping algorithm chooses the unlabeled instances with the highest probability of being correctly labeled and use them to augment labeled training data iteratively. Although previous work (Yarowsky, 1995; Blum and Mitchell, 1998; Abney, 2000; Zhang, 2004) has tackled the bootstrapping approach from both the theoretical and practical point of view, many key problems still remain unresolved, such as the selection of initial seed set. Since the size of the initial seed set is usually small (e.g. 1437 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1437–1445, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP 100 instances), the imbalance of relation types or manifold structure (cluster structure) in it will severely weaken the strength of bootstrapping"
D09-1149,W02-1010,0,0.0615167,"Missing"
D09-1149,P06-1104,1,0.85742,"ned types) from text documents in a certain domain and put them in a structured format. According to the scope of the NIST Automatic Content Extraction (ACE) program (ACE, 20002007), current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005). Current work on semantic relation extraction task mainly uses supervised learning methods, since it achieves relatively better performance. However this method requires a large amount of manually labeled relation instances, which is both time-consuming and laborious. In the contrast, unsupervised methods do not need definitions of relation types and hand-tagged data, but it is difficult"
D09-1149,I05-1034,1,0.893397,"as three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005). Current work on semantic relation extraction task mainly uses supervised learning methods, since it achieves relatively better performance. However this method requires a large amount of manually labeled relation instances, which is both time-consuming and laborious. In the contrast, unsupervised methods do not need definitions of relation types and hand-tagged data, but it is difficult to evaluate their performance since there are no criteria for evaluation. Therefore, semi-supervised learning has received more and more attention, as it can balance the advantages and disadvantages between s"
D09-1149,P05-1053,1,0.94391,"usually of pre-defined types) from text documents in a certain domain and put them in a structured format. According to the scope of the NIST Automatic Content Extraction (ACE) program (ACE, 20002007), current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005). Current work on semantic relation extraction task mainly uses supervised learning methods, since it achieves relatively better performance. However this method requires a large amount of manually labeled relation instances, which is both time-consuming and laborious. In the contrast, unsupervised methods do not need definitions of relation types and hand-tagged data,"
D09-1149,I08-1005,1,0.69481,"nt Extraction (ACE) program (ACE, 20002007), current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005). Current work on semantic relation extraction task mainly uses supervised learning methods, since it achieves relatively better performance. However this method requires a large amount of manually labeled relation instances, which is both time-consuming and laborious. In the contrast, unsupervised methods do not need definitions of relation types and hand-tagged data, but it is difficult to evaluate their performance since there are no criteria for evaluation. Therefore, semi-supervised learning has received more and more a"
D09-1149,P02-1046,0,\N,Missing
D10-1034,P01-1017,0,0.0208673,"Missing"
D10-1034,I05-2045,0,0.0239854,"ps between a pair of named entities occurring in a natural language text. Many machine learning approaches have been proposed to attack this problem, including supervised (Miller et al., 2000; Zelenko et al., 2003; Culotta and Soresen, 2004; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006; Zhou and Zhang, 2007; Zhou et al., 2007; Qian et al., 2008; Zhou et al., 2010), semisupervised (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Qian et al., 2009; Zhou et al., 2009), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005; Chen et al., 2005). Current work on relation extraction mainly adopts supervised learning methods, since they achieve much better performance. However, they normally require a large number of manually labeled relation instances, whose acquisition is both time consuming and labor intensive. In contrast, unsupervised methods do not need any manually labeled instances. Nevertheless, it is difficult to assess their performance due to the lack of evaluation criteria. As something between them, semisupervised learning has received more and more attention recently. With the plenitude of unlabeled natural language text"
D10-1034,P06-1017,0,0.609074,"ng the one with golden clustering. 1 Introduction Semantic relation extraction aims to detect and classify semantic relationships between a pair of named entities occurring in a natural language text. Many machine learning approaches have been proposed to attack this problem, including supervised (Miller et al., 2000; Zelenko et al., 2003; Culotta and Soresen, 2004; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006; Zhou and Zhang, 2007; Zhou et al., 2007; Qian et al., 2008; Zhou et al., 2010), semisupervised (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Qian et al., 2009; Zhou et al., 2009), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005; Chen et al., 2005). Current work on relation extraction mainly adopts supervised learning methods, since they achieve much better performance. However, they normally require a large number of manually labeled relation instances, whose acquisition is both time consuming and labor intensive. In contrast, unsupervised methods do not need any manually labeled instances. Nevertheless, it is difficult to assess their performance due to the lack of evaluation criteria. As something between th"
D10-1034,P04-1054,0,0.617501,"Missing"
D10-1034,A00-2030,0,0.0155531,"cular, we compare various clustering algorithms on the stratified bootstrapping performance. Experimental results on the ACE RDC 2004 corpus show that our clusteringbased stratified bootstrapping approach achieves the best F1-score of 75.9 on the subtask of semantic relation classification, approaching the one with golden clustering. 1 Introduction Semantic relation extraction aims to detect and classify semantic relationships between a pair of named entities occurring in a natural language text. Many machine learning approaches have been proposed to attack this problem, including supervised (Miller et al., 2000; Zelenko et al., 2003; Culotta and Soresen, 2004; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006; Zhou and Zhang, 2007; Zhou et al., 2007; Qian et al., 2008; Zhou et al., 2010), semisupervised (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Qian et al., 2009; Zhou et al., 2009), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005; Chen et al., 2005). Current work on relation extraction mainly adopts supervised learning methods, since they achieve much better performance. However, they normally require a large number of"
D10-1034,C08-1088,1,0.902674,"Missing"
D10-1034,D09-1149,1,0.576604,"den clustering. 1 Introduction Semantic relation extraction aims to detect and classify semantic relationships between a pair of named entities occurring in a natural language text. Many machine learning approaches have been proposed to attack this problem, including supervised (Miller et al., 2000; Zelenko et al., 2003; Culotta and Soresen, 2004; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006; Zhou and Zhang, 2007; Zhou et al., 2007; Qian et al., 2008; Zhou et al., 2010), semisupervised (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Qian et al., 2009; Zhou et al., 2009), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005; Chen et al., 2005). Current work on relation extraction mainly adopts supervised learning methods, since they achieve much better performance. However, they normally require a large number of manually labeled relation instances, whose acquisition is both time consuming and labor intensive. In contrast, unsupervised methods do not need any manually labeled instances. Nevertheless, it is difficult to assess their performance due to the lack of evaluation criteria. As something between them, semisupervised"
D10-1034,P04-1075,1,0.933102,"Missing"
D10-1034,P02-1016,0,0.25456,"ed learning for relation extraction, most of previous work construct the seed set either randomly (Zhang, 2004; Chen et al., 2006) or sequentially (Zhou et al., 2009). Qian et al. (2009) adopt a stratified sampling strategy to select the seed set. However, their method needs a stratification variable such as the known distribution of the relation types, while our method uses clustering to divide relation instances into different strata. In the literature, clustering techniques have been employed in active learning to sample representative seeds in a certain extent (Nguyen and Smeulders, 2004; Tang et al., 2002; Shen et al., 2004). Our work is similar to the formal framework, as proposed in Nguyen and Smeulders (2004), in which K-medoids clustering is incorporated into active learning. The cluster centers are used to construct a classifier and which in turn propagates classification decision to other examples via a local noise model. Unlike their probabilistic models, we apply various clustering algorithms together with intra-stratum sampling to select a seed set in discriminative models like SVMs. In active learning for syntactic parsing, Tang et al. (2002) employ a sampling strategy of “most uncer"
D10-1034,I05-1034,1,0.911681,"semantic relationships between a pair of named entities occurring in a natural language text. Many machine learning approaches have been proposed to attack this problem, including supervised (Miller et al., 2000; Zelenko et al., 2003; Culotta and Soresen, 2004; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006; Zhou and Zhang, 2007; Zhou et al., 2007; Qian et al., 2008; Zhou et al., 2010), semisupervised (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Qian et al., 2009; Zhou et al., 2009), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005; Chen et al., 2005). Current work on relation extraction mainly adopts supervised learning methods, since they achieve much better performance. However, they normally require a large number of manually labeled relation instances, whose acquisition is both time consuming and labor intensive. In contrast, unsupervised methods do not need any manually labeled instances. Nevertheless, it is difficult to assess their performance due to the lack of evaluation criteria. As something between them, semisupervised learning has received more and more attention recently. With the plenitude of unlabeled n"
D10-1034,P06-1104,1,0.921998,"04 corpus show that our clusteringbased stratified bootstrapping approach achieves the best F1-score of 75.9 on the subtask of semantic relation classification, approaching the one with golden clustering. 1 Introduction Semantic relation extraction aims to detect and classify semantic relationships between a pair of named entities occurring in a natural language text. Many machine learning approaches have been proposed to attack this problem, including supervised (Miller et al., 2000; Zelenko et al., 2003; Culotta and Soresen, 2004; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006; Zhou and Zhang, 2007; Zhou et al., 2007; Qian et al., 2008; Zhou et al., 2010), semisupervised (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Qian et al., 2009; Zhou et al., 2009), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005; Chen et al., 2005). Current work on relation extraction mainly adopts supervised learning methods, since they achieve much better performance. However, they normally require a large number of manually labeled relation instances, whose acquisition is both time consuming and labor intensive. In contrast, unsupervised meth"
D10-1034,P05-1052,0,0.305119,"ance. Experimental results on the ACE RDC 2004 corpus show that our clusteringbased stratified bootstrapping approach achieves the best F1-score of 75.9 on the subtask of semantic relation classification, approaching the one with golden clustering. 1 Introduction Semantic relation extraction aims to detect and classify semantic relationships between a pair of named entities occurring in a natural language text. Many machine learning approaches have been proposed to attack this problem, including supervised (Miller et al., 2000; Zelenko et al., 2003; Culotta and Soresen, 2004; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006; Zhou and Zhang, 2007; Zhou et al., 2007; Qian et al., 2008; Zhou et al., 2010), semisupervised (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Qian et al., 2009; Zhou et al., 2009), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005; Chen et al., 2005). Current work on relation extraction mainly adopts supervised learning methods, since they achieve much better performance. However, they normally require a large number of manually labeled relation instances, whose acquisition is both time consuming and labor in"
D10-1034,P05-1053,1,0.907305,"s on the ACE RDC 2004 corpus show that our clusteringbased stratified bootstrapping approach achieves the best F1-score of 75.9 on the subtask of semantic relation classification, approaching the one with golden clustering. 1 Introduction Semantic relation extraction aims to detect and classify semantic relationships between a pair of named entities occurring in a natural language text. Many machine learning approaches have been proposed to attack this problem, including supervised (Miller et al., 2000; Zelenko et al., 2003; Culotta and Soresen, 2004; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006; Zhou and Zhang, 2007; Zhou et al., 2007; Qian et al., 2008; Zhou et al., 2010), semisupervised (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Qian et al., 2009; Zhou et al., 2009), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005; Chen et al., 2005). Current work on relation extraction mainly adopts supervised learning methods, since they achieve much better performance. However, they normally require a large number of manually labeled relation instances, whose acquisition is both time consuming and labor intensive. In contras"
D10-1034,D07-1076,1,0.943138,"ratified bootstrapping approach achieves the best F1-score of 75.9 on the subtask of semantic relation classification, approaching the one with golden clustering. 1 Introduction Semantic relation extraction aims to detect and classify semantic relationships between a pair of named entities occurring in a natural language text. Many machine learning approaches have been proposed to attack this problem, including supervised (Miller et al., 2000; Zelenko et al., 2003; Culotta and Soresen, 2004; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006; Zhou and Zhang, 2007; Zhou et al., 2007; Qian et al., 2008; Zhou et al., 2010), semisupervised (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Qian et al., 2009; Zhou et al., 2009), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005; Chen et al., 2005). Current work on relation extraction mainly adopts supervised learning methods, since they achieve much better performance. However, they normally require a large number of manually labeled relation instances, whose acquisition is both time consuming and labor intensive. In contrast, unsupervised methods do not need any manually labeled inst"
D10-1034,P04-1053,0,\N,Missing
D10-1034,P02-1046,0,\N,Missing
D10-1070,W05-0620,0,0.319338,"n evaluated on negation and speculation scope learning and can be easily ported to other scope learning tasks, it ignores syntactic information and suffers from low performance. Alternatively, even if the rule-based methods may be effective for a special scope learning task (e.g., speculation scope learning), it is not readily adoptable to other scope learning tasks (e.g., negation scope learning). Instead, this paper explores scope learning from parse tree perspective and formulates it as a simplified shallow semantic parsing problem, which has been extensively studied in the past few years (Carreras and Màrquez, 2005). In particular, the cue is recast as the predicate and the scope is recast as the arguments of the cue. The motivation behind is that the structured syntactic information plays a critical role in scope learning and should be paid much more attention, as indicated by previous studies in shallow semantic parsing (Gildea and Palmer, 2002; Punyakanok et al., 2005). Although our approach is evaluated only on negation and speculation scope learning here, it is portable to other kinds of scope learning. The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 introdu"
D10-1070,P02-1031,0,0.0103291,"other scope learning tasks (e.g., negation scope learning). Instead, this paper explores scope learning from parse tree perspective and formulates it as a simplified shallow semantic parsing problem, which has been extensively studied in the past few years (Carreras and Màrquez, 2005). In particular, the cue is recast as the predicate and the scope is recast as the arguments of the cue. The motivation behind is that the structured syntactic information plays a critical role in scope learning and should be paid much more attention, as indicated by previous studies in shallow semantic parsing (Gildea and Palmer, 2002; Punyakanok et al., 2005). Although our approach is evaluated only on negation and speculation scope learning here, it is portable to other kinds of scope learning. The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 introduces the Bioscope corpus on which our approach is evaluated. Section 4 describes our parsing approach by formulating scope learning as a simplified shallow semantic parsing problem. Section 5 presents the experimental results. Finally, Section 6 concludes the work. 715 2 Related Work Most of the previous research on scope learning falls"
D10-1070,W06-1617,0,0.0962286,"Missing"
D10-1070,W02-1006,0,0.0211184,"ght et al., 2004); 2) machine learning approaches, which train a classifier with either supervised or semisupervised learning methods (e.g., Özgür and Radev, 2009; Szarvas, 2008). Without loss of generality, we adopt a machine learning approach and train a classifier with supervised learning. In particular, we make an independent classification for each word with a BIO label to indicate whether it 719 is the first word of a cue, inside a cue, or outside of it, respectively. Inspired by previous studies on similar tasks such as WSD and nominal predicate recognition in shallow semantic parsing (Lee and Ng, 2002; Li et al., 2009), where various features on the word itself, surrounding words and syntactic information have been successfully used, we believe that such information is also valuable to automatic recognition of cues. Table 3 shows the features employed for cue recognition. In particular, we categorize these features into three groups: 1) features about the cue candidate itself (CC in short); 2) features about surrounding words (SW in short); and 3) structural features derived from the syntactic parse tree (SF in short). Feature Remarks Cue Candidate (CC) related CC1 The cue candidate itself"
D10-1070,P07-1125,0,0.183215,"’2010 shared task (Farkas et al., 2010) aims to detect uncertain information in resolving the scopes of speculation cues. Most of the initial research in this literature focused on either recognizing negated terms or identifying speculative sentences, using some heuristic Corresponding author 714 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 714–724, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics rules (Chapman et al., 2001; Light et al., 2004), and machine learning methods (Goldin and Chapman, 2003; Medlock and Briscoe, 2007). However, scope learning has been largely ignored until the recent release of the BioScope corpus (Szarvas et al., 2008), where negation/speculation cues and their scopes are annotated explicitly. Morante et al. (2008) and Morante and Daelemans (2009a & 2009b) pioneered the research on scope learning by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a cue. Alternatively, Özgür and Radev (2009) and Øvrelid et al. (2010) defined heuristic rules for speculation scope learning from constituency and dependency parse tree persp"
D10-1070,C10-1155,0,0.068977,"(Chapman et al., 2001; Light et al., 2004), and machine learning methods (Goldin and Chapman, 2003; Medlock and Briscoe, 2007). However, scope learning has been largely ignored until the recent release of the BioScope corpus (Szarvas et al., 2008), where negation/speculation cues and their scopes are annotated explicitly. Morante et al. (2008) and Morante and Daelemans (2009a & 2009b) pioneered the research on scope learning by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a cue. Alternatively, Özgür and Radev (2009) and Øvrelid et al. (2010) defined heuristic rules for speculation scope learning from constituency and dependency parse tree perspectives, respectively. Although the chunking approach has been evaluated on negation and speculation scope learning and can be easily ported to other scope learning tasks, it ignores syntactic information and suffers from low performance. Alternatively, even if the rule-based methods may be effective for a special scope learning task (e.g., speculation scope learning), it is not readily adoptable to other scope learning tasks (e.g., negation scope learning). Instead, this paper explores sco"
D10-1070,D09-1145,0,0.37542,"tational Linguistics rules (Chapman et al., 2001; Light et al., 2004), and machine learning methods (Goldin and Chapman, 2003; Medlock and Briscoe, 2007). However, scope learning has been largely ignored until the recent release of the BioScope corpus (Szarvas et al., 2008), where negation/speculation cues and their scopes are annotated explicitly. Morante et al. (2008) and Morante and Daelemans (2009a & 2009b) pioneered the research on scope learning by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a cue. Alternatively, Özgür and Radev (2009) and Øvrelid et al. (2010) defined heuristic rules for speculation scope learning from constituency and dependency parse tree perspectives, respectively. Although the chunking approach has been evaluated on negation and speculation scope learning and can be easily ported to other scope learning tasks, it ignores syntactic information and suffers from low performance. Alternatively, even if the rule-based methods may be effective for a special scope learning task (e.g., speculation scope learning), it is not readily adoptable to other scope learning tasks (e.g., negation scope learning). Instea"
D10-1070,N07-1051,0,0.0292449,"thus share some common characteristics in statistics, such as the number of words in the negation/speculation scope to the right (or left) of the negation/speculation cue and the average scope length. In comparison, the clinical reports subcorpus consists of clinical radiology reports with short sentences. For detailed statistics and annotation 1 http://www.inf.u-szeged.hu/rgai/bioscope 716 guidelines about the three subcorpora, please see Morante and Daelemans (2009a & 2009b). For preprocessing, all the sentences in the Bioscope corpus are tokenized and then parsed using the Berkeley parser (Petrov and Klein, 2007) 2 trained on the GENIA TreeBank (GTB) 1.0 (Tateisi et al., 2005) 3 , which is a bracketed corpus in (almost) PTB style. 10-fold cross-validation on GTB1.0 shows that the parser achieves the performance of 86.57 in F1-measure. It is worth noting that the GTB1.0 corpus includes all the sentences in the abstracts subcorpus of the Bioscope corpus. 4 Scope Learning via Simplified Shallow Semantic Parsing In this section, we first formulate the scope learning task as a simplified shallow semantic parsing problem. Then, we deal with it using a simplified shallow semantic parsing framework. 4.1 Formu"
D10-1070,W05-0639,0,0.0529931,"Missing"
D10-1070,P08-1033,0,0.0746707,"boundary. k k * = arg max ∏ Pi ∗ k i =1 m ∏ (1 − Pi ) i = k +1 Similarly, the right boundary of the given cue can be decided. 4.5 Cue Recognition Automatic recognition of cues of a special interest is the prerequisite for a scope learning system. The approaches to recognizing cues of a special interest usually fall into two categories: 1) substring matching approaches, which require a set of cue words or phrases in advance (e.g., Light et al., 2004); 2) machine learning approaches, which train a classifier with either supervised or semisupervised learning methods (e.g., Özgür and Radev, 2009; Szarvas, 2008). Without loss of generality, we adopt a machine learning approach and train a classifier with supervised learning. In particular, we make an independent classification for each word with a BIO label to indicate whether it 719 is the first word of a cue, inside a cue, or outside of it, respectively. Inspired by previous studies on similar tasks such as WSD and nominal predicate recognition in shallow semantic parsing (Lee and Ng, 2002; Li et al., 2009), where various features on the word itself, surrounding words and syntactic information have been successfully used, we believe that such infor"
D10-1070,W08-0606,0,0.20885,"of the initial research in this literature focused on either recognizing negated terms or identifying speculative sentences, using some heuristic Corresponding author 714 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 714–724, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics rules (Chapman et al., 2001; Light et al., 2004), and machine learning methods (Goldin and Chapman, 2003; Medlock and Briscoe, 2007). However, scope learning has been largely ignored until the recent release of the BioScope corpus (Szarvas et al., 2008), where negation/speculation cues and their scopes are annotated explicitly. Morante et al. (2008) and Morante and Daelemans (2009a & 2009b) pioneered the research on scope learning by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a cue. Alternatively, Özgür and Radev (2009) and Øvrelid et al. (2010) defined heuristic rules for speculation scope learning from constituency and dependency parse tree perspectives, respectively. Although the chunking approach has been evaluated on negation and speculation scope learning and c"
D10-1070,P02-1053,0,0.00400064,"Missing"
D10-1070,W04-3212,0,0.0195919,"ic label, scope identification does not involve semantic label classification and thus could be divided into three consequent phases: argument pruning, argument identification and postprocessing. 4.2 Argument Pruning Similar to the predicate-argument structures in common shallow semantic parsing, the cue-scope structures in scope learning can be also classified into several certain types and argument pruning can be done by employing several heuristic rules accordingly to filter out constituents, which are most likely non-arguments of a given cue. Similar to the heuristic algorithm proposed in Xue and Palmer (2004) for argument pruning in common shallow semantic parsing, the argument pruning algorithm adopted here starts from designating the cue node as the current node and collects its siblings. It then iteratively moves one level up to the parent of the current node and collects its siblings. The algorithm ends when it reaches the root of the parse tree. To sum up, except the cue node itself and its ancestral constituents, any constituent in the parse tree whose parent covers the given cue will be collected as argument candidates. Taking the negation cue node “RB7,7” in Figure 2 as an example, constit"
D10-1070,I05-2038,0,\N,Missing
D10-1070,D08-1075,0,\N,Missing
D10-1070,J08-2004,0,\N,Missing
D10-1070,W09-1105,0,\N,Missing
D10-1070,W10-3001,0,\N,Missing
D10-1070,D09-1133,1,\N,Missing
D10-1070,E99-1043,0,\N,Missing
D10-1070,W09-1304,0,\N,Missing
D10-1070,W04-3103,0,\N,Missing
D10-1086,P00-1022,0,0.0676614,"However, this model needed case frames to detect zero anaphors and a largescale corpus to construct these case frames automatically. For Japanese zero anaphora, we do not see any reports about zero anaphora categories. Moreover, all the above related works we can find on Japanese zero anaphora resolution ignore zero anaphor detection, focusing on either anaphoricity determination or antecedent identification. Maybe, it is easy to detect zero anaphors in Japanese. However, it is out of the scope of our knowledge and this paper. Zero anaphora resolution in Spanish As the only work we can find, Ferrandez and Peral (2000) proposed a hand-engineered rule-based method for both anaphoricity determination and antecedent identification. That is, they ignored zero anaphor detection. Besides, they only dealt with zero anaphors that were in the subject position. 2.2 necessary structural information in the parse tree for anaphora resolution of pronouns and the context-sensitive convolution tree kernel much outperformed other tree kernels. Tree kernel-based anaphora resolution Although there is no research on tree kernel-based zero anaphora resolution in the literature, tree kernel-based methods have been explored in tr"
D10-1086,P06-1079,0,0.162423,"the sub-tasks of anaphoric identification and antecedent identification with the help of a verb dictionary. They did not perform zero anaphor detection, assuming the availability of golden zero anaphors. Besides, their model needed a largescale corpus to estimate the probabilities to prevent them from the data sparseness problem. Isozaki and Hirao (2003) explored some ranking rules and a machine learning method on zero anaphora resolution. However, they assumed that zero anaphors were already detected and each zero anaphor’s grammatical case was already determined by a zero anaphor detector. Iida et al. (2006) explored a machine learning method for the sub-task of antecedent identification using rich syntactic pattern features, assuming the availability of golden anaphoric zero anaphors. Sasano et al. (2008) proposed a fully-lexicalized probabilistic model for zero anaphora resolution, which estimated case assignments for the overt case components and the antecedents of zero anaphors simultaneously. However, this model needed case frames to detect zero anaphors and a largescale corpus to construct these case frames automatically. For Japanese zero anaphora, we do not see any reports about zero anap"
D10-1086,P04-1043,0,0.0333246,"hough there is no research on tree kernel-based zero anaphora resolution in the literature, tree kernel-based methods have been explored in traditional anaphora resolution to certain extent and achieved comparable performance with the dominated feature-based ones. One main advantage of kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structural information in various NLP techniques and applications, such as syntactic parsing (Collins and Duffy 2001; Moschitti 2004), semantic relation extraction (Zelenko et al. 2003; Zhao and Grishman 2005; Zhou et al. 2007; Qian et al. 2008), and semantic role labeling (Moschitti 2004). Representative works in tree kernel-based anaphora resolution include Yang et al. (2006) and Zhou et al (2008). Yang et al. (2006) employed a convolution tree kernel on anaphora resolution of pronouns. In particular, a document-level syntactic parse tree for an entire text was constructed by attaching the parse trees of all its sentences to a newadded upper node. Examination of three parse tree structures using different construction sch"
D10-1086,C08-1088,1,0.852049,"methods have been explored in traditional anaphora resolution to certain extent and achieved comparable performance with the dominated feature-based ones. One main advantage of kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structural information in various NLP techniques and applications, such as syntactic parsing (Collins and Duffy 2001; Moschitti 2004), semantic relation extraction (Zelenko et al. 2003; Zhao and Grishman 2005; Zhou et al. 2007; Qian et al. 2008), and semantic role labeling (Moschitti 2004). Representative works in tree kernel-based anaphora resolution include Yang et al. (2006) and Zhou et al (2008). Yang et al. (2006) employed a convolution tree kernel on anaphora resolution of pronouns. In particular, a document-level syntactic parse tree for an entire text was constructed by attaching the parse trees of all its sentences to a newadded upper node. Examination of three parse tree structures using different construction schemes (Min-Expansion, Simple-Expansion and FullExpansion) on the ACE 2003 corpus showed promising results. Howeve"
D10-1086,C02-1078,0,0.569025,"anaphors with explicit noun phrase referents and discarded those with split an883 tecedents or referring to events. Moreover, they focused on the sub-tasks of anaphoricity determination and antecedent identification. For zero anaphor detection, a simple heuristic rule was employed. Although this rule can recover almost all the zero anaphors, it suffers from very low precision by introducing too many false zero anaphors and thus leads to low performance in anaphoricity determination, much due to the imbalance between positive and negative training examples. Zero anaphora resolution in Japanese Seki et al. (2002) proposed a probabilistic model for the sub-tasks of anaphoric identification and antecedent identification with the help of a verb dictionary. They did not perform zero anaphor detection, assuming the availability of golden zero anaphors. Besides, their model needed a largescale corpus to estimate the probabilities to prevent them from the data sparseness problem. Isozaki and Hirao (2003) explored some ranking rules and a machine learning method on zero anaphora resolution. However, they assumed that zero anaphors were already detected and each zero anaphor’s grammatical case was already dete"
D10-1086,C08-1097,0,0.541127,"Missing"
D10-1086,P02-1014,0,0.121552,"Missing"
D10-1086,P06-1006,0,0.016264,"feature-based ones. One main advantage of kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structural information in various NLP techniques and applications, such as syntactic parsing (Collins and Duffy 2001; Moschitti 2004), semantic relation extraction (Zelenko et al. 2003; Zhao and Grishman 2005; Zhou et al. 2007; Qian et al. 2008), and semantic role labeling (Moschitti 2004). Representative works in tree kernel-based anaphora resolution include Yang et al. (2006) and Zhou et al (2008). Yang et al. (2006) employed a convolution tree kernel on anaphora resolution of pronouns. In particular, a document-level syntactic parse tree for an entire text was constructed by attaching the parse trees of all its sentences to a newadded upper node. Examination of three parse tree structures using different construction schemes (Min-Expansion, Simple-Expansion and FullExpansion) on the ACE 2003 corpus showed promising results. However, among the three constructed parse tree structures, there exists no obvious overwhelming one, which can well cover structured syntact"
D10-1086,P05-1052,0,0.0177348,"tion in the literature, tree kernel-based methods have been explored in traditional anaphora resolution to certain extent and achieved comparable performance with the dominated feature-based ones. One main advantage of kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structural information in various NLP techniques and applications, such as syntactic parsing (Collins and Duffy 2001; Moschitti 2004), semantic relation extraction (Zelenko et al. 2003; Zhao and Grishman 2005; Zhou et al. 2007; Qian et al. 2008), and semantic role labeling (Moschitti 2004). Representative works in tree kernel-based anaphora resolution include Yang et al. (2006) and Zhou et al (2008). Yang et al. (2006) employed a convolution tree kernel on anaphora resolution of pronouns. In particular, a document-level syntactic parse tree for an entire text was constructed by attaching the parse trees of all its sentences to a newadded upper node. Examination of three parse tree structures using different construction schemes (Min-Expansion, Simple-Expansion and FullExpansion) on the ACE 2003 co"
D10-1086,I08-1004,1,0.948154,"e main advantage of kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structural information in various NLP techniques and applications, such as syntactic parsing (Collins and Duffy 2001; Moschitti 2004), semantic relation extraction (Zelenko et al. 2003; Zhao and Grishman 2005; Zhou et al. 2007; Qian et al. 2008), and semantic role labeling (Moschitti 2004). Representative works in tree kernel-based anaphora resolution include Yang et al. (2006) and Zhou et al (2008). Yang et al. (2006) employed a convolution tree kernel on anaphora resolution of pronouns. In particular, a document-level syntactic parse tree for an entire text was constructed by attaching the parse trees of all its sentences to a newadded upper node. Examination of three parse tree structures using different construction schemes (Min-Expansion, Simple-Expansion and FullExpansion) on the ACE 2003 corpus showed promising results. However, among the three constructed parse tree structures, there exists no obvious overwhelming one, which can well cover structured syntactic information. One pr"
D10-1086,D07-1076,1,0.648721,"Missing"
D10-1086,D09-1103,1,\N,Missing
D10-1086,W03-1024,0,\N,Missing
D10-1086,D07-1057,0,\N,Missing
D10-1086,J08-3002,0,\N,Missing
D10-1086,J01-4004,0,\N,Missing
D10-1086,P03-1023,1,\N,Missing
D11-1084,2002.tmi-tmiw.2,0,0.00920517,"ation in above three kinds of caches. Evaluation shows the effectiveness of our cache-based approach to document-level translation with the performance improvement of 0.81 in BLUE score over Moses. Especially, detailed analysis and discussion are presented to give new insights to document-level translation. 1 Introduction During last decade, tremendous work has been done to improve the quality of statistical machine __________________ * Corresponding author. translation (SMT) systems. However, there is still a huge performance gap between the state-of-theart SMT systems and human translators. Bond (2002) suggested nine ways to improve machine translation by imitating the best practices of human translators (Nida, 1964), with parsing the entire document before translation as the first priority. However, most SMT systems still treat parallel corpora as a list of independent sentence-pairs and ignore document-level information. Document-level information can and should be used to help document-level machine translation. At least, the topic of a document can help choose specific translation candidates, since when taken out of the context from their document, some words, phrases and even sentences"
D11-1084,2005.eamt-1.19,0,0.0713323,"Missing"
D11-1084,N03-1017,0,0.118031,"Missing"
D11-1084,W04-3250,0,0.561859,"Missing"
D11-1084,D07-1036,0,0.0639858,"Missing"
D11-1084,W02-1018,0,0.0730207,"Missing"
D11-1084,W04-3225,0,0.0279222,"e is analogous to “cache memory” in hardware terminology, which tracks short-term fluctuation (Iyer et al., 1999). As the cache changes with different documents, the documentlevel information should be capable of influencing SMT. Previous cache-based approaches mainly point to cache-based language modeling (Kuhn and Mori, 1990), which uses a large global language model to mix with a small local model estimated from recent history data. However, applying such a language model in SMT is very difficult due to the risk of introducing extra noise (Raab, 2007). For cache-based translation modeling, Nepveu et al. (2004) explored user-edited translations in the context of interactive machine translation. Tiedemann (2010) proposed to fill the cache with bilingual phrase pairs from the best translation hypotheses of previous sentences in the test document. Both Nepveu et al. (2004) and Tiedemann (2010) also explored traditional cache-based language models and found that a cache-based language model often contributes much more than a cache-based translation model. 3 Cache-based document-level SMT Given a test document, our system works as follows: 1) clears the static, topic and dynamic caches when switching to"
D11-1084,P03-1021,0,0.102446,"translation candidates, since when taken out of the context from their document, some words, phrases and even sentences may be rather ambiguous and thus difficult to understand. Another advantage of document-level machine translation is its ability in keeping a consistent translation. However, document-level translation has drawn little attention from the SMT research community. The reasons are manifold. First of all, most of parallel corpora lack the annotation of document boundaries (Tam, 2007). Secondly, although it is easy to incorporate a new feature into the classical log-linear model (Och, 2003), it is difficult to capture document-level information and model it via some simple features. Thirdly, reference translations of a test document written by human translators tend to have flexible expressions in order to avoid producing monotonous texts. This makes the evaluation of document-level SMT systems extremely difficult. Tiedemann (2010) showed that the repetition and consistency are very important when modeling natural language and translation. He proposed to employ cache-based language and translation models in a phrase-based SMT system for domain 909 Proceedings of the 2011 Confere"
D11-1084,W09-2404,0,0.354359,"employing an adaptive language model with the advantage of avoiding the interpolation of a global language model with a specific domain language model. The rest of this paper is organized as follows. Section 2 reviews the related work. Section 3 presents our cache-based approach to documentlevel SMT. Section 4 presents the experimental results. Session 5 gives new insights on cachebased document-level translation. Finally, we conclude this paper in Section 6. 2 Related work There are only a few studies on document-level SMT. Representative work includes Zhao et al. (2006), Tam et al. (2007), Carpuat (2009). Zhao et al. (2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT. Tam et al. (2007) proposed a bilingual-LSA model on the basis of a parallel document corpus and built a topic-based language model for each language. By automatically building the correspondence between the source and target language models, this me"
D11-1084,P00-1056,0,0.134938,"Missing"
D11-1084,D08-1033,0,0.0156691,"Missing"
D11-1084,P02-1040,0,0.0870566,"ords extracted from target-side documents 4 Experimentation We have systematically evaluated our cache-based approach to document-level SMT on the ChineseEnglish translation task. 4.1 Experimental Setting Here, we use SRI language modeling toolkit to train a trigram general language model on English newswire text, mostly from the Xinhua portion of the Gigaword corpus (2007) and performed word alignment on the training parallel corpus using GIZA++(Och and Ney,2000) in two directions. For evaluation, the NIST BLEU script (version 13) with the default setting is used to calculate the Bleu score (Papineni et al. 2002), which measures case-insensitive matching of n-grams with n up to 4. To see whether an improvement is statistically significant, we also conduct significance tests using the paired bootstrap approach (Koehn, 2004)2. In this paper, ‘***’, ‘**’, and ‘*’ denote p-values less than or equal to 0.01, in-between (0.01, 0.05), and bigger than 0.05, which mean significantly better, moderately better and slightly better, respectively. 2 http://www.ark.cs.cmu.edu/MT 914 In this paper, we use FBIS as the training data, the 2003 NIST MT evaluation test data as the development data, and the 2005 NIST MT te"
D11-1084,H92-1045,0,0.580187,"topic-specific bilingual translation model. It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT. Tam et al. (2007) proposed a bilingual-LSA model on the basis of a parallel document corpus and built a topic-based language model for each language. By automatically building the correspondence between the source and target language models, this method can match the topic-based language model and improve the performance of SMT. Carpuat (2009) revisited the “one sense per discourse” hypothesis of Gale et al. (1992) and gave a detailed comparison and analysis of the “one translation per discourse” hypothesis. However, she failed to propose an effective way to integrate document-level information into a SMT system. For example, she simply recommended some translation candidates to replace some target words in the post-process stage. In principle, the cache-based approach can be well suited for document-level translation. Basically, the cache is analogous to “cache memory” in hardware terminology, which tracks short-term fluctuation (Iyer et al., 1999). As the cache changes with different documents, the do"
D11-1084,W10-2602,0,0.776373,"tention from the SMT research community. The reasons are manifold. First of all, most of parallel corpora lack the annotation of document boundaries (Tam, 2007). Secondly, although it is easy to incorporate a new feature into the classical log-linear model (Och, 2003), it is difficult to capture document-level information and model it via some simple features. Thirdly, reference translations of a test document written by human translators tend to have flexible expressions in order to avoid producing monotonous texts. This makes the evaluation of document-level SMT systems extremely difficult. Tiedemann (2010) showed that the repetition and consistency are very important when modeling natural language and translation. He proposed to employ cache-based language and translation models in a phrase-based SMT system for domain 909 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 909–919, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics adaptation. Especially, the cache in the translation model dynamically grows up by adding bilingual phrase pairs from the best translation hypotheses of previous sentences. One problem"
D11-1084,P10-1049,0,0.0175437,"Missing"
D11-1084,C04-1059,0,0.0672583,"Missing"
D11-1084,J93-2003,0,\N,Missing
D11-1084,P06-2124,0,\N,Missing
D12-1013,P11-1013,0,0.0126695,"lts. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 140 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanced selection strategy in co-training would generate many errors in late iterations due to the imbalanced nature of the"
D12-1013,P09-1083,0,0.0167307,"nwhile, automatically label most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1 1 Introduction Sentiment classification is the task of identifying the sentiment polarity (e.g., positive or negative) of ‡ a natural language text towards a given topic (Pang et al., 2002; Turney, 2002) and has become the core component of many important applications in opinion analysis (Cui et al., 2006; Li et al., 2009; Lloret et al., 2009; Zhang and Ye, 2008). Most of previous studies in sentiment classification focus on learning models from a large number of labeled data. However, in many real-world applications, manual annotation is expensive and time-consuming. In these situations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active learning for imbalanced sentiment classification faces some unique challenges. As a specific type of sentiment classification, imbalance"
D12-1013,P08-2065,1,0.839243,"e experimental results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 140 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanced selection strategy in co-training would generate many errors in late iterations due to the imbala"
D12-1013,P10-1043,1,0.760833,"e learning approach for imbalanced sentiment classification. Section 4 reports the experimental results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 140 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanced selection strat"
D12-1013,P07-1056,0,0.296743,". Section 4 reports the experimental results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 140 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanced selection strategy in co-training would generate many errors in late iteration"
D12-1013,N09-3013,0,0.012429,"ally label most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1 1 Introduction Sentiment classification is the task of identifying the sentiment polarity (e.g., positive or negative) of ‡ a natural language text towards a given topic (Pang et al., 2002; Turney, 2002) and has become the core component of many important applications in opinion analysis (Cui et al., 2006; Li et al., 2009; Lloret et al., 2009; Zhang and Ye, 2008). Most of previous studies in sentiment classification focus on learning models from a large number of labeled data. However, in many real-world applications, manual annotation is expensive and time-consuming. In these situations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active learning for imbalanced sentiment classification faces some unique challenges. As a specific type of sentiment classification, imbalanced sentiment classific"
D12-1013,W02-1011,0,0.0234829,"rmative minority-class samples for manual annotation by leveraging a certainty measurement and an uncertainty measurement, and in the meanwhile, automatically label most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1 1 Introduction Sentiment classification is the task of identifying the sentiment polarity (e.g., positive or negative) of ‡ a natural language text towards a given topic (Pang et al., 2002; Turney, 2002) and has become the core component of many important applications in opinion analysis (Cui et al., 2006; Li et al., 2009; Lloret et al., 2009; Zhang and Ye, 2008). Most of previous studies in sentiment classification focus on learning models from a large number of labeled data. However, in many real-world applications, manual annotation is expensive and time-consuming. In these situations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active l"
D12-1013,P02-1053,0,0.00907258,"ass samples for manual annotation by leveraging a certainty measurement and an uncertainty measurement, and in the meanwhile, automatically label most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1 1 Introduction Sentiment classification is the task of identifying the sentiment polarity (e.g., positive or negative) of ‡ a natural language text towards a given topic (Pang et al., 2002; Turney, 2002) and has become the core component of many important applications in opinion analysis (Cui et al., 2006; Li et al., 2009; Lloret et al., 2009; Zhang and Ye, 2008). Most of previous studies in sentiment classification focus on learning models from a large number of labeled data. However, in many real-world applications, manual annotation is expensive and time-consuming. In these situations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active learning for imb"
D12-1013,P09-1027,0,0.0155781,"s our active learning approach for imbalanced sentiment classification. Section 4 reports the experimental results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 140 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanc"
D12-1013,D07-1082,0,0.265593,"les which have the largest disagreement among several committee classifiers. Besides query by committee (QBC) as the first of such type (Freund et al., 1997), co-testing learns a committee of member classifiers from different views and selects those contention points (i.e., unlabeled examples on which the views predict different labels) for manual annotation (Muslea et al., 2006). However, most previous studies focus on the scenario of balanced class distribution and only a few recent studies address the active learning issue on imbalanced classification problems including Yang and Ma (2010), Zhu and Hovy (2007), Ertekin et al. (2007a) and Ertekin et al. (2007b)2. Unfortunately, they straightly adopt the uncertainty sampling as the active selection strategy to address active learning in imbalanced classification, which completely ignores the class imbalance problem in the selected samples. Attenberg and Provost (2010) highlights the importance of selecting samples by considering the proportion of the classes. Their simulation experiment on text categorization confirms that selecting class-balanced samples is more important than traditional active selection strategies like uncertainty. However, the pr"
D12-1026,vilar-etal-2006-error,0,0.0507574,"in a sentence and thus supervise SMT to reduce tense inconsistency errors against Observations (1) and (2) in the sentence-level. In comparison, Observation (3) actually reflects the tense distributions among one document. After extracting each main tense for each sentence, we build another tense ngram model in the document-level. For clarity, this paper denotes document-level tense as “inter-tense” and sentence-level tense as “intra-tense”. After that, we propose to integrate such tense models into SMT systems in a dynamic way. It is well known there are many errors in the current MT output (David et al., 2006). Unlike previously making trouble with reference texts, the BLEU-4 score cannot be influenced obviously by modifying a small part of abnormal sentences in a static way. In our system, both inter-tense and intra-tense model are integrated into a SMT system via additional features and thus can supervise the decoding procedure. During decoding, once some words with correct tense can be determined, with the help of language model and other related features, the small component–“tense”–can affect surrounding words and improve the performance of the whole sentence. Our experimental results (see the"
D12-1026,P92-1033,0,0.823893,"Missing"
D12-1026,D11-1084,1,0.858213,"ure. During decoding, once some words with correct tense can be determined, with the help of language model and other related features, the small component–“tense”–can affect surrounding words and improve the performance of the whole sentence. Our experimental results (see the examples in Sec277 tion 6.4) show the effectiveness of this way. Rather than the rule-based model, our models are fully statistical-based. So they can be easily scaled up and integrated into either phrase-based or syntaxbased SMT systems. In this paper, we employ a strong phrase-based SMT baseline system, as proposed in Gong et al. (2011), which uses document as translation unit, for better incorporating documentlevel information. The rest of this paper is organized as follows: Section 2 reviews the related work. Section 3 and 4 are related to tense models. Section 3 describes the preprocessing work for building tense models. Section 4 presents how to build target-side tense models and discuss their characteristics. Section 5 shows our way of integrating such tense models into a SMT system. Session 6 gives the experimental results. Finally, we conclude this paper in Section 7. 2 Related Work In this section, we focus on relate"
D12-1026,N03-1017,0,0.125285,"Missing"
D12-1026,W04-3250,0,0.0420808,"tal Setting for SMT In our experiment, SRI language modeling toolkit was used to train a 5-Gram general language model on the Xinhua portion of the Gigaword corpus. Word alignment was performed on the training parallel corpus using GIZA++ ( Och and Ney, 2000) in two directions. For evaluation, the NIST BLEU script (version 13) with the default setting is used to calculate the BLEU score (Papineni et al., 2002), which measures case-insensitive matching of 4-grams. To see whether an improvement is statistically significant, we also conduct significance tests using the paired bootstrap approach (Koehn, 2004). In this paper, “***” and “**” denote p-values equal to 0.05, and bigger than 0.05, which mean significantly better, moderately better respectively. Role Train Dev Test Corpus Name FBIS NIST2003 NIST2005 Sentences Documents 228455 919 1082 10000 100 100 6.3 Experimental Results All the experiment results are showed on the table 3. Our Baseline is a modified Moses. The major modification is input and output module in order to translate using document as unit. The performance of our baseline exceeds the baseline reported by Gong et al. (2011) about 2 percent based on the similar training and te"
D12-1026,I11-1125,0,0.141759,"nd subordinate clauses connected with some special temporal marker words, such as “after” and “before”, and employed them in temporal inference. Another typical task is cross-lingual tense predication. Some languages, such as English, are inflectional, whose verbs can express tense via certain stems or suffix, while others, such as Chinese often lack inflectional forms. Take Chinese to English translation as example, if Chinese text contains particle word “ (Le)”, the nearest Chinese verb prefers to be translated into English verb with the past tense. Ye and Zhang (2005), Ye et al. (2007) and Liu et al. (2011) focus on labeling the tenses for keywords in source-side language. 3 Ye and Zhang (2005) first built a small amount of manually-labeled data, which provide the tense mapping from Chinese text to English text. Then, they trained a CRF-based tense classifier to label tense on Chinese documents. Ye et al. (2007) further reported that syntactic features contribute most to the marking of aspectual information. Liu et al. (2011) proposed a parallel mapping method to automatically generate annotated data. In particular, they used English verbs to label tense information for Chinese verbs via a paral"
D12-1026,P00-1056,0,0.0454932,"s into SMT In this section, we discuss how to integrate the previous tense models into a SMT system. 5.1 Basic phrase-based SMT system It is well known that the translation process of SMT can be modeled as obtaining the best translation e of the source sentence f by maximizing following posterior probability(Brown et al., 1993): ebest = arg max P (e|f ) e = arg max P (f |e)Plm (e) (2) e where P (e|f ) is a translation model and Plm is a language model. Our baseline is a modified Moses, which follows Koehn et al. (2003) and adopts similar six groups of features. Besides, the log-linear model ( Och and Ney, 2000) is employed to linearly interpolate these features for obtaining the best translation according to the formula 3: ebest = arg max e M X λm hm (e, f ) (3) m=1 where hm (e, f ) is a feature function, and λm is the weight of hm (e, f ) optimized by a discriminative training method on a held-out development data(Och, 2003). 5.2 first obtains tense sequence for such hypothesis and computes intra-tense feature Fs (see Section 5.3). At the same time, it recognizes the main tense of this hypothesis and associate the main tense of previous sentence to compute inter-tense feature Fm (see Section 5.3)."
D12-1026,2002.tmi-tutorials.2,0,0.0350901,"nse sequence is about 2.5, we mainly consider intra-tense bigram model and thus n equals to 2. 5 5.4 Determining Tense For SMT Output The current SMT systems often produce odd translations partly because of abnormal word ordering and uncompleted text etc. For these abnormal translated texts, the syntactic parser cannot work well in our initial experiments, so the previous method to parse main tense and tense sequence of regular texts cannot be applied here too. Fortunately, the solely utilization of Stanford POS tagger for our SMT output is not bad although it has the same issues described in Och et al. (2002). The reason may be that phrase-based SMT contains short contexts that POS tagger can utilize while the syntax parser fails. Once obtaining a completed hypothesis, the decoder will pass it to the Stanford POS tagger and according to tense verbs to get all tense sequence for this hypothesis. However, since the POS tagger can not return the information about level structures, the decoder cannot recognize the main tense from such tense sequence. Liu et al. (2011) once used target-side verbs to label tense of source-side verbs. It is natural to consider whether Chinese verbs can provide similar cl"
D12-1026,P03-1021,0,0.0376538,": ebest = arg max P (e|f ) e = arg max P (f |e)Plm (e) (2) e where P (e|f ) is a translation model and Plm is a language model. Our baseline is a modified Moses, which follows Koehn et al. (2003) and adopts similar six groups of features. Besides, the log-linear model ( Och and Ney, 2000) is employed to linearly interpolate these features for obtaining the best translation according to the formula 3: ebest = arg max e M X λm hm (e, f ) (3) m=1 where hm (e, f ) is a feature function, and λm is the weight of hm (e, f ) optimized by a discriminative training method on a held-out development data(Och, 2003). 5.2 first obtains tense sequence for such hypothesis and computes intra-tense feature Fs (see Section 5.3). At the same time, it recognizes the main tense of this hypothesis and associate the main tense of previous sentence to compute inter-tense feature Fm (see Section 5.3). Next, the decoder uses such two additional feature values to re-score this hypothesis automatically and choose one hypothesis with highest score as the final translation. After translating one sentence, the decoder caches its main tense and pass it to the next sentence. When one document has been processed, the decoder"
D12-1026,2001.mtsummit-papers.47,0,0.174117,"Missing"
D12-1026,P02-1040,0,0.0831374,"Missing"
D12-1026,I05-1077,0,0.0561652,"particular, they trained models on main and subordinate clauses connected with some special temporal marker words, such as “after” and “before”, and employed them in temporal inference. Another typical task is cross-lingual tense predication. Some languages, such as English, are inflectional, whose verbs can express tense via certain stems or suffix, while others, such as Chinese often lack inflectional forms. Take Chinese to English translation as example, if Chinese text contains particle word “ (Le)”, the nearest Chinese verb prefers to be translated into English verb with the past tense. Ye and Zhang (2005), Ye et al. (2007) and Liu et al. (2011) focus on labeling the tenses for keywords in source-side language. 3 Ye and Zhang (2005) first built a small amount of manually-labeled data, which provide the tense mapping from Chinese text to English text. Then, they trained a CRF-based tense classifier to label tense on Chinese documents. Ye et al. (2007) further reported that syntactic features contribute most to the marking of aspectual information. Liu et al. (2011) proposed a parallel mapping method to automatically generate annotated data. In particular, they used English verbs to label tense i"
D12-1026,2007.mtsummit-papers.69,0,0.839163,"ined models on main and subordinate clauses connected with some special temporal marker words, such as “after” and “before”, and employed them in temporal inference. Another typical task is cross-lingual tense predication. Some languages, such as English, are inflectional, whose verbs can express tense via certain stems or suffix, while others, such as Chinese often lack inflectional forms. Take Chinese to English translation as example, if Chinese text contains particle word “ (Le)”, the nearest Chinese verb prefers to be translated into English verb with the past tense. Ye and Zhang (2005), Ye et al. (2007) and Liu et al. (2011) focus on labeling the tenses for keywords in source-side language. 3 Ye and Zhang (2005) first built a small amount of manually-labeled data, which provide the tense mapping from Chinese text to English text. Then, they trained a CRF-based tense classifier to label tense on Chinese documents. Ye et al. (2007) further reported that syntactic features contribute most to the marking of aspectual information. Liu et al. (2011) proposed a parallel mapping method to automatically generate annotated data. In particular, they used English verbs to label tense information for Chi"
D12-1026,J93-2003,0,\N,Missing
D12-1026,N04-1021,0,\N,Missing
D12-1026,W06-0107,0,\N,Missing
D12-1092,W06-0901,0,0.391218,"e Chinese triggers and discourse consistency between Chinese trigger mentions. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our approach over a strong baseline. 1 Introduction Event extraction, a classic information extraction task, is to identify instances of a predefined event type and can be typically divided into four subtasks: trigger identification, trigger type determination, argument identification and argument role determination. In the literature, most studies focus on English event extraction and have achieved certain success (e.g. Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006; Maslennikov and Chua, 2007; Finkel et al., 2005; Ji and Grishman, 2008; Patwardhan and Riloff, 2009, 2011; Liao and Grishman 2010; Hong et al., 2011). In comparison, there are few successful stories regarding Chinese event extraction due to special characteristics in Chinese trigger identification. In particular, there are two major reasons for the low performance: unknown triggers 1 and word segmentation errors to known triggers. Table 1 gives the statistics of unknown triggers and word segmentation errors to known triggers in both the 1 In this paper, a trigger word/phr"
D12-1092,W09-2307,0,0.0270667,"type is correctly determined if its event type and position in the document match a reference trigger;  An argument is correctly identified if its involved event type and position in the document match any of the reference argument mentions;  An argument role is correctly determined if its involved event type, position in the document, and role match any of the reference argument mentions. Finally, all sentences in the corpus are divided into words using a word segmentation tool ICTCLAS3 with all entities annotated in the corpus kept. Besides, we use Stanford Parser (Levy and Manning, 2003, Chang, et al., 2009) to create the constituent and dependency parse trees and employ the ME model to train individual component classifiers. 3.2 Experimental Results Table 2 and 3 show the Precision (P), Recall (R) and F1-Measure (F) on the held-out test set. It shows that our baseline system outperforms Chen and Ji (2009b) by 1.8, 2.2, 3.9 and 2.3 in F1measure on trigger identification, trigger type 3 http://ictclas.org/ determination, argument identification and argument role determination, respectively, with both gains in precision and recall. This is simply due to contribution of the newly-added refined and d"
D12-1092,W09-2209,0,0.623209,"d crossentity (Hong et al., 2011) information. 2.1 Chinese Event Extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Tan et al. (2008) modeled event extraction as a pipeline of classification tasks. Specially, they used a local feature selection approach to ensure the performance of trigger classification (trigger identification + trigger type determination) and applied multiple levels of patterns to improve the coverage of patterns in argument classification (argument identification + argument role determination). Chen and Ji (2009a) proposed a bootstrapping framework, which exploited extra information captured by an English event extraction system. Chen and Ji (2009b) applied various kinds of lexical, syntactic and semantic features to address the specific issues in Chinese. They also constructed a global errata table to 1007 record the inconsistency in the training set and used it to correct the inconsistency in the test set. Ji (2009) extracted cross-lingual predicate clusters using bilingual parallel corpora and a cross-lingual information extraction system, and then used the derived clusters to improve the performa"
D12-1092,N09-2053,0,0.59694,"d crossentity (Hong et al., 2011) information. 2.1 Chinese Event Extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Tan et al. (2008) modeled event extraction as a pipeline of classification tasks. Specially, they used a local feature selection approach to ensure the performance of trigger classification (trigger identification + trigger type determination) and applied multiple levels of patterns to improve the coverage of patterns in argument classification (argument identification + argument role determination). Chen and Ji (2009a) proposed a bootstrapping framework, which exploited extra information captured by an English event extraction system. Chen and Ji (2009b) applied various kinds of lexical, syntactic and semantic features to address the specific issues in Chinese. They also constructed a global errata table to 1007 record the inconsistency in the training set and used it to correct the inconsistency in the test set. Ji (2009) extracted cross-lingual predicate clusters using bilingual parallel corpora and a cross-lingual information extraction system, and then used the derived clusters to improve the performa"
D12-1092,P05-1045,0,0.153706,"Missing"
D12-1092,P09-2093,0,0.510214,"inese triggers and discourse consistency between Chinese trigger mentions. Section 6 presents the experimental results. Section 7 concludes the paper and points out future work. 2 Related Work Almost all the existing studies on event extraction concern English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Liao and Grishman, 2010; Gupta and Ji, 2009) and crossentity (Hong et al., 2011) information. 2.1 Chinese Event Extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Tan et al. (2008) modeled event extraction as a pipeline of classification tasks. Specially, they used a local feature selection approach to ensure the performance of trigger classification (trigger identification + trigger type determination) and applied multiple levels of patterns to improve the coverage of patterns in argument classification (argument identification + argument role determination"
D12-1092,W09-1704,0,0.46078,"gers and discourse consistency between Chinese trigger mentions. Section 6 presents the experimental results. Section 7 concludes the paper and points out future work. 2 Related Work Almost all the existing studies on event extraction concern English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Liao and Grishman, 2010; Gupta and Ji, 2009) and crossentity (Hong et al., 2011) information. 2.1 Chinese Event Extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Tan et al. (2008) modeled event extraction as a pipeline of classification tasks. Specially, they used a local feature selection approach to ensure the performance of trigger classification (trigger identification + trigger type determination) and applied multiple levels of patterns to improve the coverage of patterns in argument classification (argument identification + argument role determination"
D12-1092,P08-1030,0,0.55668,"valuation on the ACE 2005 Chinese corpus justifies the effectiveness of our approach over a strong baseline. 1 Introduction Event extraction, a classic information extraction task, is to identify instances of a predefined event type and can be typically divided into four subtasks: trigger identification, trigger type determination, argument identification and argument role determination. In the literature, most studies focus on English event extraction and have achieved certain success (e.g. Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006; Maslennikov and Chua, 2007; Finkel et al., 2005; Ji and Grishman, 2008; Patwardhan and Riloff, 2009, 2011; Liao and Grishman 2010; Hong et al., 2011). In comparison, there are few successful stories regarding Chinese event extraction due to special characteristics in Chinese trigger identification. In particular, there are two major reasons for the low performance: unknown triggers 1 and word segmentation errors to known triggers. Table 1 gives the statistics of unknown triggers and word segmentation errors to known triggers in both the 1 In this paper, a trigger word/phrase occurring in the training data is called a known trigger and otherwise, an unknown trigg"
D12-1092,P03-1056,0,0.0203705,"ce trigger;  A trigger type is correctly determined if its event type and position in the document match a reference trigger;  An argument is correctly identified if its involved event type and position in the document match any of the reference argument mentions;  An argument role is correctly determined if its involved event type, position in the document, and role match any of the reference argument mentions. Finally, all sentences in the corpus are divided into words using a word segmentation tool ICTCLAS3 with all entities annotated in the corpus kept. Besides, we use Stanford Parser (Levy and Manning, 2003, Chang, et al., 2009) to create the constituent and dependency parse trees and employ the ME model to train individual component classifiers. 3.2 Experimental Results Table 2 and 3 show the Precision (P), Recall (R) and F1-Measure (F) on the held-out test set. It shows that our baseline system outperforms Chen and Ji (2009b) by 1.8, 2.2, 3.9 and 2.3 in F1measure on trigger identification, trigger type 3 http://ictclas.org/ determination, argument identification and argument role determination, respectively, with both gains in precision and recall. This is simply due to contribution of the new"
D12-1092,P10-1081,0,0.210927,"ional semantics inside Chinese triggers and discourse consistency between Chinese trigger mentions. Section 6 presents the experimental results. Section 7 concludes the paper and points out future work. 2 Related Work Almost all the existing studies on event extraction concern English. While earlier studies focus on sentence-level extraction (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006), later ones turn to employ high-level information, such as document (Maslennikov and Chua, 2007; Finkel et al., 2005; Patwardhan and Riloff, 2009), cross-document (Ji and Grishman, 2008), cross-event (Liao and Grishman, 2010; Gupta and Ji, 2009) and crossentity (Hong et al., 2011) information. 2.1 Chinese Event Extraction Compared with tremendous efforts in English event extraction, there are only a few studies on Chinese event extraction. Tan et al. (2008) modeled event extraction as a pipeline of classification tasks. Specially, they used a local feature selection approach to ensure the performance of trigger classification (trigger identification + trigger type determination) and applied multiple levels of patterns to improve the coverage of patterns in argument classification (argument identification + argume"
D12-1092,P11-1141,0,0.0152721,"2.2 Compositional Semantics Almost all the related studies on compositional semantics focus on how to combine words together to convey complex meanings, such as semantic parser (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Liang et al., 2011). However, the compositional semantics mentioned in this paper is more fined-grained and focuses on how to construct Chinese characters into a word and mine the semantics of words from the word structures, especially of verbs as event triggers. To our knowledge, there is only one paper associated with compositional semantics inside Chinese words. Li (2011) discussed the internal structures inside Chinese nouns and used it in word segmentation. 2.3 Discourse Consistency Discourse consistency is an important hypothesis in natural languages and has been applied to many natural language processing applications, such as named entity recognition and coreference resolution. Specially, several studies have successfully incorporated trigger or entity consistency constraint into event extraction. Yarowsky (1995) and Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber et al., 2007) applied cross-document inference to refine local extraction results f"
D12-1092,P11-1060,0,0.0580709,"Missing"
D12-1092,N07-1042,0,0.0603607,"ese nouns and used it in word segmentation. 2.3 Discourse Consistency Discourse consistency is an important hypothesis in natural languages and has been applied to many natural language processing applications, such as named entity recognition and coreference resolution. Specially, several studies have successfully incorporated trigger or entity consistency constraint into event extraction. Yarowsky (1995) and Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber et al., 2007) applied cross-document inference to refine local extraction results for disease name, location and start/end time. Mann (2007) proposed some specific inference rules to improve extraction of personal information. Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topicrelated documents. Gupta and Ji (2009) used a similar approach to recover implicit time information for events. Liao and Grishman (2011) also used a similar approach and a self-training strategy to extract events. Liao and Grishman (2010) employed cross-event consistency information to improve sentence-level event extraction. Hong et al. (2011) regarded entity type consistency as a key feature to"
D12-1092,P07-1075,0,0.222956,"e consistency between Chinese trigger mentions. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our approach over a strong baseline. 1 Introduction Event extraction, a classic information extraction task, is to identify instances of a predefined event type and can be typically divided into four subtasks: trigger identification, trigger type determination, argument identification and argument role determination. In the literature, most studies focus on English event extraction and have achieved certain success (e.g. Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006; Maslennikov and Chua, 2007; Finkel et al., 2005; Ji and Grishman, 2008; Patwardhan and Riloff, 2009, 2011; Liao and Grishman 2010; Hong et al., 2011). In comparison, there are few successful stories regarding Chinese event extraction due to special characteristics in Chinese trigger identification. In particular, there are two major reasons for the low performance: unknown triggers 1 and word segmentation errors to known triggers. Table 1 gives the statistics of unknown triggers and word segmentation errors to known triggers in both the 1 In this paper, a trigger word/phrase occurring in the training data is called a k"
D12-1092,D07-1075,0,0.105167,"Missing"
D12-1092,D09-1016,0,0.584146,"05 Chinese corpus justifies the effectiveness of our approach over a strong baseline. 1 Introduction Event extraction, a classic information extraction task, is to identify instances of a predefined event type and can be typically divided into four subtasks: trigger identification, trigger type determination, argument identification and argument role determination. In the literature, most studies focus on English event extraction and have achieved certain success (e.g. Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006; Maslennikov and Chua, 2007; Finkel et al., 2005; Ji and Grishman, 2008; Patwardhan and Riloff, 2009, 2011; Liao and Grishman 2010; Hong et al., 2011). In comparison, there are few successful stories regarding Chinese event extraction due to special characteristics in Chinese trigger identification. In particular, there are two major reasons for the low performance: unknown triggers 1 and word segmentation errors to known triggers. Table 1 gives the statistics of unknown triggers and word segmentation errors to known triggers in both the 1 In this paper, a trigger word/phrase occurring in the training data is called a known trigger and otherwise, an unknown trigger. Language Chinese English"
D12-1092,P07-1121,0,0.0049963,"ouns and used it in word segmentation. 2.3 Discourse Consistency Discourse consistency is an important hypothesis in natural languages and has been applied to many natural language processing applications, such as named entity recognition and coreference resolution. Specially, several studies have successfully incorporated trigger or entity consistency constraint into event extraction. Yarowsky (1995) and Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber et al., 2007) applied cross-document inference to refine local extraction results for disease name, location and start/end time. Mann (2007) proposed some specific inference rules to improve extraction of personal information. Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topicrelated documents. Gupta and Ji (2009) used a similar approach to recover implicit time information for events. Liao and Grishman (2011) also used a similar approach and a self-training strategy to extract events. Liao and Grishman (2010) employed cross-event consistency information to improve sentence-level event extraction. Hong et al. (2011) regarded entity type consistency as a key feature to"
D12-1092,H05-1008,0,0.0526678,"Missing"
D12-1092,P95-1026,0,0.0413084,"ctures, especially of verbs as event triggers. To our knowledge, there is only one paper associated with compositional semantics inside Chinese words. Li (2011) discussed the internal structures inside Chinese nouns and used it in word segmentation. 2.3 Discourse Consistency Discourse consistency is an important hypothesis in natural languages and has been applied to many natural language processing applications, such as named entity recognition and coreference resolution. Specially, several studies have successfully incorporated trigger or entity consistency constraint into event extraction. Yarowsky (1995) and Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber et al., 2007) applied cross-document inference to refine local extraction results for disease name, location and start/end time. Mann (2007) proposed some specific inference rules to improve extraction of personal information. Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topicrelated documents. Gupta and Ji (2009) used a similar approach to recover implicit time information for events. Liao and Grishman (2011) also used a similar approach and a self-training strategy to"
D12-1092,D07-1071,0,0.00703224,"ddress the specific issues in Chinese. They also constructed a global errata table to 1007 record the inconsistency in the training set and used it to correct the inconsistency in the test set. Ji (2009) extracted cross-lingual predicate clusters using bilingual parallel corpora and a cross-lingual information extraction system, and then used the derived clusters to improve the performance of Chinese event extraction. 2.2 Compositional Semantics Almost all the related studies on compositional semantics focus on how to combine words together to convey complex meanings, such as semantic parser (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Liang et al., 2011). However, the compositional semantics mentioned in this paper is more fined-grained and focuses on how to construct Chinese characters into a word and mine the semantics of words from the word structures, especially of verbs as event triggers. To our knowledge, there is only one paper associated with compositional semantics inside Chinese words. Li (2011) discussed the internal structures inside Chinese nouns and used it in word segmentation. 2.3 Discourse Consistency Discourse consistency is an important hypothesis in natural languages and has been"
D12-1092,P11-1113,1,\N,Missing
D12-1132,P04-1015,0,0.0654939,"t of all possible parses for sentence x, and Score(y) is a real-valued linear function: Score(y) = Φ(y) · w ~ (2) 11 leftmost child LC and rightmost child RC unified dependency parsers are invited to study this example carefully. 5.2 Model y∈GEN(x) where Φ(y) is a global feature vector extracted from parsing result y, and w ~ is a vector of weighting parameters. Because of its linearity, Score(y) can be computed incrementally, following the transition of each parsing step. Parameter vector w ~ is trained with the generalized perceptron algorithm of Collins (2002). The early-update strategy of Collins and Roark (2004) is used so as to improve accuracy and speed up the training. 5.3 Feature Templates For a particular parse y, we now describe the way of computing its feature vector Φ(y) in the linear 1449 Feature Templates S0wt; S0w; S0t S1wt; S1w; S1t S1wtS0wt; S1wtS0w S1wS0wt; S1wtS0t S1tS0wt; S1wS0w; S1tS0t Q0; Q1; Q2; Q3 Q0Q1; Q1Q2; Q2Q3 Q0Q1Q2; Q1Q2Q3 STwtQ0; STwQ0; STtQ0 STwtQ0Q1; STwQ0Q1 STtQ0Q1 STwtQ0Q1Q2 STwQ0Q1Q2; STtQ0Q1Q2 PtSTtQ0; PtSTtQ0Q1 PtSTtQ0Q1Q2 STtLCtQ0; STtLCtQ0Q1 STtLCtQ0Q1Q2 STtRCtQ0; STtRCtQ0Q1 STtRCtQ0Q1Q2 Table 5: Transition-based feature templates. Q0 is the first character in Q, e"
D12-1132,W02-1001,0,0.0364071,"(y) (1) 10 parent P of ST Here GEN(x) is a set of all possible parses for sentence x, and Score(y) is a real-valued linear function: Score(y) = Φ(y) · w ~ (2) 11 leftmost child LC and rightmost child RC unified dependency parsers are invited to study this example carefully. 5.2 Model y∈GEN(x) where Φ(y) is a global feature vector extracted from parsing result y, and w ~ is a vector of weighting parameters. Because of its linearity, Score(y) can be computed incrementally, following the transition of each parsing step. Parameter vector w ~ is trained with the generalized perceptron algorithm of Collins (2002). The early-update strategy of Collins and Roark (2004) is used so as to improve accuracy and speed up the training. 5.3 Feature Templates For a particular parse y, we now describe the way of computing its feature vector Φ(y) in the linear 1449 Feature Templates S0wt; S0w; S0t S1wt; S1w; S1t S1wtS0wt; S1wtS0w S1wS0wt; S1wtS0t S1tS0wt; S1wS0w; S1tS0t Q0; Q1; Q2; Q3 Q0Q1; Q1Q2; Q2Q3 Q0Q1Q2; Q1Q2Q3 STwtQ0; STwQ0; STtQ0 STwtQ0Q1; STwQ0Q1 STtQ0Q1 STwtQ0Q1Q2 STwQ0Q1Q2; STtQ0Q1Q2 PtSTtQ0; PtSTtQ0Q1 PtSTtQ0Q1Q2 STtLCtQ0; STtLCtQ0Q1 STtLCtQ0Q1Q2 STtRCtQ0; STtRCtQ0Q1 STtRCtQ0Q1Q2 Table 5: Transition-bas"
D12-1132,N09-1037,0,0.0731668,"Missing"
D12-1132,P08-1043,0,0.0568122,"s and labeled dependency relations, both of which were absent in Zhao’s parser. More importantly, the AP PEND transition for handling flat words were unseen in previous studies as far as we know. The difference can best be described with an example: For the sentence in Section 3, Zhao’s parser output the result in Figure 4 while in contrast our output is Figure 2. In recent years, considerable efforts have been made in joint modeling and learning in natural language processing (Lee et al., 2011; Sun, 2011; Li et al., 2011; Finkel and Manning, 2009; Kruengkrai et al., 2009; Jiang et al., 2008; Goldberg and Tsarfaty, 2008). Joint modeling can improve the performance of NLP systems due to the obvious reason of being able to make use of various levels of information simultaneously. However, the thesis of this paper, i.e, unified parsing of Chinese word and phrase structures, bears a deeper meaning. As demonstrated in Section 1 and by Li (2011), structures of words and phrases usually have significant similarity, and the distinction between them is very difficult to define, even for expert linguists. But for real world applications, such subtle matters can safely be ignored if we could analyzed morphological and s"
D12-1132,P08-1102,0,0.209604,"Missing"
D12-1132,P09-1058,0,0.271858,"th phrase and sentence structures. Therefore, with the addition of word structures, the overall dependency parsing accuracy naturally can be improved. 6.2 Chinese Word Segmentation From the example in Figure 2, it is clear that output of unified parser contains Chinese word segmentation information. Therefore, we can get results of word segmentation for each sentence in the test sets, K2009 This Paper P N/A 97.63 R N/A 97.38 F 97.87 97.50 甘 肃 省 重 视 保 险 业 Table 9: Word segmentation results of our parser and the best performance reported in literature on the same dataset. K2009 is the result of Kruengkrai et al. (2009). K2009 ZC2011 This Paper P N/A N/A 93.42 R N/A N/A 93.20 F 93.67 93.67 93.31 Table 10: Joint word segmentation and POS tagging scores. K2009 is result of Kruengkrai et al. (2009). ZC2011 is result of Zhang and Clark (2011). and evaluate their accuracies. For maximal comparability, we train the unified parser on the original CTB 5 data used by previous studies. The result is in Table 9. Despite the fact that the performance of our unified parser does not exceed the best reported result so far, which probably might be caused by some minute implementation specific details, it’s fair to say that"
D12-1132,P11-1089,0,0.0328793,"ncy parsing algorithms. Our parser is different in two important ways. The first is we output both part-of-speech tags and labeled dependency relations, both of which were absent in Zhao’s parser. More importantly, the AP PEND transition for handling flat words were unseen in previous studies as far as we know. The difference can best be described with an example: For the sentence in Section 3, Zhao’s parser output the result in Figure 4 while in contrast our output is Figure 2. In recent years, considerable efforts have been made in joint modeling and learning in natural language processing (Lee et al., 2011; Sun, 2011; Li et al., 2011; Finkel and Manning, 2009; Kruengkrai et al., 2009; Jiang et al., 2008; Goldberg and Tsarfaty, 2008). Joint modeling can improve the performance of NLP systems due to the obvious reason of being able to make use of various levels of information simultaneously. However, the thesis of this paper, i.e, unified parsing of Chinese word and phrase structures, bears a deeper meaning. As demonstrated in Section 1 and by Li (2011), structures of words and phrases usually have significant similarity, and the distinction between them is very difficult to define, even for expe"
D12-1132,D11-1109,0,0.253014,"nd syntax. A fullfledged parser is needed to analyze structures of these words, which incidentally provides us with another motivation for unified morphological and syntactic parsing of Chinese. 5 To map a raw sentence directly to output shown in Figure 2, we define four transitions for the unified dependency parser. They act on a stack containing the incremental parsing results, and a queue holding the incoming Chinese characters of the sentence: Unified Dependency Parsing All previous dependency parsers for Chinese take it for granted that the input sentence is already segmented into words (Li et al., 2011). Most systems even require words to be tagged with their part-ofspeeches (Zhang and Nivre, 2011). Hence current off-the-shelf algorithms are inadequate for parsing 1448 RIGHT : the top two words of the stack are connected, but with the top word being the child. The precondition is the same as that of L EFT. APPEND : the first character in the queue is appended to the word at the top of the stack. There are two preconditions. First, the queue should not be empty. Second, the top of the stack must be a word with no arcs connected to other words (i.e, up to now it has got neither children nor pa"
D12-1132,P11-1141,1,0.6754,"ry between morphology and syntax, for which we now give two arguments. 1 Corresponding author is Guodong Zhou. Of course, the way out of this dilemma is to parse the internal structures of these words. That is to say, we can still regard characters like 者 as suffixes, taking into account the fact that they cannot be used alone. Meanwhile, pseudo OOVs can be largely eliminated through analyzing their structures, thus greatly facilitating syntactic and semantic analysis of sentences. In fact, previous studies have revealed other good reasons for parsing internal structures of words (Zhao, 2009; Li, 2011). The second argument is that in Chinese many linguistic units can form both words and phrases with exactly the same meaning and part-of-speech, which 1445 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1445–1454, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics MOD MOD 环境 NN 保护 NN 法 NN MOD corpus CTB 6 刑 NN 法 NN MSR PKU Figure 1: Unified parsing of words and phrases. AS CITYU causes lots of incompatible annotations in currently available corpora. Take character"
D12-1132,P11-1139,0,0.0897086,"thms. Our parser is different in two important ways. The first is we output both part-of-speech tags and labeled dependency relations, both of which were absent in Zhao’s parser. More importantly, the AP PEND transition for handling flat words were unseen in previous studies as far as we know. The difference can best be described with an example: For the sentence in Section 3, Zhao’s parser output the result in Figure 4 while in contrast our output is Figure 2. In recent years, considerable efforts have been made in joint modeling and learning in natural language processing (Lee et al., 2011; Sun, 2011; Li et al., 2011; Finkel and Manning, 2009; Kruengkrai et al., 2009; Jiang et al., 2008; Goldberg and Tsarfaty, 2008). Joint modeling can improve the performance of NLP systems due to the obvious reason of being able to make use of various levels of information simultaneously. However, the thesis of this paper, i.e, unified parsing of Chinese word and phrase structures, bears a deeper meaning. As demonstrated in Section 1 and by Li (2011), structures of words and phrases usually have significant similarity, and the distinction between them is very difficult to define, even for expert linguist"
D12-1132,P07-1031,0,0.0234288,"demonstrated in Section 1 and by Li (2011), structures of words and phrases usually have significant similarity, and the distinction between them is very difficult to define, even for expert linguists. But for real world applications, such subtle matters can safely be ignored if we could analyzed morphological and syntactic structures in a unified framework. What applications really cares is structures instead of whether a linguistic unit is a word or phrase. Another notable line of research closely related to the present work is to annotate and parse the flat structures of noun phrases (NP) (Vadas and Curran, 2007; Vadas and Curran, 2011). This paper differs from those previous work on parsing NPs in at least two significant ways. First, we aim to parse all kinds of words (e.g, nouns, verbs, adverbs, adjectives etc) whose structures are not annotated by CTB , and whose presence could cause lots of pseudo OOV s and incompatible annotations. Second, the problem we are trying to solve is a crucial observation specific to Chinese language, that is, in lots of cases forcing a separation of words and phrases leads to awkward situations for NLP systems. Remember that in Section 2 we demonstrated that all corp"
D12-1132,J11-4006,0,0.0231971,"1 and by Li (2011), structures of words and phrases usually have significant similarity, and the distinction between them is very difficult to define, even for expert linguists. But for real world applications, such subtle matters can safely be ignored if we could analyzed morphological and syntactic structures in a unified framework. What applications really cares is structures instead of whether a linguistic unit is a word or phrase. Another notable line of research closely related to the present work is to annotate and parse the flat structures of noun phrases (NP) (Vadas and Curran, 2007; Vadas and Curran, 2011). This paper differs from those previous work on parsing NPs in at least two significant ways. First, we aim to parse all kinds of words (e.g, nouns, verbs, adverbs, adjectives etc) whose structures are not annotated by CTB , and whose presence could cause lots of pseudo OOV s and incompatible annotations. Second, the problem we are trying to solve is a crucial observation specific to Chinese language, that is, in lots of cases forcing a separation of words and phrases leads to awkward situations for NLP systems. Remember that in Section 2 we demonstrated that all corpora we examined had the p"
D12-1132,W03-3023,0,0.0648884,"dequate for parsing 1448 RIGHT : the top two words of the stack are connected, but with the top word being the child. The precondition is the same as that of L EFT. APPEND : the first character in the queue is appended to the word at the top of the stack. There are two preconditions. First, the queue should not be empty. Second, the top of the stack must be a word with no arcs connected to other words (i.e, up to now it has got neither children nor parent). We see that these transitions mimic the general arcstandard dependency parsing models. The first three of them were used, for example, by Yamada and Matsumoto (2003) to parse English sentences. The only novel addition is APPEND, which is necessary because we are dealing with raw sentences. Its sole purpose is to assemble characters into words with no internal structures, such as 西 雅 图 ‘Seattle’. Thus this transition is the key for removing the need of Chinese word segmentation and parsing unsegmented sentences directly. To also output part-of-speech tags and dependency labels, the transitions above can be augmented accordingly. Hence we can change SHIFT to SHIFT · X where X represents a certain POS tag. Also, LEFT and RIGHT should be augmented with approp"
D12-1132,D08-1059,0,0.0470118,"candidate do 7: newc ← E XPAND(candidate, action) 8: if C OMPLETED(newc) then 9: completed.I NSERT(newc) 10: else 11: agenda.I NSERT(newc) 12: end if 13: end for 14: end for 15: if E MPTY(agenda) then 16: return T OP(completed) 17: end if 18: candidates ← T OP B(agenda, B) 19: agenda ← φ 20: completed ← T OP B(completed, B) 21: end loop 6 Experiments and Evaluation We describe the experiments carried out and our method of evaluation of the unified dependency parser. We used Penn2Malt 4 to convert constituent trees of CTB to dependency relations. The head rules for this conversion was given by Zhang and Clark (2008). In all experiments, we followed the stan4 http://w3.msi.vxu.se/˜nivre/research/ Penn2Malt.html our method, labeled our method, unlabeled ZC2011, unlabeled P 78.54 81.01 N/A R 80.93 83.77 N/A F 79.72 82.37 75.09 Table 7: Evaluation results on the original CTB 5. N/A means the value is not available to us. ZC2011 is Zhang and Clark (2011). dard split of the data into training, testing and development data (Zhang and Clark, 2011). Though we annotated structures of words in CTB 6, most previously results were on CTB 5, a subset of the former treebank. Hence we report our results of evaluation on"
D12-1132,J11-1005,0,0.307542,"STwQ0Q1Q2; STtQ0Q1Q2 PtSTtQ0; PtSTtQ0Q1 PtSTtQ0Q1Q2 STtLCtQ0; STtLCtQ0Q1 STtLCtQ0Q1Q2 STtRCtQ0; STtRCtQ0Q1 STtRCtQ0Q1Q2 Table 5: Transition-based feature templates. Q0 is the first character in Q, etc. w = word, t = POS tag. model of Equation (2). If S denotes the stack holding the partial results, and Q the queue storing the incoming Chinese characters of a raw sentence, then transition-based parsing features are extracted from S and Q according to those feature templates in Table 5. Although we employ transition-based parsing, nothing prevents us from using graph-based features. As shown by Zhang and Clark (2011), depen1 2 3 Description parent word child word P and C 4 neighbor word of P and C left (L) or right (R) 5 sibling(S) of C 6 leftmost and rightmost child left (la) and right (ra) arity of P 7 Feature Templates Pwt; Pw; Pt Cwt; Cw; Ct PwtCwt; PwtCw; PwCwt PtCwt; PwCw; PtCt PwtCt PtPLtCtCLt; PtPLtCtCRt PtPRtCtCLt; PtPRtCtCRt PtPLtCLt; PtPLtCRt PtPRtCLt; PtPRtCRt PLtCtCLt; PLtCtCRt PRtCtCLt; PRtCtCRt PtCtCLt; PtCtCRt PtPLtCt; PtPRtCt CwSw;CtSt; CwSt CtSw; PtCtSt PtCtCLCt PtCtCRCt Ptla; Ptra Pwtla; Pwtra Pwla; Pwra Table 6: Graph-based feature templates for the unified parser. Most of these templa"
D12-1132,P11-2033,0,0.0260228,"entally provides us with another motivation for unified morphological and syntactic parsing of Chinese. 5 To map a raw sentence directly to output shown in Figure 2, we define four transitions for the unified dependency parser. They act on a stack containing the incremental parsing results, and a queue holding the incoming Chinese characters of the sentence: Unified Dependency Parsing All previous dependency parsers for Chinese take it for granted that the input sentence is already segmented into words (Li et al., 2011). Most systems even require words to be tagged with their part-ofspeeches (Zhang and Nivre, 2011). Hence current off-the-shelf algorithms are inadequate for parsing 1448 RIGHT : the top two words of the stack are connected, but with the top word being the child. The precondition is the same as that of L EFT. APPEND : the first character in the queue is appended to the word at the top of the stack. There are two preconditions. First, the queue should not be empty. Second, the top of the stack must be a word with no arcs connected to other words (i.e, up to now it has got neither children nor parent). We see that these transitions mimic the general arcstandard dependency parsing models. The"
D12-1132,E09-1100,0,0.235006,"clear boundary between morphology and syntax, for which we now give two arguments. 1 Corresponding author is Guodong Zhou. Of course, the way out of this dilemma is to parse the internal structures of these words. That is to say, we can still regard characters like 者 as suffixes, taking into account the fact that they cannot be used alone. Meanwhile, pseudo OOVs can be largely eliminated through analyzing their structures, thus greatly facilitating syntactic and semantic analysis of sentences. In fact, previous studies have revealed other good reasons for parsing internal structures of words (Zhao, 2009; Li, 2011). The second argument is that in Chinese many linguistic units can form both words and phrases with exactly the same meaning and part-of-speech, which 1445 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1445–1454, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics MOD MOD 环境 NN 保护 NN 法 NN MOD corpus CTB 6 刑 NN 法 NN MSR PKU Figure 1: Unified parsing of words and phrases. AS CITYU causes lots of incompatible annotations in currently available corpora. Tak"
D13-1067,P10-1015,0,0.0802864,"Missing"
D13-1067,W04-3247,0,0.0724279,"Missing"
D13-1067,I11-1054,0,0.0451692,"Missing"
D13-1067,W04-1013,0,0.0134177,"Missing"
D13-1067,J98-3005,0,0.158407,"some motivational analysis. In Section 5, we explain our proposed model and describe algorithms for parameter estimation and prediction. In Section 6, we present our experimental results. We sum up our work and discuss future directions in Section 7. 2 Related Work In this section, we will introduce the related work on the traditional topic-based summarization, social-based summarization and factor graph model respectively. 716 2.1 Topic-based Summarization Generally, traditional topic-based summarization can be categorized into two categories: extractive (Radev et al., 2004) and abstractive (Radev and McKeown, 1998) summarization. The former selects a subset of sentences from original document(s) to form a summary; the latter reorganizes some sentences to form a summary where several complex technologies, such as information fusion, sentence compression and reformulation are necessarily employed (Wan and Yang, 2008; Celikyilmaz and Hakkani-Tur, 2011; Wang and Zhou, 2012). This study focuses on extractive summarization. Radev et al. (2004) proposed a centroid-based method to rank the sentences in a document set, using various kinds of features, such as the cluster centroid, position and TF-IDF features. R"
D13-1067,P11-1077,0,0.0136695,", supervised learning for summarization is relatively rare. A typical work is Shen et al., (2007) which present a Conditional Random Fields (CRF) based framework to treat the summarization task as a sequence labeling problem. However, different from all existing studies, our work is the first attempt to consider both textual information and social relationship information for supervised summarization. 2.2 Social-based Summarization As web 2.0 has empowered people to actively interact with each other, studies focusing on social media have attracted much attention recently (Meeder et al., 2011; Rosenthal and McKeown, 2011; Yang et al., 2011a). Social-based summarization is exactly a special case of summarization where the social connection is employed to help obtaining the summarization. Although topicbased summarization has been extensively studied, studies on social-based summarization are relative new and rare. Hu et al., (2011) proposed an unsupervised PageRank-based social summarization approach by incorporating both document context and user context in the sentence evaluation process. Meng et al., (2012) proposed a unified optimization framework to produce opinion summaries of tweets through integrating"
D13-1067,D12-1024,0,0.0173319,") summarization. The former selects a subset of sentences from original document(s) to form a summary; the latter reorganizes some sentences to form a summary where several complex technologies, such as information fusion, sentence compression and reformulation are necessarily employed (Wan and Yang, 2008; Celikyilmaz and Hakkani-Tur, 2011; Wang and Zhou, 2012). This study focuses on extractive summarization. Radev et al. (2004) proposed a centroid-based method to rank the sentences in a document set, using various kinds of features, such as the cluster centroid, position and TF-IDF features. Ryang and Abekawa (2012) proposed a reinforcement learning approach on text summarization, which models the summarization within a reinforcement learning-based framework. Compared to unsupervised approaches, supervised learning for summarization is relatively rare. A typical work is Shen et al., (2007) which present a Conditional Random Fields (CRF) based framework to treat the summarization task as a sequence labeling problem. However, different from all existing studies, our work is the first attempt to consider both textual information and social relationship information for supervised summarization. 2.2 Social-ba"
D13-1067,P11-1155,0,0.133274,"lly. To the best of our knowledge, this is the first research that explores automatic summarization of personal profiles in social media. A straightforward approach is to consider personal profile summarization as a traditional document summarization problem, which treating each personal profile independently and generate a summary for each personal profile individually. For example, the well-known extraction and ranking approaches (e.g. PageRank, HITS) extract a certain amount of important sentences from a document according to some ranking measurements to form a summary (Wan and Yang, 2008; Wan, 2011). However, such straightforward approaches are not sufficient to benefit from the carrier of personal profiles. As the centroid of social networking, people are usually connected to others with similar 715 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 715–725, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics background in social media (e.g. co-major, cocorporation). Therefore, it is reasonable to leverage social connection to improve the performance of profile summarizing. For example if there are co-"
D13-1067,P11-1050,0,\N,Missing
D13-1099,P11-2049,0,0.12218,"Missing"
D13-1099,P06-2010,0,0.0655085,"Missing"
D13-1099,E06-1015,0,0.0089802,"et al, 2009; Øvrelid et al, 2010). However, these flat features are hardly to reflect the information implicit in syntactic parse tree structures. Our intuition is that the segments of the syntactic parse tree around a negative or speculative cue is effective for scope detection. The related structures normally underlay the indirect clues to identify the relations between cues and their scopes, e.g., in sentence 1), “but something”, as a frequently co-occurred syntactic structure with “not something”, is an effective clue to determine the linguistic scope of “not”. The tree kernel classifier (Moschitti, 2006) based on support vector machines uses a kernel function between two trees, affording a comparison between their substructures. Therefore, a tree kernel-based scope detection approach with structured syntactic parse tree is employed. The tree 968 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 968–976, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics kernel has been already proved to be effective in semantic role labeling (Che et al, 2006) and relation extraction (Zhou et al, 2007). In addition, the emp"
D13-1099,C10-1155,0,0.0303614,"he second stage. That is, by given golden cues, we detect their linguistic scopes. We propose a tree kernel-based negation and speculation scope detection with structured syntactic parse features. In detail, we regard the scope detection task as a binary classification issue, which is to classify the tokens in a sentence as being inside or outside the scope. In the basic framework, we focus on the analysis and application of structured syntactic parse features as follows: Both constituent and dependency syntactic features have been proved to be effective in scope detection (Özgür et al, 2009; Øvrelid et al, 2010). However, these flat features are hardly to reflect the information implicit in syntactic parse tree structures. Our intuition is that the segments of the syntactic parse tree around a negative or speculative cue is effective for scope detection. The related structures normally underlay the indirect clues to identify the relations between cues and their scopes, e.g., in sentence 1), “but something”, as a frequently co-occurred syntactic structure with “not something”, is an effective clue to determine the linguistic scope of “not”. The tree kernel classifier (Moschitti, 2006) based on support"
D13-1099,D09-1145,0,0.10024,"Missing"
D13-1099,N07-1051,0,0.018078,"Missing"
D13-1099,E99-1043,0,0.228952,"a <cue type=”negation” ref=”X26.8.1”> can not </cue> be explained by abnormalities in corticosteroid receptor characteristics </xcope></xcope> . </sentence> (Note: <Sentence> denotes one sentence and the tag “id” denotes its serial number; <xcope> denotes the scope of a cue; <cue> denotes the cue, the tag “type” denotes the specific kind of cues and the tag “ref” is the cue’s serial number.) Figure 1. An annotated sentence in BioScope. The BioScope corpus consists of three subcorpora: biological Full Papers from FlyBase and BMC Bioinformatics, biological paper Abstracts from the GENIA corpus (Collier et al, 1999), and Clinical Reports. Among them, the Full Papers sub-corpus and the Abstracts sub-corpus come from the same genre. In comparison, the Clinical Reports sub-corpus consists of clinical radiology reports with short sentences. 1 In our experiments, if there is more than one cue in a sentence, we treat them as different cue and scope (two independent instances). The statistical data for our corpus is presented in Table 1 in below. The average length of sentences in the negation portion is almost as long as that in speculation, while the average length of scope in negation is shorter than that in"
D13-1099,W10-3001,0,0.0191079,"Missing"
D13-1099,W08-0606,0,0.0306192,"ristic rule based methods have further involved the syntactic features. Huang et al (2007) implemented a hybrid approach to automated negation scope detection. They combined the regular expression matching with grammatical parsing: negations are classified on the basis of syntactic categories and located in parse trees. Their hybrid approach is able to identify negated concepts in radiology reports even when they are located at some distance from the negative term. 969 Machine Learning based Methods The machine learning based methods have been ignored until the release of the BioScope corpus (Szarvas et al, 2008), where the large-scale data of manually annotated cues and corresponding scopes can support machine learning well. Morante et al (2008) formulated scope detection as a chunk classification problem. It is worth noting that they also proposed an effective proper post-processing approach to ensure the consecutiveness of scope. Then, for further improving the scope detection, Morante et al (2009a) applied a meta-learner that uses the predictions of the three classifiers (TiMBL/SVM/CRF) to predict the scope. For the competitive task in CoNLL’2010 (Farkas et al, 2010), Morante et al (2010) used a m"
D13-1099,I05-2038,0,0.0251458,"Missing"
D13-1099,W06-1617,0,0.188693,"Missing"
D13-1099,D08-1075,0,0.0780942,"Missing"
D13-1099,W09-1105,0,0.136727,"Missing"
D13-1099,W09-1304,0,0.0206763,"Missing"
D13-1099,W10-3006,0,0.0296829,"Missing"
D13-1099,D07-1076,1,0.816069,"Missing"
D13-1099,W10-3018,0,\N,Missing
D13-1099,J12-2005,0,\N,Missing
D14-1008,A00-2018,0,0.297316,"Missing"
D14-1008,W05-0305,0,0.0196111,"Missing"
D14-1008,I11-1120,0,0.784703,"e current sentence containing the connective and its immediately preceding sentence as the text span where Arg1 occurs, similar to what was done in (Lin et al., 2014). 3 Related Work For argument labeling in discourse parsing on the PDTB corpus, the related work can be classified into two categories: locating parts of arguments, and labeling full argument spans. As a representative on locating parts of arguments, Wellner and Pustejovsky (2007) proposed several machine learning approaches to identify the head words of the two arguments for discourse As a representative linear tagging approach, Ghosh et al. (2011) cast argument labeling as a linear tagging task using conditional random fields. Ghosh et al. (2012) further improved the perfor69 S VP NP CC But PRP its NNS competitors VBP have and NP ADJP RB VP CC RB VP JJR NN so are NNS VP VBP ADVP VBN RBR cushioned business interests better much broader PP NP IN against NN NNS price swings Figure 1: The gold-standard parse tree corresponding to Example (1) stituents marked NULL) may overwhelm positive instances. To address this problem, we use a simple algorithm to prune out these constituents which are clearly not arguments to the connective in question"
D14-1008,W12-1622,0,0.525034,"where Arg1 occurs, similar to what was done in (Lin et al., 2014). 3 Related Work For argument labeling in discourse parsing on the PDTB corpus, the related work can be classified into two categories: locating parts of arguments, and labeling full argument spans. As a representative on locating parts of arguments, Wellner and Pustejovsky (2007) proposed several machine learning approaches to identify the head words of the two arguments for discourse As a representative linear tagging approach, Ghosh et al. (2011) cast argument labeling as a linear tagging task using conditional random fields. Ghosh et al. (2012) further improved the perfor69 S VP NP CC But PRP its NNS competitors VBP have and NP ADJP RB VP CC RB VP JJR NN so are NNS VP VBP ADVP VBN RBR cushioned business interests better much broader PP NP IN against NN NNS price swings Figure 1: The gold-standard parse tree corresponding to Example (1) stituents marked NULL) may overwhelm positive instances. To address this problem, we use a simple algorithm to prune out these constituents which are clearly not arguments to the connective in question. mance with integration of the n-best results. While the subtree extraction approach locates argumen"
D14-1008,J93-2004,0,0.0501949,"Missing"
D14-1008,miltsakaki-etal-2004-penn,0,0.0218776,"Missing"
D14-1008,P06-1055,0,0.0816101,"Missing"
D14-1008,prasad-etal-2008-penn,0,0.20935,"oach via integer linear programming. Evaluation on PDTB shows significant performance improvements of our constituent-based approach over the best state-of-the-art system. It also shows the effectiveness of our joint inference mechanism in modeling global information across arguments. 1 Introduction Discourse parsing determines the internal structure of a text and identifies the discourse relations between its text units. It has attracted increasing attention in recent years due to its importance in text understanding, especially since the release of the Penn Discourse Treebank (PDTB) corpus (Prasad et al., 2008), which adds a layer of discourse annotations on top of the Penn Treebank ∗ The research reported in this paper was carried out while Fang Kong was a research fellow at the National University of Singapore. 68 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 68–77, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2 Penn Discourse Treebank connectives. Following this work, Elwell and Baldridge (2008) combined general and connective specific rankers to improve the performance of labeling the head words of the"
D14-1008,prasad-etal-2010-exploiting,0,0.315938,"Missing"
D14-1008,J08-2005,0,0.035491,"label Arg1 or Arg2. k X E E E Constraint 4: Since we view the previous complete sentence as a special Arg1 constituent candidate, denoted as m, there is at least one candidate assigned as Arg1 for every connective. X D D Constraint 3: For a connective, there is at least one constituent candidate assigned as Arg2. X D $ • How do we unify the two candidate lists? In principle, constituents spanning the same sequence of words should be viewed as the same candidate. That is, for different candidates, we can unify them by adding phantom candidates. This is similar to the approach proposed by Punyakanok et al. (2008) for the semantic role labeling task. For example, Figure 2 shows the candidate lists generated by our pruning algorithm based on two different parse trees given the segment “its competitors have much broader business interests”. Dashed lines are used for phantom candidates and solid lines for true candidates. Here, system A produces one candidate a1, with two phantom candidates a2 and a3 added. Analogously, phantom candidate b3 is added to the candidate list output by System B. In this way, we can get the unified candidate list: “its competitors have much broader business interests”, “its com"
D14-1008,D07-1010,0,0.0634122,"Missing"
D14-1008,W04-3212,0,0.0497855,"then determining the role of every constituent as part of Arg1, Arg2, or NULL, and finally, merging all the constituents for Arg1 and Arg2 to obtain the Arg1 and Arg2 text spans respectively. Obviously, the key to the success of our constituent-based approach is constituent-based argument classification, which determines the role of every constituent argument candidate. As stated above, the PDTB views a connective as the predicate of a discourse relation. Similar to semantic role labeling (SRL), for a given connective, the majority of the constituents in a parse tree may not be its arguments (Xue and Palmer, 2004). This indicates that negative instances (con70 Feature CON-Str CON-LStr CON-Cat CON-iLSib CON-iRSib NT-Ctx CON-NT-Path CON-NT-Position CON-NT-Path-iLsib Description The string of the given connective (case-sensitive) The lowercase string of the given connective The syntactic category of the given connective: subordinating, coordinating, or discourse adverbial Number of left siblings of the connective Number of right siblings of the connective The context of the constituent. We use POS combination of the constituent, its parent, left sibling and right sibling to represent the context. When the"
D14-1008,D09-1161,0,0.0359182,"Missing"
D14-1224,P09-1077,0,0.0886063,"Missing"
D14-1224,miltsakaki-etal-2004-penn,0,0.0235068,"Missing"
D14-1224,I11-1170,0,0.134621,"Missing"
D14-1224,W05-0312,0,0.0968714,"Missing"
D14-1224,P11-2111,0,0.0160278,"Missing"
D14-1224,W01-1605,0,\N,Missing
D14-1224,D09-1036,0,\N,Missing
D14-1224,prasad-etal-2008-penn,0,\N,Missing
D14-1224,P12-1008,0,\N,Missing
D15-1170,P13-2009,0,0.382508,"before pre-‐processing NL’: what be the area of sea0le MRL’: answer@1 area_1@1 cityid@2 sea0le@s _@0 (b) aGer pre-‐processing Figure 1: Example of a sentence pair in NL and MRL. naturally viewed as a statistical machine translation (SMT) task, which translates a sentence in NL (i.e., the source language in SMT) into its meaning representation in MRL (i.e., the target language in SMT). Indeed, many attempts have been made to directly apply statistical machine translation (SMT) systems (or methodologies) to semantic parsing (Papineni et al., 1997; Macherey et al., 2001; Wong and Mooney, 2006; Andreas et al., 2013). However, although recent studies (Wong and Mooney, 2006; Andreas et al., 2013) show that semantic parsing with SCFGs, which form the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2007), achieves favorable results, this approach is still behind the most recent state-of-the-art. For details, please see performance comparison in Andreas et al. (2013) and Lu (2014). Introduction Semantic parsing, the task of mapping natural language (NL) sentences into a formal meaning representation language (MRL), has recently received a significant amount"
D15-1170,D10-1119,0,0.187631,"the hybrid tree structures, to learn a generative process under a Bayesian framework. RHT (Lu, 2014) defined distributions over relaxed hybrid tree structures that jointly represented both sentences and semantics. Most recently, f-RHT (Lu, 2015) introduced constrained semantic forests to improve RHT model. SCISSOR (Ge and Mooney, 2005) augmented syntactic parse tree with semantic information and then performed integrated semantic and syntactic parsing to NL sentences. KRISP (Mooney, 2006) used string classifiers to label substrings of an NL with entities from the meaning representation. UBL (Kwiatkowski et al., 2010) performed semantic parsing with an automatically-induced CCG lexicon. Table 7 shows the evaluation results of our system as well as those of several other comparable related works which share the same experiment setup as ours. We can observe from Table 7 that semantic parsing with SMT components gives 1462 System non-enriched + gdfa non-enriched + all enriched + gdfa enriched + all enriched + all + unknown word translation English Acc. F1 77.5 83.5 81.5 85.2 78.9 83.9 82.9 86.1 86.3 87.1 German Acc. F1 66.0 74.9 72.1 76.8 66.7 74.6 75.4 79.5 79.1 80.3 Greek Acc. F1 65.6 74.1 75.2 80.5 67.8 76"
D15-1170,P14-1133,0,0.0598895,"a CCG lexicon for CCG-based semantic parser (Kwiatkowski et al., 2011; Wang et al., 2014). Experiments on benchmark data have shown that our model is competitive to previous work and achieves state-of-the-art performance across a few different languages. Recently the research of semantic parsing in open domain with weakly (or un-) supervised setups, under different settings where the goal was to optimize the performance of certain downstream NLP tasks such as answering questions, has received a significant amount of attention (Poon and Domingos, 2009; Clarke et al., 2010; Berant et al., 2013; Berant and Liang, 2014). One direction of our future work is to extend the current framework to support the generation of synthetic translation rules from weaker signals (e.g., from question-answer pairs), rather than from aligned parallel data. We also noticed recent advance in tree-based SMT. Applying such string-to-tree or tree-to-tree translation models (Yamada and Knight, 2001; Shen et al., 2008) to semantic parsing will naturally resolve the inconsistent semantic structure issue, though they require additional information to generate tree labels on the target side. However, due to the constraint that each targ"
D15-1170,D11-1140,0,0.299131,"to obtain improvement on Thai, the performance is still lower than those of RHT and TREETRANS. This is probably because of the low quality of word alignment output between this Asian language and MRL. 6 Conclusion and Future Work In this paper, we have presented an enriched SCFG approach for semantic parsing which realizes the potential of the SMT approach. The performance improvement is contributed from the extension of translation rules with informative symbols and increased coverage. Such an extension share a similar spirit as generalization of a CCG lexicon for CCG-based semantic parser (Kwiatkowski et al., 2011; Wang et al., 2014). Experiments on benchmark data have shown that our model is competitive to previous work and achieves state-of-the-art performance across a few different languages. Recently the research of semantic parsing in open domain with weakly (or un-) supervised setups, under different settings where the goal was to optimize the performance of certain downstream NLP tasks such as answering questions, has received a significant amount of attention (Poon and Domingos, 2009; Clarke et al., 2010; Berant et al., 2013; Berant and Liang, 2014). One direction of our future work is to exten"
D15-1170,D13-1160,0,0.109238,"as generalization of a CCG lexicon for CCG-based semantic parser (Kwiatkowski et al., 2011; Wang et al., 2014). Experiments on benchmark data have shown that our model is competitive to previous work and achieves state-of-the-art performance across a few different languages. Recently the research of semantic parsing in open domain with weakly (or un-) supervised setups, under different settings where the goal was to optimize the performance of certain downstream NLP tasks such as answering questions, has received a significant amount of attention (Poon and Domingos, 2009; Clarke et al., 2010; Berant et al., 2013; Berant and Liang, 2014). One direction of our future work is to extend the current framework to support the generation of synthetic translation rules from weaker signals (e.g., from question-answer pairs), rather than from aligned parallel data. We also noticed recent advance in tree-based SMT. Applying such string-to-tree or tree-to-tree translation models (Yamada and Knight, 2001; Shen et al., 2008) to semantic parsing will naturally resolve the inconsistent semantic structure issue, though they require additional information to generate tree labels on the target side. However, due to the"
D15-1170,D13-1161,0,0.120885,"ry dataset, which is publicly available. 4.1 Experimental Settings Data GeoQuery dataset consists of 880 questions paired with their corresponding tree structured semantic representations. Following the experimental setup in Jones et al. (2012), we use the 600 question pairs to train and tune our SMT de1459 SCFG coder, and evaluated on the remaining 280. Note that there is another version of GeoQuery dataset where the semantic representation is annotated with lambda calculus expressions and which is extensively studied (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). Performance on the version of lambda calculus is higher than that on the tree structured version, however, the results obtained over the two versions are not directly comparable. SMT Setting We use cdec (Dyer et al., 2010) as our HPB decoder. As mentioned above, 600 instances are used to train and tune our decoder. To get fair results, we split the 600 instances into 10 folds, each having 60 instances. Then for each fold, we use it as the tuning data while the other 540 instances and the NP list are used as training data.7 We use IRSTLM toolkit (Federico et al., 2008) to train a 5-gram LM on"
D15-1170,J93-2003,0,0.0296164,"glue rules can be applied to any two neighboring translation nodes if the non-terminal symbols are matched. 3.2 Word Alignment for Semantic Parsing Word alignment is an essential step for rule extraction in SMT, where recognizing that wo shi in Chinese is a good translation for I am in English requires establishing a correspondence between wo and I, and between shi and am. In the SMT community, researchers have developed standard, proven alignment tools such as GIZA++ (Och and Ney, 2003), which can be used to train IBM Models 1-5. However, there is one fundamental problem with the IBM models (Brown et al., 1993): each word on one side can be traced back to exactly one particular on the other word (or the null token which indicates the word aligns to no word on the other side). Figure 2(a) shows an example of GIZA++ alignment output from source side to target side, from which we can see that each source word aligns to exactly one target word. While alignment of multiple target words to one source word is common in SMT, a trick is then to run IBM model training in both directions. Then two resulting word alignments can be symmetrized, for instance, taking the intersection or the union of alignment poin"
D15-1170,W12-3128,1,0.907527,"Missing"
D15-1170,D08-1024,0,0.0333419,"Missing"
D15-1170,J07-2003,0,0.823312,"ch translates a sentence in NL (i.e., the source language in SMT) into its meaning representation in MRL (i.e., the target language in SMT). Indeed, many attempts have been made to directly apply statistical machine translation (SMT) systems (or methodologies) to semantic parsing (Papineni et al., 1997; Macherey et al., 2001; Wong and Mooney, 2006; Andreas et al., 2013). However, although recent studies (Wong and Mooney, 2006; Andreas et al., 2013) show that semantic parsing with SCFGs, which form the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2007), achieves favorable results, this approach is still behind the most recent state-of-the-art. For details, please see performance comparison in Andreas et al. (2013) and Lu (2014). Introduction Semantic parsing, the task of mapping natural language (NL) sentences into a formal meaning representation language (MRL), has recently received a significant amount of attention with various models proposed over the past few years. Consider the NL sentence paired with its corresponding MRL in Figure 1(a). Semantic parsing can be The key issues behind the limited success of applying SMT systems directly"
D15-1170,W10-2903,0,0.116255,"are a similar spirit as generalization of a CCG lexicon for CCG-based semantic parser (Kwiatkowski et al., 2011; Wang et al., 2014). Experiments on benchmark data have shown that our model is competitive to previous work and achieves state-of-the-art performance across a few different languages. Recently the research of semantic parsing in open domain with weakly (or un-) supervised setups, under different settings where the goal was to optimize the performance of certain downstream NLP tasks such as answering questions, has received a significant amount of attention (Poon and Domingos, 2009; Clarke et al., 2010; Berant et al., 2013; Berant and Liang, 2014). One direction of our future work is to extend the current framework to support the generation of synthetic translation rules from weaker signals (e.g., from question-answer pairs), rather than from aligned parallel data. We also noticed recent advance in tree-based SMT. Applying such string-to-tree or tree-to-tree translation models (Yamada and Knight, 2001; Shen et al., 2008) to semantic parsing will naturally resolve the inconsistent semantic structure issue, though they require additional information to generate tree labels on the target side."
D15-1170,P10-4002,0,0.0148194,"et al. (2012), we use the 600 question pairs to train and tune our SMT de1459 SCFG coder, and evaluated on the remaining 280. Note that there is another version of GeoQuery dataset where the semantic representation is annotated with lambda calculus expressions and which is extensively studied (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). Performance on the version of lambda calculus is higher than that on the tree structured version, however, the results obtained over the two versions are not directly comparable. SMT Setting We use cdec (Dyer et al., 2010) as our HPB decoder. As mentioned above, 600 instances are used to train and tune our decoder. To get fair results, we split the 600 instances into 10 folds, each having 60 instances. Then for each fold, we use it as the tuning data while the other 540 instances and the NP list are used as training data.7 We use IRSTLM toolkit (Federico et al., 2008) to train a 5-gram LM on the MRL0 side of the training data, using modified Kneser-Ney smoothing. We use Mira (Chiang et al., 2008) to tune the parameters of the system to maximize BLEU (Papineni et al., 2002). When extracting translation rules fro"
D15-1170,P11-1060,0,0.333497,"Missing"
D15-1170,D08-1082,1,0.929001,"to avoid the issues caused by word alignment between NL and MRL, we triple training data with each sentence pair having multiple alignments. However, WASP used a sequence of productions to represent MRL before running GIZA++. Third, we use typical features in HPB SMT (e.g., phrase translation probabilities, lexical translation probabilities, language model feature, etc.) while WASP used rule identity features. SMT-SemParse (Andreas et al., 2013) adapted standard SMT components for semantic parsing. The present work is based on theirs with all the extensions detailed in Section 3. HYBRIDTREE+ (Lu et al., 2008) learned a synchronous generative model which simultaneously generated a NL sentence and an MRL tree. tsVB (Jones et al., 2012) used tree transducers, which were similar to the hybrid tree structures, to learn a generative process under a Bayesian framework. RHT (Lu, 2014) defined distributions over relaxed hybrid tree structures that jointly represented both sentences and semantics. Most recently, f-RHT (Lu, 2015) introduced constrained semantic forests to improve RHT model. SCISSOR (Ge and Mooney, 2005) augmented syntactic parse tree with semantic information and then performed integrated se"
D15-1170,D14-1137,1,0.858388,"rectly apply statistical machine translation (SMT) systems (or methodologies) to semantic parsing (Papineni et al., 1997; Macherey et al., 2001; Wong and Mooney, 2006; Andreas et al., 2013). However, although recent studies (Wong and Mooney, 2006; Andreas et al., 2013) show that semantic parsing with SCFGs, which form the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2007), achieves favorable results, this approach is still behind the most recent state-of-the-art. For details, please see performance comparison in Andreas et al. (2013) and Lu (2014). Introduction Semantic parsing, the task of mapping natural language (NL) sentences into a formal meaning representation language (MRL), has recently received a significant amount of attention with various models proposed over the past few years. Consider the NL sentence paired with its corresponding MRL in Figure 1(a). Semantic parsing can be The key issues behind the limited success of applying SMT systems directly to semantic parsing lie in the difference between semantic parsing and SMT: MRL is not a real natural language with different properties from natural language. First, MRL is mach"
D15-1170,P15-2121,1,0.776128,"e (Andreas et al., 2013) adapted standard SMT components for semantic parsing. The present work is based on theirs with all the extensions detailed in Section 3. HYBRIDTREE+ (Lu et al., 2008) learned a synchronous generative model which simultaneously generated a NL sentence and an MRL tree. tsVB (Jones et al., 2012) used tree transducers, which were similar to the hybrid tree structures, to learn a generative process under a Bayesian framework. RHT (Lu, 2014) defined distributions over relaxed hybrid tree structures that jointly represented both sentences and semantics. Most recently, f-RHT (Lu, 2015) introduced constrained semantic forests to improve RHT model. SCISSOR (Ge and Mooney, 2005) augmented syntactic parse tree with semantic information and then performed integrated semantic and syntactic parsing to NL sentences. KRISP (Mooney, 2006) used string classifiers to label substrings of an NL with entities from the meaning representation. UBL (Kwiatkowski et al., 2010) performed semantic parsing with an automatically-induced CCG lexicon. Table 7 shows the evaluation results of our system as well as those of several other comparable related works which share the same experiment setup as"
D15-1170,P06-1115,0,0.552136,"Missing"
D15-1170,J03-1002,0,0.012028,"bility p (α |γ), and a glue rule penalty. Table 1(b) shows examples of a straight and an inverted glue rules. Moreover, these glue rules can be applied to any two neighboring translation nodes if the non-terminal symbols are matched. 3.2 Word Alignment for Semantic Parsing Word alignment is an essential step for rule extraction in SMT, where recognizing that wo shi in Chinese is a good translation for I am in English requires establishing a correspondence between wo and I, and between shi and am. In the SMT community, researchers have developed standard, proven alignment tools such as GIZA++ (Och and Ney, 2003), which can be used to train IBM Models 1-5. However, there is one fundamental problem with the IBM models (Brown et al., 1993): each word on one side can be traced back to exactly one particular on the other word (or the null token which indicates the word aligns to no word on the other side). Figure 2(a) shows an example of GIZA++ alignment output from source side to target side, from which we can see that each source word aligns to exactly one target word. While alignment of multiple target words to one source word is common in SMT, a trick is then to run IBM model training in both directio"
D15-1170,W05-0602,0,0.0486525,"e present work is based on theirs with all the extensions detailed in Section 3. HYBRIDTREE+ (Lu et al., 2008) learned a synchronous generative model which simultaneously generated a NL sentence and an MRL tree. tsVB (Jones et al., 2012) used tree transducers, which were similar to the hybrid tree structures, to learn a generative process under a Bayesian framework. RHT (Lu, 2014) defined distributions over relaxed hybrid tree structures that jointly represented both sentences and semantics. Most recently, f-RHT (Lu, 2015) introduced constrained semantic forests to improve RHT model. SCISSOR (Ge and Mooney, 2005) augmented syntactic parse tree with semantic information and then performed integrated semantic and syntactic parsing to NL sentences. KRISP (Mooney, 2006) used string classifiers to label substrings of an NL with entities from the meaning representation. UBL (Kwiatkowski et al., 2010) performed semantic parsing with an automatically-induced CCG lexicon. Table 7 shows the evaluation results of our system as well as those of several other comparable related works which share the same experiment setup as ours. We can observe from Table 7 that semantic parsing with SMT components gives 1462 Syst"
D15-1170,P12-1051,0,0.209181,"nd. words. Note that it may generate a synthetic rule with null at the target side since the lexical translation table derived from aligned training data contains translation to null. Each synthetic translation rule for unknown words is associated with three features returned from function generate rule. 4 Experimentation In this section, we test our approach on the GeoQuery dataset, which is publicly available. 4.1 Experimental Settings Data GeoQuery dataset consists of 880 questions paired with their corresponding tree structured semantic representations. Following the experimental setup in Jones et al. (2012), we use the 600 question pairs to train and tune our SMT de1459 SCFG coder, and evaluated on the remaining 280. Note that there is another version of GeoQuery dataset where the semantic representation is annotated with lambda calculus expressions and which is extensively studied (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). Performance on the version of lambda calculus is higher than that on the tree structured version, however, the results obtained over the two versions are not directly comparable. SMT Setting We use cdec (Dyer et al.,"
D15-1170,P02-1040,0,0.0958993,"ectly comparable. SMT Setting We use cdec (Dyer et al., 2010) as our HPB decoder. As mentioned above, 600 instances are used to train and tune our decoder. To get fair results, we split the 600 instances into 10 folds, each having 60 instances. Then for each fold, we use it as the tuning data while the other 540 instances and the NP list are used as training data.7 We use IRSTLM toolkit (Federico et al., 2008) to train a 5-gram LM on the MRL0 side of the training data, using modified Kneser-Ney smoothing. We use Mira (Chiang et al., 2008) to tune the parameters of the system to maximize BLEU (Papineni et al., 2002). When extracting translation rules from aligned training data, we include both tight and untight phrases. Evaluation We use the standard evaluation criteria for evaluation by executing both the predicted MRL and the gold standard against the database and obtaining their respective answer. Specifically, we convert a translation from MRL0 into MRL (if exists). The translation then is considered correct if and only if its MRL retrieves the same answers as the gold standard MRL (Jones et al., 2012), allowing for a fair comparison between our systems and previous works. As in Jones et al. (2012),"
D15-1170,P07-2045,0,0.00630803,"Missing"
D15-1170,D09-1001,0,0.0817093,"age. Such an extension share a similar spirit as generalization of a CCG lexicon for CCG-based semantic parser (Kwiatkowski et al., 2011; Wang et al., 2014). Experiments on benchmark data have shown that our model is competitive to previous work and achieves state-of-the-art performance across a few different languages. Recently the research of semantic parsing in open domain with weakly (or un-) supervised setups, under different settings where the goal was to optimize the performance of certain downstream NLP tasks such as answering questions, has received a significant amount of attention (Poon and Domingos, 2009; Clarke et al., 2010; Berant et al., 2013; Berant and Liang, 2014). One direction of our future work is to extend the current framework to support the generation of synthetic translation rules from weaker signals (e.g., from question-answer pairs), rather than from aligned parallel data. We also noticed recent advance in tree-based SMT. Applying such string-to-tree or tree-to-tree translation models (Yamada and Knight, 2001; Shen et al., 2008) to semantic parsing will naturally resolve the inconsistent semantic structure issue, though they require additional information to generate tree label"
D15-1170,P08-1066,0,0.0136646,"goal was to optimize the performance of certain downstream NLP tasks such as answering questions, has received a significant amount of attention (Poon and Domingos, 2009; Clarke et al., 2010; Berant et al., 2013; Berant and Liang, 2014). One direction of our future work is to extend the current framework to support the generation of synthetic translation rules from weaker signals (e.g., from question-answer pairs), rather than from aligned parallel data. We also noticed recent advance in tree-based SMT. Applying such string-to-tree or tree-to-tree translation models (Yamada and Knight, 2001; Shen et al., 2008) to semantic parsing will naturally resolve the inconsistent semantic structure issue, though they require additional information to generate tree labels on the target side. However, due to the constraint that each target phrase needs to map to a syntactic constituent, phrase tables in tree-based translation models usually suffer from the low coverage issue, especially if the training data size is small. Therefore, another direction of our future work is to explore specific problems that will emerge when employing tree-based SMT systems to semantic parsing, and provide solutions to them. Ackno"
D15-1170,C12-2122,0,0.0498778,"Missing"
D15-1170,D14-1135,0,0.10458,"Thai, the performance is still lower than those of RHT and TREETRANS. This is probably because of the low quality of word alignment output between this Asian language and MRL. 6 Conclusion and Future Work In this paper, we have presented an enriched SCFG approach for semantic parsing which realizes the potential of the SMT approach. The performance improvement is contributed from the extension of translation rules with informative symbols and increased coverage. Such an extension share a similar spirit as generalization of a CCG lexicon for CCG-based semantic parser (Kwiatkowski et al., 2011; Wang et al., 2014). Experiments on benchmark data have shown that our model is competitive to previous work and achieves state-of-the-art performance across a few different languages. Recently the research of semantic parsing in open domain with weakly (or un-) supervised setups, under different settings where the goal was to optimize the performance of certain downstream NLP tasks such as answering questions, has received a significant amount of attention (Poon and Domingos, 2009; Clarke et al., 2010; Berant et al., 2013; Berant and Liang, 2014). One direction of our future work is to extend the current framew"
D15-1170,N06-1056,0,0.475908,"erman and Greek. 1 (a) before pre-‐processing NL’: what be the area of sea0le MRL’: answer@1 area_1@1 cityid@2 sea0le@s _@0 (b) aGer pre-‐processing Figure 1: Example of a sentence pair in NL and MRL. naturally viewed as a statistical machine translation (SMT) task, which translates a sentence in NL (i.e., the source language in SMT) into its meaning representation in MRL (i.e., the target language in SMT). Indeed, many attempts have been made to directly apply statistical machine translation (SMT) systems (or methodologies) to semantic parsing (Papineni et al., 1997; Macherey et al., 2001; Wong and Mooney, 2006; Andreas et al., 2013). However, although recent studies (Wong and Mooney, 2006; Andreas et al., 2013) show that semantic parsing with SCFGs, which form the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2007), achieves favorable results, this approach is still behind the most recent state-of-the-art. For details, please see performance comparison in Andreas et al. (2013) and Lu (2014). Introduction Semantic parsing, the task of mapping natural language (NL) sentences into a formal meaning representation language (MRL), has recently receiv"
D15-1170,P07-1121,0,0.64142,"section, we test our approach on the GeoQuery dataset, which is publicly available. 4.1 Experimental Settings Data GeoQuery dataset consists of 880 questions paired with their corresponding tree structured semantic representations. Following the experimental setup in Jones et al. (2012), we use the 600 question pairs to train and tune our SMT de1459 SCFG coder, and evaluated on the remaining 280. Note that there is another version of GeoQuery dataset where the semantic representation is annotated with lambda calculus expressions and which is extensively studied (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2013). Performance on the version of lambda calculus is higher than that on the tree structured version, however, the results obtained over the two versions are not directly comparable. SMT Setting We use cdec (Dyer et al., 2010) as our HPB decoder. As mentioned above, 600 instances are used to train and tune our decoder. To get fair results, we split the 600 instances into 10 folds, each having 60 instances. Then for each fold, we use it as the tuning data while the other 540 instances and the NP list are used as training data.7 We use IRSTLM toolkit"
D15-1170,P01-1067,0,0.654953,"anslation (SMT) task, which translates a sentence in NL (i.e., the source language in SMT) into its meaning representation in MRL (i.e., the target language in SMT). Indeed, many attempts have been made to directly apply statistical machine translation (SMT) systems (or methodologies) to semantic parsing (Papineni et al., 1997; Macherey et al., 2001; Wong and Mooney, 2006; Andreas et al., 2013). However, although recent studies (Wong and Mooney, 2006; Andreas et al., 2013) show that semantic parsing with SCFGs, which form the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2007), achieves favorable results, this approach is still behind the most recent state-of-the-art. For details, please see performance comparison in Andreas et al. (2013) and Lu (2014). Introduction Semantic parsing, the task of mapping natural language (NL) sentences into a formal meaning representation language (MRL), has recently received a significant amount of attention with various models proposed over the past few years. Consider the NL sentence paired with its corresponding MRL in Figure 1(a). Semantic parsing can be The key issues behind the limited success of applying SMT s"
D15-1170,W06-3119,0,0.0500807,"te which nonterminal occurrences are linked by ∼. The fact that SCFGs in HPB models contain only one type of non-terminal symbol2 is responsible for ill-formed translation (e.g., answer@1 state@1). To this end, we enrich the nonterminals to capture the tree structure information, guiding the translation in favor of well-formed translations. The enrichment of non-terminals is two-fold: first, it can handle MRL with a nested structure to guarantee the well-formed translations; second, related studies in SMT have shown that introducing multiple non-terminal symbols in SCFGs benefits translation (Zollmann and Venugopal, 2006; Li et al., 2012). Given a word sequence eij from position i to position j in MRL0 , we enrich the non-terminal symbol X to reflect the internal structure of the word sequence of eij . A correct translation rule selection therefore not only maps source terminals into target terminals, but is both constrained and guided by structure information in the nonterminals. As mentioned earlier, we regard the nested structure in MRL0 as function-argument structure, where each function takes one or more arguments as input while its return serves as an argument to the outside function. As in Figure 1, fu"
D15-1187,D10-1036,0,0.0276342,"osed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) participated in. They identified negation focus by three heuristic rules. Our previous work (Zou et al., 2014) demonstrates the effectiveness of contextual information for negation focus identification. On this basis, we further optimize the graph model in both the topical layer and the PageRank algorithm in this paper. In recent years, many algorithms are widely used to incorporate word graph models and topical information within random walk. Our work is originally inspired by Liu et al. (2010). Their method runs decomposed Topical PageRank (TPR) for each topic separately, and then calculates the word scores with respect to different topics. When setting the edge weights, only word co-occurrence is considered. Different from their work, our word-topic graph model runs on a twolayers (word layer and topical layer) graph model and sets the edge weights by measuring both word similarity and topic distribution. 3 Methods The word-topic graph model consists of word layer and topical layer, as shown in Figure 1. While the word layer is constructed according to word co-occurrence within a"
D15-1187,S12-1035,0,0.0896157,"rimental results and analysis. Finally, we conclude our work in Section 5. Corresponding author 1632 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1632–1636, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. 2 3.1 Related Work So far there is little work on negation focus identification, which was pioneered by Blanco and Moldovan (2011) who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) participated in. They identified negation focus by three heuristic rules. Our previous work (Zou et al., 2014) demonstrates the effectiveness of contextual information for negation focus identification. On this basis, we further optimize the graph model in both the topical layer and the PageRank algorithm in this paper. In recent years, many algorithms are widely used to incorporate word graph models and topical information within random walk. Our work is originally inspire"
D15-1187,S12-1039,0,0.264952,"e 2015 Conference on Empirical Methods in Natural Language Processing, pages 1632–1636, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. 2 3.1 Related Work So far there is little work on negation focus identification, which was pioneered by Blanco and Moldovan (2011) who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) participated in. They identified negation focus by three heuristic rules. Our previous work (Zou et al., 2014) demonstrates the effectiveness of contextual information for negation focus identification. On this basis, we further optimize the graph model in both the topical layer and the PageRank algorithm in this paper. In recent years, many algorithms are widely used to incorporate word graph models and topical information within random walk. Our work is originally inspired by Liu et al. (2010). Their method runs decomposed Topical PageRank (TPR) for each topic separately, and then calculate"
D15-1187,P14-1049,1,0.731887,"Missing"
D15-1187,P11-1059,0,0.288678,"e due to manual preparation of annotated corpus. To address this problem, we propose an unsupervised word-topic graph model to represent and measure the focus candidates from both lexical and topic perspectives. Moreover, we propose a document-sensitive biased PageRank algorithm to optimize the ranking scores of focus candidates. Evaluation on the *SEM 2012 shared task corpus shows that our proposed method outperforms the state of the art on negation focus identification. * 1 Introduction Negation is used to reverse the polarity of part of statements that are otherwise affirmative by default (Blanco and Moldovan, 2011), which is common in natural language. Negation focus is defined as the special part in sentence, which is most prominently or explicitly negated by a negative expression. For example, sentence (1) could be interpreted as He stopped, but not until he got to Jackson Hole with a positive part he stopped and a negative part until he got to Jackson Hole. (1) He didn&apos;t stop until he got to Jackson Hole. Our previous work (Zou et al., 2014) showed that contextual information plays a critical role on negation focus identification. For better illustration of this conclusion, they manually analyze the"
D15-1187,W04-3252,0,\N,Missing
D16-1078,P15-1017,0,0.323926,"ection. CNN models, firstly invented to capture more abstract features for computer vision (LeCun et al., 1989), have achieved certain success on various NLP tasks in 1 In this paper, cues are in bold face, and scopes are in [brackets] in the example sentences. 815 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 815–825, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics recent years, such as semantic role labeling (Collobert et al., 2011), machine translation (Meng et al., 2015; Hu et al., 2015), event extraction (Chen et al., 2015; Nguyen et al., 2015), etc. These studies have proved the ability of CNN models in learning meaningful features. In particular, our CNN-based model extracts various kinds of meaningful features from the syntactic paths between the cue and the candidate token in both constituency and dependency parse trees. The importance of syntactic information in scope detection has been justified in previous work (Velldal et al., 2012; Lapponi et al., 2012; Zou et al., 2013, etc). Our model can also benefit from the ability of neural networks in extracting useful information from syntactic paths (Xu et al."
D16-1078,E99-1043,0,0.269878,"tences 25.47 24.54 7.71 #Sentences 2101 519 855 #Scopes 2659 672 1112 Spe Ave. Len Sentences 29.77 30.76 11.96 Ave. Len Scopes 15.10 13.38 4.92 #Sentences 1597 339 865 #Scopes 1719 376 870 Neg Ave. Len Sentences 29.28 30.55 8.53 Ave. Len Scopes 7.60 7.35 3.87 (Notes: “Ave. Len” denotes average length; “Abs”, “Papers” and “Cli” denote Abstracts, Full Papers and Clinical Records, respectively; “Spe” and “Neg” denote speculation and negation, respectively.) Table 1: Statistics on the BioScope corpus. BioScope includes 3 different sub-corpora: Abstracts of biological papers from the GENIA corpus (Collier et al., 1999), Full scientific Papers from Flybase and BMC Bioinformatics website, and Clinical radiology Records corpus. These texts in three sub-corpora ensure that BioScope can capture the heterogeneity of language use in biomedical domain. While Abstracts and Full Papers share the same genre, Clinical Records consists of shorter sentences. Previous studies regarded Abstracts as the main resource for text mining applications due to its public accessibility (e.g. through PubMed). Table 1 shows the statistics of the BioScope corpus. While in both Abstracts and Full Papers, the average lengths of speculati"
D16-1078,P15-2088,0,0.0231757,"rk (CNN)based approach for scope detection. CNN models, firstly invented to capture more abstract features for computer vision (LeCun et al., 1989), have achieved certain success on various NLP tasks in 1 In this paper, cues are in bold face, and scopes are in [brackets] in the example sentences. 815 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 815–825, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics recent years, such as semantic role labeling (Collobert et al., 2011), machine translation (Meng et al., 2015; Hu et al., 2015), event extraction (Chen et al., 2015; Nguyen et al., 2015), etc. These studies have proved the ability of CNN models in learning meaningful features. In particular, our CNN-based model extracts various kinds of meaningful features from the syntactic paths between the cue and the candidate token in both constituency and dependency parse trees. The importance of syntactic information in scope detection has been justified in previous work (Velldal et al., 2012; Lapponi et al., 2012; Zou et al., 2013, etc). Our model can also benefit from the ability of neural networks in extracting useful inform"
D16-1078,S12-1042,0,0.722304,"guistics recent years, such as semantic role labeling (Collobert et al., 2011), machine translation (Meng et al., 2015; Hu et al., 2015), event extraction (Chen et al., 2015; Nguyen et al., 2015), etc. These studies have proved the ability of CNN models in learning meaningful features. In particular, our CNN-based model extracts various kinds of meaningful features from the syntactic paths between the cue and the candidate token in both constituency and dependency parse trees. The importance of syntactic information in scope detection has been justified in previous work (Velldal et al., 2012; Lapponi et al., 2012; Zou et al., 2013, etc). Our model can also benefit from the ability of neural networks in extracting useful information from syntactic paths (Xu et al., 2015a; Xu et al., 2015b) or more complex syntactic trees (Ma et al., 2015; Tai et al., 2015). Moreover, instead of traditional average pooling, our CNN-based model utilizes probabilistic weighted average pooling to alleviate the overfitting problem (Zeiler et al., 2013). Experimental results on BioScope prove the effectiveness of our CNNbased model. The reminder of this paper is organized as follows: Section 2 gives an overview of the relate"
D16-1078,C10-1076,1,0.942203,"Missing"
D16-1078,P15-2029,0,0.0611826,"bility of CNN models in learning meaningful features. In particular, our CNN-based model extracts various kinds of meaningful features from the syntactic paths between the cue and the candidate token in both constituency and dependency parse trees. The importance of syntactic information in scope detection has been justified in previous work (Velldal et al., 2012; Lapponi et al., 2012; Zou et al., 2013, etc). Our model can also benefit from the ability of neural networks in extracting useful information from syntactic paths (Xu et al., 2015a; Xu et al., 2015b) or more complex syntactic trees (Ma et al., 2015; Tai et al., 2015). Moreover, instead of traditional average pooling, our CNN-based model utilizes probabilistic weighted average pooling to alleviate the overfitting problem (Zeiler et al., 2013). Experimental results on BioScope prove the effectiveness of our CNNbased model. The reminder of this paper is organized as follows: Section 2 gives an overview of the related work. Section 3 describes our CNN-based model with probabilistic weighted average pooling for scope detection. Section 4 illustrates the experimental settings, and reports the experimental results and analysis. Finally, Sectio"
D16-1078,P15-1003,0,0.0212434,"tional Neural Network (CNN)based approach for scope detection. CNN models, firstly invented to capture more abstract features for computer vision (LeCun et al., 1989), have achieved certain success on various NLP tasks in 1 In this paper, cues are in bold face, and scopes are in [brackets] in the example sentences. 815 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 815–825, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics recent years, such as semantic role labeling (Collobert et al., 2011), machine translation (Meng et al., 2015; Hu et al., 2015), event extraction (Chen et al., 2015; Nguyen et al., 2015), etc. These studies have proved the ability of CNN models in learning meaningful features. In particular, our CNN-based model extracts various kinds of meaningful features from the syntactic paths between the cue and the candidate token in both constituency and dependency parse trees. The importance of syntactic information in scope detection has been justified in previous work (Velldal et al., 2012; Lapponi et al., 2012; Zou et al., 2013, etc). Our model can also benefit from the ability of neural networks in extrac"
D16-1078,D08-1075,0,0.845804,"Missing"
D16-1078,W09-1304,0,0.107602,"Missing"
D16-1078,W09-1105,0,0.0242881,"Missing"
D16-1078,P15-2060,0,0.0543651,"Missing"
D16-1078,D09-1145,0,0.33263,"Missing"
D16-1078,C10-1155,0,0.0817697,"the speculative cue “may” governs the scope “may harm our lungs”, while the negative cue “not” governs the scope “not like playing football” in sentence S2. Previous work have achieved quite success on cue identification (e.g., with F1-score of 86.79 for speculative cue detection in Tang et al. (2010)). In comparison, speculation and negation scope detection is still a challenge due to its inherent difficulties and those upstream errors. In this paper, we focus on scope detection. Previous work on scope detection can be classified into heuristic rules based methods (e.g., Özgür et al., 2009; Øvrelid et al., 2010), machine learning based methods (e.g., Tang et al., 2010; Zou et al., 2013), and hybrid approaches which integrate empirical models with manual rules (Velldal et al., 2012). Different from those previous studies, this paper presents a Convolutional Neural Network (CNN)based approach for scope detection. CNN models, firstly invented to capture more abstract features for computer vision (LeCun et al., 1989), have achieved certain success on various NLP tasks in 1 In this paper, cues are in bold face, and scopes are in [brackets] in the example sentences. 815 Proceedings of the 2016 Conference o"
D16-1078,W08-0606,0,0.442727,"and θ={W0, W1, b1, W2, b2, W3, b3} is the set of parameters. To train the CNN-based model, the Stochastic Gradient Descent algorithm is applied to fine-tune θ. 4 Experimentation In this section, we first introduce the evaluation data, and then describe the experimental settings. Finally, we report the experimental results and analysis. 4.1 scopes in Clinical Records are shorter than those of other two sub-corpora (Average length: 11.96 (speculation sentence), 8.53 (negation sentence), 4.92 (speculation scope) and 3.87 (negation scope)). 4.2 Corpus We evaluate our CNN-based model on BioScope (Szarvas et al., 2008; Vincze et al., 2008), a widely used and freely available resource consisting of sentences annotated with speculative and negative cues and their scopes in biomedical domain. Abs Papers Cli #Documents 1273 9 1954 11871 2670 6383 Total #Sentences Ave. Len Sentences 25.47 24.54 7.71 #Sentences 2101 519 855 #Scopes 2659 672 1112 Spe Ave. Len Sentences 29.77 30.76 11.96 Ave. Len Scopes 15.10 13.38 4.92 #Sentences 1597 339 865 #Scopes 1719 376 870 Neg Ave. Len Sentences 29.28 30.55 8.53 Ave. Len Scopes 7.60 7.35 3.87 (Notes: “Ave. Len” denotes average length; “Abs”, “Papers” and “Cli” denote Abstr"
D16-1078,P15-1150,0,0.0470671,"Missing"
D16-1078,W10-3002,0,0.611654,"ainty and negation, while negation is a grammatical category which reverses the truth value of a proposition. Commonly, speculation and negation extraction involves two typical subtasks: cue identification and scope detection. Here, a cue is a word or phrase that has speculative or negative meaning In sentence S1, the speculative cue “may” governs the scope “may harm our lungs”, while the negative cue “not” governs the scope “not like playing football” in sentence S2. Previous work have achieved quite success on cue identification (e.g., with F1-score of 86.79 for speculative cue detection in Tang et al. (2010)). In comparison, speculation and negation scope detection is still a challenge due to its inherent difficulties and those upstream errors. In this paper, we focus on scope detection. Previous work on scope detection can be classified into heuristic rules based methods (e.g., Özgür et al., 2009; Øvrelid et al., 2010), machine learning based methods (e.g., Tang et al., 2010; Zou et al., 2013), and hybrid approaches which integrate empirical models with manual rules (Velldal et al., 2012). Different from those previous studies, this paper presents a Convolutional Neural Network (CNN)based approa"
D16-1078,I05-2038,0,0.0331896,"Missing"
D16-1078,W10-3003,0,0.072964,"Missing"
D16-1078,D15-1062,0,0.062542,"al., 2015; Nguyen et al., 2015), etc. These studies have proved the ability of CNN models in learning meaningful features. In particular, our CNN-based model extracts various kinds of meaningful features from the syntactic paths between the cue and the candidate token in both constituency and dependency parse trees. The importance of syntactic information in scope detection has been justified in previous work (Velldal et al., 2012; Lapponi et al., 2012; Zou et al., 2013, etc). Our model can also benefit from the ability of neural networks in extracting useful information from syntactic paths (Xu et al., 2015a; Xu et al., 2015b) or more complex syntactic trees (Ma et al., 2015; Tai et al., 2015). Moreover, instead of traditional average pooling, our CNN-based model utilizes probabilistic weighted average pooling to alleviate the overfitting problem (Zeiler et al., 2013). Experimental results on BioScope prove the effectiveness of our CNNbased model. The reminder of this paper is organized as follows: Section 2 gives an overview of the related work. Section 3 describes our CNN-based model with probabilistic weighted average pooling for scope detection. Section 4 illustrates the experimental setting"
D16-1078,D15-1206,0,0.0923259,"al., 2015; Nguyen et al., 2015), etc. These studies have proved the ability of CNN models in learning meaningful features. In particular, our CNN-based model extracts various kinds of meaningful features from the syntactic paths between the cue and the candidate token in both constituency and dependency parse trees. The importance of syntactic information in scope detection has been justified in previous work (Velldal et al., 2012; Lapponi et al., 2012; Zou et al., 2013, etc). Our model can also benefit from the ability of neural networks in extracting useful information from syntactic paths (Xu et al., 2015a; Xu et al., 2015b) or more complex syntactic trees (Ma et al., 2015; Tai et al., 2015). Moreover, instead of traditional average pooling, our CNN-based model utilizes probabilistic weighted average pooling to alleviate the overfitting problem (Zeiler et al., 2013). Experimental results on BioScope prove the effectiveness of our CNNbased model. The reminder of this paper is organized as follows: Section 2 gives an overview of the related work. Section 3 describes our CNN-based model with probabilistic weighted average pooling for scope detection. Section 4 illustrates the experimental setting"
D16-1078,C14-1220,0,0.0625728,"d a CRF model with POS, chunks, NERs, dependency relations as features. Similarly, Lapponi et al. (2012) employed a CRF model with lexical and dependency features for negation scope and event resolution on the Conan Doyle corpus. These machine learning methods manifest the effectiveness of syntactic features. 2.2 CNN based NLP Applications Currently, CNNs have obtained certain success on various NLP tasks, e.g., part-of-speech tagging, chunking, named entity recognition (Collobert et al., 2011). Specifically, CNNs have been proven effective in extracting sentence-level features. For instance, Zeng et al. (2014) utilized a CNN-based model to extract sentence-level features for relation classification. Zhang et al. (2015) proposed a shallow CNN-based model for implicit discourse relation recognition. Chen et al. (2015) presented a CNN-based model with dynamic multi-pooling on event extraction. More recently, researchers tend to learn features from complex syntactic trees. Ma et al. (2015) use Input Input Position Features & W0 Word embeddings of a path Embeddings Features may↑MD↑VP↓VP↓NP↓PRPP↓our not↑neg↑does↓prep↓like↓pcomp↓playing Path Features W1× Convolutional Layer Position features + b1 Path fea"
D16-1078,D15-1266,0,0.0713733,"yed a CRF model with lexical and dependency features for negation scope and event resolution on the Conan Doyle corpus. These machine learning methods manifest the effectiveness of syntactic features. 2.2 CNN based NLP Applications Currently, CNNs have obtained certain success on various NLP tasks, e.g., part-of-speech tagging, chunking, named entity recognition (Collobert et al., 2011). Specifically, CNNs have been proven effective in extracting sentence-level features. For instance, Zeng et al. (2014) utilized a CNN-based model to extract sentence-level features for relation classification. Zhang et al. (2015) proposed a shallow CNN-based model for implicit discourse relation recognition. Chen et al. (2015) presented a CNN-based model with dynamic multi-pooling on event extraction. More recently, researchers tend to learn features from complex syntactic trees. Ma et al. (2015) use Input Input Position Features & W0 Word embeddings of a path Embeddings Features may↑MD↑VP↓VP↓NP↓PRPP↓our not↑neg↑does↓prep↓like↓pcomp↓playing Path Features W1× Convolutional Layer Position features + b1 Path features concatenate Max Dropout Layer ×Mmask Softmax Layer softmax(W3× +b3) Hidden Layer CNN-based Modeling with"
D16-1078,D13-1099,1,0.572046,"ative cue “not” governs the scope “not like playing football” in sentence S2. Previous work have achieved quite success on cue identification (e.g., with F1-score of 86.79 for speculative cue detection in Tang et al. (2010)). In comparison, speculation and negation scope detection is still a challenge due to its inherent difficulties and those upstream errors. In this paper, we focus on scope detection. Previous work on scope detection can be classified into heuristic rules based methods (e.g., Özgür et al., 2009; Øvrelid et al., 2010), machine learning based methods (e.g., Tang et al., 2010; Zou et al., 2013), and hybrid approaches which integrate empirical models with manual rules (Velldal et al., 2012). Different from those previous studies, this paper presents a Convolutional Neural Network (CNN)based approach for scope detection. CNN models, firstly invented to capture more abstract features for computer vision (LeCun et al., 1989), have achieved certain success on various NLP tasks in 1 In this paper, cues are in bold face, and scopes are in [brackets] in the example sentences. 815 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 815–825, c Austin,"
D16-1078,J12-2005,0,\N,Missing
D18-1079,W10-4310,0,0.0217568,"was released in 2008 (Prasad et al., 2008), there is a significant amount of research has been carried out on discourse-level relation recognition ∗ Corresponding author 725 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherfor"
D18-1079,C18-1048,0,0.0754989,"Missing"
D18-1079,D16-1020,0,0.0169071,"t relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017), only if the omission of inherent conjunctions will not distort the original semantic relations (Rutherford and Xue, 2015). Benefiting from the high-accuracy explicit relation recognition, a simple pattern, such as Argument1+because+Argument2, may enable the acquisition of countless explicitly-related arguments from texts. It makes it p"
D18-1079,W12-1614,0,0.0585359,"8 (Prasad et al., 2008), there is a significant amount of research has been carried out on discourse-level relation recognition ∗ Corresponding author 725 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud"
D18-1079,P09-2004,0,0.0594837,"enn Discourse Treebank of version 2.0 (PDTB) was released in 2008 (Prasad et al., 2008), there is a significant amount of research has been carried out on discourse-level relation recognition ∗ Corresponding author 725 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating i"
D18-1079,P16-1163,0,0.150888,"age Processing, pages 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017), only if the omission of inherent conjunctions will not distort the original semantic relations (Rutherford and Xue, 2015). Benefiting from the high-accuracy explicit rela"
D18-1079,prasad-etal-2008-penn,0,0.188193,"Missing"
D18-1079,C18-1046,0,0.586217,"Missing"
D18-1079,D16-1246,0,0.54997,"s in Natural Language Processing, pages 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017), only if the omission of inherent conjunctions will not distort the original semantic relations (Rutherford and Xue, 2015). Benefiting from the high-acc"
D18-1079,P17-1093,0,0.11412,"es 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017), only if the omission of inherent conjunctions will not distort the original semantic relations (Rutherford and Xue, 2015). Benefiting from the high-accuracy explicit relation recognition,"
D18-1079,D17-1134,0,0.443223,"Missing"
D18-1079,E14-1068,0,0.0416032,", there is a significant amount of research has been carried out on discourse-level relation recognition ∗ Corresponding author 725 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al."
D18-1079,D09-1036,0,0.513605,"Missing"
D18-1079,N15-1081,0,0.376031,"a causal relation. We tackle discourse-level relation recognition, a problem of determining semantic relations between text spans. Implicit relation recognition is challenging due to the lack of explicit relational clues. The increasingly popular neural network techniques have been proven effective for semantic encoding, whereby widely employed to boost semantic relation discrimination. However, learning to predict semantic relations at a deep level heavily relies on a great deal of training data, but the scale of the publicly available data in this field is limited. In this paper, we follow Rutherford and Xue (2015) to expand the training data set using the corpus of explicitly-related arguments, by arbitrarily dropping the overtly presented discourse connectives. On the basis, we carry out an experiment of sampling, in which a simple active learning approach is used, so as to take the informative instances for data expansion. The goal is to verify whether the selective use of external data not only reduces the time consumption of retraining but also ensures a better system performance. Using the expanded training data, we retrain a convolutional neural network (CNN) based classifer which is a simplified"
D18-1079,D16-1130,0,0.126022,"els, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017), only if the omission of inherent conjunctions will not distort the original semantic relations (Rutherford and Xue, 2015). Benefiting from the high-accuracy explicit relation recognition, a simple pattern, s"
D18-1079,D16-1253,0,0.135559,"Missing"
D18-1079,P17-2042,0,0.0622622,"to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017), only if the omission of inherent conjunctions will not distort the original semantic relations (Rutherford and Xue, 2015). Benefiting from the high-accuracy explicit relation recognition, a simple pattern, such as Argument1+because+Argument2, may enable the acquisition of countless explicitly-related arguments from texts. It makes it possible to cooperate with Rutherford"
D18-1079,D15-1266,0,0.135733,"on Empirical Methods in Natural Language Processing, pages 725–731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work Multi-class implicit relation recognition can be boiled down to a classification problem. This encorages the study of supervised classification at the earlier time (Pitler and Nenkova, 2009; Lin et al., 2009; Louis et al., 2010; Park and Cardie, 2012; Rutherford and Xue, 2014). Recently, the neural network based approaches become increasingly popular due to the capacity of deep semantic learning and understanding (Zhang et al., 2015; Qin et al., 2016; Chen et al., 2016; Qin et al., 2017; Liu and Li, 2016). However, a large amount of labeled data is urgently needed to train the models. (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017). The explicitly-related arguments in the corpus of PDTB has been sufficiently proven to be usable for creating implicitly-related arguments (Rutherford and Xue, 2015; Braud and Denis, 2016; Liu et al., 2016; Wu et al., 2017), only if the omission of inherent conjunctions will not distort the original semantic relations (Rutherford and Xue, 2015). Benefiting"
D18-1079,C08-1143,0,0.0404349,"n Imp Exp Unlabeled Data Adoption Expansion Experts Exp2Imp General AL workflow Cooperating AL with Exp2Imp Table 1: Hyperparameter settings of CNN data set will be taken, only if their informativeness scores are higher than a constant threshold θ: Figure 2: Workflows of ALs U 0 = {xi |Inf (xi ) &gt; θ, ∀xi ∈ U } where, U is the unlabeled data set while U 0 consists of the potentially informative instances. a series of successive iterations, or termination after a fixed number of iterations. 5 5.2 Cooperating AL with Exp2Imp Informativeness Measurement We employ an uncertainty sampling function (Zhu et al., 2008; Settles, 2010; Yang et al., 2015; Ramirez-Loaiza et al., 2017) to measure the informativeness: X Inf (xi ) = Irj (xi ; M ) (2) 5.3 X P (rj |xi ) log P (rj |xi ) CNN based Classification We follow Qin et al. (2016) to use Siamese CNN for argument modeling and relation classification. The 300-dimensional word embeddings and 50dimensional POS embeddings are used to represent the arguments. We also follow Mikolov et al. (2013) to pretrain the word embeddings and initialize the POS by random sampling in [-1,1]. Table 1 shows the hyperparameter settings. The source codes of AL, Exp2Imp and Siamese"
D18-1401,P07-1056,0,0.0552003,"g by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a"
D18-1401,D15-1007,0,0.0135061,"darker. Is the sun cream really effective? Answer 2: No, just depending on my own experience. Figure 1: Two examples of QA text pairs from “customer questions & answers” section in Amazon. Introduction Sentiment analysis, a.k.a. opinion mining, is a task which aims to identify the user sentiment orientation of a product/brand/service by monitoring the online textual data, e.g., reviews and social media messages. It has attracted huge attention in both academic and industrial communities due to its widespread applications, such like recommendation (Zhang et al., 2014) and social media mining (Chambers et al., 2015). As the fundamental component in sentiment analysis, sentiment classification mainly classifies the sentiment polarity as positive or negative, and has been well-studied from both sentence-level (Kim and Hovy, 2004) and document-level (Xu et al., 2016). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With th"
D18-1401,P17-1055,0,0.0244919,"ed by concatenating the forward and backward hidden states. For simplicity, we note contextual representation of SQi as HQi , and contextual representation of SAj as HAj respectively: HQi = [hi,1 , hi,2 , ..., hi,n , ..., hi,Ni ] (1) HAj = [hj,1 , hj,2 , ..., hj,m , ..., hj,Mj ] (2) where D[i,j] ∈ RNi ×Mj denotes the bidirectional matching matrix for the [SQi , SAj ] unit. Each element in D[i,j] is the score that measures how well the word in SQi semantically matches the word in SAj and vice versa. Given the bidirectional matching matrix D[i,j] , we use attention mechanism (Yang et al., 2016; Cui et al., 2017) to mine the sentiment matching information between question and answer from two directions, which could be seen as an Answerto-Question attention and a Question-to-Answer attention as follows. • Answer-to-Question Attention: We employ row-wise operations to compute the attention r weight vector α[i,j] as follows: r &gt; U[i,j] = tanh(Wr · D[i,j] ) (4) r r α[i,j] = softmax(wr&gt; · U[i,j] ) (5) r where α[i,j] ∈ RNi is the Answer-to-Question attention weight vector regarding the importance degrees of all words in Q-sentence SQi , Wr ∈ 0 0 Rd ×Mj and wr ∈ Rd are weight matrices. After computing the An"
D18-1401,P11-2104,0,0.019958,"ional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level. Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis. Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits. Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995). More recently, some studies aim to learn better word embedding of a word rather than its polarity. Tang et al. (2014) developed three neural networks to learn word em3655 Beauty Shoe Electronic Positive 3,676 4,025 3,807 Negative 981 819 1,017 Conflict 318 412 528 Neutral 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporating sentiment polarities o"
D18-1401,P10-1041,0,0.015063,"eral, the research on traditional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level. Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis. Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits. Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995). More recently, some studies aim to learn better word embedding of a word rather than its polarity. Tang et al. (2014) developed three neural networks to learn word em3655 Beauty Shoe Electronic Positive 3,676 4,025 3,807 Negative 981 819 1,017 Conflict 318 412 528 Neutral 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporati"
D18-1401,P11-1013,0,0.0608791,"Missing"
D18-1401,C04-1200,0,0.274096,"is, a.k.a. opinion mining, is a task which aims to identify the user sentiment orientation of a product/brand/service by monitoring the online textual data, e.g., reviews and social media messages. It has attracted huge attention in both academic and industrial communities due to its widespread applications, such like recommendation (Zhang et al., 2014) and social media mining (Chambers et al., 2015). As the fundamental component in sentiment analysis, sentiment classification mainly classifies the sentiment polarity as positive or negative, and has been well-studied from both sentence-level (Kim and Hovy, 2004) and document-level (Xu et al., 2016). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics c"
D18-1401,D15-1180,0,0.0466625,"Missing"
D18-1401,P10-1043,1,0.831516,"Missing"
D18-1401,P15-2005,1,0.840444,"e annotated data in three domains. bedding by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classificatio"
D18-1401,D17-1048,0,0.011926,"et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance. Wang et al."
D18-1401,P14-5010,0,0.00460503,"Missing"
D18-1401,D15-1298,0,0.0617943,"Missing"
D18-1401,W02-1011,0,0.0273117,"[Q-sentence, A-sentence] unit for exploring sentiment information. Finally, the self-matching attention layer in the model can capture the importance of these [Q-sentence, A-sentence] matching vectors obtained from QA bidirectional matching layer, which could effectively refine the evidence for inferring the sentiment polarity of a QA text pair. Experimental results show that the proposed approach significantly outperforms several strong baselines for QA-style sentiment classification. 2 Related Work Sentiment classification has become a hot research field in NLP since the pioneering work by Pang et al. (2002). In general, the research on traditional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level. Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis. Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the searc"
D18-1401,P13-4009,0,0.17484,"Missing"
D18-1401,W06-1652,0,0.0451964,"l 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel"
D18-1401,P15-1098,0,0.01816,"iment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011). On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an a"
D18-1401,C14-1053,0,0.0241938,"6). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics comparing with traditional reviews (Wachsmuth et al., 2014; Zhou et al., 2015a). Comparing to the traditional reviews, the QA style reviews can be more informative and convincing. More importantly, because answer providers are randomly picked from the users who already purchased the target item, this new form of review can be more reliable and trustful. Regarding QA-style sentiment analysis, one straightforward method is to directly employ an existing sentiment classification approach that works well on traditional reviews, such as RNN (Nguyen and Shirai, 2015) and LSTM (Chen et al., 2016). However, because of the significant differences between QA-s"
D18-1401,D16-1058,0,0.308112,"assification. Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification. Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance. Wang et al. (2018) proposed a hierarchical attention network to explore both word-level and clause-level sentiment information towards a target aspect. Unlike all the prior studies, this paper focuses on a very different kind of text representation, i.e., QA-style text level, for sentiment classificatio"
D18-1401,P15-1102,0,0.0469393,"Missing"
D18-1401,D16-1172,0,0.173147,"ch aims to identify the user sentiment orientation of a product/brand/service by monitoring the online textual data, e.g., reviews and social media messages. It has attracted huge attention in both academic and industrial communities due to its widespread applications, such like recommendation (Zhang et al., 2014) and social media mining (Chambers et al., 2015). As the fundamental component in sentiment analysis, sentiment classification mainly classifies the sentiment polarity as positive or negative, and has been well-studied from both sentence-level (Kim and Hovy, 2004) and document-level (Xu et al., 2016). ∗ Corresponding author Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics comparing with traditional reviews (Wa"
D18-1401,N16-1174,0,0.313624,"f each word is formed by concatenating the forward and backward hidden states. For simplicity, we note contextual representation of SQi as HQi , and contextual representation of SAj as HAj respectively: HQi = [hi,1 , hi,2 , ..., hi,n , ..., hi,Ni ] (1) HAj = [hj,1 , hj,2 , ..., hj,m , ..., hj,Mj ] (2) where D[i,j] ∈ RNi ×Mj denotes the bidirectional matching matrix for the [SQi , SAj ] unit. Each element in D[i,j] is the score that measures how well the word in SQi semantically matches the word in SAj and vice versa. Given the bidirectional matching matrix D[i,j] , we use attention mechanism (Yang et al., 2016; Cui et al., 2017) to mine the sentiment matching information between question and answer from two directions, which could be seen as an Answerto-Question attention and a Question-to-Answer attention as follows. • Answer-to-Question Attention: We employ row-wise operations to compute the attention r weight vector α[i,j] as follows: r &gt; U[i,j] = tanh(Wr · D[i,j] ) (4) r r α[i,j] = softmax(wr&gt; · U[i,j] ) (5) r where α[i,j] ∈ RNi is the Answer-to-Question attention weight vector regarding the importance degrees of all words in Q-sentence SQi , Wr ∈ 0 0 Rd ×Mj and wr ∈ Rd are weight matrices. Aft"
D18-1401,D16-1021,0,0.0277727,"Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification. Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task. Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance. Wang et al. (2018) proposed a hierarchical attention network to explore both word-level and clause-level sentiment information towards a target aspect. Unlike all the prior studies, this paper focuses on a very different kind of text representation, i.e., QA-style text level, for sentiment classification. To the best of our knowledge, this is the first attempt to perform sentiment classification on this text level. 3 Data Collection and Annotation We collect QA text pairs from “Asking"
D18-1401,P14-1146,0,0.0370401,"their efforts to predicting the sentiment polarity of a word with different learning models and resources. Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits. Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995). More recently, some studies aim to learn better word embedding of a word rather than its polarity. Tang et al. (2014) developed three neural networks to learn word em3655 Beauty Shoe Electronic Positive 3,676 4,025 3,807 Negative 981 819 1,017 Conflict 318 412 528 Neutral 5,025 4,744 4,648 Total 10,000 10,000 10,000 Table 1: Category distribution of the annotated data in three domains. bedding by incorporating sentiment polarities of text in loss functions. Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding. Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis. On o"
D18-1401,P15-1042,0,0.129521,"or Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao. In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s). With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics comparing with traditional reviews (Wachsmuth et al., 2014; Zhou et al., 2015a). Comparing to the traditional reviews, the QA style reviews can be more informative and convincing. More importantly, because answer providers are randomly picked from the users who already purchased the target item, this new form of review can be more reliable and trustful. Regarding QA-style sentiment analysis, one straightforward method is to directly employ an existing sentiment classification approach that works well on traditional reviews, such as RNN (Nguyen and Shirai, 2015) and LSTM (Chen et al., 2016). However, because of the significant differences between QA-style and classical"
D19-1168,N18-1118,0,0.0631454,"el machine translation. Most of existing studies aim to improve overall translation quality with the aid of document context. Among them, Maruf and Haffrai (2018), Wang et al. (2017), Zhang et al. (2018) and Miculicich et al. (2018) use extractionbased models to extract partial document context from previous sentences of the current sentence. In addition, Tu et al. (2018) and Kuang et al. (2018) employ cache-based models to selectively memorize the most relevant information in the document context. Different from above extraction-based models and cache-based models, there are also some works (Bawden et al., 2018; Voita et al., 2018) that pay much attention to discourse phenomena (Mitkov, 1999) related to document-level translation. Although these approaches have achieved some progress in document-level machine translation, they still suffer from incomplete document context. Further more, most of previous works are based on the RNNSearch model, and only few exceptions (Zhang et al., 2018; Miculicich et al., 2018) are on top of the state-of-the-art Transformer model. 6 Conclusion We have presented a hierarchical model to capture the global document context for documentlevel NMT. The proposed model can"
D19-1168,2012.eamt-1.60,0,0.231333,": X logP (y|x; θd , θˆs ) (12) θˆd = arg max θd 3 &lt;x,y>∈Dd Experimentation To examine the effect of our proposed HM-GDC model, we conduct experiments on both ChineseEnglish and German-English translation. 3.1 Experimental Settings Datasets For Chinese-English translation, we carry out experiments with sentence- and document-level corpora on two different domains: news and talks. For the sentence-level corpus, we use 2.8M sentence pairs from news corpora LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hongkong Hansards/Laws/News). We use the Ted talks corpus from the IWSLT 2017 (Cettolo et al., 2012) evaluation campaigns1 as the document-level parallel corpus, including 1,906 documents with 226K sentence pairs. We use dev2010 which contains 8 documents with 879 sentence pairs for development and tst2012tst2015 which contain 62 documents with 5566 sentence pairs for testing. For German-English translation, we use the document-level parallel Ted talks corpus from the IWSLT 2014 (Cettolo et al., 2012) evaluation campaigns, which contain 1,361 documents with 172K 1579 1 https://wit3.fbk.eu Method pre-training tst12 tst13 tst14 tst15 Avg RNNSearch (Bahdanau et al., 2015) Wang et al. (2017) Our"
D19-1168,P17-4012,0,0.160195,"Missing"
D19-1168,C18-1050,1,0.568516,"he NMT model to take global context into consideration and thus it can safely disambiguate those multi-sense words like chengxu. 5 Related Work Recent years have witnessed a variety of approaches proposed for document-level machine translation. Most of existing studies aim to improve overall translation quality with the aid of document context. Among them, Maruf and Haffrai (2018), Wang et al. (2017), Zhang et al. (2018) and Miculicich et al. (2018) use extractionbased models to extract partial document context from previous sentences of the current sentence. In addition, Tu et al. (2018) and Kuang et al. (2018) employ cache-based models to selectively memorize the most relevant information in the document context. Different from above extraction-based models and cache-based models, there are also some works (Bawden et al., 2018; Voita et al., 2018) that pay much attention to discourse phenomena (Mitkov, 1999) related to document-level translation. Although these approaches have achieved some progress in document-level machine translation, they still suffer from incomplete document context. Further more, most of previous works are based on the RNNSearch model, and only few exceptions (Zhang et al., 2"
D19-1168,P18-1118,0,0.0980894,"Missing"
D19-1168,D18-1325,0,0.732557,"ument-level translation under the guidance of context. Introduction ∗ 瀖 瀖 translation and results on news benchmark test sets have shown its “translation quality at human parity when compared to professional human translators” (Hassan et al., 2018). However, when turning to document-level translation, even the Transformer model yields a low performance as it translates each sentence in the document independently and suffers from the problem of ignoring document context. To address above challenge, various extractionbased methods (Maruf and Haffari, 2018; Wang et al., 2017; Zhang et al., 2018; Miculicich et al., 2018) have been proposed to extract previous context (pre-context) to guide the translation of the current sentence si , as shown in Figure 1. However, when there exists a huge gap between the pre-context and the context after the current sentence si , the guidance from pre-context is not sufficient for the NMT model to fully disambiguate the sentence si . On the one hand, the translation of the current sentence si may be inaccurate due to the one-sidedness of partial context. On the other hand, translating the succeeding sentences in the document may much suffer from the semantic bias due to the t"
D19-1168,W17-4802,0,0.312779,"antly improved by 5.26 BLEU points (the last row in Table 4). The overall results prove that our proposed model is robust and promising. It can significantly improve the performance of document-level translation when a two-step training strategy is used. 4.4 Pronoun & Noun Translation To intuitively illustrate how the translation performance is improved by our proposed HM-GDC model, we conduct a further analysis on pronoun and noun translation. For the pronoun translation, we evaluate the coreference and anaphora using the referencebased metric: the accuracy of pronoun translation (Miculicich Werlen and Popescu-Belis, 2017) in Chinese-English and German-English translation as shown in Table 5. From the results, our proposed HM-GDC model can well improve the performance of pronoun translation in both corpora due to the well captured global document context assigned to each word. Correspondingly, we display a translation example in Table 6 to further illustrate this. From the example, given the surrounding context, our proposed HM-GDC model can well infer the latent pronoun it and thus improve the translation performance of the Transformer model. For the analysis of noun translation, we display another example in"
D19-1168,P02-1040,0,0.103662,"the performance of our proposed HM-GDC model, we integrate the HM-GDC into the standard RNNSearch model to serve as a supplementary experiment. For the RNNSearch network, we borrow the implementation from OpenNMT (Klein et al., 2017). The encoder and decoder layers are all set to 2, the size of the hidden layer is set to 500, and the batch size is set to 64. Same as the Transformer model, we use the most frequent 50K words for both source and target vocabularies. We borrow other settings from (Bahdanau et al., 2015). The evaluation metric for both tasks is case-insensitive BLEU (multi-bleu) (Papineni et al., 2002). 3.2 Experimental Results In this paper, we compare our model with four strong baselines as shown in Table 1. Among them, the RNNSearch (Bahdanau et al., 2015) is a traditional RNN-based encoder-decoder model. Wang et al. (2017) propose to use a hierarchical model to extract partial document context based on the RNNSearch model. To compare with these two RNN-based works, we integrate our proposed HM-GDC model into the encoder of the RNNSearch model using the same method in Section 2.2 and keep other settings the same as the basic RNNSearch model. Different from RNNbased works, Vaswani et al."
D19-1168,P16-1159,0,0.0857443,"s, we combine the outputs of both the Encoder-Decoder attention sub-layer and the DocEnc-Decoder attention sub-layer into one single output H (n) : H (n) = E (n) + G(n) (10) where E (n) is the output of the Encoder-Decoder attention sub-layer, and G(n) is the output of the DocEnc-Decoder attention sub-layer. 2.3 Model Training In document-level translation, the standard training objective is to maximize the log-likelihood of the document-level parallel corpus. However, due to the size limitation of document-level parallel corpora, previous studies (Zhang et al., 2018; Miculicich et al., 2018; Shen et al., 2016) use two-step training strategies to take advantage of large-scale sentence-level parallel pairs. Following their studies, we also take a two-step training strategy to train our model. Specially, we borrow a large-scale corpus with out-of-domain sentencelevel parallel pairs Ds to pre-train our model first, and then use a small-scale corpus with in-domain document-level parallel pairs Dd to fine-tune it. In this work, we follow Voita et al. (2018) to make the sentence and document encoders share the same model parameters. For the RNNSearch model, we share the parameters in the hidden layers of"
D19-1168,Q18-1029,0,0.137865,"ate the HM-GDC into the NMT model to take global context into consideration and thus it can safely disambiguate those multi-sense words like chengxu. 5 Related Work Recent years have witnessed a variety of approaches proposed for document-level machine translation. Most of existing studies aim to improve overall translation quality with the aid of document context. Among them, Maruf and Haffrai (2018), Wang et al. (2017), Zhang et al. (2018) and Miculicich et al. (2018) use extractionbased models to extract partial document context from previous sentences of the current sentence. In addition, Tu et al. (2018) and Kuang et al. (2018) employ cache-based models to selectively memorize the most relevant information in the document context. Different from above extraction-based models and cache-based models, there are also some works (Bawden et al., 2018; Voita et al., 2018) that pay much attention to discourse phenomena (Mitkov, 1999) related to document-level translation. Although these approaches have achieved some progress in document-level machine translation, they still suffer from incomplete document context. Further more, most of previous works are based on the RNNSearch model, and only few exc"
D19-1168,P18-1117,0,0.27534,"arallel corpus. However, due to the size limitation of document-level parallel corpora, previous studies (Zhang et al., 2018; Miculicich et al., 2018; Shen et al., 2016) use two-step training strategies to take advantage of large-scale sentence-level parallel pairs. Following their studies, we also take a two-step training strategy to train our model. Specially, we borrow a large-scale corpus with out-of-domain sentencelevel parallel pairs Ds to pre-train our model first, and then use a small-scale corpus with in-domain document-level parallel pairs Dd to fine-tune it. In this work, we follow Voita et al. (2018) to make the sentence and document encoders share the same model parameters. For the RNNSearch model, we share the parameters in the hidden layers of Bi-RNNs in the sentence and document encoders. For the Transformer model, we share the parameters of the multi-head self-attention layers in the sentence and document encoders. During training, we first optimize the sentencelevel parameters θs (colored in wheat in Figure 3) with the large-scale sentence-level parallel pairs Ds : X θˆs = arg max logP (y|x; θs ) (11) θs &lt;x,y>∈Ds Then, we optimize the document-level parameters θd (colored in pale bl"
D19-1168,D17-1301,0,0.402415,"""$ !""# Figure 1: An illustration of document-level translation under the guidance of context. Introduction ∗ 瀖 瀖 translation and results on news benchmark test sets have shown its “translation quality at human parity when compared to professional human translators” (Hassan et al., 2018). However, when turning to document-level translation, even the Transformer model yields a low performance as it translates each sentence in the document independently and suffers from the problem of ignoring document context. To address above challenge, various extractionbased methods (Maruf and Haffari, 2018; Wang et al., 2017; Zhang et al., 2018; Miculicich et al., 2018) have been proposed to extract previous context (pre-context) to guide the translation of the current sentence si , as shown in Figure 1. However, when there exists a huge gap between the pre-context and the context after the current sentence si , the guidance from pre-context is not sufficient for the NMT model to fully disambiguate the sentence si . On the one hand, the translation of the current sentence si may be inaccurate due to the one-sidedness of partial context. On the other hand, translating the succeeding sentences in the document may m"
D19-1168,D18-1049,0,0.377626,"Missing"
D19-1230,W12-3808,0,0.503265,"s for negative focus detection. Moreover, Zou et al. (2014; 2015) proposed graph-based models to enrich intra-sentence features with inter-sentence features from both lexical and topic perspectives, respectively. Almost all of the above studies have demonstrated that the information contained in context plays a critical role in negative focus detection. In this paper, we explore the effectiveness of different representations and processing manners for modeling context in this task. In addition to the SEM’12 shared task corpus, there are several annotated datasets for negative focus detection. Anand and Martell (2012) made a point about how to describe the contribution of negative focus to a sentence via the theory of question under discussion, and reannotated a part of the SEM’12 shared task corpus. Matsuyoshi et al. (2014) annotated 1,327 instances of negative foci in Japanese text, and proposed a heuristic rulebased approach to identify them. Banjade and Rus (2016) annotated the DT-Neg corpus on tutorial dialogue genre, which contains 1,088 of negative focus instances. Considering the scale and accessibility of the dataset, we utilize the SEM’12 shared task corpus for experimentation. 3 Contextual Atten"
D19-1230,P11-2049,0,0.0194198,"zed as follows. In Section 2, we give a brief review of related work. In Section 3, we introduce the details of the proposed approach. We show our experimental results and discussions in Section 4. Finally, Section 5 concludes with possible directions for future work. 2 Related Work Earlier studies of negation processing in computational linguistics mainly lie in biomedical information extraction (Chapman et al., 2001; Goldin and Chapman, 2003). With the release of the BioScope corpus (Vincze et al., 2008), the negation processing techniques received a substantial boost (Morante et al., 2008; Apostolova et al., 2011; Zou et al., 2013). In addition, negation also was studied as a critical factor in polarity shifting in sentiment and opinion analysis with various lexical and syntactic features (Socher et al., 2013; Cruz et al., 2016). In general, existing studies of negation processing mainly concentrate on three aspects: 1) cue detection, which finds negative triggers or expressions in text; 2) scope resolution, which determines the grammatical scope in a sentence affected by a negative cue; and 3) focus detection, which identifies the most prominently/explicitly negated part in a negative statement. A ne"
D19-1230,L16-1597,0,0.226586,"this paper, we explore the effectiveness of different representations and processing manners for modeling context in this task. In addition to the SEM’12 shared task corpus, there are several annotated datasets for negative focus detection. Anand and Martell (2012) made a point about how to describe the contribution of negative focus to a sentence via the theory of question under discussion, and reannotated a part of the SEM’12 shared task corpus. Matsuyoshi et al. (2014) annotated 1,327 instances of negative foci in Japanese text, and proposed a heuristic rulebased approach to identify them. Banjade and Rus (2016) annotated the DT-Neg corpus on tutorial dialogue genre, which contains 1,088 of negative focus instances. Considering the scale and accessibility of the dataset, we utilize the SEM’12 shared task corpus for experimentation. 3 Contextual Attention-based Negative Focus Detection In this section, we first introduce the BiLSTMCRF framework. On the basis, we then come up with two kinds of attention mechanisms to model the contextual information, i.e. word-level attention and topic-level attention. 3.1 BiLSTM-CRF Framework In this paper, we recast negative focus detection problem as an I/O tagging"
D19-1230,P11-1059,0,0.738492,"etection, which finds negative triggers or expressions in text; 2) scope resolution, which determines the grammatical scope in a sentence affected by a negative cue; and 3) focus detection, which identifies the most prominently/explicitly negated part in a negative statement. A negative focus could be considered as the semantic part in scope that is intended to be interpreted as false to make other parts to be true. Among above these subtasks, negative focus detection is most extremely challenging in negation processing. In the literature, research on negative focus detection was pioneered by Blanco and Moldovan (2011), who proposed a supervised learning model with a set of highly hand-crafted features as a benchmark for this task. On the basis, Blanco and Moldvan (2013) incorporates negation into other existing semantic representations. The dataset annotated by Blanco and Moldovan is used as a standard evaluation corpus for the SEM’12 shared task (Morante and Blanco, 2012). In this shared task, Rosenberg and Bergler (2012) employed three 2252 Output O O O I I CRF layer c1 c2 c3 c4 c5 Backword LSTM r1 r2 r3 r4 r5 Forward LSTM l1 l2 l3 l4 l5 X* w Embedding layer Input X* p w2 w3 w4 X* n Xw softmax Xp w1 Mixe"
D19-1230,matsuyoshi-etal-2014-annotating,0,0.0212634,". Almost all of the above studies have demonstrated that the information contained in context plays a critical role in negative focus detection. In this paper, we explore the effectiveness of different representations and processing manners for modeling context in this task. In addition to the SEM’12 shared task corpus, there are several annotated datasets for negative focus detection. Anand and Martell (2012) made a point about how to describe the contribution of negative focus to a sentence via the theory of question under discussion, and reannotated a part of the SEM’12 shared task corpus. Matsuyoshi et al. (2014) annotated 1,327 instances of negative foci in Japanese text, and proposed a heuristic rulebased approach to identify them. Banjade and Rus (2016) annotated the DT-Neg corpus on tutorial dialogue genre, which contains 1,088 of negative focus instances. Considering the scale and accessibility of the dataset, we utilize the SEM’12 shared task corpus for experimentation. 3 Contextual Attention-based Negative Focus Detection In this section, we first introduce the BiLSTMCRF framework. On the basis, we then come up with two kinds of attention mechanisms to model the contextual information, i.e. wor"
D19-1230,morante-2010-descriptive,0,0.0687896,"Missing"
D19-1230,S12-1035,0,0.598287,"be interpreted as false to make other parts to be true. Among above these subtasks, negative focus detection is most extremely challenging in negation processing. In the literature, research on negative focus detection was pioneered by Blanco and Moldovan (2011), who proposed a supervised learning model with a set of highly hand-crafted features as a benchmark for this task. On the basis, Blanco and Moldvan (2013) incorporates negation into other existing semantic representations. The dataset annotated by Blanco and Moldovan is used as a standard evaluation corpus for the SEM’12 shared task (Morante and Blanco, 2012). In this shared task, Rosenberg and Bergler (2012) employed three 2252 Output O O O I I CRF layer c1 c2 c3 c4 c5 Backword LSTM r1 r2 r3 r4 r5 Forward LSTM l1 l2 l3 l4 l5 X* w Embedding layer Input X* p w2 w3 w4 X* n Xw softmax Xp w1 Mixed embeddings softmax w5 Sp Contextual embeddings Xn Xw S Word-level contextual attention Sn Figure 1: Architecture of BiLSTM-CRF network with word-level contextual attention mechanism for negative focus detection. kinds of heuristic rules for negative focus detection. Moreover, Zou et al. (2014; 2015) proposed graph-based models to enrich intra-sentence featur"
D19-1230,D08-1075,0,0.0450462,"f this paper is organized as follows. In Section 2, we give a brief review of related work. In Section 3, we introduce the details of the proposed approach. We show our experimental results and discussions in Section 4. Finally, Section 5 concludes with possible directions for future work. 2 Related Work Earlier studies of negation processing in computational linguistics mainly lie in biomedical information extraction (Chapman et al., 2001; Goldin and Chapman, 2003). With the release of the BioScope corpus (Vincze et al., 2008), the negation processing techniques received a substantial boost (Morante et al., 2008; Apostolova et al., 2011; Zou et al., 2013). In addition, negation also was studied as a critical factor in polarity shifting in sentiment and opinion analysis with various lexical and syntactic features (Socher et al., 2013; Cruz et al., 2016). In general, existing studies of negation processing mainly concentrate on three aspects: 1) cue detection, which finds negative triggers or expressions in text; 2) scope resolution, which determines the grammatical scope in a sentence affected by a negative cue; and 3) focus detection, which identifies the most prominently/explicitly negated part in a"
D19-1230,J05-1004,0,0.402828,"Missing"
D19-1230,N18-1202,0,0.0638671,"total, this corpus contains 3,544 instances of negative focus. For fair comparison, we adopt the same partition as SEM’12 shared task in all experiments, i.e., with 2,304 for training, 531 for development, and 712 for testing. For each instance, the corpus provides the previous and next sentences as contexts. We evaluate our results in terms of accuracy (acc for short), i.e., for each negation, the predicted focus is considered correct if it is a perfect match with its gold annotation. Table 1 lists the hyper-parameters in our experiments. We utilize the pre-trained word embeddings by ELMo5 (Peters et al., 2018). The semantic role embeddings are initialized randomly by a continuous uniform distribution. All the models are optimized using the stochastic gradient descent (SGD). We pick the parameters showing the best performance on the development set when no further improvement occurs within 50 epochs, and report the performances on the test set. To verify the effectiveness of our approach, we compare with several baselines on negative focus detection, which are briefly introduced as follows. LSTM: The LSTM model which contains only a forward LSTM layer with a fully-connected layer followed by a softm"
D19-1230,D16-1078,1,0.847874,"n input layer within hidden layer Acc 70.22 69.38 70.51 69.80 Embeddings word word+chunking label word+dependency label word+PoS tag word+semantic role all features Table 6: Performance comparison for different composition manners of attention matrices. Word Embedding Senna Glove Word2vec BERT ELMo Dimension 50 100 300 768 1,024 Table 8: Performance comparison for different features with the BiLSTM-CRF model. Acc 70.08 69.66 69.10 70.22 70.51 Table 7: Performance comparison for different pretrained word embeddings on the T-Att BiLSTM-CRF model. lexical or syntactic features (Zou et al., 2013; Qian et al., 2016), the negative focus detection task tends to consider more semantic relation between the focus and other words and phrases. Impact of Attention Manner. In this paper, our contextual attention matrices are concatenated into the input embeddings. In addition, we also explore another manner of adding attention mechanisms, which directly concatenates the attention matrix into the hidden layer of BiLSTMCRF framework. As shown in Table 6, it indicates that the composition manner within the input layer is more effective than that within the hidden layer for both word-level and topic-level attention m"
D19-1230,S12-1039,0,0.836723,"be true. Among above these subtasks, negative focus detection is most extremely challenging in negation processing. In the literature, research on negative focus detection was pioneered by Blanco and Moldovan (2011), who proposed a supervised learning model with a set of highly hand-crafted features as a benchmark for this task. On the basis, Blanco and Moldvan (2013) incorporates negation into other existing semantic representations. The dataset annotated by Blanco and Moldovan is used as a standard evaluation corpus for the SEM’12 shared task (Morante and Blanco, 2012). In this shared task, Rosenberg and Bergler (2012) employed three 2252 Output O O O I I CRF layer c1 c2 c3 c4 c5 Backword LSTM r1 r2 r3 r4 r5 Forward LSTM l1 l2 l3 l4 l5 X* w Embedding layer Input X* p w2 w3 w4 X* n Xw softmax Xp w1 Mixed embeddings softmax w5 Sp Contextual embeddings Xn Xw S Word-level contextual attention Sn Figure 1: Architecture of BiLSTM-CRF network with word-level contextual attention mechanism for negative focus detection. kinds of heuristic rules for negative focus detection. Moreover, Zou et al. (2014; 2015) proposed graph-based models to enrich intra-sentence features with inter-sentence features from both lexical a"
D19-1230,D13-1170,0,0.00505769,"y, Section 5 concludes with possible directions for future work. 2 Related Work Earlier studies of negation processing in computational linguistics mainly lie in biomedical information extraction (Chapman et al., 2001; Goldin and Chapman, 2003). With the release of the BioScope corpus (Vincze et al., 2008), the negation processing techniques received a substantial boost (Morante et al., 2008; Apostolova et al., 2011; Zou et al., 2013). In addition, negation also was studied as a critical factor in polarity shifting in sentiment and opinion analysis with various lexical and syntactic features (Socher et al., 2013; Cruz et al., 2016). In general, existing studies of negation processing mainly concentrate on three aspects: 1) cue detection, which finds negative triggers or expressions in text; 2) scope resolution, which determines the grammatical scope in a sentence affected by a negative cue; and 3) focus detection, which identifies the most prominently/explicitly negated part in a negative statement. A negative focus could be considered as the semantic part in scope that is intended to be interpreted as false to make other parts to be true. Among above these subtasks, negative focus detection is most"
D19-1230,W08-0606,0,0.416054,"Missing"
D19-1230,D16-1058,0,0.0886288,"Missing"
D19-1230,W10-3111,0,0.526958,"Missing"
D19-1230,P16-2034,0,0.0914165,"Missing"
D19-1230,D13-1099,1,0.896942,"Missing"
D19-1230,P14-1049,1,0.790292,"a standard evaluation corpus for the SEM’12 shared task (Morante and Blanco, 2012). In this shared task, Rosenberg and Bergler (2012) employed three 2252 Output O O O I I CRF layer c1 c2 c3 c4 c5 Backword LSTM r1 r2 r3 r4 r5 Forward LSTM l1 l2 l3 l4 l5 X* w Embedding layer Input X* p w2 w3 w4 X* n Xw softmax Xp w1 Mixed embeddings softmax w5 Sp Contextual embeddings Xn Xw S Word-level contextual attention Sn Figure 1: Architecture of BiLSTM-CRF network with word-level contextual attention mechanism for negative focus detection. kinds of heuristic rules for negative focus detection. Moreover, Zou et al. (2014; 2015) proposed graph-based models to enrich intra-sentence features with inter-sentence features from both lexical and topic perspectives, respectively. Almost all of the above studies have demonstrated that the information contained in context plays a critical role in negative focus detection. In this paper, we explore the effectiveness of different representations and processing manners for modeling context in this task. In addition to the SEM’12 shared task corpus, there are several annotated datasets for negative focus detection. Anand and Martell (2012) made a point about how to describ"
D19-1230,D15-1187,1,0.743173,"Missing"
D19-1548,W13-2322,0,0.305832,"en two concepts. Experimental results on English AMR benchmark datasets show that our approach significantly outperforms the state of the art with 29.66 and 31.82 BLEU scores on LDC2015E86 and LDC2017T10, respectively. To the best of our knowledge, these are the best results achieved so far by supervised models on the benchmarks. 1 Introduction AMR-to-text generation is a task of automatically generating a natural language sentence from an Abstract Meaning Representation (AMR) graph. Due to the importance of AMR as a widely adopted semantic formalism in representing the meaning of a sentence (Banarescu et al., 2013), AMR has become popular in semantic representation and AMR-to-text generation has been drawing more and more attention in the last decade. As the example in Figure 1(a) shows, nodes, such as he and convict-01, represent semantic concepts ∗ Corresponding Author: Junhui Li. and edges, such as “:ARG1” and “:quant”, refer to semantic relations between the concepts. Since two concepts close in an AMR graph may map into two segments that are distant in the corresponding sentence, AMR-to-text generation is challenging. For example in Figure 1, the neighboring concepts he and convict-01 correspond to"
D19-1548,W05-0909,0,0.102631,"the filter size dw as 128. We use OpenNMT (Klein et al., 2017) as the implementation of the Transformer seq2seq model.2 In parameter setting, we set the number of layers in both the encoder and decoder to 6. For optimization we use Adam with β1 = 0.1 (Kingma and Ba, 2015). The number of heads is set to 8. In addition, we set the embedding and the hidden sizes to 512 and the batch token-size to 4096. Accordingly, the dx and dz in Section 2 are 64. In all experiments, we train the models for 300K steps on a single K40 GPU. For performance evaluation, we use BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), and CHRF++ (Popovi, 2017) as metrics. We report results of single models that are tuned on the development set. We make our code available at https://github.com/Amazing-J/ structural-transformer. 3.2 2 https://github.com/OpenNMT/OpenNMT-py Meteor 33.20 31.60 31.78 28.04 CHRF++ 60.30 58.09 58.43 51.88 Table 2: Ablation results of our baseline system on the LDC2015E86 development set. line system (an improvement from 18.77 to 24.93 in BLEU), revealing the fact that they are two effective ways to address the issue of data sparseness for AMR-to-text generation. Table"
D19-1548,P18-1026,0,0.261134,"hich locate at the different ends of the sentence. To address the above mentioned challenge, recent studies on AMR-to-text generation regard the task as a sequence-to-sequence (seq2seq) learning problem by properly linearizing an AMR graph into a sequence (Konstas et al., 2017). Such an input representation, however, is apt to lose useful structural information due to the removal of reentrant structures for linearization. To better model graph structures, previous studies propose various graph-based seq2seq models to incorporate graphs as an additional input representation (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019). Although such graph-to-sequence models can achieve the state-of-the-art results, they focus on modeling one-hop relations only. That is, they only model concept pairs connected directly by an edge (Song et al., 2018; Beck et al., 2018), and as a result, ignore explicit structural information of indirectly connected concepts in AMR graphs, e.g. the relation between concepts he and possible in Figure 1. To make better use of structural information in an AMR graph, we attempt to model arbitrary concept pairs no matter whether directly connected or not. To this end, we"
D19-1548,N19-1223,0,0.679159,"roposed approach achieves higher performance than the baseline. As representative, we use CNN-based method to obtain structural representation. 5463 LDC2015E86 LDC2017T10 BLEU Meteor CHRF++ #P (M) BLEU Meteor CHRF++ Baseline 25.50 33.16 59.88 49.1 27.43 34.62 61.85 feature-based 27.23 34.53 61.55 49.4 30.18 35.83 63.20 avg-based 28.37 35.10 62.29 49.1 29.56 35.24 62.86 Our Approach sum-based 28.69 34.97 62.05 49.1 29.92 35.68 63.04 SA-based 29.66 35.45 63.00 49.3 31.54 36.02 63.84 CNN-based 29.10 35.00 62.10 49.2 31.82 36.38 64.05 Previous works with single models Konstas et al. (2017)∗ 22.00 Cao and Clark (2019)∗ 23.5 26.8 † Song et al. (2018) 23.30 23.3 50.4 Beck et al. (2018)† Damonte and Cohen (2019)† 24.40 23.60 24.54 24.07 Guo et al. (2019)† 25.7 27.6 57.3 Song et al. (2016)‡ 22.44 Previous works with either ensemble models or unlabelled data, or both Konstas et al. (2017)∗ 33.8 † Song et al. (2018) 33.0 Beck et al. (2018)† 27.5 53.5 Guo et al. (2019)† 35.3 System Table 3: Comparison results of our approaches and related studies on the test sets of LDC2015E86 and LDC2017T10. #P indicates the size of parameters in millions. ∗ indicates seq2seq-based systems while † for graph-based models, and ‡ f"
D19-1548,J07-2003,0,0.0231668,"on (SMT) and neural machine translation (NMT). Flanigan et al. (2016) first transform an AMR graph into a tree, then specify a number of tree-to-string transduction rules based on alignments that are used to drive a tree-based SMT model (Graehl and Knight, 2004). Pourdamghani et al. (2016) develop a method that learns to linearize AMR graphs into AMR strings, and then feed them into a phrase-based SMT model (Koehn et al., 2003). Song et al. (2017) use synchronous node replacement grammar (SNRG) to generate text. Different from synchronous context-free grammar in hierarchical phrase-based SMT (Chiang, 2007), SNRG is a grammar over graphs. Moving to neural seq2seq approaches, Konstas et al. (2017) successfully apply seq2seq model together with large-scale unlabeled data for both text-to-AMR parsing and AMR-to-text generation. With special interest in the target side syn5466 tax, Cao and Clark (2019) use seq2seq models to generate target syntactic structure, and then the surface form. To prevent the information loss in linearizing AMR graphs into sequences, (Song et al., 2018; Beck et al., 2018) propose graphto-sequence models to encode graph structure directly. Focusing on reentrancies, Damonte a"
D19-1548,N19-1366,0,0.508017,"different ends of the sentence. To address the above mentioned challenge, recent studies on AMR-to-text generation regard the task as a sequence-to-sequence (seq2seq) learning problem by properly linearizing an AMR graph into a sequence (Konstas et al., 2017). Such an input representation, however, is apt to lose useful structural information due to the removal of reentrant structures for linearization. To better model graph structures, previous studies propose various graph-based seq2seq models to incorporate graphs as an additional input representation (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019). Although such graph-to-sequence models can achieve the state-of-the-art results, they focus on modeling one-hop relations only. That is, they only model concept pairs connected directly by an edge (Song et al., 2018; Beck et al., 2018), and as a result, ignore explicit structural information of indirectly connected concepts in AMR graphs, e.g. the relation between concepts he and possible in Figure 1. To make better use of structural information in an AMR graph, we attempt to model arbitrary concept pairs no matter whether directly connected or not. To this end, we extend the encoder in the"
D19-1548,W14-3348,0,0.0301044,"We use OpenNMT (Klein et al., 2017) as the implementation of the Transformer seq2seq model.2 In parameter setting, we set the number of layers in both the encoder and decoder to 6. For optimization we use Adam with β1 = 0.1 (Kingma and Ba, 2015). The number of heads is set to 8. In addition, we set the embedding and the hidden sizes to 512 and the batch token-size to 4096. Accordingly, the dx and dz in Section 2 are 64. In all experiments, we train the models for 300K steps on a single K40 GPU. For performance evaluation, we use BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), and CHRF++ (Popovi, 2017) as metrics. We report results of single models that are tuned on the development set. We make our code available at https://github.com/Amazing-J/ structural-transformer. 3.2 2 https://github.com/OpenNMT/OpenNMT-py Meteor 33.20 31.60 31.78 28.04 CHRF++ 60.30 58.09 58.43 51.88 Table 2: Ablation results of our baseline system on the LDC2015E86 development set. line system (an improvement from 18.77 to 24.93 in BLEU), revealing the fact that they are two effective ways to address the issue of data sparseness for AMR-to-text generation. Table 3 presents the comparison of"
D19-1548,N16-1087,0,0.303256,"pectively. SEQ2 and GRAPH are the outputs of the seq2seq and the graph models in Damonte and Cohen (2019), respectively. tion in AMR graphs, our model tends to use past tense, as provided and did in Example (1) and (2). Similarly, without information concerning singular form and plural form, our model is more likely to use plural nouns, as centers and lawyers in Example (1) and (5). 5 Related Work Most studies in AMR-to-text generation regard it as a translation problem and are motivated by the recent advances in both statistical machine translation (SMT) and neural machine translation (NMT). Flanigan et al. (2016) first transform an AMR graph into a tree, then specify a number of tree-to-string transduction rules based on alignments that are used to drive a tree-based SMT model (Graehl and Knight, 2004). Pourdamghani et al. (2016) develop a method that learns to linearize AMR graphs into AMR strings, and then feed them into a phrase-based SMT model (Koehn et al., 2003). Song et al. (2017) use synchronous node replacement grammar (SNRG) to generate text. Different from synchronous context-free grammar in hierarchical phrase-based SMT (Chiang, 2007), SNRG is a grammar over graphs. Moving to neural seq2se"
D19-1548,P16-1014,0,0.0281535,"l. (2017) to linearize AMR graphs and to obtain simplified AMRs. We remove variables, wiki links and sense tags before linearization. Figure 1(b) shows an example linearization result for the AMR graph in Figure 1(a). Note that the reentrant concept he in Figure 1 (a) maps to two different tokens in the linearized sequence. Vocabulary: Training AMR-to-text generation systems solely on labeled data may suffer from data sparseness. To attack this problem, previous works adopt techniques like anonymization to remove named entities and rare words (Konstas et al., 2017), or apply a copy mechanism (Gulcehre et al., 2016) such that the models can learn to copy rare words from the input sequence. In this paper we instead use two simple yet effective techniques. One is to apply Byte Pair Encoding (BPE) (Sennrich et al., 2016) to split words into smaller, more frequent sub-word units. The other is to use a shared vocabulary for both source and target sides. Experiments in Section 3.2 demonstrate the necessity of the techniques in building a strong baseline. xi W Q 2.2  Modeling Graph Structures in Transformer Input Representation: We also use the depthfirst traversal strategy to linearize AMR graphs and to obtai"
D19-1548,Q19-1019,0,0.653449,"/OpenNMT-py Meteor 33.20 31.60 31.78 28.04 CHRF++ 60.30 58.09 58.43 51.88 Table 2: Ablation results of our baseline system on the LDC2015E86 development set. line system (an improvement from 18.77 to 24.93 in BLEU), revealing the fact that they are two effective ways to address the issue of data sparseness for AMR-to-text generation. Table 3 presents the comparison of our approach and related works on the test sets of LDC2015E86 and LDC2017T10. From the results we can see that the Transformer-based baseline outperforms most of graph-to-sequence models and is comparable with the latest work by Guo et al. (2019). The strong performance of the baseline is attributed to the capability of the Transformer to encode global and implicit structural information in AMR graphs. By comparing the five methods of learning graph structure representations, we have the following observations. • All of them achieve significant improvements over the baseline: the biggest improvements are 4.16 and 4.39 BLEU scores on LDC2015E86 and LDC2017T10, respectively. • Methods using continuous representations (such as SA-based and CNN-based) outperform the methods using discrete representations (such as feature-based). • Compare"
D19-1548,P14-1062,0,0.0146263,"sum of its hidden states: r= k X αi hi (10) i=1 li (8) i=1 Self-Attention-based (SA-based for short) As shown in Figure 2, given the label sequence s = s1 , · · · , sk , we first obtain the sequence e, whose element is the addition of a word embedding and the corresponding position embedding. Then we use the self-attention, as presented in Eq. 1 to obtain its hidden states h, i.e, h = Attention(e), where hi ∈ Rdz . Our aim is to encode a variable length sentence into a dz sized vector. Motivated by (Lin et al., 2017), we achieve this by choosing a linear combination of CNN-based Motivated by (Kalchbrenner et al., 2014), we use convolutional neural network (CNN) to convolute the label sequence l into a vector r, as follow: 5462 conv = Conv1D(kernel size = (m), strides = 1, f ilters = dz , (11) input shape = dz activation =0 relu0 ) r = conv (l) (12) Model Baseline -BPE -Share Vocab. -Both where kernel size m is set to 4 in our experiments. 3 Experimentation 3.1 Experimental Settings For evaluation of our approach, we use the sentences annotated with AMRs from the LDC release LDC2015E86 and LDC2017T10. The two datasets contain 16,833 and 36,521 training AMRs, respectively, and share 1,368 development AMRs and"
D19-1548,W16-6603,0,0.31594,"). Similarly, without information concerning singular form and plural form, our model is more likely to use plural nouns, as centers and lawyers in Example (1) and (5). 5 Related Work Most studies in AMR-to-text generation regard it as a translation problem and are motivated by the recent advances in both statistical machine translation (SMT) and neural machine translation (NMT). Flanigan et al. (2016) first transform an AMR graph into a tree, then specify a number of tree-to-string transduction rules based on alignments that are used to drive a tree-based SMT model (Graehl and Knight, 2004). Pourdamghani et al. (2016) develop a method that learns to linearize AMR graphs into AMR strings, and then feed them into a phrase-based SMT model (Koehn et al., 2003). Song et al. (2017) use synchronous node replacement grammar (SNRG) to generate text. Different from synchronous context-free grammar in hierarchical phrase-based SMT (Chiang, 2007), SNRG is a grammar over graphs. Moving to neural seq2seq approaches, Konstas et al. (2017) successfully apply seq2seq model together with large-scale unlabeled data for both text-to-AMR parsing and AMR-to-text generation. With special interest in the target side syn5466 tax,"
D19-1548,P16-1162,0,0.563838,"igure 1(a). Note that the reentrant concept he in Figure 1 (a) maps to two different tokens in the linearized sequence. Vocabulary: Training AMR-to-text generation systems solely on labeled data may suffer from data sparseness. To attack this problem, previous works adopt techniques like anonymization to remove named entities and rare words (Konstas et al., 2017), or apply a copy mechanism (Gulcehre et al., 2016) such that the models can learn to copy rare words from the input sequence. In this paper we instead use two simple yet effective techniques. One is to apply Byte Pair Encoding (BPE) (Sennrich et al., 2016) to split words into smaller, more frequent sub-word units. The other is to use a shared vocabulary for both source and target sides. Experiments in Section 3.2 demonstrate the necessity of the techniques in building a strong baseline. xi W Q 2.2  Modeling Graph Structures in Transformer Input Representation: We also use the depthfirst traversal strategy to linearize AMR graphs and to obtain simplified AMRs which only consist of concepts. As shown in Figure 1 (c), the input sequence is much shorter than the input sequence in the baseline. Besides, we also obtain a matrix which records the gra"
D19-1548,N18-2074,0,0.160242,"ut sequence in the baseline. Besides, we also obtain a matrix which records the graph structure between every concept pair, which implies their semantic relationship (Section2.3). Vocabulary: To be compatible with sub-words, we extend the original AMR graph, if necessary, to include the structures of sub-words. As sentence01 in Figure 1(a) is segmented into sent@@ ence01, we split the original node into two connected ones with an edge labeled as the incoming edge of the first unit. Figure 1(d) shows the graph structure for sub-words sent@@ ence-01. Structure-Aware Self-Attention: Motivated by Shaw et al. (2018), we extend the conventional self-attention architecture to explicitly encode the relation between an element pair (xi , xj ) in the alignment model by replacing Equation 4 with Equation 5. Note that the relation rij ∈ Rdz is the vector representation for element pair (xi , xj ), and will be learned in Section 2.3.  T xi W Q xj W K + rij W R √ eij = (5) dz where W R ∈ Rdz ×dz is a parameter matrix. Then, we update Equation 2 accordingly to propagate structure information to the sublayer output by: zi = n X αij xj W V + rij W F  (6) j=1 where W F ∈ Rdz ×dz is a parameter matrix. 2.3 Learning"
D19-1548,P17-2002,0,0.218561,"5 Related Work Most studies in AMR-to-text generation regard it as a translation problem and are motivated by the recent advances in both statistical machine translation (SMT) and neural machine translation (NMT). Flanigan et al. (2016) first transform an AMR graph into a tree, then specify a number of tree-to-string transduction rules based on alignments that are used to drive a tree-based SMT model (Graehl and Knight, 2004). Pourdamghani et al. (2016) develop a method that learns to linearize AMR graphs into AMR strings, and then feed them into a phrase-based SMT model (Koehn et al., 2003). Song et al. (2017) use synchronous node replacement grammar (SNRG) to generate text. Different from synchronous context-free grammar in hierarchical phrase-based SMT (Chiang, 2007), SNRG is a grammar over graphs. Moving to neural seq2seq approaches, Konstas et al. (2017) successfully apply seq2seq model together with large-scale unlabeled data for both text-to-AMR parsing and AMR-to-text generation. With special interest in the target side syn5466 tax, Cao and Clark (2019) use seq2seq models to generate target syntactic structure, and then the surface form. To prevent the information loss in linearizing AMR gra"
D19-1548,D16-1224,0,0.0392513,"Missing"
D19-1548,P18-1150,0,0.704359,"he and convicted which locate at the different ends of the sentence. To address the above mentioned challenge, recent studies on AMR-to-text generation regard the task as a sequence-to-sequence (seq2seq) learning problem by properly linearizing an AMR graph into a sequence (Konstas et al., 2017). Such an input representation, however, is apt to lose useful structural information due to the removal of reentrant structures for linearization. To better model graph structures, previous studies propose various graph-based seq2seq models to incorporate graphs as an additional input representation (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019). Although such graph-to-sequence models can achieve the state-of-the-art results, they focus on modeling one-hop relations only. That is, they only model concept pairs connected directly by an edge (Song et al., 2018; Beck et al., 2018), and as a result, ignore explicit structural information of indirectly connected concepts in AMR graphs, e.g. the relation between concepts he and possible in Figure 1. To make better use of structural information in an AMR graph, we attempt to model arbitrary concept pairs no matter whether directly connected or no"
D19-1548,P17-4012,0,0.047258,"tated with AMRs from the LDC release LDC2015E86 and LDC2017T10. The two datasets contain 16,833 and 36,521 training AMRs, respectively, and share 1,368 development AMRs and 1,371 testing AMRs. We segment words into sub-word units by BPE (Sennrich et al., 2016) with 10K operations on LDC2015E86 and 20K operations on LDC2017T10. For efficiently learning graph structure representation for concept pairs (except the featurebased method), we limit the maximum label sequence length to 4 and ignore the labels exceeding the maximum. In SA-based method, we set the filter size dw as 128. We use OpenNMT (Klein et al., 2017) as the implementation of the Transformer seq2seq model.2 In parameter setting, we set the number of layers in both the encoder and decoder to 6. For optimization we use Adam with β1 = 0.1 (Kingma and Ba, 2015). The number of heads is set to 8. In addition, we set the embedding and the hidden sizes to 512 and the batch token-size to 4096. Accordingly, the dx and dz in Section 2 are 64. In all experiments, we train the models for 300K steps on a single K40 GPU. For performance evaluation, we use BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), and CHRF"
D19-1548,P15-1150,0,0.168289,"Missing"
D19-1548,W04-3250,0,0.0272772,"7 Guo et al. (2019)† 25.7 27.6 57.3 Song et al. (2016)‡ 22.44 Previous works with either ensemble models or unlabelled data, or both Konstas et al. (2017)∗ 33.8 † Song et al. (2018) 33.0 Beck et al. (2018)† 27.5 53.5 Guo et al. (2019)† 35.3 System Table 3: Comparison results of our approaches and related studies on the test sets of LDC2015E86 and LDC2017T10. #P indicates the size of parameters in millions. ∗ indicates seq2seq-based systems while † for graph-based models, and ‡ for other models. All our proposed systems are significant over the baseline at 0.01, tested by bootstrap resampling (Koehn, 2004). System Baseline Our approach No indirectly connected concept pairs BLEU 27.43 31.82 29.92 Table 4: Performance on the test set of our approach with or without modeling structural information of indirectly connected concept pairs. shows that by modeling structural information of indirectly connected concept pairs, our approach improves the performance on the test set from 29.92 to 31.82 in BLEU scores. It also shows that even without modeling structural information of indirectly connected concept pairs, our approach achieves better performance than the baseline. 4.2 4.1 Effect of Modeling Str"
D19-1548,N03-1017,0,0.015868,"Example (1) and (5). 5 Related Work Most studies in AMR-to-text generation regard it as a translation problem and are motivated by the recent advances in both statistical machine translation (SMT) and neural machine translation (NMT). Flanigan et al. (2016) first transform an AMR graph into a tree, then specify a number of tree-to-string transduction rules based on alignments that are used to drive a tree-based SMT model (Graehl and Knight, 2004). Pourdamghani et al. (2016) develop a method that learns to linearize AMR graphs into AMR strings, and then feed them into a phrase-based SMT model (Koehn et al., 2003). Song et al. (2017) use synchronous node replacement grammar (SNRG) to generate text. Different from synchronous context-free grammar in hierarchical phrase-based SMT (Chiang, 2007), SNRG is a grammar over graphs. Moving to neural seq2seq approaches, Konstas et al. (2017) successfully apply seq2seq model together with large-scale unlabeled data for both text-to-AMR parsing and AMR-to-text generation. With special interest in the target side syn5466 tax, Cao and Clark (2019) use seq2seq models to generate target syntactic structure, and then the surface form. To prevent the information loss in"
D19-1548,N19-1238,0,0.105079,"Missing"
D19-1548,P17-1014,0,0.400799,"uch as “:ARG1” and “:quant”, refer to semantic relations between the concepts. Since two concepts close in an AMR graph may map into two segments that are distant in the corresponding sentence, AMR-to-text generation is challenging. For example in Figure 1, the neighboring concepts he and convict-01 correspond to the words he and convicted which locate at the different ends of the sentence. To address the above mentioned challenge, recent studies on AMR-to-text generation regard the task as a sequence-to-sequence (seq2seq) learning problem by properly linearizing an AMR graph into a sequence (Konstas et al., 2017). Such an input representation, however, is apt to lose useful structural information due to the removal of reentrant structures for linearization. To better model graph structures, previous studies propose various graph-based seq2seq models to incorporate graphs as an additional input representation (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019). Although such graph-to-sequence models can achieve the state-of-the-art results, they focus on modeling one-hop relations only. That is, they only model concept pairs connected directly by an edge (Song et al., 2018; Beck et al., 201"
D19-1548,P02-1040,0,0.107013,"mum. In SA-based method, we set the filter size dw as 128. We use OpenNMT (Klein et al., 2017) as the implementation of the Transformer seq2seq model.2 In parameter setting, we set the number of layers in both the encoder and decoder to 6. For optimization we use Adam with β1 = 0.1 (Kingma and Ba, 2015). The number of heads is set to 8. In addition, we set the embedding and the hidden sizes to 512 and the batch token-size to 4096. Accordingly, the dx and dz in Section 2 are 64. In all experiments, we train the models for 300K steps on a single K40 GPU. For performance evaluation, we use BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), and CHRF++ (Popovi, 2017) as metrics. We report results of single models that are tuned on the development set. We make our code available at https://github.com/Amazing-J/ structural-transformer. 3.2 2 https://github.com/OpenNMT/OpenNMT-py Meteor 33.20 31.60 31.78 28.04 CHRF++ 60.30 58.09 58.43 51.88 Table 2: Ablation results of our baseline system on the LDC2015E86 development set. line system (an improvement from 18.77 to 24.93 in BLEU), revealing the fact that they are two effective ways to address the issue of data sparseness"
D19-1548,W17-4770,0,0.120852,"Missing"
D19-1552,P17-1067,0,0.497484,"tal results show the usefulness of personal attributes, and the effectiveness of our proposed NPD approach in capturing such personal attributes with significant gains over the state-of-the-art models. 1 Introduction The advent of social media and its prosperity enable the creation of massive online user-generated content including opinions and product reviews. Analyzing such user-generated contents allows to detect the users’ emotional states, which are useful for various downstream applications. In the literature, there are a large number of works on emotion detection (Roberts et al., 2012; Abdul-Mageed and Ungar, 2017; Gupta et al., 2017), both discrete and neural models have been ∗ corresponding auther used to predict the emotions of posts in social media. For example, Roberts et al. (2012) used a series of binary SVM classifiers to detect the emotion of a post, while Gupta et al. (2017) used sentiment based semantic embedding and a LSTM model to learn the representation of a post for emotion detection. Different from previous researches, which consider each post individually, we think that posts in social media are much correlated by the authors’ backgrounds. Motivated by the principle of homophily (Laza"
D19-1552,D16-1053,0,0.0134642,"ere,1) SVM is a widely used baseline to predict the emotion of a post in social media (Yang et al., 2007).2) Abdul17 is a standard LSTM model which consist of a LSTM layer and a fully connected layer, and it is modified from the model in Abdul-Mageed and Ungar (2017). The LSTM model yields the stateof-the-art performance on emotion detection in recent researches.3)Vaswani17 is an improved LSTM model with a self-attention mechanism. The self-attention mechanism is used to capture the structural information and has been successfully applied in various natural language processing tasks recently (Cheng et al., 2016; Vaswani et al., 2017) From Table 2, we find that all of the neural models outperform SVM significantly. This indicates that neural models are much more effective than discrete models in emotion detection. In addition, our proposed NPD model outperforms both the standard LSTM model (Abdul17) and the improved LSTM model with self-attention (Vaswani17) significantly. This shows the effectiveness of our proposed NPD model with both adversarial discriminators and attention mechanisms. This also shows the usefulness of personal attributes for emotion detection. Moreover, we find that the performan"
D19-1552,P15-1073,0,0.0289832,"ollobert et al., 2011; Goldberg, 2016). However, few works use neural network models for emotion detection. Abdul-Mageed and Ungar (2017) used a gated recurrent neural network model for emotion detection with a largescale dataset. Zhang et al. (2018) used an auxiliary and attention based LSTM to detect emotion on a cross-lingual dataset. Lexicon and social information are very important for emotion detection, and there are many researches focus on this topic. For example, Strapparava and Mihalcea (2008) used WordNetAffect to compute the sentimental score of a post. More recently, In addition, Hovy (2015) used both the age and gender information of the authors to improve the performance of sentiment analysis. Vosoughi et al. (2016) explored the relationship among locations, date time, authors and sentiments. Different from previous works which consider each post individually, we think that the posts in social media can be connected through the authors’ backgrounds and should be better addressed. On the basis, we propose a neural personal discrimination model to determine the personal background attributes from each post through adversarial discriminators, and aggregate the representation of in"
D19-1552,roberts-etal-2012-empatweet,0,0.18827,"r addressed. Experimental results show the usefulness of personal attributes, and the effectiveness of our proposed NPD approach in capturing such personal attributes with significant gains over the state-of-the-art models. 1 Introduction The advent of social media and its prosperity enable the creation of massive online user-generated content including opinions and product reviews. Analyzing such user-generated contents allows to detect the users’ emotional states, which are useful for various downstream applications. In the literature, there are a large number of works on emotion detection (Roberts et al., 2012; Abdul-Mageed and Ungar, 2017; Gupta et al., 2017), both discrete and neural models have been ∗ corresponding auther used to predict the emotions of posts in social media. For example, Roberts et al. (2012) used a series of binary SVM classifiers to detect the emotion of a post, while Gupta et al. (2017) used sentiment based semantic embedding and a LSTM model to learn the representation of a post for emotion detection. Different from previous researches, which consider each post individually, we think that posts in social media are much correlated by the authors’ backgrounds. Motivated by th"
D19-1552,W15-2904,0,0.0729155,"Missing"
D19-1552,C14-1050,1,0.814011,"eat this in such weather!) However, the personal attributes are not easy to obtain in most social media websites. On one hand, most websites may not contain useful personal information. On the other hand, people are normally not willing to attach their personal information in social media. Besides, integrating personal attributes into emotion detection is challenging, since it is hard to capture attributes-aware words, such as “little brother” and “bashi”(comfortable), to connect the posts with similar backgrounds. Although there are some related works on either personal attribute extraction (Wang et al., 2014) or emotion detection with personal attributes (Li et al., 2016), none of them address both challenges at the same time. In this paper, we propose a Neural Personal Discrimination (NPD) model with both adversarial discriminators and attention mechanisms to tackle above challenges. Here, the Adversarial discriminators (Goodfellow et al., 2014) are used to determine the personal attributes, e.g., gender or location, of a post, providing the inherent correlationship between emotions and personal backgrounds, while the Attention mechanisms (Wang et al., 2016) are utilized to aggregate the represen"
D19-1552,C16-1153,1,0.929679,"either personal attribute extraction (Wang et al., 2014) or emotion detection with personal attributes (Li et al., 2016), none of them address both challenges at the same time. In this paper, we propose a Neural Personal Discrimination (NPD) model with both adversarial discriminators and attention mechanisms to tackle above challenges. Here, the Adversarial discriminators (Goodfellow et al., 2014) are used to determine the personal attributes, e.g., gender or location, of a post, providing the inherent correlationship between emotions and personal backgrounds, while the Attention mechanisms (Wang et al., 2016) are utilized to aggregate the representation of informative attributes-aware words into a vector for emotion prediction, providing insights into which words contribute to a personal background. Experimental results show the usefulness of personal attributes in emotion detection, and the effectiveness of our proposed NPD model with both adversarial discriminators and attention mechanisms over the state-of-the-art discrete and neural models. 2 Related Work Earlier works on emotion detection are based on discrete models. For example, Yang et al. (2007) built a support vector machine (SVM) model"
D19-1560,D16-1011,0,0.0517837,"ground-truth sentiment rating for aspect xaspect . δ The model will assign a reward score to each seis a L2 regularization. quence according to the designed scores function, and then estimates b(τ h ) as the average of those 3 Experimentation rewards. Similarly, the policy gradient w.r.t. θl of 3.1 Experimental Settings low-level policy is given by, ∇θl J(θ ) = Eτ l ∼πl [ l ki  Rl ∇θl log π l (ai,j |sli,j ; θl )] j=1 (8) Data. We conduct our experiments on three public datasets on DASC, i.e., TripUser (Li et al., 2018), TripAdvisor (Wang et al., 2010) and BeerAdvocate (McAuley et al., 2012; Lei et al., 2016). In the experiment, we adopt Discourse Segmentation 5585 TripUser Development Test Acc.↑ MSE↓ Acc.↑ MSE↓ SVM 46.35† 1.025† LSTM 53.23 0.787 52.74 0.794 MAMC 55.49† 0.583† HARN 58.15† 0.528† HUARN 60.70† 0.514† C-HAN 58.49 0.602 57.38 0.543 HS-LSTM 59.75 0.566 59.01 0.524 RL-Word-Selection 60.15 0.475 59.55 0.519 RL-Clause-Selection 61.32 0.433 60.54 0.461 HRL 62.97 0.336 62.84 0.351 Approaches TripAdvisor Development Test Acc.↑ MSE↓ Acc.↑ MSE↓ 34.30‡ 1.982‡ 35.26‡ 1.963‡ 43.85‡ 1.525‡ 44.02‡ 1.470‡ 46.21‡ 1.091‡ 46.56‡ 1.083‡ 48.21‡ 0.923‡ 47.61 0.914 47.08 0.955 48.45 0.947 46.84 1.013 48.55"
D19-1560,D15-1167,0,0.042799,".8; λ1 , λ2 and λ3 are 0.25, 0.25 and 0.5 respectively. λ1 , λ2 are 0.6 and 0.4. Additionally, the batch size is set to be 64, regularization weight is set to be 10−5 and the dropout rate is 0.2. Evaluation Metrics. The performance is evaluated using Accuracy (Acc.) and MSE as Yin et al. (2017). Moreover, t-test is used to evaluate the signiﬁcance of the performance difference between two approaches (Yang and Liu, 1999). Baselines. We compare HRL with the following baselines: 1) SVM (Yin et al., 2017). This approach only adopts unigram, bigram as features to train an SVM classiﬁer. 2) LSTM (Tang et al., 2015). This is a neural network approach to document-level sentiment classiﬁcation which employs gated LSTM to learn text representation. 3) MAMC (Yin et al., 2017). This approach employs hierarchical iterative attention to learn aspect-speciﬁc representation. This is a state-of3 http://alt.qcri.org/tools/discourse-parser/ the-art approach to DASC. 4) HARN (Li et al., 2018). This approach adopts hierarchical attention to incorporate overall rating and aspect information so as to learn aspect-speciﬁc representation. This is another state-of-the-art approach to DASC. 5) HUARN (Li et al., 2018). This"
D19-1560,C18-1079,0,0.214585,"t classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - service: # # # # $ (4) ral models have shown to be effective for performance improvement on DASC. Despite the advantages, these complex neural network approaches often offer little transparency w.r.t. their inner working mechanisms and suffer from the lack of interpretability. However, clearly understanding where and how such a model makes such a decision is rather important for developing real-world applications (Liu et al., 2018; Marcus, 2018). As human beings, if asked to evaluate the sentiment rating for a speciﬁc asp"
D19-1560,D16-1021,0,0.142468,"Missing"
D19-1560,D16-1127,0,0.103263,"Missing"
D19-1560,P10-1043,1,0.876096,"oach to DASC over the state-of-the-art baselines. 1 is a little uncomfortable .]Clause3 [I’m often nitpicking for room decoration.]]Clause4 [ Besides, the price is very expensive ] Clause5 [ although the staff service is professional .]]Clause6 Rating of Each Aspect - location: # # # # # (5) - value: # $ $ $ $ (1) Figure 1: An example of a review document, where clauses and words with different colors refer to different aspects. Introduction Document-level Aspect Sentiment Classiﬁcation (DASC) is a ﬁne-grained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - ser"
D19-1560,P08-1036,0,0.0746941,"assiﬁcation (DASC) is a ﬁne-grained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - service: # # # # $ (4) ral models have shown to be effective for performance improvement on DASC. Despite the advantages, these complex neural network approaches often offer little transparency w.r.t. their inner working mechanisms and suffer from the lack of interpretability. However, clearly understanding where and how such a model makes such a decision is rather important for developing real-world applications (Liu et al., 2018; Marcus, 2018). As human beings, if asked to evaluate th"
D19-1560,D16-1058,0,0.252027,"Missing"
D19-1560,D17-1217,0,0.61734,"e-grained sentiment classiﬁcation task in the ﬁeld of sentiment analysis (Pang and Lee, 2007; Li et al., 2010). This task aims to predict the sentiment rating for each given aspect mentioned in a document-level review. For instance, Figure 1 shows a review document with four given aspects of a hotel (i.e., location, room, value, service). The goal of DASC is to predict the rating score towards each aspect by analyzing the whole document. In the last decade, this task has been drawing more and more interests of researchers in the Natural Language Processing community (Titov and McDonald, 2008; Yin et al., 2017; Li et al., 2018). In previous studies, neu∗ Corresponding author - room: # # # $ $ (3) - service: # # # # $ (4) ral models have shown to be effective for performance improvement on DASC. Despite the advantages, these complex neural network approaches often offer little transparency w.r.t. their inner working mechanisms and suffer from the lack of interpretability. However, clearly understanding where and how such a model makes such a decision is rather important for developing real-world applications (Liu et al., 2018; Marcus, 2018). As human beings, if asked to evaluate the sentiment rating"
D19-1560,C18-1074,0,0.0316687,"ect sentiment-relevant words and discard those irrelevant and noisy words. For instance, for aspect location, words “this”, “is” in Clause1 are noisy words and should be discarded since they make no contribution to implying the sentiment rating. One possible way to alleviate this problem is to also leverage the soft-attention mechanism as proposed in Li et al. (2018). However, this soft-attention mechanism may induce additional noise and lack interpretability because it tends to assign higher weights to some domain-speciﬁc words rather than real sentiment-relevant words (Mudinas et al., 2012; Zou et al., 2018). For instance, this soft-attention mechanism tends to regard the name of a hotel “Hilton” with a good reputation in Clause3 as a positive word which could mislead the model into assigning a higher rating to aspect room. Therefore, a well-behaved approach should highlight sentiment-relevant words and discard noisy words for a speciﬁc aspect during model training. In this paper, we propose a Hierarchical Reinforcement Learning (HRL) approach with a highlevel policy and a low-level policy to address the above two challenges in DASC. First, a highlevel policy is leveraged to select aspect-relevan"
I05-1034,A00-2030,0,0.0584531,"stness in handling large-scale or new domain data due to two reasons. First, rules have to be rewritten for different tasks or when porting to different domains. Second, generating rules manually is quite labor- and time-consuming. 1 GPE is an acronym introduced by the ACE (2004) program to represent a Geo-Political Entity --- an entity with land and a government. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 378 – 389, 2005. © Springer-Verlag Berlin Heidelberg 2005 Discovering Relations Between Named Entities from a Large Raw Corpus 379 Since then, various supervised learning approaches [2,3,4,5] have been explored extensively in relation extraction. These approaches automatically learn relation patterns or models from a large annotated corpus. To decrease the corpus annotation requirement, some researchers turned to weakly supervised learning approaches [6,7], which rely on a small set of initial seeds instead of a large annotated corpus. However, there is no systematic way in selecting initial seeds and deciding an “optimal” number of them. Alternatively, Hasegawa et al. [8] proposed a cosine similarity-based unsupervised learning approach for extracting relations from a large raw c"
I05-1034,P03-1005,0,0.0243155,"Missing"
I05-1034,P04-1043,0,0.255007,"Missing"
I05-1034,sekine-etal-2002-extended,0,0.0148849,"ht, NP], [sold, NP, yesterday]) = 1 + K ( bought, sold ) + K (NP, NP) = 1+0.25+0.25+K c ([a, red, car], [the, flat]) = 1.5 + K (a, the) + K (car, flat ) =2 The above similarity score is more than one. This is because we did not normalize the score using Formula (6). 2.3 Tree Similarity Based Unsupervised Learning Our method consists of five steps: 1) Named Entity (NE) tagging and sentence parsing: Detailed and accurate NE types provide more effective information for relation discovery. Here we use Sekine’s NE tagger [20], where 150 hierarchical types and subtypes of Named Entities are defined [21]. This NE tagger has also been adopted by Hasegawa et al. [8]. Besides, Collin’s parser [18] is adopted to generate shallow parse trees. 2) Similarity calculation: The similarity between two relation instances is defined between two parse trees. However, the state-of-the-art of parser is always error-prone. Therefore, we only use the minimum span parse tree including the NE pairs when calculating the similarity function [4]. Please note that the two entities may not be the leftmost or rightmost node in the sub-tree. 3) NE pairs clustering: Clustering of NE pairs is based on the similarity scor"
I05-1034,J03-4003,0,\N,Missing
I05-1034,P04-1054,0,\N,Missing
I05-1034,P04-1053,0,\N,Missing
I05-1034,P02-1034,0,\N,Missing
I05-1034,P04-1016,0,\N,Missing
I05-1047,P02-1060,1,0.785297,"ord segmentation using a discriminative Markov model, called Mutual Information Independence Model (MIIM). Then a maximum entropy model is applied to integrate various types of contexts and resolve the data sparseness problem in MIIM. Finally, an error-driven learning approach is proposed to select useful contexts and reduce the context feature vector dimension. 2.1 Mutual Information Independence Model and Unknown Word Detection Mutual Information Independence Model In this paper, we use a discriminative Markov model, called Mutual Information Independence Model (MIIM) proposed by Zhou et al [17] 4 , in unknown word detection by chunking. MIIM is derived from a conditional probability model. Given an observation sequence O1n = o1o2 L o n , the goal of a conditional probability model is to find a stochastic optimal state(tag) sequence S1n = s1 s 2 L s n that maximizes: log P( S1n |O1n ) = log P ( S1n ) + log P ( S1n , O1n ) P( S1n ) ⋅ P(O1n ) (1) The second term in Equation (1) is the pair-wise mutual information (PMI) between S1n and O1n . In order to simplify the computation of this term, we assume a pair-wise mutual information independence (2): n PMI ( S1n , O1n ) = ∑ PMI ( s i , O"
I05-1047,P96-1041,0,0.0240028,"sing MIIM From Equation (3), we can see that the state transition model of MIIM can be computed by using ngram modeling [20, 21, 22], where each tag is assumed to be dependent on the N-1 previous tags (e.g. 2). The problem with the above MIIM lies in n the data sparseness problem raised by its output model: ∑ log P(s i =1 i |O1n ) . Ideally, we would have sufficient training data for every event whose conditional probability we wish to calculate. Unfortunately, there is rarely enough training data to compute accurate probabilities when decoding on new data. Generally, two smoothing approaches [21, 22, 23] are applied to resolve this problem: linear interpolation and back-off. However, these two approaches only work well when the number of different information sources is very limited. When a few features and/or a long context are considered, the number of different information sources is exponential. This makes smoothing approaches inappropriate in our system. In this paper, the maximum entropy model [24] is proposed to integrate various context information sources and resolve the data sparseness problem in our system. The reason that we choose the maximum entropy model for this purpose is tha"
I05-1047,W96-0213,0,0.0498466,"probability we wish to calculate. Unfortunately, there is rarely enough training data to compute accurate probabilities when decoding on new data. Generally, two smoothing approaches [21, 22, 23] are applied to resolve this problem: linear interpolation and back-off. However, these two approaches only work well when the number of different information sources is very limited. When a few features and/or a long context are considered, the number of different information sources is exponential. This makes smoothing approaches inappropriate in our system. In this paper, the maximum entropy model [24] is proposed to integrate various context information sources and resolve the data sparseness problem in our system. The reason that we choose the maximum entropy model for this purpose is that it represents the state-of– the-art in the machine learning research community and there are good implementations of the algorithm available. Here, we use the open NLP maximum entropy package6 in our system. 2.2 Maximum Entropy The maximum entropy model is a probability distribution estimation technique widely used in recent years for natural language processing tasks. The principle of the maximum entro"
I05-1047,W03-1730,0,0.14758,"Missing"
I05-1047,W03-1727,0,0.0242835,"Missing"
I05-1047,W03-1721,0,0.137001,"Missing"
I05-1047,W03-1722,0,0.0639502,"Missing"
I08-1004,P06-1006,0,0.620807,"structured objects in NLP, e.g. the parse tree structure in coreference resolution. During recent years, various tree kernels, such as the convolution tree kernel (Collins and Duffy 2001), the shallow parse tree kernel (Zelenko et al 2003) and the dependency tree kernel (Culota and Sorensen 2004), have been proposed in the literature. Among previous tree kernels, the convolution tree kernel represents the state-of-the-art and have been successfully applied by Collins and Duffy (2002) on parsing, Moschitti (2004) on semantic role labeling, Zhang et al (2006) on semantic relation extraction and Yang et al (2006) on pronoun resolution. However, there exist two problems in Collins and Duffy’s kernel. The first is that the sub-trees enumerated in the tree kernel are context-free. That is, each sub-tree enumerated in the tree kernel does not consider the context information outside the sub-tree. The second is how to decide a proper tree span in the tree kernel computation according to the particular application. To resolve above two problems, this paper proposes a new tree span scheme and applies a new tree kernel and to better capture syntactic structured information in pronoun Abstract This paper propo"
I08-1004,P06-1104,1,0.834442,"tive at reducing the burden of feature engineering for structured objects in NLP, e.g. the parse tree structure in coreference resolution. During recent years, various tree kernels, such as the convolution tree kernel (Collins and Duffy 2001), the shallow parse tree kernel (Zelenko et al 2003) and the dependency tree kernel (Culota and Sorensen 2004), have been proposed in the literature. Among previous tree kernels, the convolution tree kernel represents the state-of-the-art and have been successfully applied by Collins and Duffy (2002) on parsing, Moschitti (2004) on semantic role labeling, Zhang et al (2006) on semantic relation extraction and Yang et al (2006) on pronoun resolution. However, there exist two problems in Collins and Duffy’s kernel. The first is that the sub-trees enumerated in the tree kernel are context-free. That is, each sub-tree enumerated in the tree kernel does not consider the context information outside the sub-tree. The second is how to decide a proper tree span in the tree kernel computation according to the particular application. To resolve above two problems, this paper proposes a new tree span scheme and applies a new tree kernel and to better capture syntactic struc"
I08-1004,P05-1053,1,0.865224,"Missing"
I08-1004,D07-1076,1,0.936292,"in pronoun resolution. However, there is one problem with this tree kernel: the subtrees involved in the tree kernel computation are context-free (That is, they do not consider the information outside the sub-trees.). This is contrast to the tree kernel proposed in Culota and Sorensen (2004) which is context-sensitive, that is, it considers the path from the tree root node to the sub-tree root node. In order to integrate the advantages of both tree kernels and resolve the problem in Collins and Duffy’s kernel, this paper applies the same context-sensitive convolution tree kernel, proposed by Zhou et al (2007) on relation extraction. It works by taking ancestral information (i.e. the root node path) of sub-trees into consideration: m K C (T [1], T [2]) = ∑ i =1 ∑ ∆(n [1], n [2]) i 1 i 1 (1) n1i [1]∈ N 1i [1] n1i [ 2 ]∈ N 1i [ 2 ] Scheme/m 1 2 3 4 Min 78.5 79.8 80.8 80.8 Simple 79.8 81.0 81.7 81.6 Full 78.3 80.1 81.0 81.1 Dynamic 80.8 82.3 83.0 82.9 Table 1: Comparison of different context-sensitive convolution tree kernels and tree span schemes (with entity type info attached at both the anaphor and the antecedent candidate nodes by default) In this paper, the m parameter in our contextsensitive co"
I08-1004,P95-1017,0,0.142746,"Missing"
I08-1004,P06-1005,0,0.0673405,"Missing"
I08-1004,P01-1017,0,0.0324075,"context-sensitive sub-trees by taking their ancestor node paths into consideration. Moreover, this paper also implements a dynamic-expansion tree span scheme by taking predicate- and antecedent competitor-related information into consideration. 3 Context Sensitive Convolution Tree Kernel for Pronoun Resolution In this section, we first propose an algorithm to dynamically determine a proper tree span for pronoun resolution and then present a contextsensitive convolution tree kernel to compute similarity between two tree spans. In this paper, all the texts are parsed using the Charniak parser (Charniak 2001) based on which the tree span is determined. 3.1 Dynamic-Expansion Tree Span Scheme Normally, parsing is done on the sentence level. To deal with the cases that an anaphor and an antecedent candidate do not occur in the same sentence, we construct a pseudo parse tree for an entire text by attaching the parse trees of all its sentences to an upper “S”node, similar to Yang et al (2006). Given the parse tree of a text, the problem is how to choose a proper tree span to well cover syntactic structured information in the tree kernel computation. Generally, the more a tree span includes, the more sy"
I08-1004,P04-1054,0,0.0389763,"Missing"
I08-1004,P98-2143,0,0.0856917,"Missing"
I08-1004,P04-1043,0,0.0988994,"the kernel-based methods could be very effective at reducing the burden of feature engineering for structured objects in NLP, e.g. the parse tree structure in coreference resolution. During recent years, various tree kernels, such as the convolution tree kernel (Collins and Duffy 2001), the shallow parse tree kernel (Zelenko et al 2003) and the dependency tree kernel (Culota and Sorensen 2004), have been proposed in the literature. Among previous tree kernels, the convolution tree kernel represents the state-of-the-art and have been successfully applied by Collins and Duffy (2002) on parsing, Moschitti (2004) on semantic role labeling, Zhang et al (2006) on semantic relation extraction and Yang et al (2006) on pronoun resolution. However, there exist two problems in Collins and Duffy’s kernel. The first is that the sub-trees enumerated in the tree kernel are context-free. That is, each sub-tree enumerated in the tree kernel does not consider the context information outside the sub-tree. The second is how to decide a proper tree span in the tree kernel computation according to the particular application. To resolve above two problems, this paper proposes a new tree span scheme and applies a new tre"
I08-1004,J01-4004,0,0.755499,"Missing"
I08-1004,P03-1023,1,0.895263,"s done by: 1) Determining the min-expansion tree span via the shortest path, as shown in Figure 1(a). 2) Attaching all the antecedent competitors along the corresponding paths to the shortest path. As shown in Figure 1(b), “the woman”is attached while “the room”is not attached since the former is compatible with the anaphor and the latter is not compatible with the anaphor. In this way, the competition between the considered candidate and other compatible candidates can be included in the tree span. In some sense, this is a natural extension of the twin-candidate learning approach proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. 3) For each node in the tree span, attaching the path from the node to the predicate terminal node if it is a predicate-headed node. As shown in Figure 1(c), “said”and “bit”are attached. 4) Pruning those nodes (except POS nodes) with the single in-arc and the single out-arc and with its syntactic phrase type same as its child node. As shown in Figure 1(d), the left child of the “SBAR”node, the “NP”node, is removed and the sub-tree (NP the/DT woman/NN) is attached to the “SBAR”node directly. To show the difference amon"
I08-1004,J03-4003,0,\N,Missing
I08-1004,J94-4002,0,\N,Missing
I08-1004,C98-2138,0,\N,Missing
I08-1005,I05-1034,1,0.894483,"wer the query “Who is the president of the United States?”, and information retrieval, e.g. to expand the query “George W. Bush”with “the president of the United States” via his relationship with “the United States”. During the last decade, many methods have been proposed in relation extraction, such as supervised learning (Miller et al 2000; Zelenko et al 2003; Culota and Sorensen 2004; Zhao and Grishman 2005; Zhang et al 2006; Zhou et al 2005, 2006), semi-supervised learning (Brin 1998; Agichtein and Gravano 2000; Zhang 2004; Chen et al 2006), and unsupervised learning (Hasegawa et al 2004; Zhang et al 2005). Among these methods, supervised learning-based methods perform much 2 Related Work Generally, supervised learning is preferable to unsupervised learning due to prior knowledge in the 32 annotated training data and better performance. However, the annotated data is usually expensive to obtain. Hence, there has been growing interest in semi-supervised learning, aiming at inducing classifiers by leveraging a small amount of labeled data and a large amount of unlabeled data. Related work in relation extraction using semi-supervised learning can be classified into two categories: bootstrapping-ba"
I08-1005,P05-1052,0,0.0406849,"uction Relation extraction is to detect and classify various predefined semantic relations between two entities from text and can be very useful in many NLP applications such as question answering, e.g. to answer the query “Who is the president of the United States?”, and information retrieval, e.g. to expand the query “George W. Bush”with “the president of the United States” via his relationship with “the United States”. During the last decade, many methods have been proposed in relation extraction, such as supervised learning (Miller et al 2000; Zelenko et al 2003; Culota and Sorensen 2004; Zhao and Grishman 2005; Zhang et al 2006; Zhou et al 2005, 2006), semi-supervised learning (Brin 1998; Agichtein and Gravano 2000; Zhang 2004; Chen et al 2006), and unsupervised learning (Hasegawa et al 2004; Zhang et al 2005). Among these methods, supervised learning-based methods perform much 2 Related Work Generally, supervised learning is preferable to unsupervised learning due to prior knowledge in the 32 annotated training data and better performance. However, the annotated data is usually expensive to obtain. Hence, there has been growing interest in semi-supervised learning, aiming at inducing classifiers b"
I08-1005,P05-1053,1,0.913338,"Missing"
I08-1005,P06-1016,1,0.773743,"Missing"
I08-1005,A00-2030,0,\N,Missing
I08-1005,P06-1017,0,\N,Missing
I08-1005,P04-1043,0,\N,Missing
I08-1005,P04-1054,0,\N,Missing
I08-1005,P04-1053,0,\N,Missing
I08-1005,P05-1049,0,\N,Missing
I08-1005,P06-1104,1,\N,Missing
I08-1005,P01-1017,0,\N,Missing
I11-1002,H05-1091,0,0.0954116,"Missing"
I11-1002,C10-2087,1,0.789499,"Missing"
I11-1002,D10-1096,0,0.0217792,"endency path tree or the dependency graph. Currently, there are two established dependency representations available, viz. CoNLL scheme (adopted by CoNLL’2007 and CoNLL’2008 Shared tasks) (Nivre et al., 2007; Surdeanu et al., 2008) and Standford scheme (adopted by Stanford parser) (de Marneffe et al., 2006). These two schemes differ significantly in the representation of passive construction, position of auxiliary and modal verb, or coordination. It is generally acknowledged that the Stanford scheme is closer to the targeted semantic representation from the perspective of relation extraction (Buyko and Hahn, 2010). Particularly, among the four styles of Stanford representations, “collapsed dependency” can much simplify patterns in relation extraction since dependencies involving preposition, conjunct as well as referent of relative clause are effectively collapsed to reflect direct dependencies between content words. Therefore, the collapsed variant of Stanford scheme is adopted in this paper to refine the constituent parse tree as described in the next subsection. ciated constituents are added, the more important portion of “association between” and their associated constituents are still missing whil"
I11-1002,D09-1013,0,0.0435354,"Missing"
I11-1002,P04-1054,0,0.728128,"o many NLP tasks, particularly for semantic relation extraction and semantic role labeling, various kernels such as subsequence kernels (Bunescu et al., 2005a) and tree kernels (Li et al., 2008) have been applied to PPI extraction. On one hand, dependency-based kernels, such as edit distance kernels (Erkan et al. 2007), graph kernels (Airola et al., 2008) and subsequence kernels (Kim et al., 2010), show some promising results for PPI extraction. This suggests that dependency information plays a critical role in PPI extraction, much like semantic relation extraction in the newswire narratives (Culotta and Sorensen, 2004; Bunescu et al., 2005b). On the other hand, while tree kernels based on constituent parse trees achieve great success in semantic relation extraction (Zhang et al., 2006; Zhou et al., 2007a; Qian et al., 2008) and semantic role labeling (Moschitti, 2004; Zhang et al., 2008) from the newswire narratives, they haven’t been fully explored for PPI extraction in the biomedical domain. Considering the similarity between the task of PPI extraction from the biomedical domain and that of relation extraction from the newswire domain, one question naturally arises: “How can kernel-based Abstract Structu"
I11-1002,P08-1006,0,0.019841,"DT NN NNS cyclin B1 / PROT2 was detected in the Hela cells S T2: SPT T3: CS-SPT NP PP NP NP NN CC NN PROT1 and cyclin T4: DSPT NP NP NN NN B1 NP / NN NP PROT2 NN CC NN PROT1 and cyclin VP NP VP NN CC NN NN VBN PROT1 and PROT2 PROT2 detected NP NN NN B1 / NP Figure 1. Different tree setups for a PPI instance between PROT1 and PROT2 from sentence “Association between PROT1 and cyclin B1 / PROT2 was detected in the HeLa cells.” in the AIMed corpus that previous tree kernels over constituent parse trees have not yet achieved promising results for PPI extraction just as they do in the news domain. Miyao et al. (2008) conduct a comprehensive comparison of different syntactic representations for PPI extraction and find that the phrase structure tree in the form of the constituent parse tree (called PTB in their paper) performs significantly worse than the other representations. Tikk et al. (2010) extensively compares different kernelbased methods on PPI extraction and show that the tree kernel over the constituent parse tree only achieves the F1-score of 34.6 on the AIMed corpus. Actually, our preliminary experiment on PPI extraction via the convolution tree kernel over SPT only achieves the F1-score of abo"
I11-1002,de-marneffe-etal-2006-generating,0,0.057443,"Missing"
I11-1002,P04-1043,0,0.0980119,"nels, such as edit distance kernels (Erkan et al. 2007), graph kernels (Airola et al., 2008) and subsequence kernels (Kim et al., 2010), show some promising results for PPI extraction. This suggests that dependency information plays a critical role in PPI extraction, much like semantic relation extraction in the newswire narratives (Culotta and Sorensen, 2004; Bunescu et al., 2005b). On the other hand, while tree kernels based on constituent parse trees achieve great success in semantic relation extraction (Zhang et al., 2006; Zhou et al., 2007a; Qian et al., 2008) and semantic role labeling (Moschitti, 2004; Zhang et al., 2008) from the newswire narratives, they haven’t been fully explored for PPI extraction in the biomedical domain. Considering the similarity between the task of PPI extraction from the biomedical domain and that of relation extraction from the newswire domain, one question naturally arises: “How can kernel-based Abstract Structured information plays a critical role in many NLP tasks, such as semantic relation extraction between named entities and semantic role labeling. This paper proposes a principled way to automatically generate constituent structure representation for tree"
I11-1002,J08-2003,0,0.0481173,"Missing"
I11-1002,D07-1024,0,0.267904,"eature-based methods. However, the featurebased methods often fail to effectively capture the structured information, which is essential to identify the relationship between two proteins in a constituent or dependency-based syntactic representation. With the wide adoption of kernel-based methods to many NLP tasks, particularly for semantic relation extraction and semantic role labeling, various kernels such as subsequence kernels (Bunescu et al., 2005a) and tree kernels (Li et al., 2008) have been applied to PPI extraction. On one hand, dependency-based kernels, such as edit distance kernels (Erkan et al. 2007), graph kernels (Airola et al., 2008) and subsequence kernels (Kim et al., 2010), show some promising results for PPI extraction. This suggests that dependency information plays a critical role in PPI extraction, much like semantic relation extraction in the newswire narratives (Culotta and Sorensen, 2004; Bunescu et al., 2005b). On the other hand, while tree kernels based on constituent parse trees achieve great success in semantic relation extraction (Zhang et al., 2006; Zhou et al., 2007a; Qian et al., 2008) and semantic role labeling (Moschitti, 2004; Zhang et al., 2008) from the newswire"
I11-1002,E06-1051,0,0.0235383,"considering constituent dependencies on each node along the shortest path between two entity mentions and discarding irrelevant nodes. However, their adopted constituent dependency rules are manually constructed and thus difficult to adapt to other domains and languages. Related Work Due to space limitation, this section only gives an overview on kernel-based methods on PPI extraction in the biomedical domain as well as semantic relation extraction in the newswire domain. For details about feature-based methods, please refer to related studies in the biomedical domain (Mitsumori et al., 2006; Giuliano et al., 2006; Liu et al., 2010) and those in the newswire domain (Zhao et al., 2005; Zhou et al. 2005, 2007b), respectively. PPI extraction in biomedical domain Representative kernel-based methods on PPI extraction take advantage of lexical or dependency information. Bunescu et al. (2005a) adopt a generalized substring kernel over a mixture of words and word classes to extract protein interactions from biomedical corpora and semantic relations from newswire corpora. Particularly, they achieve the F1-score of 54.2 in extracting protein interactions from the AIMed corpus. Erkan et al. (2007) first define tw"
I11-1002,C08-1088,1,0.786359,"I extraction. On one hand, dependency-based kernels, such as edit distance kernels (Erkan et al. 2007), graph kernels (Airola et al., 2008) and subsequence kernels (Kim et al., 2010), show some promising results for PPI extraction. This suggests that dependency information plays a critical role in PPI extraction, much like semantic relation extraction in the newswire narratives (Culotta and Sorensen, 2004; Bunescu et al., 2005b). On the other hand, while tree kernels based on constituent parse trees achieve great success in semantic relation extraction (Zhang et al., 2006; Zhou et al., 2007a; Qian et al., 2008) and semantic role labeling (Moschitti, 2004; Zhang et al., 2008) from the newswire narratives, they haven’t been fully explored for PPI extraction in the biomedical domain. Considering the similarity between the task of PPI extraction from the biomedical domain and that of relation extraction from the newswire domain, one question naturally arises: “How can kernel-based Abstract Structured information plays a critical role in many NLP tasks, such as semantic relation extraction between named entities and semantic role labeling. This paper proposes a principled way to automatically generate co"
I11-1002,P06-1104,1,0.906463,"Li et al., 2008) have been applied to PPI extraction. On one hand, dependency-based kernels, such as edit distance kernels (Erkan et al. 2007), graph kernels (Airola et al., 2008) and subsequence kernels (Kim et al., 2010), show some promising results for PPI extraction. This suggests that dependency information plays a critical role in PPI extraction, much like semantic relation extraction in the newswire narratives (Culotta and Sorensen, 2004; Bunescu et al., 2005b). On the other hand, while tree kernels based on constituent parse trees achieve great success in semantic relation extraction (Zhang et al., 2006; Zhou et al., 2007a; Qian et al., 2008) and semantic role labeling (Moschitti, 2004; Zhang et al., 2008) from the newswire narratives, they haven’t been fully explored for PPI extraction in the biomedical domain. Considering the similarity between the task of PPI extraction from the biomedical domain and that of relation extraction from the newswire domain, one question naturally arises: “How can kernel-based Abstract Structured information plays a critical role in many NLP tasks, such as semantic relation extraction between named entities and semantic role labeling. This paper proposes a pri"
I11-1002,I08-2109,0,0.0532465,"t distance kernels (Erkan et al. 2007), graph kernels (Airola et al., 2008) and subsequence kernels (Kim et al., 2010), show some promising results for PPI extraction. This suggests that dependency information plays a critical role in PPI extraction, much like semantic relation extraction in the newswire narratives (Culotta and Sorensen, 2004; Bunescu et al., 2005b). On the other hand, while tree kernels based on constituent parse trees achieve great success in semantic relation extraction (Zhang et al., 2006; Zhou et al., 2007a; Qian et al., 2008) and semantic role labeling (Moschitti, 2004; Zhang et al., 2008) from the newswire narratives, they haven’t been fully explored for PPI extraction in the biomedical domain. Considering the similarity between the task of PPI extraction from the biomedical domain and that of relation extraction from the newswire domain, one question naturally arises: “How can kernel-based Abstract Structured information plays a critical role in many NLP tasks, such as semantic relation extraction between named entities and semantic role labeling. This paper proposes a principled way to automatically generate constituent structure representation for tree kernel-based protein-"
I11-1002,P05-1052,0,0.0711989,"Missing"
I11-1002,P05-1053,1,0.89358,"entions and discarding irrelevant nodes. However, their adopted constituent dependency rules are manually constructed and thus difficult to adapt to other domains and languages. Related Work Due to space limitation, this section only gives an overview on kernel-based methods on PPI extraction in the biomedical domain as well as semantic relation extraction in the newswire domain. For details about feature-based methods, please refer to related studies in the biomedical domain (Mitsumori et al., 2006; Giuliano et al., 2006; Liu et al., 2010) and those in the newswire domain (Zhao et al., 2005; Zhou et al. 2005, 2007b), respectively. PPI extraction in biomedical domain Representative kernel-based methods on PPI extraction take advantage of lexical or dependency information. Bunescu et al. (2005a) adopt a generalized substring kernel over a mixture of words and word classes to extract protein interactions from biomedical corpora and semantic relations from newswire corpora. Particularly, they achieve the F1-score of 54.2 in extracting protein interactions from the AIMed corpus. Erkan et al. (2007) first define two similarity functions based on co11 S T1: FPT NP VP PP VP NP NP PP NP NN IN NN NP NP CC"
I11-1002,W08-2121,0,\N,Missing
I11-1002,D07-1096,0,\N,Missing
I11-1002,D07-1076,1,\N,Missing
I11-1072,P07-2026,0,0.0161257,"not guarantee that the translations obtained in each iteration are good e643 T-MERT, we observe that the performance is always the best when the inner-MERT terminates as Hi reaches peak1 . 3.4 Table 1: Statistics on development and test data sets. DATA S ET MT03 MT05 MT06 MT08 Hypothesis Selection with Minimum Bayes Risk(MBR) From T-MERT algorithm, we can get M different results from M outer-translation rounds. Due to intrinsic property and the randomness in MERT, the results from outer-translation step of T-MERT are not quite stable, making the hypotheses selection a necessity. According to (Ehling et al., 2007), for each source sentence with N different translations, we could select the final translation based on the following Minimum Bayes Risk principal: eˆ = arg min{ e (P r(e0 |f ) · (1 − BLEU (e0 , e)))} (6) Here P r(e0 |f ) denotes the posterior probability for translation e0 and BLEU (e0 , e) represents the sentence-level BLEU score for e0 using e as reference. However, since the translation hypotheses are generated under different groups of weights, the corresponding posterior probability is no longer comparable. Here we simplify this problem under the assumption that all available translatio"
I11-1072,P07-1007,0,0.0299923,", since overtraining is serious due to the limited size of development data. In this work, we use hyper-parameter to indicate the overtraining in the estimation step. Finally, our method is more efficient than adaptation on the translation & language model. In Ueffing et al.(2007), training model building is necessary for each round, which is time consuming. By comparison, the running time is much shorter for our method, since no model building is required, although it is still longer than simple onepass translation under baseline. the previous works of transductive learning(Liu et al., 2010; Chan and Ng, 2007), the unlabeled data can be used to improve the model training so as to tackle the bias-estimation problem. Under such framework, the weight learned on both development and test dataset, in which the test dataset is constructed using n-best translations as pseudo references, moves towards the test data with regularization of development data, which alleviates the overtraining in normal MERT and matches the test data better. The remainder of this paper is structured as follows: Related works on model adaptation in SMT are presented in Section 2, and our transductive MERT is proposed in Section"
I11-1072,P05-1033,0,0.0256378,". We build the 5-gram language model on the English section of all bilingual training data together with the Xinhua portion of the English Gigaword corpus. The development and test dataset pairs are selected from NIST2003 (MT03), NIST2005 (MT05), NIST2006 (MT06) and NIST2008 (MT08). The data statistics are shown in Table 1. In the experiments, all translation results are measured in case-insensitive BLEU scores (Papineni et al., 2002). X e0 #S ENTENCE 919 1,082 1,664 1,357 Experimental Results Experimental Setup In the experiments, we re-implement a hierarchical phrase-based decoder based on (Chiang, 2005). The word alignment is trained by GIZA++ under an intersect-diag-grow heuristics refinement. The plain phrases are extracted from all bilingual training data available from LDC, including LDC2002E18, LDC2003E07, LDC2003E14, 1 And we find that in experiments, hyper-parameter Hi of the second round is always maximal. 644 vor those incorrectly generated translation references, which makes the overtraining more serious and hurts the final performance. By applying the hyper-parameter as the stop metric, we could control the learning procedure to avoid the overtraining. We can also review the roles"
I11-1072,W07-0717,0,0.041765,"Missing"
I11-1072,2005.eamt-1.19,0,0.0290747,"tion 2, and our transductive MERT is proposed in Section 3. Experimental results are shown in Section 4, followed by conclusions and future work in the last section. 2 Related Work Model adaptation in SMT has attracted increasing attentions in recent years. As mentioned in the previous section, corresponding to the two steps in SMT pipeline, there are two directions for adaptations. The first one is feature adaptation, which tries to build model (translation model & language model) that could fit the development or test dataset better. This direction includes data selection (L¨u et al., 2007; Hildebrand et al., 2005) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). However, efficiency is the main obstacle for these methods (esp. data selection approach) since model building is time consuming. The second direction is model parameter adaptation, which includes the transductive MERT method we propose in this article. Nevertheless, little attention has been paid to this direction to date. Mohit et al (2009) tried to build a classifier to predict whether or not a phrase is difficult. The language model weight is then adapted for each phrase segment based on this difficulty. In Li et al (2010"
I11-1072,W04-3250,0,0.128511,"Missing"
I11-1072,C10-1075,1,0.782011,"et al., 2005) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). However, efficiency is the main obstacle for these methods (esp. data selection approach) since model building is time consuming. The second direction is model parameter adaptation, which includes the transductive MERT method we propose in this article. Nevertheless, little attention has been paid to this direction to date. Mohit et al (2009) tried to build a classifier to predict whether or not a phrase is difficult. The language model weight is then adapted for each phrase segment based on this difficulty. In Li et al (2010), a related subset of development dataset is extracted for given test dataset. The test dataset is then translated under weight learned on this subset. Besides, Sanchis-Trilles and Casacuberta (2010) propose Bayesian adaptation for weight optimization based on a small amount of labeled test data, which is not necessary in our work. The most similar previous work with ours is Ueffing et al (2007), who also propose a transductive learning framework for SMT. However, our 3 Transductive MERT for Machine Translation 3.1 Minimum Error Rate Training In SMT, given a development dataset containing sour"
I11-1072,D07-1036,0,0.0469544,"Missing"
I11-1072,D09-1074,0,0.0149196,"ntal results are shown in Section 4, followed by conclusions and future work in the last section. 2 Related Work Model adaptation in SMT has attracted increasing attentions in recent years. As mentioned in the previous section, corresponding to the two steps in SMT pipeline, there are two directions for adaptations. The first one is feature adaptation, which tries to build model (translation model & language model) that could fit the development or test dataset better. This direction includes data selection (L¨u et al., 2007; Hildebrand et al., 2005) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). However, efficiency is the main obstacle for these methods (esp. data selection approach) since model building is time consuming. The second direction is model parameter adaptation, which includes the transductive MERT method we propose in this article. Nevertheless, little attention has been paid to this direction to date. Mohit et al (2009) tried to build a classifier to predict whether or not a phrase is difficult. The language model weight is then adapted for each phrase segment based on this difficulty. In Li et al (2010), a related subset of development dataset is extracted for given t"
I11-1072,2009.eamt-1.22,0,0.0146165,"on, which tries to build model (translation model & language model) that could fit the development or test dataset better. This direction includes data selection (L¨u et al., 2007; Hildebrand et al., 2005) and data weighting (Foster and Kuhn, 2007; Matsoukas et al., 2009). However, efficiency is the main obstacle for these methods (esp. data selection approach) since model building is time consuming. The second direction is model parameter adaptation, which includes the transductive MERT method we propose in this article. Nevertheless, little attention has been paid to this direction to date. Mohit et al (2009) tried to build a classifier to predict whether or not a phrase is difficult. The language model weight is then adapted for each phrase segment based on this difficulty. In Li et al (2010), a related subset of development dataset is extracted for given test dataset. The test dataset is then translated under weight learned on this subset. Besides, Sanchis-Trilles and Casacuberta (2010) propose Bayesian adaptation for weight optimization based on a small amount of labeled test data, which is not necessary in our work. The most similar previous work with ours is Ueffing et al (2007), who also pro"
I11-1072,P03-1021,0,0.110377,"t dataset is then translated under weight learned on this subset. Besides, Sanchis-Trilles and Casacuberta (2010) propose Bayesian adaptation for weight optimization based on a small amount of labeled test data, which is not necessary in our work. The most similar previous work with ours is Ueffing et al (2007), who also propose a transductive learning framework for SMT. However, our 3 Transductive MERT for Machine Translation 3.1 Minimum Error Rate Training In SMT, given a development dataset containing source sentences F1S with corresponding reference translations R1S , the purpose of MERT (Och, 2003) is to find a set of parameters λM 1 which optimizes an automated evaluation metric (e.g., BLEU) under a log-linear model: ˆ M = arg min λ 1 S X ˆ s ; λM ))) (3) (Err(Rs , E(F 1 λM 1 s=1 in which the number of errors in sentence E is obtained by comparing it with a reference sentence R using function Err(R, E) and ˆ s ; λM ) = arg max E(F 1 E S X (λm hm (E, Fs )) (4) s=1 As shown in Algorithm 1, the decoder translates development dataset under current weight(default weight for first round), and generates N-best translation hypotheses for each sentence. The weight is then updated according to e"
I11-1072,P02-1038,0,0.0748791,"ures and their corresponding weights are both important issues in SMT. In this article we focus on the latter issue, i.e., the model parameter adaptation. From the viewpoint of machine learning, development data is labeled data used for parameter learning, while test data is unlabeled and applied for evaluation. In Introduction Machine translation (MT) is the automatic translation from one natural language into another using computer, while SMT is an approach to MT that is characterized by the use of machine learning methods (Lopez, 2008). Nowadays, SMT is usually built on a log-linear model (Och and Ney, 2002), which can be abstracted into two steps: the ∗ Part of this work is done during the first author’s internship at Microsoft Research Asia. 641 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 641–648, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP method is different from their in the following three aspects: Firstly, our method focuses on model parameter adaptation, while Ueffing et al (2007) pays attention to feature adaptation. In their work, the training model is rebuilt by combining original training data with n-best translation output"
I11-1072,P02-1040,0,0.0979398,"archical rules are only extracted from selected data sets, including LDC2003E14, LDC2003E07, LDC2005T10, LDC2006E34, LDC2006E85, and LDC2006E92, which contain about 467K sentence pairs. We build the 5-gram language model on the English section of all bilingual training data together with the Xinhua portion of the English Gigaword corpus. The development and test dataset pairs are selected from NIST2003 (MT03), NIST2005 (MT05), NIST2006 (MT06) and NIST2008 (MT08). The data statistics are shown in Table 1. In the experiments, all translation results are measured in case-insensitive BLEU scores (Papineni et al., 2002). X e0 #S ENTENCE 919 1,082 1,664 1,357 Experimental Results Experimental Setup In the experiments, we re-implement a hierarchical phrase-based decoder based on (Chiang, 2005). The word alignment is trained by GIZA++ under an intersect-diag-grow heuristics refinement. The plain phrases are extracted from all bilingual training data available from LDC, including LDC2002E18, LDC2003E07, LDC2003E14, 1 And we find that in experiments, hyper-parameter Hi of the second round is always maximal. 644 vor those incorrectly generated translation references, which makes the overtraining more serious and h"
I11-1072,C10-2124,0,0.191458,"ce model building is time consuming. The second direction is model parameter adaptation, which includes the transductive MERT method we propose in this article. Nevertheless, little attention has been paid to this direction to date. Mohit et al (2009) tried to build a classifier to predict whether or not a phrase is difficult. The language model weight is then adapted for each phrase segment based on this difficulty. In Li et al (2010), a related subset of development dataset is extracted for given test dataset. The test dataset is then translated under weight learned on this subset. Besides, Sanchis-Trilles and Casacuberta (2010) propose Bayesian adaptation for weight optimization based on a small amount of labeled test data, which is not necessary in our work. The most similar previous work with ours is Ueffing et al (2007), who also propose a transductive learning framework for SMT. However, our 3 Transductive MERT for Machine Translation 3.1 Minimum Error Rate Training In SMT, given a development dataset containing source sentences F1S with corresponding reference translations R1S , the purpose of MERT (Och, 2003) is to find a set of parameters λM 1 which optimizes an automated evaluation metric (e.g., BLEU) under"
I11-1072,P07-1004,0,0.296145,"is an approach to MT that is characterized by the use of machine learning methods (Lopez, 2008). Nowadays, SMT is usually built on a log-linear model (Och and Ney, 2002), which can be abstracted into two steps: the ∗ Part of this work is done during the first author’s internship at Microsoft Research Asia. 641 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 641–648, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP method is different from their in the following three aspects: Firstly, our method focuses on model parameter adaptation, while Ueffing et al (2007) pays attention to feature adaptation. In their work, the training model is rebuilt by combining original training data with n-best translation outputs of development and test data, in order to overcome the data sparseness problem. In contrast, we try to solve the parameter bias-estimated problem using the information of both development and test data. Secondly, the parameter adaptation problem is more complicated in SMT, since overtraining is serious due to the limited size of development data. In this work, we use hyper-parameter to indicate the overtraining in the estimation step. Finally,"
K15-2004,J93-2004,0,0.0496088,"ng determines the internal structure of a text via identifying the discourse relations between its text units and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to 2 System Architecture In this section, after a quick overview of our system, we describe"
K15-2004,W13-3303,0,0.0770476,"Missing"
K15-2004,P09-2004,0,0.725356,"ts and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to 2 System Architecture In this section, after a quick overview of our system, we describe the details involved in implementing the end-to-end shallow discourse parser. 2.1 System Overview A typical te"
K15-2004,P05-1018,0,0.133453,"Missing"
K15-2004,prasad-etal-2008-penn,0,0.740449,"Missing"
K15-2004,W01-1605,0,0.336334,"11), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to 2 System Architecture In this section, after a quick overview of our system, we describe the details involved in implementing the end-to-end shallow discourse parser. 2.1 System Overview A typical text consists of sentences glued together in a systematic way to form a coherent discourse. 32 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 32–36,"
K15-2004,prasad-etal-2010-exploiting,0,0.058121,"role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to 2 System Architecture In this section, after a quick overview of our system, we describe the details involved in implementing the end-to-end shallow discourse parser. 2.1 System Overview A typical text consists of senten"
K15-2004,I11-1120,0,0.0144547,"age understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to 2 System Architecture In this section, after a quick overview of our system, we describe the details involved in implementing the end-to-end shallow discourse parser. 2.1 System Overview A typical text consists of sentences glued together i"
K15-2004,H05-1044,0,0.030634,"al. (2009), we extract the pairs of verbs from the given adjacent sentence pair (i.e., Arg1 and Arg2). Besides that, the number of verb pairs which have the same highest VerbNet verb class (Kipper et al., 2006) is included as a feature. the average length of verb phrases in each argument, and the POS of main verbs are also included. Polarity: This set of features record the number of positive, negated positive, negative and neutral words in both arguments and their cross-product. The polarity of every word in arguments is derived from Multi-perspective Question Answering Opinion Corpus(MPQA) (Wilson et al., 2005). Intuitively, polarity features would help recognize Comparison relations. Modality: We include a set of features to record the presence or absence of specific modal words (i.e., can, may, will, shall, must, need) in Arg1 and Arg2, and their cross-product. The intuition • Connective related features: connective itself, its syntactic category, its sense class.3 • Number of left/right siblings of the connective. • The context of the constituent. We use POS combination of the constituent, its parent, left sibling and right sibling to represent the context. When there is no parent or siblings, it"
K15-2004,kipper-etal-2006-extending,0,0.0392896,"on-explicit sense Classification Referring to the PDTB, the non-explicit relations4 are annotated for all adjacent sentence pairs within paragraphs. So non-explicit sense classification only considers the sense of every adjacent sentence pair within a paragraph without explicit discourse relations. Our non-explicit sense classifier includes seven traditional features: Verbs: Following the work of Pitler et al. (2009), we extract the pairs of verbs from the given adjacent sentence pair (i.e., Arg1 and Arg2). Besides that, the number of verb pairs which have the same highest VerbNet verb class (Kipper et al., 2006) is included as a feature. the average length of verb phrases in each argument, and the POS of main verbs are also included. Polarity: This set of features record the number of positive, negated positive, negative and neutral words in both arguments and their cross-product. The polarity of every word in arguments is derived from Multi-perspective Question Answering Opinion Corpus(MPQA) (Wilson et al., 2005). Intuitively, polarity features would help recognize Comparison relations. Modality: We include a set of features to record the presence or absence of specific modal words (i.e., can, may,"
K15-2004,K15-2001,0,0.0925518,"Missing"
K15-2004,D14-1008,1,0.843423,"Missing"
K15-2004,D09-1036,0,0.24317,"Missing"
K15-2004,P11-1100,0,0.13464,"Missing"
K15-2004,P12-1106,0,0.180206,"Missing"
K16-2009,P05-1018,0,0.0218917,"Missing"
K16-2009,W01-1605,0,0.340772,"Missing"
K16-2009,prasad-etal-2008-penn,0,0.383481,"Missing"
K16-2009,D09-1036,0,0.0918186,"Missing"
K16-2009,prasad-etal-2010-exploiting,0,0.0617725,"Missing"
K16-2009,K15-2002,0,0.202279,"Missing"
K16-2009,P11-1100,0,0.0291601,"Missing"
K16-2009,P12-1106,0,0.0282517,"Missing"
K16-2009,J93-2004,0,0.0617108,"Missing"
K16-2009,W13-3303,0,0.0314232,"Missing"
K16-2009,P09-2004,0,0.458186,"ts and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations In this paper, we describe the system submission from the NLP group of Soochow university (SoNLP-DP). Our shallow discourse parser consists of multiple components in a pipeline architecture, including a connective classifier, argument labeler, explicit classifier, non-explicit classifier. Our system is evaluated on the CoNLL-2016 Shared Task clo"
K16-2009,D12-1092,1,\N,Missing
K16-2009,P14-1080,0,\N,Missing
K16-2009,K15-2001,0,\N,Missing
K16-2009,K15-2004,1,\N,Missing
K16-2009,D14-1224,1,\N,Missing
K16-2009,P12-1008,0,\N,Missing
K16-2009,D14-1008,1,\N,Missing
K16-2009,K16-2001,0,\N,Missing
K16-2011,P05-1018,0,0.0470744,"Missing"
K16-2011,kipper-etal-2006-extending,0,0.014395,"oosing one word cluster from Arg1 and the other from Arg2. course relation by merging all the first EDUs of the EDU pairs as Arg1 of the connective, and merging all the second EDUs of the EDU pairs as Arg2. Non-explicit discourse relations. If a valid EDU pair is not linked to any explicit connective, we construct a non-explicit discourse relation by regarding the first EDU as Arg1 and the second as Arg2. 2.6 Besides the above features, the research on English sense classification for non-explicit discourse relations has explored other useful features about polarity, modality, and verb class (Karin et al., 2006). Unfortunately, the shared task on Chinese does not provide relevant resources to obtain those features. Sense Classification for Explicit discourse relations Once an explicit discourse relation is identified, the sense classifier is used to predict its sense. Due to the fact the connective themselves are strong hint for their sense, we follow (Lin et al., 2014) to define a few lexical features to train a sense classifier: the connective words themselves, their partof-speeches and the previous words of each connective word. 2.7 3 Experimentation We evaluate our system on the Chinese dataset p"
K16-2011,K15-2004,1,0.703846,"Missing"
K16-2011,D12-1092,1,0.889202,"Missing"
K16-2011,D14-1224,1,0.879909,"Missing"
K16-2011,D09-1036,0,0.0710323,"Missing"
K16-2011,P11-1100,0,0.0299653,"Missing"
K16-2011,prasad-etal-2008-penn,0,0.405095,"Missing"
K16-2011,P14-1080,0,0.0317992,"Missing"
K16-2011,K15-2001,0,0.0866757,"Missing"
K16-2011,K16-2001,0,0.0387758,"Missing"
K16-2011,P12-1008,0,0.261375,"Missing"
K16-2011,W01-1605,0,\N,Missing
K16-2011,D14-1008,1,\N,Missing
N19-1287,D18-1389,0,0.124693,"ed the corpora annotated with sentence-level event factuality information, such as ACE 20051 , LU (Diab et al., 2009), FactBank (Saur´ı and Pustejovsky, 2009), and UDS-IH2 (Rudinger et al., 2018). On the other hand, previous studies only con1 https://catalog.ldc.upenn.edu/LDC2006T06 2799 Proceedings of NAACL-HLT 2019, pages 2799–2809 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics sidered information within sentences, using rules (Saur´ı, 2008; Saur´ı and Pustejovsky, 2012), machine learning models (de Marneffe et al., 2012; Werner et al., 2015; Baly et al., 2018), and combinations of them (Qian et al., 2015; Stanovsky et al., 2017) for modeling. Neural network models have also recently been used for the sentence-level event factuality identification (He et al., 2017; Rudinger et al., 2018; Qian et al., 2018). According to Figure 1, document-level event factuality can not be deduced from each sentence-level factuality separately, but depends on the comprehensive semantic information of sentences. However, no suitable model for document-level task has been proposed yet. To solve the issues above, this paper focuses on document-level event factuality ide"
N19-1287,W17-4803,0,0.0245325,"have studied document-level tasks in many NLP applications, e.g., sentiment analysis (Xu et al., 2016; Dou, 2017), named entity recognition (Luo et al., 2018), and machine translation (Born et al., 2017). But related studies on event factuality are limited to the sentence-level task. Diab et al. (2009) and Prabhakaran et al. (2010) presented studies of belief annotation and tagging, and classified predicate events into Committed Belief (CB), Non-CB or Not Applicable using a supervised framework. For factuality assessment, Lee et al. (2015) employed dependency features, while Stanovsky et al. (2017) considered deep linguistic information, such as modality classes, syntactic re-ordering with PropS tree annotation structure (Lotan et al., 2013). Baly et al. (2018) considered a set of features and predicted the factuality of reporting and bias of news media. 2806 Saur´ı (2008) and Saur´ı and Pustejovsky (2012) proposed a rule-based model to identify event factuality on FactBank. de Marneffe et al. (2012) used a machine learning model and Qian et al. (2015) utilized a two-step framework combining machine learning and rule-based approaches on FactBank. In addition to FactBank, Prabhakaran et"
N19-1287,N18-2055,0,0.0312686,"de Marneffe et al. (2012) used a machine learning model and Qian et al. (2015) utilized a two-step framework combining machine learning and rule-based approaches on FactBank. In addition to FactBank, Prabhakaran et al. (2015) proposed a ongoing framework for a larger corpus based on LU, and Cao et al. (2013) constructed a Chinese corpus annotated with event factuality based on ACE 2005. However, no previous work annotated a document-level corpus. We construct DLEF corpus with document-level event factuality for the first time. Some studies focused on document-level event identification task. Choubey et al. (2018) designed a rule-based classifier to identify central events according to event coreference relations. Liu et al. (2018) utilized a kernel-based neural model that captured semantic relations between discourse units for event salience identification. However, they did not consider the documentlevel event factuality. To our best knowledge, this paper is the first work on document-level event factuality identification task. Previous studies (He et al., 2017; Rudinger et al., 2018; Qian et al., 2018) have tried neural network models on sentence-level factuality identification. Recent research has"
N19-1287,W09-3012,0,0.0813525,"n answer questions about the text. According to the document in Figure 1, the answer of the following question should be “No”, which is consistent with the document-level factuality of the event “reach” (CT-): Q: Does the U.S. reach an agreement with Mexico on the new trade deal before December 2017? A: No. Previous studies mostly reported on sentencelevel event factuality identification tasks. On one hand, due to the scarcity of document-level event factuality corpus, these studies only considered the corpora annotated with sentence-level event factuality information, such as ACE 20051 , LU (Diab et al., 2009), FactBank (Saur´ı and Pustejovsky, 2009), and UDS-IH2 (Rudinger et al., 2018). On the other hand, previous studies only con1 https://catalog.ldc.upenn.edu/LDC2006T06 2799 Proceedings of NAACL-HLT 2019, pages 2799–2809 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics sidered information within sentences, using rules (Saur´ı, 2008; Saur´ı and Pustejovsky, 2012), machine learning models (de Marneffe et al., 2012; Werner et al., 2015; Baly et al., 2018), and combinations of them (Qian et al., 2015; Stanovsky et al., 2017) for modeling. Neural network"
N19-1287,S15-1009,0,0.0395026,"Missing"
N19-1287,D17-1054,0,0.0294498,"cro-averaged F1 of joint optimization model on English and Chinese corpus are 82.89/75.64 and 83.83/81.48, respectively. Although document-level event factuality is based on the factuality information in sentences, sentencelevel factuality value of an event only depends on the current sentence, and is likely to have a different value compared to the current document-level factuality. Therefore, the joint model can not improve the performance of document-level task. 5 Related Work Researchers have studied document-level tasks in many NLP applications, e.g., sentiment analysis (Xu et al., 2016; Dou, 2017), named entity recognition (Luo et al., 2018), and machine translation (Born et al., 2017). But related studies on event factuality are limited to the sentence-level task. Diab et al. (2009) and Prabhakaran et al. (2010) presented studies of belief annotation and tagging, and classified predicate events into Committed Belief (CB), Non-CB or Not Applicable using a supervised framework. For factuality assessment, Lee et al. (2015) employed dependency features, while Stanovsky et al. (2017) considered deep linguistic information, such as modality classes, syntactic re-ordering with PropS tree ann"
N19-1287,D15-1189,0,0.0544598,"e sentence before and after the current sentence containing the event as the input. Compared to Att 2, Att 2+AT considers Adversarial Training (AT, the same below). We also consider the following baselines for the comparison with our models: MaxEntVote is a maximum entropy model that only considers the view of AUTHOR (de Marneffe et al., 2012). We use maximum entropy model to identify sentence-level event factuality, and consider voting mechanism, i.e., choose the value committed by the most sentences as the document-level factuality value. We also consider other machine learning models, e.g. Lee et al. (2015), but obtain lower micro-/macro-averaged F1 on English (59.38/33.36) and Chinese corpus (53.91/43.20). SentVote identifies sentence-level event factuality, and does not consider inter-sequence attention in the model proposed in Section 3. Similar to MaxEntVote model, voting mechanism is used to identify document-level event factuality in this SentVote model. MP 2 considers Max-Pooling instead of attention compared with Att 2. Att 1 considers only intra-sequence attention, but not the inter-sequence attention. For an event, we concatenate its i dependency paths and j sentences into one path and"
N19-1287,D16-1130,0,0.0293199,"elations between discourse units for event salience identification. However, they did not consider the documentlevel event factuality. To our best knowledge, this paper is the first work on document-level event factuality identification task. Previous studies (He et al., 2017; Rudinger et al., 2018; Qian et al., 2018) have tried neural network models on sentence-level factuality identification. Recent research has shown that neural networks with multi-level attention can extract meaningful information from heterogeneous input and improve the performance of NLP tasks, e.g., discourse relation (Liu and Li, 2016), relation classification (Wang et al., 2016), and question answering (Yu et al., 2017). Moreover, to improve the robustness of neural networks, related studies considered adversarial perturbation and training on text classification (Miyato et al., 2016) and relation extraction (Wu et al., 2017). This paper is in line in proposing an adversarial neural network with both intra- and inter-sequence attention. 6 Conclusion We investigated document-level event factuality identification task by constructing a corpus annotated with document- and sentence-level event factuality based on both English a"
N19-1287,D18-1154,0,0.0249346,"ine learning and rule-based approaches on FactBank. In addition to FactBank, Prabhakaran et al. (2015) proposed a ongoing framework for a larger corpus based on LU, and Cao et al. (2013) constructed a Chinese corpus annotated with event factuality based on ACE 2005. However, no previous work annotated a document-level corpus. We construct DLEF corpus with document-level event factuality for the first time. Some studies focused on document-level event identification task. Choubey et al. (2018) designed a rule-based classifier to identify central events according to event coreference relations. Liu et al. (2018) utilized a kernel-based neural model that captured semantic relations between discourse units for event salience identification. However, they did not consider the documentlevel event factuality. To our best knowledge, this paper is the first work on document-level event factuality identification task. Previous studies (He et al., 2017; Rudinger et al., 2018; Qian et al., 2018) have tried neural network models on sentence-level factuality identification. Recent research has shown that neural networks with multi-level attention can extract meaningful information from heterogeneous input and im"
N19-1287,N13-1091,0,0.0218295,"with PropS tree annotation structure (Lotan et al., 2013). Baly et al. (2018) considered a set of features and predicted the factuality of reporting and bias of news media. 2806 Saur´ı (2008) and Saur´ı and Pustejovsky (2012) proposed a rule-based model to identify event factuality on FactBank. de Marneffe et al. (2012) used a machine learning model and Qian et al. (2015) utilized a two-step framework combining machine learning and rule-based approaches on FactBank. In addition to FactBank, Prabhakaran et al. (2015) proposed a ongoing framework for a larger corpus based on LU, and Cao et al. (2013) constructed a Chinese corpus annotated with event factuality based on ACE 2005. However, no previous work annotated a document-level corpus. We construct DLEF corpus with document-level event factuality for the first time. Some studies focused on document-level event identification task. Choubey et al. (2018) designed a rule-based classifier to identify central events according to event coreference relations. Liu et al. (2018) utilized a kernel-based neural model that captured semantic relations between discourse units for event salience identification. However, they did not consider the docu"
N19-1287,J12-2003,0,0.0294517,"Missing"
N19-1287,C10-2117,0,0.0207184,"entences, sentencelevel factuality value of an event only depends on the current sentence, and is likely to have a different value compared to the current document-level factuality. Therefore, the joint model can not improve the performance of document-level task. 5 Related Work Researchers have studied document-level tasks in many NLP applications, e.g., sentiment analysis (Xu et al., 2016; Dou, 2017), named entity recognition (Luo et al., 2018), and machine translation (Born et al., 2017). But related studies on event factuality are limited to the sentence-level task. Diab et al. (2009) and Prabhakaran et al. (2010) presented studies of belief annotation and tagging, and classified predicate events into Committed Belief (CB), Non-CB or Not Applicable using a supervised framework. For factuality assessment, Lee et al. (2015) employed dependency features, while Stanovsky et al. (2017) considered deep linguistic information, such as modality classes, syntactic re-ordering with PropS tree annotation structure (Lotan et al., 2013). Baly et al. (2018) considered a set of features and predicted the factuality of reporting and bias of news media. 2806 Saur´ı (2008) and Saur´ı and Pustejovsky (2012) proposed a ru"
N19-1287,E17-1110,0,0.0273382,"Figure 2, to extract feature representations of events from the view of documents, we consider both intra- and inter-sequence attention for dependency paths and sentences. In addition, due to the diversity of contents of docIn addition, we also consider the above features in contexts of each sentence containing the event as the input, and set the windows size as 3, i.e., one sentence before and after the current one. If adjacent sentences contain speculative or negative cues, the dependency path is the concatenation of the path from the cue to the root and the path from the root to the event (Quirk and Poon, 2017). 2802 3.2 LSTM with Two Attention Layers A dependency path or sentence can be represented as X0 according to the embedding table. We employ LSTM with hidden units nh to model the sequences from both directions to produce the for− → ward hidden sequence H, the backward hidden − → − → sequence H, and the output sequence H = H + ← − H. We adopt the attention mechanism to capture the most important information from H, and obtain the output h: Hm = tanh(H) (1) α = softmax(v T H) (2) T h = tanh(Hα ) (3) where v ∈ Rnh is the parameter. One event can have k sequences X0 , X1 , . . . , Xk−1 , whose re"
N19-1287,N18-1067,0,0.0369035,"Missing"
N19-1287,J12-2002,0,0.0660719,"Missing"
N19-1287,P17-2056,0,0.034055,"Missing"
N19-1287,W08-0606,0,0.142283,"Missing"
N19-1287,P16-1123,0,0.0769809,"Missing"
N19-1287,W15-1304,0,0.0160473,"studies only considered the corpora annotated with sentence-level event factuality information, such as ACE 20051 , LU (Diab et al., 2009), FactBank (Saur´ı and Pustejovsky, 2009), and UDS-IH2 (Rudinger et al., 2018). On the other hand, previous studies only con1 https://catalog.ldc.upenn.edu/LDC2006T06 2799 Proceedings of NAACL-HLT 2019, pages 2799–2809 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics sidered information within sentences, using rules (Saur´ı, 2008; Saur´ı and Pustejovsky, 2012), machine learning models (de Marneffe et al., 2012; Werner et al., 2015; Baly et al., 2018), and combinations of them (Qian et al., 2015; Stanovsky et al., 2017) for modeling. Neural network models have also recently been used for the sentence-level event factuality identification (He et al., 2017; Rudinger et al., 2018; Qian et al., 2018). According to Figure 1, document-level event factuality can not be deduced from each sentence-level factuality separately, but depends on the comprehensive semantic information of sentences. However, no suitable model for document-level task has been proposed yet. To solve the issues above, this paper focuses on document-level"
N19-1287,D17-1187,0,0.0573663,"Missing"
N19-1287,D16-1172,0,0.0158433,"8. The micro-/macro-averaged F1 of joint optimization model on English and Chinese corpus are 82.89/75.64 and 83.83/81.48, respectively. Although document-level event factuality is based on the factuality information in sentences, sentencelevel factuality value of an event only depends on the current sentence, and is likely to have a different value compared to the current document-level factuality. Therefore, the joint model can not improve the performance of document-level task. 5 Related Work Researchers have studied document-level tasks in many NLP applications, e.g., sentiment analysis (Xu et al., 2016; Dou, 2017), named entity recognition (Luo et al., 2018), and machine translation (Born et al., 2017). But related studies on event factuality are limited to the sentence-level task. Diab et al. (2009) and Prabhakaran et al. (2010) presented studies of belief annotation and tagging, and classified predicate events into Committed Belief (CB), Non-CB or Not Applicable using a supervised framework. For factuality assessment, Lee et al. (2015) employed dependency features, while Stanovsky et al. (2017) considered deep linguistic information, such as modality classes, syntactic re-ordering with Pr"
N19-1287,P15-1064,1,0.903845,"Missing"
P02-1060,M98-1015,0,0.00617687,"Missing"
P02-1060,E99-1001,0,0.0189434,"Missing"
P02-1060,1993.eamt-1.1,0,0.0634917,"Missing"
P02-1060,W00-0737,1,0.410295,"Missing"
P02-1060,W00-1309,1,0.468852,"Missing"
P02-1060,M98-1004,0,\N,Missing
P02-1060,M98-1012,0,\N,Missing
P02-1060,M98-1014,0,\N,Missing
P02-1060,M98-1021,0,\N,Missing
P02-1060,W97-0312,0,\N,Missing
P02-1060,M98-1018,0,\N,Missing
P02-1060,M98-1019,0,\N,Missing
P02-1060,M98-1020,0,\N,Missing
P02-1060,M95-1012,0,\N,Missing
P02-1060,W00-0726,0,\N,Missing
P02-1060,J95-4004,0,\N,Missing
P02-1060,M98-1009,0,\N,Missing
P03-1023,P95-1017,0,0.890131,"candidates. Mitkov’s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success. Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value. The confidence values are generally used as the competition criterion for the antecedent candidates. For example, the “Best-First” selection algorithms (Aone and Bennett, 1995; Ng and Cardie, 2002a) link the anaphor to the candidate with the maximal confidence value (above 0.5). One problem of the single-candidate model, h"
P03-1023,P99-1048,0,0.113874,"Missing"
P03-1023,P87-1022,0,0.240326,"be the antecedent of an anaphor (Mitkov, 1999). Whether a candidate is coreferential to an anaphor is often determined by the competition among all the candidates. So far, various algorithms have been proposed to determine the preference relationship between two candidates. Mitkov’s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success. Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value. The confidence values are generally used as the competition"
P03-1023,P98-2143,0,0.285496,"ce resolution is the process of linking together multiple expressions of a given entity. The key to solve this problem is to determine the antecedent for each referring expression in a document. In coreference resolution, it is common that two or more candidates compete to be the antecedent of an anaphor (Mitkov, 1999). Whether a candidate is coreferential to an anaphor is often determined by the competition among all the candidates. So far, various algorithms have been proposed to determine the preference relationship between two candidates. Mitkov’s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 200"
P03-1023,P02-1014,0,0.894344,"hod (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success. Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value. The confidence values are generally used as the competition criterion for the antecedent candidates. For example, the “Best-First” selection algorithms (Aone and Bennett, 1995; Ng and Cardie, 2002a) link the anaphor to the candidate with the maximal confidence value (above 0.5). One problem of the single-candidate model, however, is that it only takes into account the relations"
P03-1023,C02-1139,0,0.844771,"hod (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success. Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value. The confidence values are generally used as the competition criterion for the antecedent candidates. For example, the “Best-First” selection algorithms (Aone and Bennett, 1995; Ng and Cardie, 2002a) link the anaphor to the candidate with the maximal confidence value (above 0.5). One problem of the single-candidate model, however, is that it only takes into account the relations"
P03-1023,P98-2204,0,0.0157449,"n anaphor (Mitkov, 1999). Whether a candidate is coreferential to an anaphor is often determined by the competition among all the candidates. So far, various algorithms have been proposed to determine the preference relationship between two candidates. Mitkov’s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success. Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value. The confidence values are generally used as the competition criterion for"
P03-1023,M95-1005,0,0.609474,"ables). Without candidate filtering, the recall may rise as the correct antecedents would not be eliminated wrongly. Nevertheless, the precision drops largely due to the numerous invalid NPs in the candidate set. As a result, a significantly low Fmeasure is obtained in their approach. Table 4 summarizes the overall performance of different approaches to coreference resolution. Different from Table 2 and 3, here we focus on whether a coreferential chain could be correctly identified. For this purpose, we obtain the recall, the precision and the F-measure using the standard MUC scoring program (Vilain et al. 1995) for the coreference resolution task. Here the recall means the correct resolved chains over the whole coreferential chains in the data set, and precision means the correct resolved chains over the whole resolved chains. In line with the previous experiments, we see reasonable improvement in the performance of the coreference resolution: compared with the baseline approach based on the single-candidate model, the F-measure of approach increases from 69.4 to 71.3 for MUC-6, and from 58.7 to 60.2 for MUC-7. 6 Related Work A similar twin-candidate model was adopted in the anaphoric resolution sys"
P03-1023,W00-1309,1,0.849053,"Missing"
P03-1023,P02-1060,1,0.566195,"Missing"
P03-1023,J01-4003,0,\N,Missing
P03-1023,J01-4004,0,\N,Missing
P03-1023,C98-2138,0,\N,Missing
P03-1023,C98-2199,0,\N,Missing
P04-1017,P87-1022,0,0.522925,"uns usually depend on the center of attention throughout the local discourse segment (Mitkov, 1999). To determine the salience of a candidate in the local context, we may need to check the coreferential information of the candidate, such as the existence and properties of its antecedents. In fact, such information has been used for pronoun resolution in many heuristicbased systems. The S-List model (Strube, 1998), for example, assumes that a co-referring candidate is a hearer-old discourse entity and is preferred to other hearer-new candidates. In the algorithms based on the centering theory (Brennan et al., 1987; Grosz et al., 1995), if a candidate and its antecedent are the backwardlooking centers of two subsequent utterances respectively, the candidate would be the most preferred since the CONTINUE transition is always ranked higher than SHIFT or RETAIN. In this paper, we present a supervised learning-based pronoun resolution system which incorporates coreferential information of candidates in a trainable model. For each candidate, we take into consideration the properties of its antecedents in terms of features (henceforth backward features), and use the supervised learning method to explore their"
P04-1017,W98-1119,0,0.0510693,"eferential information of candidates into pronoun resolution. Preliminary experiments show that our model will boost the resolution performance given the right antecedents of the candidates. We further discuss how to apply our model in real resolution where the antecedents of the candidate are found by a separate noun phrase resolution module. The experimental results show that our model still achieves better performance than the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedent"
P04-1017,P83-1007,0,0.509301,"tter performance than the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the center of attention throughout the local discourse segment (Mitkov, 1999). To determine the salience of a candidate in the local context, we may need to check the coreferential information of the candidate, such as the existence and properties of its antecedents. In fact, such information has been used for pronoun resolution in many heuristicbased systems. The S-List model"
P04-1017,J95-2003,0,0.907422,"n the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the center of attention throughout the local discourse segment (Mitkov, 1999). To determine the salience of a candidate in the local context, we may need to check the coreferential information of the candidate, such as the existence and properties of its antecedents. In fact, such information has been used for pronoun resolution in many heuristicbased systems. The S-List model (Strube, 1998), for"
P04-1017,W03-2604,0,0.0339131,"on. Although the algorithm provided no further 0 0 refinement for DTpron , we can use DTpron , as suggested in Section 5.2, to calculate backward features and classify instances by running 0 PRON-RESOLVE(DTnon−pron , DTpron ). The results of such a system, RealResolve-4, are listed in the last line of Table 4. For both MUC6 and MUC-7, RealResolve-4 obtains exactly the same performance as RealResolve-3. 6 Related Work To our knowledge, our work is the first effort that systematically explores the influence of coreferential information of candidates on pronoun resolution in learning-based ways. Iida et al. (2003) also take into consideration the contextual clues in their coreference resolution system, by using two features to reflect the ranking order of a candidate in Salience Reference List (SRL). However, similar to common centering models, in their system the ranking of entities in SRL is also heuristic-based. The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003). However, for an entity,"
P04-1017,P98-2143,0,0.153326,"ion of candidates on pronoun resolution in learning-based ways. Iida et al. (2003) also take into consideration the contextual clues in their coreference resolution system, by using two features to reflect the ranking order of a candidate in Salience Reference List (SRL). However, similar to common centering models, in their system the ranking of entities in SRL is also heuristic-based. The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003). However, for an entity, the coreferential length only reflects its global salience in the whole text(s), instead of the local salience in a discourse segment which is nevertheless more informative for pronoun resolution. Moreover, during resolution, the found coreferential length of an entity is often incomplete, and thus the obtained length value is usually inaccurate for the salience evaluation. 7 Conclusion and Future Work In this paper we have proposed a model which incorporates coreferential information of candidates to improve pronoun resolu"
P04-1017,P02-1014,0,0.294769,"into pronoun resolution. Preliminary experiments show that our model will boost the resolution performance given the right antecedents of the candidates. We further discuss how to apply our model in real resolution where the antecedents of the candidate are found by a separate noun phrase resolution module. The experimental results show that our model still achieves better performance than the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the cent"
P04-1017,W99-0207,0,0.0143157,"tes on pronoun resolution in learning-based ways. Iida et al. (2003) also take into consideration the contextual clues in their coreference resolution system, by using two features to reflect the ranking order of a candidate in Salience Reference List (SRL). However, similar to common centering models, in their system the ranking of entities in SRL is also heuristic-based. The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003). However, for an entity, the coreferential length only reflects its global salience in the whole text(s), instead of the local salience in a discourse segment which is nevertheless more informative for pronoun resolution. Moreover, during resolution, the found coreferential length of an entity is often incomplete, and thus the obtained length value is usually inaccurate for the salience evaluation. 7 Conclusion and Future Work In this paper we have proposed a model which incorporates coreferential information of candidates to improve pronoun resolution. When evaluati"
P04-1017,J81-4001,0,0.440311,"ll achieves better performance than the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the center of attention throughout the local discourse segment (Mitkov, 1999). To determine the salience of a candidate in the local context, we may need to check the coreferential information of the candidate, such as the existence and properties of its antecedents. In fact, such information has been used for pronoun resolution in many heuristicbased syste"
P04-1017,J01-4004,0,0.860656,"ation of candidates into pronoun resolution. Preliminary experiments show that our model will boost the resolution performance given the right antecedents of the candidates. We further discuss how to apply our model in real resolution where the antecedents of the candidate are found by a separate noun phrase resolution module. The experimental results show that our model still achieves better performance than the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usual"
P04-1017,P03-1022,0,0.175942,"ion. Preliminary experiments show that our model will boost the resolution performance given the right antecedents of the candidates. We further discuss how to apply our model in real resolution where the antecedents of the candidate are found by a separate noun phrase resolution module. The experimental results show that our model still achieves better performance than the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the center of attention throughou"
P04-1017,P98-2204,0,0.294892,"Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the center of attention throughout the local discourse segment (Mitkov, 1999). To determine the salience of a candidate in the local context, we may need to check the coreferential information of the candidate, such as the existence and properties of its antecedents. In fact, such information has been used for pronoun resolution in many heuristicbased systems. The S-List model (Strube, 1998), for example, assumes that a co-referring candidate is a hearer-old discourse entity and is preferred to other hearer-new candidates. In the algorithms based on the centering theory (Brennan et al., 1987; Grosz et al., 1995), if a candidate and its antecedent are the backwardlooking centers of two subsequent utterances respectively, the candidate would be the most preferred since the CONTINUE transition is always ranked higher than SHIFT or RETAIN. In this paper, we present a supervised learning-based pronoun resolution system which incorporates coreferential information of candidates in a tr"
P04-1017,J01-4003,0,0.132668,"troduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the center of attention throughout the local discourse segment (Mitkov, 1999). To determine the salience of a candidate in the local context, we may need to check the coreferential information of the candidate, such as the existence and properties of its antecedents. In fact, such information has been used for pronoun resolution in many heuristicbased systems. The S-List model (Strube, 1998), for example, assumes"
P04-1017,M95-1005,0,0.0794761,"Missing"
P04-1017,P03-1023,1,0.735862,"nts show that our model will boost the resolution performance given the right antecedents of the candidates. We further discuss how to apply our model in real resolution where the antecedents of the candidate are found by a separate noun phrase resolution module. The experimental results show that our model still achieves better performance than the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the center of attention throughout the local discours"
P04-1017,C98-2138,0,\N,Missing
P04-1017,C98-2199,0,\N,Missing
P04-1075,W02-0301,0,0.00997657,"he named entity recognizer. The contributions not only come from the above measures, but also the two sample selection strategies which effectively incorporate informativeness, representativeness and diversity criteria. To our knowledge, it is the first work on considering the three criteria all together for active learning. Furthermore, such measures and strategies can be easily adapted to other active learning tasks as well. 2 Multi-criteria for NER Active Learning Support Vector Machines (SVM) is a powerful machine learning method, which has been applied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al. 2003). In this paper, we apply active learning methods to a simple and effective SVM model to recognize one class of names at a time, such as protein names, person names, etc. In NER, SVM is to classify a word into positive class “1”indicating that the word is a part of an entity, or negative class “-1” indicating that the word is not a part of an entity. Each word in SVM is represented as a high-dimensional feature vector including surface word information, orthographic features, POS feature and semantic trigger features (Shen et al. 2003). The semantic trigger features consist o"
P04-1075,W03-1305,0,0.0392232,"gnizer. The contributions not only come from the above measures, but also the two sample selection strategies which effectively incorporate informativeness, representativeness and diversity criteria. To our knowledge, it is the first work on considering the three criteria all together for active learning. Furthermore, such measures and strategies can be easily adapted to other active learning tasks as well. 2 Multi-criteria for NER Active Learning Support Vector Machines (SVM) is a powerful machine learning method, which has been applied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al. 2003). In this paper, we apply active learning methods to a simple and effective SVM model to recognize one class of names at a time, such as protein names, person names, etc. In NER, SVM is to classify a word into positive class “1”indicating that the word is a part of an entity, or negative class “-1” indicating that the word is not a part of an entity. Each word in SVM is represented as a high-dimensional feature vector including surface word information, orthographic features, POS feature and semantic trigger features (Shen et al. 2003). The semantic trigger features consist of some special hea"
P04-1075,P00-1016,0,0.432559,"ted corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample sele ction) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classif ication (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that 1 a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labe ling and add the labeled example to training set to retrain model. This procedure is repeated until the model achieves a certain level of performance. Practically, a batch of examples are selected at a time, called batchedbased sample sele ction (Lewis and C"
P04-1075,W03-1307,1,0.3891,"ied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al. 2003). In this paper, we apply active learning methods to a simple and effective SVM model to recognize one class of names at a time, such as protein names, person names, etc. In NER, SVM is to classify a word into positive class “1”indicating that the word is a part of an entity, or negative class “-1” indicating that the word is not a part of an entity. Each word in SVM is represented as a high-dimensional feature vector including surface word information, orthographic features, POS feature and semantic trigger features (Shen et al. 2003). The semantic trigger features consist of some special head nouns for an entity class which is supplied by users. Furthermore, a window (size = 7), which represents the local context of the target word w, is also used to classify w. However, for active learning in NER, it is not reasonable to select a single word without context for human to label. Even if we require human to label a single word, he has to make an addition effort to refer to the context of the word. In our active learning process, we select a word sequence which consists of a machine-annotated named entity and its context rat"
P04-1075,N03-1031,0,0.0302235,"models are generally trained on large annotated corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample sele ction) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classif ication (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that 1 a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labe ling and add the labeled example to training set to retrain model. This procedure is repeated until the model achieves a certain level of performance. Practically, a batch of examples are selected at a time, cal"
P04-1075,P02-1016,0,0.784918,"processing (NLP), models are generally trained on large annotated corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample sele ction) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classif ication (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that 1 a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labe ling and add the labeled example to training set to retrain model. This procedure is repeated until the model achieves a certain level of performance. Practically, a batch of examples are"
P04-1075,W02-2029,0,\N,Missing
P04-1075,W00-1306,0,\N,Missing
P05-1053,P04-1054,0,0.650545,"Missing"
P05-1053,A00-2030,0,0.0346955,"tion on the 5 ACE relation types. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 and Section 4 describe our approach and various features employed respectively. Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. 2 Related Work The relation extraction task was formulated at the 7th Message Understanding Conference (MUC-7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived fr"
P05-1053,C02-1151,0,\N,Missing
P05-1053,J03-4003,0,\N,Missing
P06-1016,P04-1054,0,0.168458,"he previous best-reported system. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome the data sparseness problem, generative models typic"
P06-1016,A00-2030,0,0.0727802,"rchical strategy outperforms the previous best-reported system. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome t"
P06-1016,I05-1034,1,0.93572,"tion 2 presents related work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome the data sparseness problem, generative models typically applied some smoothing techniques to integrate different scales of cont"
P06-1016,P05-1052,0,0.412762,"ve steady performance given the current corpus size. Given the relative large size of this corpus, it will be time-consuming and very expensive to further expand the corpus with a reasonable gain in performance. Even if we can somehow expend the corpus and achieve steady performance on major relation subtypes, it will be still far beyond practice for those minor subtypes given the much unevenly distribution among different relation subtypes. While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations. This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem by modeling the commonality among related classes. Through organizing various classes hierarchically, a linear discriminative function is determined for each class in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-lev"
P06-1016,P02-1060,1,0.825946,"Missing"
P06-1016,P05-1053,1,0.864337,"Missing"
P06-1016,C02-1151,0,\N,Missing
P06-1016,J03-4003,0,\N,Missing
P06-1016,P04-1053,0,\N,Missing
P06-1016,H05-1091,0,\N,Missing
P06-1104,P01-1017,0,0.0536781,"Missing"
P06-1104,A00-2030,0,\N,Missing
P06-1104,P05-1053,1,\N,Missing
P06-1104,P04-1043,0,\N,Missing
P06-1104,P04-1054,0,\N,Missing
P06-1104,P03-1005,0,\N,Missing
P06-1104,H05-1091,0,\N,Missing
P06-1104,P05-1052,0,\N,Missing
P07-1026,P98-1013,0,0.0102809,"nvolution tree kernel on the data set of the CoNLL-2005 SRL shared task. The remainder of the paper is organized as follows: Section 2 reviews the previous work and Section 3 discusses our grammar-driven convolution tree kernel. Section 4 shows the experimental results. We conclude our work in Section 5. 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treeba"
P07-1026,W05-0620,0,0.371187,"classification are regarded as two key steps in semantic role labeling. Semantic role identification involves classifying each syntactic element in a sentence into either a semantic argument or a non-argument while semantic role classification involves classifying each semantic argument identified into a specific semantic role. This paper focuses on semantic role classification task with the assumption that the semantic arguments have been identified correctly. Both feature-based and kernel-based learning methods have been studied for semantic role classification (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). In feature-based methods, a flat feature vector is used to represent a predicateargument structure while, in kernel-based methods, a kernel function is used to measure directly the similarity between two predicate-argument structures. As we know, kernel methods are more effective in capturing structured features. Moschitti (2004) and Che et al. (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. The convolution tree kernel takes sub-tree as its feature and counts the number of common sub-trees as the similarity between two predicate-arguments. Th"
P07-1026,A00-2018,0,0.0600695,"Missing"
P07-1026,J05-1004,0,0.0606769,"Missing"
P07-1026,W04-3212,0,0.110658,"ork and Section 3 discusses our grammar-driven convolution tree kernel. Section 4 shows the experimental results. We conclude our work in Section 5. 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treebank/ for the detailed definitions of the grammar tags used in the paper. 2 Some rewrite rules in English grammar are generalizations of others: for example, “N"
P07-1026,P06-1104,1,0.842473,"hods for SRL: as an alternative, kernel methods are more effective in modeling structured objects. This is because a kernel can measure the similarity between two structured objects using the original representation of the objects instead of explicitly enumerating their features. Many kernels have been proposed and applied to the NLP study. In particular, Haussler (1999) proposed the well-known convolution kernels for a discrete structure. In the context of it, more and more kernels for restricted syntaxes or specific domains (Collins and Duffy, 2001; Lodhi et al., 2002; Zelenko et al., 2003; Zhang et al., 2006) are proposed and explored in the NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel. He selected portions of syntactic parse trees as predicateargument feature spaces, which include salient substructures of predicate-arguments, to define convolution kernels for the task of semantic role classification. Under the same framework, Che et al. (2006) proposed a hybrid convolution tree kernel, which consists of two individual convolution kernels: a Path kernel and a Constituent Structure kern"
P07-1026,W04-3211,0,\N,Missing
P07-1026,J93-2004,0,\N,Missing
P07-1026,N06-2025,0,\N,Missing
P07-1026,W03-1012,0,\N,Missing
P07-1026,C04-1197,0,\N,Missing
P07-1026,P05-1072,0,\N,Missing
P07-1026,C98-1013,0,\N,Missing
P07-1026,P04-1043,0,\N,Missing
P07-1026,J02-3001,0,\N,Missing
P07-1026,P04-1016,0,\N,Missing
P07-1026,P06-2010,1,\N,Missing
P07-1026,W04-2412,0,\N,Missing
P09-1007,J90-2002,0,0.510764,"Missing"
P09-1007,D08-1092,0,0.0385535,"from CTL, City University of Hong Kong. 1 It is a tradition to call an annotated syntactic corpus as treebank in parsing community. 55 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 55–63, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP target language can effectively make use of them by only considering the most related information extracted from the translated text. The basic idea to support this work is to make use of the semantic connection between different languages. In this sense, it is related to the work of (Merlo et al., 2002) and (Burkett and Klein, 2008). The former showed that complementary information about English verbs can be extracted from their translations in a second language (Chinese) and the use of multilingual features improves classification performance of the English verbs. The latter iteratively trained a model to maximize the marginal likelihood of tree pairs, with alignments treated as latent variables, and then jointly parsing bilingual sentences in a translation pair. The proposed parser using features from monolingual and mutual constraints helped its log-linear model to achieve better performance for both monolingual parse"
P09-1007,I08-1012,0,0.573078,"n draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treeb"
P09-1007,D07-1098,0,0.0104809,"ting overlapped features as our work in (Zhao and Kit, 2008), especially, those character-level ones for Chinese parsing. Our implementation of maximum entropy adopts L-BFGS algorithm for parameter optimization as usual. 4.2 Parsing using a Beam Search Algorithm In Table 2, the feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. These are a group of Markovian features. Without this type of features, a shift-reduce parser may directly scan through an input sequence in linear time. Otherwise, following the work of (Duan et al., 2007) and (Zhao, 2009), the parsing algorithm is to search a parsing action sequence with the maximal probability. Sdi = argmax Y p(di |di−1 di−2 ...), i where Sdi is the object parsing action sequence, p(di |di−1 ...) is the conditional probability, and di 58 Figure 1: A comparison before and after translation is i-th parsing action. We use a beam search algorithm to find the object parsing action sequence. Table 2: Features for Parsing 5 Exploiting the Translated Treebank in .f orm, n = 0, 1 i.f orm + i1 .f orm in .char2 + in+1 .char2 , n = −1, 0 i.char−1 + i1 .char−1 in .char−2 n = 0, 3 i1 .char"
P09-1007,P06-1072,0,0.011487,"nsufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treebank, while all the previous works focus on the unlabeled data. Although cross-language technique has been used in other natural"
P09-1007,P08-1068,0,0.00863736,"ble. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treebank, while all the previous works focus on the unlabeled data. Although cross-language technique has been used in other natural language processing tasks, it is basically new"
P09-1007,E03-1008,0,0.0270136,"a resources to enhance an existing parser, it is related to domain adaption for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are"
P09-1007,P06-1043,0,0.0262404,"an existing parser, it is related to domain adaption for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our metho"
P09-1007,P08-1061,0,0.0118388,"scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treebank, while all the previous works focus on the unlabeled data. Although cross-language technique has been used in other natural language processing tasks,"
P09-1007,D07-1013,0,0.0336862,"om the source language to the target one. In detail, a word-based decoding is used, which adopts a loglinear framework as in (Och and Ney, 2002) with only two features, translation model and language model, P exp[ 2i=1 λi hi (c, e)] P (c|e) = P P2 c exp[ i=1 λi hi (c, e)] Where h1 (c, e) = log(pγ (c|e)) is the translation model, which is converted from the bilingual lexicon, and h2 (c, e) = log(pθ (c)) 4 Dependency Parsing: Baseline 4.1 Learning Model and Features is the language model, a word trigram model trained from the CTB. In our experiment, we set two weights λ1 = λ2 = 1. According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based. 2 StarDict is an open source dictionary software, available at http://stardict.sourceforge.net/. 57 With notations defined in Table 1, a feature set as shown in Table 2 is adopted. Here, we explain some terms in Tables 1 and 2. We used a large scale feature selection approach as in (Zhao et al., 2009) to obtain the feature set in Table 2. Some feature notations in this paper are also borrowed from that work. The feature curroot returns the root of"
P09-1007,W05-1516,0,0.0299467,"Missing"
P09-1007,E06-1011,0,0.0426299,"Missing"
P09-1007,W03-3023,0,0.385266,"Missing"
P09-1007,P05-1012,0,0.211703,"d-by-word decoding, where not a parallel corpus but a bilingual lexicon is necessary, is adopted for the treebank translation. Using an ensemble method, the key information extracted from word pairs with dependency relations in the translated text is effectively integrated into the parser for the target language. The proposed method is evaluated in English and Chinese treebanks. It is shown that a translated English treebank helps a Chinese parser obtain a state-ofthe-art result. 1 Introduction Although supervised learning methods bring stateof-the-art outcome for dependency parser inferring (McDonald et al., 2005; Hall et al., 2007), a large enough data set is often required for specific parsing accuracy according to this type of methods. However, to annotate syntactic structure, either phrase- or dependency-based, is a costly job. Until now, the largest treebanks1 in various languages for syntax learning are with around one million words (or some other similar units). Limited data stand in the way of further performance enhancement. This is the case for each individual language at least. But, this is not the case as we observe all treebanks in different languages as a whole. For example, of ten treeb"
P09-1007,C08-1132,0,0.217068,"ted in (Wang et al., 2007). The experimental results in (McDonald and Nivre, 2007) show a negative impact on the parsing accuracy from too long dependency relation. For the proposed method, the improvement relative to dependency length is shown in Figure 2. From the figure, it is seen that our method gives observable better performance when dependency lengths are larger than 4. Although word order is changed, the results here show that the useful information from the translated treebank still help those long distance dependencies. 4 There is a slight exception: using the same data splitting, (Yu et al., 2008) reported UAS without p as 0.873 versus ours, 0.870. 61 chosen to generate some additional features to enhance the parser for the target language. The experimental results in English and Chinese treebanks show the proposed method is effective and helps the Chinese parser in this work achieve a state-of-the-art result. Note that our method is evaluated in two treebanks with a similar annotation style and it avoids using too many linguistic properties. Thus the method is in the hope of being used in other similarly annotated treebanks 5 . For an immediate example, we may adopt a translated Chine"
P09-1007,P02-1027,0,0.011287,"by a research fellowship from CTL, City University of Hong Kong. 1 It is a tradition to call an annotated syntactic corpus as treebank in parsing community. 55 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 55–63, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP target language can effectively make use of them by only considering the most related information extracted from the translated text. The basic idea to support this work is to make use of the semantic connection between different languages. In this sense, it is related to the work of (Merlo et al., 2002) and (Burkett and Klein, 2008). The former showed that complementary information about English verbs can be extracted from their translations in a second language (Chinese) and the use of multilingual features improves classification performance of the English verbs. The latter iteratively trained a model to maximize the marginal likelihood of tree pairs, with alignments treated as latent variables, and then jointly parsing bilingual sentences in a translation pair. The proposed parser using features from monolingual and mutual constraints helped its log-linear model to achieve better performa"
P09-1007,I08-3008,0,0.108101,"g bilingual sentences in a translation pair. The proposed parser using features from monolingual and mutual constraints helped its log-linear model to achieve better performance for both monolingual parsers and machine translation system. In this work, cross-language features will be also adopted as the latter work. However, although it is not essentially different, we only focus on dependency parsing itself, while the parsing scheme in (Burkett and Klein, 2008) based on a constituent representation. Among of existing works that we are aware of, we regard that the most similar one to ours is (Zeman and Resnik, 2008), who adapted a parser to a new language that is much poorer in linguistic resources than the source language. However, there are two main differences between their work and ours. The first is that they considered a pair of sufficiently related languages, Danish and Swedish, and made full use of the similar characteristics of two languages. Here we consider two quite different languages, English and Chinese. As fewer language properties are concerned, our approach holds the more possibility to be extended to other language pairs than theirs. The second is that a parallel corpus is required for"
P09-1007,W08-2127,1,0.799644,"g actions, a shift action and a reduce action are also defined to maintain the stack and the unprocessed sequence. In this work, we adopt a left-to-right arc-eager parsing model, that means that the parser scans the input sequence from left to right and right dependents are attached to their heads as soon as possible (Hall et al., 2007). While memory-based and margin-based learning approaches such as support vector machines are popularly applied to shift-reduce parsing, we apply maximum entropy model as the learning model for efficient training and adopting overlapped features as our work in (Zhao and Kit, 2008), especially, those character-level ones for Chinese parsing. Our implementation of maximum entropy adopts L-BFGS algorithm for parameter optimization as usual. 4.2 Parsing using a Beam Search Algorithm In Table 2, the feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. These are a group of Markovian features. Without this type of features, a shift-reduce parser may directly scan through an input sequence in linear time. Otherwise, following the work of (Duan et al., 2007) and (Zhao, 2009), the parsing algorithm i"
P09-1007,W03-3017,0,0.04529,"he first letter of POS tag of word coarse POS: the first two POS tags of word the left nearest verb The first character of a word The first two characters of a word The last character of a word The last two characters of a word ’s, i.e., ‘s.dprel’ means dependent label of character in the top of stack Feature combination, i.e., ‘s.char+i.char’ means both s.char and i.char work as a feature function. Although the former will be also used as comparison, the latter is chosen as the main parsing framework by this study for the sake of efficiency. In detail, a shift-reduce method is adopted as in (Nivre, 2003), where a classifier is used to make a parsing decision step by step. In each step, the classifier checks a word pair, namely, s, the top of a stack that consists of the processed words, and, i, the first word in the (input) unprocessed sequence, to determine if a dependent relation should be established between them. Besides two dependency arc building actions, a shift action and a reduce action are also defined to maintain the stack and the unprocessed sequence. In this work, we adopt a left-to-right arc-eager parsing model, that means that the parser scans the input sequence from left to ri"
P09-1007,W09-1208,1,0.28587,"nd Features is the language model, a word trigram model trained from the CTB. In our experiment, we set two weights λ1 = λ2 = 1. According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based. 2 StarDict is an open source dictionary software, available at http://stardict.sourceforge.net/. 57 With notations defined in Table 1, a feature set as shown in Table 2 is adopted. Here, we explain some terms in Tables 1 and 2. We used a large scale feature selection approach as in (Zhao et al., 2009) to obtain the feature set in Table 2. Some feature notations in this paper are also borrowed from that work. The feature curroot returns the root of a partial parsing tree that includes a specified node. The feature charseq returns a character sequence whose members are collected from all identified children for a specified word. In Table 2, as for concatenating multiple substrings into a feature string, there are two ways, seq and bag. The former is to concatenate all substrings without do something special. The latter will remove all duplicated substrings, sort the rest and concatenate all"
P09-1007,P02-1038,0,0.0152292,"y reached in fact, as the following case is frequently encountered, multiple English words have to be translated into one Chinese word. To solve this problem, we use a policy that lets the output Chinese word only inherits the attached information of the highest syntactic head in the original multiple English words. 3.2 Translation A word-by-word statistical machine translation strategy is adopted to translate words attached with the respective dependency information from the source language to the target one. In detail, a word-based decoding is used, which adopts a loglinear framework as in (Och and Ney, 2002) with only two features, translation model and language model, P exp[ 2i=1 λi hi (c, e)] P (c|e) = P P2 c exp[ i=1 λi hi (c, e)] Where h1 (c, e) = log(pγ (c|e)) is the translation model, which is converted from the bilingual lexicon, and h2 (c, e) = log(pθ (c)) 4 Dependency Parsing: Baseline 4.1 Learning Model and Features is the language model, a word trigram model trained from the CTB. In our experiment, we set two weights λ1 = λ2 = 1. According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either grap"
P09-1007,P07-1078,0,0.0121418,"is related to domain adaption for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation metho"
P09-1007,E09-1100,1,0.164926,"as our work in (Zhao and Kit, 2008), especially, those character-level ones for Chinese parsing. Our implementation of maximum entropy adopts L-BFGS algorithm for parameter optimization as usual. 4.2 Parsing using a Beam Search Algorithm In Table 2, the feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. These are a group of Markovian features. Without this type of features, a shift-reduce parser may directly scan through an input sequence in linear time. Otherwise, following the work of (Duan et al., 2007) and (Zhao, 2009), the parsing algorithm is to search a parsing action sequence with the maximal probability. Sdi = argmax Y p(di |di−1 di−2 ...), i where Sdi is the object parsing action sequence, p(di |di−1 ...) is the conditional probability, and di 58 Figure 1: A comparison before and after translation is i-th parsing action. We use a beam search algorithm to find the object parsing action sequence. Table 2: Features for Parsing 5 Exploiting the Translated Treebank in .f orm, n = 0, 1 i.f orm + i1 .f orm in .char2 + in+1 .char2 , n = −1, 0 i.char−1 + i1 .char−1 in .char−2 n = 0, 3 i1 .char−2 + i2 .char−2 +"
P09-1007,D07-1111,0,0.00895734,"for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle g"
P09-1007,D07-1097,0,\N,Missing
P09-1007,D07-1096,0,\N,Missing
P10-1043,P07-1056,0,0.914534,"rmally domain-independent and serves as highly relevant clues to sentiment classification. The latter type of statement called impersonal view, e.g. ‘it is too small’, contains Y ’s “objective” (i.e. or at least criteria-based) evaluation of the target object. This kind of information tends to contain much domain-specific classification knowledge. Although such information is sometimes not as explicit as personal views in classifying the sentiment of a text, speaker’s sentiment is usually implied by the evaluation result. It is well-known that sentiment classification is very domain-specific (Blitzer et al., 2007), so it is critical to eliminate its dependence on a large-scale labeled data for its wide applications. Since the unlabeled data is ample and easy to collect, a successful semi-supervised sentiment classification system would significantly minimize the involvement of labor and time. Therefore, given the two different views mentioned above, one promising application is to adopt them in co-training algorithms, which has been proven to be an effective semi-supervised learning strategy of incorporating unlabeled data to further improve the classification performance (Zhu, 2005). In addition, we w"
P10-1043,P09-1027,0,0.158629,"7) present a domain adaptation approach for sentiment classification. Semi-supervised methods combine unlabeled data with labeled training data (often small-scaled) to improve the models. Compared to the supervised and unsupervised methods, semi-supervised methods for sentiment classification are relatively new and have much less related studies. Dasgupta and Ng (2009) integrate various methods in semi-supervised sentiment classification including spectral clustering, active learning, transductive learning, and ensemble learning. They achieve a very impressive improvement across five domains. Wan (2009) applies a co-training method to semi-supervised learning with labeled English corpus and unlabeled Chinese corpus for Chinese sentiment classification. 3 Unsupervised Mining of Personal and Impersonal Views As mentioned in Section 1, the objective of sentiment classification is to classify a specific binary relation: X ’s evaluation on Y, where X is an object set including different kinds of persons and Y is another object set including the target objects to be evaluated. First of all, we focus on an analysis on sentences in product reviews regarding the two views: personal and impersonal vie"
P10-1043,J09-3003,0,0.0225,"Section 3 presents our unsupervised approach for mining personal and impersonal views. Section 4 and Section 5 propose our supervised and semi-supervised methods on sentiment classification respectively. Experimental results are presented and analyzed in Section 6. Section 7 discusses on the differences between personal/impersonal and subjective/objective. Finally, Section 8 draws our conclusions and outlines the future work. 2 Related Work Recently, a variety of studies have been reported on sentiment classification at different levels: word level (Esuli and Sebastiani, 2005), phrase level (Wilson et al., 2009), sentence level (Kim and Hovy, 2004; Liu et al., 2005), and document level (Turney, 2002; Pang et al., 2002). This paper focuses on the document-level sentiment classification. Generally, document-level sentiment classification methods can be categorized into three types: unsupervised, supervised, and semi-supervised. Unsupervised methods involve deriving a sentiment classifier without any labeled documents. Most of previous work use a set of labeled sentiment words called seed words to perform unsupervised classification. Turney (2002) determines the sentiment orientation of a document by ca"
P10-1043,C08-1135,0,0.0450621,"on methods can be categorized into three types: unsupervised, supervised, and semi-supervised. Unsupervised methods involve deriving a sentiment classifier without any labeled documents. Most of previous work use a set of labeled sentiment words called seed words to perform unsupervised classification. Turney (2002) determines the sentiment orientation of a document by calculating point-wise mutual information between the words in the document and the seed words of ‘excellent’ and ‘poor’. Kennedy and Inkpen (2006) use a term-counting method with a set of seed words to determine the sentiment. Zagibalov and Carroll (2008) first propose a seed word selection approach and then apply the same term-counting method for Chinese sentiment classifications. These unsupervised approaches are believed to be domain-independent for sentiment classification. Supervised methods consider sentiment classification as a standard classification problem in which labeled data in a domain are used to train a domain-specific classifier. Pang et al. (2002) are the first to apply supervised machine learning methods to sentiment classification. Subsequently, many other studies make efforts to improve the performance of machine learning-"
P10-1043,C04-1200,0,0.118119,"pproach for mining personal and impersonal views. Section 4 and Section 5 propose our supervised and semi-supervised methods on sentiment classification respectively. Experimental results are presented and analyzed in Section 6. Section 7 discusses on the differences between personal/impersonal and subjective/objective. Finally, Section 8 draws our conclusions and outlines the future work. 2 Related Work Recently, a variety of studies have been reported on sentiment classification at different levels: word level (Esuli and Sebastiani, 2005), phrase level (Wilson et al., 2009), sentence level (Kim and Hovy, 2004; Liu et al., 2005), and document level (Turney, 2002; Pang et al., 2002). This paper focuses on the document-level sentiment classification. Generally, document-level sentiment classification methods can be categorized into three types: unsupervised, supervised, and semi-supervised. Unsupervised methods involve deriving a sentiment classifier without any labeled documents. Most of previous work use a set of labeled sentiment words called seed words to perform unsupervised classification. Turney (2002) determines the sentiment orientation of a document by calculating point-wise mutual informat"
P10-1043,P07-1055,0,0.0367316,"nt classification. Supervised methods consider sentiment classification as a standard classification problem in which labeled data in a domain are used to train a domain-specific classifier. Pang et al. (2002) are the first to apply supervised machine learning methods to sentiment classification. Subsequently, many other studies make efforts to improve the performance of machine learning-based classifiers by various means, such as using subjectivity summarization (Pang and Lee, 2004), seeking new superior textual features (Riloff et al., 2006), and employing document subcomponent information (McDonald et al., 2007). As far as the challenge of domain-dependency is concerned, Blitzer et al. (2007) present a domain adaptation approach for sentiment classification. Semi-supervised methods combine unlabeled data with labeled training data (often small-scaled) to improve the models. Compared to the supervised and unsupervised methods, semi-supervised methods for sentiment classification are relatively new and have much less related studies. Dasgupta and Ng (2009) integrate various methods in semi-supervised sentiment classification including spectral clustering, active learning, transductive learning, and ens"
P10-1043,W02-1011,0,0.0239994,"statements towards a target object for evaluation. To obtain them, an unsupervised mining approach is proposed. On this basis, an ensemble method and a co-training algorithm are explored to employ the two views in supervised and semi-supervised sentiment classification respectively. Experimental results across eight domains demonstrate the effectiveness of our proposed approach. 1 Introduction As a special task of text classification, sentiment classification aims to classify a text according to the expressed sentimental polarities of opinions such as ‘thumb up’ or ‘thumb down’ on the movies (Pang et al., 2002). This task has recently received considerable interests in the Natural Language Processing (NLP) community due to its wide applications. In general, the objective of sentiment classification can be represented as a kind of binary relation R, defined as an ordered triple (X, Y, G), where X is an object set including different kinds of people (e.g. writers, reviewers, or users), Y is another object set including the target objects (e.g. products, events, or even some people), and G is a subset of the Cartesian product X × Y . The concerned relation in sentiment classification is X ’s evaluation"
P10-1043,W06-1652,0,0.105249,"supervised approaches are believed to be domain-independent for sentiment classification. Supervised methods consider sentiment classification as a standard classification problem in which labeled data in a domain are used to train a domain-specific classifier. Pang et al. (2002) are the first to apply supervised machine learning methods to sentiment classification. Subsequently, many other studies make efforts to improve the performance of machine learning-based classifiers by various means, such as using subjectivity summarization (Pang and Lee, 2004), seeking new superior textual features (Riloff et al., 2006), and employing document subcomponent information (McDonald et al., 2007). As far as the challenge of domain-dependency is concerned, Blitzer et al. (2007) present a domain adaptation approach for sentiment classification. Semi-supervised methods combine unlabeled data with labeled training data (often small-scaled) to improve the models. Compared to the supervised and unsupervised methods, semi-supervised methods for sentiment classification are relatively new and have much less related studies. Dasgupta and Ng (2009) integrate various methods in semi-supervised sentiment classification inclu"
P10-1043,P02-1053,0,0.0110004,"and Section 5 propose our supervised and semi-supervised methods on sentiment classification respectively. Experimental results are presented and analyzed in Section 6. Section 7 discusses on the differences between personal/impersonal and subjective/objective. Finally, Section 8 draws our conclusions and outlines the future work. 2 Related Work Recently, a variety of studies have been reported on sentiment classification at different levels: word level (Esuli and Sebastiani, 2005), phrase level (Wilson et al., 2009), sentence level (Kim and Hovy, 2004; Liu et al., 2005), and document level (Turney, 2002; Pang et al., 2002). This paper focuses on the document-level sentiment classification. Generally, document-level sentiment classification methods can be categorized into three types: unsupervised, supervised, and semi-supervised. Unsupervised methods involve deriving a sentiment classifier without any labeled documents. Most of previous work use a set of labeled sentiment words called seed words to perform unsupervised classification. Turney (2002) determines the sentiment orientation of a document by calculating point-wise mutual information between the words in the document and the seed wo"
P10-1043,P04-1035,0,\N,Missing
P10-1043,P09-1079,0,\N,Missing
P10-1113,P98-1013,0,0.00848289,"arguments (roles) of the predicate. In both English and Chinese PropBank (Palmer et al., 2005; Xue and Palmer, 2003), and English and Chinese NomBank (Meyers et al., 2004; Xue, 2006), these semantic arguments include core arguments (e.g., Arg0 for agent and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both feature-based and kernel-based methods, have been extensively studied for SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Pradhan et al., 2005; Liu and Ng, 2007). Nevertheless, for both verbal and nominal SRL, state-of-the-art systems depend heavily on the top-best parse tree and there exists a large performance gap between SRL based on the gold parse tree and the top-best parse tree. For example, Pradhan et al. (2005) suffered a performance drop of 7.3 in F1-measure on English PropBank when using the top-b"
P10-1113,W04-2412,0,0.0830406,"ue, 2006), these semantic arguments include core arguments (e.g., Arg0 for agent and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both feature-based and kernel-based methods, have been extensively studied for SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Pradhan et al., 2005; Liu and Ng, 2007). Nevertheless, for both verbal and nominal SRL, state-of-the-art systems depend heavily on the top-best parse tree and there exists a large performance gap between SRL based on the gold parse tree and the top-best parse tree. For example, Pradhan et al. (2005) suffered a performance drop of 7.3 in F1-measure on English PropBank when using the top-best parse tree returned from Charniak’s parser (Charniak, 2001). Liu and Ng (2007) reported a performance drop of 4.21 in F1-measure on English NomBank. Compared with English SRL,"
P10-1113,W05-0620,0,0.203606,"guments include core arguments (e.g., Arg0 for agent and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both feature-based and kernel-based methods, have been extensively studied for SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Pradhan et al., 2005; Liu and Ng, 2007). Nevertheless, for both verbal and nominal SRL, state-of-the-art systems depend heavily on the top-best parse tree and there exists a large performance gap between SRL based on the gold parse tree and the top-best parse tree. For example, Pradhan et al. (2005) suffered a performance drop of 7.3 in F1-measure on English PropBank when using the top-best parse tree returned from Charniak’s parser (Charniak, 2001). Liu and Ng (2007) reported a performance drop of 4.21 in F1-measure on English NomBank. Compared with English SRL, Chinese SRL suffers more ser"
P10-1113,J05-1004,0,0.0735968,"s importance in natural language processing (NLP) applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Kong et al., 2009). Given a sentence Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive, Singapore 117417 nght@comp.nus.edu.sg and a predicate (either a verb or a noun) in the sentence, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) of the predicate. In both English and Chinese PropBank (Palmer et al., 2005; Xue and Palmer, 2003), and English and Chinese NomBank (Meyers et al., 2004; Xue, 2006), these semantic arguments include core arguments (e.g., Arg0 for agent and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both"
P10-1113,P01-1017,0,0.30443,"techniques, including both feature-based and kernel-based methods, have been extensively studied for SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Pradhan et al., 2005; Liu and Ng, 2007). Nevertheless, for both verbal and nominal SRL, state-of-the-art systems depend heavily on the top-best parse tree and there exists a large performance gap between SRL based on the gold parse tree and the top-best parse tree. For example, Pradhan et al. (2005) suffered a performance drop of 7.3 in F1-measure on English PropBank when using the top-best parse tree returned from Charniak’s parser (Charniak, 2001). Liu and Ng (2007) reported a performance drop of 4.21 in F1-measure on English NomBank. Compared with English SRL, Chinese SRL suffers more seriously from syntactic parsing. Xue (2008) evaluated on Chinese PropBank and showed that the performance of Chinese verbal SRL drops by about 25 in F1-measure when replacing gold parse trees with automatic ones. Likewise, Xue (2008) and Li et al. (2009) reported a performance drop of about 12 in F1-measure in Chinese NomBank SRL. 1108 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1108–1117, c Uppsala, Sw"
P10-1113,N09-1037,0,0.023537,"eir method was biased against these roles in general, thus lowering recall for them (e.g., precision of 87.6 and recall of 65.8). There have been other efforts in NLP on joint learning with various degrees of success. In particular, the recent shared tasks of CoNLL 2008 and 2009 (Surdeanu et al., 2008; Hajic et al., 2009) tackled joint parsing of syntactic and semantic dependencies. However, all the top 5 reported systems decoupled the tasks, rather than building joint models. Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al. (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. In addition, attempts on joint Chinese word segmentation and part-of-speech (POS) tagging (Ng and Low, 2004; Zhang and Clark, 2008) also illustrate the benefits of joint learning. 1109 TOP IP VP Arg1/Rel2 Arg0/Rel1 VP Arg0/Rel2 Arg2/Rel1 NP NN 中国 Chinese 政府 govt. P 向 to Arg1/Rel1 Sup/Rel2 Rel1 PP NR PU NP NR NP VV NN 朝鲜 政府 N. Korean govt. 。 . ArgM-MNR/Rel2 Rel2 NN NN 提供 provide 人民币 RMB Chinese government provides RMB loan to North Korean govern"
P10-1113,D09-1103,1,0.85366,"top-best parse tree. For example, Pradhan et al. (2005) suffered a performance drop of 7.3 in F1-measure on English PropBank when using the top-best parse tree returned from Charniak’s parser (Charniak, 2001). Liu and Ng (2007) reported a performance drop of 4.21 in F1-measure on English NomBank. Compared with English SRL, Chinese SRL suffers more seriously from syntactic parsing. Xue (2008) evaluated on Chinese PropBank and showed that the performance of Chinese verbal SRL drops by about 25 in F1-measure when replacing gold parse trees with automatic ones. Likewise, Xue (2008) and Li et al. (2009) reported a performance drop of about 12 in F1-measure in Chinese NomBank SRL. 1108 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1108–1117, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics While it may be difficult to further improve syntactic parsing, a promising alternative is to perform both syntactic and semantic parsing in an integrated way. Given the close interaction between the two tasks, joint learning not only allows uncertainty about syntactic parsing to be carried forward to semantic parsing but als"
P10-1113,P07-1027,1,0.827603,"nt and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both feature-based and kernel-based methods, have been extensively studied for SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Pradhan et al., 2005; Liu and Ng, 2007). Nevertheless, for both verbal and nominal SRL, state-of-the-art systems depend heavily on the top-best parse tree and there exists a large performance gap between SRL based on the gold parse tree and the top-best parse tree. For example, Pradhan et al. (2005) suffered a performance drop of 7.3 in F1-measure on English PropBank when using the top-best parse tree returned from Charniak’s parser (Charniak, 2001). Liu and Ng (2007) reported a performance drop of 4.21 in F1-measure on English NomBank. Compared with English SRL, Chinese SRL suffers more seriously from syntactic parsing. Xue (2008)"
P10-1113,H05-1078,0,0.11609,"Missing"
P10-1113,W08-2101,0,0.0534059,"hey incorporated semantic role information into syntactic parse trees by extending syntactic constituent labels with their coarse-grained semantic roles (core argument or adjunct argument) in the sentence, and thus unified semantic parsing and syntactic parsing. The actual fine-grained semantic roles are assigned, as in other methods, by an ensemble classifier. However, the results obtained with this method were negative, and they concluded that semantic parsing on PropBank was too difficult due to the differences between chunk annotation and tree structure. Motivated by Yi and Palmer (2005), Merlo and Musillo (2008) first extended a statistical parser to produce a richly annotated tree that identifies and labels nodes with semantic role labels as well as syntactic labels. Then, they explored both rule-based and machine learning techniques to extract predicate-argument structures from this enriched output. Their experiments showed that their method was biased against these roles in general, thus lowering recall for them (e.g., precision of 87.6 and recall of 65.8). There have been other efforts in NLP on joint learning with various degrees of success. In particular, the recent shared tasks of CoNLL 2008 a"
P10-1113,meyers-etal-2004-annotating,0,0.0172328,"ion answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Kong et al., 2009). Given a sentence Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive, Singapore 117417 nght@comp.nus.edu.sg and a predicate (either a verb or a noun) in the sentence, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) of the predicate. In both English and Chinese PropBank (Palmer et al., 2005; Xue and Palmer, 2003), and English and Chinese NomBank (Meyers et al., 2004; Xue, 2006), these semantic arguments include core arguments (e.g., Arg0 for agent and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both feature-based and kernel-based methods, have been extensively studied for SR"
P10-1113,A00-2030,0,0.0214221,"xperiments showed that their method was biased against these roles in general, thus lowering recall for them (e.g., precision of 87.6 and recall of 65.8). There have been other efforts in NLP on joint learning with various degrees of success. In particular, the recent shared tasks of CoNLL 2008 and 2009 (Surdeanu et al., 2008; Hajic et al., 2009) tackled joint parsing of syntactic and semantic dependencies. However, all the top 5 reported systems decoupled the tasks, rather than building joint models. Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al. (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. In addition, attempts on joint Chinese word segmentation and part-of-speech (POS) tagging (Ng and Low, 2004; Zhang and Clark, 2008) also illustrate the benefits of joint learning. 1109 TOP IP VP Arg1/Rel2 Arg0/Rel1 VP Arg0/Rel2 Arg2/Rel1 NP NN 中国 Chinese 政府 govt. P 向 to Arg1/Rel1 Sup/Rel2 Rel1 PP NR PU NP NR NP VV NN 朝鲜 政府 N. Korean govt. 。 . ArgM-MNR/Rel2 Rel2 NN NN 提供 provide 人民币 RMB Chinese government provides R"
P10-1113,W06-1617,1,0.740505,"d nominal predicates in an integrated way. The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 presents our baseline systems for syntactic and semantic parsing. Section 4 presents our proposed method of joint syntactic and semantic parsing for Chinese texts. Section 5 presents the experimental results. Finally, Section 6 concludes the paper. 2 Related Work Compared to the large body of work on either syntactic parsing (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001; Petrov and Klein, 2007), or SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Jiang and Ng, 2006), there is relatively less work on their joint learning. Koomen et al. (2005) adopted the outputs of multiple SRL systems (each on a single parse tree) and combined them into a coherent predicate argument output by solving an optimization problem. Sutton and McCallum (2005) adopted a probabilistic SRL system to re-rank the N-best results of a probabilistic syntactic parser. However, they reported negative results, which they blamed on the inaccurate probability estimates from their locally trained SRL model. As an alternative to the above pseudo-joint learning methods (strictly speaking, they"
P10-1113,W05-0625,0,0.017597,"ed as follows. Section 2 reviews related work. Section 3 presents our baseline systems for syntactic and semantic parsing. Section 4 presents our proposed method of joint syntactic and semantic parsing for Chinese texts. Section 5 presents the experimental results. Finally, Section 6 concludes the paper. 2 Related Work Compared to the large body of work on either syntactic parsing (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001; Petrov and Klein, 2007), or SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Jiang and Ng, 2006), there is relatively less work on their joint learning. Koomen et al. (2005) adopted the outputs of multiple SRL systems (each on a single parse tree) and combined them into a coherent predicate argument output by solving an optimization problem. Sutton and McCallum (2005) adopted a probabilistic SRL system to re-rank the N-best results of a probabilistic syntactic parser. However, they reported negative results, which they blamed on the inaccurate probability estimates from their locally trained SRL model. As an alternative to the above pseudo-joint learning methods (strictly speaking, they are still pipeline methods), one can augment the syntactic label of a constit"
P10-1113,N07-1051,0,0.0110215,"nowledge, this is the first research on exploring syntactic parsing and SRL for verbal and nominal predicates in an integrated way. The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 presents our baseline systems for syntactic and semantic parsing. Section 4 presents our proposed method of joint syntactic and semantic parsing for Chinese texts. Section 5 presents the experimental results. Finally, Section 6 concludes the paper. 2 Related Work Compared to the large body of work on either syntactic parsing (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001; Petrov and Klein, 2007), or SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Jiang and Ng, 2006), there is relatively less work on their joint learning. Koomen et al. (2005) adopted the outputs of multiple SRL systems (each on a single parse tree) and combined them into a coherent predicate argument output by solving an optimization problem. Sutton and McCallum (2005) adopted a probabilistic SRL system to re-rank the N-best results of a probabilistic syntactic parser. However, they reported negative results, which they blamed on the inaccurate probability estimates from their locally trained SRL model. A"
P10-1113,P03-1002,0,0.074885,"ion Semantic parsing maps a natural language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most previous work focuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate in a sentence. In particular, the well-defined semantic role labeling (SRL) task has been drawing increasing attention in recent years due to its importance in natural language processing (NLP) applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Kong et al., 2009). Given a sentence Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive, Singapore 117417 nght@comp.nus.edu.sg and a predicate (either a verb or a noun) in the sentence, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) of the predicate. In both English and Chinese PropBank (Palmer et al., 2005; Xue and Palmer, 2003), and English and Chinese NomBank (Meyers et al., 2004; Xue, 2006), these semantic arguments include core arguments (e.g., Arg0"
P10-1113,W05-0636,0,0.0930155,"ic parsing for Chinese texts. Section 5 presents the experimental results. Finally, Section 6 concludes the paper. 2 Related Work Compared to the large body of work on either syntactic parsing (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001; Petrov and Klein, 2007), or SRL (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005; Jiang and Ng, 2006), there is relatively less work on their joint learning. Koomen et al. (2005) adopted the outputs of multiple SRL systems (each on a single parse tree) and combined them into a coherent predicate argument output by solving an optimization problem. Sutton and McCallum (2005) adopted a probabilistic SRL system to re-rank the N-best results of a probabilistic syntactic parser. However, they reported negative results, which they blamed on the inaccurate probability estimates from their locally trained SRL model. As an alternative to the above pseudo-joint learning methods (strictly speaking, they are still pipeline methods), one can augment the syntactic label of a constituent with semantic information, like what function parsing does (Merlo and Musillo, 2005). Yi and Palmer (2005) observed that the distributions of semantic labels could potentially interact with th"
P10-1113,W03-1707,0,0.0557221,"al language processing (NLP) applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Kong et al., 2009). Given a sentence Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive, Singapore 117417 nght@comp.nus.edu.sg and a predicate (either a verb or a noun) in the sentence, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) of the predicate. In both English and Chinese PropBank (Palmer et al., 2005; Xue and Palmer, 2003), and English and Chinese NomBank (Meyers et al., 2004; Xue, 2006), these semantic arguments include core arguments (e.g., Arg0 for agent and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both feature-based and kern"
P10-1113,xue-2006-annotating,0,0.0182294,"nan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Kong et al., 2009). Given a sentence Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive, Singapore 117417 nght@comp.nus.edu.sg and a predicate (either a verb or a noun) in the sentence, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) of the predicate. In both English and Chinese PropBank (Palmer et al., 2005; Xue and Palmer, 2003), and English and Chinese NomBank (Meyers et al., 2004; Xue, 2006), these semantic arguments include core arguments (e.g., Arg0 for agent and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). According to predicate type, SRL can be divided into SRL for verbal predicates (verbal SRL, in short) and SRL for nominal predicates (nominal SRL, in short). With the availability of large annotated corpora such as FrameNet (Baker et al., 1998), PropBank, and NomBank in English, data-driven techniques, including both feature-based and kernel-based methods, have been extensively studied for SRL (Carreras"
P10-1113,J08-2004,0,0.108807,"Ng, 2007). Nevertheless, for both verbal and nominal SRL, state-of-the-art systems depend heavily on the top-best parse tree and there exists a large performance gap between SRL based on the gold parse tree and the top-best parse tree. For example, Pradhan et al. (2005) suffered a performance drop of 7.3 in F1-measure on English PropBank when using the top-best parse tree returned from Charniak’s parser (Charniak, 2001). Liu and Ng (2007) reported a performance drop of 4.21 in F1-measure on English NomBank. Compared with English SRL, Chinese SRL suffers more seriously from syntactic parsing. Xue (2008) evaluated on Chinese PropBank and showed that the performance of Chinese verbal SRL drops by about 25 in F1-measure when replacing gold parse trees with automatic ones. Likewise, Xue (2008) and Li et al. (2009) reported a performance drop of about 12 in F1-measure in Chinese NomBank SRL. 1108 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1108–1117, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics While it may be difficult to further improve syntactic parsing, a promising alternative is to perform both syntactic"
P10-1113,W05-0639,0,0.0182374,"m into a coherent predicate argument output by solving an optimization problem. Sutton and McCallum (2005) adopted a probabilistic SRL system to re-rank the N-best results of a probabilistic syntactic parser. However, they reported negative results, which they blamed on the inaccurate probability estimates from their locally trained SRL model. As an alternative to the above pseudo-joint learning methods (strictly speaking, they are still pipeline methods), one can augment the syntactic label of a constituent with semantic information, like what function parsing does (Merlo and Musillo, 2005). Yi and Palmer (2005) observed that the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefined the boundaries of constituents. Based on this observation, they incorporated semantic role information into syntactic parse trees by extending syntactic constituent labels with their coarse-grained semantic roles (core argument or adjunct argument) in the sentence, and thus unified semantic parsing and syntactic parsing. The actual fine-grained semantic roles are assigned, as in other methods, by an ensemble classifier. However, the results obtained with this"
P10-1113,P08-1101,0,0.0406815,"anu et al., 2008; Hajic et al., 2009) tackled joint parsing of syntactic and semantic dependencies. However, all the top 5 reported systems decoupled the tasks, rather than building joint models. Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al. (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. In addition, attempts on joint Chinese word segmentation and part-of-speech (POS) tagging (Ng and Low, 2004; Zhang and Clark, 2008) also illustrate the benefits of joint learning. 1109 TOP IP VP Arg1/Rel2 Arg0/Rel1 VP Arg0/Rel2 Arg2/Rel1 NP NN 中国 Chinese 政府 govt. P 向 to Arg1/Rel1 Sup/Rel2 Rel1 PP NR PU NP NR NP VV NN 朝鲜 政府 N. Korean govt. 。 . ArgM-MNR/Rel2 Rel2 NN NN 提供 provide 人民币 RMB Chinese government provides RMB loan to North Korean government. 贷款 loan Figure 1: Two predicates (Rel1 and Rel2) and their arguments in the style of Chinese PropBank and NomBank. 3 Baseline: Pipeline Top-Best Parse Tree Parsing on In this section, we briefly describe our approach to syntactic parsing and semantic role labeling, as well as"
P10-1113,C04-1100,0,0.0259772,"and nominal predicates in an integrated way. 1 Introduction Semantic parsing maps a natural language sentence into a formal representation of its meaning. Due to the difficulty in deep semantic parsing, most previous work focuses on shallow semantic parsing, which assigns a simple structure (such as WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate in a sentence. In particular, the well-defined semantic role labeling (SRL) task has been drawing increasing attention in recent years due to its importance in natural language processing (NLP) applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Kong et al., 2009). Given a sentence Hwee Tou Ng Department of Computer Science National University of Singapore 13 Computing Drive, Singapore 117417 nght@comp.nus.edu.sg and a predicate (either a verb or a noun) in the sentence, SRL recognizes and maps all the constituents in the sentence into their corresponding semantic arguments (roles) of the predicate. In both English and Chinese PropBank (Palmer et al., 2005; Xue and Palmer, 2003), and English and Chinese NomBank (Meyers et al., 2004; Xue, 2006), these semant"
P10-1113,W04-3236,1,\N,Missing
P10-1113,W08-2121,0,\N,Missing
P10-1113,J03-4003,0,\N,Missing
P10-1113,C98-1013,0,\N,Missing
P10-1113,D09-1133,1,\N,Missing
P10-1113,W09-1201,0,\N,Missing
P11-1113,W06-0901,0,0.285055,"d-Position revoke Role=Person a single doctor Role=Position doctor Role=Time-within the last five years Table 1: Event extraction example It is noteworthy that event extraction depends on previous phases like name identification, entity mention co-reference and classification. Thereinto, the name identification is another hard task in ACE evaluation and not the focus in this paper. So we skip the phase and instead directly use the entity labels provided by ACE. 3 Related Work Almost all the current ACE event extraction systems focus on processing one sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardyet al. 2006). However, there have been several studies using high-level information from a wider scope: Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a patternbased framework to incorporate wider context to refine the performance of relation extraction. They claimed that discourse information could filter noisy dependency paths as well as increasing the reliability of dependency path extraction. Finkel et al. (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simul"
P11-1113,P05-1045,0,0.051541,"ided by ACE. 3 Related Work Almost all the current ACE event extraction systems focus on processing one sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardyet al. 2006). However, there have been several studies using high-level information from a wider scope: Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a patternbased framework to incorporate wider context to refine the performance of relation extraction. They claimed that discourse information could filter noisy dependency paths as well as increasing the reliability of dependency path extraction. Finkel et al. (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. They used this technique to augment an information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. Ji and Grishman (2008) were inspired from the hypothesis of “One Sense Per Discourse” (Ya1 Selected"
P11-1113,P09-2093,0,0.219939,"tion. Patwardhan and Riloff (2009) proposed an event extraction model which consists of two components: a model for sentential event recognition, which offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event; and a model for recognizing plausible role fillers, which identifies phrases as role fillers based upon the assumption that the surrounding context is discussing a relevant event. This unified probabilistic model allows the two components to jointly make decisions based upon both the local evidence surrounding each phrase and the “peripheral vision”. Gupta and Ji (2009) used cross-event information within ACE extraction, but only for recovering implicit time information for events. Liao and Grishman (2010) propose document level cross-event inference to improve event extraction. In contrast to Gupta’s work, Liao do not limit themselves to time information for events, but rather use related events and event-type consistency to make predictions or resolve ambiguities regarding a given event. 4 Motivation In event extraction, current transductive inference methods focus on the issue that many events are missing or spuriously tagged because the local information"
P11-1113,P08-1030,0,0.226986,"ncy paths as well as increasing the reliability of dependency path extraction. Finkel et al. (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. They used this technique to augment an information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. Ji and Grishman (2008) were inspired from the hypothesis of “One Sense Per Discourse” (Ya1 Selected from the file “CNN_CF_20030304.1900.02” in ACE-2005 corpus. 1129 rowsky, 1995); they extended the scope from a single document to a cluster of topic-related documents and employed a rule-based approach to propagate consistent trigger classification and event arguments across sentences and documents. Combining global evidence from related documents with local decisions, they obtained an appreciable improvement in both event and event argument identification. Patwardhan and Riloff (2009) proposed an event extraction mo"
P11-1113,P10-1081,0,0.154457,"ecognition, which offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event; and a model for recognizing plausible role fillers, which identifies phrases as role fillers based upon the assumption that the surrounding context is discussing a relevant event. This unified probabilistic model allows the two components to jointly make decisions based upon both the local evidence surrounding each phrase and the “peripheral vision”. Gupta and Ji (2009) used cross-event information within ACE extraction, but only for recovering implicit time information for events. Liao and Grishman (2010) propose document level cross-event inference to improve event extraction. In contrast to Gupta’s work, Liao do not limit themselves to time information for events, but rather use related events and event-type consistency to make predictions or resolve ambiguities regarding a given event. 4 Motivation In event extraction, current transductive inference methods focus on the issue that many events are missing or spuriously tagged because the local information is not sufficient to make a confident decision. The solution is to mine credible evidences of event occurrences from global information an"
P11-1113,P07-1075,0,0.0279729,"xtraction example It is noteworthy that event extraction depends on previous phases like name identification, entity mention co-reference and classification. Thereinto, the name identification is another hard task in ACE evaluation and not the focus in this paper. So we skip the phase and instead directly use the entity labels provided by ACE. 3 Related Work Almost all the current ACE event extraction systems focus on processing one sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardyet al. 2006). However, there have been several studies using high-level information from a wider scope: Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a patternbased framework to incorporate wider context to refine the performance of relation extraction. They claimed that discourse information could filter noisy dependency paths as well as increasing the reliability of dependency path extraction. Finkel et al. (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local struc"
P11-1113,D07-1075,0,0.0177436,"Missing"
P11-1113,D09-1016,0,0.613694,"ion template consistency constraints. Ji and Grishman (2008) were inspired from the hypothesis of “One Sense Per Discourse” (Ya1 Selected from the file “CNN_CF_20030304.1900.02” in ACE-2005 corpus. 1129 rowsky, 1995); they extended the scope from a single document to a cluster of topic-related documents and employed a rule-based approach to propagate consistent trigger classification and event arguments across sentences and documents. Combining global evidence from related documents with local decisions, they obtained an appreciable improvement in both event and event argument identification. Patwardhan and Riloff (2009) proposed an event extraction model which consists of two components: a model for sentential event recognition, which offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event; and a model for recognizing plausible role fillers, which identifies phrases as role fillers based upon the assumption that the surrounding context is discussing a relevant event. This unified probabilistic model allows the two components to jointly make decisions based upon both the local evidence surrounding each phrase and the “peripheral vision”. Gupta and Ji (2009) used cross-eve"
P11-1113,P95-1026,0,0.0796203,"Missing"
P12-2007,2011.eamt-1.38,0,0.0724794,"of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides fl"
P12-2007,P07-1005,0,0.31714,"ianming 支持/VV 美国/NR 立场/NN zhichi meiguo lichang Eight European countries jointly support America’s stand Figure 1: An example word alignment for a ChineseEnglish sentence pair with the dependency parse tree for the Chinese sentence. Here, each Chinese word is attached with its POS tag and Pinyin. ible reordering in a derivation in a natural way. Experiments on Chinese-English translation using four NIST MT test sets show that our HD-HPB model significantly outperforms Chiang’s HPB as well as a SAMT-style refined version of HPB. 2 Head-Driven HPB Translation Model Like Chiang (2005) and Chiang (2007), our HDHPB translation model adopts a synchronous context free grammar, a rewriting system which generates source and target side string pairs simultaneously using a context-free grammar. Instead of collapsing all non-terminals in the source language into a single symbol X as in Chiang (2007), given a word sequence f ij from position i to position j, we first find heads and then concatenate the POS tags of these heads as f ij ’s non-terminal symbol. Specifically, we adopt unlabeled dependency structure to derive heads, which are defined as: Definition 1. For word sequence f ij , word fk (i ≤"
P12-2007,A00-2018,0,0.186505,"ion is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Introduction Chiang’s hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2"
P12-2007,P05-1033,0,0.82208,"iak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Introduction Chiang’s hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; Chiang, 2007) and has been widely adopted in statistical machine translation (SMT). Typically, such models define two types of translation rules: hierarchical (translation) rules which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a"
P12-2007,J07-2003,0,0.732834,"lins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Introduction Chiang’s hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; Chiang, 2007) and has been widely adopted in statistical machine translation (SMT). Typically, such models define two types of translation rules: hierarchical (translation) rules which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way w"
P12-2007,J03-4003,0,0.0227783,"how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads using linguistically motivated dependency parsing, and use their POS to refine X. In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation. Introduction Chiang’s hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; Chiang, 200"
P12-2007,D11-1079,0,0.0366988,"Missing"
P12-2007,D10-1054,0,0.020809,"inals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules. In addition, once a 1 Another non-terminal symbol S is used in glue rules. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang’s HPB model while at the same time incorporating head information and flex33 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics root 欧洲/NR Ouzhou 八国/NN 联名/AD baguo"
P12-2007,D10-1014,0,0.0889748,"Missing"
P12-2007,N03-1017,0,0.0465152,"respectively) as the test data. To find heads, we parse the source sentences with the Berkeley Parser3 (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and use the Penn2Malt toolkit4 to obtain (unlabeled) dependency structures. We obtain the word alignments by running 2 This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 3 http://code.google.com/p/berkeleyparser/ 4 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html/ GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the BLEU scores. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (Koehn, 2004). In this paper, ‘**’ and ‘*’ denote p-values less than 0.01 and in-between [0.01, 0.05), respectively. Table 2 lists the rule t"
P12-2007,W04-3250,0,0.126206,"nn2Malt.html/ GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the BLEU scores. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (Koehn, 2004). In this paper, ‘**’ and ‘*’ denote p-values less than 0.01 and in-between [0.01, 0.05), respectively. Table 2 lists the rule table sizes. The full rule table size (including HD-HRs and NRRs) of our HDHPB model is ˜1.5 times that of Chiang’s, largely due to refining the non-terminal symbol X in Chiang’s model into head-informed ones in our model. It is also unsurprising, that the test set-filtered rule table size of our model is only ˜0.7 times that of Chiang’s: this is due to the fact that some of the refined translation rule patterns required by the test set are unattested in the training d"
P12-2007,P08-1114,0,0.0400471,"which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules. In addition, once a 1 Another non-terminal symbol S is used in glue rules. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang’s HPB model while at the same time incorporating head information and flex33 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics"
P12-2007,P11-1065,0,0.0933141,"Missing"
P12-2007,P00-1056,0,0.331027,"Missing"
P12-2007,J04-4002,0,0.151762,"t in this paper we only refine non-terminal X on the source side to headinformed ones, while still using X on the target side. According to the occurrence of terminals in 34 translation rules, we group rules in the HD-HPB model into two categories: head-driven hierarchical rules (HD-HRs) and non-terminal reordering rules (NRRs), where the former have at least one terminal on both source and target sides and the later have no terminals. For rule extraction, we first identify initial phrase pairs on word-aligned sentence pairs by using the same criterion as most phrase-based translation models (Och and Ney, 2004) and Chiang’s HPB model (Chiang, 2005; Chiang, 2007). We extract HD-HRs and NRRs based on initial phrase pairs, respectively. 2.1 HD-HRs: Head-Driven Hierarchical Rules As mentioned, a HD-HR has at least one terminal on both source and target sides. This is the same as the hierarchical rules defined in Chiang’s HPB model (Chiang, 2007), except that we use head POSinformed non-terminal symbols in the source language. We look for initial phrase pairs that contain other phrases and then replace sub-phrases with POS tags corresponding to their heads. Given the word alignment in Figure 1, Table 1 d"
P12-2007,P03-1021,0,0.0579183,"and use the Penn2Malt toolkit4 to obtain (unlabeled) dependency structures. We obtain the word alignments by running 2 This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 3 http://code.google.com/p/berkeleyparser/ 4 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html/ GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the BLEU scores. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (Koehn, 2004). In this paper, ‘**’ and ‘*’ denote p-values less than 0.01 and in-between [0.01, 0.05), respectively. Table 2 lists the rule table sizes. The full rule table size (including HD-HRs and NRRs) of our HDHPB model is ˜1.5 times that of Chiang’s, largely due to refining the non-te"
P12-2007,N07-1051,0,0.0482787,"rt cell contains at most b derivations). For Moses HPB, we use “grow-diag-final-and” to obtain symmetric word alignments, 10 for the maximum phrase length, and the recommended default values for all other parameters. We train our model on a dataset with ˜1.5M sentence pairs from the LDC dataset.2 We use the 2002 NIST MT evaluation test data (878 sentence pairs) as the development data, and the 2003, 2004, 2005, 2006-news NIST MT evaluation test data (919, 1788, 1082, and 616 sentence pairs, respectively) as the test data. To find heads, we parse the source sentences with the Berkeley Parser3 (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and use the Penn2Malt toolkit4 to obtain (unlabeled) dependency structures. We obtain the word alignments by running 2 This dataset includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 3 http://code.google.com/p/berkeleyparser/ 4 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html/ GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gig"
P12-2007,D09-1008,0,0.0548675,"minals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. Due to lack of linguistic knowledge, Chiang’s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.1 What is more, Chiang’s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules. In addition, once a 1 Another non-terminal symbol S is used in glue rules. Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011), our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang’s HPB model while at the same time incorporating head information and flex33 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33–37, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics root 欧洲/NR Ouzhou 八"
P12-2007,W06-3119,0,0.0463139,"lled Head-Driven HPB (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important"
P12-2007,P11-1001,0,0.0533361,"syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima’an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable. Inspired by previous work in parsing (Charniak, 2000; Collins, 2003), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB. We identify heads us"
P13-1145,W06-0901,0,0.632042,"ork. Section 3 describes a state-of-the-art Chinese argument extraction system as the baseline. Section 4 introduces our global model in inferring those inter-sentence arguments. Section 5 reports experimental results and gives deep analysis. Finally, we conclude our work in Section 6. 2 Related Work Almost all the existing studies on argument extraction concern English. While some apply pattern-based approaches (e.g., Riloff, 1996; Califf and Mooney, 2003; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011), the others use machine learning-based approaches (e.g., Grishman et al., 2005; Ahn, 2006; Patwardhan and Riloff, 2009; Lu and Roth, 2012), most of which rely on various kinds of features in the context of a sentence. In comparison, there are only a few studies exploring inter-sentence information or argument semantics (e.g., Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011, 2012). Compared with the tremendous work on English event extraction, there are only a few studies (e.g., Tan et al., 2008; Chen and Ji, 2009b; Fu et al., 2010; Qin et al., 2010; Li et al., 2012) on Chinese event extraction with focus on either feature engineering or trigger expansion, under"
P13-1145,N06-1046,0,0.0302296,"the former always uses a verbal noun to refer to an event mentioned in current or previous sentence and the latter is that an event is mentioned twice or more actually. For example, the relation between E2 and E3 in D1 is NC while the trigger of E3 is only a verbal noun without any direct arguments and it refers to E2. We adopt a simple rule to recognize those NC relations: for each event mention whose trigger is a noun and doesn’t act as the subject/object, we regard their relation as NC if there is another event mention with the same trigger in current or previous sentence. Inspired by Ahn (2006), we use the following conditions to infer the EC relations between two event mentions with the same event type: 1) Their trigger mentions refer to the same trigger; 2) They have at least one same or similar 1 It acts as the governing semantic element in a Chinese word. 2 If they have the same event type, they will be regarded as a single event mention. In algorithm 1, HM(tri) is to identify the head morpheme in trigger tri and FindAllMP(hm1, et1, hm2, et2) is to find all event mention pairs in the training set which satisfy the condition that their head morphemes are hm1 and hm2, and their ev"
P13-1145,P11-1062,0,0.0302435,"nd also explore the relation between the argument and its role. Besides, those entities act as non-argument also have the consistency with high probabilities. To let the global argument inference model combine those knowledges of argument semantics, we compute the prior probabilities P(X<i,j&gt;=1) and P(Y<i,j,m&gt;=1) that entity enj occurrs in a specific event type eti as an argument and its role is Rm respectively. To overcome the sparsity of the entities, we cluster those entities into more cohesive subtype following Hong et al. (2011). Hence, following the independence assumptions described by Berant et al. (2011), we modify the fI(EZ) and fD(EZ,Rm)in Eq. 1 as follows: f I ( E Z ) = log f D ( EZ , Rm ) = log where δ and λ are the thresholds learned from the P( X Z = 1 |FZ ) P( X Z = 1) (1 − P( X Z = 1 |FZ ) P( X Z = 0) P (Y< Z , m &gt; = 1 |F< Z , m &gt; ) P ( X < Z , m &gt; = 1) (1 − P ( X < Z , m &gt; = 1 |F< Z , m &gt; ) P( X < Z , m &gt; = 0) (8) (9) where P( X Z = 1 |FZ ) and P(Y< Z ,m &gt; = 1 |F< Z ,m &gt; ) are the probabilities from the AI and AD respectively while FZ and F<Z,m&gt; are the feature vectors. Besides, P ( X < Z ,m &gt; = 1) and P( X Z = 1) are the prior probabilities learning from the training set. 5 Experime"
P13-1145,P08-1090,0,0.0575506,"ese argument extraction. Li et al. (2012b) introduce more refined features to the system of Chen and Ji (2009b) as their baseline. Specially, several studies have successfully incorporated cross-document or document-level information and argument semantics into event extraction, most of them focused on English. Yangarber et al. (2007) apply a crossdocument inference mechanism to refine local extraction results for the disease name, location and start/end time. Mann (2007) proposes some constraints on relationship rescoring to impose the discourse consistency on the CEO’s personal information. Chambers and Jurafsky (2008) propose a narrative event chain which are partially ordered sets of event mentions centered around a common protagonist and this chain can represent the relationship among the relevant event mentions in a document. Ji and Grishman (2008) employ a rule-based approach to propagate consistent triggers and arguments across topic-related documents. Liao and Grishman (2010) mainly focus on employing the cross-event consistency information to improve sentence-level trigger extraction and they also propose an inference method to infer the arguments following role consistency in a document. Hong et al"
P13-1145,P11-1098,0,0.0722166,"Missing"
P13-1145,W09-2209,0,0.351314,"eline. 1 Introduction The task of event extraction is to recognize event mentions of a predefined event type and their arguments (participants and attributes). Generally, it can be divided into two subtasks: trigger extraction, which aims to identify trigger/event mentions and determine their event type, and argument extraction, which aims to extract various arguments of a specific event and assign the roles to them. In this paper, we focus on argument extraction in Chinese event extraction. While most of previous studies in Chinese event extraction deal with Chinese trigger extraction (e.g., Chen and Ji, 2009a; Qin et al., 2010; Li et al., 2012a, 2012b), there are only a few on Chinese argument extraction (e.g., Tan et al., 2008; Chen and Ji, 2009b). Following previous studies, we divide argument extraction into two components, argument identification and role determination, where the former recognizes the arguments in a specific event mention and the latter classifies these arguments by roles. With regard to methodology, most of previous studies on argument extraction recast it as a Semantic Role Labeling (SRL) task and focus on intra-sentence information to identify the arguments and their roles"
P13-1145,N09-2053,0,0.60189,"eline. 1 Introduction The task of event extraction is to recognize event mentions of a predefined event type and their arguments (participants and attributes). Generally, it can be divided into two subtasks: trigger extraction, which aims to identify trigger/event mentions and determine their event type, and argument extraction, which aims to extract various arguments of a specific event and assign the roles to them. In this paper, we focus on argument extraction in Chinese event extraction. While most of previous studies in Chinese event extraction deal with Chinese trigger extraction (e.g., Chen and Ji, 2009a; Qin et al., 2010; Li et al., 2012a, 2012b), there are only a few on Chinese argument extraction (e.g., Tan et al., 2008; Chen and Ji, 2009b). Following previous studies, we divide argument extraction into two components, argument identification and role determination, where the former recognizes the arguments in a specific event mention and the latter classifies these arguments by roles. With regard to methodology, most of previous studies on argument extraction recast it as a Semantic Role Labeling (SRL) task and focus on intra-sentence information to identify the arguments and their roles"
P13-1145,D12-1062,0,0.102635,"following discourse as a sample: D3: 这批战俘离开(E6)阿尔及利亚西部城市廷 杜夫前往(E7)摩洛哥西南部城市阿加迪尔。 (These prisoners left (E6) Tindouf, a western city of Algeria, and went (E7) to Agadir, a southwestern city of Morocco.) - From Xin20001215.2000.0158 In D3, there are two Transport mentions and it is natural to infer 阿 加 迪 尔 (Agadir) as the Destination role of E6 and 廷杜夫 (Tindouf) as the Origin role of E7 via their Sequence relation. 1480 4.3 Identifying Relations of Event Mention Pairs Currently, there are only few studies focusing on such area (e.g., Ahn, 2006; Chamber and Jurafsky, 2007; Huang and Rillof, 2012; Do et al., 2012) and their approaches cannot be introduced to our system directly for the language nature and the different goal. We try to achieve a higher accuracy in this stage so that our argument inference can recover more true arguments. Inspired by Li and Zhou (2012), we also use the morphological structure to identify the Parallel relation. Two parallel event mentions with the adjacent trigger mentions w1 and w2 must satisfy follows two conditions: subject/object; 3) The score of cosine similarity of two event mentions is more than a threshold3. Finally, for the Sequence relation, instead of identifyi"
P13-1145,P08-1030,0,0.124638,"nt semantics into event extraction, most of them focused on English. Yangarber et al. (2007) apply a crossdocument inference mechanism to refine local extraction results for the disease name, location and start/end time. Mann (2007) proposes some constraints on relationship rescoring to impose the discourse consistency on the CEO’s personal information. Chambers and Jurafsky (2008) propose a narrative event chain which are partially ordered sets of event mentions centered around a common protagonist and this chain can represent the relationship among the relevant event mentions in a document. Ji and Grishman (2008) employ a rule-based approach to propagate consistent triggers and arguments across topic-related documents. Liao and Grishman (2010) mainly focus on employing the cross-event consistency information to improve sentence-level trigger extraction and they also propose an inference method to infer the arguments following role consistency in a document. Hong et al. (2011) employ the background information to divide an entity type into more cohesive subtypes to create the bridge between two entities and then infer arguments and their roles using cross-entity inference on the subtypes of entities. H"
P13-1145,C10-1068,1,0.897317,"Missing"
P13-1145,C12-1100,1,0.791859,"Missing"
P13-1145,C12-1099,1,0.919259,"port mentions and it is natural to infer 阿 加 迪 尔 (Agadir) as the Destination role of E6 and 廷杜夫 (Tindouf) as the Origin role of E7 via their Sequence relation. 1480 4.3 Identifying Relations of Event Mention Pairs Currently, there are only few studies focusing on such area (e.g., Ahn, 2006; Chamber and Jurafsky, 2007; Huang and Rillof, 2012; Do et al., 2012) and their approaches cannot be introduced to our system directly for the language nature and the different goal. We try to achieve a higher accuracy in this stage so that our argument inference can recover more true arguments. Inspired by Li and Zhou (2012), we also use the morphological structure to identify the Parallel relation. Two parallel event mentions with the adjacent trigger mentions w1 and w2 must satisfy follows two conditions: subject/object; 3) The score of cosine similarity of two event mentions is more than a threshold3. Finally, for the Sequence relation, instead of identifying and classifying the relations clearly and correctly, our goal is to identify whether there are relevant event mentions in a long sentence or two adjacent short sentences who share arguments. Algorithm 1 illustrates a knowledge-based approach to identify t"
P13-1145,P06-1047,0,0.0311251,". For the errors in the syntactic parsing, the second single-morpheme trigger is often assigned a wrong tag (e.g., NN, JJ) and this leads to the errors in the argument extraction. Therefore, inferring the arguments of the second singlemorpheme trigger from that of the first one based on Parallel relation is also an available way to recover arguments. Like that the topic is an axis in a discourse, the relations among those relevant event mentions with the different types is the bone to link them into a narration. There are a few studies on using the event relations in NLP (e.g., summarization (Li et al., 2006), learning narrative event chains (Chambers and Jurafsky, 2007)) to ensure its effectiveness. In this paper, we define two types of Sequence relations of relevant event mentions: Cause and Temporal for their high probabilities of sharing arguments. The Cause relation between the event mentions are similar to that in the Penn Discourse TreeBank 2.0 (Prasad et al., 2008). For example, an Attack event often is the cause of an Die or Injure event. Our Temporal relation is limited to those mentions with the same or relevant event types (e.g., Transport and Arrest) for the high probabilities of shar"
P13-1145,P10-1081,0,0.744362,"ences omit many of these entities already mentioned in previous sentences. Similarly, it is hard to recognize 两名以色列人 (two Israelites) as the Target role for event mention E2 and identify 炸 弹 (bomb) as the Instrument role for event mention E1. An alternative way is to employ various relationships among relevant event mentions in a discourse to infer those intersentence arguments. The contributions of this paper are: 1) We propose a novel global argument inference model, in which various kinds of event relations are involved to infer more arguments on their semantic relations. 2) Different from Liao and Grishman (2010) and Hong et al. (2011), which only consider document-level consistency, we propose a more fine-gained consistency model to enforce the consistency in the sentence, discourse and document layers. 3) We incorporate argument semantics into our global argument inference model to unify the semantics of the event and its arguments. The rest of this paper is organized as follows. Section 2 overviews the related work. Section 3 describes a state-of-the-art Chinese argument extraction system as the baseline. Section 4 introduces our global model in inferring those inter-sentence arguments. Section 5 r"
P13-1145,P12-1088,0,0.0454713,"art Chinese argument extraction system as the baseline. Section 4 introduces our global model in inferring those inter-sentence arguments. Section 5 reports experimental results and gives deep analysis. Finally, we conclude our work in Section 6. 2 Related Work Almost all the existing studies on argument extraction concern English. While some apply pattern-based approaches (e.g., Riloff, 1996; Califf and Mooney, 2003; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011), the others use machine learning-based approaches (e.g., Grishman et al., 2005; Ahn, 2006; Patwardhan and Riloff, 2009; Lu and Roth, 2012), most of which rely on various kinds of features in the context of a sentence. In comparison, there are only a few studies exploring inter-sentence information or argument semantics (e.g., Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011, 2012). Compared with the tremendous work on English event extraction, there are only a few studies (e.g., Tan et al., 2008; Chen and Ji, 2009b; Fu et al., 2010; Qin et al., 2010; Li et al., 2012) on Chinese event extraction with focus on either feature engineering or trigger expansion, under the same framework as English trigger identificat"
P13-1145,N07-1042,0,0.015109,"e special issues in Chinese argument extraction. Fu et al. (2010) use a feature weighting scheme to re-weight various features for Chinese argument extraction. Li et al. (2012b) introduce more refined features to the system of Chen and Ji (2009b) as their baseline. Specially, several studies have successfully incorporated cross-document or document-level information and argument semantics into event extraction, most of them focused on English. Yangarber et al. (2007) apply a crossdocument inference mechanism to refine local extraction results for the disease name, location and start/end time. Mann (2007) proposes some constraints on relationship rescoring to impose the discourse consistency on the CEO’s personal information. Chambers and Jurafsky (2008) propose a narrative event chain which are partially ordered sets of event mentions centered around a common protagonist and this chain can represent the relationship among the relevant event mentions in a document. Ji and Grishman (2008) employ a rule-based approach to propagate consistent triggers and arguments across topic-related documents. Liao and Grishman (2010) mainly focus on employing the cross-event consistency information to improve"
P13-1145,D07-1075,0,0.0324836,"l to unify the semantics of the event and its arguments. The rest of this paper is organized as follows. Section 2 overviews the related work. Section 3 describes a state-of-the-art Chinese argument extraction system as the baseline. Section 4 introduces our global model in inferring those inter-sentence arguments. Section 5 reports experimental results and gives deep analysis. Finally, we conclude our work in Section 6. 2 Related Work Almost all the existing studies on argument extraction concern English. While some apply pattern-based approaches (e.g., Riloff, 1996; Califf and Mooney, 2003; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011), the others use machine learning-based approaches (e.g., Grishman et al., 2005; Ahn, 2006; Patwardhan and Riloff, 2009; Lu and Roth, 2012), most of which rely on various kinds of features in the context of a sentence. In comparison, there are only a few studies exploring inter-sentence information or argument semantics (e.g., Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011, 2012). Compared with the tremendous work on English event extraction, there are only a few studies (e.g., Tan et al., 2008; Chen and Ji, 2009b; Fu et al., 2010; Qin et al.,"
P13-1145,P11-1114,0,0.0171445,"ated Work Almost all the existing studies on argument extraction concern English. While some apply pattern-based approaches (e.g., Riloff, 1996; Califf and Mooney, 2003; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011), the others use machine learning-based approaches (e.g., Grishman et al., 2005; Ahn, 2006; Patwardhan and Riloff, 2009; Lu and Roth, 2012), most of which rely on various kinds of features in the context of a sentence. In comparison, there are only a few studies exploring inter-sentence information or argument semantics (e.g., Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011, 2012). Compared with the tremendous work on English event extraction, there are only a few studies (e.g., Tan et al., 2008; Chen and Ji, 2009b; Fu et al., 2010; Qin et al., 2010; Li et al., 2012) on Chinese event extraction with focus on either feature engineering or trigger expansion, under the same framework as English trigger identification. In additional, there are only very few of them focusing on Chinese argument extraction and almost all aim to feature engineering and are based on sentence-level information and recast this task as an SRL-style task. Tan et al. (2008) introduce multipl"
P13-1145,prasad-etal-2008-penn,0,0.047479,"topic is an axis in a discourse, the relations among those relevant event mentions with the different types is the bone to link them into a narration. There are a few studies on using the event relations in NLP (e.g., summarization (Li et al., 2006), learning narrative event chains (Chambers and Jurafsky, 2007)) to ensure its effectiveness. In this paper, we define two types of Sequence relations of relevant event mentions: Cause and Temporal for their high probabilities of sharing arguments. The Cause relation between the event mentions are similar to that in the Penn Discourse TreeBank 2.0 (Prasad et al., 2008). For example, an Attack event often is the cause of an Die or Injure event. Our Temporal relation is limited to those mentions with the same or relevant event types (e.g., Transport and Arrest) for the high probabilities of sharing arguments. Take the following discourse as a sample: D3: 这批战俘离开(E6)阿尔及利亚西部城市廷 杜夫前往(E7)摩洛哥西南部城市阿加迪尔。 (These prisoners left (E6) Tindouf, a western city of Algeria, and went (E7) to Agadir, a southwestern city of Morocco.) - From Xin20001215.2000.0158 In D3, there are two Transport mentions and it is natural to infer 阿 加 迪 尔 (Agadir) as the Destination role of E6 and"
P13-1145,D12-1092,1,\N,Missing
P13-1145,P11-2111,0,\N,Missing
P13-1145,P10-1113,1,\N,Missing
P13-1145,P11-1113,1,\N,Missing
P13-1145,D09-1016,0,\N,Missing
P13-2091,I08-1041,0,0.0192107,"ve and negative) with a high probability and does not apply to fine-grained emotion categories (e.g., happy, angry, and sad). This motivates our joint modeling in terms of the coarse-grained emotion categories. Specifically, we consider the news text and the comment text as two different views of expressing either the news reader’s or comment writer’s emotions. Given the two views, a co-training algorithm is proposed to perform semi-supervised emotion classification so that the information in the unlabeled data can be exploited to improve the classification performance. ing (Alm et al., 2005; Aman and Szpakowicz, 2008; Chen et al., 2010; Purver and Battersby, 2012; Moshfeghi et al., 2011), and so far, we have not seen any studies on semi-supervised learning on fine-grained emotion classification. 2 3 2.1 Related Work Comment Writer’s Emotion Classification Comment writer’s emotion classification has been a hot research topic in NLP during the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment cl"
P13-2091,C10-1021,1,0.605042,". I still can not forget last year. (2) My father-in-law got to experience this quake... what a suffering. Comment Writer’s emotion: sad Comment Reader’s emotion: Unknown Introduction Emotion classification aims to predict the emotion categories (e.g., happy, angry, or sad) of a given text (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). With the rapid growth of computer mediated communication applications, such as social websites and miro-blogs, the research on emotion classification has been attracting more and more attentions recently from the natural language processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012). In general, a single text may possess two kinds of emotions, writer’s emotion and reader’s emotion, where the former concerns the emotion expressed by the writer when writing the text and the latter concerns the emotion expressed by a reader after reading the text. For example, consider two short texts drawn from a news and corresponding comments, as shown in Figure 1. On * * Corresponding author Figure 1: An example of writer’s and reader’s emotions on a news and its comments Accordingly, emotion classification can be grouped into two categories: reader’s emotio"
P13-2091,W02-1011,0,0.0151406,"the two views, a co-training algorithm is proposed to perform semi-supervised emotion classification so that the information in the unlabeled data can be exploited to improve the classification performance. ing (Alm et al., 2005; Aman and Szpakowicz, 2008; Chen et al., 2010; Purver and Battersby, 2012; Moshfeghi et al., 2011), and so far, we have not seen any studies on semi-supervised learning on fine-grained emotion classification. 2 3 2.1 Related Work Comment Writer’s Emotion Classification Comment writer’s emotion classification has been a hot research topic in NLP during the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et"
P13-2091,E12-1049,0,0.120126,"orget last year. (2) My father-in-law got to experience this quake... what a suffering. Comment Writer’s emotion: sad Comment Reader’s emotion: Unknown Introduction Emotion classification aims to predict the emotion categories (e.g., happy, angry, or sad) of a given text (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). With the rapid growth of computer mediated communication applications, such as social websites and miro-blogs, the research on emotion classification has been attracting more and more attentions recently from the natural language processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012). In general, a single text may possess two kinds of emotions, writer’s emotion and reader’s emotion, where the former concerns the emotion expressed by the writer when writing the text and the latter concerns the emotion expressed by a reader after reading the text. For example, consider two short texts drawn from a news and corresponding comments, as shown in Figure 1. On * * Corresponding author Figure 1: An example of writer’s and reader’s emotions on a news and its comments Accordingly, emotion classification can be grouped into two categories: reader’s emotion and writer’s emotion classi"
P13-2091,D09-1150,0,0.697347,"emotion while the emotion of a reader after reading the comments is not clear (Some may feel sorry but others might feel careless). News: Today's Japan earthquake could be 2011 quake aftershock. …… News Writer’s emotion: None News Reader’s emotion: sad, worried Comments: (1) I hope everything is ok, so sad. I still can not forget last year. (2) My father-in-law got to experience this quake... what a suffering. Comment Writer’s emotion: sad Comment Reader’s emotion: Unknown Introduction Emotion classification aims to predict the emotion categories (e.g., happy, angry, or sad) of a given text (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). With the rapid growth of computer mediated communication applications, such as social websites and miro-blogs, the research on emotion classification has been attracting more and more attentions recently from the natural language processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012). In general, a single text may possess two kinds of emotions, writer’s emotion and reader’s emotion, where the former concerns the emotion expressed by the writer when writing the text and the latter concerns the emotion expressed by a reader after reading the te"
P13-2091,P09-2038,0,0.311983,"motion of a reader after reading the comments is not clear (Some may feel sorry but others might feel careless). News: Today's Japan earthquake could be 2011 quake aftershock. …… News Writer’s emotion: None News Reader’s emotion: sad, worried Comments: (1) I hope everything is ok, so sad. I still can not forget last year. (2) My father-in-law got to experience this quake... what a suffering. Comment Writer’s emotion: sad Comment Reader’s emotion: Unknown Introduction Emotion classification aims to predict the emotion categories (e.g., happy, angry, or sad) of a given text (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). With the rapid growth of computer mediated communication applications, such as social websites and miro-blogs, the research on emotion classification has been attracting more and more attentions recently from the natural language processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012). In general, a single text may possess two kinds of emotions, writer’s emotion and reader’s emotion, where the former concerns the emotion expressed by the writer when writing the text and the latter concerns the emotion expressed by a reader after reading the text. For example, consider two"
P13-2091,W06-1652,0,0.014076,"in NLP during the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grained emotion classification aims to classify a text into multiple emotion categories, such as happy, angry, and sad. One main group of related studies on this task is about emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). Besides, all the related studies focus on supervised learn2.2 News Reader’s Emotion Classification While comment writer’s emotion cl"
P13-2091,P09-1079,0,0.0353899,"st decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grained emotion classification aims to classify a text into multiple emotion categories, such as happy, angry, and sad. One main group of related studies on this task is about emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). Besides, all the related studies focus on supervised learn2.2 News Reader’s Emotion Classification While comment writer’s emotion classification has been e"
P13-2091,P02-1053,0,0.00981,"-training algorithm is proposed to perform semi-supervised emotion classification so that the information in the unlabeled data can be exploited to improve the classification performance. ing (Alm et al., 2005; Aman and Szpakowicz, 2008; Chen et al., 2010; Purver and Battersby, 2012; Moshfeghi et al., 2011), and so far, we have not seen any studies on semi-supervised learning on fine-grained emotion classification. 2 3 2.1 Related Work Comment Writer’s Emotion Classification Comment writer’s emotion classification has been a hot research topic in NLP during the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li"
P13-2091,P10-1043,1,0.556285,"2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grained emotion classification aims to classify a text into multiple emotion categories, such as happy, angry, and sad. One main group of related studies on this task is about emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). Besides, all the related studies focus on supervised learn2.2 News Reader’s Emotion Classification While comment writer’s emotion classification has been extensively studie"
P13-2091,E12-1031,0,0.100839,"slike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grained emotion classification aims to classify a text into multiple emotion categories, such as happy, angry, and sad. One main group of related studies on this task is about emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). Besides, all the related studies focus on supervised learn2.2 News Reader’s Emotion Classification While comment writer’s emotion classification has been extensively studied, there are only a few studies on news reader’s emotion classification from the NLP and related communities. Lin et al. (2007) first describe the task of reader’s emotion classification on the news articles and then employ some standard machine learning approaches to train a classifier for determining the reader’s em"
P13-2091,J09-3003,0,0.0290799,"to perform semi-supervised emotion classification so that the information in the unlabeled data can be exploited to improve the classification performance. ing (Alm et al., 2005; Aman and Szpakowicz, 2008; Chen et al., 2010; Purver and Battersby, 2012; Moshfeghi et al., 2011), and so far, we have not seen any studies on semi-supervised learning on fine-grained emotion classification. 2 3 2.1 Related Work Comment Writer’s Emotion Classification Comment writer’s emotion classification has been a hot research topic in NLP during the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grai"
P13-2091,C10-1136,0,0.440288,"uch as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grained emotion classification aims to classify a text into multiple emotion categories, such as happy, angry, and sad. One main group of related studies on this task is about emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). Besides, all the related studies focus on supervised learn2.2 News Reader’s Emotion Classification While comment writer’s emotion classification has been extensively studied, there are only a few studies on news reader’s emotion classification from the NLP and related communities. Lin et al. (2007) first describe the task of reader’s emotion classification on the news articles and then employ some standard machine learning approaches to train a classifier for dete"
P13-2091,H05-1073,0,\N,Missing
P14-1049,P11-1059,0,0.102688,"mally, negation focus is defined as the special part in the sentence, which is most prominently or explicitly negated by a negative expression. Hereafter, we denote negative expression in boldface and negation focus underlined. (2) He didn&apos;t stop until he got to Jackson Hole. While people tend to employ stress or intonation in speech to emphasize negation focus and thus it is easy to identify negation focus in speech corpora, such stress or intonation information often misses in the dominating text corpora. This poses serious challenges on negation focus identification. Current studies (e.g., Blanco and Moldovan, 2011; Rosenberg and Bergler, 2012) sort to various kinds of intra-sentence information, such as lexical features, syntactic features, semantic role features and so on, ignoring less-obvious inter-sentence information. This largely defers the performance of negation focus identification and its wide applications, since such contextual discourse information plays a critical role on negation focus identification. Take following sentence as an example. (3) Helen didn’t allow her youngest son to play the violin. In sentence (3), there are several scenarios on identification of negation focus, with rega"
P14-1049,de-marneffe-etal-2006-generating,0,0.0392268,"Missing"
P14-1049,P05-1045,0,0.0467056,"Missing"
P14-1049,P09-1083,0,0.0321475,"intra-sentence information provides the local features from lexical, syntactic and semantic perspectives, both have their own contributions on negation focus identification. In this paper, we first propose a graph model to gauge the importance of contextual discourse 524 information. Then, we incorporate both intraand inter-sentence features into a machine learning-based framework for negation focus identification. 4.1 Graph Model Graph models have been proven successful in many NLP applications, especially in representing the link relationships between words or sentences (Wan and Yang, 2008; Li et al., 2009). Generally, such models could construct a graph to compute the relevance between document theme and words. In this paper, we propose a graph model to represent the contextual discourse information from both lexical and topic perspectives. In particular, a word-based graph model is proposed to represent the explicit relatedness among words in a discourse from the lexical perspective, while a topic-driven word-based model is proposed to enrich the implicit relatedness between words, by adding one more layer to the word-based graph model in representing the global topic distribution of the whole"
P14-1049,S12-1039,0,0.487036,"fined as the special part in the sentence, which is most prominently or explicitly negated by a negative expression. Hereafter, we denote negative expression in boldface and negation focus underlined. (2) He didn&apos;t stop until he got to Jackson Hole. While people tend to employ stress or intonation in speech to emphasize negation focus and thus it is easy to identify negation focus in speech corpora, such stress or intonation information often misses in the dominating text corpora. This poses serious challenges on negation focus identification. Current studies (e.g., Blanco and Moldovan, 2011; Rosenberg and Bergler, 2012) sort to various kinds of intra-sentence information, such as lexical features, syntactic features, semantic role features and so on, ignoring less-obvious inter-sentence information. This largely defers the performance of negation focus identification and its wide applications, since such contextual discourse information plays a critical role on negation focus identification. Take following sentence as an example. (3) Helen didn’t allow her youngest son to play the violin. In sentence (3), there are several scenarios on identification of negation focus, with regard to negation expression n’t,"
P14-1049,C10-1076,1,0.857993,"ics (e.g. Horn, 1989; van der Wouden, 1997), and there were only a few in natural language processing with focus on negation recognition in the biomedical domain. For example, Chapman et al. (2001) developed a rule-based negation recognition system, NegEx, to determine whether a finding mentioned within narrative medical reports is present or absent. Since the release of the BioScope corpus (Vincze et al., 2008), a freely available resource consisting of medical and biological texts, machine learning approaches begin to dominate the research on negation recognition (e.g. Morante et al., 2008; Li et al., 2010). Generally, negation recognition includes three subtasks: cue detection, which detects and identifies possible negative expressions in a sentence, scope resolution, which determines the grammatical scope in a sentence affected by a negative expression, and focus identification, which identifies the constituent in a sentence most prominently or explicitly negated by a negative expression. This paper concentrates on the third subtask, negation focus identification. Due to the increasing demand on deep understanding of natural language text, negation recognition has been drawing more and more at"
P14-1049,W09-1401,0,0.0204889,"cal scope in a sentence affected by a negative expression, and focus identification, which identifies the constituent in a sentence most prominently or explicitly negated by a negative expression. This paper concentrates on the third subtask, negation focus identification. Due to the increasing demand on deep understanding of natural language text, negation recognition has been drawing more and more attention in recent years, with a series of shared tasks and workshops, however, with focus on cue detection and scope resolution, such as the BioNLP 2009 shared task for negative event detection (Kim et al., 2009) and the ACL 2010 Workshop for scope resolution of negation and speculation (Morante and Sporleder, 2010), followed by a special issue of Computational Linguistics (Morante and Sporleder, 2012) for modality and negation. The research on negation focus identification was pioneered by Blanco and Moldovan (2011), who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one tea"
P14-1049,P03-1054,0,0.0133396,"Missing"
P14-1049,D08-1075,0,0.0622497,"ere almost in linguistics (e.g. Horn, 1989; van der Wouden, 1997), and there were only a few in natural language processing with focus on negation recognition in the biomedical domain. For example, Chapman et al. (2001) developed a rule-based negation recognition system, NegEx, to determine whether a finding mentioned within narrative medical reports is present or absent. Since the release of the BioScope corpus (Vincze et al., 2008), a freely available resource consisting of medical and biological texts, machine learning approaches begin to dominate the research on negation recognition (e.g. Morante et al., 2008; Li et al., 2010). Generally, negation recognition includes three subtasks: cue detection, which detects and identifies possible negative expressions in a sentence, scope resolution, which determines the grammatical scope in a sentence affected by a negative expression, and focus identification, which identifies the constituent in a sentence most prominently or explicitly negated by a negative expression. This paper concentrates on the third subtask, negation focus identification. Due to the increasing demand on deep understanding of natural language text, negation recognition has been drawin"
P14-1049,S12-1035,0,0.696511,"s previous sentence, the negation focus should be play the violin, yielding interpretation Helen didn’t allow her youngest son to play the violin, but it didn’t show whether he was allowed to do other things. In this paper, to well accommodate such contextual discourse information in negation focus identification, we propose a graph model to enrich normal intra-sentence features with various kinds of inter-sentence features from both lexical and topic perspectives. Besides, the standard PageRank algorithm is employed to optimize the graph model. Evaluation on the *SEM 2012 shared task corpus (Morante and Blanco, 2012) justifies our approach over several strong baselines. The rest of this paper is organized as follows. Section 2 overviews the related work. Section 3 presents several strong baselines on negation focus identification with only intra-sentence features. Section 4 introduces our topic-driven word-based graph model with contextual discourse information. Section 5 reports the experimental results and analysis. Finally, we conclude our work in Section 6. 2 Related Work Earlier studies of negation were almost in linguistics (e.g. Horn, 1989; van der Wouden, 1997), and there were only a few in natura"
P14-1049,J12-2001,0,0.0547728,"ve expression. This paper concentrates on the third subtask, negation focus identification. Due to the increasing demand on deep understanding of natural language text, negation recognition has been drawing more and more attention in recent years, with a series of shared tasks and workshops, however, with focus on cue detection and scope resolution, such as the BioNLP 2009 shared task for negative event detection (Kim et al., 2009) and the ACL 2010 Workshop for scope resolution of negation and speculation (Morante and Sporleder, 2010), followed by a special issue of Computational Linguistics (Morante and Sporleder, 2012) for modality and negation. The research on negation focus identification was pioneered by Blanco and Moldovan (2011), who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) 1 participated in this task. They identified negation focus using three kinds of heuristics and achieved 58.40 in F1-measure. This indicates great expectation in"
P14-1049,morante-daelemans-2012-conandoyle,0,0.196969,"Missing"
P14-1049,W08-0606,0,\N,Missing
P14-1049,J08-2005,0,\N,Missing
P14-1055,P11-1056,0,0.10782,"ge Processing (NLP). With its aim to identify and classify the semantic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of"
P14-1055,P07-1007,0,0.0247155,"number of unlabeled instances in both languages, our method differs from theirs in that we adopt a bilingual active learning paradigm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspe"
P14-1055,P09-1049,0,0.0196458,"represented as Tc and Te. In order to take full advantage of bilingual resources, we translate both labeled and unlabeled instances in one language to ones in the other language as follows: each other when jointly performed in the BAL framework. Yet, to our knowledge, this issue remains unexplored. An important issue for bilingual learning is how to obtain two language views for relation instances from multilingual resources. There are three solutions to this problem, i.e. parallel corpora (Lu et al., 2011), translated corpora (aka. pseudo parallel corpora) (Wan 2009), and bilingual lexicons (Oh et al., 2009). We adopt the one with pseudo parallel corpora, using the machine translation method to generate instances from one language to the other in the BAL paradigm, as depicted in Fig. 2. Lc Æ Let Uc Æ Uet Le Æ Lct Ue Æ Lct The objective is to learn SVM classifiers in both languages, denoted as SVMc and SVMe respectively, in a BAL fashion to improve their classification performance. 4.2 Labeled Chinese Instances (Lc) Machine Translation Labeled Translated Chinese Instances (Lct) Machine Translation Unlabeled Chinese Instances (Uc) Machine Translation Unlabeled Translated Chinese Instances (Uct) Mac"
P14-1055,N04-1012,0,0.0850822,"digm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sark"
P14-1055,P06-1017,0,0.0283279,"tic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled inst"
P14-1055,D10-1034,1,0.905524,"(ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add"
P14-1055,P04-1054,0,0.206954,"Missing"
P14-1055,P96-1042,0,0.28825,"e and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fashion. Essentially active learning attempts to decrease the quantity of labeled instances by enhancing their quality, gauged by their informativeness to the learner. Since its emergence, active learning has been successfully applied to many tasks in NLP (Engelson and Dagan, 1996; Hwa, 2004; Tomanek et al., 2007; Settles and Craven, 2008). It is trivial to validate, as we will do later in this paper, that active learning can also alleviate the annotation burden for relation extraction in one language while retaining the extraction performance. However, there are cases when we may exploit relation extraction in multiple languages and there are corpora with relation instances annotated for more than one language, such as the ACE RDC 2005 English and Chinese corpora. Hu et al. (2013) shows that supervised relation extraction in one language (e.g. Chinese) 582 Proceedings"
P14-1055,N09-1047,0,0.02862,"se Instances (Uc) Machine Translation Unlabeled Translated Chinese Instances (Uct) Machine Translation Chinese View Labeled Translated English Instances (Let) Labeled English Instances (Le) Unlabeled Translated English Instances (Uet) Unlabeled English Instances (Ue) English View Bilingual active learning Bilingual Active Learning Framework Currently, AL is widely used in NLP tasks in a single language, i.e., during iterations unlabeled instances least confident only in one language are picked and manually labeled to augment the training data. The only exception is AL for machine translation (Haffari et al., 2009; Haffari and Sarkar, 2009), whose purpose is to select the most informative sentences in the source language to be manually translated into the target language. Previous studies (Reichart et al., 2008; Haffari and Sarkar, 2009) show that multi-task active learning (MTAL) can yield promising overall results, no matter whether they are two different tasks or the task of machine translation on multiple language pairs. If a specific NLP task on two languages, such as relation classification, can be regarded as two tasks, it is reasonable to argue that these two tasks can benefit Test Chinese Inst"
P14-1055,P09-1021,0,0.113151,"Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sarkar, 2009) deals with active learning for multilingual 583 machine translation, which make use of multilingual corpora to decrease human annotation efforts by selecting highly informative sentences for a newly added language in multilingual parallel corpora. While machine translation inherently deals with multilingual parallel corpora, our task focuses on relation extraction by pseudo parallel corpora in two languages. 3 Baseline Systems This section first introduces the fundamental supervised learning method, and then describes a baseline active learning algorithm. 3.1 Algorithm uncertainty-based activ"
P14-1055,J04-3001,0,0.167303,"e last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fashion. Essentially active learning attempts to decrease the quantity of labeled instances by enhancing their quality, gauged by their informativeness to the learner. Since its emergence, active learning has been successfully applied to many tasks in NLP (Engelson and Dagan, 1996; Hwa, 2004; Tomanek et al., 2007; Settles and Craven, 2008). It is trivial to validate, as we will do later in this paper, that active learning can also alleviate the annotation burden for relation extraction in one language while retaining the extraction performance. However, there are cases when we may exploit relation extraction in multiple languages and there are corpora with relation instances annotated for more than one language, such as the ACE RDC 2005 English and Chinese corpora. Hu et al. (2013) shows that supervised relation extraction in one language (e.g. Chinese) 582 Proceedings of the 52n"
P14-1055,C10-1064,0,0.428561,"n extraction focus on monolingual resources. As far as representation of relation instances is concerned, there are feature-based methods (Zhao et al., 2004; Zhou et al., 2005; Chan and Roth, 2011) and kernelbased methods (Zelenko et al., 2003; Zhang et al., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping of two entities involved in a relation instance may leads to errors. Therefore, Kim and Lee (2012) further employ a graph-based semisupervised learning method, namely Label Propagation (LP), to indirectly propagate labels from the source language to the target language in an iterative fashion. Both studies transfer relation annotations via parallel corpora from the resource-rich language (English) to the resourcepoor language (Korean), b"
P14-1055,P12-2010,0,0.0149777,"., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping of two entities involved in a relation instance may leads to errors. Therefore, Kim and Lee (2012) further employ a graph-based semisupervised learning method, namely Label Propagation (LP), to indirectly propagate labels from the source language to the target language in an iterative fashion. Both studies transfer relation annotations via parallel corpora from the resource-rich language (English) to the resourcepoor language (Korean), but not vice versa. Based on a small number of labeled instances and a large number of unlabeled instances in both languages, our method differs from theirs in that we adopt a bilingual active learning paradigm via machine translation and improve the perform"
P14-1055,N06-2018,0,0.0288255,"n at least two words in between WBL: the last word in between when at least two words in between WBO: other words in between except the first and last words when at least three words in between b) Entity type ET12: combination of entity types EST12: combination of entity subtypes EC12: combination of entity classes c) Mention level ML12: combination of entity mention levels MT12: combination of LDC mention types d) Overlap #WB: number of other mentions in between #MB: number of words in between M1>M2 or M1&lt;M2: flag indicating whether M2/M1 is included in M1/M2. 3.2 Culotta and McCallum, 2005; Kim et al., 2006) for both Chinese and English relation classification as illustrated in Fig. 1. During iterations a batch of unlabeled instances are chosen in terms of their informativeness to the current classifier, labeled by an oracle and in turn added into the labeled data to retrain the classifier. Due to our focus on the effectiveness of bilingual active learning on relation classification, we only use uncertainty sampling without incorporating more complex measures, such as diversity and representativeness (Settles and Craven, 2008), and leave them for future work. Active Learning Algorithm We use a po"
P14-1055,P08-2023,0,0.0169982,"ere are only a few on multilingual relation extraction in the literature. Monolingual relation extraction: A wide range of studies on relation extraction focus on monolingual resources. As far as representation of relation instances is concerned, there are feature-based methods (Zhao et al., 2004; Zhou et al., 2005; Chan and Roth, 2011) and kernelbased methods (Zelenko et al., 2003; Zhang et al., 2006; Qian et al., 2008), mainly for the English language. Both methods are also widely used in relation extraction in other languages, such as those in Chinese relation extraction (Che et al., 2005; Li et al., 2008; Yu et al., 2010). Multilingual relation extraction: There are only two studies related to multilingual relation extraction. Kim et al. (2010) propose a crosslingual annotation projection approach which uses parallel corpora to acquire a relation detector on the target language. However, the mapping of two entities involved in a relation instance may leads to errors. Therefore, Kim and Lee (2012) further employ a graph-based semisupervised learning method, namely Label Propagation (LP), to indirectly propagate labels from the source language to the target language in an iterative fashion. Bot"
P14-1055,D12-1013,1,0.845398,"in that we adopt a bilingual active learning paradigm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across tw"
P14-1055,P11-1033,0,0.0372471,"Missing"
P14-1055,C08-1088,1,0.949018,"l as Natural Language Processing (NLP). With its aim to identify and classify the semantic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances"
P14-1055,P08-1098,0,0.122012,"otential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sarkar, 2009) deals with active learning for multilingual 583 machine translation, which make use of multilingual corpora to decrease human annotation efforts by selecting highly"
P14-1055,W07-1516,0,0.020613,"on a small number of labeled instances and a large number of unlabeled instances in both languages, our method differs from theirs in that we adopt a bilingual active learning paradigm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided an"
P14-1055,D08-1112,0,0.384465,"rned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fashion. Essentially active learning attempts to decrease the quantity of labeled instances by enhancing their quality, gauged by their informativeness to the learner. Since its emergence, active learning has been successfully applied to many tasks in NLP (Engelson and Dagan, 1996; Hwa, 2004; Tomanek et al., 2007; Settles and Craven, 2008). It is trivial to validate, as we will do later in this paper, that active learning can also alleviate the annotation burden for relation extraction in one language while retaining the extraction performance. However, there are cases when we may exploit relation extraction in multiple languages and there are corpora with relation instances annotated for more than one language, such as the ACE RDC 2005 English and Chinese corpora. Hu et al. (2013) shows that supervised relation extraction in one language (e.g. Chinese) 582 Proceedings of the 52nd Annual Meeting of the Association for Computati"
P14-1055,P04-1075,1,0.799897,"both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sarkar, 2009) deals with active learning for multilin"
P14-1055,P09-1117,0,0.0222932,"arning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an active learning framework for the same task, but across two different languages. Another related study (Haffari and Sarkar, 2009) deals with active learning for multilingual 583 machine translation, which make use of"
P14-1055,D07-1051,0,0.440042,"de researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fashion. Essentially active learning attempts to decrease the quantity of labeled instances by enhancing their quality, gauged by their informativeness to the learner. Since its emergence, active learning has been successfully applied to many tasks in NLP (Engelson and Dagan, 1996; Hwa, 2004; Tomanek et al., 2007; Settles and Craven, 2008). It is trivial to validate, as we will do later in this paper, that active learning can also alleviate the annotation burden for relation extraction in one language while retaining the extraction performance. However, there are cases when we may exploit relation extraction in multiple languages and there are corpora with relation instances annotated for more than one language, such as the ACE RDC 2005 English and Chinese corpora. Hu et al. (2013) shows that supervised relation extraction in one language (e.g. Chinese) 582 Proceedings of the 52nd Annual Meeting of th"
P14-1055,P09-1027,0,0.278507,"e complementariness between relation instances in two languages, particularly when the training data is scarce. One natural question is: Can this characteristic be made full use of so that active learning can maximally benefit relation extraction in two languages? To the best of our knowledge, so far the issue of joint active learning in two languages has yet been addressed. Moreover, the success of joint bilingual learning may lend itself to many inherent multilingual NLP tasks such as POS tagging (Yarowsky and Ngai, 2001), name entity recognition (Yarowsky et al., 2001), sentiment analysis (Wan, 2009), and semantic role labeling (Sebastian and Lapata, 2009) etc. This paper proposes a bilingual active learning (BAL) paradigm to relation classification with a small number of labeled relation instances and a large number of unlabeled instances in two languages (non-parallel). Instead of using a parallel corpus which should have entity/relation alignment information and is thus difficult to obtain, this paper employs an off-the-shelf machine translator to translate both labeled and unlabeled instances from one language into the other language, forming pseudo parallel corpora. These translated"
P14-1055,N01-1026,0,0.0811263,"instances translated from another language (e.g. English). This demonstrates that there is some complementariness between relation instances in two languages, particularly when the training data is scarce. One natural question is: Can this characteristic be made full use of so that active learning can maximally benefit relation extraction in two languages? To the best of our knowledge, so far the issue of joint active learning in two languages has yet been addressed. Moreover, the success of joint bilingual learning may lend itself to many inherent multilingual NLP tasks such as POS tagging (Yarowsky and Ngai, 2001), name entity recognition (Yarowsky et al., 2001), sentiment analysis (Wan, 2009), and semantic role labeling (Sebastian and Lapata, 2009) etc. This paper proposes a bilingual active learning (BAL) paradigm to relation classification with a small number of labeled relation instances and a large number of unlabeled instances in two languages (non-parallel). Instead of using a parallel corpus which should have entity/relation alignment information and is thus difficult to obtain, this paper employs an off-the-shelf machine translator to translate both labeled and unlabeled instances from one lan"
P14-1055,H01-1035,0,0.0500654,"Missing"
P14-1055,I05-1034,1,0.784011,"NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manually annotated and add them into the training data in an iterative fashion. Essentially activ"
P14-1055,P06-1104,1,0.885509,"traction (IE) as well as Natural Language Processing (NLP). With its aim to identify and classify the semantic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number o"
P14-1055,P05-1052,0,0.132596,"Missing"
P14-1055,I08-1005,1,0.825005,"tween two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, given a small number of labeled instances and a large number of unlabeled instances, selects the most informative unlabeled instances to be manuall"
P14-1055,P05-1053,1,0.840665,"k of Information Extraction (IE) as well as Natural Language Processing (NLP). With its aim to identify and classify the semantic relationship between two entities (ACE 2002-2007), relation extraction is of great significance to many NLP applications, such as question answering, information fusion, social network construction, and knowledge mining and population etc. * Corresponding author In the literature, the mainstream research on relation extraction adopts statistical machine learning methods, which can be grouped into supervised learning (Zelenko et al., 2003; Culotta and Soresen, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008; Chan and Roth, 2011), semi-supervised learning (Zhang et al., 2004; Chen et al., 2006; Zhou et al., 2008; Qian et al., 2010) and unsupervised learning (Hasegawa et al., 2004; Zhang et al., 2005) in terms of the amount of labeled training data they need. Usually the extraction performance depends heavily on the quality and quantity of the labeled data, however, the manual annotation of a largescale corpus is labor-intensive and timeconsuming. In the last decade researchers have turned to another effective learning paradigm-active learning (AL), which, gi"
P14-1055,D07-1082,0,0.0353247,"d instances in both languages, our method differs from theirs in that we adopt a bilingual active learning paradigm via machine translation and improve the performance for both languages simultaneously. Active Learning in NLP: Active learning has become an active research topic due to its potential to significantly reduce the amount of labeled training data while achieving comparable performance with supervised learning. It has been successfully applied to many NLP applications, such as POS tagging (Engelson and Dagan, 1996; Ringger et al., 2007), word sense disambiguation (Chan and Ng, 2007; Zhu and Hovy, 2007), sentiment detection (Brew et al., 2010; Li et al., 2012), syntactical parsing (Hwa, 2004; Osborne and Baldridge, 2004), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007; Tomanek and Hahn, 2009) etc. Different from these AL studies on a single task, Reichart et al. (2008) introduce a multi-task active learning (MTAL) paradigm, where unlabeled instances are selected for two annotation tasks (i.e. named entity and syntactic parse tree). They demonstrate that MTAL in the same language outperforms one-sided and random selection AL. From a different perspective, we propose an"
P14-1055,P04-1053,0,\N,Missing
P14-2136,W06-0901,0,0.473053,"trigger type determination in event extraction. To make the training data in different languages help each other, we propose a uniform text representation with bilingual features to represent the samples and handle the difficulty of locating the triggers in the translated text from both monolingual and bilingual perspectives. Empirical studies demonstrate the effectiveness of the proposed approach to bilingual classification on trigger type determination.  1 Introduction Event extraction is an increasingly hot and challenging research topic in the natural language processing (NLP) community (Ahn, 2006; Saun et al. 2006; Zhao et al. 2008). It aims to automatically extract certain types of events with the arguments to present the texts under a structured form. In event extraction, there are four primary subtasks, named trigger identification, trigger type determination, argument identification, and argument role determination (Chen and NG, 2012). As an important technology in information extraction, event extraction could be applied to many fields such as information retrieval, summarization, text mining, and question answering. Recently, the dominative approach to event extraction is based"
P14-2136,C12-1033,0,0.0128044,"udies demonstrate the effectiveness of the proposed approach to bilingual classification on trigger type determination.  1 Introduction Event extraction is an increasingly hot and challenging research topic in the natural language processing (NLP) community (Ahn, 2006; Saun et al. 2006; Zhao et al. 2008). It aims to automatically extract certain types of events with the arguments to present the texts under a structured form. In event extraction, there are four primary subtasks, named trigger identification, trigger type determination, argument identification, and argument role determination (Chen and NG, 2012). As an important technology in information extraction, event extraction could be applied to many fields such as information retrieval, summarization, text mining, and question answering. Recently, the dominative approach to event extraction is based on supervised learning where a set of labeled samples are exploited to train a model to extract the events. However, the availa * Corresponding author ble labeled data are rather sparse due to various kinds of event categories. For example, the event taxonomy in ACE 2005 1 (Automatic Content Extraction) includes 8 types of events, with 33 subtype"
P14-2136,N09-2053,0,0.0196321,"ction. Hong et al. (2011) leverage cross-entity information to improve traditional event extraction, regarding entity type consistency as a key feature. More recently, Li et al. (2013) propose a joint framework based on structured prediction which extracts triggers and arguments together. In Chinese, relevant studies in event extraction are in a relatively primary stage with focus on more special characteristics and challenges. Tan et al. (2008) employ local feature selection and explicit discrimination of positive and negative features to ensure the performance of trigger type determination. Chen and Ji (2009) apply lexical, syntactic and semantic features in trigger labeling and argument labeling to improve the performance. More recently, Li et al. (2012) and Li et al. (2013) introduce two inference mechanisms to infer unknown triggers and recover trigger mentions respectively with morphological structures. In comparison with above studies, we focus on bilingual event extraction. Although bilingual classification has been paid lots of attention in other fields (Wan 2008; Haghighi et al., 2008; Ismail et al., 2010; Lu et al., 2011；Li et al., 2013), there is few related work in event extraction. The"
P14-2136,P08-1088,0,0.0919865,"Missing"
P14-2136,P11-1113,1,0.872285,"approaches have been explored recently. Bethard and Martin (2006) formulate the event identification as a classification problem in a wordchunking paradigm, introducing a variety of linguistically motivated features. Ahn (2006) proposes a trigger-based method. It first identifies the trigger in an event, and then uses a multiclassifier to implement trigger type determination. Ji and Grishman (2008) employ an approach to propagate consistent event arguments across sentences and documents. Liao and Grishman (2010) apply document level information to improve the performance of event extraction. Hong et al. (2011) leverage cross-entity information to improve traditional event extraction, regarding entity type consistency as a key feature. More recently, Li et al. (2013) propose a joint framework based on structured prediction which extracts triggers and arguments together. In Chinese, relevant studies in event extraction are in a relatively primary stage with focus on more special characteristics and challenges. Tan et al. (2008) employ local feature selection and explicit discrimination of positive and negative features to ensure the performance of trigger type determination. Chen and Ji (2009) apply"
P14-2136,P10-1081,0,0.164099,"nt extraction has been mainly studied in both English and Chinese. In English, various supervised learning approaches have been explored recently. Bethard and Martin (2006) formulate the event identification as a classification problem in a wordchunking paradigm, introducing a variety of linguistically motivated features. Ahn (2006) proposes a trigger-based method. It first identifies the trigger in an event, and then uses a multiclassifier to implement trigger type determination. Ji and Grishman (2008) employ an approach to propagate consistent event arguments across sentences and documents. Liao and Grishman (2010) apply document level information to improve the performance of event extraction. Hong et al. (2011) leverage cross-entity information to improve traditional event extraction, regarding entity type consistency as a key feature. More recently, Li et al. (2013) propose a joint framework based on structured prediction which extracts triggers and arguments together. In Chinese, relevant studies in event extraction are in a relatively primary stage with focus on more special characteristics and challenges. Tan et al. (2008) employ local feature selection and explicit discrimination of positive and"
P14-2136,P11-1033,0,0.0212729,"features to ensure the performance of trigger type determination. Chen and Ji (2009) apply lexical, syntactic and semantic features in trigger labeling and argument labeling to improve the performance. More recently, Li et al. (2012) and Li et al. (2013) introduce two inference mechanisms to infer unknown triggers and recover trigger mentions respectively with morphological structures. In comparison with above studies, we focus on bilingual event extraction. Although bilingual classification has been paid lots of attention in other fields (Wan 2008; Haghighi et al., 2008; Ismail et al., 2010; Lu et al., 2011；Li et al., 2013), there is few related work in event extraction. The only one related work we find is Ji (2009) which proposes an inductive learning approach to exploit cross-lingual predicate clusters to improve the event extraction task with the main goal to get the event taggers from extra resources, i.e., an English and Chinese parallel corpus. Differently, our goal is to make the lawww.google.com 843 beled data from two languages help each other without any other extra resources, which is original in the study of event extraction. 3 The Proposed Approach Trigger type determination aims t"
P14-2136,W99-0604,0,0.360088,"Missing"
P14-2136,D08-1058,0,0.469981,"e determination. Accordingly, our goal is to design a classifier which is trained with labeled data from two different languages and is capable of classifying the test data from both languages. Generally, this task possesses two main challenges. The first challenge is text representation, namely, how to eliminate the language gap between the two languages. To tackle this, we first employ Google Translate2, a state-of-the-art machine translation system, to gain the translation of an event instance, similar to what has been widely done by previous studies in bilingual classification tasks e.g., Wan (2008); Then, we uniformly represent each text with bilingual word features. That is, we augment each original feature vector into a novel one which contains the translated features. The second challenge is the translation for some specific features. It is well-known that some specific features, such as the triggers and their context features, are extremely important for determining the event types. For example, in E3, both trigger “left” and named entity “Saddam” are important features to tell the event type, i.e., &quot;Transport/Movement&quot;. When it is translated to Chinese, it is also required to know"
P14-2136,C10-2055,0,0.0598022,"Missing"
P14-2136,W09-1704,0,0.254569,"ng et al. (2011) leverage cross-entity information to improve traditional event extraction, regarding entity type consistency as a key feature. More recently, Li et al. (2013) propose a joint framework based on structured prediction which extracts triggers and arguments together. In Chinese, relevant studies in event extraction are in a relatively primary stage with focus on more special characteristics and challenges. Tan et al. (2008) employ local feature selection and explicit discrimination of positive and negative features to ensure the performance of trigger type determination. Chen and Ji (2009) apply lexical, syntactic and semantic features in trigger labeling and argument labeling to improve the performance. More recently, Li et al. (2012) and Li et al. (2013) introduce two inference mechanisms to infer unknown triggers and recover trigger mentions respectively with morphological structures. In comparison with above studies, we focus on bilingual event extraction. Although bilingual classification has been paid lots of attention in other fields (Wan 2008; Haghighi et al., 2008; Ismail et al., 2010; Lu et al., 2011；Li et al., 2013), there is few related work in event extraction. The"
P14-2136,P08-1030,0,0.150833,"al studies. In Section 5, we conclude our work and give some future work. 2 Related Work In the NLP community, event extraction has been mainly studied in both English and Chinese. In English, various supervised learning approaches have been explored recently. Bethard and Martin (2006) formulate the event identification as a classification problem in a wordchunking paradigm, introducing a variety of linguistically motivated features. Ahn (2006) proposes a trigger-based method. It first identifies the trigger in an event, and then uses a multiclassifier to implement trigger type determination. Ji and Grishman (2008) employ an approach to propagate consistent event arguments across sentences and documents. Liao and Grishman (2010) apply document level information to improve the performance of event extraction. Hong et al. (2011) leverage cross-entity information to improve traditional event extraction, regarding entity type consistency as a key feature. More recently, Li et al. (2013) propose a joint framework based on structured prediction which extracts triggers and arguments together. In Chinese, relevant studies in event extraction are in a relatively primary stage with focus on more special character"
P14-2136,N03-1017,0,0.0126015,"Missing"
P14-2136,C12-1099,1,0.885174,"Missing"
P14-2136,P13-1008,0,0.0914419,"roducing a variety of linguistically motivated features. Ahn (2006) proposes a trigger-based method. It first identifies the trigger in an event, and then uses a multiclassifier to implement trigger type determination. Ji and Grishman (2008) employ an approach to propagate consistent event arguments across sentences and documents. Liao and Grishman (2010) apply document level information to improve the performance of event extraction. Hong et al. (2011) leverage cross-entity information to improve traditional event extraction, regarding entity type consistency as a key feature. More recently, Li et al. (2013) propose a joint framework based on structured prediction which extracts triggers and arguments together. In Chinese, relevant studies in event extraction are in a relatively primary stage with focus on more special characteristics and challenges. Tan et al. (2008) employ local feature selection and explicit discrimination of positive and negative features to ensure the performance of trigger type determination. Chen and Ji (2009) apply lexical, syntactic and semantic features in trigger labeling and argument labeling to improve the performance. More recently, Li et al. (2012) and Li et al. (2"
P14-2136,W06-1618,0,\N,Missing
P15-1064,baker-etal-2010-modality,0,0.0224804,"computational linguistics has been shown to be Corresponding author 656 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 656–665, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics useful for biomedical text processing (Morante et al., 2008; Chowdhury and Lavelli, 2013), information retrieval (Averbuch, 2004), sentiment analysis (Councill et al., 2010; Zhu et al., 2014), recognizing textual entailment (Snow et al., 2006), machine translation (Baker et al., 2010; Wetzel and Bond, 2012), and so forth. The research on negation and speculation identification in English has received a noticeable boost. However, in contrast to the significant achievements concerning English, the research progress in Chinese language is quite limited. The main reason includes the following two aspects: First, the scarcity of linguistic resource seriously limits the advance of related research. To the best of our knowledge, there are no publicly available standard Chinese corpus of reasonable size annotated with negation and speculation. Second, this may be attributed to th"
P15-1064,N13-1093,0,0.0296922,"tification is very relevant for almost all NLP applications involving text understanding which need to discriminate between factual and non-factual information. The treatment of negation and speculation in computational linguistics has been shown to be Corresponding author 656 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 656–665, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics useful for biomedical text processing (Morante et al., 2008; Chowdhury and Lavelli, 2013), information retrieval (Averbuch, 2004), sentiment analysis (Councill et al., 2010; Zhu et al., 2014), recognizing textual entailment (Snow et al., 2006), machine translation (Baker et al., 2010; Wetzel and Bond, 2012), and so forth. The research on negation and speculation identification in English has received a noticeable boost. However, in contrast to the significant achievements concerning English, the research progress in Chinese language is quite limited. The main reason includes the following two aspects: First, the scarcity of linguistic resource seriously limits the advance of relat"
P15-1064,W10-3110,0,0.101512,"hich need to discriminate between factual and non-factual information. The treatment of negation and speculation in computational linguistics has been shown to be Corresponding author 656 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 656–665, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics useful for biomedical text processing (Morante et al., 2008; Chowdhury and Lavelli, 2013), information retrieval (Averbuch, 2004), sentiment analysis (Councill et al., 2010; Zhu et al., 2014), recognizing textual entailment (Snow et al., 2006), machine translation (Baker et al., 2010; Wetzel and Bond, 2012), and so forth. The research on negation and speculation identification in English has received a noticeable boost. However, in contrast to the significant achievements concerning English, the research progress in Chinese language is quite limited. The main reason includes the following two aspects: First, the scarcity of linguistic resource seriously limits the advance of related research. To the best of our knowledge, there are no publicly available standard"
P15-1064,W06-1617,0,0.17891,"us constituent (i.e., the scope candidate) and “不(not)” as the given cue, regarding candidate b1 in Figure 1(2)). For clarity, we categorize the features into three groups according to their relevance with the given cue (C, in short), scope candidate (S, in short), and the relationship between cue andcandidate (R, in short). Figure 2 shows four kinds of positional features between cue and scope candidate we defined (R4). Figure 2. Positional features. Some features proposed above may not be effective in classification. Therefore, we adopt a greedy feature se-lection algorithm as described in (Jiang and Ng, 2006) to pick up positive features incrementally according to their contribu661 tions on the development data. Additionally, a cue should have one continuous block as its scope, but the scope identifier may result in discontinuous scope due to independent candidate in classification. For this reason, we employ a post-processing algorithm as described in Zhu et al. (2010) to identify the boundaries. 5 Experimentation In this section, we evaluate our feature-based sequence labeling model and cross-lingual cue expansion strategy on cue detection, and report the experimental results to justify the appr"
P15-1064,P14-2093,0,0.0330872,"Missing"
P15-1064,D08-1075,0,0.145443,"n and speculation identification is very relevant for almost all NLP applications involving text understanding which need to discriminate between factual and non-factual information. The treatment of negation and speculation in computational linguistics has been shown to be Corresponding author 656 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 656–665, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics useful for biomedical text processing (Morante et al., 2008; Chowdhury and Lavelli, 2013), information retrieval (Averbuch, 2004), sentiment analysis (Councill et al., 2010; Zhu et al., 2014), recognizing textual entailment (Snow et al., 2006), machine translation (Baker et al., 2010; Wetzel and Bond, 2012), and so forth. The research on negation and speculation identification in English has received a noticeable boost. However, in contrast to the significant achievements concerning English, the research progress in Chinese language is quite limited. The main reason includes the following two aspects: First, the scarcity of linguistic resource serious"
P15-1064,J12-2001,0,0.0289766,"priateness of our syntactic structure-based framework which obtained significant improvement over the stateof-the-art on negation and speculation identification in Chinese language. * 1 Introduction Negation and speculation are ubiquitous phenomena in natural language. While negation is a grammatical category which comprises various kinds of devices to reverse the truth value of a proposition, speculation is a grammatical category which expresses the attitude of a speaker towards a statement in terms of degree of certainty, * reliability, subjectivity, sources of information, and perspective (Morante and Sporleder, 2012). Current studies on negation and speculation identification mainly focus on two tasks: 1) cue detection, which aims to detect the signal of a negative or speculative expression, and 2) scope resolution, which aims to determine the linguistic coverage of a cue in sentence, in distinguishing unreliable or uncertain information from facts. For example, (E1) and (E2) include a negative cue and a speculative cue respectively, both denoted in boldface with their linguistic scopes denoted in square brackets (adopted hereinafter). In sentence (E1), the negative cue “不(not)” triggers the scope of “不会追"
P15-1064,J03-1002,0,0.00637992,"Missing"
P15-1064,C10-3004,0,0.0610142,"Missing"
P15-1064,N06-1005,0,0.0266548,"treatment of negation and speculation in computational linguistics has been shown to be Corresponding author 656 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 656–665, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics useful for biomedical text processing (Morante et al., 2008; Chowdhury and Lavelli, 2013), information retrieval (Averbuch, 2004), sentiment analysis (Councill et al., 2010; Zhu et al., 2014), recognizing textual entailment (Snow et al., 2006), machine translation (Baker et al., 2010; Wetzel and Bond, 2012), and so forth. The research on negation and speculation identification in English has received a noticeable boost. However, in contrast to the significant achievements concerning English, the research progress in Chinese language is quite limited. The main reason includes the following two aspects: First, the scarcity of linguistic resource seriously limits the advance of related research. To the best of our knowledge, there are no publicly available standard Chinese corpus of reasonable size annotated with negation and speculat"
P15-1064,W12-4203,0,0.0189381,"istics has been shown to be Corresponding author 656 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 656–665, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics useful for biomedical text processing (Morante et al., 2008; Chowdhury and Lavelli, 2013), information retrieval (Averbuch, 2004), sentiment analysis (Councill et al., 2010; Zhu et al., 2014), recognizing textual entailment (Snow et al., 2006), machine translation (Baker et al., 2010; Wetzel and Bond, 2012), and so forth. The research on negation and speculation identification in English has received a noticeable boost. However, in contrast to the significant achievements concerning English, the research progress in Chinese language is quite limited. The main reason includes the following two aspects: First, the scarcity of linguistic resource seriously limits the advance of related research. To the best of our knowledge, there are no publicly available standard Chinese corpus of reasonable size annotated with negation and speculation. Second, this may be attributed to the limitations of Chinese"
P15-1064,D10-1070,1,0.847272,"tures between cue and scope candidate we defined (R4). Figure 2. Positional features. Some features proposed above may not be effective in classification. Therefore, we adopt a greedy feature se-lection algorithm as described in (Jiang and Ng, 2006) to pick up positive features incrementally according to their contribu661 tions on the development data. Additionally, a cue should have one continuous block as its scope, but the scope identifier may result in discontinuous scope due to independent candidate in classification. For this reason, we employ a post-processing algorithm as described in Zhu et al. (2010) to identify the boundaries. 5 Experimentation In this section, we evaluate our feature-based sequence labeling model and cross-lingual cue expansion strategy on cue detection, and report the experimental results to justify the appropriateness of our syntactic structure-based framework on scope resolution in Chinese language. The performance is measured by Precision (P), Recall (R), and F1-score (F). In addition, for scope resolution, we also report the accuracy in PCS (Percentage of Correct Scopes), within which a scope is fully correct if the output of scope resolution system and the correct"
P15-1064,P14-1029,0,0.0177105,"te between factual and non-factual information. The treatment of negation and speculation in computational linguistics has been shown to be Corresponding author 656 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 656–665, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics useful for biomedical text processing (Morante et al., 2008; Chowdhury and Lavelli, 2013), information retrieval (Averbuch, 2004), sentiment analysis (Councill et al., 2010; Zhu et al., 2014), recognizing textual entailment (Snow et al., 2006), machine translation (Baker et al., 2010; Wetzel and Bond, 2012), and so forth. The research on negation and speculation identification in English has received a noticeable boost. However, in contrast to the significant achievements concerning English, the research progress in Chinese language is quite limited. The main reason includes the following two aspects: First, the scarcity of linguistic resource seriously limits the advance of related research. To the best of our knowledge, there are no publicly available standard Chinese corpus of"
P15-1064,D13-1099,1,0.723881,"Missing"
P15-1064,W08-0606,0,\N,Missing
P15-1064,P04-1035,0,\N,Missing
P15-1101,H05-1073,0,0.059114,"context dependence in the corpus. Section 4 proposes our DFG approach to sentence-level emotion classification. Section 5 evaluates the proposed approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system. They leverage a huge collection of emotion-provoking event instances from the Web to deal with the data sparseness problem in sentence-level emotion classification. Bhowmick et al. (2009) and Bhowmick et al."
P15-1101,C10-1021,1,0.385285,"analysis. Section 3 presents our observations on label and context dependence in the corpus. Section 4 proposes our DFG approach to sentence-level emotion classification. Section 5 evaluates the proposed approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system. They leverage a huge collection of emotion-provoking event instances from the Web to deal with the data sparseness problem in sentence-level emotion c"
P15-1101,P13-2091,1,0.778375,"she turn over to me and her little soft hand fall onto my face.</S1&gt; <S2&gt;Praise the Lord, that is all I want.</S2&gt; <S3&gt;Feeling the warm of her hand and the attachment she hold to me, I couldn’t afford to move even a little, fearing I may lost her hand.</S3&gt;)……) ------------------------------------------------------------------- Sentence-level Emotion Classification  Input: Introduction  Output: Predicting emotion categories, such as anger, joy, and anxiety, expressed by a piece of text encompasses a variety of applications, such as online chatting (Galik et al., 2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). Over the past decade, there has been a substantial body of research on emotion classification, where a considerable amount of work has focused on document-level emotion classification. Recently, the research community has become increasingly aware of the need on sentence-level emotion classification due to its wide potential applications, e.g. the massively growing importance of analyzing short text in social media (Kiritchenko et al., 2014; Wen and Wan, 2014). In general, sentence-level emotion classification exhibits two challenges. 1 S1, S2, S3 S1"
P15-1101,D09-1150,0,0.054791,"t of work has focused on document-level emotion classification. Recently, the research community has become increasingly aware of the need on sentence-level emotion classification due to its wide potential applications, e.g. the massively growing importance of analyzing short text in social media (Kiritchenko et al., 2014; Wen and Wan, 2014). In general, sentence-level emotion classification exhibits two challenges. 1 S1, S2, S3 S1 : joy, love S2: joy S3: joy, love, anxiety Figure 1: An example of a paragraph and the sentences therein with their emotion categories from the corpus collected by Quan and Ren (2009) On one hand, like document-level emotion classification, sentence-level emotion classification is naturally a multi-label classification problem. That is, each sentence might involve more than one emotion category. For example, as shown in Figure 1, in one paragraph, two sentences, i.e., S1 and S3, have two and three emotion categories respectively. Automatically classifying instances with multiple possible categories is * Corresponding author 1045 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lang"
P15-1101,C08-1111,0,0.0139364,"Missing"
P15-1101,C10-1136,0,0.0192099,"sification into a unified framework. The remainder of this paper is organized as follows. Section 2 overviews related work on emotion analysis. Section 3 presents our observations on label and context dependence in the corpus. Section 4 proposes our DFG approach to sentence-level emotion classification. Section 5 evaluates the proposed approach. Finally, Section 6 gives the conclusion and future work. 2 Related Work Over the last decade, there has been an explosion of work exploring various aspects of emotion analysis, such as emotion resource creation (Wiebe et al., 2005; Quan and Ren, 2009; Xu et al., 2010), writer’s emotion vs. reader’s emotion analysis (Lin et al., 2008; Liu et al., 2013), emotion cause event analysis (Chen et al., 2010), document-level emotion classification (Alm et al., 2005; Li et al., 2014) and sentence-level or short text-level emotion classification (Tokushisa et al., 2008; Bhowmick et al., 2009; Xu et al., 2012). This work focuses on sentence-level emotion classification. Among the studies on sentence-level emotion classification, Tokushisa et al. (2008) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system. They leverage a"
P15-2005,W02-1011,0,0.0350279,"ation to guarantee a suitable size of the data for training the meta-classifier. Evaluation on four domains shows that such a semi-stacking strategy performs consistently better than its member algorithms. 1 Introduction The past decade has witnessed a huge exploding interest in sentiment analysis from the natural language processing and data mining communities due to its inherent challenges and wide applications (Pang et al., 2008; Liu, 2012). One fundamental task in sentiment analysis is sentiment classification, which aims to determine the sentimental orientation a piece of text expresses (Pang et al., 2002). For instance, the sentence &quot;I absolutely love this product.&quot; is supposed to be determined as a positive expression in sentimental orientation.  While early studies focus on supervised learning, where only labeled data are required to train the classification model (Pang et al., 2002), recent studies devote more and more to reduce the heavy dependence on the large amount of labeled data by exploiting semi-supervised learning approaches, such as co-training (Wan, 2009; Li et al., 2011), label propagation (Sindhwani and Melville, 2008), and deep learning (Zhou et al., 2013), to sentiment class"
P15-2005,W06-1652,0,0.629157,"the learning algorithm aims to learn a classifier from a small scale of labeled samples, named initial labeled data, with a large number of unlabeled samples. In the sequel, we refer the labeled data as L  {( xi , yi )}inL1 where xi R d is the d dimensional input vector, and yi is its output label. The unlabeled data in the target domain is denoted as U  {( xk )}nkU1 . Suppose l semi is a semi-supervised Related Work Early studies on sentiment classification mainly focus on supervised learning methods with algorithm designing and feature engineering (Pang et al., 2002; Cui et al., 2006; Riloff et al., 2006; Li et al., 2009). Recently, most studies on sentiment classification aim to improve the performance by exploiting unlabeled data in two main aspects: semi-supervised learning (Dasgupta and Ng, 2009; Wan, 2009; Li et al., 2010) and cross-domain learning (Blitzer et al. 2007; He et al. 2011; Li et al., 2013). Specifically, existing approaches to semi-supervised sentiment classification could be categorized into two main groups: bootstrappingstyle and graph-based. As for bootstrapping-style approaches, Wan (2009) considers two different languages as two views and applies co-training to conduct"
P15-2005,P07-1056,0,0.374058,"and yi is its output label. The unlabeled data in the target domain is denoted as U  {( xk )}nkU1 . Suppose l semi is a semi-supervised Related Work Early studies on sentiment classification mainly focus on supervised learning methods with algorithm designing and feature engineering (Pang et al., 2002; Cui et al., 2006; Riloff et al., 2006; Li et al., 2009). Recently, most studies on sentiment classification aim to improve the performance by exploiting unlabeled data in two main aspects: semi-supervised learning (Dasgupta and Ng, 2009; Wan, 2009; Li et al., 2010) and cross-domain learning (Blitzer et al. 2007; He et al. 2011; Li et al., 2013). Specifically, existing approaches to semi-supervised sentiment classification could be categorized into two main groups: bootstrappingstyle and graph-based. As for bootstrapping-style approaches, Wan (2009) considers two different languages as two views and applies co-training to conduct semi-supervised sentiment classification. Similarly, Li et al. (2010) propose two views, named personal and impersonal views, and apply co-training to use unlabeled data in a monolingual corpus. More recently, Gao et al. (2014) propose a feature subspace-based self-training"
P15-2005,P09-1027,0,0.242552,"analysis is sentiment classification, which aims to determine the sentimental orientation a piece of text expresses (Pang et al., 2002). For instance, the sentence &quot;I absolutely love this product.&quot; is supposed to be determined as a positive expression in sentimental orientation.  While early studies focus on supervised learning, where only labeled data are required to train the classification model (Pang et al., 2002), recent studies devote more and more to reduce the heavy dependence on the large amount of labeled data by exploiting semi-supervised learning approaches, such as co-training (Wan, 2009; Li et al., 2011), label propagation (Sindhwani and Melville, 2008), and deep learning (Zhou et al., 2013), to sentiment classification. Empirical evaluation on various domains demonstrates the effectiveness of the unlabeled data in enhancing the performance  * Corresponding author 27 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 27–31, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics seen as an extension of the famous sta"
P15-2005,P09-1079,0,0.0173928,"ata as L  {( xi , yi )}inL1 where xi R d is the d dimensional input vector, and yi is its output label. The unlabeled data in the target domain is denoted as U  {( xk )}nkU1 . Suppose l semi is a semi-supervised Related Work Early studies on sentiment classification mainly focus on supervised learning methods with algorithm designing and feature engineering (Pang et al., 2002; Cui et al., 2006; Riloff et al., 2006; Li et al., 2009). Recently, most studies on sentiment classification aim to improve the performance by exploiting unlabeled data in two main aspects: semi-supervised learning (Dasgupta and Ng, 2009; Wan, 2009; Li et al., 2010) and cross-domain learning (Blitzer et al. 2007; He et al. 2011; Li et al., 2013). Specifically, existing approaches to semi-supervised sentiment classification could be categorized into two main groups: bootstrappingstyle and graph-based. As for bootstrapping-style approaches, Wan (2009) considers two different languages as two views and applies co-training to conduct semi-supervised sentiment classification. Similarly, Li et al. (2010) propose two views, named personal and impersonal views, and apply co-training to use unlabeled data in a monolingual corpus. More"
P15-2005,P11-1013,0,0.351351,"label. The unlabeled data in the target domain is denoted as U  {( xk )}nkU1 . Suppose l semi is a semi-supervised Related Work Early studies on sentiment classification mainly focus on supervised learning methods with algorithm designing and feature engineering (Pang et al., 2002; Cui et al., 2006; Riloff et al., 2006; Li et al., 2009). Recently, most studies on sentiment classification aim to improve the performance by exploiting unlabeled data in two main aspects: semi-supervised learning (Dasgupta and Ng, 2009; Wan, 2009; Li et al., 2010) and cross-domain learning (Blitzer et al. 2007; He et al. 2011; Li et al., 2013). Specifically, existing approaches to semi-supervised sentiment classification could be categorized into two main groups: bootstrappingstyle and graph-based. As for bootstrapping-style approaches, Wan (2009) considers two different languages as two views and applies co-training to conduct semi-supervised sentiment classification. Similarly, Li et al. (2010) propose two views, named personal and impersonal views, and apply co-training to use unlabeled data in a monolingual corpus. More recently, Gao et al. (2014) propose a feature subspace-based self-training to semi-supervis"
P15-2005,P10-1043,1,0.861229,"e xi R d is the d dimensional input vector, and yi is its output label. The unlabeled data in the target domain is denoted as U  {( xk )}nkU1 . Suppose l semi is a semi-supervised Related Work Early studies on sentiment classification mainly focus on supervised learning methods with algorithm designing and feature engineering (Pang et al., 2002; Cui et al., 2006; Riloff et al., 2006; Li et al., 2009). Recently, most studies on sentiment classification aim to improve the performance by exploiting unlabeled data in two main aspects: semi-supervised learning (Dasgupta and Ng, 2009; Wan, 2009; Li et al., 2010) and cross-domain learning (Blitzer et al. 2007; He et al. 2011; Li et al., 2013). Specifically, existing approaches to semi-supervised sentiment classification could be categorized into two main groups: bootstrappingstyle and graph-based. As for bootstrapping-style approaches, Wan (2009) considers two different languages as two views and applies co-training to conduct semi-supervised sentiment classification. Similarly, Li et al. (2010) propose two views, named personal and impersonal views, and apply co-training to use unlabeled data in a monolingual corpus. More recently, Gao et al. (2014)"
P15-2005,P09-1078,1,0.73468,"hm aims to learn a classifier from a small scale of labeled samples, named initial labeled data, with a large number of unlabeled samples. In the sequel, we refer the labeled data as L  {( xi , yi )}inL1 where xi R d is the d dimensional input vector, and yi is its output label. The unlabeled data in the target domain is denoted as U  {( xk )}nkU1 . Suppose l semi is a semi-supervised Related Work Early studies on sentiment classification mainly focus on supervised learning methods with algorithm designing and feature engineering (Pang et al., 2002; Cui et al., 2006; Riloff et al., 2006; Li et al., 2009). Recently, most studies on sentiment classification aim to improve the performance by exploiting unlabeled data in two main aspects: semi-supervised learning (Dasgupta and Ng, 2009; Wan, 2009; Li et al., 2010) and cross-domain learning (Blitzer et al. 2007; He et al. 2011; Li et al., 2013). Specifically, existing approaches to semi-supervised sentiment classification could be categorized into two main groups: bootstrappingstyle and graph-based. As for bootstrapping-style approaches, Wan (2009) considers two different languages as two views and applies co-training to conduct semi-supervised se"
P15-2125,P13-2037,0,0.0572364,"n social media has become of great value to market predictions and analysis (Liu et al., 2013; Lee et al., 2014). Previous researches on emotion analysis have mainly focused on emotion expressions in monolingual texts (Chen et al., 2010; Lee et al., 2013a). However, in informal settings such as micro-blogs, emotions are often expressed by a mixture of different natural languages. Such a mixture of language is called codeswitching. Specifically, code-switching text is defined as text that contains more than one language (code). It is a common phenomenon in multilingual communities (Auer, 1999; Adel et al., 2013). For instance, [E1-E3] are three examples of codeswitching emotional posts containing both Chi*Corresponding author 763 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 763–768, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics dentify the code-switching posts. After removing posts containing noise and advertisements, we extract 4,195 code-switching posts from the dataset for emotion annotation. Five basic emotions are annotate"
P15-2125,J90-2002,0,0.86105,"arch. We systematically explore both the bilingual and sentimental information to detect emotions in code-switching posts. Moreover, we use a term-document bipartite graph to incorporate these two kinds of information, and propose a Label Propagation (LP) based approach to learn and predict emotion in code-switching texts. In the following subsections, we will discuss these issues one by one. 4.1 The candidate target sentences made up of a sequence of the optional target words are ranked by the language model. The output will be generated only if it reaches the maximum probability as follows (Brown et al., 1990; Zhao et al., 2009): c = argmax 4.2 For using bilingual information, a word-by-word statistical machine translation strategy is adopted to translate words from English into Chinese. For better clarity, a word-based decoding, which adopts a log-linear framework as in (Och and Ney, 2002) with translation model and language model being the only features, is used: P (1) h1 (c, e) = log(pγ (c|e)) (2) p(wc ) (4) Sentimental Information Sentimental information is very useful in emotion detection (Gao et al., 2013). In this paper, we extract polarity from both Chinese and English texts to ensure text"
P15-2125,C10-1021,1,0.941793,"ard approach to handle this issue is to translate texts from one language into another. Since Chinese is the dominant language in our data set, a word-by-word statistical machine translation strategy (Zhao et al., 2009) is adopted to translate English words into Chinese. Additionally, as text from micro-blogs is informal, With the rapid development of Web 2.0, emotion analysis in social media has become of great value to market predictions and analysis (Liu et al., 2013; Lee et al., 2014). Previous researches on emotion analysis have mainly focused on emotion expressions in monolingual texts (Chen et al., 2010; Lee et al., 2013a). However, in informal settings such as micro-blogs, emotions are often expressed by a mixture of different natural languages. Such a mixture of language is called codeswitching. Specifically, code-switching text is defined as text that contains more than one language (code). It is a common phenomenon in multilingual communities (Auer, 1999; Adel et al., 2013). For instance, [E1-E3] are three examples of codeswitching emotional posts containing both Chi*Corresponding author 763 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7"
P15-2125,lee-etal-2014-annotating,1,0.859796,"bridging the gap between different languages becomes essential for emotion detection in code-switching texts. A straightforward approach to handle this issue is to translate texts from one language into another. Since Chinese is the dominant language in our data set, a word-by-word statistical machine translation strategy (Zhao et al., 2009) is adopted to translate English words into Chinese. Additionally, as text from micro-blogs is informal, With the rapid development of Web 2.0, emotion analysis in social media has become of great value to market predictions and analysis (Liu et al., 2013; Lee et al., 2014). Previous researches on emotion analysis have mainly focused on emotion expressions in monolingual texts (Chen et al., 2010; Lee et al., 2013a). However, in informal settings such as micro-blogs, emotions are often expressed by a mixture of different natural languages. Such a mixture of language is called codeswitching. Specifically, code-switching text is defined as text that contains more than one language (code). It is a common phenomenon in multilingual communities (Auer, 1999; Adel et al., 2013). For instance, [E1-E3] are three examples of codeswitching emotional posts containing both Ch"
P15-2125,C12-1102,0,0.0621802,"; Volkova et al., 2012; Lee et al., 2014). Moreover, emotion classification is one of the most important tasks in emotion analysis, while emotion classification aims to classify text into multiple emotion categories (Chen et al., 2010; Liu et al., 2013). Despite a growing body of research on emotion analysis, little has been done on the analysis of emotion in code-switching due to the complexities of processing two languages at the same time. Besides, although several research studies have focused on analyzing bilingual (Wan, 2009; Lu et al., 2011; Tang et al., 2014) and code-switching texts (Li and Fung, 2012; Ling et al., 2013; Lignos and Marcus, 2013), none of them has studied the multilingual code-switching issues in emotion detection. This research area is especially crucial when public emotions are mostly expressed in the free-form text on the Internet. 3 Figure 1: Distribution of Emotions and Languages The joint distribution between emotions and caused languages is illustrated in Figure 1. The Y-axis of the figure presents the conditional probability of a post expressing the emotion ei given that lj is the caused language, p(ei |lj ). It is suggested in Figure 2 that: 1) happiness occurs mor"
P15-2125,P13-1018,0,0.0517095,"012; Lee et al., 2014). Moreover, emotion classification is one of the most important tasks in emotion analysis, while emotion classification aims to classify text into multiple emotion categories (Chen et al., 2010; Liu et al., 2013). Despite a growing body of research on emotion analysis, little has been done on the analysis of emotion in code-switching due to the complexities of processing two languages at the same time. Besides, although several research studies have focused on analyzing bilingual (Wan, 2009; Lu et al., 2011; Tang et al., 2014) and code-switching texts (Li and Fung, 2012; Ling et al., 2013; Lignos and Marcus, 2013), none of them has studied the multilingual code-switching issues in emotion detection. This research area is especially crucial when public emotions are mostly expressed in the free-form text on the Internet. 3 Figure 1: Distribution of Emotions and Languages The joint distribution between emotions and caused languages is illustrated in Figure 1. The Y-axis of the figure presents the conditional probability of a post expressing the emotion ei given that lj is the caused language, p(ei |lj ). It is suggested in Figure 2 that: 1) happiness occurs more frequently than o"
P15-2125,P13-2091,1,0.940989,"selves.) [E2] • ˜é{Ò´/Oœv kk5 §Ø O â´1n ö0""shit! (A quote, to my great disgust, is ”There’s no staking claims in a relationship based on who got there first - the one who isn.t loved is the true third party.” Shit!) [E3] ù o ‡ y - : { "" "" "" ¶ ‚holdØ Ø4 Bœœœ (The so-called ”highlighting”...we can’t hold it anymore.) Introduction It is more difficult to detect emotions in codeswitching texts than in monolingual ones since emotions in code-switching posts can be expressed through one or two languages. Hence, traditional automatic emotion detection methods which simply consider monolingual texts (Liu et al., 2013; Lee et al., 2013a) would not be readily applicable. The key issue of emotion detection in codeswitching texts is to deal with the emotions expressed through different languages. Thus bridging the gap between different languages becomes essential for emotion detection in code-switching texts. A straightforward approach to handle this issue is to translate texts from one language into another. Since Chinese is the dominant language in our data set, a word-by-word statistical machine translation strategy (Zhao et al., 2009) is adopted to translate English words into Chinese. Additionally, as te"
P15-2125,P11-1033,0,0.0210093,"s task is about emotion resource construction (Xu et al., 2010; Volkova et al., 2012; Lee et al., 2014). Moreover, emotion classification is one of the most important tasks in emotion analysis, while emotion classification aims to classify text into multiple emotion categories (Chen et al., 2010; Liu et al., 2013). Despite a growing body of research on emotion analysis, little has been done on the analysis of emotion in code-switching due to the complexities of processing two languages at the same time. Besides, although several research studies have focused on analyzing bilingual (Wan, 2009; Lu et al., 2011; Tang et al., 2014) and code-switching texts (Li and Fung, 2012; Ling et al., 2013; Lignos and Marcus, 2013), none of them has studied the multilingual code-switching issues in emotion detection. This research area is especially crucial when public emotions are mostly expressed in the free-form text on the Internet. 3 Figure 1: Distribution of Emotions and Languages The joint distribution between emotions and caused languages is illustrated in Figure 1. The Y-axis of the figure presents the conditional probability of a post expressing the emotion ei given that lj is the caused language, p(ei"
P15-2125,P02-1038,0,0.116703,"dict emotion in code-switching texts. In the following subsections, we will discuss these issues one by one. 4.1 The candidate target sentences made up of a sequence of the optional target words are ranked by the language model. The output will be generated only if it reaches the maximum probability as follows (Brown et al., 1990; Zhao et al., 2009): c = argmax 4.2 For using bilingual information, a word-by-word statistical machine translation strategy is adopted to translate words from English into Chinese. For better clarity, a word-based decoding, which adopts a log-linear framework as in (Och and Ney, 2002) with translation model and language model being the only features, is used: P (1) h1 (c, e) = log(pγ (c|e)) (2) p(wc ) (4) Sentimental Information Sentimental information is very useful in emotion detection (Gao et al., 2013). In this paper, we extract polarity from both Chinese and English texts to ensure text of similar polarity will be connected. In this paper, both Chinese5 and English6 sentimental lexicons are employed to identify candidate opinion expressions by searching the occurrences of negative and positive expressions in text, and predict the polarity of both Chinese and English t"
P15-2125,D09-1150,0,0.258839,"Missing"
P15-2125,P02-1053,0,0.0201887,"l being the only features, is used: P (1) h1 (c, e) = log(pγ (c|e)) (2) p(wc ) (4) Sentimental Information Sentimental information is very useful in emotion detection (Gao et al., 2013). In this paper, we extract polarity from both Chinese and English texts to ensure text of similar polarity will be connected. In this paper, both Chinese5 and English6 sentimental lexicons are employed to identify candidate opinion expressions by searching the occurrences of negative and positive expressions in text, and predict the polarity of both Chinese and English texts through the word-counting approach (Turney, 2002). Bilingual Information exp [ 2i=1 λi hi (c, e)] P (c|e) = P P2 c exp [ i=1 λi hi (c, e)] Y 4.3 where LP-based Emotion Detection For the knowledge of bilingual and sentimental information to be well incorporated, we use a termdocument bipartite graph to incorporate the information, and propose a label propagation based approach to learn and predict emotion in codeswitching texts. The input of the LP algorithm is a graph describing the relationship between each sample pair in the labeled and test data (Sindhwani and Melville, 2008; Li et al., 2013). In a bipartite graph, the nodes consist of tw"
P15-2125,E12-1031,0,0.0275893,"pose a label propagation (Zhu and Ghahramani, 2002) based approach to learn and predict in the graph. Specially, the label information between Chinese and English texts would be propagated through the bipartite graph by word-document relations, bilingual information, and sentiment information. Evaluation of the data set indicates the importance of the task and the effectiveness of our proposed approach. 2 Related Work Emotion analysis has been a hot research topic in NLP in the last decade. One main group of related studies on this task is about emotion resource construction (Xu et al., 2010; Volkova et al., 2012; Lee et al., 2014). Moreover, emotion classification is one of the most important tasks in emotion analysis, while emotion classification aims to classify text into multiple emotion categories (Chen et al., 2010; Liu et al., 2013). Despite a growing body of research on emotion analysis, little has been done on the analysis of emotion in code-switching due to the complexities of processing two languages at the same time. Besides, although several research studies have focused on analyzing bilingual (Wan, 2009; Lu et al., 2011; Tang et al., 2014) and code-switching texts (Li and Fung, 2012; Lin"
P15-2125,C10-1136,0,0.115476,"formation and propose a label propagation (Zhu and Ghahramani, 2002) based approach to learn and predict in the graph. Specially, the label information between Chinese and English texts would be propagated through the bipartite graph by word-document relations, bilingual information, and sentiment information. Evaluation of the data set indicates the importance of the task and the effectiveness of our proposed approach. 2 Related Work Emotion analysis has been a hot research topic in NLP in the last decade. One main group of related studies on this task is about emotion resource construction (Xu et al., 2010; Volkova et al., 2012; Lee et al., 2014). Moreover, emotion classification is one of the most important tasks in emotion analysis, while emotion classification aims to classify text into multiple emotion categories (Chen et al., 2010; Liu et al., 2013). Despite a growing body of research on emotion analysis, little has been done on the analysis of emotion in code-switching due to the complexities of processing two languages at the same time. Besides, although several research studies have focused on analyzing bilingual (Wan, 2009; Lu et al., 2011; Tang et al., 2014) and code-switching texts ("
P15-2125,P09-1027,0,0.0798541,"dies on this task is about emotion resource construction (Xu et al., 2010; Volkova et al., 2012; Lee et al., 2014). Moreover, emotion classification is one of the most important tasks in emotion analysis, while emotion classification aims to classify text into multiple emotion categories (Chen et al., 2010; Liu et al., 2013). Despite a growing body of research on emotion analysis, little has been done on the analysis of emotion in code-switching due to the complexities of processing two languages at the same time. Besides, although several research studies have focused on analyzing bilingual (Wan, 2009; Lu et al., 2011; Tang et al., 2014) and code-switching texts (Li and Fung, 2012; Ling et al., 2013; Lignos and Marcus, 2013), none of them has studied the multilingual code-switching issues in emotion detection. This research area is especially crucial when public emotions are mostly expressed in the free-form text on the Internet. 3 Figure 1: Distribution of Emotions and Languages The joint distribution between emotions and caused languages is illustrated in Figure 1. The Y-axis of the figure presents the conditional probability of a post expressing the emotion ei given that lj is the cause"
P15-2125,P09-1007,1,0.915067,"omatic emotion detection methods which simply consider monolingual texts (Liu et al., 2013; Lee et al., 2013a) would not be readily applicable. The key issue of emotion detection in codeswitching texts is to deal with the emotions expressed through different languages. Thus bridging the gap between different languages becomes essential for emotion detection in code-switching texts. A straightforward approach to handle this issue is to translate texts from one language into another. Since Chinese is the dominant language in our data set, a word-by-word statistical machine translation strategy (Zhao et al., 2009) is adopted to translate English words into Chinese. Additionally, as text from micro-blogs is informal, With the rapid development of Web 2.0, emotion analysis in social media has become of great value to market predictions and analysis (Liu et al., 2013; Lee et al., 2014). Previous researches on emotion analysis have mainly focused on emotion expressions in monolingual texts (Chen et al., 2010; Lee et al., 2013a). However, in informal settings such as micro-blogs, emotions are often expressed by a mixture of different natural languages. Such a mixture of language is called codeswitching. Spe"
P15-2125,D09-1102,1,0.884445,"Missing"
P17-1064,J07-2003,0,0.0490256,"Figure 5 suggest that the Mixed RNN encoder is the simplest. Moreover, comparing to conventional NMT encoders, the difference lies only in the length of the input sequence. Statistics on our training data reveal that the Mixed RNN encoder approximately triples the input sequence length compared to conventional NMT encoders. 4 Experimentation We have presented our approaches to incorporating the source syntax into NMT encoders. In this section, we evaluate their effectiveness on Chinese-to-English translation. 4.1 • cdec (Dyer et al., 2010): an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data.7 Experimental Settings Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5M English words respectively.4 We choose NIST MT 06 dataset (1664 sentence pairs) as our development set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respectively) as our test sets.5 To get the source syntax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser 6"
P17-1064,W14-4012,0,0.0168517,"Missing"
P17-1064,D14-1179,0,0.0247785,"Missing"
P17-1064,D16-1257,0,0.0120836,"any two different words. However, considering the lack of efficient way to directly model structural information, an alternative way is to linearize the phrase parse tree into a sequence of structural labels and learn the structural context through the sequence. For example, Figure 3(c) shows the structural label sequence of Figure 3(b) in a simple way following a depth-first traversal order. Note that linearizing a parse tree in a depth-first traversal order into a sequence of structural labels has also been widely adopted in recent advances in neural syntactic parsing (Vinyals et al., 2015; Choe and Charniak, 2016), suggesting that the linearized sequence can be viewed as an alternative to its tree structure.2 There is no doubt that the structural label sequence is much longer than its word sequence. In order to obtain the structural label annotation vector for wi in word sequence, we simply look for wi ’s part-of-speech (POS) tag in the label sequence and view the tag’s annotation vector as wi ’s label annotation vector. This is because wi ’s POS tag location can also represent wi ’s location in the parse tree. For example, in Figure 3, word w1 in (a) maps to l3 in (c) since l3 is the POS tag of w1 . L"
P17-1064,P10-4002,0,0.0137269,"ur method with two state-of-theart models of SMT and NMT: • Figure 4 and Figure 5 suggest that the Mixed RNN encoder is the simplest. Moreover, comparing to conventional NMT encoders, the difference lies only in the length of the input sequence. Statistics on our training data reveal that the Mixed RNN encoder approximately triples the input sequence length compared to conventional NMT encoders. 4 Experimentation We have presented our approaches to incorporating the source syntax into NMT encoders. In this section, we evaluate their effectiveness on Chinese-to-English translation. 4.1 • cdec (Dyer et al., 2010): an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data.7 Experimental Settings Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5M English words respectively.4 We choose NIST MT 06 dataset (1664 sentence pairs) as our development set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respectively) as our test sets.5 To get the source syntax for sentences on th"
P17-1064,P16-1078,0,0.160511,"Missing"
P17-1064,P16-1162,0,0.0130766,"Missing"
P17-1064,W15-3014,0,0.0140583,"n-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT exp"
P17-1064,P08-1066,0,0.0134567,"improves translation by integrating various kinds of syntactic knowledge (Liu et al., 2006; Marton and Resnik, 2008; Introduction Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) ∗ VV Work done at Huawei Noah’s Ark Lab, HongKong. 688 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1064 Shen et al., 2008; Li et al., 2013). While it is yet to be seen how syntax can benefit NMT effectively, we find that translations of NMT sometimes fail to well respect source syntax. Figure 1 (a) shows a Chinese-to-English translation example of NMT. In this example, the NMT seq2seq model incorrectly translates the Chinese noun phrase (i.e., 新 生/xinsheng 银 行/yinhang) into a discontinuous phrase in English (i.e., new ... bank) due to the failure of capturing the internal syntactic structure in the input Chinese sentence. Statistics on our development set show that one forth of Chinese noun phrases are translate"
P17-1064,W04-3250,0,0.104681,"Missing"
P17-1064,D16-1159,0,0.168192,"e syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translati"
P17-1064,N13-1060,1,0.934388,"Missing"
P17-1064,Q17-1007,1,0.0517642,"Missing"
P17-1064,P06-1077,0,0.0689507,"training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translation accuracy. In principle, syntax is a promising avenue for translation modeling. This has been verified by tremendous encouraging studies on syntaxbased SMT that substantially improves translation by integrating various kinds of syntactic knowledge (Liu et al., 2006; Marton and Resnik, 2008; Introduction Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) ∗ VV Work done at Huawei Noah’s Ark Lab, HongKong. 688 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1064 Shen et al., 2008; Li et al., 2013). While it is yet to be seen how syntax can benefit NMT"
P17-1064,P16-1008,1,0.860335,"we group sentences of similar lengths together and compute BLEU scores. Figure 6 presents the BLEU scores over different lengths of input sentences. It shows that Mixed RNN system outperforms RNNSearch over sentences with all different lengths. It also shows that the performance drops substantially 693 System RNNSearch Mixed RNN AER 50.1 47.9 System RNNSearch Table 2: Evaluation of alignment quality. The lower the score, the better the alignment quality. when the length of input sentences increases. This performance trend over the length is consistent with the findings in (Cho et al., 2014a; Tu et al., 2016, 2017a). We also observe that the NMT systems perform surprisingly bad on sentences over 50 in length, especially compared to the performance of SMT system (i.e., cdec). We think that the bad behavior of NMT systems towards long sentences (e.g., length of 50) is due to the following two reasons: (1) the maximum source sentence length limit is set as 50 in training, 9 making the learned models not ready to translate sentences over the maximum length limit; (2) NMT systems tend to stop early for long input sentences. 5.2 Mixed RNN Cont. 57.3 59.8 47.3 54.0 58.1 63.3 63.1 54.5 56.2 60.4 Dis. 33."
P17-1064,2015.iwslt-evaluation.11,0,0.0637674,"ves is provided to reveal how source syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syn"
P17-1064,D15-1166,0,0.0233336,"vided to reveal how source syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syn"
P17-1064,P08-1114,0,0.0438962,"m to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translation accuracy. In principle, syntax is a promising avenue for translation modeling. This has been verified by tremendous encouraging studies on syntaxbased SMT that substantially improves translation by integrating various kinds of syntactic knowledge (Liu et al., 2006; Marton and Resnik, 2008; Introduction Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) ∗ VV Work done at Huawei Noah’s Ark Lab, HongKong. 688 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1064 Shen et al., 2008; Li et al., 2013). While it is yet to be seen how syntax can benefit NMT effectively, we find tha"
P17-1064,D16-1249,0,0.0283303,"Missing"
P17-1064,J03-1002,0,0.00476306,"Word Alignment Due to the capability of carrying syntactic information in source annotation vectors, we conjecture that our model with source syntax is also beneficial for alignment. To test this hypothesis, we carry out experiments of the word alignment task on the evaluation dataset from Liu and Sun (2015), which contains 900 manually aligned Chinese-English sentence pairs. We force the decoder to output reference translations, as to get automatic alignments between input sentences and their reference translations. To evaluate alignment performance, we report the alignment error rate (AER) (Och and Ney, 2003) in Table 2. Table 2 shows that source syntax information improves the attention model as expected by maintaining an annotation vector summarizing structural information on each source word. 5.3 XP PP NP CP QP ALL PP NP CP QP ALL 5.4 Analysis on Over Translation To estimate the over translation generated by NMT, we propose ratio of over translation (ROT): Analysis on Phrase Alignment ROT = The above subsection examines the alignment performance at the word level. In this subsection, we turn to phrase alignment analysis by moving from word unit to phrase unit. Given a source phrase XP, we use w"
P17-1064,P02-1040,0,0.13215,"Missing"
P17-1064,N07-1051,0,0.0913188,"Missing"
P17-1064,W16-2209,0,0.284718,"tion, over translation usually happens along with the disrespect of syntax which results in the repeated translation of the same source words in multiple positions of the target sentence. In this paper we are not aiming at solving any particular issue, either the discontinuous translation or the over translation. Alternatively, we address how to incorporate explicitly the source syntax to improve the NMT translation accuracy with the expectation of alleviating the issues above in general. Specifically, rather than directly assigning each source word with manually designed syntactic labels, as Sennrich and Haddow (2016) do, we linearize a phrase parse tree into a structural label sequence and let the model automatically learn useful syntactic information. On the basis, we systematically propose and compare several different approaches to incorporating the label sequence into the seq2seq NMT model. Experimentation on Chinese-to-English translation demonstrates that all proposed approaches are able to improve the translation accuracy. 2 h h1 h1 yi hm h hm si-1 h1 h1 x1 x2 ….. xm (a) encoder Atten ci MLP si RNN yi-1 (b) decoder Figure 2: Attention-based NMT model. mulated using a pair of neural networks, i.e.,"
P18-1048,doddington-etal-2004-automatic,0,0.569132,"contrary, D the parameters θdˇ to minimize the loss: Task Deﬁnition The task of event detection is to determine whether there is one or more event triggers in a sentence. Trigger is deﬁned as a token or nugget that best signals the occurrence of an event. If successfully identiﬁed, a trigger is required to be assigned a tag to indicate the event type: Input: Either its bad or good Output: its &lt;trigger&gt;; Marry &lt;type&gt; We formalize the event detection problem as a multi-class classiﬁcation problem. Given a sentence, we classify every token of the sentence into one of the predeﬁned event classes (Doddington et al., 2004) or non-trigger class. 3 Self-Regulated Learning (SELF) SELF is a double-channel model (Figure 1), consisted of a cooperative network (Islam et al., 2003) and a generative adversarial net (GAN) (Goodfellow et al., 2014). A memory suppressor S is used to regulate communication between the channels. 3.1 θgˇ = argmax L(ˆ y , y) (2) y , y) θdˇ = argmin L(ˆ (3) Numerous studies have conﬁrmed that the twoˇ and D ˇ to implayer minmax game enables both G prove their methods (Goodfellow et al., 2014; Liu and Tuzel, 2016; Huang et al., 2017). Cooperative Network In channel 1, the generator G is speciﬁed"
P18-1048,W06-0901,0,0.869053,"networks that consist of a Bi-LSTM and a CNN. Besides, we compare our model with Liu et al (2016b)’s artiﬁcial neural networks (ANNs), Liu et al (2017b)’s attention-based ANN (ANN-S2) and Chen et al (2017)’s DM-CNN∗ . The models recently have become popular because, although simple in structure, they are very analytic by learning from richer event examples, such as those in Experimentation 5.1 Resource and Experimental Datasets We test the presented model on the ACE 2005 corpus. The corpus is annotated with single-token event triggers and has 33 predeﬁned event types (Doddington et al., 2004; Ahn, 2006), along with one class “None” for the non-trigger tokens, constitutes a 34-class classiﬁcation problem. For comparison purpose, we use the corpus in the traditional way, randomly selecting 30 articles in English from different genres as the development set, and utilizing a separate set of 40 English newswire articles as the test set. The remaining 529 English articles are used as the training set. 5.2 Compared Systems Hyperparameter Settings The word embeddings are initialized with the 300dimensional real-valued vectors. We follow Chen et al (2015) and Feng et al (2016) to pre-train the embedd"
P18-1048,W06-1615,0,0.223765,"Missing"
P18-1048,P16-2011,0,0.639539,"here W ∈ R4d×(d+e) and b ∈ R4d are parameters of afﬁne transformation; σ refers to the logistic sigmoid function and  denotes element-wise multiplication. The output functions of both the generators in ˇ can be boiled down to the SELF, i.e., G and G, d output gate ot ∈ R of the LSTM cell: Recurrent Models for SELF RNN with long short-term memory (abbr., LSTM) is adopted due to the superior performance in a variety of NLP tasks (Liu et al., 2016a; Lin et al., 2017; Liu et al., 2017a). Furthermore, the bidirectional LSTM (Bi-LSTM) architecture (Schuster and Paliwal, 1997; Ghaeini et al., 2016; Feng et al., 2016) is strictly followed. This architecture enables modeling of the semantics of a token with both the preceding and following contexts. 4.1 ht = ot  tanh(ct ) ot = LST M (xt ; θ) (8) where, the function LSTM (·;·) is a shorthand for Eq. (5-7) and θ represents all the parameters of ˇ θ are initialized with the LSTM. For both G and G, same values in experiments. But due to the distinct ˇ (diligence or makingtraining goals of G and G trouble), the values of the parameters in the two LSTM based Generator Given a sentence, we follow Chen et al (2015) to take all the tokens of the whole sentence as t"
P18-1048,C16-1309,0,0.0428892,"Missing"
P18-1048,R15-1010,0,0.0405548,"Missing"
P18-1048,P16-2060,0,0.671957,"ct−1  ft ct =  (7) where W ∈ R4d×(d+e) and b ∈ R4d are parameters of afﬁne transformation; σ refers to the logistic sigmoid function and  denotes element-wise multiplication. The output functions of both the generators in ˇ can be boiled down to the SELF, i.e., G and G, d output gate ot ∈ R of the LSTM cell: Recurrent Models for SELF RNN with long short-term memory (abbr., LSTM) is adopted due to the superior performance in a variety of NLP tasks (Liu et al., 2016a; Lin et al., 2017; Liu et al., 2017a). Furthermore, the bidirectional LSTM (Bi-LSTM) architecture (Schuster and Paliwal, 1997; Ghaeini et al., 2016; Feng et al., 2016) is strictly followed. This architecture enables modeling of the semantics of a token with both the preceding and following contexts. 4.1 ht = ot  tanh(ct ) ot = LST M (xt ; θ) (8) where, the function LSTM (·;·) is a shorthand for Eq. (5-7) and θ represents all the parameters of ˇ θ are initialized with the LSTM. For both G and G, same values in experiments. But due to the distinct ˇ (diligence or makingtraining goals of G and G trouble), the values of the parameters in the two LSTM based Generator Given a sentence, we follow Chen et al (2015) to take all the tokens of the"
P18-1048,R15-1011,0,0.214348,"Missing"
P18-1048,P17-1038,0,0.102877,"and Grishman, 2015), the non-consecutive Ngrams based CNN (NC-CNN) (Nguyen and Grishman, 2016) and the CNN that is assembled with a dynamic multi-pooling layer (DM-CNN) (Chen et al., 2015). Others include Ghaeini et al (2016)’s forward-backward recurrent neural network (FBRNN) which is developed using gated recurrent units (GRU), Nguyen et al (2016)’s bidirectional RNN (Bi-RNN) and Feng et al (2016)’s Hybrid networks that consist of a Bi-LSTM and a CNN. Besides, we compare our model with Liu et al (2016b)’s artiﬁcial neural networks (ANNs), Liu et al (2017b)’s attention-based ANN (ANN-S2) and Chen et al (2017)’s DM-CNN∗ . The models recently have become popular because, although simple in structure, they are very analytic by learning from richer event examples, such as those in Experimentation 5.1 Resource and Experimental Datasets We test the presented model on the ACE 2005 corpus. The corpus is annotated with single-token event triggers and has 33 predeﬁned event types (Doddington et al., 2004; Ahn, 2006), along with one class “None” for the non-trigger tokens, constitutes a 34-class classiﬁcation problem. For comparison purpose, we use the corpus in the traditional way, randomly selecting 30 art"
P18-1048,P15-1017,0,0.827042,"chuster and Paliwal, 1997; Ghaeini et al., 2016; Feng et al., 2016) is strictly followed. This architecture enables modeling of the semantics of a token with both the preceding and following contexts. 4.1 ht = ot  tanh(ct ) ot = LST M (xt ; θ) (8) where, the function LSTM (·;·) is a shorthand for Eq. (5-7) and θ represents all the parameters of ˇ θ are initialized with the LSTM. For both G and G, same values in experiments. But due to the distinct ˇ (diligence or makingtraining goals of G and G trouble), the values of the parameters in the two LSTM based Generator Given a sentence, we follow Chen et al (2015) to take all the tokens of the whole sentence as the in517 2 where,  · F denotes the squared Frobenius norm (Bousmalis et al., 2016), which is used to calculate the similarity between matrices. It is noteworthy that the feature vectors a generator outputs are required to serve as the rows in the matrix, deployed in a top-down manner and arranged in the order in which they are generated. For example, the feature vector og,t the generator G outputs at the time t needs to be placed in the t-th row of the matrix Og . At the very beginning of the measurement, the similarity between every feature"
P18-1048,P16-1098,0,0.0606388,"structure as the generators. And both the discriminators are implemented as a fullyconnected layer followed by a softmax layer. 4 (6) ct  it + ct−1  ft ct =  (7) where W ∈ R4d×(d+e) and b ∈ R4d are parameters of afﬁne transformation; σ refers to the logistic sigmoid function and  denotes element-wise multiplication. The output functions of both the generators in ˇ can be boiled down to the SELF, i.e., G and G, d output gate ot ∈ R of the LSTM cell: Recurrent Models for SELF RNN with long short-term memory (abbr., LSTM) is adopted due to the superior performance in a variety of NLP tasks (Liu et al., 2016a; Lin et al., 2017; Liu et al., 2017a). Furthermore, the bidirectional LSTM (Bi-LSTM) architecture (Schuster and Paliwal, 1997; Ghaeini et al., 2016; Feng et al., 2016) is strictly followed. This architecture enables modeling of the semantics of a token with both the preceding and following contexts. 4.1 ht = ot  tanh(ct ) ot = LST M (xt ; θ) (8) where, the function LSTM (·;·) is a shorthand for Eq. (5-7) and θ represents all the parameters of ˇ θ are initialized with the LSTM. For both G and G, same values in experiments. But due to the distinct ˇ (diligence or makingtraining goals of G and"
P18-1048,P11-1113,1,0.958583,"Soochow University No.1, Shizi ST, Suzhou, China, 215006 {tianxianer, wxchow024, jlzhang05}@gmail.com {qmzhu, gdzhou}@suda.edu.cn Abstract 2016; Feng et al., 2016; Liu et al., 2017b; Chen et al., 2017), which allows semantics of event mentions (trigger plus context) to be encoded in a high-dimensional latent feature space. This facilitates the learning of deep-level semantics. Besides, the use of neural networks not only strengthens current supervised classiﬁcation of events but alleviates the complexity of feature engineering. However, compared to the earlier study (Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), in which the features are carefully designed by experts, the neural network based methods suffer more from spurious features. Here, spurious feature is speciﬁed as the latent information which looks like the semantically related information to an event, but actually not (Liu et al., 2017a). For example, in the following sample, the semantic information of the word “prison” most probably enables spurious features to come into being, because the word often co-occurs with the trigger ”taken” to evoke an Arrest-jail event instead of the ground-truth event Transport: Due to the"
P18-1048,P17-1001,0,0.0687126,"Missing"
P18-1048,P16-1201,0,0.210946,"structure as the generators. And both the discriminators are implemented as a fullyconnected layer followed by a softmax layer. 4 (6) ct  it + ct−1  ft ct =  (7) where W ∈ R4d×(d+e) and b ∈ R4d are parameters of afﬁne transformation; σ refers to the logistic sigmoid function and  denotes element-wise multiplication. The output functions of both the generators in ˇ can be boiled down to the SELF, i.e., G and G, d output gate ot ∈ R of the LSTM cell: Recurrent Models for SELF RNN with long short-term memory (abbr., LSTM) is adopted due to the superior performance in a variety of NLP tasks (Liu et al., 2016a; Lin et al., 2017; Liu et al., 2017a). Furthermore, the bidirectional LSTM (Bi-LSTM) architecture (Schuster and Paliwal, 1997; Ghaeini et al., 2016; Feng et al., 2016) is strictly followed. This architecture enables modeling of the semantics of a token with both the preceding and following contexts. 4.1 ht = ot  tanh(ct ) ot = LST M (xt ; θ) (8) where, the function LSTM (·;·) is a shorthand for Eq. (5-7) and θ represents all the parameters of ˇ θ are initialized with the LSTM. For both G and G, same values in experiments. But due to the distinct ˇ (diligence or makingtraining goals of G and"
P18-1048,P17-1164,0,0.519794,"t feature space. This facilitates the learning of deep-level semantics. Besides, the use of neural networks not only strengthens current supervised classiﬁcation of events but alleviates the complexity of feature engineering. However, compared to the earlier study (Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), in which the features are carefully designed by experts, the neural network based methods suffer more from spurious features. Here, spurious feature is speciﬁed as the latent information which looks like the semantically related information to an event, but actually not (Liu et al., 2017a). For example, in the following sample, the semantic information of the word “prison” most probably enables spurious features to come into being, because the word often co-occurs with the trigger ”taken” to evoke an Arrest-jail event instead of the ground-truth event Transport: Due to the ability of encoding and mapping semantic information into a highdimensional latent feature space, neural networks have been successfully used for detecting events to a certain extent. However, such a feature space can be easily contaminated by spurious features inherent in event detection. In this paper, we"
P18-1048,N13-1090,0,0.0056182,"e” for the non-trigger tokens, constitutes a 34-class classiﬁcation problem. For comparison purpose, we use the corpus in the traditional way, randomly selecting 30 articles in English from different genres as the development set, and utilizing a separate set of 40 English newswire articles as the test set. The remaining 529 English articles are used as the training set. 5.2 Compared Systems Hyperparameter Settings The word embeddings are initialized with the 300dimensional real-valued vectors. We follow Chen et al (2015) and Feng et al (2016) to pre-train the embeddings over NYT corpus using Mikolov et al (2013)’s skip-gram tool. The entity type embeddings, as usual (Nguyen et al., 2016; Feng et al., 2016; Liu et al., 2017b), are speciﬁed as the 50dimensional real-valued vectors. They are initialized with the 32-bit ﬂoating-point values, which are all randomly sampled from the uniformly distributed values in [-1, 1]1 . We initialize other adjustable parameters of the back-propagation algorithm by randomly sampling in [-0.1, 0.1]. We follow Feng et al (2016) to set the dropout rate as 0.2 and the mini-batch size as 10. We 2 https://github.com/JoeZhouWenxuan/Self-regulationEmploying-a-Generative-Advers"
P18-1048,N16-1034,0,0.526925,"KBP evaluations for nugget and coreference detection (Hong et al., 2014, 2015; Yu et al., 2016). It is based on structured perceptron and combines the local and global features. Neural network based approaches: including the convolutional neural network (CNN) (Nguyen and Grishman, 2015), the non-consecutive Ngrams based CNN (NC-CNN) (Nguyen and Grishman, 2016) and the CNN that is assembled with a dynamic multi-pooling layer (DM-CNN) (Chen et al., 2015). Others include Ghaeini et al (2016)’s forward-backward recurrent neural network (FBRNN) which is developed using gated recurrent units (GRU), Nguyen et al (2016)’s bidirectional RNN (Bi-RNN) and Feng et al (2016)’s Hybrid networks that consist of a Bi-LSTM and a CNN. Besides, we compare our model with Liu et al (2016b)’s artiﬁcial neural networks (ANNs), Liu et al (2017b)’s attention-based ANN (ANN-S2) and Chen et al (2017)’s DM-CNN∗ . The models recently have become popular because, although simple in structure, they are very analytic by learning from richer event examples, such as those in Experimentation 5.1 Resource and Experimental Datasets We test the presented model on the ACE 2005 corpus. The corpus is annotated with single-token event trigger"
P18-1048,P13-1008,0,0.718851,"No.1, Shizi ST, Suzhou, China, 215006 {tianxianer, wxchow024, jlzhang05}@gmail.com {qmzhu, gdzhou}@suda.edu.cn Abstract 2016; Feng et al., 2016; Liu et al., 2017b; Chen et al., 2017), which allows semantics of event mentions (trigger plus context) to be encoded in a high-dimensional latent feature space. This facilitates the learning of deep-level semantics. Besides, the use of neural networks not only strengthens current supervised classiﬁcation of events but alleviates the complexity of feature engineering. However, compared to the earlier study (Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), in which the features are carefully designed by experts, the neural network based methods suffer more from spurious features. Here, spurious feature is speciﬁed as the latent information which looks like the semantically related information to an event, but actually not (Liu et al., 2017a). For example, in the following sample, the semantic information of the word “prison” most probably enables spurious features to come into being, because the word often co-occurs with the trigger ”taken” to evoke an Arrest-jail event instead of the ground-truth event Transport: Due to the ability of encodin"
P18-1048,P14-2012,0,0.035215,"Missing"
P18-1048,D14-1198,1,0.918773,"Missing"
P18-1048,P15-2060,0,0.157074,"n the development set. Grid search (Liu et al., 2017a) is used to seek for the optimal parameters. Eventually, we take the coefﬁcient λ of 0.1+3 , learning rate of 0.3 and L2 norm of 0. The source code of SELF2 to reproduce the experiments has been made publicly available. where λ is a hyper-parameter, which is used to harmonize the two losses. The min-max game is utilized for training the ygˇ, y); adversarial net in SELF: θgˇ = argmax L(ˆ ygˇ, y). θdˇ = argmin L(ˆ All the networks in SELF are trained jointly using the same batches of samples. They are trained via stochastic gradient descent (Nguyen and Grishman, 2015) with shufﬂed mini-batches and the AdaDelta update rule (Zeiler, 2012). The gradients are computed using back propagation. And regularization is implemented by a dropout (Hinton et al., 2012). 5 5.3 The state-of-the-art models proposed in the past decade are compared with ours. By taking learning framework as the criterion, we divide the models into three classes: Minimally supervised approach: is Peng et al (2016)’s MSEP-EMD. Feature based approaches: primarily including Liao and Grishman (2010)’s Cross-Event inference model, which is based on the max-entropy classiﬁcation and embeds the docu"
P18-1048,P10-1081,0,0.666678,"r Science and Technology, Soochow University No.1, Shizi ST, Suzhou, China, 215006 {tianxianer, wxchow024, jlzhang05}@gmail.com {qmzhu, gdzhou}@suda.edu.cn Abstract 2016; Feng et al., 2016; Liu et al., 2017b; Chen et al., 2017), which allows semantics of event mentions (trigger plus context) to be encoded in a high-dimensional latent feature space. This facilitates the learning of deep-level semantics. Besides, the use of neural networks not only strengthens current supervised classiﬁcation of events but alleviates the complexity of feature engineering. However, compared to the earlier study (Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), in which the features are carefully designed by experts, the neural network based methods suffer more from spurious features. Here, spurious feature is speciﬁed as the latent information which looks like the semantically related information to an event, but actually not (Liu et al., 2017a). For example, in the following sample, the semantic information of the word “prison” most probably enables spurious features to come into being, because the word often co-occurs with the trigger ”taken” to evoke an Arrest-jail event instead of the ground-truth event Tra"
P18-1048,D16-1085,0,0.496184,". To address the challenge, we suggest to regulate the learning process with a two-channel selfregulated learning strategy. In the self-regulation process, on one hand, a generative adversarial network is trained to produce the most spurious features, while on the other hand, a neural network 1) Generality – taken home &lt;Transport&gt; Ambiguity 1 – campaign in Iraq &lt;Attack&gt; Ambiguity 2 – political campaign &lt;Elect&gt; Coreference – Either its bad or good &lt;Marry&gt; A promising solution to this challenge is through semantic understanding. Recently, neural networks have been widely used in this direction (Nguyen and Grishman, 2016; Ghaeini et al., ∗ Corresponding author 515 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 515–526 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 濵濴濶濾澳瀃瀅瀂瀃濴濺濴瀇濼瀂瀁澳 澳   Therefore, G and D cooperate with each other during training, developing the parameters θg and θd with the same goal – to minimize the performance loss L(ˆ y , y) in the detection task:   θg = argmin L(ˆ y , y) (1) θd 濣瀅濸濷濼濶瀇濼瀂瀁澳 ݔ ෙ ෙ  濵濴濶濾澳瀃瀅瀂瀃濴濺濴瀇濼瀂瀁澳 Figure 1: Self-regulated learning scheme where, y denotes the g"
P18-1048,D09-1016,0,0.0759906,"or the out-domain case, ideally, both Hybrid and SELF encounter the problem that there is lack of target domain data available for training. In this case, SELF displays less performance degradation • It relies on the use of spurious features to implement self-regulation during training. 522 Event mentions And it still does We had no part in it Nobody questions if this is right or ... And that is what ha- what is happening Oh, yeah, it wasn’t perfect Type Die Arrest-Jail Attack End-Position Marry ety of strategies have emerged for converting classiﬁcation clues into feature vectors (Ahn, 2006; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013, 2014; Wei et al., 2017). Beneﬁting from the general modeling framework, the methods enable the fusion of multiple features, and more importantly, they are ﬂexible to use by feature selection. But considerable expertise is required for feature engineering. Recently, the use of neural networks for event detection has become a promising line of research. The closely related work has been presented in section 5.3. The primary advantages of neural networks have been demonstrated in the work, such as performance enhancement, self-learnin"
P18-1048,D16-1038,0,0.0701099,"Missing"
P18-1048,P13-1147,0,0.0208894,"Missing"
P18-1048,P10-1040,0,0.00938638,"between Og and Ogˇ for measuring the loss of self-regulation learning Ldif f . The higher the similarity, the greater the loss. During training, the generator G is required to develop the parameters θg to minimize the loss: θg = argmin Ldif f (og , ogˇ) put. Before feeding the tokens into the network, we transform each of them into a real-valued vector x ∈ Re . The vector is formed by concatenating a word embedding with an entity type embedding. • Word Embedding: It is a ﬁxed-dimensional real-valued vector which represents the hidden semantic properties of a token (Collobert and Weston, 2008; Turian et al., 2010). • Entity Type Embedding: It is specially used to characterize the entity type associated with a token. The BIO2 tagging scheme (Wang and Manning, 2013; Huang et al., 2015) is employed for assigning a type label to each token in the sentence. (4) For the input token xt at the current time step t, the LSTM generates the latent feature vector ot ∈ Rd by the previous memory. Meanwhile, the token is used to update the current memory. The LSTM possesses a long-term memory unit ct ∈ Rd . In addition, it ct ∈ Rd and short-term  is equipped with the input gate it , forgetting gate ft and a hidden st"
P18-1048,I13-1183,0,0.0170587,"enerator G is required to develop the parameters θg to minimize the loss: θg = argmin Ldif f (og , ogˇ) put. Before feeding the tokens into the network, we transform each of them into a real-valued vector x ∈ Re . The vector is formed by concatenating a word embedding with an entity type embedding. • Word Embedding: It is a ﬁxed-dimensional real-valued vector which represents the hidden semantic properties of a token (Collobert and Weston, 2008; Turian et al., 2010). • Entity Type Embedding: It is specially used to characterize the entity type associated with a token. The BIO2 tagging scheme (Wang and Manning, 2013; Huang et al., 2015) is employed for assigning a type label to each token in the sentence. (4) For the input token xt at the current time step t, the LSTM generates the latent feature vector ot ∈ Rd by the previous memory. Meanwhile, the token is used to update the current memory. The LSTM possesses a long-term memory unit ct ∈ Rd . In addition, it ct ∈ Rd and short-term  is equipped with the input gate it , forgetting gate ft and a hidden state ht , which are assembled together to promote the use of memory, as well as dynamic memory updating. Similarly, they are deﬁned as a d-dimensional ve"
P18-1048,P17-2046,0,0.0252994,"f target domain data available for training. In this case, SELF displays less performance degradation • It relies on the use of spurious features to implement self-regulation during training. 522 Event mentions And it still does We had no part in it Nobody questions if this is right or ... And that is what ha- what is happening Oh, yeah, it wasn’t perfect Type Die Arrest-Jail Attack End-Position Marry ety of strategies have emerged for converting classiﬁcation clues into feature vectors (Ahn, 2006; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013, 2014; Wei et al., 2017). Beneﬁting from the general modeling framework, the methods enable the fusion of multiple features, and more importantly, they are ﬂexible to use by feature selection. But considerable expertise is required for feature engineering. Recently, the use of neural networks for event detection has become a promising line of research. The closely related work has been presented in section 5.3. The primary advantages of neural networks have been demonstrated in the work, such as performance enhancement, self-learning capability and robustness. The generative adversarial network (Goodfellow et al., 20"
P19-1045,D18-1024,0,0.017936,"gan to apply adversarial learning to various NLP tasks. Zhang et al. (2016) and Zhao et al. (2017) constructed adversarial networks with CNNs and LSTMs to train text generation models. Wu et al. (2017) proposed two types of adversarial models which consist of CNNs and RNNs, respectively. They discussed the advantages and disadvantages of two implementations on two relation extraction datasets. Masumura et al. (2018) proposed an adversarial training approach for multi-task multilingual learning, which jointly conducts task discrimination among languages and language discrimination among tasks. Chen and Cardie (2018) applied adversarial learning to multilingual word representation learning which maps word embedValence Score (SV) Discriminating Score (P) Arousal Score (SA) Regressor for Valence (RV) Discriminator (D) Regressor for Arousal (RA) Feature Extractor A (Ext) Attention for Valence (Att V) Shared Attention (Att S) Attention for Arousal (Att A) Input (X) Figure 2: The framework of the Valence-Arousal Adversarial Attention Network which conducts adversarial learning between a pair of emotion dimensions. The frameworks of Valence-Dominance AAN and ArousalDominance AAN can be inferred in the same mann"
P19-1045,E17-2092,0,0.0970912,"ion. The dimensional emotion score ranges from 1.0 to 5.0. In this example, the word very in blue only suggests one emotion dimension (i.e, a high Arousal score). The word scared and disaster in red suggest two emotion dimensions. Specifically, scared suggests a low Valence score and a low Dominance score, while Disaster denotes a low Valence score and a high Arousal score. suitable for fine-grained emotion analysis and has gained an increasing attention recently due to the availability of several emotion regression corpora in the last few years (Preotiuc-Pietro et al., 2016; Yu et al., 2016; Hahn and Buechel, 2017). In principle, these emotion regression corpora apply the widely-admitted Valence-Arousal model or Valence-Arousal-Dominance model (Barrett, 2006) to describe emotions with a continuous real number space in two or three dimensions. Moreover, while different emotion classification corpora often apply different classification systems, they describe emotions with a limited number of discrete pre-defined emotion categories. In the literature, most of the existing studies in emotion regression focus on a single emotion dimension by training multiple independent models for different emotion dimensi"
P19-1045,P17-1001,0,0.0453615,"Missing"
P19-1045,P17-1067,0,0.0923803,"Missing"
P19-1045,D18-1064,0,0.111373,"scores can be inferred from the colored words in this figure. Although the degree adverb, such as very, only suggests a high Arousal score, an emotional word often suggests more than one dimensional emotion score. This hints a possibility that the relationship between two emotion dimensions can be leveraged, which is overlooked by existing singledimensional emotion regression studies. In this paper, we try to model the multidimensional learning task as a multi-task learning task through adversarial learning. Recently, studies in multi-task learning via adversarial learning (Liu et al., 2017; Masumura et al., 2018), which tried to conduct adversarial learning (Goodfellow et al., 2014) between multiple tasks to learn taskspecific features for achieving better performance for each task, has achieved a great success. We apply adversarial learning to model the task not only due to its capability of multi-task learning, but also due to its inherent collocability with attention mechanism. In the literature, adversarial learning has the difficulty in learning latent representations from discrete structures (e.g., sequence of word embeddings). Thus, most of existing studies in NLP apply adversarial learning wit"
P19-1045,P15-2030,0,0.0310476,"-CNN in all cases. Furthermore, AAN outperforms its two counterparts (i.e., Attention Network and Joint Learning), justifying the effectiveness of the proposed adversarial learning approach. However, the overall r-values on EMOBANK are relatively low. This indicates the inherent difficulty of emotion regression on EMOBANK. As a reference, the average oracle rvalue between human annotators of EMOBANK is about 0.6 (Hahn and Buechel, 2017). 4.2 Baselines In this study, the following baselines for emotion regression are implemented for fair comparison: • Deep CNN: A CNN-based approach proposed by Bitvai and Cohn (2015). This approach applies multiple parallel CNNs to extract multiple n-gram features in a text, and 476 Domain News Domain Fictions Domain Blogs Domain Essays Domain Letters Domain Travel Guides Domain Approach Deep CNN Regional CNN-LSTM Context LSTM-CNN Attention Network Joint Learning AAN Deep CNN Regional CNN-LSTM Context LSTM-CNN Attention Network Joint Learning AAN Deep CNN Regional CNN-LSTM Context LSTM-CNN Attention Network Joint Learning AAN Deep CNN Regional CNN-LSTM Context LSTM-CNN Attention Network Joint Learning AAN Deep CNN Regional CNN-LSTM Context LSTM-CNN Attention Network Joint"
P19-1045,E17-2093,0,0.428358,"ores. Specifically, our proposed AAN has two features: • Second, unlike existing single-dimensional emotion regression studies which separately train models for different emotion dimensions, AAN can leverage shared information between emotion dimensions (e.g., word scare contributes to both Valence and Dominance in the example shown in Figure 1) to better rate different emotion dimension scores, and thus achieve better regression results. We apply AAN to the task of multi-dimensional emotion regression on a large-scale emotion regression corpus, namely EMOBANK, contributed by Hahn and Buechel (2017). Empirical evaluation on EMOBANK Reader’s and Writer’s multidimensional emotion regression tasks shows that AAN achieves significant improvements in rvalues over several strong baselines. Furthermore, it also shows that adversarial training between two attention layers is more effective than simply applying attention mechanism individually to each emotion dimension, or simply training two regressors jointly for a pair of emotion dimensions. 2 Related Work 2.1 Emotion Regression Compared with emotion classification, emotion regression had a late start due to the severe lack of large-scale anno"
P19-1045,W16-0404,0,0.253732,"Missing"
P19-1045,D18-1107,0,0.0487028,"Missing"
P19-1045,P16-2037,0,0.451261,"emotion regression corpora apply the widely-admitted Valence-Arousal model or Valence-Arousal-Dominance model (Barrett, 2006) to describe emotions with a continuous real number space in two or three dimensions. Moreover, while different emotion classification corpora often apply different classification systems, they describe emotions with a limited number of discrete pre-defined emotion categories. In the literature, most of the existing studies in emotion regression focus on a single emotion dimension by training multiple independent models for different emotion dimensions (Yu et al., 2015; Wang et al., 2016a). Hence in this paper, we seek to solve multi-dimensional emotion regression via a joint approach. Recently, attention mechanism Introduction Emotion analysis aims to recognize human emotion expression in a given text (Mishne et al., 2005; Abdul-Mageed and Ungar, 2017). Typically, studies in emotion analysis can be divided into either emotion classification (Yang et al., 2007; Tripathi et al., 2017) or emotion regression (Yu et al., 2015; Wang et al., 2016a). While emotion classification aims to label an input text with a single or multiple emotion categories, emotion regression aims to rate"
P19-1045,D16-1058,0,0.537606,"emotion regression corpora apply the widely-admitted Valence-Arousal model or Valence-Arousal-Dominance model (Barrett, 2006) to describe emotions with a continuous real number space in two or three dimensions. Moreover, while different emotion classification corpora often apply different classification systems, they describe emotions with a limited number of discrete pre-defined emotion categories. In the literature, most of the existing studies in emotion regression focus on a single emotion dimension by training multiple independent models for different emotion dimensions (Yu et al., 2015; Wang et al., 2016a). Hence in this paper, we seek to solve multi-dimensional emotion regression via a joint approach. Recently, attention mechanism Introduction Emotion analysis aims to recognize human emotion expression in a given text (Mishne et al., 2005; Abdul-Mageed and Ungar, 2017). Typically, studies in emotion analysis can be divided into either emotion classification (Yang et al., 2007; Tripathi et al., 2017) or emotion regression (Yu et al., 2015; Wang et al., 2016a). While emotion classification aims to label an input text with a single or multiple emotion categories, emotion regression aims to rate"
P19-1045,D17-1187,0,0.435229,"ores. Specifically, our proposed AAN has two features: • Second, unlike existing single-dimensional emotion regression studies which separately train models for different emotion dimensions, AAN can leverage shared information between emotion dimensions (e.g., word scare contributes to both Valence and Dominance in the example shown in Figure 1) to better rate different emotion dimension scores, and thus achieve better regression results. We apply AAN to the task of multi-dimensional emotion regression on a large-scale emotion regression corpus, namely EMOBANK, contributed by Hahn and Buechel (2017). Empirical evaluation on EMOBANK Reader’s and Writer’s multidimensional emotion regression tasks shows that AAN achieves significant improvements in rvalues over several strong baselines. Furthermore, it also shows that adversarial training between two attention layers is more effective than simply applying attention mechanism individually to each emotion dimension, or simply training two regressors jointly for a pair of emotion dimensions. 2 Related Work 2.1 Emotion Regression Compared with emotion classification, emotion regression had a late start due to the severe lack of large-scale anno"
P19-1045,N16-1066,0,0.44401,"l emotion regression. The dimensional emotion score ranges from 1.0 to 5.0. In this example, the word very in blue only suggests one emotion dimension (i.e, a high Arousal score). The word scared and disaster in red suggest two emotion dimensions. Specifically, scared suggests a low Valence score and a low Dominance score, while Disaster denotes a low Valence score and a high Arousal score. suitable for fine-grained emotion analysis and has gained an increasing attention recently due to the availability of several emotion regression corpora in the last few years (Preotiuc-Pietro et al., 2016; Yu et al., 2016; Hahn and Buechel, 2017). In principle, these emotion regression corpora apply the widely-admitted Valence-Arousal model or Valence-Arousal-Dominance model (Barrett, 2006) to describe emotions with a continuous real number space in two or three dimensions. Moreover, while different emotion classification corpora often apply different classification systems, they describe emotions with a limited number of discrete pre-defined emotion categories. In the literature, most of the existing studies in emotion regression focus on a single emotion dimension by training multiple independent models for"
P19-1045,P15-2129,0,0.431875,"principle, these emotion regression corpora apply the widely-admitted Valence-Arousal model or Valence-Arousal-Dominance model (Barrett, 2006) to describe emotions with a continuous real number space in two or three dimensions. Moreover, while different emotion classification corpora often apply different classification systems, they describe emotions with a limited number of discrete pre-defined emotion categories. In the literature, most of the existing studies in emotion regression focus on a single emotion dimension by training multiple independent models for different emotion dimensions (Yu et al., 2015; Wang et al., 2016a). Hence in this paper, we seek to solve multi-dimensional emotion regression via a joint approach. Recently, attention mechanism Introduction Emotion analysis aims to recognize human emotion expression in a given text (Mishne et al., 2005; Abdul-Mageed and Ungar, 2017). Typically, studies in emotion analysis can be divided into either emotion classification (Yang et al., 2007; Tripathi et al., 2017) or emotion regression (Yu et al., 2015; Wang et al., 2016a). While emotion classification aims to label an input text with a single or multiple emotion categories, emotion regr"
P19-1058,P14-1048,0,0.0258586,"evaluated their models on PDTB (Prasad et al., 2008) and RST-DT (Carlson et al., 2003), which are two English discourse corpora that were available up to now. PDTB is the largest English discourse corpus with 2312 annotated documents from Wall Street Journal using the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arg"
P19-1058,C18-1046,0,0.209915,"Missing"
P19-1058,C18-1048,0,0.0648367,"ra that were available up to now. PDTB is the largest English discourse corpus with 2312 annotated documents from Wall Street Journal using the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arguments and how to incorporate their semantic interactions. From this regard, most of them focused on improving representatio"
P19-1058,P16-1163,0,0.17057,"itler et al., 2009; Lin et al., 2009; Wang et al., 2017; Kong and Zhou, 2017) that directly rely on feature engineering, recent neural network models (Liu et al., 2017; Qin et al., 2017; Guo et al., 2018; Bai and Zhao, 2018) can capture deeper semantic cues and learn better representations (Zhang et al., 2015). In particular, most neural network-based methods encode arguments using variants of BiLSTM or CNN (Qin et al., 2016; Guo et al., 2018) and propose various models (e.g., the gated relevance network, the encoder-decoder model, and interactive attention) to measure the semantic relevance (Chen et al., 2016; Cianflone and Kosseim, 2018; Guo et al., 2018) Due to the large differences between the hypotactic English language and the paratactic Chinese language, English-based models, which rely heavily on sentence-level representations, may not function well on Chinese. Due to its paratactic nature, Chinese is flooded with a broad range of flexible sentence structures and semantic cohesion, such as ellipses, references, substitutions, and conIn the literature, most of the previous studies on English implicit discourse relation recognition only use sentence-level representations, which cannot provide"
P19-1058,D17-1134,0,0.456089,"ly, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arguments and how to incorporate their semantic interactions. From this regard, most of them focused on improving representations or incorporating the complex interactions. Bai and Zhao (2018) proposed a deep enhanced representation to represent arguments at the character, subword, word, and sentence levels. Chen et al. (2016) introduced a gated relevance network to model both the linear and nonlinear correlations between two arguments. Guo et a"
P19-1058,D16-1246,0,0.061779,"multi-level attention model that simulates the repeated reading process by stacking multiple attention layers with external memory; (2) R¨onnqvist (R¨onnqvist et al., 2017): a Bi-LSTM model with attention mechanism that first links argument pairs by inserting special labels; and (3) Guo (Guo et al., 2018): a neural tensor network that encodes the arguments by BiLSTM and interactive attention. Among them, GCN uses the same settings as our model. Following Liu and Li (2016), the hidden size for each direction of Bi-LSTM is set to 350, the same as the dimension of the word embeddings. Following Qin et al. (2016), the convolution kernel size and the number in CNN are set to 2 and 1024, respectively. The three state-of-the-art models are reproduced following their corresponding work. The experimental results on CDTB are illustrated in Table 2. It shows that our TTN model outperforms the other baselines in both the micro and macro F1-scores. This indicated that topiclevel information is a vital evidence to reveal the relationships among arguments and justify the effectiveness of our TTN model. Compared with the basic recurrent neural network Bi-LSTM, the CNN and GCN significantly improve the micro and m"
P19-1058,D14-1224,1,0.906946,"Missing"
P19-1058,P17-1093,0,0.240562,"Missing"
P19-1058,D09-1036,0,0.0871272,"Missing"
P19-1058,D16-1130,0,0.614404,"on et al., 2003), which are two English discourse corpora that were available up to now. PDTB is the largest English discourse corpus with 2312 annotated documents from Wall Street Journal using the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arguments and how to incorporate their semantic interactions. From th"
P19-1058,P17-2040,0,0.337484,"Missing"
P19-1058,I17-1049,0,0.0141754,"all Street Journal using the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arguments and how to incorporate their semantic interactions. From this regard, most of them focused on improving representations or incorporating the complex interactions. Bai and Zhao (2018) proposed a deep enhanced representation to repr"
P19-1058,P17-2029,0,0.0190496,"on PDTB (Prasad et al., 2008) and RST-DT (Carlson et al., 2003), which are two English discourse corpora that were available up to now. PDTB is the largest English discourse corpus with 2312 annotated documents from Wall Street Journal using the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arguments and how to in"
P19-1058,P14-1028,0,0.0341615,"and POS embedding, each gated convolutional layer hl is computed as follows: where fn (·) is a standard nonlinear function, M ∈ Rd×d×m is a 3rd-order transformation tensor, U ∈ Rm×2d and s ∈ Rm are parameters. The tensor product x&gt; M [1:m] y results in a vector c ∈ Rm , where each entry is computed by slice i of the tensor M as ci = x&gt; M [i] y, and it is equivalent to including m Bilinear models that simultaneously capture multiple linear interactions between vectors. However, it increases the parameters and the computational complexity of the model; therefore, we adopt tensor factorization (Pei et al., 2014), which uses two low rank matrices to approximate each tensor slice M [i] , as follows: hl (X) = (X · W + b) ⊗ σ(X · V + c) + X (6) where X ∈ RN ×D is the input of layer hl (either the input sequence E or the outputs of previous layers), W ∈ RC×D×D , b ∈ RD , V ∈ RC×D×D , c ∈ RD are model parameters, and C is the size of the convolution kernel. σ(·) is the sigmoid function and ⊗ is the element-wise product between matrices. After stacking L layers on top of the input, we can obtain the semantic representation sequence of the argument H = hL ◦ ... ◦ h1 (E) ∈ RN ×D . Finally, the Mean Pooling op"
P19-1058,D18-1079,1,0.847383,"ng the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arguments and how to incorporate their semantic interactions. From this regard, most of them focused on improving representations or incorporating the complex interactions. Bai and Zhao (2018) proposed a deep enhanced representation to represent arguments at th"
P19-1058,P09-1077,0,0.17798,". 2 Related Work Most previous studies evaluated their models on PDTB (Prasad et al., 2008) and RST-DT (Carlson et al., 2003), which are two English discourse corpora that were available up to now. PDTB is the largest English discourse corpus with 2312 annotated documents from Wall Street Journal using the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods"
P19-1058,K16-2001,0,0.0256395,"Missing"
P19-1058,D18-1351,0,0.0210975,"ss entropy beb k recontween the BoW representation Bk and B structed by the decoder. Since decreasing the KL (Kullback-Leibler) divergence makes all p(Z|B) approximate the standard normal distribution, the noise can be prevented from being zero with the result as follows. Similar to the LDA-style topic models, we believe that there is an association between the word distribution Bk of an argument and its topic distribution Zk . For each Bk , we can infer a latent topic distribution Zk ∈ RK through our topic model, where K denotes the number of topics. Inspired by the Neural Topic Model (NTM) (Zeng et al., 2018; Miao et al., 2016), we propose a simplified topic model STM based on the Variational AutoEncoder (VAE) (Kingma and Welling, 2013). Unlike NTM, our model does not attempt to reconstruct the document during the decoding phase, and it only restores the word distributions. Although STM cannot learn the semantic word embeddings, it significantly reduces the training parameters to perform unsupervised training on the discourse corpus with a small sample size. Similar to NTM, we can interpret our STM as a VAE: a neural network encoder p(Z|B) first compresses the BoW representation Bk into a continu"
P19-1058,D15-1266,0,0.399428,"Missing"
P19-1058,C10-2172,0,0.0258068,"PDTB is the largest English discourse corpus with 2312 annotated documents from Wall Street Journal using the PTB-style predicate-argument structure. RSTDT is another popular English discourse corpus, which annotates 385 documents from Wall Street Journal using the RST tree scheme. Basically, previous studies can be categorized into traditional models that focus on linguistically informed features (Pitler et al., 2009; Lin et al., 2009; Feng and Hirst, 2014; Wang et al., 2017), and neural network methods (Liu and Li, 2016; Chen et al., 2016; Guo et al., 2018; Bai and Zhao, 2018). Especially, Zhou et al., (2010) attempted to predict implicit connectives. Qin et al. (2017), Shi et al. (2017) and Xu et al. (2018) attempted to leverage explicit examples for data augmentation. Other studies resorted to unlabeled data to perform multi-task or unsupervised learning (Liu et al., 2016; Lan et al., 2017). Since discourse relation recognition is essentially a classification problem, what those neural network methods need to consider is how to model the arguments and how to incorporate their semantic interactions. From this regard, most of them focused on improving representations or incorporating the complex i"
P19-1345,D17-1047,0,0.161136,"e-ofthe-art approaches to ASC as baselines. Since the input of all these approaches should be a single sequence, we concatenate question and answer text to generate a single sequence. Besides, we employ some QA matching approaches to ASC-QA and implement several basic versions of RBAN as baselines. Note that, for fair comparison, all the above baselines adopt the same pre-trained word embeddings as RBAN. The baselines are listed as follows in detail: 1) LSTM (Wang et al., 2016). This approach only adopts a standard LSTM network to model the text without considering aspect information. 2) RAM (Chen et al., 2017). This is a state-of-theart deep memory network approach to ASC. 3) GCAE (Xue and Li, 2018). This is a state-ofthe-art approach to ASC which combines CNN and gating mechanisms to learn text representation. 4) S-LSTM (Wang and Lu, 2018). This is a state-of-the-art approach to ASC which considers structural dependencies between targets and opinion terms. 5) BIDAF (Seo et al., 2016). This is a QA matching approach to reading comprehension. We substitute its decoding layer with softmax decoder to perform ASC-QA. 6) HMN (Shen et al., 2018a). This is a QA matching approach to coarse-grained sentimen"
P19-1345,P14-2009,0,0.0298397,"QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answering (QA)”, has become increasingly popular and a large-scale of such QA style reviews (as shown in Figure 1) could be found in several famous e-commerce platforms (e.g., Amazon and Taobao). Compared to traditional non-interactive cust"
P19-1345,P11-1016,0,0.0880512,"ent Classiﬁcation Towards QA - Input: QA text pair with given aspects - Output: [battery life]: Positive [operating speed]: Negative Figure 1: An example for illustrating the proposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Cust"
P19-1345,D16-1011,0,0.0706496,"Missing"
P19-1345,C18-1079,0,0.0249668,"Missing"
P19-1345,D13-1171,0,0.358181,"Missing"
P19-1345,S15-2082,0,0.190919,"Missing"
P19-1345,S14-2004,0,0.52776,"oposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answering (QA)”, has become increasingly popular and a large-scale of such QA style reviews (as shown in Figure 1) could be found in several famous e-commerce platfor"
P19-1345,P13-4009,0,0.0428117,"Missing"
P19-1345,D18-1401,1,0.853282,"Missing"
P19-1345,D16-1021,0,0.411895,"wards QA - Input: QA text pair with given aspects - Output: [battery life]: Positive [operating speed]: Negative Figure 1: An example for illustrating the proposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answe"
P19-1345,P08-1036,0,0.0510394,"entence-level text classiﬁcation which aims to incorporate aspect information into a model. Recently, Wang et al. (2016); Ma et al. (2017) propose an attention based LSTM to ASC by exploring the connection between an aspect and the content of a sentence. Tang et al. (2016b), Chen et al. (2017) and Wang et al. (2018b) employ memory networks to model the context and aspect. Wang and Lu (2018) propose a segmentation attention to capture structural dependency between target and opinion terms. Document-level ASC aims to predict sentiment ratings for aspects inside a long text. Traditional studies (Titov and McDonald, 2008; Wang et al., 2010; Pontiki et al., 2016) solve document-level ASC as a sub-problem by utilizing heuristic based methods or topic models. Recently, Lei et al. (2016) focus on extracting rationales for aspects in a document. Li et al. (2018) propose an useraware attention approach to document-level ASC. Yin et al. (2017) model document-level ASC as a machine comprehension problem, of which the input is also a parallel unit, i.e., question and answer. However, their question texts are pseudo and artiﬁcially constructed. This disaccords with the fact that real-world question texts also possibly"
P19-1345,P18-1088,0,0.34373,"text pair with given aspects - Output: [battery life]: Positive [operating speed]: Negative Figure 1: An example for illustrating the proposed task of Aspect Sentiment Classiﬁcation towards QuestionAnswering (ASC-QA). Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neutral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC focus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer review “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and expresses positive sentiment towards the former and negative sentiment towards the latter. ∗ Corresponding author Recently, a new interactive reviewing form, namely “Customer Question-Answering (QA)”, has beco"
P19-1345,D16-1058,0,0.427641,"determine the polarity towards each aspect category discussed in a QA text pair. 4.2 Baselines For comparison, we implement several state-ofthe-art approaches to ASC as baselines. Since the input of all these approaches should be a single sequence, we concatenate question and answer text to generate a single sequence. Besides, we employ some QA matching approaches to ASC-QA and implement several basic versions of RBAN as baselines. Note that, for fair comparison, all the above baselines adopt the same pre-trained word embeddings as RBAN. The baselines are listed as follows in detail: 1) LSTM (Wang et al., 2016). This approach only adopts a standard LSTM network to model the text without considering aspect information. 2) RAM (Chen et al., 2017). This is a state-of-theart deep memory network approach to ASC. 3) GCAE (Xue and Li, 2018). This is a state-ofthe-art approach to ASC which combines CNN and gating mechanisms to learn text representation. 4) S-LSTM (Wang and Lu, 2018). This is a state-of-the-art approach to ASC which considers structural dependencies between targets and opinion terms. 5) BIDAF (Seo et al., 2016). This is a QA matching approach to reading comprehension. We substitute its decod"
P19-1345,P18-1234,0,0.0124307,"e a single sequence, we concatenate question and answer text to generate a single sequence. Besides, we employ some QA matching approaches to ASC-QA and implement several basic versions of RBAN as baselines. Note that, for fair comparison, all the above baselines adopt the same pre-trained word embeddings as RBAN. The baselines are listed as follows in detail: 1) LSTM (Wang et al., 2016). This approach only adopts a standard LSTM network to model the text without considering aspect information. 2) RAM (Chen et al., 2017). This is a state-of-theart deep memory network approach to ASC. 3) GCAE (Xue and Li, 2018). This is a state-ofthe-art approach to ASC which combines CNN and gating mechanisms to learn text representation. 4) S-LSTM (Wang and Lu, 2018). This is a state-of-the-art approach to ASC which considers structural dependencies between targets and opinion terms. 5) BIDAF (Seo et al., 2016). This is a QA matching approach to reading comprehension. We substitute its decoding layer with softmax decoder to perform ASC-QA. 6) HMN (Shen et al., 2018a). This is a QA matching approach to coarse-grained sentiment classiﬁcation towards QA style reviews. 7) MAMC (Yin et al., 2017). This is a QA matching"
P19-1345,D17-1217,0,0.0355952,"Missing"
P98-2239,J93-2002,0,0.0255408,"Missing"
P98-2239,C90-3010,0,0.0667123,"Missing"
P98-2239,J90-1003,0,0.0395411,"Missing"
P98-2239,J93-1005,0,0.289791,"Missing"
P98-2239,C94-2139,0,0.0413607,"Missing"
P98-2239,O90-1010,0,\N,Missing
W00-0737,P98-1010,0,0.0353167,"Missing"
W00-0737,C92-3126,0,0.0394535,"Missing"
W00-0737,A88-1019,0,0.200947,"Missing"
W00-0737,W96-0102,0,0.0702374,"Missing"
W00-0737,W99-0707,0,0.077773,"Missing"
W00-0737,W95-0107,0,0.17493,"Missing"
W00-0737,C98-1010,0,\N,Missing
W00-1309,P98-1010,0,0.0393795,"Missing"
W00-1309,C92-3126,0,0.0219637,"Missing"
W00-1309,C92-3150,0,0.105101,"Missing"
W00-1309,W99-0707,0,0.0376181,"Missing"
W00-1309,J93-2004,0,0.0451101,"le and efficient processing algorithms. Recently, many researchers have looked at text chunking in two different ways: Some 71 entries and make it possible to further improve the accuracy by merging different contextdependent lexicons into one after automatic analysis of the chunking errors. Finally, the conclusion is given. Tin and the given token sequence G~. By assuming that the mutual information between G~ and T1~ is equal to the summation of mutual information between G~ and the individual tag ti(l_&lt;i_&lt;n ) : The data used for all our experiments is extracted from the PENN&quot; WSJ Treebank (Marcus et al. 1993) by the program provided by Sabine Buchholz from Tilbug University. We use sections 00-19 as the training data and 20-24 as test data. Therefore, the performance is on large scale task instead of small scale task on CoNLL-2000 with the same evaluation program. log /3 2. precision + recall 1 = ~ log e(Tln ). P(G~) i=1 P(t,, G~) P(t,). P(G? ) or n MI(T~ ~ , G~n ) = ~ MI(t,, G? ) , i=l For evaluation of our results, we use the precision and recall measures. Precision is the percentage of predicted chunks that are actually correct while the recall is the percentage of correct chunks that are actua"
W00-1309,W96-0102,0,\N,Missing
W00-1309,W99-0629,0,\N,Missing
W00-1309,P93-1003,0,\N,Missing
W00-1309,C98-1010,0,\N,Missing
W03-1307,W02-2025,0,0.0528396,"Missing"
W03-1307,W00-0904,0,0.0241239,"Missing"
W03-1307,W02-2029,0,0.0190907,"bution based on the state transition probabilities. Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002). 3 3.1 Feature Set Simple Deterministic Features (Fsd) The purpose of simple deterministic features is to capture the capitalization, digitalization and word formation information. This kind of features have been widely used in both newswire NER system, such as (Zhou and Su 2002), and biomedical NER system, such as (Nobata et al. 1999; Gaizauskas et al. 2000; Collier et al. 2000; Takeuchi and Collier 2002; Kazama et al. 2002). Based on the characteristics of biomedical NEs, we designed simple deterministic features manually. Table 1 shows the simple deterministic features with descending order of priority. Fsd Name Comma Dot LRB RRB LSB RSB RomanDigit GreekLetter StopWord ATCGsequence OneDigit AllDigits DigitCommaDigit DigitDotDigit OneCap AllCaps CapLowAlpha CapMixAlpha LowMixAlpha AlphaDigitAlpha AlphaDigit DigitAlphaDigit DigitAlpha Example , . ( ) [ ] II Beta in, at AACAAAG 5 60 1,25 0.5 T CSF All IgM kDa H2A T4 6C2 19D Table 1: Simple deterministic features From Table 1, we can find that:"
W03-1307,C00-1030,0,0.300876,"on As the research in biomedical domain has grown rapidly in recent years, a huge amount of nature language resources have been developed and become a rich knowledge base. The technique of named entity (NE) recognition (NER) is strongly demanded to be applied in biomedical domain. Since in previous work, many NER systems have been applied successfully in newswire domain (Zhou and Su 2002; Bikel et al. 1999; Borthwich et al. 1999), more and more explorations have been done to port existing NER system into biomedical domain (Kazama et al. 2002; Takeuchi et al. 2002; Nobata et al. 1999 and 2000; Collier et al. 2000; Gaizauskas et al. 2000; Fukuda et al. 1998; Proux et al. 1998). However, compared with those in newswire domain, these systems haven’t got high performance. It is probably because of the following factors of biomedical NE (Zhang et al. 2003): 1. Some modifiers are often before basic NEs, e.g. activated B cell lines, and sometimes biomedical NEs are very long, e.g. 47 kDa sterol regulatory element binding factor. This kind of factor highlights the difficulty for identifying the boundary of NE. 2. Two or more NEs share one head noun by using conjunction or disjunction construction, e.g. 91 and"
W03-1307,P02-1060,1,0.692033,". Our experiments on GENIA V3.0 and GENIA V1.1 achieve the 66.1 and 62.5 F-measure respectively, which outperform the previous best published results by 8.1 F-measure when using the same training and testing data. 1 Introduction As the research in biomedical domain has grown rapidly in recent years, a huge amount of nature language resources have been developed and become a rich knowledge base. The technique of named entity (NE) recognition (NER) is strongly demanded to be applied in biomedical domain. Since in previous work, many NER systems have been applied successfully in newswire domain (Zhou and Su 2002; Bikel et al. 1999; Borthwich et al. 1999), more and more explorations have been done to port existing NER system into biomedical domain (Kazama et al. 2002; Takeuchi et al. 2002; Nobata et al. 1999 and 2000; Collier et al. 2000; Gaizauskas et al. 2000; Fukuda et al. 1998; Proux et al. 1998). However, compared with those in newswire domain, these systems haven’t got high performance. It is probably because of the following factors of biomedical NE (Zhang et al. 2003): 1. Some modifiers are often before basic NEs, e.g. activated B cell lines, and sometimes biomedical NEs are very long, e.g. 47"
W03-1307,W02-0301,0,\N,Missing
W03-1710,J92-4003,0,0.0332184,"Missing"
W03-1710,H90-1056,0,0.0600738,"0 word pairs decreases the perplexity of the MI-Trigram model by 20 percent compared with the trigram model. In the meanwhile, evaluation on Chinese word segmentation shows that about 35 percent of errors can be corrected by using the MI-Trigram model compared with the trigram model. 1 Introduction Language modeling is the attempt to characterize, capture and exploit the regularities and constraints in natural language. Among various language modeling approaches, ngram modeling has been widely used in many applications, such as speech recognition, machine translation (Katz 1987; Jelinek 1989; Gale and Church 1990; Brown et al. 1992; Yang et al. 1996; Bai et al 1998; Zhou et al 1999; Rosenfeld 2000; Gao et al 2002). Although ngram modeling is simple in nature and easy to use, it has obvious deficiencies. For instance, ngram modeling can only capture the short distance context dependency within an N-word window where currently the largest practical N for natural language is three. In the meantime, it is found that there always exist many preferred relationships between words. Two highly associated word pairs are 不仅/而且 (“not only/but also”) and 医 生 / 护 士 (“doctor/nurse”). Psychological experiments in Mey"
W03-1710,P02-1024,0,0.0400223,"Missing"
W03-1710,J93-1005,0,0.0153598,"Missing"
W03-1710,P98-2239,1,0.892682,"Missing"
W03-1710,C98-2234,1,\N,Missing
W03-1711,H91-1060,0,0.0191495,"Missing"
W03-1711,P97-1003,0,0.0280652,"Missing"
W03-1711,W00-0726,0,0.0803629,"Missing"
W03-1711,W00-0737,1,0.888538,"Missing"
W03-1711,W00-1309,1,\N,Missing
W03-1731,W00-1309,1,\N,Missing
W04-1201,W02-0301,0,0.0817422,"Missing"
W04-1201,W03-1305,0,0.032823,"Missing"
W04-1201,W03-1307,1,0.916434,"Missing"
W04-1201,W03-1306,0,0.0321535,"Missing"
W04-1201,P02-1060,1,0.863358,"Missing"
W04-1219,P02-1060,1,0.55395,"Missing"
W04-1219,W03-1307,1,\N,Missing
W06-0125,C04-1004,1,0.905303,"Missing"
W06-0125,I05-1047,1,0.897369,"Missing"
W06-0125,P02-1060,1,\N,Missing
W08-2137,J02-3001,0,0.0407468,"valuation on the shared task shows that our system achieves 82.53 in labeled macro F1, 86.39 in labeled attachment score, and 78.64 in labeled F1, using MSTParser on combined test set. This suggests that proper pruning and extensive feature engineering contributes much in dependency tree-based SRL. 1 Introduction Although CoNLL 2008 shared task mainly evaluates joint learning of syntactic and semantic parsing, we focus on dependency tree-based semantic role labeling (SRL). SRL refers to label the semantic roles of predicates (either verbs or nouns) in a sentence. Most of previous SRL systems (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Punyakanok et al., 2005; Pradhan © 2008. Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. et al., 2004, 2005) work on constituent structure trees and has shown to achieve remarkable results. For example, Punyakanok et al. (2005) achieved the best performance in the CoNLL 2005 shared task with 79.44 in F-measure on the WSJ test set and 77.92 on the combined test set (WSJ +Brown). With rapid development of dependency parsing in the last few years, mor"
W08-2137,P02-1031,0,0.0222461,"k shows that our system achieves 82.53 in labeled macro F1, 86.39 in labeled attachment score, and 78.64 in labeled F1, using MSTParser on combined test set. This suggests that proper pruning and extensive feature engineering contributes much in dependency tree-based SRL. 1 Introduction Although CoNLL 2008 shared task mainly evaluates joint learning of syntactic and semantic parsing, we focus on dependency tree-based semantic role labeling (SRL). SRL refers to label the semantic roles of predicates (either verbs or nouns) in a sentence. Most of previous SRL systems (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Punyakanok et al., 2005; Pradhan © 2008. Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. et al., 2004, 2005) work on constituent structure trees and has shown to achieve remarkable results. For example, Punyakanok et al. (2005) achieved the best performance in the CoNLL 2005 shared task with 79.44 in F-measure on the WSJ test set and 77.92 on the combined test set (WSJ +Brown). With rapid development of dependency parsing in the last few years, more and more researchers tu"
W08-2137,C04-1186,0,0.177864,"mmercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. et al., 2004, 2005) work on constituent structure trees and has shown to achieve remarkable results. For example, Punyakanok et al. (2005) achieved the best performance in the CoNLL 2005 shared task with 79.44 in F-measure on the WSJ test set and 77.92 on the combined test set (WSJ +Brown). With rapid development of dependency parsing in the last few years, more and more researchers turn to dependency tree-based SRL with hope to advance SRL from viewpoint of dependency parsing. Hacioglu (2004) pioneered this work by formulating SRL as a classification problem of mapping various dependency relations into semantic roles. Compared with previous researches on constituent structure tree-based SRL which adopts constituents as labeling units, dependency tree-based SRL adopts dependency relations as labeling units. Due to the difference between constituent structure trees and dependency trees, their feature spaces are expected to be somewhat different. In the CoNLL 2008 shared task, we extend the framework by Hacioglu (2004) with maximum entropy as our classifier. For evaluation, we will m"
W08-2137,H05-1066,0,0.0268603,"Missing"
W08-2137,P05-1013,0,0.0350793,"problem of mapping various dependency relations into semantic roles. Compared with previous researches on constituent structure tree-based SRL which adopts constituents as labeling units, dependency tree-based SRL adopts dependency relations as labeling units. Due to the difference between constituent structure trees and dependency trees, their feature spaces are expected to be somewhat different. In the CoNLL 2008 shared task, we extend the framework by Hacioglu (2004) with maximum entropy as our classifier. For evaluation, we will mainly report our official SRL performance using MaltParser (Nivre and Nilsson, 2005). Besides, we will also present our unofficial system by 1) applying a new effective pruning algorithm; 2) including additional features; and 3) adopting a better dependency parser, MSTParser (McDonald, 2005). In the remainder of this paper, we will briefly describe our system architecture, present various features used by our models and report the performance on CoNLL 2008 shared task (both official and unofficial). 253 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 253–257 Manchester, August 2008 2 System Description In CoNLL 2008 shared task"
W08-2137,P05-1072,0,0.0300297,"Missing"
W08-2137,W05-0625,0,0.0238577,"chieves 82.53 in labeled macro F1, 86.39 in labeled attachment score, and 78.64 in labeled F1, using MSTParser on combined test set. This suggests that proper pruning and extensive feature engineering contributes much in dependency tree-based SRL. 1 Introduction Although CoNLL 2008 shared task mainly evaluates joint learning of syntactic and semantic parsing, we focus on dependency tree-based semantic role labeling (SRL). SRL refers to label the semantic roles of predicates (either verbs or nouns) in a sentence. Most of previous SRL systems (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Punyakanok et al., 2005; Pradhan © 2008. Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. et al., 2004, 2005) work on constituent structure trees and has shown to achieve remarkable results. For example, Punyakanok et al. (2005) achieved the best performance in the CoNLL 2005 shared task with 79.44 in F-measure on the WSJ test set and 77.92 on the combined test set (WSJ +Brown). With rapid development of dependency parsing in the last few years, more and more researchers turn to dependency tree-bas"
W08-2137,W08-2121,0,\N,Missing
W08-2137,W04-3212,0,\N,Missing
W08-2137,N04-1030,0,\N,Missing
W09-1208,burchardt-etal-2006-salsa,0,0.0394038,"Missing"
W09-1208,kawahara-etal-2002-construction,0,0.0206595,"Missing"
W09-1208,W08-2121,0,0.200501,"Missing"
W09-1208,taule-etal-2008-ancora,0,0.0308401,"Missing"
W09-1208,W04-3212,0,0.0624688,"actly same way. When no constraint available, however, all word pairs in the an input sequence must be considered, leading to very poor efficiency in computation for no gain in effectiveness. Thus, the training sample needs to be pruned properly. As predicates overtly known in the share task, we only consider how to effectively prune argument candidates. We adopt five types of argument pruning strategies for seven languages. All of them assume that a syntactic dependency parsing tree is available. As for Chinese and English, we continue to use a dependency version of the pruning algorithm of (Xue and Palmer, 2004) as described in (Zhao and Kit, 2008). The pruning algorithm is readdressed as the following. Initialization: Set the given predicate candidate as the current node; (1) The current node and all of its syntactic children are selected as argument candidates. (2) Reset the current node to its syntactic head and repeat step (1) until the root is reached. Note that the given predicate candidate itself is excluded from the argument candidate list for Chinese, that is slightly different from English. The above pruning algorithm has been shown effective. However, it is still inefficient for a singlest"
W09-1208,W08-2127,1,0.844842,"40861 (CityU 1318/03H), CityU Strategic Research Grant 7002037, Projects 60673041 and 60873041 under the National Natural Science Foundation of China and Project 2006AA01Z147 under the ”863” National High-Tech Research and Development of China. 55 We opt for the maximum entropy model with Gaussian prior as our learning model for all classification subtasks in the shared task. Our implementation of the model adopts L-BFGS algorithm for parameter optimization as usual. No additional feature selection techniques are applied. Our system is basically improved from its early version for CoNLL-2008 (Zhao and Kit, 2008). By introducing a virtual root for every predicates, The job to determine both argument labels and predicate senses is formulated as a word-pair classification task in four languages, namely, Catalan, Spanish, Czech and Japanese. In other three languages, Chinese, English and German, a predicate sense classifier is individually trained before argument label classification. Note that traditionally (or you may say that most semantic parsing systems did so) argument identification and classification are handled in a two-stage pipeline, while ours always tackles them in one step, in addition, pre"
W09-1208,W09-1209,1,0.783826,"ust and stable results. The first is that two results for development and test sets in the same language are quite close. The second is about out-ofdomain (OOD) task. Though for each OOD task, we just used the same model trained from the respective language and did nothing to strengthen it, this does not hinder our system to obtain top results in Czech and English OOD tasks. In addition, the feature template sets from automatical selection procedure in this task were used for the joint task of this shared task, and also output top results according to the average score of semantic labeled F1 (Zhao et al., 2009). Development with Gold Development Test (official scores) Out-of-domain average 81.24 80.46 80.47 74.34 Catalan 81.52 80.66 80.32 Chinese 78.32 77.90 77.72 Czech 86.96 85.35 85.19 85.44 English 84.19 84.01 85.44 73.31 German 77.75 76.55 75.99 64.26 Japanese 78.67 78.41 78.15 Spanish 81.32 80.39 80.46 Table 6: Semantic labeled F1 Catalan Sense Argument Training memory (MB) Training time (Min.) Test time (Min.) Training memory (GB) Training time (Hours) Test time (Min.) 0.4 3.0 3.0 Chinese 418.0 11.0 0.7 3.7 13.8 144.0 Czech 3.2 24.9 27.1 English 136.0 2.5 0.2 3.8 12.4 88.0 German 63.0 1.7 0.03"
W10-3013,W06-1617,0,0.0139458,"(dprel). Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm, ln, rm and rn) and high (low) support verb, noun or preposition. Here we specify the last one as an example, support verb(noun/preposition). From a given word to the syntactic root along the syntactic tree, the first verb/noun/preposition that is met is called its low support verb/noun/preposition, and the nearest one to the root(farthest to the given word) is called as its high support verb/noun/preposition. The concept of support verb was broadly used (Toutanova et al., 2005; Xue, 2006; Jiang and Ng, 2006), and it is extended to nouns and prepositions in Zhao et al. (2009b). In addition, a slightly modified syntactic head, pphead, is introduced, it returns the left most sibling of a given word if the word is headed by a preposition, otherwise it returns the original head. Path. There are two basic types of path. One is the linear path (linePath) in the sequence, the other is the path in the syntactic parsing tree (dpPath). For example, m:n|dpPath represents the dependency path from word m to n. Assuming that the two paths from m and n to the root are 3.2 Feature template sets for each task As o"
W10-3013,W08-0607,0,0.12836,"tion while the first step of scope finding aims at high accuracy of labeling hedge cues. Therefore, three independent procedures of feature selection are conducted for BioScope corpus dataset. As Wikipedia is not involved in the task of scope finding, it only needs one final feature set. About 200 feature templates are initially considered for each task. We mainly borrow ideas and are enlightened by following sources while initializing feature template sets: Feature selection a) Previous papers on hedge detection and scope finding (Light et al., 2004; Medlock, 2008; Medlock and Briscoe, 2008; Kilicoglu and Bergler, 2008; Szarvas, 2008; Ganter and Strube, 2009; Morante and Daelemans, 2009); Since hedge and scope finding are quite novel tasks and it is not easy to determine the effective features by experience, a greedy feature selection is conducted. As it mentioned in section 2, our system divides scope finding into two sub-tasks: 94 pm and pn , m:n|dpPathShare, m:n|dpPathPred and m:n|dpPathArgu represent the common part of pm and pn , part of pm which does not belong to pn and part of pn which does not belong to pm , respectively. Family. A children set includes all syntactic children(children) are used in"
W10-3013,W04-3103,0,0.761849,"other for scope finding. In particular, various kinds of syntactic features are systemically exploited and effectively integrated using a large-scale normalized feature selection method. Evaluation on the CoNLL-2010 shared task shows that our system achieves stable and competitive results for all the closed tasks. Furthermore, post-deadline experiments show that the performance can be much further improved using a sufficient feature selection. 1 Introduction Hedges are linguistic devices representing speculative parts of articles. Previous works such as (Hyland, 1996; Marco and Mercer, 2004; Light et al., 2004; Thompson et al., 2008) present research on hedge mainly as a linguistic phenomenon. Meanwhile, detecting hedges and their scopes automatically are increasingly important tasks in natural language processing and information extraction, especially in biomedical community. The shared task of CoNLL-2010 described in Farkas et al. (2010) aims at detecting hedges (task 1) and finding their scopes (task 2) for the literature 2 Methods Basically, the tasks are formulated as sequence labeling in our approach. The available label set differs between task 1 and 2. In addition, it is needed to introduce"
W10-3013,H05-1066,0,0.0129845,"in any additional hedge cues from other resources. An indicator (indicator) is given for multi-hedge scope finding, as specified in section 2.At last, in feature set for scope labeling, hedge represents that the word is in a hedge cue. At last, we take x as current token to be labeled, and xm to denote neighbor words. m > 0 represents that it is a word goes mth after current word and m &lt; 0 for word −mth before current word. b) Related works such as named entity recognition (Collins, 1999) and text chunking (Zhang et al., 2001); c) Some literature on dependency parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Nivre, 2009; Zhao et al., 2009c; Zhao et al., 2009a); 3.1 Notations of Feature Template A large amount of advanced syntactic features including syntactic connections, paths, families and their concatenations are introduced. Many of these features come from dependency parsing, which aims at building syntactic tree expressed by dependencies between words. More details about dependency parsing are given in Nivre and Scholz (2004) and McDonald et al. (2005). The parser in Zhao et al. (2009a) is used to construct dependency structures in our system, and some of the notations in this paper adopt t"
W10-3013,D09-1004,1,0.875918,"Missing"
W10-3013,W09-1304,0,0.527458,"o consideration and be treated as the head and tail tokens of the scopes of specific hedge cues. Furthermore , inhibition can be blocked by actinomycin D , indicating a requirement for de novo transcription . ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... Hedge 2.3 Scope finding for multi-hedge cases Sentences with more than one hedge cue are quite common in both datasets of BioScope corpus and Wikipedia. By counting hedges in every sentence, we find that about one fourth of the sentences with hedges have more than one hedge cue in all three data sources (Table 2). In Morante and Daelemans (2009), three classifiers predict whether each token is Begin, End or None and a postprocessing is needed to associate Begins and Ends with their corresponding hedge cues. In our approach, in order to decrease ambiguous or illegal outputs e.g. inequivalent numbers of Begins and Ends, a pair of Begin and End without their corresponding hedge cue between them, etc., sentences with more than one hedge cue will be preprocessed by making copies as many as the number of hedges and be handled separately. The sentence which is selected as a sample has two hedge cues: “suggesting” and “may”, so our system pr"
W10-3013,C04-1010,0,0.0161011,"nonopen, we do not put in any additional hedge cues from other resources. An indicator (indicator) is given for multi-hedge scope finding, as specified in section 2.At last, in feature set for scope labeling, hedge represents that the word is in a hedge cue. At last, we take x as current token to be labeled, and xm to denote neighbor words. m > 0 represents that it is a word goes mth after current word and m &lt; 0 for word −mth before current word. b) Related works such as named entity recognition (Collins, 1999) and text chunking (Zhang et al., 2001); c) Some literature on dependency parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Nivre, 2009; Zhao et al., 2009c; Zhao et al., 2009a); 3.1 Notations of Feature Template A large amount of advanced syntactic features including syntactic connections, paths, families and their concatenations are introduced. Many of these features come from dependency parsing, which aims at building syntactic tree expressed by dependencies between words. More details about dependency parsing are given in Nivre and Scholz (2004) and McDonald et al. (2005). The parser in Zhao et al. (2009a) is used to construct dependency structures in our system, and some of the notation"
W10-3013,W09-1208,1,0.930701,"her resources. An indicator (indicator) is given for multi-hedge scope finding, as specified in section 2.At last, in feature set for scope labeling, hedge represents that the word is in a hedge cue. At last, we take x as current token to be labeled, and xm to denote neighbor words. m > 0 represents that it is a word goes mth after current word and m &lt; 0 for word −mth before current word. b) Related works such as named entity recognition (Collins, 1999) and text chunking (Zhang et al., 2001); c) Some literature on dependency parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Nivre, 2009; Zhao et al., 2009c; Zhao et al., 2009a); 3.1 Notations of Feature Template A large amount of advanced syntactic features including syntactic connections, paths, families and their concatenations are introduced. Many of these features come from dependency parsing, which aims at building syntactic tree expressed by dependencies between words. More details about dependency parsing are given in Nivre and Scholz (2004) and McDonald et al. (2005). The parser in Zhao et al. (2009a) is used to construct dependency structures in our system, and some of the notations in this paper adopt those presented in Zhao et al. (2"
W10-3013,P09-1040,0,0.0149305,"cues from other resources. An indicator (indicator) is given for multi-hedge scope finding, as specified in section 2.At last, in feature set for scope labeling, hedge represents that the word is in a hedge cue. At last, we take x as current token to be labeled, and xm to denote neighbor words. m > 0 represents that it is a word goes mth after current word and m &lt; 0 for word −mth before current word. b) Related works such as named entity recognition (Collins, 1999) and text chunking (Zhang et al., 2001); c) Some literature on dependency parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Nivre, 2009; Zhao et al., 2009c; Zhao et al., 2009a); 3.1 Notations of Feature Template A large amount of advanced syntactic features including syntactic connections, paths, families and their concatenations are introduced. Many of these features come from dependency parsing, which aims at building syntactic tree expressed by dependencies between words. More details about dependency parsing are given in Nivre and Scholz (2004) and McDonald et al. (2005). The parser in Zhao et al. (2009a) is used to construct dependency structures in our system, and some of the notations in this paper adopt those presente"
W10-3013,W08-0606,0,0.210283,"Missing"
W10-3013,P08-1033,0,0.578306,"ent Systems Shanghai Jiao Tong University 3 School of Computer Science and Technology, Soochow University zhangsd.sjtu@gmail.com, zhaohai@cs.sjtu.edu.cn gdzhou@suda.edu.cn, blu@cs.sjtu.edu.cn Abstract from BioScope corpus (Szarvas et al., 2008) and Wikipedia. This paper describes a system adopting sequence labeling which performs competitive in the official evaluation, as well as further test. In addition, a large-scale feature selection procedure is applied in training and development. Considering that BioScope corpus is annotated by two independent linguists according to a formal guideline (Szarvas, 2008), while Wikipedia weasels are tagged by netizens who are diverse in background and various in evaluation criterion, it is needed to handle them separately. Our system selects features for Wikipedia and BioScope corpus independently and evaluate them respectively, leading to fine performances for all of them. The rest of the paper is organized as follows. The next section presents the technical details of our system of hedge detection and scope finding. Section 3 gives information of features. Section 4 shows the evaluation results, including official results and further ones after official out"
W10-3013,P05-1073,0,0.0447631,"Missing"
W10-3013,N06-1055,0,0.0134354,"ency label (dprel). Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm, ln, rm and rn) and high (low) support verb, noun or preposition. Here we specify the last one as an example, support verb(noun/preposition). From a given word to the syntactic root along the syntactic tree, the first verb/noun/preposition that is met is called its low support verb/noun/preposition, and the nearest one to the root(farthest to the given word) is called as its high support verb/noun/preposition. The concept of support verb was broadly used (Toutanova et al., 2005; Xue, 2006; Jiang and Ng, 2006), and it is extended to nouns and prepositions in Zhao et al. (2009b). In addition, a slightly modified syntactic head, pphead, is introduced, it returns the left most sibling of a given word if the word is headed by a preposition, otherwise it returns the original head. Path. There are two basic types of path. One is the linear path (linePath) in the sequence, the other is the path in the syntactic parsing tree (dpPath). For example, m:n|dpPath represents the dependency path from word m to n. Assuming that the two paths from m and n to the root are 3.2 Feature template se"
W10-3013,P01-1069,0,0.0308878,"dic) is introduced into feature templates. As the evaluation is nonopen, we do not put in any additional hedge cues from other resources. An indicator (indicator) is given for multi-hedge scope finding, as specified in section 2.At last, in feature set for scope labeling, hedge represents that the word is in a hedge cue. At last, we take x as current token to be labeled, and xm to denote neighbor words. m > 0 represents that it is a word goes mth after current word and m &lt; 0 for word −mth before current word. b) Related works such as named entity recognition (Collins, 1999) and text chunking (Zhang et al., 2001); c) Some literature on dependency parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Nivre, 2009; Zhao et al., 2009c; Zhao et al., 2009a); 3.1 Notations of Feature Template A large amount of advanced syntactic features including syntactic connections, paths, families and their concatenations are introduced. Many of these features come from dependency parsing, which aims at building syntactic tree expressed by dependencies between words. More details about dependency parsing are given in Nivre and Scholz (2004) and McDonald et al. (2005). The parser in Zhao et al. (2009a) is used to const"
W10-3013,W09-1209,1,0.924063,"her resources. An indicator (indicator) is given for multi-hedge scope finding, as specified in section 2.At last, in feature set for scope labeling, hedge represents that the word is in a hedge cue. At last, we take x as current token to be labeled, and xm to denote neighbor words. m > 0 represents that it is a word goes mth after current word and m &lt; 0 for word −mth before current word. b) Related works such as named entity recognition (Collins, 1999) and text chunking (Zhang et al., 2001); c) Some literature on dependency parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Nivre, 2009; Zhao et al., 2009c; Zhao et al., 2009a); 3.1 Notations of Feature Template A large amount of advanced syntactic features including syntactic connections, paths, families and their concatenations are introduced. Many of these features come from dependency parsing, which aims at building syntactic tree expressed by dependencies between words. More details about dependency parsing are given in Nivre and Scholz (2004) and McDonald et al. (2005). The parser in Zhao et al. (2009a) is used to construct dependency structures in our system, and some of the notations in this paper adopt those presented in Zhao et al. (2"
W10-4171,E09-1013,0,0.0434854,"Missing"
W10-4171,S07-1002,0,0.0738724,"Missing"
W10-4171,P04-3026,0,0.0772313,"Missing"
W10-4171,E06-1018,0,\N,Missing
W12-3128,N07-1051,0,\N,Missing
W12-3128,A00-2018,0,\N,Missing
W12-3128,D10-1014,0,\N,Missing
W12-3128,D10-1054,0,\N,Missing
W12-3128,D11-1079,0,\N,Missing
W12-3128,W05-1506,0,\N,Missing
W12-3128,J03-4003,0,\N,Missing
W12-3128,D11-1020,0,\N,Missing
W12-3128,P11-1065,0,\N,Missing
W12-3128,P11-1001,0,\N,Missing
W12-3128,J04-4002,0,\N,Missing
W12-3128,D09-1008,0,\N,Missing
W12-3128,P07-1005,0,\N,Missing
W12-3128,P96-1021,0,\N,Missing
W12-3128,P08-1114,0,\N,Missing
W12-3128,P05-1033,0,\N,Missing
W12-3128,N03-1017,0,\N,Missing
W12-3128,P02-1038,0,\N,Missing
W12-3128,W06-3119,0,\N,Missing
W12-3128,W04-3250,0,\N,Missing
W12-3128,J07-2003,0,\N,Missing
W12-3128,2011.eamt-1.38,0,\N,Missing
W12-3128,P00-1056,0,\N,Missing
W12-3128,P03-1021,0,\N,Missing
W15-2504,W12-3156,0,0.0920358,".arbylon.net/projects/ 34 (2) 3 Cohesion Score based on Simplified Lexical Chain Text adequacy is the most important standard for the purpose of successful communication. According to the work of Wong and Kit (2012), cohesion is another important element to organize text. They found: SMT systems tend to use less lexical cohesion devices than those of human translators. Here lexical cohesion devices mainly refer to content words reiterating once or more times in a document. They propose to build document-level MT metrics by integrating cohesion score based on lexical cohesion devices. However, Carpuat and Simard (2012) draw a different conclusion: MT output tend to have more incorrect repetition than human translation when the MT model is especially trained on smaller corpora. Suppose these incorrect repetition as “false” cohesion, metrics in (Wong and Kit, 2012) will fail to distinguish such ”false” cohesion devices. In our opinion, the lack of Wong’s work is completely ignoring text cohesion of references, and they only model the cohesion score of MT output. In this study, we assume the correct cohesion of MT output should be consistent with the one of references. Reference is the equivalent of its source"
W15-2504,N07-1006,0,0.0212351,"This method is also adopted by famous MetricsMaTr (the NIST Metrics for Machine Translation Challenge) and approximated in Gimenez et al. (2010) and Wong and Kit (2012). (6) Finally, the METEOR score is obtained as follows: score = (1 − pen)Fmean . (8) where Gmdoc refers to document-level BLEU or METEOR score (one score per document), Smdoc to gist consistency score(Stopic ) or text cohesion score(Doccs ) proposed in this paper. α and β are weights which are tuned on MTC2 evaluation dataset (see Section 5.1) by a gradient ascending algorithm with the optimum goal of maximum correlation value (Liu and Gildea, 2007). where pn is the precision of n-gram and BP is a penalty factor, preventing BLEU from favoring short segments due to the lack of direct consideration of recall. It is obvious that, although BLEU takes all n-grams into consideration, the importance of different n-grams is ignored except their lengths. METEOR is based on unigram alignment of references and MT output. Each unigram in one system translation is at most mapped to one unigram in the references first and then three successive stages of “exact”, “porter stem” and “WN synonymy” are used to create alignment in turn. Once the final align"
W15-2504,J91-1002,0,0.749156,"Missing"
W15-2504,P02-1040,0,0.0961081,"Missing"
W15-2504,W12-3117,0,0.0348898,"Missing"
W15-2504,D11-1084,1,0.93525,"Missing"
W15-2504,2013.mtsummit-posters.13,0,0.0424254,"Missing"
W15-2504,2006.amta-papers.20,0,0.0353304,"can be tuned with the minimal perplexity (Blei et al., 2003). 2.2 After constructing a trained topic model, the “document-topic” distribution of MT output and reference on evaluation dataset (see Section 5.1) can be respectively inferred. We use Kullback-Leibler divergence to measure topic consistency between MT output and reference with the basic unit of document. Denote the “document-topic” distribution of one reference (dr ) as P (Z|dr ), and the one of its MT output (dt ) as Q(Z|dt ), the KL divergence of Q from P is defined to be: Gist Consistency Score based on Topic Model DKL (P ||Q) = Reeder (2006) proposes to measure MT adequacy at the document level with Latent Semantic Analysis (LSA) (Landauer et al., 1998). However, Reeder only uses a set of complex configuration to show the close correlation between LSA model and human assessments and does not suggest how to use it to design an evaluation metric. Raphael et al. (2012; 2013) exploit bilingual topic models to do quality estimation (without references) for machine translation. In this study, since each evaluation document has 4 references, we show a simple way to design document-level metrics with monolingual topic model. 2.1 G X P (z"
W15-2504,W10-2602,0,0.448563,"Missing"
W15-2504,2011.mtsummit-papers.13,0,0.085941,"Missing"
W15-2504,P12-1079,1,0.906029,"Missing"
W15-2504,D13-1163,1,0.898249,"Missing"
W15-2504,D12-1097,0,0.663858,"vements by using system-level metrics, such as BLEU (Papineni et al., 2002). Whether improvements in performance at system level are really able to reflect the change of text-level translation quality is still to doubt. Nowadays, the study of real document-level MT metrics has been drawing more and more attention. Based on Discourse Representation Theory (Kamp and Reyle, 1993), Gimenez et al. (2010) propose to use co-reference and discourse relations to build evaluation metrics. The metrics by extending traditional metrics with lexical cohesion devices show some positive experimental results (Wong and Kit, 2012). Bilingual topic model (Blei et al., 2003) is applied to do MT quality estimation(Raphael et al., 2012; Raphael et al, 2013). Guzman et al. (2014) use two discourse-aware similarity measures based on discourse structure to improve existing MT evaluation metrics. According to the afore-mentioned definition of text, the most important standard of evaluating translation quality for one document should be to what degree the MT output correctly communicates the main idea of origin text. From this regard, this paper first proposes to measure gist consistency of text via topic model. Topic model is"
W15-2504,W10-1750,0,\N,Missing
W15-2504,N12-1046,0,\N,Missing
W15-2504,W05-0909,0,\N,Missing
W15-2504,C04-1046,0,\N,Missing
W15-2504,P14-1065,0,\N,Missing
W15-2504,D12-1108,0,\N,Missing
Y07-1043,H05-1091,0,0.0675025,"earch and Development of China. We would also like to thank Dr. Alessando Moschitti for his great help in using his Tree Kernel Toolkits, including binary package and source codes. Copyright 2007 by Longhua Qian, Guodong Zhou, Qiaomin Zhu, Peide Qian 1 http://www.ldc.upenn.edu/Projects/ACE/. 415 feature-based approach. Thereafter, kernel methods especially on discrete structures (Haussler 1999) attract more and more attentions in relation extraction as well as other fields in NLP. Prior work on kernel methods for relation extraction includes Zelenko et al. (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005). Due to strong constraints that matching nodes be at the same layer and in the identical path starting from the roots to the current nodes, their kernels achieve good precision but much lower recall on the ACE2003 corpus. Zhang et al. (2006) proposed a composite kernel that consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples. However, their method needs to manually tune parameters in composite kernels that are often difficult to determine. This paper describes an"
Y07-1043,P01-1017,0,0.0327323,"ed the entity information (especially the location where we attach) in the parse tree in order to achieve the best performance. 3. Experiments 3.1.Experimental Corpus and Setting We use the ACE RDC 2004 corpus as our experiment data. The ACE RDC 2004 data contains 451 documents and 5702 relation instances. It defines 7 entity types, 7 major relation types and 23 subtypes. The portion of training data we use contains 347 documents, 121K words and 4307 relations. Evaluation of kernel is done on the training data using 5-fold cross-validation. First, the corpus is parsed using Charniak’s parser (Charniak, 2001). Then, we iterate over all pairs of entity mentions occurring in the same sentence to generate potential relation instances. We choose SVM (Vapnik 1998) as the binary classifier, since SVM has achieved the state-ofthe-art performances for many classification problems like text categorization (Joachims 1998). For efficiency, we apply the one-against-others approach to convert binary classifier to multiclass classifier. The final decision of a relation instance in the multi-class classification is determined by the classifier which has the maximal SVM output. In our implementation, we use the b"
Y07-1043,P04-1054,0,0.0507288,"“863” National High-Tech Research and Development of China. We would also like to thank Dr. Alessando Moschitti for his great help in using his Tree Kernel Toolkits, including binary package and source codes. Copyright 2007 by Longhua Qian, Guodong Zhou, Qiaomin Zhu, Peide Qian 1 http://www.ldc.upenn.edu/Projects/ACE/. 415 feature-based approach. Thereafter, kernel methods especially on discrete structures (Haussler 1999) attract more and more attentions in relation extraction as well as other fields in NLP. Prior work on kernel methods for relation extraction includes Zelenko et al. (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005). Due to strong constraints that matching nodes be at the same layer and in the identical path starting from the roots to the current nodes, their kernels achieve good precision but much lower recall on the ACE2003 corpus. Zhang et al. (2006) proposed a composite kernel that consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples. However, their method needs to manually tune parameters in composite kernels that are often difficult to determi"
Y07-1043,P04-1043,0,0.179257,"VBP NNS TP cr owds r oam t he st r eet s cr owds PER r oam st r eet s FAC T3: E- CPT S TP1 TP2 PER FAC S T4: T- CPT VP VP NP NP NP NP E1- PER E2- FAC E1 E2 NNS VBP NNS NNS VBP NNS cr owds r oam st r eet s cr owds r oam st r eet s Figure 1: Different representations of a relation instance in the example sentence “in many cities, angry crowds roam the streets.”, which is excerpted from the ACE2004 corpus, where a relation “PHSY.Located” holds between the first entity “crowds”(PER) and the second entity “streets” (FAC). We employ the same convolution tree kernel used by Collins and Duffy (2001), Moschitti (2004) and Zhang et al. (2006). This convolution tree kernel counts the number of subtrees that have similar productions on every node between two parse trees. However, the kernel value will depend greatly on the size of the trees, so we should normalize the kernel. 416 From ACE definition on relation types and subtypes, we know that entity features impose a strong constraint on relation types. For example, PER-SOC relations describe the relationship between entities of type PER. Zhang et al. (2006) described five cases to extract the portion of parse tree for relation extraction. Their experiments"
Y07-1043,P06-1104,1,0.200159,"an 1 http://www.ldc.upenn.edu/Projects/ACE/. 415 feature-based approach. Thereafter, kernel methods especially on discrete structures (Haussler 1999) attract more and more attentions in relation extraction as well as other fields in NLP. Prior work on kernel methods for relation extraction includes Zelenko et al. (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005). Due to strong constraints that matching nodes be at the same layer and in the identical path starting from the roots to the current nodes, their kernels achieve good precision but much lower recall on the ACE2003 corpus. Zhang et al. (2006) proposed a composite kernel that consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples. However, their method needs to manually tune parameters in composite kernels that are often difficult to determine. This paper describes an expanded convolution parse tree kernel to incorporate entity information into syntactic structure of relation examples. Similar to Zhang et al. (2006), we employ a convolution parse tree kernel in order to model syntactic structures. Differe"
Y07-1043,P05-1052,0,0.104948,"Missing"
Y07-1043,P05-1053,1,0.832113,"rge amount of text documents in digital archives and the WWW. Information extraction subsumes three main tasks, including Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). This paper will focus on the ACE RDC task 1 and employ kernel method to extract semantic relationships between named entity pairs. Many feature-based approaches transform relation instances into feature vectors of high dimension, and compute the inner dot product between these feature vectors. Current research (Kambhatla 2004, Zhao et al 2005, Zhou et al. 2005, Wang et al. 2006) shows that it is very difficult to extract new effective features from relation examples. Kernel methods are non-parametric estimation techniques that computer a kernel function between data instances. By avoiding transforming data examples into feature vectors, kernel methods can implicitly explore much larger feature space than could be searched by a * This research is supported by Project 60673041 under the National Natural Science Foundation of China and Project 2006AA01Z147 under the “863” National High-Tech Research and Development of China. We would also like to than"
Y11-1043,P99-1048,0,0.0159848,"mination (e.g. Soon et al. 2001; Ng and Cardie 2002b; Yang et al. 2003, 2008; Kong et al. 2009), knowledge of NP anaphoricity is expected to much improve the performance of a coreference resolution system, since a non-anaphoric NP does not have an antecedent and thus does not need to be resolved. Recently, anaphoricity determination has been the subject of increased attention in coreference resolution. A variety of techniques have been proposed to address anaphoricity determination as an independent task (e.g. Paice and Husk 1987; Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998; Bean and Riloff 1999; Vieira and Poesio 2000; Evans 2001; Cherry, Cherry and Bergsma 2005; Bergsma et al. 2008). Since then, more studies have been done to incorporate anaphoricity determination into coreference resolution in a pipeline way (e.g. Ng and Cardie 2002a; Yang et al. 2005; Kong et al. 2010) or in a joint way (e.g. Denis and Balbridge 2007; Luo 2007; Finkel and Manning 2008; Zhou and Kong 2009; Ng 2009), and achieved promising results. While it is well known that NP anaphoricity interacts with various kinds of structured factors, most of previous studies only consider constituent-based syntactic inform"
Y11-1043,P08-1002,0,0.0133527,"009), knowledge of NP anaphoricity is expected to much improve the performance of a coreference resolution system, since a non-anaphoric NP does not have an antecedent and thus does not need to be resolved. Recently, anaphoricity determination has been the subject of increased attention in coreference resolution. A variety of techniques have been proposed to address anaphoricity determination as an independent task (e.g. Paice and Husk 1987; Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998; Bean and Riloff 1999; Vieira and Poesio 2000; Evans 2001; Cherry, Cherry and Bergsma 2005; Bergsma et al. 2008). Since then, more studies have been done to incorporate anaphoricity determination into coreference resolution in a pipeline way (e.g. Ng and Cardie 2002a; Yang et al. 2005; Kong et al. 2010) or in a joint way (e.g. Denis and Balbridge 2007; Luo 2007; Finkel and Manning 2008; Zhou and Kong 2009; Ng 2009), and achieved promising results. While it is well known that NP anaphoricity interacts with various kinds of structured factors, most of previous studies only consider constituent-based syntactic information and there are few studies on exploring dependency-based syntactic information for ana"
Y11-1043,N07-1010,0,0.0333078,"Missing"
Y11-1043,C02-1139,0,0.031887,"n the real world and can be divided into two subtasks: anaphoricity determination and antecedent identification. The first subtask, anaphoricity determination, determines whether a given noun phrases (NP) is anaphoric or not. Here we say an NP is anaphoric if it has any antecedent in the context preceding it, and non-anaphoric otherwise. The second subtask, antecedent identification, identifies the antecedent of a given anaphoric NP. Although machine learning approaches have performed reasonably well in coreference resolution without explicit anaphoricity determination (e.g. Soon et al. 2001; Ng and Cardie 2002b; Yang et al. 2003, 2008; Kong et al. 2009), knowledge of NP anaphoricity is expected to much improve the performance of a coreference resolution system, since a non-anaphoric NP does not have an antecedent and thus does not need to be resolved. Recently, anaphoricity determination has been the subject of increased attention in coreference resolution. A variety of techniques have been proposed to address anaphoricity determination as an independent task (e.g. Paice and Husk 1987; Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998; Bean and Riloff 1999; Vieira and Poesio 2000; Evans"
Y11-1043,P04-1020,0,0.0389651,"Missing"
Y11-1043,P10-1142,0,0.0250815,"Missing"
Y11-1043,P09-1074,0,0.0259555,"Missing"
Y11-1043,I05-1063,0,0.0151633,"does not need to be resolved. Recently, anaphoricity determination has been the subject of increased attention in coreference resolution. A variety of techniques have been proposed to address anaphoricity determination as an independent task (e.g. Paice and Husk 1987; Lappin and Leass 1994; Kennedy and Boguraev 1996; Denber 1998; Bean and Riloff 1999; Vieira and Poesio 2000; Evans 2001; Cherry, Cherry and Bergsma 2005; Bergsma et al. 2008). Since then, more studies have been done to incorporate anaphoricity determination into coreference resolution in a pipeline way (e.g. Ng and Cardie 2002a; Yang et al. 2005; Kong et al. 2010) or in a joint way (e.g. Denis and Balbridge 2007; Luo 2007; Finkel and Manning 2008; Zhou and Kong 2009; Ng 2009), and achieved promising results. While it is well known that NP anaphoricity interacts with various kinds of structured factors, most of previous studies only consider constituent-based syntactic information and there are few studies on exploring dependency-based syntactic information for anaphoricity determination. In this paper, we first study the effectiveness of dependency-based syntactic information for anaphoricity determination. Then, we propose different"
Y11-1043,de-marneffe-etal-2006-generating,0,\N,Missing
Y11-1043,N09-1065,0,\N,Missing
Y11-1043,D09-1102,1,\N,Missing
Y11-1043,D09-1103,1,\N,Missing
Y11-1043,J08-3002,0,\N,Missing
Y11-1043,P02-1060,1,\N,Missing
Y11-1043,C10-1068,1,\N,Missing
Y11-1043,W05-0612,0,\N,Missing
Y11-1043,J94-4002,0,\N,Missing
Y11-1043,N07-1030,0,\N,Missing
Y11-1043,P02-1014,0,\N,Missing
Y11-1043,P01-1017,0,\N,Missing
Y11-1043,J01-4004,0,\N,Missing
Y98-1018,J93-1005,0,0.0699,"Missing"
Y98-1018,C94-2139,0,0.0604156,"Missing"
Y98-1018,J93-2002,0,\N,Missing
Y98-1018,O90-1010,0,\N,Missing
