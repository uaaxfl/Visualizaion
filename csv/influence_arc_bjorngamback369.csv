1992.tc-1.14,J90-3001,1,0.812324,"ext is analyzed into a representation (“Quasi Logical Form”, or QLF), which has been carefully designed so as to represent exactly the aspects of linguistic meaning which do not involve context or “common-sense” knowledge. The source QLF representation is transferred into a target counterpart, from which target-language generation is used to produce the target text. In other applications, such as NL query interfaces to databases, the QLF representation would be subjected to further phases of processing; contextually determined factors, such as the referents of pronouns may be added, (Alshawi, (1)), and vague linguistic predicates replaced by more precisely defined relations (Rayner and Alshawi (9)). However, our hypothesis has been that a useful translation can be obtained by performing transfer directly on QLFs, when necessary dealing with problems of contextual interpretation by querying the user. These questions are phrased in such a way as to assume no knowledge on the source-user’s part of either linguistics or the target language. Our judgement, based on the experience gained during the first year of the project, is that QLF-based transfer successfully circumvents many of the di"
1992.tc-1.14,P91-1021,1,0.851495,"Missing"
1992.tc-1.14,P88-1019,0,0.025275,"ic representation. Knowledge-based interlingua-based systems, in contrast, perform translation in two stages: the source text is reduced to a language-independent intermediate representation, and the target text is generated from it directly. Very few systems are of course completely pure examples of either approach; in particular, many architectures based on syntactic transfer also employ some interlingual semantic ideas, of which the most important is usually a version of 1 A similar, though less sophisticated, system for translation between Japanese and English is reported in (Miike et al, (7)) 167 case-grammar. This does not, however, substantially affect the following discussion. On the positive side, syntactic transfer is the easier alternative to implement, since the techniques of syntactic analysis (and to a lesser extent generation) are well-understood and relatively straightforward. However, the fact that different languages use widely different syntactic forms places a great burden on the transfer component, which becomes correspondingly more complex and harder to understand. To take an example from the English-Swedish language-pair: although the structures of He hired a ca"
1992.tc-1.14,A92-1001,1,0.796087,"so as to represent exactly the aspects of linguistic meaning which do not involve context or “common-sense” knowledge. The source QLF representation is transferred into a target counterpart, from which target-language generation is used to produce the target text. In other applications, such as NL query interfaces to databases, the QLF representation would be subjected to further phases of processing; contextually determined factors, such as the referents of pronouns may be added, (Alshawi, (1)), and vague linguistic predicates replaced by more precisely defined relations (Rayner and Alshawi (9)). However, our hypothesis has been that a useful translation can be obtained by performing transfer directly on QLFs, when necessary dealing with problems of contextual interpretation by querying the user. These questions are phrased in such a way as to assume no knowledge on the source-user’s part of either linguistics or the target language. Our judgement, based on the experience gained during the first year of the project, is that QLF-based transfer successfully circumvents many of the difficulties that arise using pure transfer or interlingua methods; it manages to factor out the problems"
1992.tc-1.14,H91-1015,0,\N,Missing
1994.amta-1.12,P92-1005,0,0.020883,"very specific to the translation of particular sentences. This will be further discussed below (Section 7). 91 5 Verb-phrases in the grammar We will now describe the way verb-phrases are treated in the grammars of SLT. The most important differences between English and Swedish syntax are to be found within in the verb-phrases; however, in this section we will see that this difference of main verb syntax can be factored away in the QLFbased semantics, while some auxiliary verb cases still cause some problems. The treatment of verb-phrase semantics in the CLE follows the strategy suggested by (Alshawi & Crouch 1992): information about tense, modality, etc., is packaged declaratively in the compositional semantics. This information could then be unpacked later on to determine the implicit points in time, etc., not shown in the surface form of the sentence. For a translation system like the SLT, this is not necessarily important: the tense/aspect information can in many cases be translated as it is to the other language (see Section 6.1). Part of the treatment here of verb semantics is based on the fact that it indeed is going to be used in a translation system, so even though auxiliaries behave quite diff"
1994.amta-1.12,P91-1021,1,0.853444,"Missing"
1994.amta-1.12,C92-2098,0,0.0243512,"ssed as frequently as the lexically triggered ones (see Section 4). One reason for this is that interlingua-based MT system can avoid complex, restructure! transfer when treating things like tense, aspect and determination; however, the representation of such phenomena still is a difficult task which must be addressed in any MT system (or at least in any NLP system aiming at a deeper level of utterance interpretation). Some difficult non-lexical problems like number differences in coordinate structures and the translation of anaphoric relationships are discussed in (Pulman (ed.) 1991), while (Gawrońska 1992) discusses the translation of aspect; here, we will concentrate on the translation of tense and mood. In the SLT system, there are two atomic rules translating logical constants used to signal mood feature values: The first eliminates subject-aux inversion, which is not marked in Swedish. The fact that a QLF represents a yes/no-question will be sufficient to generate the corresponding Swedish sentence through inversion of subject and verb. Emphatic &quot;do&quot;, as in &quot;Do fly to Boston!&quot; does not 94 have a corresponding auxiliary in Swedish, and needs to be expressed through prosodic features, which a"
1994.amta-1.12,H92-1051,0,0.0333535,"Missing"
1994.amta-1.12,H93-1042,1,0.880097,"Missing"
2020.alw-1.3,S19-2007,0,0.101929,"Missing"
2020.alw-1.3,N19-1423,0,0.539727,"in all subtasks. P´erez and Luque (2019) and Indurthi et al. (2019) were the top teams in SemEval-2019 Task 5, using ELMo together with LSTM networks. ELMo (Embeddings from Language Model; Peters et al., 2018) uses a bidirectional Language Model to create deeply contextualised word representations, with unsupervised pre-training. GPT (Generative Pre-training Transformer; Radford et al., 2018, 2019) expanded the amount of text the language model can be trained on by combining the ideas of unsupervised pre-training (Dai and Le, 2015) and transformers (Vaswani et al., 2017) with attention. BERT (Devlin et al., 2019) is a direct descendant of GPT, although instead of using a stack of transformer decoders, BERT uses a stack of transformer encoders, and while GPT only trains a forward language model, BERT is bidirectional. With the release of two pre-trained language models, BERTBASE and BERTLARGE , BERT can be used as a language model for tasks such as hate speech detection. Liu et al. (2019) used BERTBASE to deliver some of the best results in SemEval-2019 Task 6, while several SemEval-2020 tasks saw continuous transformer multitask pre-training (ERNIE 2.0; Sun et al., 2020) outperforming other solutions."
2020.alw-1.3,S19-2131,0,0.0216397,"h word embeddings from word2vec Mikolov et al. (2013) in various Convolutional Neural Network (CNN) setups, with the best performing model using transferred knowledge from word2vec. Adding character n-grams boosted precision, but lowered recall. Badjatiya et al. (2017) experimented with several machine learners and neural networks, with the best performer being an Long Short-Term Memory (LSTM) with random word vectors where the network’s output was used as input to a Gradient Boosted Decision Tree. However, their results have shown questionable and difficult to reproduce (Mishra et al., 2018; Fortuna et al., 2019). Pavlopoulos et al. (2017a,b) tested word embeddings from both GloVe and word2vec in an Recurrent Neural Network (RNN), while Pitsilis et al. (2018) utilised an RNN ensemble, although without use of word embeddings, but feeding standard vectorized word uni-grams to multiple LSTM networks, aggregating the classifications, to outperform the previous state-of-the-art. amateur annotators on platforms such as CrowdFlower (Davidson et al., 2017). However, the task of hate speech detection lacks a shared benchmark dataset (Schmidt and Wiegand, 2017) that can be used to measure the performance of dif"
2020.alw-1.3,P18-1031,0,0.138204,"ning data, HuggingFace’s version of the Adam optimiser (Kingma and Ba, 2017) is used with weight decay fix, warmup, and linear learning rate decay. Figure 1 gives an overview of the system architecture implemented for the experiments. 4.3 Further Language Model Training Starting from BERT’s Wikipedia and BookCorpus checkpoint, it is possible to further train the language model with domain-specific corpora. This technique of using unlabelled data from the same 19 domain as the target task to train the language model further using the original pre-training objective(s) was first seen in ULMFiT (Howard and Ruder, 2018). Since the approach taken here only uses two datasets, there are still a lot of datasets from the target domain available. Remember, the pre-training only requires the raw text, and so the labels are irrelevant. All available datasets mentioned at the beginning of Section 2, except the two used for the target task, were collected and used to further train BERT on domain data. Furthermore, BERT’s English vocabulary consists of 30,522 segmented subword units learned beforehand. Some vocabulary entries are placeholders that can be replaced with new words. ElSherief et al. (2018) created a list o"
2020.alw-1.3,S19-2009,0,0.0232842,"Missing"
2020.alw-1.3,W17-3013,1,0.874247,"Missing"
2020.alw-1.3,W18-5104,0,0.0385476,"Missing"
2020.alw-1.3,gao-huang-2017-detecting,0,0.0159873,"e a common flaw of not distinguishing between offensive and hateful language. One important reason to keep these two separate is that hate speech is considered a felony in many countries. The task of separating offensive 2 Data Many existing datasets containing hate speech are publicly available for use and consist of data from several sources online, mainly Twitter (Waseem and Hovy, 2016; Waseem, 2016; Chatzakou et al., 2017; Golbeck et al., 2017; Davidson et al., 2017; Ross et al., 2016; ElSherief et al., 2018; Founta et al., 2018b), while some cover other sources such as Fox News comments (Gao and Huang, 2017) and sentences from posts on the white supremacist online forum Stormfront (de Gibert et al., 2018). Almost all available datasets are labelled by humans,2 which results in different approaches taken when creating and annotating the datasets. Some researchers use expert annotators (Waseem and Hovy, 2016), others use majority voting among several 2 Except for the 12M tweet SOLID dataset (Rosenthal et al., 2020). It is, however, distance-learned based on the manually annotated 14k OLID tweet set (Zampieri et al., 2019). ∗ Currently at Bekk Consulting AS 1 https://www.perspectiveapi.com 16 Procee"
2020.alw-1.3,W18-5113,0,0.0730343,"s were sampled and analysed. The model tends to predict instances containing clear racist or homophobic slurs as hate speech, while obvious hate speech appears more straightforward for the model to understand and accurately predict. Several instances annotated as ‘Hateful’, but predicted as ‘Normal’ or Offensive’ by the model do not appear to be clear hate speech and are perhaps mislabelled by the human coders and 22 System BERT Large Davidson et al. (2017) Founta et al. (2018a) Kshirsagar et al. (2018) P R F1 0.91 0.91 0.89 – 0.91 0.90 0.89 – 0.90 0.90 0.89 0.92 System BERT Base* BERT Large* Lee et al. (2018) CNNw Lee et al. (2018) RNN-LTCw Table 5: Dataset D comparison (weighted averages) R F1 0.800 0.802 0.789 0.804 0.812 0.809 0.808 0.815 0.806 0.805 0.783 0.805 Table 6: Dataset F comparison (weighted averages) System correctly predicted by the model. The text “ISIS message calls Trump ’foolish idiot”’ was found four times in the original dataset with different authors, being annotated twice as ‘Hateful’ and twice as ‘Offensive’, with the model predicting the human-chosen label on only one of the instances. As stated by Chatzakou et al. (2017), annotation is even hard for humans and this is an"
2020.alw-1.3,W18-5102,0,0.0274447,"Missing"
2020.alw-1.3,S19-2011,0,0.0339425,"Missing"
2020.alw-1.3,W19-3516,1,0.802264,"Missing"
2020.alw-1.3,C18-1093,0,0.0140738,"ms in combination with word embeddings from word2vec Mikolov et al. (2013) in various Convolutional Neural Network (CNN) setups, with the best performing model using transferred knowledge from word2vec. Adding character n-grams boosted precision, but lowered recall. Badjatiya et al. (2017) experimented with several machine learners and neural networks, with the best performer being an Long Short-Term Memory (LSTM) with random word vectors where the network’s output was used as input to a Gradient Boosted Decision Tree. However, their results have shown questionable and difficult to reproduce (Mishra et al., 2018; Fortuna et al., 2019). Pavlopoulos et al. (2017a,b) tested word embeddings from both GloVe and word2vec in an Recurrent Neural Network (RNN), while Pitsilis et al. (2018) utilised an RNN ensemble, although without use of word embeddings, but feeding standard vectorized word uni-grams to multiple LSTM networks, aggregating the classifications, to outperform the previous state-of-the-art. amateur annotators on platforms such as CrowdFlower (Davidson et al., 2017). However, the task of hate speech detection lacks a shared benchmark dataset (Schmidt and Wiegand, 2017) that can be used to measure"
2020.alw-1.3,W17-3006,0,0.0150886,"ble 1. The dataset created by Founta et al. (2018b) contains almost 100k annotated tweets with four labels, “Normal”, “Spam”, “Hateful” and “Abusive”. As the authors only provide tweet IDs for researches to retrieve tweets through the Twitter Application Programming Interface (API), some tweets may for several reasons not be retrievable, e.g., a tweet or the user account behind a tweet may have been deleted; thus, of the 99,799 provided tweet IDs, only 68,299 tweets were retrieved. The label distribution of those compared to the original label distribution for dataset F is shown in Table 2. 3 Park and Fung (2017) created a hybrid system that tried to capture features from two input levels, using two CNNs, one character-based and one word-based. Meyer and Gamb¨ack (2019) proposed an optimised architecture combining components with CNNs and LSTMs into one system. One part of the system used character n-grams as input while the other part used word embeddings. They used the dataset from Waseem and Hovy (2016), obtaining better results than previous solutions. Most of the research discussed above used that dataset (with labels ‘Sexist’, ‘Racist’ or ‘Neither’) or a slightly modified version (Waseem, 2016)."
2020.alw-1.3,W17-1101,0,0.0257336,"onable and difficult to reproduce (Mishra et al., 2018; Fortuna et al., 2019). Pavlopoulos et al. (2017a,b) tested word embeddings from both GloVe and word2vec in an Recurrent Neural Network (RNN), while Pitsilis et al. (2018) utilised an RNN ensemble, although without use of word embeddings, but feeding standard vectorized word uni-grams to multiple LSTM networks, aggregating the classifications, to outperform the previous state-of-the-art. amateur annotators on platforms such as CrowdFlower (Davidson et al., 2017). However, the task of hate speech detection lacks a shared benchmark dataset (Schmidt and Wiegand, 2017) that can be used to measure the performance of different machine learning models. Further, most annotation schemata follow Waseem and Hovy (2016) by splitting the data into only two basic classes, either hate and none hate or offensive and non-offensive (classes that then also often are split, e.g., labelling hateful tweets as either sexist or racist). However, it is debatable whether those labels are sufficient to represent hateful and abusive language. In contrast, a few datasets make the distinction between hateful and offensive language, e.g., Davidson et al. (2017) and Founta et al. (201"
2020.alw-1.3,W17-3004,0,0.0441745,"Missing"
2020.alw-1.3,D17-1117,0,0.0194004,"word2vec Mikolov et al. (2013) in various Convolutional Neural Network (CNN) setups, with the best performing model using transferred knowledge from word2vec. Adding character n-grams boosted precision, but lowered recall. Badjatiya et al. (2017) experimented with several machine learners and neural networks, with the best performer being an Long Short-Term Memory (LSTM) with random word vectors where the network’s output was used as input to a Gradient Boosted Decision Tree. However, their results have shown questionable and difficult to reproduce (Mishra et al., 2018; Fortuna et al., 2019). Pavlopoulos et al. (2017a,b) tested word embeddings from both GloVe and word2vec in an Recurrent Neural Network (RNN), while Pitsilis et al. (2018) utilised an RNN ensemble, although without use of word embeddings, but feeding standard vectorized word uni-grams to multiple LSTM networks, aggregating the classifications, to outperform the previous state-of-the-art. amateur annotators on platforms such as CrowdFlower (Davidson et al., 2017). However, the task of hate speech detection lacks a shared benchmark dataset (Schmidt and Wiegand, 2017) that can be used to measure the performance of different machine learning mo"
2020.alw-1.3,N18-1202,0,0.132063,"Missing"
2020.alw-1.3,W16-5618,0,0.0541703,"guage, e.g., automatic detection of offensive language in comments (Systrom, 2017, 2018) or giving a percentage of how likely a text is to be perceived as toxic.1 However, these and other existing tools share a common flaw of not distinguishing between offensive and hateful language. One important reason to keep these two separate is that hate speech is considered a felony in many countries. The task of separating offensive 2 Data Many existing datasets containing hate speech are publicly available for use and consist of data from several sources online, mainly Twitter (Waseem and Hovy, 2016; Waseem, 2016; Chatzakou et al., 2017; Golbeck et al., 2017; Davidson et al., 2017; Ross et al., 2016; ElSherief et al., 2018; Founta et al., 2018b), while some cover other sources such as Fox News comments (Gao and Huang, 2017) and sentences from posts on the white supremacist online forum Stormfront (de Gibert et al., 2018). Almost all available datasets are labelled by humans,2 which results in different approaches taken when creating and annotating the datasets. Some researchers use expert annotators (Waseem and Hovy, 2016), others use majority voting among several 2 Except for the 12M tweet SOLID data"
2020.alw-1.3,S19-2010,0,0.0285901,"Missing"
2020.alw-1.3,N16-2013,0,0.521188,"in tracking abusive language, e.g., automatic detection of offensive language in comments (Systrom, 2017, 2018) or giving a percentage of how likely a text is to be perceived as toxic.1 However, these and other existing tools share a common flaw of not distinguishing between offensive and hateful language. One important reason to keep these two separate is that hate speech is considered a felony in many countries. The task of separating offensive 2 Data Many existing datasets containing hate speech are publicly available for use and consist of data from several sources online, mainly Twitter (Waseem and Hovy, 2016; Waseem, 2016; Chatzakou et al., 2017; Golbeck et al., 2017; Davidson et al., 2017; Ross et al., 2016; ElSherief et al., 2018; Founta et al., 2018b), while some cover other sources such as Fox News comments (Gao and Huang, 2017) and sentences from posts on the white supremacist online forum Stormfront (de Gibert et al., 2018). Almost all available datasets are labelled by humans,2 which results in different approaches taken when creating and annotating the datasets. Some researchers use expert annotators (Waseem and Hovy, 2016), others use majority voting among several 2 Except for the 12M tw"
2020.icon-main.33,P17-4008,0,0.0139846,"us lines, subject to admissible tonal pattern and structural constraints. Yi et al. (2016) also generated Chinese quatrains line-byline based on user keywords, but using a sequenceto-sequence model with attention mechanism (Bahdanau et al., 2014), with a bi-directional RNN with gated recurrent units (GRU; Cho et al., 2014) as encoder-decoder to learn semantic relevance. Wang et al. (2016a) used a similar approach for character-by-character iambics generation, utilising a bi-LSTM as encoder and another LSTM as decoder to alleviate the quick-forgetting problem associated with conventional RNNs. Ghazvininejad et al. (2017) combined hard format constraints with an RNN to generate 14-line classical sonnets in iambic pentameter, given a usersupplied topic and a set of related words, using word2vec (Mikolov et al., 2013). Rhyme words were found using CMU Pronouncing Dictionary (CMUdict),1 with fixed pairs of often used words added to make the system find rhymes in rare topics. A Finite-state acceptor was built with paths for all 1 http://www.speech.cs.cmu.edu/cgi-bin/cmudict 246 Proceedings of the 17th International Conference on Natural Language Processing, pages 246–256 Patna, India, December 18 - 21, 2020. ©2020"
2020.icon-main.33,2020.acl-main.223,0,0.0628204,"Missing"
2020.icon-main.33,strapparava-valitutti-2004-wordnet,0,0.584354,"Missing"
2020.icon-main.33,W18-5813,0,0.195796,"e model with a polishing schema, refining the RNN-generated poem through several iterations. Also, while previous models were based on maximum likelihood estimation (MLE), which optimizes word-level loss and can lead to the systems remembering common patterns of the training corpus, Yi et al. (2018) added reinforcement learning to a basic generator pre-trained with MLE, simultaneously training two generators that learn both from the teacher (rewarder) and from each other. Automatic rewarders were designed corresponding to four criteria: fluency, coherence, meaningfulness, and overall quality. Tikhonov and Yamshchikov (2018b) aimed to generate poetry in the style of a specific author, using an LSTM to predict the next word based on a previous word sequence, with the embeddings of the document currently being analysed used to support the model at every step. Two datasets were used to train the model, in English and Russian. Several of the systems presented above implement a form of user input to influence the mood of the poetry, often related to a given sentiment. However, two such systems are particularly important in the way they include the use of sentiment: In the corpus-based approach of Full-FACE Poetry Gen"
2020.icon-main.33,C16-1100,0,0.073191,"a Recurrent Neural Network (RNN) to generate Chinese quatrains (stanzas with four lines), with the first line based on user-provided keywords giving the main concepts of the poem. Subsequent lines were generated based on previous lines, subject to admissible tonal pattern and structural constraints. Yi et al. (2016) also generated Chinese quatrains line-byline based on user keywords, but using a sequenceto-sequence model with attention mechanism (Bahdanau et al., 2014), with a bi-directional RNN with gated recurrent units (GRU; Cho et al., 2014) as encoder-decoder to learn semantic relevance. Wang et al. (2016a) used a similar approach for character-by-character iambics generation, utilising a bi-LSTM as encoder and another LSTM as decoder to alleviate the quick-forgetting problem associated with conventional RNNs. Ghazvininejad et al. (2017) combined hard format constraints with an RNN to generate 14-line classical sonnets in iambic pentameter, given a usersupplied topic and a set of related words, using word2vec (Mikolov et al., 2013). Rhyme words were found using CMU Pronouncing Dictionary (CMUdict),1 with fixed pairs of often used words added to make the system find rhymes in rare topics. A Fin"
2020.icon-main.33,D18-1353,0,0.050336,"ms in both English and French. Human evaluators scored the output highly with regard to fluency, coherence, meaningfulness and poeticness, even though only non-poetic text was used as training data for the generator. Unlike the one-pass generation for previous neural networks models, Yan (2016) proposed a generative model with a polishing schema, refining the RNN-generated poem through several iterations. Also, while previous models were based on maximum likelihood estimation (MLE), which optimizes word-level loss and can lead to the systems remembering common patterns of the training corpus, Yi et al. (2018) added reinforcement learning to a basic generator pre-trained with MLE, simultaneously training two generators that learn both from the teacher (rewarder) and from each other. Automatic rewarders were designed corresponding to four criteria: fluency, coherence, meaningfulness, and overall quality. Tikhonov and Yamshchikov (2018b) aimed to generate poetry in the style of a specific author, using an LSTM to predict the next word based on a previous word sequence, with the embeddings of the document currently being analysed used to support the model at every step. Two datasets were used to train"
2020.icon-main.33,P17-1125,0,0.0252941,"Missing"
2020.icon-main.33,D14-1074,0,0.034727,"nherent sentiment, which can be experienced by the readers. Earlier approaches to poetry generation followed a range of paths, such as methods based on templates (Gonc¸alo Oliveira, 2012) or corpora (Colton et al., 2012), evolutionary (Levy, 2001) or Case-Based Reasoning (Gerv´as, 2001) approaches, and generate-and-test (Gerv´as, 2000) or Blackboard (Misztal and Indurkhya, 2014) architectures. However, in recent years deep learners have proven powerful as poetry generators, including systems combining neural models with other techniques. As described below, Long Short-Term Memory Related Work Zhang and Lapata (2014) used a Recurrent Neural Network (RNN) to generate Chinese quatrains (stanzas with four lines), with the first line based on user-provided keywords giving the main concepts of the poem. Subsequent lines were generated based on previous lines, subject to admissible tonal pattern and structural constraints. Yi et al. (2016) also generated Chinese quatrains line-byline based on user keywords, but using a sequenceto-sequence model with attention mechanism (Bahdanau et al., 2014), with a bi-directional RNN with gated recurrent units (GRU; Cho et al., 2014) as encoder-decoder to learn semantic relev"
2020.icon-main.35,W13-1725,0,0.0199108,"ction 4 introduces the model architectures, before Section 5 provides an overview of the experiments and their results. Finally, Section 6 sums up the findings and provides suggestions for further study. 2 Data Sets The field of Native Language Identification is in a broader perspective quite young, and only gained serious momentum during the previous decade, most notably after Koppel et al. (2005) trained a Support Vector Machine (SVM) model on a vast amount of features including part-of-speech (POS) tags, n-grams and grammatical errors on ICLE, the International Corpus of Learner’s English (Brooke and Hirst, 2013). Since then, two shared tasks have been dedicated to NLI, in 2013 and 2017. The experiments below and the discussed related work all focus on two data sets, TOEFL11 (the main data set used in the shared tasks) and Reddit-L2: The TOEFL11 data set1 (Blanchard et al., 2013) consists of English essays written by people with 11 different first languages for the collegeentrance Test of English as a Foreign Language: Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu and Turkish. The corpus contains 13,100 essays, with 1,100 essays per language distributed as evenly a"
2020.icon-main.35,W17-5049,0,0.0262067,"Missing"
2020.icon-main.35,W17-5021,0,0.0343065,"Missing"
2020.icon-main.35,J18-3003,0,0.108142,"ia users of high proficiency (Goldin et al., 2018) have enabled the collection of much larger data sets with more languages. State-of-the-art approaches on TOEFL11 reach over 0.88 accuracy in identifying the native language of the author using text only (Cimino and Dell’Orletta, 2017), while the best results on more English-proficient social media users approach 0.69 when evaluated on the same topics as trained on (Goldin et al., 2018). However, today’s state-of-the-art systems are known for quite substantial drops in performance when tested on documents about topics not seen during training (Malmasi and Dras, 2018). While the NLI field is evolving, so are advances in deep learning. Recently, attention-based models have shown promising results on various NLP tasks, and have become the de facto standard in sequence-to-sequence processing. Models such as the Transformer (Vaswani et al., 2017) rely solely on attention mechanisms, allowing for heavy parallelisation in the model training, as no recurrence or convolution is required. Using a network of transformers, BERT (Bidirectional Encoder Representations from Transformers) obtained new stateof-the-art results on 11 natural language processing tasks varyin"
2020.icon-main.35,D18-1395,0,0.0635805,"Missing"
2020.icon-main.35,W17-5007,0,0.0342298,"t training on external corpora while testing on TOEFL11 caused a significant drop in accuracy. 262 The most common features used were word, character and part-of-speech n-grams. Four of the top five teams used at least word 4-grams, and some as high as 7- and 9-grams. The best team achieved an accuracy of 0.846 (Tetreault et al., 2013), and like the overwhelming majority of the participating teams used SVMs. Tetreault et al. (2012) additionally improved performance using ensemble methods. Later, Ionescu et al. (2014) improved the results further using string kernels. The 2017 NLI shared task (Malmasi et al., 2017) was similar to the 2013 task, but in addition to the text utilised TOEFL11 data from a speech-based NLI task included in the 2016 INTERSPEECH Computational Paralinguistics Challenge (Schuller et al., 2016). The raw speech data could not be distributed in that challenge, so the data set consisted of textual transcripts together with so called i-vectors, which are vectors of fixed length (here 800), working as lowerdimensional representations of high-dimensional sequential speech recordings. Hence the 2017 shared task was divided into three tracks: essays only, speech only, and a combined ‘fusi"
2020.icon-main.35,C18-1293,0,0.0119758,"er training of the pre-trained models from BERT’s check points on data containing spelling mistakes, providing a custom BERT model with embeddings tuned to include spelling mistakes. As BERT obtained such promising results on the Reddit-L2 in-domain data set, future work should look into how to increase the accuracy on TOEFL11, the standard data set for NLI. In addition of further pre-training of BERT to learn embeddings of spelling errors and other domain attributes, this would include using additional information sources, such as the sentiment and emotions reflected in the texts, as done by Markov et al. (2018b), or including information about punctuation (Markov et al., 2018a) or capitalisation. Clearly, training on more relevant data would also improve performance, so including training data from the italki corpus (Hudson and Jaf, 2018) should be feasible. It contains about 122,000 documents gathered from the language learning site italki, in the same languages as TOEFL11. With more power available, running BERT-large with a sequence length of 512 should be feasible, or alternatively extensions of BERT or similar models, such as ALBERT (A lite BERT; Lan et al., 2020), GPT-3 (Generative Pre-traine"
2020.icon-main.35,W17-5024,0,0.0151875,"a combined ‘fusion’ track using both text and speech. The fusion track submissions showed that combining written and spoken responses provided a large boost in prediction accuracy (Malmasi et al., 2017). Furthermore, ensemble-based systems were the most effective in all tasks, but typically the same features were used as in 2013, and SVMs were still the most popular approach, dominating deep learning models. Ircing et al. (2017) hypothesise that this could be due to the size of the TOEFL11 data set, and that more training examples could help deep learning models perform better. UnibicKernel (Ionescu and Popescu, 2017) performed best on the fusion track (0.932 accuracy),3 topped the speech-only track, and placed in the top tier in the essay track, by using multiple kernel learning and kernel discriminant analysis, KDA. KDA is a kernelised version of LDA, Linear Discriminant Analysis, that is, a method for finding the best linear combination of features that characterise or separate two or more classes, by projecting the data points down from a feature space where they not are linearly separable to a lower dimensional space where they are. 3 The 2017 shared task used macro-averaged F1 score as the official e"
2020.icon-main.35,W18-6218,0,0.0136036,"er training of the pre-trained models from BERT’s check points on data containing spelling mistakes, providing a custom BERT model with embeddings tuned to include spelling mistakes. As BERT obtained such promising results on the Reddit-L2 in-domain data set, future work should look into how to increase the accuracy on TOEFL11, the standard data set for NLI. In addition of further pre-training of BERT to learn embeddings of spelling errors and other domain attributes, this would include using additional information sources, such as the sentiment and emotions reflected in the texts, as done by Markov et al. (2018b), or including information about punctuation (Markov et al., 2018a) or capitalisation. Clearly, training on more relevant data would also improve performance, so including training data from the italki corpus (Hudson and Jaf, 2018) should be feasible. It contains about 122,000 documents gathered from the language learning site italki, in the same languages as TOEFL11. With more power available, running BERT-large with a sequence length of 512 should be feasible, or alternatively extensions of BERT or similar models, such as ALBERT (A lite BERT; Lan et al., 2020), GPT-3 (Generative Pre-traine"
2020.icon-main.35,D14-1142,0,0.0203475,"OpenTraining-2 allowed the use of any data, including TOEFL11. However, Open-Training-1 showed that training on external corpora while testing on TOEFL11 caused a significant drop in accuracy. 262 The most common features used were word, character and part-of-speech n-grams. Four of the top five teams used at least word 4-grams, and some as high as 7- and 9-grams. The best team achieved an accuracy of 0.846 (Tetreault et al., 2013), and like the overwhelming majority of the participating teams used SVMs. Tetreault et al. (2012) additionally improved performance using ensemble methods. Later, Ionescu et al. (2014) improved the results further using string kernels. The 2017 NLI shared task (Malmasi et al., 2017) was similar to the 2013 task, but in addition to the text utilised TOEFL11 data from a speech-based NLI task included in the 2016 INTERSPEECH Computational Paralinguistics Challenge (Schuller et al., 2016). The raw speech data could not be distributed in that challenge, so the data set consisted of textual transcripts together with so called i-vectors, which are vectors of fixed length (here 800), working as lowerdimensional representations of high-dimensional sequential speech recordings. Hence"
2020.icon-main.35,W13-1706,0,0.0128467,"Serbian. The number of users per label was capped at 104, which is the number of users present for the labels with fewest users (Lithuania and Slovenia). For labels with more than 104 users available, 104 users were selected at random. For each user, the median number of chunks was selected. 3 chunks per user for an in-domain scenario, and 17 chunks per user for an out-of-domain scenario, 3 Related Work As mentioned in the previous section, two shared tasks have been dedicated to NLI, taking place in 2013 and 2017, and with respectively 29 and 19 teams participating. The 2013 NLI Shared Task (Tetreault et al., 2013) introduced the TOEFL11 corpus and was divided into three sub-tasks: Closed-Training, Open-Training-1 and Open-Training-2. The first was the main task, allowing usage of the training and development sets only. The Open-Training-1 task allowed the use of any training data except TOEFL11, while OpenTraining-2 allowed the use of any data, including TOEFL11. However, Open-Training-1 showed that training on external corpora while testing on TOEFL11 caused a significant drop in accuracy. 262 The most common features used were word, character and part-of-speech n-grams. Four of the top five teams use"
2020.icon-main.35,C12-1158,0,0.0159586,"only. The Open-Training-1 task allowed the use of any training data except TOEFL11, while OpenTraining-2 allowed the use of any data, including TOEFL11. However, Open-Training-1 showed that training on external corpora while testing on TOEFL11 caused a significant drop in accuracy. 262 The most common features used were word, character and part-of-speech n-grams. Four of the top five teams used at least word 4-grams, and some as high as 7- and 9-grams. The best team achieved an accuracy of 0.846 (Tetreault et al., 2013), and like the overwhelming majority of the participating teams used SVMs. Tetreault et al. (2012) additionally improved performance using ensemble methods. Later, Ionescu et al. (2014) improved the results further using string kernels. The 2017 NLI shared task (Malmasi et al., 2017) was similar to the 2013 task, but in addition to the text utilised TOEFL11 data from a speech-based NLI task included in the 2016 INTERSPEECH Computational Paralinguistics Challenge (Schuller et al., 2016). The raw speech data could not be distributed in that challenge, so the data set consisted of textual transcripts together with so called i-vectors, which are vectors of fixed length (here 800), working as l"
2020.semeval-1.100,2020.semeval-1.163,0,0.0830955,"Missing"
2020.semeval-1.100,W18-3219,1,0.85107,"ma et al., 2015). To advance research in code-mixed language processing, few workshops have also been conducted. Four successful series of Mixed Script Information Retrieval have been organized at the Forum for Information Retrieval Evaluation (FIRE) (SahaRoy et al., 2013; Choudhury et al., 2014; Sequiera et al., 2015; Banerjee et al., 2016). Three workshops on Computational Approaches to Linguistic Code-Switching (CALCS) have been conducted which included shared tasks on language identification and Named Entity Recognition (NER) in code-mixed data (Solorio et al., 2014a; Molina et al., 2016; Aguilar et al., 2018). For our SentiMix Spanglish dataset, we adopt the SentiStrength (Vilares et al., 2015) annotation mechanism and conduct the annotation process over the unified corpus from the three CALCS workshops. 3 Task Description Although code-mixing has received some attention recently, properly annotated data is still scarce. We run a shared task to perform sentiment analysis of code-mixed tweets crawled from social media. Each tweet is classified into one of the three polarity classes - Positive, Negative, Neutral. Each tweet also has word-level language marking. We release two datasets - Spanglish an"
2020.semeval-1.100,2020.semeval-1.118,0,0.094663,"Missing"
2020.semeval-1.100,W15-4319,0,0.042123,"Missing"
2020.semeval-1.100,2020.semeval-1.172,0,0.0729464,"Missing"
2020.semeval-1.100,2020.semeval-1.182,0,0.0811279,"Missing"
2020.semeval-1.100,2020.semeval-1.175,0,0.0894491,"Missing"
2020.semeval-1.100,2020.semeval-1.121,0,0.0808882,"Missing"
2020.semeval-1.100,2020.semeval-1.119,0,0.0648513,"Missing"
2020.semeval-1.100,Q17-1010,0,0.0443846,"2014), making the task more difficult. Naturally, the difficulty will increase as the amount of code-mixing increases. To quantify the level of code-switching between languages in a sentence, Gamb¨ack and Das (2016) introduced a measure called Code Mixing Index (CMI) which considers the number of tokens of each language in a sentence and the number of tokens where the language switches. Finding the sentiment from code-mixed text has been attempted by some researchers. Mohammad et al. (2013) used SVM-based classifiers to detect sentiment in tweets and text messages using semantic information. Bojanowski et al. (2017) proposed a skip-gram based word representation model that classifies the sentiment of tweets and provides an extensive vocabulary list for language. Giatsoglou et al. (2017) trained lexicon-based document vectors, word embedding, and hybrid systems with the polarity of words to classify the sentiment of a tweet. Sharma et al. (2016) attempted shallow parsing of code-mixed data obtained from online social media, and Chittaranjan et al. (2014) tried word-level identification of code-mixed data to classify the sentiment. Some researchers also tried normalizing the text with lexicon lookup for se"
2020.semeval-1.100,2020.semeval-1.165,0,0.0668742,"Missing"
2020.semeval-1.100,W14-3908,0,0.0238306,"ttempted by some researchers. Mohammad et al. (2013) used SVM-based classifiers to detect sentiment in tweets and text messages using semantic information. Bojanowski et al. (2017) proposed a skip-gram based word representation model that classifies the sentiment of tweets and provides an extensive vocabulary list for language. Giatsoglou et al. (2017) trained lexicon-based document vectors, word embedding, and hybrid systems with the polarity of words to classify the sentiment of a tweet. Sharma et al. (2016) attempted shallow parsing of code-mixed data obtained from online social media, and Chittaranjan et al. (2014) tried word-level identification of code-mixed data to classify the sentiment. Some researchers also tried normalizing the text with lexicon lookup for sentiment analysis of code-mixed data (Sharma et al., 2015). To advance research in code-mixed language processing, few workshops have also been conducted. Four successful series of Mixed Script Information Retrieval have been organized at the Forum for Information Retrieval Evaluation (FIRE) (SahaRoy et al., 2013; Choudhury et al., 2014; Sequiera et al., 2015; Banerjee et al., 2016). Three workshops on Computational Approaches to Linguistic Co"
2020.semeval-1.100,P19-4007,0,0.0481051,"Missing"
2020.semeval-1.100,W14-5152,1,0.859608,"Missing"
2020.semeval-1.100,N19-1423,0,0.0289635,"y positive. The intention to proceed in this way is to enrich the original corpus annotations with sentiment-level labels. Moreover, the splits do not share the same distribution (i.e., development and test are more skewed than training) because we were annotating data on-demand rather than having available the entire corpus at any stage of the competition. Some annotated examples are provided in Table 2. The average CMI for the train, validation, and test sets are 21.84, 20.52, and 17.23, respectively. 5 Baseline We develop our baseline system using the pre-trained multilingual BERT (M-BERT; Devlin et al. (2019)). M-BERT was trained on 104 languages’ entire Wikipedia dump and the WordPiece (Wu et al., 2016) vocabulary of this model contains 110K sub-word tokens from these 104 languages. To balance the risk of low-resource languages being under-represented or over-fitted due to small training resources during pretraining, exponentially smoothed weighting was performed on the data during pre-training data creation and vocabulary creation. Although M-BERT was trained on monolingual data from different languages, it is capable of multilingual generalization in code-switching scenarios (Pires et al., 2019"
2020.semeval-1.100,2020.semeval-1.164,0,0.0861606,"Missing"
2020.semeval-1.100,W13-1102,0,0.0133803,"are inherently multilingual environments.2 Besides, multilingual communities around the world regularly express their thoughts in social media employing and alternating different languages in the same utterance. This mixing of languages, also known as code-mixing or code-switching,3 is a norm in multilingual societies and is one of the many NLP challenges that social media has facilitated. 1.1 Code-Mixing Challenges In addition to the writing aspects in social media, such as flexible grammar, permissive spelling, arbitrary punctuation, slang, and informal abbreviations (Baldwin et al., 2015; Eisenstein, 2013), code-mixing has introduced a diverse set of linguistic challenges. For instance, multilingual speakers tend to code-mix using a single alphabet regardless of whether the languages involved belong to different writing systems ∗ 1 Equal contribution. https://ritual-uh.github.io/sentimix2020/ This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 2 Statistics show that half of the messages on Twitter are in a language other than English (Schroeder, 2010). 3 We use code-mixing and code-switching intercha"
2020.semeval-1.100,L16-1292,1,0.903347,"Missing"
2020.semeval-1.100,2020.semeval-1.171,0,0.0951278,"Missing"
2020.semeval-1.100,2020.semeval-1.176,0,0.0834599,"Missing"
2020.semeval-1.100,2020.semeval-1.125,0,0.0948821,"Missing"
2020.semeval-1.100,2020.semeval-1.166,0,0.0581426,"Missing"
2020.semeval-1.100,P18-1031,0,0.0665313,"Missing"
2020.semeval-1.100,2020.semeval-1.170,0,0.079557,"Missing"
2020.semeval-1.100,2020.semeval-1.120,0,0.0613383,"Missing"
2020.semeval-1.100,2020.semeval-1.162,0,0.0710808,"Missing"
2020.semeval-1.100,2020.semeval-1.117,0,0.09011,"Missing"
2020.semeval-1.100,2020.semeval-1.103,0,0.0585778,"Missing"
2020.semeval-1.100,2020.semeval-1.126,0,0.0563388,"Missing"
2020.semeval-1.100,2020.semeval-1.177,0,0.0948764,"Missing"
2020.semeval-1.100,S13-2053,0,0.0336157,"found on social media which contains a lot of nonstandard spellings of words and unnecessary capitalization (Das and Gamb¨ack, 2014), making the task more difficult. Naturally, the difficulty will increase as the amount of code-mixing increases. To quantify the level of code-switching between languages in a sentence, Gamb¨ack and Das (2016) introduced a measure called Code Mixing Index (CMI) which considers the number of tokens of each language in a sentence and the number of tokens where the language switches. Finding the sentiment from code-mixed text has been attempted by some researchers. Mohammad et al. (2013) used SVM-based classifiers to detect sentiment in tweets and text messages using semantic information. Bojanowski et al. (2017) proposed a skip-gram based word representation model that classifies the sentiment of tweets and provides an extensive vocabulary list for language. Giatsoglou et al. (2017) trained lexicon-based document vectors, word embedding, and hybrid systems with the polarity of words to classify the sentiment of a tweet. Sharma et al. (2016) attempted shallow parsing of code-mixed data obtained from online social media, and Chittaranjan et al. (2014) tried word-level identifi"
2020.semeval-1.100,W16-5805,1,0.702393,"code-mixed data (Sharma et al., 2015). To advance research in code-mixed language processing, few workshops have also been conducted. Four successful series of Mixed Script Information Retrieval have been organized at the Forum for Information Retrieval Evaluation (FIRE) (SahaRoy et al., 2013; Choudhury et al., 2014; Sequiera et al., 2015; Banerjee et al., 2016). Three workshops on Computational Approaches to Linguistic Code-Switching (CALCS) have been conducted which included shared tasks on language identification and Named Entity Recognition (NER) in code-mixed data (Solorio et al., 2014a; Molina et al., 2016; Aguilar et al., 2018). For our SentiMix Spanglish dataset, we adopt the SentiStrength (Vilares et al., 2015) annotation mechanism and conduct the annotation process over the unified corpus from the three CALCS workshops. 3 Task Description Although code-mixing has received some attention recently, properly annotated data is still scarce. We run a shared task to perform sentiment analysis of code-mixed tweets crawled from social media. Each tweet is classified into one of the three polarity classes - Positive, Negative, Neutral. Each tweet also has word-level language marking. We release two"
2020.semeval-1.100,2020.semeval-1.124,0,0.0681298,"Missing"
2020.semeval-1.100,2020.semeval-1.169,0,0.0479718,"Missing"
2020.semeval-1.100,P19-1493,0,0.0323392,"vlin et al. (2019)). M-BERT was trained on 104 languages’ entire Wikipedia dump and the WordPiece (Wu et al., 2016) vocabulary of this model contains 110K sub-word tokens from these 104 languages. To balance the risk of low-resource languages being under-represented or over-fitted due to small training resources during pretraining, exponentially smoothed weighting was performed on the data during pre-training data creation and vocabulary creation. Although M-BERT was trained on monolingual data from different languages, it is capable of multilingual generalization in code-switching scenarios (Pires et al., 2019). We use the Transformers (Wolf et al., 2019) library to implement our framework and we fine-tune the pre-trained BERT-Base, Multilingual Cased model separately for each of the two languages. Based on our observation on the training split for each dataset, we set the highest sequence length to 40 and 56 tokens for Spanglish and Hinglish, respectively. Then, we fine-tune the model for three epochs using AdamW (Loshchilov and Hutter, 2019) optimizer (η = 2e−5 ). 9 https://requester.mturk.com/ An assignment is done by a single annotator. 11 We use the assignment review policy ScoreMyKnownAnswers/"
2020.semeval-1.100,N16-1159,0,0.0256245,"d the number of tokens where the language switches. Finding the sentiment from code-mixed text has been attempted by some researchers. Mohammad et al. (2013) used SVM-based classifiers to detect sentiment in tweets and text messages using semantic information. Bojanowski et al. (2017) proposed a skip-gram based word representation model that classifies the sentiment of tweets and provides an extensive vocabulary list for language. Giatsoglou et al. (2017) trained lexicon-based document vectors, word embedding, and hybrid systems with the polarity of words to classify the sentiment of a tweet. Sharma et al. (2016) attempted shallow parsing of code-mixed data obtained from online social media, and Chittaranjan et al. (2014) tried word-level identification of code-mixed data to classify the sentiment. Some researchers also tried normalizing the text with lexicon lookup for sentiment analysis of code-mixed data (Sharma et al., 2015). To advance research in code-mixed language processing, few workshops have also been conducted. Four successful series of Mixed Script Information Retrieval have been organized at the Forum for Information Retrieval Evaluation (FIRE) (SahaRoy et al., 2013; Choudhury et al., 20"
2020.semeval-1.100,2020.semeval-1.173,0,0.077831,"Missing"
2020.semeval-1.100,D08-1102,1,0.598825,"74 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 774–790 Barcelona, Spain (Online), December 12, 2020. (i.e., language scripts). This behavior is known as transliteration, and code-mixers rely on the phonetic patterns of their writing (i.e., the actual sound) to convey their thoughts in the foreign language (i.e., the language adapted to a new script) (Sitaram et al., 2019). Another common pattern in code-mixing is the alternation of languages at the word level. This behavior often happens by inflecting words from one language with the rules of another language (Solorio and Liu, 2008). For instance, in the second example below, the word pushes is the result of conjugating the English verb push according to Spanish grammar rules for the present tense in third person (in this case, the inflection -es). The Hinglish example shows that phonetic Latin script typing is a popular practice in India, instead of using Devanagari script to write Hindi words. We capture both transliteration and word-level code-mixing inflections in the Hinglish and Spanglish corpora of this competition, respectively. AyeHI aurHI enjoyEN kareHI Eng. Trans.: come and enjoy NoSP meSP pushesEN pleaseEN En"
2020.semeval-1.100,W14-3907,1,0.910074,"sentiment analysis of code-mixed data (Sharma et al., 2015). To advance research in code-mixed language processing, few workshops have also been conducted. Four successful series of Mixed Script Information Retrieval have been organized at the Forum for Information Retrieval Evaluation (FIRE) (SahaRoy et al., 2013; Choudhury et al., 2014; Sequiera et al., 2015; Banerjee et al., 2016). Three workshops on Computational Approaches to Linguistic Code-Switching (CALCS) have been conducted which included shared tasks on language identification and Named Entity Recognition (NER) in code-mixed data (Solorio et al., 2014a; Molina et al., 2016; Aguilar et al., 2018). For our SentiMix Spanglish dataset, we adopt the SentiStrength (Vilares et al., 2015) annotation mechanism and conduct the annotation process over the unified corpus from the three CALCS workshops. 3 Task Description Although code-mixing has received some attention recently, properly annotated data is still scarce. We run a shared task to perform sentiment analysis of code-mixed tweets crawled from social media. Each tweet is classified into one of the three polarity classes - Positive, Negative, Neutral. Each tweet also has word-level language ma"
2020.semeval-1.100,2020.semeval-1.122,0,0.373251,"Missing"
2020.semeval-1.100,2020.semeval-1.168,0,0.0472143,"Missing"
2020.semeval-1.100,2020.semeval-1.167,0,0.0594151,"Missing"
2020.semeval-1.100,2020.semeval-1.181,0,0.0809492,"Missing"
2020.semeval-1.100,W15-2902,0,0.181507,"Missing"
2020.semeval-1.100,2020.semeval-1.174,0,0.044579,"Missing"
2020.semeval-1.100,2020.semeval-1.179,0,0.0543759,"Missing"
2020.semeval-1.100,2020.semeval-1.183,0,0.0848258,"Missing"
2020.semeval-1.99,2020.semeval-1.112,0,0.0785168,"Missing"
2020.semeval-1.99,2020.semeval-1.157,0,0.0754856,"Missing"
2020.semeval-1.99,2020.semeval-1.102,0,0.0886475,"Missing"
2020.semeval-1.99,2020.semeval-1.146,0,0.0820491,"Missing"
2020.semeval-1.99,P19-1285,0,0.0118077,"d logistic regression for solving the task of humour classification and significant score. 765 • Hitachi: They have proposed simple but effective MODALITY ENSEMBLE that incorporates visual and textual deep-learning models, which are independently trained, rather than providing a single multi-modal joint network. They fine-tuned four pre-trained visual models (i.e., InceptionResNet (Szegedy et al., 2016), Polynet (Zhang et al., 2016), SENet (Hu et al., 2017), and PNASNet (Liu et al., 2017)) and four textual models (i.e., BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2018), Transformer-XL (Dai et al., 2019), and XLNet (Yang et al., 2019)), followed by the fusion of their predictions by ensemble methods to effectively capture cross- modal correlations. Task-A Sentiment Analsysis Participant / Team Macro-F1 Comparison with baseline(+/-) Vkeswani IITK 0.35466 (+)0.13701 Guoym 0.35197 (+)0.13432 Aihaihara 0.35017 (+)0.13252 Sourya Diptadas 0.34885 (+)0.13120 Irina Bejan 0.34755 (+)0.12990 Sabino INGEOTEC 0.34689 (+)0.12924 U.Walinska Urszula Walin´ska 0.34639 (+)0.12874 Souvik Mishra Kraken 0.34627 (+)0.12862 Lb732 SESAM 0.34600 (+)0.12835 Li Zhen hit-mitlab 0.34583 (+)0.12818 George.Vlad Eduardgzah"
2020.semeval-1.99,N19-1423,0,0.56676,"challenge was a great success, involving total of 583 participants, with varying submissions in different tasks comprising of 31, 26 and 23 submissions in Task A, Task B and Task C respectively where in evaluation phase, a user is allowed for 5 submissions per day. 27 teams submitted the system description paper. A brief description of the task wise top performing models is shown below. 7.1 Top 3 Task A systems @Memotion • IITK Vkeswani: Employed wide variety of methods, ranging from a simple linear classifier such as FFNN, Naive Bayes to transformers like MMBT (Rahman et al., 2019) and BERT (Devlin et al., 2019). Implemented the model considering only text and the combination of image and text. • Guoym: Used ensembling Method considering the textual features extracted using Bi-GRU, BERT (Devlin et al., 2019), or ELMo (Peters et al., 2018), image features extracted by Resnet50 (He et al., 2015) network and fusion features of text and images. • Aihaihara: Implemented the model that is a concatenation of visual and textual features obtained from n-gram language model and VGG-16 (Simonyan and Zisserman, 2015) pretrained model respectively. 7.2 Top 3 Task B and Task C systems @Memotion • UPB George: In or"
2020.semeval-1.99,2020.semeval-1.115,0,0.202852,"Missing"
2020.semeval-1.99,2020.semeval-1.151,0,0.0791868,"Missing"
2020.semeval-1.99,2020.semeval-1.147,0,0.0680622,"Missing"
2020.semeval-1.99,2020.semeval-1.111,0,0.0777053,"Missing"
2020.semeval-1.99,2020.semeval-1.113,0,0.0538536,"Missing"
2020.semeval-1.99,2020.semeval-1.152,0,0.0786869,"Missing"
2020.semeval-1.99,2020.semeval-1.150,0,0.0502881,"Missing"
2020.semeval-1.99,2020.semeval-1.116,0,0.23958,"Missing"
2020.semeval-1.99,2020.semeval-1.154,0,0.411447,"Missing"
2020.semeval-1.99,2020.semeval-1.149,0,0.174495,"Missing"
2020.semeval-1.99,D14-1162,0,0.0841572,"ssed using varying textual contents, so as to convey different emotions. In some cases, different memes can have same images, but due to the different textual messages embedded in each of them, different sentimental reactions can be induced from all. Recognition of the emotion induced in such memes would require accurate modelling of the textual influence. To evaluate automated emotion recognition from the meme textual content, we built text binary classifier as shown in the bottom half of Fig.3, to understand different classes of emotion. We have used 100-D pre-trained Glove word embeddings (Pennington et al., 2014) to generate word embeddings from text emd(txt). These embeddings 762 Figure 1: Plot depicting category-wise data distribution of meme emotion data-set [For eg. There are approx. 2200 memes in the data-set tagged as “Not funny”]. Figure 2: A Multi-level system for the task of emotion intensity prediction (1×14 dimensional), using the emotion class multi-label output (1×4 dimensional). are given as input xi to the CNN, having 64 filters of size 1×5 with Relu as activation function to extract the textual features. To reduce the dimension of number of parameters generated by CNN layer we have use"
2020.semeval-1.99,N18-1202,0,0.0156868,"allowed for 5 submissions per day. 27 teams submitted the system description paper. A brief description of the task wise top performing models is shown below. 7.1 Top 3 Task A systems @Memotion • IITK Vkeswani: Employed wide variety of methods, ranging from a simple linear classifier such as FFNN, Naive Bayes to transformers like MMBT (Rahman et al., 2019) and BERT (Devlin et al., 2019). Implemented the model considering only text and the combination of image and text. • Guoym: Used ensembling Method considering the textual features extracted using Bi-GRU, BERT (Devlin et al., 2019), or ELMo (Peters et al., 2018), image features extracted by Resnet50 (He et al., 2015) network and fusion features of text and images. • Aihaihara: Implemented the model that is a concatenation of visual and textual features obtained from n-gram language model and VGG-16 (Simonyan and Zisserman, 2015) pretrained model respectively. 7.2 Top 3 Task B and Task C systems @Memotion • UPB George: In order to extract most salient features from text input, they opted to use the ALBERT (Lan et al., 2019)model while VGG -16 (Simonyan and Zisserman, 2015) is used for extracting the visual features from image input. To determine the h"
2020.semeval-1.99,2020.semeval-1.153,0,0.0699054,"Missing"
2020.semeval-1.99,2020.semeval-1.158,0,0.0942945,"Missing"
2020.semeval-1.99,2020.semeval-1.160,0,0.261832,"Missing"
2020.semeval-1.99,2020.semeval-1.148,0,0.0659778,"Missing"
2020.semeval-1.99,S19-2010,0,0.0926346,"Missing"
2020.semeval-1.99,2020.semeval-1.159,0,0.335744,"Missing"
2020.semeval-1.99,2020.semeval-1.145,0,0.0701982,"Missing"
C92-4186,C90-3010,0,0.0421423,"Missing"
C92-4186,C88-2111,0,\N,Missing
C92-4186,C90-2070,0,\N,Missing
C92-4186,E89-1019,0,\N,Missing
C92-4186,P91-1021,1,\N,Missing
C92-4186,P85-1035,0,\N,Missing
C92-4186,P89-1004,0,\N,Missing
C92-4186,C86-1091,0,\N,Missing
C92-4186,P87-1027,0,\N,Missing
C92-4186,H90-1071,0,\N,Missing
C96-1024,P91-1021,1,0.751083,"Missing"
C96-1024,C92-1017,0,0.0762618,"onotonic representation language for compositional semantics as discussed in (Alshawi and Crouch, 1992). The QLF formalism incorporates a Davidsonian approach to semantics, containing underspecified quantifiers and operators, as well as 'anaphoric terms' which stand for entities and relations to be determined by reference resolution. In these respects, the basic ideas of the QLF formalism are quite similar to LUD. 5 5.1 Syntax-Semantics Implementation Interface and Grammar The LUD semantic construction component has been implemented in the grammar formalism TUG, Trace and Unification Grammar (Block and Schachtl, 1992), in a system called TrUG (in cooperation with Siemens AG, Munich, who provided the German syntax and the TrUG system). TUG is a formalism that combines ideas from Government and Binding theory, namely the use of traces, with unification in order to account for, for example, the free word order phenomena found in German. 5.1.1 S y n t a x and S e m a n t i c s A TUG grammar basically consists of PATR-II style context free rules with feature annotations. Each syntactic rule gets annotated with a semantic counterpart. In this way, syntactic derivation and semantic construction are fully interlea"
C96-1024,1995.tmi-1.2,0,0.334937,"Missing"
C96-1024,1993.mtsummit-1.11,0,0.0813897,"he interpretation of a category on the right side of a rule subsumes the interpretation of the left side of the nile. lar tasks. The actual implementation is described in Section 5, which also discusses coverage and points to some areas of further research. Finally, Section 6 sums up the previous discussion. 2 The Verbmobil Project The project Verbmobil funded by the German Federal Ministry of Research and Technology (BMBF) combines speech technology with machine translation techniques in order to develop a system for translation in face-to-face dialogues. The overall project is described in (Wahlster, 1993); in this section we will give a short overview of the key aspects. The ambitious overall objective of the Verbmobil project is to produce a device which will provide English translations of dialogues between German and Japanese businessmen who only have a restricted active, but larger passive knowledge of English. The domain is the scheduling of business appointments. The major requirement is to provide translations as and when users need them, and do so robustly and in (near) real-time. In order to achieve this, the system is composed of time-limited processing components which on the source"
C96-1024,P92-1005,0,\N,Missing
C98-1069,P91-1021,1,0.835158,"Missing"
C98-1069,P98-1024,1,0.788101,"of the sentential complements get plugged with their own sb-labels. This complicates the implementation of rules (9) and (10) a bit; they must also account for the fact that a daughter node may carry an i s l a n d type hole. 5 I m p l e m e n t a t i o n and E v a l u a t i o n The resolution algorithm described in Section 4 has been implemented in Verbmobil, a system which translates spoken German and Japanese into English (Bub et al., 1997). The underspecified semantic representation technique we have used in this paper reflects the core semantic part of the Verbmobil Interface Term, VIT (Bos et al., 1998). The aim of VIT is to describe a consistent interface structure between the different language analysis modules within Verbmobil. Thus, in contrast to our USR, VIT is a representation that encodes all the linguistic information of an utterance; in addition to the USR semantic structure of Sectiom 2, the Verbmobil Interface Term contains prosodic, syntactic, and discourse related information. In order to evaluate the algorithm, the results of the pluggings obtained for four dialogues in the Verbmobil test set were checked (Table 1). We only consider utterances for which the VITs contain more t"
C98-1069,Y96-1006,1,0.906882,"Missing"
C98-1069,C96-1024,1,\N,Missing
C98-1069,C98-1024,1,\N,Missing
E09-2017,dybkjaer-etal-2004-usability,0,0.0295261,"h}@cs.uta.fi Abstract Multimodal conversational spoken dialogues using physical and virtual agents provide a potential interface to motivate and support users in the domain of health and fitness. The paper presents a multimodal conversational Companion system focused on health and fitness, which has both a stationary and a mobile component. 1 Introduction Figure 1: H&F Companion Architecture Spoken dialogue systems have traditionally focused on task-oriented dialogues, such as making flight bookings or providing public transport timetables. In emerging areas, such as domainoriented dialogues (Dybkjaer et al., 2004), the interaction with the system, typically modelled as a conversation with a virtual anthropomorphic character, can be the main motivation for the interaction. Recent research has coined the term “Companions” to describe embodied multimodal conversational agents having a long lasting interaction history with their users (Wilks, 2007). Such a conversational Companion within the Health and Fitness (H&F) domain helps its users to a healthier lifestyle. An H&F Companion has quite different motivations for use than traditional task-based spoken dialogue systems. Instead of helping with a single,"
E17-1069,baccianella-etal-2010-sentiwordnet,0,0.0334636,"n-grams are known to be useful for any kind of textual classification, all the teams tested various lengths of n-grams (uni, bi, and tri-grams). Categorical features like part-of-speech (POS), word level features like capital letters, repeated words were also used. Linguistic Inquiry Word Count (LIWC) features were used by all the teams as their baselines. LIWC (Pennebaker et al., 2015) is a handcrafted lexicon specifically designed for psycholinguistic experiments. Another psycholinguistic lexicon called MRC (Wilson, 1988) was also used by a few teams, as well as lexica such as SentiWordNet (Baccianella et al., 2010) and WordNet Affect (Strapparava and Valitutti, 2004). Two more important textual features were discussed by the participating teams. Linguistic nuances, introduced by Tomlinson et al. (2013), is the depth of the verbs in the Wordnet troponymy hierarchy. Speech act features were utilized by Appling et al. (2013): the authors manually annotated the given Facebook corpus with speech acts and reported their correlation with the personality traits. Related Work State-of-the-art sentiment analysis (SA) systems look at a fragment of text in isolation. However, in order to design a Schwartz model cla"
E17-1069,N13-1132,0,0.0284911,"data. Timeline data includes their own posts and all the posts they are tagged in, and posts other people made on their Timeline. So far, data from 114 unique users has been collected, but the data is highly imbalanced (for some value types the distributions of ‘Yes’ and ‘No’ classes were in 90:10 ratio). Crowd-sourcing is a cheap and fast way to collect data, but unfortunately some annotators chose random labels to minimize their cognitive thinking load. These annotators can be considered as spammers and make aggregation of crowd-sourced data a challenging problem, as discussed in detail by Hovy et al. (2013). To filter out spammers, the MACE (Hovy et al., 2013) tool was used and data from 54 users discarded, so the final dataset includes only 60 participants. The average number of messages per user in the Facebook corpus is 681. Facebook Corpus Facebook (FB) is the most popular social networking site in the world, with 1.65 billion monthly active users during the first quarter of 2016.6 Therefore, Facebook was a natural first choice for corpus collection, but since the privacy policy of Facebook is very stringent, accessing Facebook data is challenging. To collect the corpus, a Facebook Canvas we"
E17-1069,D14-1214,0,0.0292094,"Missing"
E17-1069,S13-2053,0,0.0625729,"Missing"
E17-1069,P11-2008,0,0.087564,"Missing"
E17-1069,D14-1160,0,0.0519273,"Missing"
E17-1069,strapparava-valitutti-2004-wordnet,0,0.106117,"Missing"
gamback-olsson-2000-experiences,X96-1043,0,\N,Missing
H93-1042,P92-1005,1,0.896007,"Missing"
H93-1042,H91-1060,0,0.0229358,"Missing"
H93-1042,P92-1021,1,0.851126,"Missing"
H93-1042,E89-1019,1,0.814985,"inally modified by a progressive VP (aFiights going to Boston&quot;) to Swedish NPs modified by a relative clause ( &quot;Flygningar som gdr till Boston&quot;): [and,1;r(head), form(verb ( t enne=n, perf=P, prog=y), tr (rood))] Domain Adaptation We begin by describing the customizations performed to adapt the general CLE English grammar and lexicon to the ATIS domain. First, about 500 lexical entries needed to be added. Of these, about 450 were regular content words ( airfare, Boston, seven forty seven, etc.), all of which were added by a graduate student 3 using the interactive VEX lexicon acquisition tool [7]. About 55 other entries, not of a regular form, were also added. Of these, 26 corresponded to the letters of the alphabet, which were treated as a new syntactic class, 15 or so were interjections (Sure, OK, etc.), and seven were entries for the days of the week, which turned out to have slightly different syntactic properties in American and British English. The only genuinely new entries were for available, round trip, first class, nonstop and one way, all of which failed to fit syntactic patterns previously implemented within the grammar, (e.g. &quot;Flights available from United&quot;, &quot;Flights to B"
H93-1042,J90-1003,0,0.00854805,"Missing"
H93-1042,H91-1015,0,0.0314687,"urce and target languages (Section 3.3); An Explanation Based Learning (EBL) technique for automatically chunking the grammar into commonly occurring phrase-types, which has proven valuable in maximizing return on effort expended on coverage extension, and a set of procedures for automatic testing and reporting that helps to ensure smooth integration across aspects of the effort performed at the various sites involved (Section 4). 2. C O M P O N E N T S INTERFACES AND The speech translation process begins with SRI&apos;s DECIPHER(TM) system, based on hidden Markov modeling and a progressive search [12, 13]. It outputs to the source language processor a small lattice of word hypotheses generated using acoustic and language model scores. The language processor, for both English and Swedish, is the SRI Core Language Engine (CLE) [1], a unification-based, broad coverage natural language system for analysis and generation. Transfer occurs at the level of quasi logical form (QLF); transfer rules are defined in a simple declarative formalism [2]. Speech synthesis is performed by the Swedish Telecom PROPHON system [8], based on stored polyphones. This section describes in more detail these components a"
H93-1042,J90-1004,0,0.0602124,"Missing"
H93-1042,H93-1041,0,0.0556818,"Missing"
H93-1042,1993.tmi-1.16,0,\N,Missing
H93-1042,P91-1021,1,\N,Missing
K19-1088,P07-1033,0,0.335211,"Missing"
K19-1088,W18-4416,0,0.0290597,"t abusive language detection systems. They also observed unimpressive results when using ULMFiT (Howard and Ruder, 2018) for abusive language detection, but argued that model architecture is less important than the type of data and the annotation scheme. ˇ Karan and Snajder (2018) experimented with cross-domain training and testing, and opted to use the same model (LSVM) with minimal features and to preprocess in favour of interpretability. They also reported positive improvements using Frustratingly Easy Domain Adaptation (FEDA; Daum´e III, 2007) to augment smaller datasets with larger ones. Fortuna et al. (2018) concurred, stating that although models perform better on the data they are trained on, slightly improved performance can be obtained when adding more training data from other social media. Similarly, Waseem et al. (2018) attempted to address the problem of differences between datasets by building a robust multi-task learning model, which improves upon single-task performance by using auxiliary samples from select datasets. Their work revealed that such models could be competitive with the stateof-the-art single-task models with the additional benefit of allowing prediction on other datasets"
K19-1088,W18-4401,0,0.0872961,"Missing"
K19-1088,W17-3013,1,0.905262,"Missing"
K19-1088,W18-5113,0,0.225186,"018) report user-network data to be more important. Schmidt and Wiegland (2017) provides a comprehensive overview of many of the features used and their efficacy. In terms of models, popular classical classification approaches include Logistic Regression and LSVM (Linear Support Vector Machines). Deep Neural networks such as Convolutional Neural Networks, CNN (Zhang et al., 2018; Gamb¨ack and Sikdar, 2017) and variations of Recurrent Neural Networks, RNN (Pitsilis et al., 2018; Gao and Huang, 2017) have seen widespread success, regularly obtaining state-of-the-art results on various datasets. Lee et al. (2018) used the Founta et al. (2018) dataset to conduct a comparative study of the performance of many popular models. In the ‘OffensEval’ shared task (Zampieri et al., 2019b), the use of contextual embeddings such as BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) exhibited the best results. Waseem et al. (2017b) proposed that there exists an overlap between these subtasks and subsequently proposed a typology that emphasises identifying the target of abuse and whether the abuse is implicit or explicit. Their typology could potentially be applied to all stages of system development, from d"
K19-1088,gao-huang-2017-detecting,0,0.0878373,"eatures: Waseem and Hovy (2016) claim gender information leads to improved performance, while Unsv˚ag and Gamb¨ack (2018) report user-network data to be more important. Schmidt and Wiegland (2017) provides a comprehensive overview of many of the features used and their efficacy. In terms of models, popular classical classification approaches include Logistic Regression and LSVM (Linear Support Vector Machines). Deep Neural networks such as Convolutional Neural Networks, CNN (Zhang et al., 2018; Gamb¨ack and Sikdar, 2017) and variations of Recurrent Neural Networks, RNN (Pitsilis et al., 2018; Gao and Huang, 2017) have seen widespread success, regularly obtaining state-of-the-art results on various datasets. Lee et al. (2018) used the Founta et al. (2018) dataset to conduct a comparative study of the performance of many popular models. In the ‘OffensEval’ shared task (Zampieri et al., 2019b), the use of contextual embeddings such as BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) exhibited the best results. Waseem et al. (2017b) proposed that there exists an overlap between these subtasks and subsequently proposed a typology that emphasises identifying the target of abuse and whether the abus"
K19-1088,W16-3638,0,0.0248373,"tation model (Hierarchy of Multi-Class Classifiers). Choice of features has been the crucial difference between the various approaches to abusive language detection. For the most part, word-level n-grams have been highly predictive, with other linguistic features such as part-of-speech tags (Xu et al., 2012; Davidson et al., 2017) and sentiment score (Van Hee et al., 2015; Davidson et al., 2017) providing slight improvements. Due to their ability to perform better in an online setting where spelling errors and adversarial behaviour are commonplace, character-level features have been endorsed (Mehdad and Tetreault, 2016), and also shown to often be superior to word-level information for this task (Meyer and Gamb¨ack, 2019). Metadata about users have also been used as features: Waseem and Hovy (2016) claim gender information leads to improved performance, while Unsv˚ag and Gamb¨ack (2018) report user-network data to be more important. Schmidt and Wiegland (2017) provides a comprehensive overview of many of the features used and their efficacy. In terms of models, popular classical classification approaches include Logistic Regression and LSVM (Linear Support Vector Machines). Deep Neural networks such as Convo"
K19-1088,W19-3516,1,0.813617,"Missing"
K19-1088,D14-1162,0,0.0836481,"ying maximum sequence lengths between 60 and 70. Other parameters worth mentioning are the number of epochs and the Linear Warm-up Proportion. LSTM Network The tested Deep Learning Model was built on a fairly simple LSTM architecture using Keras6 with a TensorFlow7 back end. The ‘Adam’ optimiser (Kingma and Ba, 2014) was paired with categorical cross-entropy loss function for model training. Again no statistical or linguistic features were used and the only preprocessing involved lower-casing the tweets. The first layer used a 200 dimensional GloVe embedding,8 pre-trained on 2 billion tweets (Pennington et al., 2014), with embedding weights fixed throughout the training. The Embedding Layer was followed by an LSTM layer of 200 units. The final layer was a dense layer with softmax activation and layer size dependent on the number of classes in the dataset being tested. The most significant hyperparameters were found to be dropout and class weights. 4.3 BERT 4.5 Results The experimental results are recorded in Table 2, with most improvements and decrements in performance across models being minimal. BERT exhibits the best results for all datasets used in the experiments (with a significance level of 0.05)."
K19-1088,W18-5117,0,0.317861,"Missing"
K19-1088,N18-1202,0,0.0927756,"Missing"
K19-1088,S19-2123,0,0.050153,"Missing"
K19-1088,W17-1101,0,0.20007,"mpact that reviewing online abuse can have on a worker’s mental well-being. These issues have led to many social media giants, such as Facebook, to seek machine learning-based solutions — to replace or supplement the current human moderator system. Automatic detection of abusive language online can be seen as a union of the plethora of subtasks that have been tackled: Cyberbullying, Hate Speech (also further constrained as racism, sexism, and harassment of particular minorities), Trolling, etc. Research in the field tends to focus on one of the particular subtasks. It has been argued by some (Schmidt and Wiegland, 2017; Waseem et al., 2017b) that due to this phenomenon where works tackle restricted subsets of abusive language, it has become difficult to make judgements about whether the features being used can perform well in other subtasks of abusive language detection — as they are often only evaluated on a single dataset, specific to one domain and subtask, and annotated in a specific way. Work on Abusive Language Detection has tackled a wide range of subtasks and domains. As a result of this, there exists a great deal of redundancy and non-generalisability between datasets. Through experiments on cross-"
K19-1088,N12-1084,0,0.225901,"resent multi-class classification problems, with the exception of the Kaggle’s Toxic Comment Classification challenge,3 which entails multi-label classification, and OLID (Zampieri et al., 2019a) used in the SemEval-2019 ‘OffensEval’ shared task (Zampieri et al., 2019b), which builds on a hierarchical annotation model (Hierarchy of Multi-Class Classifiers). Choice of features has been the crucial difference between the various approaches to abusive language detection. For the most part, word-level n-grams have been highly predictive, with other linguistic features such as part-of-speech tags (Xu et al., 2012; Davidson et al., 2017) and sentiment score (Van Hee et al., 2015; Davidson et al., 2017) providing slight improvements. Due to their ability to perform better in an online setting where spelling errors and adversarial behaviour are commonplace, character-level features have been endorsed (Mehdad and Tetreault, 2016), and also shown to often be superior to word-level information for this task (Meyer and Gamb¨ack, 2019). Metadata about users have also been used as features: Waseem and Hovy (2016) claim gender information leads to improved performance, while Unsv˚ag and Gamb¨ack (2018) report u"
K19-1088,N19-1144,0,0.176582,"to convey informality, humour, and emphasis. This usage of ∗ Also at: RISE SICS, Kista, Sweden. 940 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 940–950 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics annotation typically carried out on crowdsourcing platforms such as CrowdFlower (Figure Eight)1 and Amazon Mechanical Turk.2 All these datasets represent multi-class classification problems, with the exception of the Kaggle’s Toxic Comment Classification challenge,3 which entails multi-label classification, and OLID (Zampieri et al., 2019a) used in the SemEval-2019 ‘OffensEval’ shared task (Zampieri et al., 2019b), which builds on a hierarchical annotation model (Hierarchy of Multi-Class Classifiers). Choice of features has been the crucial difference between the various approaches to abusive language detection. For the most part, word-level n-grams have been highly predictive, with other linguistic features such as part-of-speech tags (Xu et al., 2012; Davidson et al., 2017) and sentiment score (Van Hee et al., 2015; Davidson et al., 2017) providing slight improvements. Due to their ability to perform better in an online sett"
K19-1088,W18-5110,1,0.894678,"Missing"
K19-1088,R15-1086,0,0.107534,"Missing"
K19-1088,S19-2010,0,0.111172,"to convey informality, humour, and emphasis. This usage of ∗ Also at: RISE SICS, Kista, Sweden. 940 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 940–950 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics annotation typically carried out on crowdsourcing platforms such as CrowdFlower (Figure Eight)1 and Amazon Mechanical Turk.2 All these datasets represent multi-class classification problems, with the exception of the Kaggle’s Toxic Comment Classification challenge,3 which entails multi-label classification, and OLID (Zampieri et al., 2019a) used in the SemEval-2019 ‘OffensEval’ shared task (Zampieri et al., 2019b), which builds on a hierarchical annotation model (Hierarchy of Multi-Class Classifiers). Choice of features has been the crucial difference between the various approaches to abusive language detection. For the most part, word-level n-grams have been highly predictive, with other linguistic features such as part-of-speech tags (Xu et al., 2012; Davidson et al., 2017) and sentiment score (Van Hee et al., 2015; Davidson et al., 2017) providing slight improvements. Due to their ability to perform better in an online sett"
K19-1088,W12-2103,0,0.0458101,"ch (Davidson et al., 2017; Founta et al., 2018; Gao and Huang, 2017; Golbeck et al., 2017), Sexism/Racism (Waseem and Hovy, 2016), Cyberbullying (Xu et al., 2012; Dadvar et al., 2013), Trolling and Aggression (Kumar et al., 2018a), and so on. Datasets for these tasks have been collected from various social media platforms, such as Twitter (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018; Burnap and Williams, 2015; Golbeck et al., 2017), Facebook (Kumar et al., 2018a), Instagram (Hosseinmardi et al., 2015; Zhong et al., 2016), Yahoo! (Nobata et al., 2016; Djuric et al., 2015; Warner and Hirschberg, 2012), YouTube (Dinakar et al., 2011), and Wikipedia (Wulczyn et al., 2017), with 1 figure-eight.com mturk.com 3 bit.ly/2HNfLaB 2 941 (Zampieri et al., 2019a) was included since it is using the contemporary hierarchical model. Some other large datasets were discarded since they are either not from Twitter (such as the Kaggle Toxicity classification of Wikipedia comments, Wulczyn et al., 2017) or not easily or openly available (e.g., Silva et al., 2016; Golbeck et al., 2017). Generalisability of a model has also come under considerable scrutiny. Works such as Karan ˇ and Snajder (2018) and Gr¨ondahl"
K19-1088,W17-3012,0,0.688946,"abuse can have on a worker’s mental well-being. These issues have led to many social media giants, such as Facebook, to seek machine learning-based solutions — to replace or supplement the current human moderator system. Automatic detection of abusive language online can be seen as a union of the plethora of subtasks that have been tackled: Cyberbullying, Hate Speech (also further constrained as racism, sexism, and harassment of particular minorities), Trolling, etc. Research in the field tends to focus on one of the particular subtasks. It has been argued by some (Schmidt and Wiegland, 2017; Waseem et al., 2017b) that due to this phenomenon where works tackle restricted subsets of abusive language, it has become difficult to make judgements about whether the features being used can perform well in other subtasks of abusive language detection — as they are often only evaluated on a single dataset, specific to one domain and subtask, and annotated in a specific way. Work on Abusive Language Detection has tackled a wide range of subtasks and domains. As a result of this, there exists a great deal of redundancy and non-generalisability between datasets. Through experiments on cross-dataset training and"
K19-1088,N16-2013,0,0.611218,"rd-level n-grams have been highly predictive, with other linguistic features such as part-of-speech tags (Xu et al., 2012; Davidson et al., 2017) and sentiment score (Van Hee et al., 2015; Davidson et al., 2017) providing slight improvements. Due to their ability to perform better in an online setting where spelling errors and adversarial behaviour are commonplace, character-level features have been endorsed (Mehdad and Tetreault, 2016), and also shown to often be superior to word-level information for this task (Meyer and Gamb¨ack, 2019). Metadata about users have also been used as features: Waseem and Hovy (2016) claim gender information leads to improved performance, while Unsv˚ag and Gamb¨ack (2018) report user-network data to be more important. Schmidt and Wiegland (2017) provides a comprehensive overview of many of the features used and their efficacy. In terms of models, popular classical classification approaches include Logistic Regression and LSVM (Linear Support Vector Machines). Deep Neural networks such as Convolutional Neural Networks, CNN (Zhang et al., 2018; Gamb¨ack and Sikdar, 2017) and variations of Recurrent Neural Networks, RNN (Pitsilis et al., 2018; Gao and Huang, 2017) have seen"
L16-1292,W14-5152,1,0.642887,"users that mix languages in their writing still tend to avoid code-switching inside a specific tweet, a fact that has been utilized to investigate which language is dominant in a tweet (Carter, 2012; Lignos and Marcus, 2013; Voss et al., 2014). However, tweets still tend to be somewhat formal by more often following grammatical norms and using standard lexical items (Hu et al., 2013), while chats are more conversational (Paolillo, 1999), and hence less formal, which tend to increase their level of code-switching (Cárdenas-Claros and Isharyanti, 2009; Paolillo, 2011; Nguyen and Do˘gruöz, 2013; Das and Gambäck, 2014). The paper is organized as follows: Section 2. describes a formal measure that can be used to compare the complexity 1850 of code-switched corpora. Section 3. then uses this corpus level switching measure in practise, applying it to a set of recently produced code-switched corpora. Finally, Section 4. sums up and elaborates on the results. 2. Measuring Code-Switching in Corpora When comparing different code-switched corpora to each other, it is desirable to have a measurement of the level of mixing between languages, in particular since error rates for various language processing application"
L16-1292,N13-1037,0,0.0216155,"of applying language processing tools to one code-switched corpus to those on another? Or more specifically: how can we compare the level of codeswitching in corpora? And for corpora containing a mix of a specific set of languages or across corpora from differThat two texts come from social media does not in itself imply that they belong to one, delimited textual domain. Rather, there is a wide spectrum of different types of texts that are transmitted through social media, and the level of formality of the language in addition depends more on the style of the writer than on the actual media (Eisenstein, 2013; Androutsopoulos, 2011). They both argue that the common denominator of social media text is not that it is ‘noisy’ and informal per se, but that it describes language in (rapid) change. Furthermore, although social media often convey more ungrammatical text than more formal writings, Baldwin et al. (2013) have shown that the relative occurrence of non-standard syntax is fairly constant among many types of media, such as mails, tweets, forums, comments, and blogs. Due to the ease of availability of Twitter, most research on social media text has so far focused on tweets (Twitter messages). Lu"
L16-1292,R15-1033,1,0.860797,"13.83 22.32 35.18 13.29 14.50 20.26 21.97 31.06 49.06 17.06 Table 1: Code-switching levels in some corpora Using these weights, Equation 10 becomes   U  100 1 X 5 fm (x)+fp (x)+δ(x) +6 S Cc = U 2 x=1 = 100 U (12) max {tLi}(x)+P (x)   X U   Li ∈L 1 5 1− S +δ(x) + 2 6 N (x) x=1 To show how the measure can be used in practice to objectively compare the complexity of code-switching, Cc values as in Equation 12 were calculated for some recently produced code-switched corpora: the Dutch-Turkish chat corpus of Nguyen and Do˘gruöz (2013), the English-Hindi Twitter and Facebook chat corpus of Jamatia et al. (2015), and the four corpora6 used in the shared task on word-level language detection in code-switched text (Solorio et al., 2014) organized by the workshop on Computational Approaches to Code Switching at the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). When comparing Cc values for different corpora, it is necessary to consider their respective tagsets and annotation guidelines. So does the annotation strategy chosen for the EMNLP corpora prescribe that elements such as abbreviations should be tagged with the language they belong to, while other annotations schemes"
L16-1292,W14-1303,0,0.0121514,"13; Androutsopoulos, 2011). They both argue that the common denominator of social media text is not that it is ‘noisy’ and informal per se, but that it describes language in (rapid) change. Furthermore, although social media often convey more ungrammatical text than more formal writings, Baldwin et al. (2013) have shown that the relative occurrence of non-standard syntax is fairly constant among many types of media, such as mails, tweets, forums, comments, and blogs. Due to the ease of availability of Twitter, most research on social media text has so far focused on tweets (Twitter messages). Lui and Baldwin (2014) note that users that mix languages in their writing still tend to avoid code-switching inside a specific tweet, a fact that has been utilized to investigate which language is dominant in a tweet (Carter, 2012; Lignos and Marcus, 2013; Voss et al., 2014). However, tweets still tend to be somewhat formal by more often following grammatical norms and using standard lexical items (Hu et al., 2013), while chats are more conversational (Paolillo, 1999), and hence less formal, which tend to increase their level of code-switching (Cárdenas-Claros and Isharyanti, 2009; Paolillo, 2011; Nguyen and Do˘gr"
L16-1292,D13-1084,0,0.117092,"Missing"
L16-1292,W14-3907,0,0.153917,"ts, Equation 10 becomes   U  100 1 X 5 fm (x)+fp (x)+δ(x) +6 S Cc = U 2 x=1 = 100 U (12) max {tLi}(x)+P (x)   X U   Li ∈L 1 5 1− S +δ(x) + 2 6 N (x) x=1 To show how the measure can be used in practice to objectively compare the complexity of code-switching, Cc values as in Equation 12 were calculated for some recently produced code-switched corpora: the Dutch-Turkish chat corpus of Nguyen and Do˘gruöz (2013), the English-Hindi Twitter and Facebook chat corpus of Jamatia et al. (2015), and the four corpora6 used in the shared task on word-level language detection in code-switched text (Solorio et al., 2014) organized by the workshop on Computational Approaches to Code Switching at the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). When comparing Cc values for different corpora, it is necessary to consider their respective tagsets and annotation guidelines. So does the annotation strategy chosen for the EMNLP corpora prescribe that elements such as abbreviations should be tagged with the language they belong to, while other annotations schemes treat them as language independent. Another potential problem can be to decide whether a tag is directly language related or"
L16-1292,voss-etal-2014-finding,0,0.0153529,"ext than more formal writings, Baldwin et al. (2013) have shown that the relative occurrence of non-standard syntax is fairly constant among many types of media, such as mails, tweets, forums, comments, and blogs. Due to the ease of availability of Twitter, most research on social media text has so far focused on tweets (Twitter messages). Lui and Baldwin (2014) note that users that mix languages in their writing still tend to avoid code-switching inside a specific tweet, a fact that has been utilized to investigate which language is dominant in a tweet (Carter, 2012; Lignos and Marcus, 2013; Voss et al., 2014). However, tweets still tend to be somewhat formal by more often following grammatical norms and using standard lexical items (Hu et al., 2013), while chats are more conversational (Paolillo, 1999), and hence less formal, which tend to increase their level of code-switching (Cárdenas-Claros and Isharyanti, 2009; Paolillo, 2011; Nguyen and Do˘gruöz, 2013; Das and Gambäck, 2014). The paper is organized as follows: Section 2. describes a formal measure that can be used to compare the complexity 1850 of code-switched corpora. Section 3. then uses this corpus level switching measure in practise, ap"
L16-1292,I13-1041,0,\N,Missing
L18-1447,D11-1052,0,0.0454464,"Missing"
L18-1447,S13-2053,0,0.0555004,"Missing"
L18-1447,S16-1001,0,0.060475,"Missing"
L18-1447,H05-1044,0,0.1881,"Missing"
P91-1021,P88-1005,0,0.0225365,"ce of referents for pronouns and definite descriptions, and relations implied by compound nouns and ellipsis. They are also neutral with respect to other ambiguities corresponding to alternative scopings of quantifiers and operators and to the collective/distributive and referential/attributive distinctions. The QLF is thus the level of representation encoding the results of compositional linguistic analysis independently of contextually sensitive aspects of understanding. These aspects are addressed by the contextual interpretation phase which has the following subphases: quantifier scoping (Moran 1988), reference resolution (Alshawi 1990), and plausibility judgement. Deriving a fairly conventional Logical Form (LF) from the RQLF is then a simple formal mapping which removes the information in the RQLF that is not concerned with truth conditions. Linguistic analysis and contextual interpretation each consist of several subphases. For analysis these are: orthography, morphological analysis, syntactic analysis (parsing), and (compositional) semantic analysis. Apart from the first, these analysis subphases are based on the unification grammar paradigm, and they all use declarative bidirectional"
P91-1021,P89-1004,1,0.91546,"Missing"
P91-1021,C86-1021,0,0.0140938,"example, topichood or the given/new distinction is preserved. LEVELS The representational structures on which transfer operates must contain information corresponding to several linguistic levels, including syntax and semantics. For transfer to be general, it must operate recursively on input representations. We call the level of representation on which this recursion operates the ""organizing"" level; semantic structure is the natural choice, since the basic requirement of translation is that it preserves meaning. Syntactic phrase structure transfer, or deepsyntax transfer (e.g. Thurmair 1990, Nagao and Tsujii 1986) results in complex transfer rules, and the predicate-argument structure which is required for the application of sortal restrictions is not represented. Finally, in contrast to systems such as Rosetta (Landsbergen, 1986) which depend on stating rule by rule correspondences between source and target grammars, we wish to make the monolingual descriptions as independent as possible from the task of translating between two languages. Apart from McCord&apos;s (1988, 1989) organizing level appears to be/hat, of surface syntax, with additional deep syntactic and semantic content attached to nodes. As we"
P91-1021,J90-3001,1,0.924809,"d the use of interaction to resolve context dependent ambiguities, but in this paper we concentrate on representational and transfer issues. 1 INTRODUCTION In this paper we describe a translation project whose aim is to build an experimental Bilingual Conversation Interpreter (BCI) which will allow communication through typed text between two monolingual humans using different languages (of Miike et al, 1988). The choice of languages for the prototype system is English and Swedish. Input sentences are analysed by the Core Language Engine (CLE 1) as far as the level of Quasi Logical Form (QLF; Alshawi, 1990), and then, instead of further ambiguity resolution, undergo transfer into another QLF having constants and predicates corresponding to word senses in the other language. The transfer rules used in this process correspond to a certain kind of meaning postulate. The CLE then generates an output, sentence from the target 2 CLE REPRESENTATION LEVELS In this section we explain how QLF fits into the overall architecture of the CLE and in section 3 we discuss the reasons for choosing it for interactive dialogue translation. 1Tile CLE is described in Alshawi (1991) which includes more detailed discus"
P91-1021,J90-1004,0,0.0504575,"Missing"
P91-1021,C86-1025,0,0.0157927,"resentation are produced by successive modular components. [past, [hire, q_term (<t =quant, n=s ing>, E, [event, E] ), a_term(<t =ref, p=pro, l=she, n=sing>, Y, [female, Y] ), q_t erm (<t =quant, n=sing>, C, a_f orm(<t =pred, l=one>, P, [ P . C ] ) ) ] ] . Generation of linguistic expressions in the CLE takes place from QLFs (or from RQLFs by mapping them to suitable QLFs). Since the rules 162 may be related to the fact that McCord&apos;s system is explicitly not symmetrical: different grammars are used for the analysis and synthesis of the same language, which are viewed as quite different tasks. Isabelle and Macklovitch (1986) argue against such asymmetry between analysis and synthesis on the grounds that, although it is tempting as a short-cut to building a structure sufficiently well-specified for synthesis to take place, asymmetry means that the transfer component must contain a lot of knowledge about the target language, with dire consequences for the modularity of the system and the reusability of different parts of it. In the BCI, however, the transfer rules contain only cross-linguistic knowledge, allowing the analysis and generation to make use of exactly the same data. in which categories are shown as list"
P91-1021,J89-1003,0,0.0238708,"Missing"
P91-1021,P88-1019,0,0.0662624,"ing for a choice between word sense paraphrases or between alternative partial bracketings of the sentence. There • is thus a strong connection between our choice of a representation sensitive to context and the use of interaction to resolve context dependent ambiguities, but in this paper we concentrate on representational and transfer issues. 1 INTRODUCTION In this paper we describe a translation project whose aim is to build an experimental Bilingual Conversation Interpreter (BCI) which will allow communication through typed text between two monolingual humans using different languages (of Miike et al, 1988). The choice of languages for the prototype system is English and Swedish. Input sentences are analysed by the Core Language Engine (CLE 1) as far as the level of Quasi Logical Form (QLF; Alshawi, 1990), and then, instead of further ambiguity resolution, undergo transfer into another QLF having constants and predicates corresponding to word senses in the other language. The transfer rules used in this process correspond to a certain kind of meaning postulate. The CLE then generates an output, sentence from the target 2 CLE REPRESENTATION LEVELS In this section we explain how QLF fits into the"
P91-1021,E89-1037,0,\N,Missing
P98-1072,P91-1021,1,0.831298,"Missing"
P98-1072,P98-1024,1,0.781595,"e lexical setting (12) holeinfo isa-hole hole island Hole So, isa-hole indicates which type of hole a structure contains. The values are no, yes, and i s l a n d , i s l a n d is used to override the argument structure to produce a plugging where 436 Implementation and Evaluation The resolution algorithm described in Section 4 has been implemented in Verbmobil, a system which translates spoken German and Japanese into English (Bub et al., 1997). The underspecified semantic representation technique we have used in this paper reflects the core semantic part of the Verbmobil Interface Term, VIT (Bos et al., 1998). The aim of VIT is to describe a consistent interface structure between the different language analysis modules within Verbmobil. Thus, in contrast to our USR, VIT is a representation that encodes all the linguistic information of an utterance; in addition to the USR semantic structure of Sectiom 2, the Verbmobil Interface Term contains prosodic, syntactic, and discourse related information. In order to evaluate the algorithm, the results of the pluggings obtained for four dialogues in the Verbmobil test set were checked (Table 1). We only consider utterances for which the VITs contain more t"
P98-1072,Y96-1006,1,0.906315,"Missing"
P98-1072,C96-1024,1,\N,Missing
P98-1072,C98-1024,1,\N,Missing
R15-1033,W14-3914,0,0.123456,"Missing"
R15-1033,W14-3902,1,0.649564,"Missing"
R15-1033,W14-3915,0,0.0978188,"Missing"
R15-1033,sankaran-etal-2008-common,0,0.0701194,"zer,3 which is a sub-module of the CMU Twitter POS tagger (Gimpel et al., 2011). Although the CMU tokenizer was originally developed for English, empirical testing showed that it works reasonably well also for the Indian languages. 3.2 Category Noun (G_N) Pronoun (G_PRP) Part-of-Speech Tagsets We experimented with both coarse-grained and fine-grained tagsets, utilizing the fine-grained set during annotation. As can be seen in Table 2, this tagset includes both the Twitter specific tags introduced by Gimpel et al. (2011) and a set of POS tags for Indian languages that combines the ILPOST tags (Baskaran et al., 2008), the tags developed by the Central Institute of Indian Languages (LDCIL), and those suggested by the Indian Government’s Department of Information Technology (TDIL),4 that is, an approach similar to that taken for Gujarati by Dholakia and Yoonus (2014). The coarse-grained tagset instead combines Gimpel et al.’s Twitter specific tags with Google’s Universal Tagset (Petrov et al., 2011).5 The mapping between our fine-grained tagset and the Google Universal Tagset is also shown in Table 2. 3.3 Verb (G_V) Adjective (G_J) Adverb (G_R) Demonstrative (G_PRP) Quantifier (G_SYM) Particles (G_PRT) Resi"
R15-1033,Y10-1011,0,0.0773972,"Missing"
R15-1033,P11-2008,0,0.238554,"Missing"
R15-1033,W12-0601,0,0.0390013,"Missing"
R15-1033,C82-1023,0,0.438276,"s, on the other hand, concentrated on English tweets, whereas the majority of these texts now are written in other media and in other languages — or in mixes of languages. Today, code-switching is generally recognised as a natural part of bi- and multilingual language use, even though it historically often was considered a sub-standard use of language. Conversational spoken language code-switching has been a common research theme in psycho- and sociolinguists for half a century, and the first work on applying language processing methods to codeswitched text was carried out in the early 1980s (Joshi, 1982), while code-switching in social media text started to be studied in the late 1990s (Paolillo, 1996). Still, code alternation in conventional texts is not so prevalent as to spur much interest by the computational linguistic research community, and it was only recently that it became a research topic in its own right, with a code-switching workshop at EMNLP 2014 (Solorio et al., 2014), and a shared tasks at EMNLP and at Forum for Information Retrieval Evaluation, FIRE 2014. Both these shared tasks were on automatic word-level language detection in code-mixed text, but here we will assume that"
R15-1033,W14-5152,1,0.263112,"ble 2. 3.3 Verb (G_V) Adjective (G_J) Adverb (G_R) Demonstrative (G_PRP) Quantifier (G_SYM) Particles (G_PRT) Residual (G_X) Comparing Corpora Complexity The error rates for various language processing applications would be expected to be higher for more complex code-mixed text. When comparing different code-mixed corpora to each other, it is thus desirable to have a measurement of the level of mixing between languages. Kilgarriff (2001) discusses various statistical measures that can be used to compare corpora more objectively, but all those measures presume the corpora to be monolingual. In Das and Gambäck (2014) we instead suggested a Code-Mixing Index, CMI, to document the frequency of languages in a corpus, which we will use here as well. In short, the measure is defined as: if an utterance only contains language independent tokens, its CMI is zero; for other utterances, the CMI is calculated by counting the Conjunction, Pre& Postposition Numeral Determiner Twitter-Specific (Gimpel et al. 2011) (G_X) Type N_NN N_NNV N_NST N_NNP PR_PRP PR_PRL PR_PRF PR_PRC PR_PRQ V_VM V_VAUX Description Common Noun Verbal Noun Spatio-temporal Proper Noun Personal Relative Reflexive Reciprocal Wh-Word Main Auxiliary"
R15-1033,R13-1026,0,0.0259527,"Missing"
R15-1033,D14-1098,0,0.0634373,"r than the combination tagger and the one based on CRFs. There are several possible avenues that could be further explored on NLP for code-mixed texts, for example, transliteration, utterance boundary detection, language identification, and parsing. We are currently working on language modelling of code-mixed text to recognize which language is mixing into which. Language modelling has not before been applied to code-mixed POS tagging, but code-switched language models have previously been integrated into speech recognisers, although mostly by naïvely interpolating between monolingual models. Li and Funng (2014) instead obtained a code-switched language model by combining the matrix language model with a translation model from the matrix language to the mixed language. In the future, we also wish to explore language modelling on code-mixed text in order to address the problems caused by unknown words. Jannis Androutsopoulos. 2011. Language change and digital media: a review of conceptions and evidence. In Tore Kristiansen and Nikolas Coupland, editors, Standard Languages and Language Standards in a Changing Europe, pages 145–159. Novus, Oslo, Norway, Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacK"
R15-1033,N13-1037,0,0.0182022,"l, that is, in regions where languages change over short geospatial distances and people generally have at least a basic knowledge of the neighbouring languages. In particular, India is home to several hundred languages, with language diversity and dialectal changes instigating frequent code-mixing. We will here look at the tasks of collecting and annotating code-mixed English-Hindi social media text, and on automatic part-of-speech (POS) 239 Proceedings of Recent Advances in Natural Language Processing, pages 239–248, Hissar, Bulgaria, Sep 7–9 2015. this way, as discussed in detail by, e.g., Eisenstein (2013) and Androutsopoulos (2011). They both argue that the common denominator of social media text is not that it is ‘noisy’ and informal per se, but that it describes language in (rapid) change, which in turn has major implications for natural language processing: if we build a system that can handle a specific type of social media text today, it will be outdated tomorrow. Something which makes it very attractive to apply machine learning and adaptive techniques to the problem. In all types of social media, the level of formality of the language depends more on the style of the writer than on the"
R15-1033,W12-2101,0,0.0347566,"Missing"
R15-1033,gimenez-marquez-2004-svmtool,0,0.0687355,"Missing"
R15-1033,D11-1141,0,0.140325,"Missing"
R15-1033,N13-1039,0,0.081552,"Missing"
R15-1033,D08-1102,0,0.437157,"Missing"
R15-1033,D08-1110,0,0.808345,"Missing"
R15-1033,W14-3907,0,0.201864,"anguage code-switching has been a common research theme in psycho- and sociolinguists for half a century, and the first work on applying language processing methods to codeswitched text was carried out in the early 1980s (Joshi, 1982), while code-switching in social media text started to be studied in the late 1990s (Paolillo, 1996). Still, code alternation in conventional texts is not so prevalent as to spur much interest by the computational linguistic research community, and it was only recently that it became a research topic in its own right, with a code-switching workshop at EMNLP 2014 (Solorio et al., 2014), and a shared tasks at EMNLP and at Forum for Information Retrieval Evaluation, FIRE 2014. Both these shared tasks were on automatic word-level language detection in code-mixed text, but here we will assume that the word-level languages are known and concentrate on the task of automatic part-of-speech tagging for these types of texts. We have collected a corpus consisting of Facebook messages and tweets (which includes all The paper reports work on collecting and annotating code-mixed English-Hindi social media text (Twitter and Facebook messages), and experiments on automatic tagging of thes"
R15-1033,C12-2096,0,0.0162427,"Missing"
R15-1033,E09-1087,0,0.0139957,"Missing"
R15-1033,D14-1105,0,0.415598,"Missing"
R15-1033,A97-1004,0,0.050424,"Missing"
R15-1033,D13-1084,0,\N,Missing
R15-1033,I13-1041,0,\N,Missing
R15-1033,W11-3501,0,\N,Missing
S13-1008,S12-1051,0,0.0588826,") and 1st place on the SMT data set. 1 Introduction Intuitively, two texts are semantically similar if they roughly mean the same thing. The task of formally establishing semantic textual similarity clearly is more complex. For a start, it implies that we have a way to formally represent the intended meaning of all texts in all possible contexts, and furthermore a way to measure the degree of equivalence between two such representations. This goes far beyond the state-of-the-art for arbitrary sentence pairs, and several restrictions must be imposed. The Semantic Textual Similarity (STS) task (Agirre et al., 2012, 2013) limits the comparison to isolated sentences As in most language processing tasks, there are two overall ways to measure sentence similarity, either by data-driven (distributional) methods or by knowledge-driven methods; in the STS’12 task the two approaches were used nearly equally much. Distributional models normally measure similarity in terms of word or word co-occurrence statistics, or through concept relations extracted from a corpus. The basic strategy taken by NTNU in the STS’13 task was to use something of a “feature carpet bombing approach” in the way of first automatically ex"
S13-1008,S12-1094,0,0.0216421,"ms. 3 http://storage.googleapis.com/books/ ngrams/books/datasetsv2.html, version 20120701, with 468,491,999,592 words 4 http://code.google.com/p/ dkpro-similarity-asl/ 2 69 by Jaccard (n = 1, 3, 4), and Word n-grams by Jaccard w/o Stopwords (n = 2, 4). Semantic similarity measures include WordNet Similarity based on the Resnik measure (two variants) and Explicit Semantic Similarity based on WordNet, Wikipedia or Wiktionary. This means that we reused all features from DKPro run 1 except for Distributional Thesaurus. 6 Systems Our systems follow previous submissions to the STS ˇ c et al., 2012; Banea et al., 2012) in task (e.g., Sari´ that feature values are extracted for each sentence pair and combined with a gold standard score in order to train a Support Vector Regressor on the resulting regression task. A postprocessing step guarantees that all scores are in the [0, 5] range and equal 5 if the two sentences are identical. SVR has been shown to be a powerful technique for predictive data analysis when the primary goal is to approximate a function, since the learning algorithm is applicable to continuous classes. Hence support vector regression differs from support vector machine classification where"
S13-1008,S12-1059,0,0.0585081,"Missing"
S13-1008,S12-1085,0,0.270722,"Missing"
S13-1008,nivre-etal-2006-maltparser,0,0.023835,"h s2 ; second, if there was a partial match until the first argument (Commerce buy:Goods(buy); and third if there was a match of the frame category (Commerce buy:Goods). In STS’12, Singh et al. (2012) matched Universal Networking Language (UNL) graphs against each other by counting matches of relations and universal words, while Bhagwani et al. (2012) calculated WordNet-based word-level similarities and created a weighted bipartite graph (see Section 2). The method employed here instead looked at the graph edit distance between dependency graphs obtained with the Maltparser dependency parser (Nivre et al., 2006). Edit distance is the defined as the minimum of the sum of the costs of the edit operations (insertion, deletion and substitution of nodes) required to transform one graph into the other. It is approximated with a fast but suboptimal algorithm based on bipartite graph matching through the Hungarian algorithm (Riesen and Bunke, 2009). 5 Reused Features ˇ c et al., 2012) obThe TakeLab ‘simple’ system (Sari´ tained 3rd place in overall Pearson correlation and 1st for normalized Pearson in STS’12. The source code1 was used to generate all its features, that is, n-gram overlap, WordNet-augmented w"
S13-1008,N10-1013,0,0.0300533,"se Random Indexing (RI; Kanerva et al., 2000; Sahlgren, 2005), also employed in STS’12 by Tovar et al. (2012); Sokolov (2012); Semeraro et al. (2012). It is an efficient method for modelling higher order cooccurrence similarities among terms, comparable to Latent Semantic Analysis (LSA; Deerwester et al., 1990). It incrementally builds a term co-occurrence matrix of reduced dimensionality through the use of a sliding window and fixed size index vectors used for training context vectors, one per unique term. A novel variant, which we have called “Multisense Random Indexing” (MSRI), inspired by Reisinger and Mooney (2010), attempts to capture one or more “senses” per unique term in an unsupervised manner, each sense represented as an individual vector in the model. The method is similar to classical sliding window RI, but each term can have multiple context vectors (referred to as “sense vectors” here) which are updated individually. When updating a term vector, instead of directly adding the index vectors of the neighbouring terms in the window to its context vector, the system first computes a separate window vector consisting of the sum of the index vectors. Then cosine similarity is calculated between the"
S13-1008,S12-1060,0,0.090198,"Missing"
S13-1008,S12-1087,0,0.271279,"lem of combining all the extracted feature values. Three different systems were created based on feature performance on the supplied development data. Section 7 discusses scores on the STS’12 and STS’13 test data. 2 Compositional Word Matching Compositional word matching similarity is based on a one-to-one alignment of words from the two sentences. The alignment is obtained by maximal weighted bipartite matching using several word similarity measures. In addition, we utilise named entity recognition and matching tools. In general, the approach is similar to the one described by Karnick et al. (2012), with a different set of tools used. Our implementation relies on the ANNIE components in GATE (Cunningham et al., 2002) and will thus be referred to as GateWordMatch. The processing pipeline for GateWordMatch is: (1) tokenization by ANNIE English Tokeniser, (2) part-of-speech tagging by ANNIE POS Tagger, (3) lemmatization by GATE Morphological Analyser, (4) stopword removal, (5) named entity recognition based on lists by ANNIE Gazetteer, (6) named entity recognition based on the JAPE grammar by the ANNIE NE Transducer, (7) matching of named entities by ANNIE Ortho Matcher, (8) computing Word"
S13-1008,S12-1098,0,0.0308299,"emantic frames as this one for the sentence “Indian air force to buy 126 Rafale fighter jets”: Commerce buy:Goods(buy,jet) Entity:Entity(jet,jet) Entity:Name(jet,Rafale) Entity:Name(jet,fighter) Possibilities:Event(hyp,buy) Request:Addressee(air,you) Request:Message(air,air) Transitive action:Beneficiary(buy,jet) Three features were extracted from this: first, if there was an exact match of the frame found in s1 with s2 ; second, if there was a partial match until the first argument (Commerce buy:Goods(buy); and third if there was a match of the frame category (Commerce buy:Goods). In STS’12, Singh et al. (2012) matched Universal Networking Language (UNL) graphs against each other by counting matches of relations and universal words, while Bhagwani et al. (2012) calculated WordNet-based word-level similarities and created a weighted bipartite graph (see Section 2). The method employed here instead looked at the graph edit distance between dependency graphs obtained with the Maltparser dependency parser (Nivre et al., 2006). Edit distance is the defined as the minimum of the sum of the costs of the edit operations (insertion, deletion and substitution of nodes) required to transform one graph into the"
S13-1008,S12-1078,0,0.133408,"n Algorithm (Kuhn, 1955). Nodes in the bipartite graph represent words from the sentences, and edges have weights that correspond to similarities between tokens obtained in step 8. Weighted bipartite matching finds the one-toone alignment that maximizes the sum of similarities between aligned tokens. Total similarity normalised by the number of words in both sentences is used as the final sentence similarity measure. 3 Distributional Similarity Our distributional similarity features use Random Indexing (RI; Kanerva et al., 2000; Sahlgren, 2005), also employed in STS’12 by Tovar et al. (2012); Sokolov (2012); Semeraro et al. (2012). It is an efficient method for modelling higher order cooccurrence similarities among terms, comparable to Latent Semantic Analysis (LSA; Deerwester et al., 1990). It incrementally builds a term co-occurrence matrix of reduced dimensionality through the use of a sliding window and fixed size index vectors used for training context vectors, one per unique term. A novel variant, which we have called “Multisense Random Indexing” (MSRI), inspired by Reisinger and Mooney (2010), attempts to capture one or more “senses” per unique term in an unsupervised manner, each sense r"
S13-1008,S12-1071,0,0.0309002,"ed using the Hungarian Algorithm (Kuhn, 1955). Nodes in the bipartite graph represent words from the sentences, and edges have weights that correspond to similarities between tokens obtained in step 8. Weighted bipartite matching finds the one-toone alignment that maximizes the sum of similarities between aligned tokens. Total similarity normalised by the number of words in both sentences is used as the final sentence similarity measure. 3 Distributional Similarity Our distributional similarity features use Random Indexing (RI; Kanerva et al., 2000; Sahlgren, 2005), also employed in STS’12 by Tovar et al. (2012); Sokolov (2012); Semeraro et al. (2012). It is an efficient method for modelling higher order cooccurrence similarities among terms, comparable to Latent Semantic Analysis (LSA; Deerwester et al., 1990). It incrementally builds a term co-occurrence matrix of reduced dimensionality through the use of a sliding window and fixed size index vectors used for training context vectors, one per unique term. A novel variant, which we have called “Multisense Random Indexing” (MSRI), inspired by Reisinger and Mooney (2010), attempts to capture one or more “senses” per unique term in an unsupervised mann"
S13-1008,S13-1004,0,\N,Missing
S13-2071,W10-3110,0,0.166202,". Looking at system output with and without the ‘NTNU’ data, both one-step SVM and MaxEnt models and SVM→MaxEnt classified more tweets as negative when trained on the extra data; however, while NTNUC benefited slightly from this, NTNUU even performed better without it. An obvious extension to the present work would be to try other classification algorithms (e.g., Conditional Random Fields or more elaborate ensembles) or other features (e.g., character n-grams). Rather than the very simple treatment of negation used here, an approach to automatic induction of scope through a negation detector (Councill et al., 2010) could be used. It would also be possible to relax the domain-independence further, in particular to utilize sentiment lexica (including twitter specific), e.g., by automatic phrase-polarity lexicon extraction (Velikovich et al., 2010). Since many users tweet from their smartphones, and a large number of them use iPhones, several tweets contain iPhone-specific smilies (“Emoji”). Emoji are implemented as their own character set (rather than consisting of characters such as ‘:)’ and ‘:(’, etc.), so a potentially major improvement could be to convert them to characterbased smilies or to emotion-s"
S13-2071,P11-2008,0,0.0929666,"Missing"
S13-2071,C00-1044,0,0.0359183,"s possible to classify the sentiment of tweets in a single step; however, the approach to TSA most used so far is a two-step strategy where the first step is subjectivity classification and the second step is polarity classification. The goal of subjectivity classification is to separate subjective and objective statements. Pak and Paroubek (2010) counted word frequencies in a subjective vs an objective set of tweets; the results showed that interjections and personal pronouns are the strongest indicators of subjectivity. In general, these word classes, adverbs and (in particular) adjectives (Hatzivassiloglou and Wiebe, 2000) have shown to be good subjectivity indicators, which has made part430 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 430–437, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics of-speech (POS) tagging a reasonable technique for filtering out objective tweets. Early research on TSA showed that the challenging vocabulary made it harder to accurately tag tweets; however, Gimpel et al. (2011) report on using a POS tagger for marking tweets, performing wit"
S13-2071,S13-2053,0,0.0414128,"Missing"
S13-2071,pak-paroubek-2010-twitter,0,0.0254898,"dman (2013) for overviews. and emotionally loaded abbreviations. Thus the data will normally go through some preprocessing before any classification is attempted, e.g., by filtering out Twitter specific symbols and functions, in particular retweets (reposting another user’s tweet), mentions (’@’, tags used to mention another user), hashtags (’#’, used to tag a tweet to a certain topic), emoticons, and URLs (linking to an external resource, e.g., a news article or a photo). The first system to really use Twitter as a corpus was created as a student course project at Stanford (Go et al., 2009). Pak and Paroubek (2010) experimented with sentiment classification of tweets using Support Vector Machines and Conditional Random Fields, benchmarked with a Na¨ıve Bayes Classifier baseline, but were unable to beat the baseline. Later, and as Twitter has grown in popularity, many other systems for Twitter Sentiment Analysis (TSA) have been developed (see, e.g., Maynard and Funk, 2011; Mukherjee et al., 2012; Saif et al., 2012; Chamlertwat et al., 2012). Clearly, it is possible to classify the sentiment of tweets in a single step; however, the approach to TSA most used so far is a two-step strategy where the first st"
S13-2071,W10-0513,0,0.0445295,"verviews. and emotionally loaded abbreviations. Thus the data will normally go through some preprocessing before any classification is attempted, e.g., by filtering out Twitter specific symbols and functions, in particular retweets (reposting another user’s tweet), mentions (’@’, tags used to mention another user), hashtags (’#’, used to tag a tweet to a certain topic), emoticons, and URLs (linking to an external resource, e.g., a news article or a photo). The first system to really use Twitter as a corpus was created as a student course project at Stanford (Go et al., 2009). Pak and Paroubek (2010) experimented with sentiment classification of tweets using Support Vector Machines and Conditional Random Fields, benchmarked with a Na¨ıve Bayes Classifier baseline, but were unable to beat the baseline. Later, and as Twitter has grown in popularity, many other systems for Twitter Sentiment Analysis (TSA) have been developed (see, e.g., Maynard and Funk, 2011; Mukherjee et al., 2012; Saif et al., 2012; Chamlertwat et al., 2012). Clearly, it is possible to classify the sentiment of tweets in a single step; however, the approach to TSA most used so far is a two-step strategy where the first st"
S13-2071,N10-1119,0,0.0130021,"g just is less effective for analysing tweet polarity. In this paper we will explore the application of a set of machine learning algorithms to the task of Twitter sentiment classification, comparing one-step and two-step approaches, and investigate a range of different preprocessing methods. What we explicitly will not do, is to utilise a sentiment lexicon, even though many methods in TSA rely on lexica with a sentiment score for each word. Nielsen (2011) manually built a sentiment lexicon specialized for Twitter, while others have tried to induce such lexica automatically with good results (Velikovich et al., 2010; Mohammad et al., 2013). However, sentiment lexica — and in particular specialized Twitter sentiment lexica — make the classification more domain dependent. Here we will instead aim to exploit domain independent approaches as far as possible, and thus abstain from using sentiment lexica. The rest of the paper is laid out as follows: Section 2 introduces the twitter data sets used in the study. Then Section 3 describes the system built for carrying out the twitter sentiment classification experiments, which in turn are reported and discussed in Sections 4 and 5. 2 Data Manually collecting info"
S13-2071,H05-1044,0,0.0167776,"es for traditional language processing systems. The posts (“tweets”) are limited to 140 characters and often contain misspellings, slang and abbreviations. On the other hand, the posts are often opinionated in nature as a very result of their informal character, which has led Twitter to being a gold mine for sentiment analysis (SA). SA for longer texts, such as movie reviews, has been explored since the 1990s;1 however, the limited amount of attributes in tweets makes the feature vectors shorter than in documents and the task of analysing them closely related to phrase- and sentence-level SA (Wilson et al., 2005; Yu and Hatzivassiloglou, 2003). Hence there are no guarantees that algorithms that perform well on document-level SA will do as well on tweets. On the other hand, it is possible to exploit some of the special features of the web language, e.g., emoticons 1 See Pang and Lee (2008); Feldman (2013) for overviews. and emotionally loaded abbreviations. Thus the data will normally go through some preprocessing before any classification is attempted, e.g., by filtering out Twitter specific symbols and functions, in particular retweets (reposting another user’s tweet), mentions (’@’, tags used to me"
S13-2071,W03-1017,0,0.0858158,"nguage processing systems. The posts (“tweets”) are limited to 140 characters and often contain misspellings, slang and abbreviations. On the other hand, the posts are often opinionated in nature as a very result of their informal character, which has led Twitter to being a gold mine for sentiment analysis (SA). SA for longer texts, such as movie reviews, has been explored since the 1990s;1 however, the limited amount of attributes in tweets makes the feature vectors shorter than in documents and the task of analysing them closely related to phrase- and sentence-level SA (Wilson et al., 2005; Yu and Hatzivassiloglou, 2003). Hence there are no guarantees that algorithms that perform well on document-level SA will do as well on tweets. On the other hand, it is possible to exploit some of the special features of the web language, e.g., emoticons 1 See Pang and Lee (2008); Feldman (2013) for overviews. and emotionally loaded abbreviations. Thus the data will normally go through some preprocessing before any classification is attempted, e.g., by filtering out Twitter specific symbols and functions, in particular retweets (reposting another user’s tweet), mentions (’@’, tags used to mention another user), hashtags (’"
S13-2071,S13-2052,0,\N,Missing
S14-2078,S13-1004,0,0.0868792,"ing ones were used for the new measures, which are based on cosine similarity between the document vectors derived from each sentence in a given pair. Introduction The Semantic Textual Similarity (STS) shared task aims at providing a unified framework for evaluating textual semantic similarity, ranging from exact semantic equivalence to completely unrelated texts. This is represented by the prediction of a similarity score between two sentences, drawn from a particular category of text, which ranges from 0 (different topics) to 5 (exactly equivalent) through six grades of semantic similarity (Agirre et al., 2013). This paper describes the NTNU submission to the SemEval 2014 STS shared task (Task 10). The approach is based on the lexical and distributional features of the baseline Takeˇ c et al., Lab system from the 2012 shared task (Sari´ 2012), but improves on it in three ways: by adding two new categories of features and by using a bagging regression model to predict similarity scores. The new feature categories added are based on soft cardinality and character n-grams, described 2.1 Soft Cardinality Measures Soft cardinality resembles classical set cardinality as it is a method for counting the num"
S14-2078,S13-2047,1,0.93627,"s from TakeLab’s baseline system. The final NTNU system is based on bagged support vector machine regression over the datasets from previous shared tasks and shows highly competitive performance, being the best system on three of the datasets and third best overall (on weighted mean over all six datasets). 1 Sergio Jimenez sgjimenezv@unal.edu.co 2 Feature Generation Methods The methods used for creating new features utilise soft cardinality and character n-grams. Soft cardinality (Jimenez et al., 2010) was used successfully for the STS task in previous SemEval editions (Jimenez et al., 2012a; Jimenez et al., 2013a). The NTNU systems utilise an ensemble of such 18 measures, based only on surface text information, which were extracted using soft cardinality with different similarity functions, as further described in Section 2.1. Section 2.2 then introduces the similarity measures based on character n-gram feature representations, which proved themselves as the strongest features in the STS 2013 task (Marsi et al., 2013). The measures used here replace character n-gram features with cluster frequencies or vector values based on the n-gram collocational structure learned in an unsupervised manner from te"
S14-2078,J92-4003,0,0.0488292,"A ∩ B| |A|/|A∪B| |B|/|A∪B| (|B|−|A∩B|)/|B| (|B|−|A∩B|)/|A∪B| |A∩B|/|A| |A∩B|/|B| |A∩B|/|A∪B| (|A∪B|−|A∩B|)/|A∩B| Table 1: Soft cardinality features. document vectors, here the centroid of the individual term vector representations, which are trained on character n-grams rather than full words. The vector representations are induced in an unsupervised manner from large unannotated corpora using word clustering, topic learning and word representation learning methods. In this paper, three different methods have been used for creating the character n-gram feature representations: Brown Clusters (Brown et al., 1992), Latent Semantic Indexing (LSI) topics (Deerwester et al., 1990), and log linear skip-gram models (Mikolov et al., 2013). The Brown clusters were trained using the implementation by Liang (2005), while the LSI topic vectors and log linear skip-gram representations were trained using the ˇ uˇrek and Gensim topic modelling framework (Reh˚ Sojka, 2010). In addition, tf-idf (Term-Frequency Inverse Document Frequency) weighting was used when training LSI topic models. We used a cosine distance measure between document vectors consisting of the centroid of the term representation vectors. For Brown"
S14-2078,S13-1008,1,0.838295,"Missing"
S14-2078,S13-1007,0,0.104058,"ews headlines images OnWN tweet-news mean weighted mean NTNU-run1 r rank 0.4369 16 0.7138 14 0.7219 17 0.8000 9 0.8348 7 0.4109 33 0.6531 20 0.6631 21 NTNU-run2 r rank 0.5084 2 0.7656 6 0.7525 13 0.8129 4 0.7767 20 0.7921 1 0.7347 4 0.7491 4 NTNU-run3 r rank 0.5305 1 0.7813 2 0.7837 1 0.8343 1 0.8502 4 0.6755 13 0.7426 2 0.7549 3 Best r 0.5305 0.7850 0.7837 0.8343 0.8745 0.7921 0.7429 0.7610 Table 4: Final evaluation results for the submitted systems. 5 Results and Discussion ing text as a bag of dependencies (syntactic soft cardinality) obtaining the best results in the typedsimilarity task (Croce et al., 2013). From our results it can be noted that for most categories the sublexical representation measures show strong performance in NTNU-run2, with a significantly better result for the combined system NTNU-run3. This indicates that while the soft cardinality features are weaker predictors overall, they are complimentary to the sublexical and lexical features of NTNU-run2. It is also indicative that this is not the case for the tweet-news category, where the text is more “free form” and less normative, so it would be expected that sublexical approaches should have stronger performance. The final eva"
S14-2078,S12-1061,1,0.86448,"lexical distance metrics from TakeLab’s baseline system. The final NTNU system is based on bagged support vector machine regression over the datasets from previous shared tasks and shows highly competitive performance, being the best system on three of the datasets and third best overall (on weighted mean over all six datasets). 1 Sergio Jimenez sgjimenezv@unal.edu.co 2 Feature Generation Methods The methods used for creating new features utilise soft cardinality and character n-grams. Soft cardinality (Jimenez et al., 2010) was used successfully for the STS task in previous SemEval editions (Jimenez et al., 2012a; Jimenez et al., 2013a). The NTNU systems utilise an ensemble of such 18 measures, based only on surface text information, which were extracted using soft cardinality with different similarity functions, as further described in Section 2.1. Section 2.2 then introduces the similarity measures based on character n-gram feature representations, which proved themselves as the strongest features in the STS 2013 task (Marsi et al., 2013). The measures used here replace character n-gram features with cluster frequencies or vector values based on the n-gram collocational structure learned in an unsu"
S14-2078,S12-1102,1,0.923082,"lexical distance metrics from TakeLab’s baseline system. The final NTNU system is based on bagged support vector machine regression over the datasets from previous shared tasks and shows highly competitive performance, being the best system on three of the datasets and third best overall (on weighted mean over all six datasets). 1 Sergio Jimenez sgjimenezv@unal.edu.co 2 Feature Generation Methods The methods used for creating new features utilise soft cardinality and character n-grams. Soft cardinality (Jimenez et al., 2010) was used successfully for the STS task in previous SemEval editions (Jimenez et al., 2012a; Jimenez et al., 2013a). The NTNU systems utilise an ensemble of such 18 measures, based only on surface text information, which were extracted using soft cardinality with different similarity functions, as further described in Section 2.1. Section 2.2 then introduces the similarity measures based on character n-gram feature representations, which proved themselves as the strongest features in the STS 2013 task (Marsi et al., 2013). The measures used here replace character n-gram features with cluster frequencies or vector values based on the n-gram collocational structure learned in an unsu"
S14-2078,S12-1060,0,0.136255,"Missing"
S14-2078,S13-1028,1,0.90744,"s from TakeLab’s baseline system. The final NTNU system is based on bagged support vector machine regression over the datasets from previous shared tasks and shows highly competitive performance, being the best system on three of the datasets and third best overall (on weighted mean over all six datasets). 1 Sergio Jimenez sgjimenezv@unal.edu.co 2 Feature Generation Methods The methods used for creating new features utilise soft cardinality and character n-grams. Soft cardinality (Jimenez et al., 2010) was used successfully for the STS task in previous SemEval editions (Jimenez et al., 2012a; Jimenez et al., 2013a). The NTNU systems utilise an ensemble of such 18 measures, based only on surface text information, which were extracted using soft cardinality with different similarity functions, as further described in Section 2.1. Section 2.2 then introduces the similarity measures based on character n-gram feature representations, which proved themselves as the strongest features in the STS 2013 task (Marsi et al., 2013). The measures used here replace character n-gram features with cluster frequencies or vector values based on the n-gram collocational structure learned in an unsupervised manner from te"
S16-1014,W10-3110,0,0.0276079,"ecific objects with the empty string: emoticons, username 104 mentions, RT (retweet) tags, URLs, only hashtag signs (#), and hashtags (incl. the string following the sign). The other four filters transform uppercase characters to lowercase, remove non-alphabetic or space characters, limit the maximum repetitions of a single character to three, and perform tokenization using Pott’s tweet tokenizer (Potts, 2011). Negation detection uses a simple approach where n words appearing after a negation cue, but before the next punctuation mark, are marked as negated. The negation cues were adopted from Councill et al. (2010), supplemented by five common misspellings obtained by looking up each negation cue in TweetNLP’s Twitter word cluster (Owoputi et al., 2013): anit, couldnt, dnt, does’nt, and wont. 2.2 Feature Extraction The feature extraction is implemented as a ScikitLearn Feature Union, which is a collection of independent transformers (feature extractors), that build a feature matrix for the classifier. Each feature is represented by a transformer. Eight such transformers have been implemented: two extract the number of punctuations (repeated alphabetical and grammatical signs) and the number of happy and"
S16-1014,R13-1026,0,0.0190989,"lection of independent transformers (feature extractors), that build a feature matrix for the classifier. Each feature is represented by a transformer. Eight such transformers have been implemented: two extract the number of punctuations (repeated alphabetical and grammatical signs) and the number of happy and sad emoticons found in the tweet. Two other transformers extract TF–IDF values for word n-grams and character n-grams using a bag-of-words vectorizer implementation, which is an extension of ScikitLearn’s default TfidfVectorizer. A part-of-speech transformer uses the GATE TwitIE tagger (Derczynski et al., 2013) to assign part-of-speech tags to every token in the text; the tag occurrences are then counted and returned. A word cluster transformer counts the occurrences of different TweetNLP word clusters (Owoputi et al., 2013), that is, if a word in a tweet is a member of a cluster, a counter for that specific cluster is incremented. The last two transformers are essentially lexical: the VADER transformer runs the lexicon-based social media sentiment analysis tool VADER (Hutto and Gilbert, 2014) and extracts its output. VADER (Valence Aware Dictionary and sEntiment Reasoner) goes beyond the bag-of-wor"
S16-1014,N13-1037,0,0.0142595,"ength and contain a lot of abbreviations, misspellings, Internet slang, and creative syntax. Although the relative occurrence of nonstandard English syntax is fairly constant among many types of social media (Baldwin et al., 2013), ∗ Thanks to Mikael Brevik, Jørgen Faret, Johan Reitan and Øyvind Selmer for their work on two previous NTNU systems. analysing such texts using traditional language processing systems can be problematic, primarily since the main common denominator of social media text is not that it is informal, but that it describes language in rapid change (Androutsopoulos, 2011; Eisenstein, 2013), so that resources targeted directly at social media language quickly become outdated. Twitter Sentiment Analysis (TSA) has been a rapibly growing research area in recent years, and a typical approach to TSA has been identified, using a supervised machine learning strategy, consisting of three main steps: preprocessing, feature extraction and classifier training. Preprocessing is used in order to remove noise and standardize the tweet format, for example, by replacing or removing URLs. Desired features of the tweets are then extracted, such as sentiment scores using specific sentiment lexica"
S16-1014,W10-0204,0,0.0248622,"s its output. VADER (Valence Aware Dictionary and sEntiment Reasoner) goes beyond the bag-of-words model, taking into consideration word order and degree modifiers. The lexicon transformer is a single transformer using a combination of six automatically and manually annotated prior polarity sentiment lexica. The automatically annotated lexica used are NRC Sentiment140 and HashtagSentiment (Kiritchenko et al., 2014), that contain sentiment scores for both unigrams and bigrams, where some are in a negated context. Similarly, two manually annotated lexica, AFINN (Nielsen, 2011) and NRC Emoticon (Mohammad and Turney, 2010), give a sentiment score for each word (AFINN) or each emoticon (NRC Emoticon). However, two further manually annotated lexica, MPQA (Wilson et al., 2005) and Bing Liu (Ding et al., 2008), do not list sentiment scores for words, but only whether a word contains positive or negative sentiment. For those two lexica, negative and positive word sentiments were mapped to the scores −1 or +1, respectively. For all lexica, four different features were extracted from each tweet. Following Kiritchenko et al. (2014), the four features for manually annotated lexica are the sums of positive scores and of"
S16-1014,N13-1039,0,0.0278559,"Missing"
S16-1014,S13-2071,1,0.90619,"Missing"
S16-1014,H05-2018,0,0.0811495,"Missing"
S16-1014,S13-2052,0,0.232492,"Missing"
S16-1014,S16-1001,0,\N,Missing
S16-1014,I13-1041,0,\N,Missing
S18-1138,P04-3022,0,0.15033,"Missing"
S18-1138,S17-2168,1,0.874493,"Missing"
S18-1138,P14-5010,0,0.00472533,"Missing"
S18-1138,H05-1091,0,0.0396721,"span containing the entities and their middle context is considered as the representation of the relation instance. As word features, unigrams and bigrams of the context and entity mentions (excluding articles, adjectives, cardinals, ordinals, pronouns, brackets and punctuations) are considered. Corresponding to word features, POS, word+POS, and lemma+POS combinations are included, as well as word and POS of entity dependency heads, context dependency heads, and their combinations. As the shortest dependency path between the entity pair contains major information for relation identification (Bunescu and Mooney, 2005), dependency path features are added for the distance from left entity head to right entity head, words belonging to the dependency path and their relaResults Three separate submissions were made on the test data. The first two submissions were on relation classification on clean (subtask 1.1) and noisy data (subtask 1.2). The third submission (subtask 2) consisted of relation identification followed by classification on clean data. In subtask 2, a separate system was created for the relation identification, while the relation classification system of subtask 1.1 was used for the classificatio"
S18-1138,W09-2421,0,0.0875075,"Missing"
S18-1138,P16-1105,0,0.0523833,"Missing"
S18-1138,E12-2021,0,0.0915495,"Missing"
S18-1138,L16-1586,0,0.0696394,"Missing"
S18-1138,W09-2415,0,0.0708861,"Missing"
S18-1138,C14-1220,0,0.0471616,"Missing"
S18-1144,P17-1143,0,0.0592527,"in the sentences into three different categories, namely Action, Entity, and Modifier. A CRF-based classifier was used also for the second subtask. The paper is organized as follows: The datasets are presented in Section 2. The malware sentence identification is described in Section 3, while the token label malware identification is described in Section 4. Results are presented in Section 5, with system comparison and error analysis reported in Sections 6 and 7, respectively. Section 8 addresses future work and concludes. Introduction Malware is a major problem in the digital world. Recently, Lim et al. (2017) addressed the malware threat by creating a new database of malware texts. In addition, they constructed different models for identifying and classifying malware sentences, and discussed the outstanding challenges. Sutskever et al. (2016) also pointed to cybersecurity defense as a key area because of its long-term impact on society. Still, there have been very few efforts addressing the problem. Many cybersecurity agencies such as Cylance (Gross, 2016) and Symantec (DiMaggio, 2015) have collected large repositories of malware-related online texts. The diversity of those texts shows that identi"
S18-1144,S18-1113,0,0.0334464,"Missing"
S19-2124,D14-1162,0,0.0945687,"oting (Kuncheva, 2004): L1-regularised Logistic Regression, L2-regularised Logistic Regression, Linear SVC, SGD, and PA. The ensemble model exhibited the best results in subtasks A and B. In subtask C, the multi-class classification problem and a severe reduction of the size of the training set led to much lower macroaveraged F1 -scores, with the ensemble model performing badly. A deep learning approach, based on an LSTM architecture (Hochreiter and Schmidhuber, 1997), was adopted specifically for this subtask. The model used a 200 dimensional GloVe embedding5 pre-trained on 2 billion tweets (Pennington et al., 2014), with trainability set to False. The embedding layer was followed by a 1D convolution layer with 64 output filters and a Rectified Linear Unit (ReLU) activation function. The output of this layer was down-sampled using a max pooling layer of size 4. These inputs were fed into an LSTM layer of 200 units and subsequently a dense layer of 3 units with a softmax activation function. The model used the ‘Adam’ optimiser and the categorical cross entropy loss function. Due to the less amount of data, overfitting was quite common on as few as 3 epochs. Therefore, the model benefited from larger dropo"
S19-2124,W17-3013,1,0.83596,"Missing"
S19-2124,W17-1101,0,0.0349419,"licit (Waseem and Hovy, 2016), but is considerably less reliable when considering implicit abuse (Dadvar et al., 2013; Justo et al., 2014; Dinakar et al., 2011). They propose a typology that can synthesise different offensive language detection subtasks. Zampieri et al. (2019a) expand on these ideas and propose a hierarchical three-level-annotation model, which is used in the OffensEval 2019 shared task. Another issue is whether the datasets should be balanced or not (Waseem and Hovy, 2016), since there are much fewer offensive comments than benign comments in randomly sampled real-life data (Schmidt and Wiegland, 2017). timent scores (Van Hee et al., 2015; Davidson et al., 2017) also having been used (Schmidt and Wiegland, 2017). More recently, meta information about the users have been suggested as features, but no consistent correlation between user information and tendency for offensive behaviour online has been shown, with Waseem and Hovy (2016) claiming gender information leading to improvements in classifier performance, but with Unsv˚ag and Gamb¨ack (2018) challenging this and reporting user-network data to be more important instead. Wulczyn et al. (2017) concluded that anonymity leads to an increase"
S19-2124,gao-huang-2017-detecting,0,0.0193989,"ulczyn et al. (2017) concluded that anonymity leads to an increase in the likelihood of a comment being an attack. Classical Machine learning algorithms have been wielded to some success in automated offensive language detection, mainly Logistic Regression (Davidson et al., 2017; Waseem and Hovy, 2016; Burnap and Williams, 2015) and Support Vector Machines (Xu et al., 2012; Dadvar et al., 2013). Recently, however, deep learning models have outperformed their traditional machine learning counterparts, with both Recurrent Neural Networks (RNN) — such as LSTM (Pitsilis et al., 2018) and Bi-LSTM (Gao and Huang, 2017) — and Convolutional Neural Networks (CNN) having been used. Gamb¨ack and Sikdar (2017) utilised a CNN model with word2vec embeddings to obtain higher F1 -score and precision than a previous logistic regression model (Waseem and Hovy, 2016), while Zhang et al. (2018) combined a CNN model with a Gated Recurrent Unit (GRU) layer. Malmasi and Zampieri (2018) used an ensemble system much like ours to separate profanity from hate speech, but reported no significant improvement over a single classifier system. The training dataset used for the shared task, the Offensive Language Identification Datas"
S19-2124,W18-5110,1,0.893947,"Missing"
S19-2124,P11-2008,0,0.275852,"Missing"
S19-2124,R15-1086,0,0.0634104,"Missing"
S19-2124,P82-1020,0,0.796928,"Missing"
S19-2124,W17-3012,0,0.0308943,"ge on Social Media’. However, what we consider offensive is often a grey area, as is evident by the low inter-annotator agreement 2 Related Work Most datasets for offensive language detection represent multiclass classification problems (Davidson et al., 2017; Founta et al., 2018; Waseem and Hovy, 2016), with the annotations often obtained via crowd-sourcing portals, with 696 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 696–703 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics varying degrees of success. Waseem et al. (2017b) state that annotation via crowd-sourcing tends to work best when the abuse is explicit (Waseem and Hovy, 2016), but is considerably less reliable when considering implicit abuse (Dadvar et al., 2013; Justo et al., 2014; Dinakar et al., 2011). They propose a typology that can synthesise different offensive language detection subtasks. Zampieri et al. (2019a) expand on these ideas and propose a hierarchical three-level-annotation model, which is used in the OffensEval 2019 shared task. Another issue is whether the datasets should be balanced or not (Waseem and Hovy, 2016), since there are muc"
S19-2124,W16-3638,0,0.0320908,"t is unclear whether this was due to a more strict definition provided by the task organisers. For example, it is not immediately clear why tweets such as:1 “@USER Ouch!” (23159), “@USER He is a beast” (50771), and “@USER That shit weird! Lol” (31404) were annotated as offensive. The annotators furthermore seemed to disagree over the cathartic and emphatic use of swearing, as in “@USER Oh my Carmen. He is SO FRICK3 In terms of features, simple bag of words models have proven to be highly predictive (Waseem and Hovy, 2016; Davidson et al., 2017; Nobata et al., 2016; Burnap and Williams, 2015). Mehdad and Tetreault (2016) endorsed the use of character n-grams over token n-grams citing their ability to glaze over the spelling errors that are frequent in online texts. Nobata et al. (2016); Chen et al. (2012) showed small improvements by including features capturing the frequency of different entities such as URLs and mentions, with other features such as part-of-speech (POS) tags (Xu et al., 2012; Davidson et al., 2017) and senData 1 697 In the examples, tweet IDs are given in parenthesis. built on the following five classifiers combined by plurality voting (Kuncheva, 2004): L1-regularised Logistic Regression, L"
S19-2124,S19-2010,0,0.303497,"be difficult, due to the broad spectrum in which language can be used to convey an insult. The nature of the abuse can be implicit — drawing from sarcasm and humour rather than offensive terms — as well as explicit, by making extensive use of traditional offensive terms and profanity. It does not help that the reverse is also entertained, with profanity often being used to imply informality in speech or for emphasis. Coincidentally, these are also the reasons why lexical detection methods have been unfruitful in classifying text as offensive or non-offensive. The OffensEval 2019 shared task (Zampieri et al., 2019b) is one of several endeavours to further the state-of-the-art in addressing the offensive language problem. The paper describes the insights obtained when tackling the shared task using an ensemble of traditional machine learning classification models and a Long Short-Term Memory (LSTM) deep learning model. Section 2 first discusses other related approaches to detecting hate speech and offensive language. Then Section 3 describes the dataset and Section 4 the ideas and methodology behind our approach. Section 5 reports the results obtained, while Section 6 discusses those results with a part"
S19-2124,N12-1084,0,0.53594,"has been shown, with Waseem and Hovy (2016) claiming gender information leading to improvements in classifier performance, but with Unsv˚ag and Gamb¨ack (2018) challenging this and reporting user-network data to be more important instead. Wulczyn et al. (2017) concluded that anonymity leads to an increase in the likelihood of a comment being an attack. Classical Machine learning algorithms have been wielded to some success in automated offensive language detection, mainly Logistic Regression (Davidson et al., 2017; Waseem and Hovy, 2016; Burnap and Williams, 2015) and Support Vector Machines (Xu et al., 2012; Dadvar et al., 2013). Recently, however, deep learning models have outperformed their traditional machine learning counterparts, with both Recurrent Neural Networks (RNN) — such as LSTM (Pitsilis et al., 2018) and Bi-LSTM (Gao and Huang, 2017) — and Convolutional Neural Networks (CNN) having been used. Gamb¨ack and Sikdar (2017) utilised a CNN model with word2vec embeddings to obtain higher F1 -score and precision than a previous logistic regression model (Waseem and Hovy, 2016), while Zhang et al. (2018) combined a CNN model with a Gated Recurrent Unit (GRU) layer. Malmasi and Zampieri (201"
S19-2124,N19-1144,0,0.294066,"be difficult, due to the broad spectrum in which language can be used to convey an insult. The nature of the abuse can be implicit — drawing from sarcasm and humour rather than offensive terms — as well as explicit, by making extensive use of traditional offensive terms and profanity. It does not help that the reverse is also entertained, with profanity often being used to imply informality in speech or for emphasis. Coincidentally, these are also the reasons why lexical detection methods have been unfruitful in classifying text as offensive or non-offensive. The OffensEval 2019 shared task (Zampieri et al., 2019b) is one of several endeavours to further the state-of-the-art in addressing the offensive language problem. The paper describes the insights obtained when tackling the shared task using an ensemble of traditional machine learning classification models and a Long Short-Term Memory (LSTM) deep learning model. Section 2 first discusses other related approaches to detecting hate speech and offensive language. Then Section 3 describes the dataset and Section 4 the ideas and methodology behind our approach. Section 5 reports the results obtained, while Section 6 discusses those results with a part"
W00-1502,gamback-olsson-2000-experiences,1,\N,Missing
W00-1502,C90-2012,0,\N,Missing
W00-1502,A97-1035,0,\N,Missing
W00-1502,C88-1008,0,\N,Missing
W00-1502,A97-2017,0,\N,Missing
W05-0109,W05-0710,1,0.876518,"Missing"
W05-0109,J00-4006,0,0.00562957,"nly, and with one of the lecturers present during the first half of the course and the other two during the second half, with some overlap in the middle. Thus the course was split into two main parts, the first concentrating on general linguistic issues, morphology and lexicology, and the second on syntax, semantics and application areas. The choice of reading was influenced by the need not to assume very elaborated student programming skills. This ruled out books based mainly on programming exercises, such as Pereira and Shieber (1987) and Gazdar and Mellish (1989), and it was decided to use Jurafsky and Martin (2000) as the main text of the course. The extensive web page provided by those authors was also a factor, since it could not be assumed that the students would have full-time access to the actual course book itself. The costs of buying a regular computer science book is normally too high for the average Ethiopian student. To partially ease the financial burden on the students, we brought some copies of the book with us and made those available at the department library. We also tried to make sure that as much as possible of the course material was available on the web. In addition to the course boo"
W05-0710,2003.mtsummit-semit.7,0,0.102753,"Missing"
W09-0715,W05-0708,0,0.0167596,"ne learning approaches to tagging essentially originates from the wish to exploit the vast amounts of unlabelled data available when constructing taggers. The area is particularly vivid when it comes to the treatment of languages for which there exist few, if any, computational resources, and for the case of adapting an existing tagger to a new language domain. Banko and Moore (2004) compared unsupervised HMM and transformation-based taggers trained on the same portions of the Penn Treebank, and showed that the quality of the lexicon used for training had a high impact on the tagging results. Duh and Kirchhoff (2005) presented a minimallysupervised approach to tagging for dialectal Arabic (Colloquial Egyptian), based on a morphological analyzer for Modern Standard Arabic and un106 Portuguese, and concluded that some, but not all, combinations yielded better accuracy than the best individual tagger. Shacham and Wintner (2007) contrasted what they refer to as being a na¨ıve way of combining taggers with a more elaborate, hierarchical one for Hebrew. In the end, the elaborated method yielded results inferior to the na¨ıve approach. De Pauw et al. (2006) came to similar conclusions when using five different w"
W09-0715,W05-0707,0,0.238944,"titute of Computer Science Kista, Sweden {gamback,fredriko}@sics.se Fredrik Olsson† Atelach Alemu Argaw? ‡ Dpt. of Computer & Information Science Norwegian University of Science & Technology Trondheim, Norway gamback@idi.ntnu.no Abstract ? Dpt. of Computer & System Sciences Stockholm University Kista, Sweden {atelach,asker}@dsv.su.se In spite of the relatively large number of speakers, Amharic is still a language for which very few computational linguistic resources have been developed, and previous efforts to create language processing tools for Amharic—e.g., Alemayehu and Willett (2002) and Fissaha (2005)—have been severely hampered by the lack of large-scale linguistic resources for the language. In contrast, the work detailed in the present paper has been able to utilize the first publicly available medium-sized tagged Amharic corpus, described in Section 5. However, first the Amharic language as such is introduced (in Section 2), and then the task of partof-speech tagging and some previous work in the field is described (Section 3). Section 4 details the tagging strategies used in the experiments, the results of which can be found in Section 6 together with a short discussion. Finally, Sect"
W09-0715,gimenez-marquez-2004-svmtool,0,0.158325,"Missing"
W09-0715,W07-0814,1,0.901791,"Missing"
W09-0715,P07-1094,0,0.0455371,"Missing"
W09-0715,C04-1080,0,0.0453333,"g and by exploring contextual cues (essentially a variant of stacking). Aires et al. (2000) experimented with 12 different ways of combining the output from taggers for Brazilian Unsupervised tagging The desire to use unsupervised machine learning approaches to tagging essentially originates from the wish to exploit the vast amounts of unlabelled data available when constructing taggers. The area is particularly vivid when it comes to the treatment of languages for which there exist few, if any, computational resources, and for the case of adapting an existing tagger to a new language domain. Banko and Moore (2004) compared unsupervised HMM and transformation-based taggers trained on the same portions of the Penn Treebank, and showed that the quality of the lexicon used for training had a high impact on the tagging results. Duh and Kirchhoff (2005) presented a minimallysupervised approach to tagging for dialectal Arabic (Colloquial Egyptian), based on a morphological analyzer for Modern Standard Arabic and un106 Portuguese, and concluded that some, but not all, combinations yielded better accuracy than the best individual tagger. Shacham and Wintner (2007) contrasted what they refer to as being a na¨ıve"
W09-0715,A00-1031,0,0.470332,"Missing"
W09-0715,W96-0213,0,0.388278,"Missing"
W09-0715,P98-1029,0,0.040931,"ndom decision, voting, and stacking (Dietterich, 1997), with the first way suited only for forming a baseline. Voting can be divided into two subclasses: unweighted votes, and weighted votes. The weights of the votes, if any, are usually calculated based on the classifiers’ performance on some initial dataset. Stacking, finally, is a way of combining the decisions made by individual taggers in which the predicted tags for a given word are used as input to a subsequent tagger which outputs a final label for the word. Committee-based approaches to POS tagging have been in focus the last decade: Brill and Wu (1998) combined four different taggers for English using unweighted voting and by exploring contextual cues (essentially a variant of stacking). Aires et al. (2000) experimented with 12 different ways of combining the output from taggers for Brazilian Unsupervised tagging The desire to use unsupervised machine learning approaches to tagging essentially originates from the wish to exploit the vast amounts of unlabelled data available when constructing taggers. The area is particularly vivid when it comes to the treatment of languages for which there exist few, if any, computational resources, and for"
W09-0715,D07-1046,0,0.0570124,"Missing"
W09-0715,J95-4004,0,0.451607,"Missing"
W09-0715,W02-1001,0,0.254324,"Missing"
W09-0715,N03-1033,0,0.119062,"Missing"
W09-0715,N04-4038,0,0.0574895,"Missing"
W09-0715,E99-1027,0,0.0486551,"Missing"
W09-0715,W07-1709,0,\N,Missing
W09-0715,C98-1029,0,\N,Missing
W12-3707,W11-0311,0,0.0386498,"Missing"
W12-3707,W10-3208,1,0.797918,"Missing"
W12-3707,D09-1061,0,0.0365449,"Missing"
W12-3707,esuli-sebastiani-2006-sentiwordnet,0,0.294433,"Missing"
W12-3707,W02-1011,0,0.016747,"Missing"
W12-3707,P05-1015,0,0.0873971,"Missing"
W12-3707,C10-2146,0,0.0426259,"Missing"
W12-3707,W11-1710,0,0.0258363,"Missing"
W12-3707,P02-1053,0,0.0153326,"Missing"
W12-3707,J06-3003,0,0.0505959,"Missing"
W12-3707,W10-4116,0,\N,Missing
W12-3707,C04-1200,0,\N,Missing
W12-3707,W09-3539,0,\N,Missing
W12-3707,P97-1023,0,\N,Missing
W12-3707,W10-3402,1,\N,Missing
W12-3707,W10-3209,0,\N,Missing
W12-3707,P05-1017,0,\N,Missing
W13-1003,D09-1050,0,0.0305354,"eedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 21–30, c Atlanta, Georgia, 13-14 June 2013. 2013 Association for Computational Linguistics In phrase-based statistical MT systems, the translation of multiword expressions can be a notable source of errors, despite the fact that those systems explicitly recognize and use alignments of sequential chunks of words. Several researchers have approached this problem by adding MWE translation tables to the systems, either through expanding the phrase tables (Ren et al., 2009) or by injecting the MWE translations into the decoder (Bai et al., 2009). Furthermore, there has been some interest in automatic mining of MWE pairs from bilingual corpora as a task in itself: Caseli et al. (2010) used a dictionary for evaluation of an automatic MWE extraction procedure using bilingual corpora. They also argued for the filtering of stopwords, similarly to the procedure described in the present paper. Sharoff et al. (2006) showed how MWE pairs can be extracted from comparable monolingual corpora instead of from a parallel bilingual corpus. The methodology introduced in this paper employs bilingual dictionaries as a source of multiword expressions."
W13-1003,E06-2001,0,0.0107707,"and Westjordanland), three concern databases (Datenbank), and one is for blood banks. The rest are connected to different types of financial institutions (such as Handelsbank and Finanzinstitut, but also by association Konto, Weldbankgeber, Banknote, Geldschein, Kredit, etc.). 2.2 N-Gram Context Matching N-gram matching is used to produce a ranked list of translation candidates and their contexts, both in order to provide robustness and to give a baseline performance. The n-gram models were built using the IRSTLM toolkit (Federico et al., 2008; Bungum and Gamb¨ack, 2012) on the DeWaC corpus (Baroni and Kilgarriff, 2006), using the stopword list from NLTK (Loper and Bird, 2002). The n-gram matching procedure consists of two steps: 1. An nth order source context is extracted and the translations for each SL word in this context are retrieved from the dictionary. This includes stopword filtering of the context. 2. All relevant n-grams are inspected in order from left to right and from more specific (5grams) to least specific (single words). For each part of the context with matching n-grams in the target language model, the appropriate target translation candidates are extracted and ranked according to their la"
W13-1003,C02-2020,0,0.0340757,"e SL method matches dictionaries of the combined lemmata of the focus word and its adjacent words to the same list of translation candidates. False positives are expected with lower constraints such as these. On the SemEval data, the contribution of the dictionary methods to the n-grams is mostly in improving the average score. The idea of acquiring lexical information from corpora is of course not new in itself. So did, e.g., Rapp (1999) use vector-space models for the purpose of extracting ranked lists of translation candidates for extending a dictionary for word translation disambiguation. Chiao and Zweigenbaum (2002) tried to identify translational equivalences by investigating the relations between target and source language word distributions in a restricted domain, and also applied reverse-translation filtering for improved performance, while Sadat et al. (2003) utilised non-aligned, comparable corpora to induce a bilingual lexicon, using a bidirectional method (SL→TL, TL→SL, and a combination of both). Extending the method to use an arbitrary size window around all words in the context of each focus word (not just the word itself) could identify more multiword expressions and generate a more accurate"
W13-1003,W07-0712,0,0.0375558,"Missing"
W13-1003,2005.mtsummit-papers.11,0,0.00389674,"in any other machine-readable form. {datenbank 4; bank 3; datenbanksystem 1; daten 1} (c) established as a band of 1 km in width from the banks of a river or the shores of a lake or coast for a length of at least 3 km. {ufer 4; flussufer 3} Table 1: Examples of contexts for the English word bank with possible German translations 2.1 The CL-WSD Datasets The data sets used for the SemEval’10 CrossLingual Word Sense Disambiguation task were constructed by making a ‘sense inventory’ of all possible target language translations of a given source language word based on word-alignments in Europarl (Koehn, 2005), with alignments involving the relevant source words being manually checked. The retrieved target words were manually lemmatised and clustered into translations with a similar sense; see Lefever and Hoste (2010a) for details. Trial and test instances were extracted from two other corpora, JRC-Acquis (Steinberger et al., 2006) and BNC (Burnard, 2007). The trial data for each language consists of five nouns (with 20 sentence contexts per noun), and the test data of twenty nouns (50 contexts each, so 1000 in total per language, with the CL-WSD data covering Dutch, French, Spanish, Italian and Ge"
W13-1003,lefever-hoste-2010-construction,0,0.0808561,"n-gram language model. The n-gram model represents a high-coverage, low-precision companion to the dictionary approach (i.e., it has complementary properties). Results show that the MWE dictionary information substantially improves the baseline system. The 2010 Semantic Evaluation exercise (SemEval’10) featured a shared task on Cross-Lingual Word Sense Disambiguation (CL-WSD), where the focus was on disambiguating the translation of a single noun in a sentence. The participating systems were given an English word in its context and asked to produce appropriate substitutes in another language (Lefever and Hoste, 2010b). The CL-WSD data covers Dutch, French, Spanish, Italian and German; however, since the purpose of the experiments in this paper just was to assess our method’s ability to choose the right translation of a word given its context, we used the English-to-German part only. 22 The next section details the employed disambiguation methodology and describes the data sets used in the experiments. Section 3 then reports on the results of experiments applying the methodology to the SemEval datasets, particularly addressing the impact of the dictionary MWE correspondences. Finally, Section 4 sums up th"
W13-1003,S10-1003,0,0.325121,"n-gram language model. The n-gram model represents a high-coverage, low-precision companion to the dictionary approach (i.e., it has complementary properties). Results show that the MWE dictionary information substantially improves the baseline system. The 2010 Semantic Evaluation exercise (SemEval’10) featured a shared task on Cross-Lingual Word Sense Disambiguation (CL-WSD), where the focus was on disambiguating the translation of a single noun in a sentence. The participating systems were given an English word in its context and asked to produce appropriate substitutes in another language (Lefever and Hoste, 2010b). The CL-WSD data covers Dutch, French, Spanish, Italian and German; however, since the purpose of the experiments in this paper just was to assess our method’s ability to choose the right translation of a word given its context, we used the English-to-German part only. 22 The next section details the employed disambiguation methodology and describes the data sets used in the experiments. Section 3 then reports on the results of experiments applying the methodology to the SemEval datasets, particularly addressing the impact of the dictionary MWE correspondences. Finally, Section 4 sums up th"
W13-1003,W02-0109,0,0.00759308,"e is for blood banks. The rest are connected to different types of financial institutions (such as Handelsbank and Finanzinstitut, but also by association Konto, Weldbankgeber, Banknote, Geldschein, Kredit, etc.). 2.2 N-Gram Context Matching N-gram matching is used to produce a ranked list of translation candidates and their contexts, both in order to provide robustness and to give a baseline performance. The n-gram models were built using the IRSTLM toolkit (Federico et al., 2008; Bungum and Gamb¨ack, 2012) on the DeWaC corpus (Baroni and Kilgarriff, 2006), using the stopword list from NLTK (Loper and Bird, 2002). The n-gram matching procedure consists of two steps: 1. An nth order source context is extracted and the translations for each SL word in this context are retrieved from the dictionary. This includes stopword filtering of the context. 2. All relevant n-grams are inspected in order from left to right and from more specific (5grams) to least specific (single words). For each part of the context with matching n-grams in the target language model, the appropriate target translation candidates are extracted and ranked according to their language model probability. This results in an n-best list o"
W13-1003,P99-1067,0,0.035944,"differences in algorithms, where the TL method matches any adjacent lemma to the focus word with the translation of the 28 pre-defined translation candidates, whereas the SL method matches dictionaries of the combined lemmata of the focus word and its adjacent words to the same list of translation candidates. False positives are expected with lower constraints such as these. On the SemEval data, the contribution of the dictionary methods to the n-grams is mostly in improving the average score. The idea of acquiring lexical information from corpora is of course not new in itself. So did, e.g., Rapp (1999) use vector-space models for the purpose of extracting ranked lists of translation candidates for extending a dictionary for word translation disambiguation. Chiao and Zweigenbaum (2002) tried to identify translational equivalences by investigating the relations between target and source language word distributions in a restricted domain, and also applied reverse-translation filtering for improved performance, while Sadat et al. (2003) utilised non-aligned, comparable corpora to induce a bilingual lexicon, using a bidirectional method (SL→TL, TL→SL, and a combination of both). Extending the me"
W13-1003,W09-2907,0,0.381741,"Missing"
W13-1003,W03-1108,0,0.0514464,"Missing"
W13-1003,sharoff-etal-2006-using,0,0.0250132,"l chunks of words. Several researchers have approached this problem by adding MWE translation tables to the systems, either through expanding the phrase tables (Ren et al., 2009) or by injecting the MWE translations into the decoder (Bai et al., 2009). Furthermore, there has been some interest in automatic mining of MWE pairs from bilingual corpora as a task in itself: Caseli et al. (2010) used a dictionary for evaluation of an automatic MWE extraction procedure using bilingual corpora. They also argued for the filtering of stopwords, similarly to the procedure described in the present paper. Sharoff et al. (2006) showed how MWE pairs can be extracted from comparable monolingual corpora instead of from a parallel bilingual corpus. The methodology introduced in this paper employs bilingual dictionaries as a source of multiword expressions. Relationships are induced between the source sentence and candidate translation lexical items based on their correspondence in the dictionary. Specifically, we use a deterministic multiword expression disambiguation procedure based on translation dictionaries in both directions (from source to target language and vice versa), and a baseline system that ranks target le"
W13-1003,W12-3901,0,0.0274766,"mportant for many language processing tasks (Sag et al., 2002), but can be crucial in MT: since the semantics of many MWEs are non-compositional, a suitable translation cannot be constructed by translating the words in isolation. Identifying MWEs can help to identify idiomatic or otherwise fixed language usage, leading to more fluent translations, and Bilingual corpora are scarce, however, and unavailable for most language pairs and target domains. An alternative approach is to build systems based on large monolingual knowledge sources and bilingual lexica, as in the hybrid MT system PRESEMT (Sofianopoulos et al., 2012). Since such a system explicitly uses a translation dictionary, it must at some point in the translation process decide which lexical entries to use; thus a separate word translation disambiguation module needs to be incorporated. To research available methods in such a module we have identified a task where we can use public datasets for measuring how well a method is able to select the optimal of many translation choices from a source language sentence. 21 Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 21–30, c Atlanta, Georgia, 13-14 June 2013. 2013 Association f"
W13-1003,steinberger-etal-2006-jrc,0,0.0106909,"ds. Several researchers have approached this problem by adding MWE translation tables to the systems, either through expanding the phrase tables (Ren et al., 2009) or by injecting the MWE translations into the decoder (Bai et al., 2009). Furthermore, there has been some interest in automatic mining of MWE pairs from bilingual corpora as a task in itself: Caseli et al. (2010) used a dictionary for evaluation of an automatic MWE extraction procedure using bilingual corpora. They also argued for the filtering of stopwords, similarly to the procedure described in the present paper. Sharoff et al. (2006) showed how MWE pairs can be extracted from comparable monolingual corpora instead of from a parallel bilingual corpus. The methodology introduced in this paper employs bilingual dictionaries as a source of multiword expressions. Relationships are induced between the source sentence and candidate translation lexical items based on their correspondence in the dictionary. Specifically, we use a deterministic multiword expression disambiguation procedure based on translation dictionaries in both directions (from source to target language and vice versa), and a baseline system that ranks target le"
W13-1003,W09-2413,0,\N,Missing
W13-3210,D10-1113,0,0.0353036,"glish as a Foreign Language”) (Kanerva et al., 2000). However, it is evident that many terms have more than one meaning or sense, some being static and some dynamic, that is, determined by the contexts the terms occur in. Sch¨utze (1998) proposed a method for clustering the contextual occurrences of terms into individual “prototype” vectors, where one term can have multiple prototype vectors representing separate senses of the term. Others have adopted the same underlying idea, using alternative methods and techniques (Reisinger and Mooney, 2010; Huang et al., 2012; Van de Cruys et al., 2011; Dinu and Lapata, 2010). This differs from the classical Random Indexing (RI) method which assumes a static sense inventory by restricting each term to have only one vector (sense) per term, as described in Section 2. The MSRI method is introduced in Section 3. Since the induced dynamic senses do not necessarily correspond to the traditional senses distinguished by humans, we perform an extrinsic evaluation by applying the resulting models to data from the Semantic Textual Similarity shared task (Agirre et al., 2013), in order to compare MSRI to the classical RI method. The experimental setup is the topic of Section"
W13-3210,J13-3003,0,0.0213147,"n the task of predicting semantic textual similarity do, however, not show a systematic difference between singleprototype and multi-prototype models. 1 Introduction Many terms have more than one meaning, or sense. Some of these senses are static and can be listed in dictionaries and thesauri, while other senses are dynamic and determined by the contexts the terms occur in. Work in Word Sense Disambiguation often concentrate on the static word senses, making the task of distinguishing between them one of classification into a predefined set of classes (i.e., the given word senses); see, e.g., Erk et al. (2013; Navigli (2009) for overviews of current work in the area. The idea of fixed generic word senses has received a fair amount of criticism in the literature (Kilgarriff, 2000). 83 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 83–90, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics the window) added to its context vector using vector summation. Then the cosine similarity measure can be used on term pairs to calculate their similarity (or “contextual similarity”). Random Indexing has achieved promising results in va"
W13-3210,P12-1092,0,0.127732,"ts, for example, on the TOEFL test (“Test of English as a Foreign Language”) (Kanerva et al., 2000). However, it is evident that many terms have more than one meaning or sense, some being static and some dynamic, that is, determined by the contexts the terms occur in. Sch¨utze (1998) proposed a method for clustering the contextual occurrences of terms into individual “prototype” vectors, where one term can have multiple prototype vectors representing separate senses of the term. Others have adopted the same underlying idea, using alternative methods and techniques (Reisinger and Mooney, 2010; Huang et al., 2012; Van de Cruys et al., 2011; Dinu and Lapata, 2010). This differs from the classical Random Indexing (RI) method which assumes a static sense inventory by restricting each term to have only one vector (sense) per term, as described in Section 2. The MSRI method is introduced in Section 3. Since the induced dynamic senses do not necessarily correspond to the traditional senses distinguished by humans, we perform an extrinsic evaluation by applying the resulting models to data from the Semantic Textual Similarity shared task (Agirre et al., 2013), in order to compare MSRI to the classical RI met"
W13-3210,S12-1051,0,0.0163617,"experiments reported on below, a range of different ways to represent sentences were tested. Sentence similarity was generally calculated by the average of the maximum similarity between pairs of terms from both sentences, respectively. The different ways of representing the data in combination with some sentence similarity measure will here be referred to as similarity features. 4 Experimental Setup In order to explore the potential of the MSRI model and the textual similarity measures proposed here, experiments were carried out on data from the Semantic Textual Similarity (STS) shared task (Agirre et al., 2012; Agirre et al., 2013). 86 The development data for STS 2013 consisted of all development and test data from STS 2012 combined, whereas test data comprised machine translation output (SMT, 750 pairs), ontology mappings both between WordNet and OntoNotes (OnWN, 561 pairs) and between WordNet and FrameNet (FNWN, 189 pairs), as well as news article headlines (HeadLine, 750 pairs). For simplicity, all development data combined were used for fitting the linear regression model, even though careful matching of development and test data sets may improve performance. Given a pair of sentences, systems"
W13-3210,S13-1008,1,0.843249,"Missing"
W13-3210,S13-1004,0,0.0675137,"methods and techniques (Reisinger and Mooney, 2010; Huang et al., 2012; Van de Cruys et al., 2011; Dinu and Lapata, 2010). This differs from the classical Random Indexing (RI) method which assumes a static sense inventory by restricting each term to have only one vector (sense) per term, as described in Section 2. The MSRI method is introduced in Section 3. Since the induced dynamic senses do not necessarily correspond to the traditional senses distinguished by humans, we perform an extrinsic evaluation by applying the resulting models to data from the Semantic Textual Similarity shared task (Agirre et al., 2013), in order to compare MSRI to the classical RI method. The experimental setup is the topic of Section 4, while the results of the experiments are given in Section 5. Section 6 then sums up the discussion and points to ways in which the present work could be continued. 2 Vector Space Models With the introduction of LSA, Latent Semantic Analysis (Deerwester et al., 1990), distributed models of lexical semantics, built from unlabelled free text data, became a popular sub-field within the language processing research community. Methods for building such semantic models rely primarily on term co-oc"
W13-3210,N10-1013,0,0.0737227,"irst assigning index vectors to each unique term. The vectors are of a predefined size (typically around 1000), and consist of a few randomly placed 1s and -1s. Context vectors of the same size are also assigned to each term, initially consisting of only zeros. When traversing a document corpus using a sliding window of a fixed size, the context vectors are continuously updated: the term in the centre of the window (the target term), has the index vectors of its neighbouring terms (the ones in 3 Multi-Sense Random Indexing, MSRI Inspired by the work of Sch¨utze (1998) and Reisinger and Mooney (2010), this paper introduces a novel variant of Random Indexing, which we have called “Multi-Sense Random Indexing”. MSRI attempts to capture one or more senses per unique term in an unsupervised and incremental manner, each sense represented as an separate vector in the model. The method is similar to classical sliding window RI, but each term can have multiple context vectors (referred to as sense vectors here) which are updated separately. When updating a term vector, instead of directly adding the index vectors of the neighbouring terms in the window to its context vector, the system first comp"
W13-3210,D11-1094,0,0.043075,"Missing"
W13-3210,widdows-ferraro-2008-semantic,0,0.0608067,"Missing"
W13-3210,J98-1004,0,0.881902,"Missing"
W13-3210,S12-1078,0,0.0167661,"e constructed by summing the index vectors of the neighbouring terms within a window, following the same procedure as used when training the MSRI model. We then find sˆ and sˆ 0 as the sense vectors best matching the context vectors: 5. RI-TermAvg: Classical Random Indexing — each term is represented as a single context vector. 6. RI-TermHA: Similarity between two sentences is calculated by applying the Hungarian Algorithm to the context vectors of each constituent term. ~ sˆ = arg maxi CosSim(~si , C) The parameters were selected based on a combination of surveying previous work on RI (e.g., Sokolov (2012)), and by analysing how sense counts evolved during training. For MSRI, we used a similarity threshold of 0.2, a vector dimensionality of 800, a non-zero count of 6, and a window size of 5 + 5. Sense vectors resulting from less than 50 observations were removed. For classical RI, we used the same parameters as for MSRI (except for a similarity threshold). ~ 0) sˆ 0 = arg maxj CosSim(~sj , C Finally, contextual similarity is defined as the similarity between these sense vectors: Simcontext (t, t0 ) = CosSim(ˆ s, sˆ 0 ) 3.2 Sentence Similarity Features In the experiments reported on below, a ran"
W14-5152,N10-1027,0,0.0481886,"Missing"
W14-5152,W14-3902,1,0.358818,"on-wide communication. Language diversity and dialect changes instigate frequent codemixing in India. Hence, Indians are multi-lingual by adaptation and necessity, and frequently change and mix languages in social media contexts. 3.1 Data Acquisition English-Hindi and English-Bengali language mixing were selected for the present study. These language combinations were chosen as Hindi and Bengali are the two largest languages in India in terms of first-language speakers (and 4th and 7th worldwide). In our study, we include corpora collected both by ourselves for this study and by Utsab Barman (Burman et al., 2014), hereforth called EN-BN1 and EN-HN1 resp. EN-BN2 and EN-HN2. Various campus Facebook groups Tag Description en English word bn hi ne ne+en_suffix ne+bn_suffix ne+hi_suffix univ Bengali word Hindi word Named Entity (NE) NE + English suffix NE + Bengali suffix NE + Hindi suffix Universal Tag en+bn_suffix en+hi_suffix bn+en_suffix hi+en_suffix acro acro+en_suffix acro+bn_suffix acro+hi_suffix undef Description English word + Bengali suffix English word + Hindi suffix Bengali word + English suffix Hindi word + English suffix Acronym Acronym + English suffix Acronym + Bengali suffix Acronym + Hind"
W14-5152,O09-5003,0,0.110054,"Missing"
W14-5152,D12-1039,0,0.0229006,"creases performance (by 0.5). 4.2 Dictionary-Based Detection Use of most-frequent-word dictionaries is another established method in language identificaˇ uˇrek and Kolkus, 2009). We tion (Alex, 2008; Reh˚ incorporated a dictionary-based language detection technique for the present task, but were faced with some challenges for the dictionary preparation, in particular since social media text is full of noise. A fully edited electronic dictionary may not have all such distorted word forms as are used in these texts (e.g., ‘gr8’ rather than ‘great’). Therefore a lexical normalisation dictionary (Han et al., 2012) prepared for Twitter was used for English. Unfortunately, no such dictionary is available for Hindi or Bengali, so we used the Samsad English-Bengali dictionary (Bi´sv¯as, 2000). The Bengali part of the Samsad dictionary is written in Unicode, but in our corpus the Bengali texts are written in transliterated/phonetic (Romanized) form. Therefore the Bengali lexicon was transliterated into Romanized text using the ModifiedJoint-Source-Channel model (Das et al., 2010). The same approach was taken when creating the Hindi dictionary, using Hindi WordNet (Narayan et al., 2002). In order to capture"
W14-5152,C82-1023,0,0.49505,"Missing"
W14-5152,N13-1131,0,0.0553511,"Missing"
W14-5152,W14-1303,0,0.0263079,"Missing"
W14-5152,Q14-1003,0,0.0667828,"Missing"
W14-5152,D13-1084,0,0.0822776,"Missing"
W14-5152,P14-2110,0,0.0397074,"Missing"
W14-5152,D08-1110,0,0.399909,"Missing"
W14-5152,voss-etal-2014-finding,0,0.0475633,"Missing"
W14-5152,P12-1102,0,0.0604059,"Missing"
W14-5152,W10-2411,1,\N,Missing
W15-2914,W11-0705,0,0.0478258,"Missing"
W15-2914,S15-2090,0,0.0382147,"Missing"
W15-2914,W10-3110,0,0.261176,"pes (PCS): For classification tasks where the output is a sequence, metrics that only consider individual units regardless of their order are often insufficient. PCS measures the accuracy of a scope classifier: a scope is considered correctly classified if, for a given negation cue, every token in its associated scope has been correctly marked. 5.1 Description Negation Classifier Architecture The classification algorithm consists of two steps: negation cue detection and scope identification. Cue detection is performed by a pattern-matching approach with a lexicon of explicit cues adopted from Councill et al. (2010), as shown in Table 2, where *n’t matches all strings with the suffix n’t. Note that this list is more extensive than the one of Potts (2011), used in many SemEval systems. Four cues on Potts’ list are not in Table 2 (noone, couldnt, wont, arent), while the 17 cues in italics are not listed by Potts. An inspection of the 37 cues appearing in the Twitter Negation Corpus revealed seven more cues / spelling variants included on neither list (idk, dnt, cudnt, ain, eint, neva, neeeever). Tweets are preprocessed with the TweeboParser dependency parser (Kong et al., 2014), that performs tokenisation,"
W15-2914,W10-3001,0,0.0232127,"Missing"
W15-2914,S14-2086,0,0.0351838,"Missing"
W15-2914,S15-2097,0,0.026682,"Missing"
W15-2914,S15-2095,0,0.0189962,"Missing"
W15-2914,D14-1108,0,0.0281456,"plicit cues adopted from Councill et al. (2010), as shown in Table 2, where *n’t matches all strings with the suffix n’t. Note that this list is more extensive than the one of Potts (2011), used in many SemEval systems. Four cues on Potts’ list are not in Table 2 (noone, couldnt, wont, arent), while the 17 cues in italics are not listed by Potts. An inspection of the 37 cues appearing in the Twitter Negation Corpus revealed seven more cues / spelling variants included on neither list (idk, dnt, cudnt, ain, eint, neva, neeeever). Tweets are preprocessed with the TweeboParser dependency parser (Kong et al., 2014), that performs tokenisation, part-of-speech tagging and parsing, labeling each token with its dependency head. A dependency-based binary CRF classifier then for each token determines whether it is in a negated or affirmative context. The CRF implementation by Okazaki (2007) is used, with a Python binding created by Peng and Korobov (2014). Table 3: Negation classifier feature set The classifier is a Twitter-tailored version of the system described by Councill et al. (2010) with one change: the dependency distance from each token to the closest negation cue has been added to the feature set, w"
W15-2914,konstantinova-etal-2012-review,0,0.0628265,"Missing"
W15-2914,S14-2111,0,0.0348763,"Missing"
W15-2914,W10-0204,0,0.0169124,"Hashtag Sentiment Lexicon and Sentiment140 Lexicon (Kiritchenko et al., 2014) contain sentiment scores for words in negated contexts. For lookups, the first negated word in a negation scope is appended with _NEGFIRST, and the rest with _NEG. The sentiment lexica feature vectors are adopted from Kiritchenko et al. (2014) and contain the number of tokens with score(w) 6= 0, the total score, the maximal score, and the score of the last token in the tweet. We also use The MPQA Subjectivity Lexicon (Wilson et al., 2005), Bing Liu’s Opinion Lexicon (Ding et al., 2008), and the NRC Emotion Lexicon (Mohammad and Turney, 2010), assigning scores of +/ − 2 for strong and +/ − 1 for weak degrees of sentiment. The resulting four 2 Feature Description Word n-grams Char n-grams Negated tokens Sentiment lexica contiguous token sequences contiguous character sequences number of negated tokens feature set for each lexicon Clusters POS All caps Elongated Emoticons Punctuation Hashtags tokens from ‘1000 word clusters’ part-of-speech tag frequency upper-case tokens tokens with repeated characters positive and negative emoticons punctuation mark sequences number of hashtags Table 6: Sentiment classifier STATE feature set featur"
W15-2914,S13-2053,0,0.0252737,"Missing"
W15-2914,S12-1033,0,0.0266495,"Missing"
W15-2914,S12-1035,0,0.0416164,"Missing"
W15-2914,W09-1105,0,0.654934,"such as clinical reports and discharge summaries, but has in recent times shifted towards sentiment analysis (SA). Early solutions were typically rule-based, such as the NegFinder (Mutalik et al., 2001) and NegEx (Chapman et al., 2001) systems, that both heavily incorporated the use of regular expressions. NSD was the focus of a shared task at *SEM 2012 (Morante and Blanco, 2012), and in 2010 CoNLL included a similar sub-task on detecting speculation cues and their affected scope (Farkas et al., 2010). Most well-performing submissions to both tasks used supervised machine learning approaches. Morante and Daelemans (2009) developed an NSD system that uses meta-learning for classification. They applied this approach to the CoNLL’10 shared task and achieved the best F1 -score of all participating teams. The tokens were first tagged and split into chunks, and the main algorithm then consisted of two steps: signal identification (negation cue detection) and scope identification. For the first phase, Morante and Daelemans (2009) used a decision tree to classify if a token is at the beginning, inside or outside a negation signal. In the second phase, a Conditional Random Fields (CRF)-based meta-learner predicted sco"
W15-2914,J12-2001,0,0.0259906,"rs are terms that change the sentimental orientation of other terms. In sentiment analysis, negators often act as valence shifters, since flipping a proposition’s truth value significantly shifts, or reverses, the valence it conveys. Givón (1993) defines two forms of grammatical negation: morphological, where individual words are negated with an affix, and syntactic, where a set of words is negated by a word or phrase; the topic of the present paper. Negators in syntactical negation, known as negation cues or negation signals, function as operators, with an associated affected scope of words (Morante and Sporleder, 2012). The most common negation cue in English is not, along with its contractions, such as couldn’t or isn’t (Tottie, 1991). Negation classifiers have been developed for other domains with dramatic performance improvements (Section 2). However, almost all state-of-theart Twitter sentiment analysis systems use a simple approach of marking as negated all terms from a negation cue to the next punctuation (Section 3). We present this simple model as a baseline, but improve on it by introducing sophisticated negation scope detection for Twitter sentiment analysis. Several negation-annotated corpora are"
W15-2914,S13-2052,0,0.0229699,"Missing"
W15-2914,N13-1039,0,0.0215733,"Missing"
W15-2914,S15-2103,0,0.0253567,"Missing"
W15-2914,S14-2009,0,0.0521739,"Missing"
W15-2914,S15-2078,0,0.0270086,"Missing"
W15-2914,S13-2071,1,0.900223,"Missing"
W15-2914,S14-2033,0,0.0467499,"Missing"
W15-2914,W08-0606,0,0.0171762,"Missing"
W15-2914,W10-3111,0,0.138987,"Missing"
W15-2914,H05-2018,0,0.0114427,"ring _NEG. The negated tokens feature is simply a count of the tokens in a negated context. The NRC Hashtag Sentiment Lexicon and Sentiment140 Lexicon (Kiritchenko et al., 2014) contain sentiment scores for words in negated contexts. For lookups, the first negated word in a negation scope is appended with _NEGFIRST, and the rest with _NEG. The sentiment lexica feature vectors are adopted from Kiritchenko et al. (2014) and contain the number of tokens with score(w) 6= 0, the total score, the maximal score, and the score of the last token in the tweet. We also use The MPQA Subjectivity Lexicon (Wilson et al., 2005), Bing Liu’s Opinion Lexicon (Ding et al., 2008), and the NRC Emotion Lexicon (Mohammad and Turney, 2010), assigning scores of +/ − 2 for strong and +/ − 1 for weak degrees of sentiment. The resulting four 2 Feature Description Word n-grams Char n-grams Negated tokens Sentiment lexica contiguous token sequences contiguous character sequences number of negated tokens feature set for each lexicon Clusters POS All caps Elongated Emoticons Punctuation Hashtags tokens from ‘1000 word clusters’ part-of-speech tag frequency upper-case tokens tokens with repeated characters positive and negative emoti"
W15-2914,D10-1070,0,0.0193669,"r, it was evaluated on the SFU Review corpus and the biological full paper part of BioScope, as that sub-corpus has proved to be difficult for negation identification. Table 5 shows the 5-fold cross-validated performance of the sophisticated negation scope identifier on both corpora, as well as the simple baseline on Bioscope together with the results reported on the same data for the approaches described in Section 2. ‘CRF’ denotes the CRF-based system from Councill et al. (2010), ‘MetaLearn’ the meta-learner of Morante and Daelemans (2009), and ‘SSP’ the shallow semantic parsing solution by Zhu et al. (2010). As can be seen, the twitter-trained sophisticated negation classifier performs reasonably well on the SFU Review Corpus, but struggles when applied to BioScope, as expected. It is outperformed in terms of F1 score by Councill et al. (2010) and Morante and Daelemans (2009), but reaches a slightly better PCS than the latter system. The modest F1 score is likely caused by the use of upstream preprocessing tools tailored towards Twitter language, which differs significantly from that of biomedical texts. 103 Notably, the simple model is a strong baseline, which actually outperforms the shallow p"
W15-5906,debole-sebastiani-2004-analysis,0,0.0235324,"results, several ways of processing the corpus have been established. Those pertain to the corpus subsets, and a specific split of it into training and test sets called the ModApt´ e split (Lewis, 1997). A further much used split means dividing the ModApt´ e into either the ten most frequent categories, the categories with at least one positive training and one test example (90), or the categories with at least one training example (115). The split with the 90 categories with at least one positive training and test example is somewhat confusingly called Apt´ eMod by Yang and Liu (1999), while Debole and Sebastiani (2004) refer to it as the R(90) subset of the ModApt´ e split. Elsewhere, the R(90) split with 10,789 documents is also called ModApt´ e (Yang et al., 2009; Chen et al., 2004; Saarikoski et al., 2011). The Natural Language Toolkit (NLTK) interface (Loper and Bird, 2002) to the Reuters Corpus provides the Apt´ eMod split as comprised by 7,769 training examples and 3,019 test instances. The category frequencies are [9160, 1173, 255, 91, 52, 27, 9, 7, 5, 3, 2, 1, 0, 2, 1], i. e., 9,160 documents with just one category, 1,173 with two, etc. A plot of the log-frequency of categories is shown in Figure 4."
W15-5906,W05-0710,1,0.82728,"Missing"
W15-5906,W02-0109,0,0.109977,"the ten most frequent categories, the categories with at least one positive training and one test example (90), or the categories with at least one training example (115). The split with the 90 categories with at least one positive training and test example is somewhat confusingly called Apt´ eMod by Yang and Liu (1999), while Debole and Sebastiani (2004) refer to it as the R(90) subset of the ModApt´ e split. Elsewhere, the R(90) split with 10,789 documents is also called ModApt´ e (Yang et al., 2009; Chen et al., 2004; Saarikoski et al., 2011). The Natural Language Toolkit (NLTK) interface (Loper and Bird, 2002) to the Reuters Corpus provides the Apt´ eMod split as comprised by 7,769 training examples and 3,019 test instances. The category frequencies are [9160, 1173, 255, 91, 52, 27, 9, 7, 5, 3, 2, 1, 0, 2, 1], i. e., 9,160 documents with just one category, 1,173 with two, etc. A plot of the log-frequency of categories is shown in Figure 4. The NLTK interface provides a data structure where the raw text and categories are indexed on filenames. This was used in the present work, and the documents were transformed into a matrix of TF-IDF values, i. e., the product of the Term Frequency (the occurrence"
W15-5906,C02-1087,0,0.0546567,"nified Distance Matrix of SOM with grid size 64x64. Top 10 categories. 6 Discussion and Related Work Using the unsupervised Self-Organizing Map approach we observe relations between categories, both through clusters of documents belonging to similar categories being placed next to each other on the map, and also in terms of some nodes in the SOM attracting separate documents with thematically adjacent labels as their Best-Matching Unit, such as money-fx and interest. This could be a reflection of these documents being in a thematical intersection between topics, or outright belonging to both. Wermter and Hung (2002) integrated the SOM with WordNet-based (Miller, 1995) semantic networks for doing classification on the Reuters Corpus. Documents were represented by significance vectors, i. e., the degree to which the documents belong to certain preassigned topics, that were in turn calculated from the importance of words in each category. Saarikoski (2009) performed several experi46 ments on using Self-Organizing Maps for Information Retrieval and document classification (Saarikoski et al., 2011), comparing the algorithm to other machine learning techniques. While they explained how the SOM was applied to t"
W15-5938,P96-1025,0,0.372296,"texts, sentence boundary detection has been considered a more or less solved problem since the 1990s, but the proliferation of social media has added new 254 challenges to language processing and new difficulties for SBD, with state-of-the-art systems failing to perform well on social media, due to the coarse nature of the texts. In spite of its important role for language processing, sentence boundary detection has so far not received enough attention. Previous research in the area has been confined to formal texts only, and either has not addressed the process of SBD directly (Brill, 1994; Collins, 1996), or not the performance related issues of sentence boundary detection (Cutting et al., 1992). In particular, no SBD research to date has addressed the problem in informal texts such as Twitter and Facebook posts. The growth of social media is a global phenomenon where people are communicating both using single languages and using mixes of several languages. The social media texts are informal in nature, and posts on Twitter and Facebook tend to be full of misspelled words, show extensive use of home-made acronyms and abbreviations, and contain plenty of punctuation applied in creative and non"
W15-5938,A92-1018,0,0.721034,"since the 1990s, but the proliferation of social media has added new 254 challenges to language processing and new difficulties for SBD, with state-of-the-art systems failing to perform well on social media, due to the coarse nature of the texts. In spite of its important role for language processing, sentence boundary detection has so far not received enough attention. Previous research in the area has been confined to formal texts only, and either has not addressed the process of SBD directly (Brill, 1994; Collins, 1996), or not the performance related issues of sentence boundary detection (Cutting et al., 1992). In particular, no SBD research to date has addressed the problem in informal texts such as Twitter and Facebook posts. The growth of social media is a global phenomenon where people are communicating both using single languages and using mixes of several languages. The social media texts are informal in nature, and posts on Twitter and Facebook tend to be full of misspelled words, show extensive use of home-made acronyms and abbreviations, and contain plenty of punctuation applied in creative and non-standard ways. The punctuation markers are also often ambiguous in these types of texts — in"
W15-5938,N09-2061,0,0.908979,"Missing"
W15-5938,P11-2008,0,0.0643046,"Missing"
W15-5938,R15-1033,1,0.863576,"Missing"
W15-5938,J06-4003,0,0.820996,"Missing"
W15-5938,J02-3002,0,0.438312,"Missing"
W15-5938,J97-2002,0,0.692022,"Missing"
W15-5938,A97-1004,0,0.534917,"Missing"
W15-5938,H89-2048,0,0.875076,"Missing"
W16-3922,W15-4308,1,0.830311,"Missing"
W16-3922,W15-4319,0,0.124928,"Missing"
W16-3922,P11-1037,0,0.265089,"Missing"
W16-3922,Q14-1019,0,0.0347158,"re lexicon-based, while the last five types primarily relate to the context. The eight feature types in the middle of the list are predominantly word internal and character-based. The contributions of these three different groups of feature types will be compared to each other in the experimental section (Section 3). Lexicon-based features Lexical data: This binary feature was extracted from the lexical data supplied by the shared task organisers. The feature is set to 1 if the current word belongs to the lexical data, otherwise 0. Babelfy named entities: Each tweet was passed to the Babelfy (Moro et al., 2014) named entity recognition system for recognizing Twitter names. If the current word belongs to the Babelfy named entities, this binary feature is set. Part-of-speech (POS): The TweeboParser2 was used for generating part-of-speech tags for each token in the tweets and the POS tag of the current word was used as a feature. The POS tags of the previous two tokens and following two tokens were also used as features, so in total there are five POS tag features for each token. Stop word match: All tokens are checked against a stop word list collected from the web.3 The binary feature is set if the c"
W16-3922,D09-1026,0,0.761135,"Missing"
W16-3922,D11-1141,0,0.743364,"Missing"
W16-3922,W16-3919,0,0.126585,"Missing"
W16-3922,W15-4320,0,0.116864,"Missing"
W16-5817,C82-1023,0,0.717516,"nd tokens from other languages, and ‘Ambiguous’ denotes tokens that cannot safely be assigned any (or only one) of the other labels. This shared task was organized again this year (Molina et al., 2016), with new datasets and slightly different labels, adding ‘Unk’ for unknown tokens.1 Work on developing tools for automatic language identification was initiated already in the 1960s (Gold, 1967), and although analysing codeswitched text is a research area which has started to achieve wide-spread attention only in recent years, the first work in the field was carried out over thirty years ago by Joshi (1982), while Bentahila and Davies (1983) examined the syntax of the intra-sentential code-switching between Arabic and French. They claimed that Arabic-French codeswitching was possible at all syntactic boundaries above the word level. Das and Gamb¨ack (2013) give a comprehensive overview of the work on code-switching until 2015. Notably, Solorio and Liu (2008) trained classifiers to predict code-switching points in Spanish and English, using different learning algorithms and transcriptions of code-switched discourse, while Nguyen and Do˘gru¨oz (2013) focused on wordlevel language identification (i"
W16-5817,W16-5805,0,0.25959,"identify the language for each word in a text, classifying the words according to six labels: ‘Lang1’, ‘Lang2’, ‘Mixed’, ‘NE’, ‘Other’, and ‘Ambiguous’. The first two labels identify tokens from the main languages that are mixed in the text, while the third is for tokens with word-internal mixing between these languages; ‘NE’ for named entities; ‘Other’ for language independent tokens (punctuation, numbers, etc.) and tokens from other languages, and ‘Ambiguous’ denotes tokens that cannot safely be assigned any (or only one) of the other labels. This shared task was organized again this year (Molina et al., 2016), with new datasets and slightly different labels, adding ‘Unk’ for unknown tokens.1 Work on developing tools for automatic language identification was initiated already in the 1960s (Gold, 1967), and although analysing codeswitched text is a research area which has started to achieve wide-spread attention only in recent years, the first work in the field was carried out over thirty years ago by Joshi (1982), while Bentahila and Davies (1983) examined the syntax of the intra-sentential code-switching between Arabic and French. They claimed that Arabic-French codeswitching was possible at all s"
W16-5817,Q14-1019,0,0.229635,"zero-mean, σ 2 variance Gaussian prior over parameters, which facilitates optimization by making the likelihood surface strictly convex. Here, we set the parameter λ to maximize the penalized log-likelihood using Limited-memory BFGS (Sha and Pereira, 2003), a quasi-Newton method that is highly efficient, and which results in only minor changes in accuracy due to changes in λ. 2.1 Features based on training data Two sets of features were developed to train the model: one extracted from the training data and the other based on information from Babelnet (Navigli and Ponzetto, 2012) and Babelfy (Moro et al., 2014), with most of the features and their settings being based on the training data. The complete set of features induced from training data was as follows: Local context. Local contexts play an important role for identifying the languages. Here the two preceding and two succeeding words were used as local context. Word suffix and prefix. Fixed length characters stripped from the beginning and ending of the current word. Up to 4 characters were removed. Word length. Analysis of the training data showed that the Spanish words on average were shorter than the English words. Words with 1–4 characters"
W16-5817,W16-2013,0,0.125704,") examined the syntax of the intra-sentential code-switching between Arabic and French. They claimed that Arabic-French codeswitching was possible at all syntactic boundaries above the word level. Das and Gamb¨ack (2013) give a comprehensive overview of the work on code-switching until 2015. Notably, Solorio and Liu (2008) trained classifiers to predict code-switching points in Spanish and English, using different learning algorithms and transcriptions of code-switched discourse, while Nguyen and Do˘gru¨oz (2013) focused on wordlevel language identification (in Dutch-Turkish news commentary). Nguyen and Cornips (2016) describe 1 An eighth label ‘FW’ was included for foreign words, but no words in the English-Spanish corpora were tagged with it. 127 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 127–131, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics work on analyzing and detecting intra-word codemixing by first segmenting words into smaller units and later identifying words composed of sequences of subunits associated with different languages in tweets (posts on the Twitter social-media site). The paper is organized as follows: Sectio"
W16-5817,D13-1084,0,0.0707104,"Missing"
W16-5817,N03-1028,0,0.0386958,"Equation 2. 128 Zo = X T X K X exp( λk × fk (st−1 , st , o, t)) (2) s t=1 k=1 To train a CRF, the objective function to be maximized is the penalized log-likelihood of the state sequences given the observation sequences: L∧ = N X i=1 log(P∧ (s(i) |o(i) )) − K X λ2k 2σ 2 (3) k=1 where {< o(i) , s(i) &gt;} is the labelled training data. The second sum corresponds to a zero-mean, σ 2 variance Gaussian prior over parameters, which facilitates optimization by making the likelihood surface strictly convex. Here, we set the parameter λ to maximize the penalized log-likelihood using Limited-memory BFGS (Sha and Pereira, 2003), a quasi-Newton method that is highly efficient, and which results in only minor changes in accuracy due to changes in λ. 2.1 Features based on training data Two sets of features were developed to train the model: one extracted from the training data and the other based on information from Babelnet (Navigli and Ponzetto, 2012) and Babelfy (Moro et al., 2014), with most of the features and their settings being based on the training data. The complete set of features induced from training data was as follows: Local context. Local contexts play an important role for identifying the languages. He"
W16-5817,D08-1102,0,0.0184696,"was initiated already in the 1960s (Gold, 1967), and although analysing codeswitched text is a research area which has started to achieve wide-spread attention only in recent years, the first work in the field was carried out over thirty years ago by Joshi (1982), while Bentahila and Davies (1983) examined the syntax of the intra-sentential code-switching between Arabic and French. They claimed that Arabic-French codeswitching was possible at all syntactic boundaries above the word level. Das and Gamb¨ack (2013) give a comprehensive overview of the work on code-switching until 2015. Notably, Solorio and Liu (2008) trained classifiers to predict code-switching points in Spanish and English, using different learning algorithms and transcriptions of code-switched discourse, while Nguyen and Do˘gru¨oz (2013) focused on wordlevel language identification (in Dutch-Turkish news commentary). Nguyen and Cornips (2016) describe 1 An eighth label ‘FW’ was included for foreign words, but no words in the English-Spanish corpora were tagged with it. 127 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 127–131, c Austin, TX, November 1, 2016. 2016 Association for Computational L"
W16-5817,W14-3907,0,0.166217,"Missing"
W16-6326,W15-4308,1,0.840373,"Missing"
W16-6326,W15-4319,0,0.0324046,"Missing"
W16-6326,W16-3920,0,0.0216148,"Missing"
W16-6326,P11-1037,0,0.0241236,"Missing"
W16-6326,D09-1026,0,0.0534657,"Missing"
W16-6326,D11-1141,0,0.172125,"Missing"
W16-6326,W16-3922,1,0.866404,"Missing"
W16-6326,W16-3919,0,0.0204367,"Missing"
W16-6326,W15-4320,0,0.0297971,"Missing"
W16-6326,P15-1049,0,0.0288062,"Missing"
W17-0210,C12-1093,0,0.0209697,"Missing"
W17-0210,D11-1024,0,0.0430693,"es. Extracting topic models Topic models can be extracted in several ways, in addition to the LDA-based methods and SemEval methods outlined above. Specifically, here three sources of information are singled out for this purpose: topic model scores, topic clustering, and hashtags. 3.1 Topic model scoring The unsupervised nature of topic discovery makes the assessment of topic models challenging. Quantitative metrics do not necessarily provide accurate reflections of a human’s perception of a topic model, and hence a variety of evaluation metrics have been proposed. The UMass coherence metric (Mimno et al., 2011) measures topic coherence: C = D(wm ,wl )+1 m−1 with (w1 , ..., wM ) ∑M m=2 ∑l=1 log D(wl ) being the M most probable words in the topic, D(w) the number of documents that contain word w, and D(wm , wl ) the number of documents that contain both words wm and wl . The metric utilizes word co-occurrence statistics gathered from the corpus, which ideally already should be accounted for in the topic model. Mimno et al. (2011) achieved reasonable results when comparing the scores obtained by this measure with human scoring on a corpus of 300,000 health journal abstracts. 3.2 Clustering tweets An im"
W17-0210,S15-2107,0,0.0495732,"Missing"
W17-0210,S15-2103,0,0.0411967,"Missing"
W17-0210,S15-2094,0,0.0606415,"Missing"
W17-3013,W16-5618,0,0.403705,"Missing"
W17-3013,S16-1001,0,0.0686449,"Missing"
W17-3013,N16-2013,0,0.220822,"Missing"
W17-4424,W16-3920,0,0.134811,"Missing"
W17-4424,W15-4308,1,0.882014,"Missing"
W17-4424,W15-4319,0,0.113456,"Missing"
W17-4424,P11-1037,0,0.241874,"Missing"
W17-4424,W17-4418,0,0.0321212,"Missing"
W17-4424,D14-1162,0,0.0809522,"ge,5 and the W-NUT 2017 workshop datasets to build the word2vec model (Mikolov et al., 2013a,b). The skip-gram approach was used with negative sampling and a context window of 5. All features were then concatenated into one vector and fed to the first LSTM network for mention recognition. After a mention had been identified, it was used as one of the features for recognition of named entities in the second LSTM model, which as features together with word-shape-1 and wordshape-2 (as above) utilized three Boolean flags (is-mention, is-start-with-capital-letter, and is-allupper-case), and GloVe (Pennington et al., 2014), 2.4 A Named Entity Recognition Ensemble In the second step, the outputs of the above three classifiers were considered as input features to a CRF classifier, which was trained using these three features together with the previous and next two context words. Note that this final CRF classifier being used a selector in the ensemble thus does not cover all features of the CRF classifier described above (Section 2.1), but only utilizes the context and the three classifiers’ outputs as features. An ensemble based on using majority voting was also tested, which selected the output of one of the cl"
W17-4424,D09-1026,0,0.116661,"Missing"
W17-4424,P82-1020,0,0.81791,"Missing"
W17-4424,C02-1054,0,0.453935,"(LSTM) recurrent neural network. The rest of the paper is organized as follows: The named entity identification methodology and the different features used are introduced in Section 2. Results are presented and discussed in Section 3, while Section 4 addresses future work and concludes. 1 2 178 https://taku910.github.io/crfpp/ Here ’-’ and ’+’ indicate the number of preceding and following words in the context window, respectively. 2.2 SVM-based Named Entity Recognition Since Support Vector Machines previously have been successfully utilized to recognize named entities in formal text, e.g. by Isozaki and Kazawa (2002), a classifier was built using the C++ based SVM package Yamcha3 with polynomial kernel and default settings. The same features as for the CRF model were used to train the SVMs. 2.3 Data set tweets named entities Training Development Test 3,394 1,009 1,287 1,975 833 1,041 Table 1: Twitter dataset statistics a pre-trained Twitter word vector (here a GloVe vector of dimension 100 was selected). These features were concatenated to train an LSTM model for 50 epochs with a batch size of 256. The network was set up as consisting of two hidden layers with 256 hidden units. LSTM-based Named Entity Rec"
W17-4424,D11-1141,0,0.319661,"Missing"
W17-4424,W16-3922,1,0.617896,"ment Test 3,394 1,009 1,287 1,975 833 1,041 Table 1: Twitter dataset statistics a pre-trained Twitter word vector (here a GloVe vector of dimension 100 was selected). These features were concatenated to train an LSTM model for 50 epochs with a batch size of 256. The network was set up as consisting of two hidden layers with 256 hidden units. LSTM-based Named Entity Recognition The proposed deep learning based name entity recognition model consists of two Long ShortTerm Memory recurrent neural network (Hochreiter and Schmidhuber, 1997), a model which was also successfully used by Lample et al. (2016) to achieve state-of-the-art named entity recognition results in formal texts. The first LSTM identifies the boundaries of a named entity (called mention) and this mention is then used as one of the features for named entity recognition in the second LSTM. For identifying mentions, two binary features, is-start-with-capital-letter and is-all-uppercase, were extracted together with the following: • word shape-1, a length 6 one-hot vector containing the following six binary flags: upper case, lower case, digit, ’@’ symbol, ’#’ symbol, and other characters, • word shape-2, a length 39 one-hot vec"
W17-4424,W16-3919,0,0.0591778,"ment Test 3,394 1,009 1,287 1,975 833 1,041 Table 1: Twitter dataset statistics a pre-trained Twitter word vector (here a GloVe vector of dimension 100 was selected). These features were concatenated to train an LSTM model for 50 epochs with a batch size of 256. The network was set up as consisting of two hidden layers with 256 hidden units. LSTM-based Named Entity Recognition The proposed deep learning based name entity recognition model consists of two Long ShortTerm Memory recurrent neural network (Hochreiter and Schmidhuber, 1997), a model which was also successfully used by Lample et al. (2016) to achieve state-of-the-art named entity recognition results in formal texts. The first LSTM identifies the boundaries of a named entity (called mention) and this mention is then used as one of the features for named entity recognition in the second LSTM. For identifying mentions, two binary features, is-start-with-capital-letter and is-all-uppercase, were extracted together with the following: • word shape-1, a length 6 one-hot vector containing the following six binary flags: upper case, lower case, digit, ’@’ symbol, ’#’ symbol, and other characters, • word shape-2, a length 39 one-hot vec"
W17-4424,N16-1030,0,0.054273,"aining Development Test 3,394 1,009 1,287 1,975 833 1,041 Table 1: Twitter dataset statistics a pre-trained Twitter word vector (here a GloVe vector of dimension 100 was selected). These features were concatenated to train an LSTM model for 50 epochs with a batch size of 256. The network was set up as consisting of two hidden layers with 256 hidden units. LSTM-based Named Entity Recognition The proposed deep learning based name entity recognition model consists of two Long ShortTerm Memory recurrent neural network (Hochreiter and Schmidhuber, 1997), a model which was also successfully used by Lample et al. (2016) to achieve state-of-the-art named entity recognition results in formal texts. The first LSTM identifies the boundaries of a named entity (called mention) and this mention is then used as one of the features for named entity recognition in the second LSTM. For identifying mentions, two binary features, is-start-with-capital-letter and is-all-uppercase, were extracted together with the following: • word shape-1, a length 6 one-hot vector containing the following six binary flags: upper case, lower case, digit, ’@’ symbol, ’#’ symbol, and other characters, • word shape-2, a length 39 one-hot vec"
W17-4424,W15-4320,0,0.0299669,"Missing"
W18-3215,W18-3219,0,0.0644326,"Missing"
W18-3215,W16-3920,0,0.0421974,"Missing"
W18-3215,A97-1029,0,0.222757,"Missing"
W18-3215,W15-3904,0,0.0575772,"Missing"
W18-3215,Q16-1026,0,0.0903835,"Missing"
W18-3215,W16-5817,1,0.876258,"Missing"
W18-3215,W17-4424,1,0.89678,"Missing"
W18-3215,C02-1054,0,0.265728,"Missing"
W18-5110,gao-huang-2017-detecting,0,0.2085,"tworks of user interactions. Several authors share this intention, but face the challenge that user information often is limited or unavailable. 3 Data Analysis Creating datasets of hate speech is time consuming, as the number of hateful instances in online communities is relatively low. The datasets available are also often created for different tasks, and from different types of media and languages, and therefore vary in characteristics and types of hate speech. Sources include Twitter (Waseem and Hovy, 2016; Fortuna, 2017; Ross et al., 2016), Wikipedia (Wulczyn et al., 2017), and Fox News (Gao and Huang, 2017). Furthermore, many datasets (from Yahoo, SpaceOrigin and Twitter) are not publicly available (Djuric et al., 2015; Nobata et al., 2016; Papegnies et al., 2017; Chatzakou et al., 2017), while others are available only under some restrictions (Davidson et al., 2017; Golbeck et al., 2017). This may be due to privacy issues or considering the content of the datasets: Pavlopoulos et al. (2017) made their Greek Gazzetta dataset available by using an encryptor to avoid directly publishing hate speech content. Here, three datasets were used to investigate the characteristics of users for increased in"
W18-5110,W16-3638,0,0.373221,"en in the next section. Classification Model: Supervised machine learning classifiers have been the most frequently used approaches to hate speech detection, in particular Support Vector Machines (SVM) and Logistic Regression (LR). Davidson et al. (2017) found LR and linear SVM to perform better than other models, such as Na¨ıve Bayes, Decision Trees, and Random Forests. A comparative study performed by Burnap and Williams (2015) concluded that an ensemble method seemed most promising. Deep learning methods have also been investigated, both Recurrent Neural Networks (Pavlopoulos et al., 2017; Mehdad and Tetreault, 2016), Convolutional Neural Networks (Gamb¨ack and Sikdar, 2017), and combinations (Zhang et al., 2018). Badjatiya et al. (2017) used various deep learning architectures to learn semantic words embeddings and showed these to outperform character and word n-grams. Here a Logistic Regression model was chosen due to its simplicity and its common usage in language classification. This is also in line with the note by Gr¨ondahl et al. (2018) that a simple LR model performed on par with more complex models in their comparison of hate speech detection classifiers. As the aim here was not to implement the"
W18-5110,N16-2013,0,0.106142,"viour, hate speech or trolling. Chen et al. (2012) 75 Proceedings of the Second Workshop on Abusive Language Online (ALW2), pages 75–85 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics in the network) were particularly useful and effective in classifying aggressive user behaviour. proposed a Lexical Syntactic Feature architecture to bridge the gap between detecting offensive content and potential offensive users in social media, arguing that although existing methods treat messages as independent instances, the focus should be on the source of the content. Waseem and Hovy (2016) stated that among various extra-linguistic features, only gender brought improvements to hate speech detection. Papegnies et al. (2017) mention a plan to use context-based features for abuse detection, and especially those based on the networks of user interactions. Several authors share this intention, but face the challenge that user information often is limited or unavailable. 3 Data Analysis Creating datasets of hate speech is time consuming, as the number of hateful instances in online communities is relatively low. The datasets available are also often created for different tasks, and f"
W18-5110,D17-1117,0,0.172764,"ia and languages, and therefore vary in characteristics and types of hate speech. Sources include Twitter (Waseem and Hovy, 2016; Fortuna, 2017; Ross et al., 2016), Wikipedia (Wulczyn et al., 2017), and Fox News (Gao and Huang, 2017). Furthermore, many datasets (from Yahoo, SpaceOrigin and Twitter) are not publicly available (Djuric et al., 2015; Nobata et al., 2016; Papegnies et al., 2017; Chatzakou et al., 2017), while others are available only under some restrictions (Davidson et al., 2017; Golbeck et al., 2017). This may be due to privacy issues or considering the content of the datasets: Pavlopoulos et al. (2017) made their Greek Gazzetta dataset available by using an encryptor to avoid directly publishing hate speech content. Here, three datasets were used to investigate the characteristics of users for increased insight and to allow comparisons of the findings. All datasets have Twitter as their source, ensuring that the same information could be retrieved. However, the datasets differ in terms of annotations, size and characteristics, and come from three different languages: English (Waseem and Hovy, 2016), Portuguese (Fortuna, 2017), and German (Ross et al., 2016). The datasets contain tweet IDs t"
W18-6215,L18-1447,1,0.887612,"below 0.5. If no score is > θc , the tweet is skipped. VADER also gives a compound score, a single sentiment score from −1 to 1 (most positive). The methods’ hyperparameters were tuned through grid searches, testing each value in increasing steps of 0.1. VADER struggled to classify positive and negative tweets as the threshold increased, and performed best at θc = 0.1. TextBlob performed best with a low subjectivity threshold, with θs = 0.1 and θp = 0.3 chosen for the final classifier, as these values gave the best Macro F1 . Lexicon Classifier (LC): A Python port of the Lexicon Classifier of Fredriksen et al. (2018), using their best performing lexicon and parameters.3 Evaluation: All manually annotated SemEval datasets from 2013 to 2016 were downloaded. They contain IDs for 50,333 tweets, but 10,251 of those had been deleted, while duplicates were removed,4 leaving 39,731 tweets for later classifier training (the second column of Table 1). For the distant supervision, further filtering removed retweets (i.e., copies of original tweets; including retweets might lead to over-representation of certain phrases), tweets containing ° symbols (mostly weather data), tweets containing URLs, and tweets ending wit"
W18-6215,S13-2052,0,0.0444553,"he entire Twitter Sentiment Analysis system performance to that of Tang et al. (2016), the unfiltered datasets from SemEval 2013 were chosen for the classifier optimisation, with training on the 7,109 tweet 2013-train set (distributed 2,660-1,010-3,439 positive-negativeneutral) and testing on the 2013-dev set (1,228 tweets distributed 430-245-553), as this was the validation set of the 2013 workshop. 6.4 Comparison to SemEval To see how the final Twitter Sentiment Analysis system fares against the state-of-the-art, its performance was compared to the published results of SemEval 2013 Task 2B (Nakov et al., 2013), SemEval 2016 Task 4A (Nakov et al., 2016), and SemEval 2017 Task 4A (Rosenthal et al., 2017). The system was trained using the training sets provided by the respective workshop. For 2013, the model was trained on 2013-train-A and tested on 2013-test-A. SemEval 2016 and 2017 allowed training on the training and development datasets of previous years, so for 2016, the model was trained on a combined 2013-2016-train-dev-A dataset and tested on 2016-test-A, while for 2017, the model was trained on all 2013-2016 datasets and tested on 2017-test-A. The results in Table 6 show that the Ternary Sent"
W18-6215,D14-1162,0,0.0878548,"eated by picking a random label from a uniform probability distribution and by picking a random label from the same distribution as in the training set. The distant supervision classifiers are as above, except that the Emoticons and Emoji+ methods add the variation that tweets containing both negative and positive emoticons are regarded as neutral. The word embeddings for Ternary Sentiment Embedding Model, GloVe and word2vec were all trained on the same set of 3M tweets, with 1M from each of the sentiment classes, assigned by the Lexicon Classifier distant supervision method. The GloVe model (Pennington et al., 2014) was used to train word embeddings of dimension 100. The word2vec (Mikolov et al., 2013) embeddings were trained using both the Continuous Bag-ofWords (CBOW) and the Continuous Skip-gram model, also with 100 dimensions. Word embeddings were also produced using the Hybrid Ranking Model of Tang et al. (2016) trained on a set of 3M tweets classified with the LC method, but using only tweets labelled as positive or negative when training word embeddings, with 1.5M tweets of each class. All the word embeddings were fed to the SVM classifier specified above. As the results in Table 4 shows, the best"
W18-6215,S17-2088,0,0.0236523,"unfiltered datasets from SemEval 2013 were chosen for the classifier optimisation, with training on the 7,109 tweet 2013-train set (distributed 2,660-1,010-3,439 positive-negativeneutral) and testing on the 2013-dev set (1,228 tweets distributed 430-245-553), as this was the validation set of the 2013 workshop. 6.4 Comparison to SemEval To see how the final Twitter Sentiment Analysis system fares against the state-of-the-art, its performance was compared to the published results of SemEval 2013 Task 2B (Nakov et al., 2013), SemEval 2016 Task 4A (Nakov et al., 2016), and SemEval 2017 Task 4A (Rosenthal et al., 2017). The system was trained using the training sets provided by the respective workshop. For 2013, the model was trained on 2013-train-A and tested on 2013-test-A. SemEval 2016 and 2017 allowed training on the training and development datasets of previous years, so for 2016, the model was trained on a combined 2013-2016-train-dev-A dataset and tested on 2016-test-A, while for 2017, the model was trained on all 2013-2016 datasets and tested on 2017-test-A. The results in Table 6 show that the Ternary Sentiment Embedding Model does not match the top systems of the different years. There are some po"
W18-6215,P14-1146,0,0.290562,"dings, and after the introduction of word2vec (Mikolov et al., 2013), which is much faster to train than its predecessors, word embeddings have become ubiquitous. This effectuated a dramatic shift in 2016 at the International Workshop on Semantic Evaluation (SemEval), with eight of the top-10 Twitter Sentiment Analysis systems using word embeddings. Word embeddings learn the representation of a word by looking at its contexts (word neighbours in a text), making it difficult to discriminate between words with opposite sentiments that appear in similar contexts, such as “good” and “bad”. Hence, Tang et al. (2014) presented SentimentSpecific Word Embeddings (or Sentiment Embeddings), a model employing both context and sentiment information in word embeddings. 2 Related Work Recent years have seen a vast number of Twitter Sentiment Analysis (TSA) systems, mainly because SemEval since 2013 has featured a TSA task, providing training data and a platform to compare different systems. This data will be uti97 Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 97–106 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational L"
W19-3516,P82-1020,0,0.650009,"Missing"
W19-3516,W16-3638,0,0.0662948,"Missing"
W19-3516,S19-2131,0,0.819222,"can thus be compared more directly. Although the SVM-Naïve Bayes classifier of Mehdad and Tetreault (2016) outperformed their RNN-based system, deep learners seem to in general perform better than purely traditional machine learning classifiers on this dataset, with the CNN-based system of Gambäck and Sikdar (2017) outperforming the Logistic Regressionbased system of Waseem and Hovy (2016). Notably, Badjatiya et al. (2017) claimed outstanding results for a hybrid system combining an LSTM with a GBDT. However, other researchers have failed to reproduce the experiments by Badjatiya et al., with Fortuna et al. (2019) indicating that Badjatiya et al.’s stated results rather were due to a faulty cross-validation process and with Mishra et al. (2018) noting that Badjatiya et al.’s decision tree-boosted version was tested on instances that the LSTM already had been trained on, leading to over-fitting. (2016) and Pavlopoulos et al. (2017), used only one of the types. Each kind of feature has its own advantage. The character n-gram approach is relatively resilient against misspellings, while word embeddings allow related words to produce similar output. In Mehdad and Tetreault (2016), word and character n-grams"
W19-3516,C18-1093,0,0.538744,"to convey hateful messages. This is often in breach of the given arena’s terms and conditions, and sometimes, in some countries, illegal. Hence, there is a need for automatic detection of these messages across a multitude of online arenas, but without depending on any information specific to a given forum, so that the systems can be used across platforms without being changed. Notably, information about the text’s author, such as their usage history or their social network and activities, have been shown to be useful when categorising hate speech (Qian et al., 2018; Unsvåg and Gambäck, 2018; Mishra et al., 2018). 2 Related Work Research on hate speech detection has attempted many kinds of input features, and many different classification methods. In the early research, the input types used were highly language dependent, utilising specific syntax features and the presence of certain words. Later, these kinds of features were exchanged for more general text representations. Specifically, the approaches got more directed towards word- and character models, and in various alternations. Some researchers, such as Gambäck and Sikdar (2017), used both types at the same time, while others, e.g., Waseem and H"
W19-3516,W17-3006,0,0.42434,"f the text. However, as the aim of this paper is to achieve classification more independent of the origin platform of the texts, such platform-dependent systems will largely be disregarded here. Early research used traditional machine learning approaches, e.g., Support Vector Machines (SVMs) (Yin et al., 2009) and Naïve Bayes-based classifiers (Razavi et al., 2010). Some more recent research has also used traditional machine learning approaches, such as Logistic Regression (Waseem and Hovy, 2016). However, most recent work has focused on Deep Learning approaches: Gambäck and Sikdar (2017) and Park and Fung (2017) used Convolutional Neural Networks (CNNs), while Pavlopoulos et al. (2017) used Recurrent Neural Networks (RNNs) with Gated Recurrent Units (GRUs). Others have combined neural network-types, with Zhang et al. (2018) utilising a CNN followed by a GRU-based RNN, and Founta et al. (2018a) a two-part approach, with one part using word embeddings fed into an RNN-layer consisting of GRU-nodes, and the other, parallel part taking metadata as input to a feed-forward network. 3 Data Set The largest data set used in research on inappropriate language is the one in Pavlopoulos et al. (2017), with 1.6 mi"
W19-3516,W17-3004,0,0.509113,"tperforming the Logistic Regressionbased system of Waseem and Hovy (2016). Notably, Badjatiya et al. (2017) claimed outstanding results for a hybrid system combining an LSTM with a GBDT. However, other researchers have failed to reproduce the experiments by Badjatiya et al., with Fortuna et al. (2019) indicating that Badjatiya et al.’s stated results rather were due to a faulty cross-validation process and with Mishra et al. (2018) noting that Badjatiya et al.’s decision tree-boosted version was tested on instances that the LSTM already had been trained on, leading to over-fitting. (2016) and Pavlopoulos et al. (2017), used only one of the types. Each kind of feature has its own advantage. The character n-gram approach is relatively resilient against misspellings, while word embeddings allow related words to produce similar output. In Mehdad and Tetreault (2016), word and character n-grams were used separately, in order to compare their performance, showing character n-grams to be more effective. Some systems, like that of Founta et al. (2018a), also apply various metadata and information about the author of the text. However, as the aim of this paper is to achieve classification more independent of the or"
W19-3516,W17-3013,1,0.755884,"n categorising hate speech (Qian et al., 2018; Unsvåg and Gambäck, 2018; Mishra et al., 2018). 2 Related Work Research on hate speech detection has attempted many kinds of input features, and many different classification methods. In the early research, the input types used were highly language dependent, utilising specific syntax features and the presence of certain words. Later, these kinds of features were exchanged for more general text representations. Specifically, the approaches got more directed towards word- and character models, and in various alternations. Some researchers, such as Gambäck and Sikdar (2017), used both types at the same time, while others, e.g., Waseem and Hovy 146 Proceedings of the Third Workshop on Abusive Language Online, pages 146–156 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Several hate speech detection systems have been tested on the data set from Waseem and Hovy (2016) and can thus be compared more directly. Although the SVM-Naïve Bayes classifier of Mehdad and Tetreault (2016) outperformed their RNN-based system, deep learners seem to in general perform better than purely traditional machine learning classifiers on this dataset, w"
W19-3516,D14-1162,0,0.0821282,"reating the input as character n-grams. The output of this convolution is sorted by locations in the input, so that results of different filters at any given location appear together. This way, the results of the convolutions imitate the time steps of an LSTM. The architecture allows for several layers of convolution. The word embeddings used here are pretrained on external data sets, so as to avoid an additional source of overfitting due to the relatively small size of the Waseem and Hovy (2016) data set. Two different kinds of embeddings were used, word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). The word2vec-embeddings have a dimensionality of 300 and were trained on about 100 billion words from the Google News data.3 The GloVe-embeddings on the other hand, were trained on Twitter, using 2 billion tweets.4 The highest available dimensionality, 200, was used. Out of Vocabulary words were given a random value of corresponding dimensionality. 4.2 In the second part of the component, the results of the convolution are input to an LSTM. The sample lengths of the LSTM are calculated from the output of the convolutions and used to extract the component output. Word Input Component 4.4 The"
W19-3516,I17-1078,0,0.0125695,"language, while the second is not straight-forwardly available. Furthermore, although the data set of Founta et al. (2018b) is substantially larger, the older one by Waseem and Hovy (2016) has been used in more previous research, and was thus taken as the basis here, too, for reasons of easier comparison to previous results. Others have tried combining deep learners with more traditional methods: Badjatiya et al. (2017) tested both a CNN-based and a Long Short-Term Memory (LSTM)-based system (Hochreiter and Schmidhuber, 1997), in combination with Gradient Boosted Decision Trees (GBDT), while Gao et al. (2017) also used an LSTM, but running in parallel with logistic regression. In the SemEval 2019 OffensEval shared task (Zampieri et al., 2019b), the best performing systems utilised pretrained contextual embeddings such as BERT (Bidirectional Encoder Representations from Transformers; Devlin et al. 2018) and ELMo (Embeddings from Language Model; Peters et al. 2018), in essence focusing on word-level n-grams (or word pieces as defined in BERT). 147 Version Neutral Racist Sexist Total Original Available 11,559 10,913 1,972 1,924 3,383 3,097 16,914 15,934 Table 1: Size of the Waseem and Hovy (2016) dat"
W19-3516,N18-1202,0,0.0539844,"p learners with more traditional methods: Badjatiya et al. (2017) tested both a CNN-based and a Long Short-Term Memory (LSTM)-based system (Hochreiter and Schmidhuber, 1997), in combination with Gradient Boosted Decision Trees (GBDT), while Gao et al. (2017) also used an LSTM, but running in parallel with logistic regression. In the SemEval 2019 OffensEval shared task (Zampieri et al., 2019b), the best performing systems utilised pretrained contextual embeddings such as BERT (Bidirectional Encoder Representations from Transformers; Devlin et al. 2018) and ELMo (Embeddings from Language Model; Peters et al. 2018), in essence focusing on word-level n-grams (or word pieces as defined in BERT). 147 Version Neutral Racist Sexist Total Original Available 11,559 10,913 1,972 1,924 3,383 3,097 16,914 15,934 Table 1: Size of the Waseem and Hovy (2016) data set Figure 1: High-level architecture of the system The data set of Waseem and Hovy (2016) originally contained 16,914 tweets labelled for racism and sexism. However, 980 of these tweets had been deleted by the time the data were collected, leaving 15,934 samples. As Table 1 shows, most of the deleted tweets were from the neutral group. As this is the large"
W19-3516,N19-1144,0,0.0303414,"he Greek sports site Gazzetta. However, the labels in this data set are based on which comments the site’s moderators found to be inappropriate in some way, including, but not restricted to, hate speech. The Twitter data set from Davidson et al. (2017) is also reasonably large and could have been an interesting option, but also somewhat lacks justifications for how each sample has been labelled: Davidson et al. attempted to differentiate between hate speech and other offensive content, but relied heavily on the crowd-sourced (CrowdFlower) annotators to make the distinction. On the other hand, Zampieri et al. (2019a), Golbeck et al. (2017), Founta et al. (2018b), and Waseem and Hovy (2016), all used extensive sets of rules when labelling their data. However, the first of those is aimed at offensive language, while the second is not straight-forwardly available. Furthermore, although the data set of Founta et al. (2018b) is substantially larger, the older one by Waseem and Hovy (2016) has been used in more previous research, and was thus taken as the basis here, too, for reasons of easier comparison to previous results. Others have tried combining deep learners with more traditional methods: Badjatiya et"
W19-3516,W18-5110,1,0.852031,"lly insult others, or even to convey hateful messages. This is often in breach of the given arena’s terms and conditions, and sometimes, in some countries, illegal. Hence, there is a need for automatic detection of these messages across a multitude of online arenas, but without depending on any information specific to a given forum, so that the systems can be used across platforms without being changed. Notably, information about the text’s author, such as their usage history or their social network and activities, have been shown to be useful when categorising hate speech (Qian et al., 2018; Unsvåg and Gambäck, 2018; Mishra et al., 2018). 2 Related Work Research on hate speech detection has attempted many kinds of input features, and many different classification methods. In the early research, the input types used were highly language dependent, utilising specific syntax features and the presence of certain words. Later, these kinds of features were exchanged for more general text representations. Specifically, the approaches got more directed towards word- and character models, and in various alternations. Some researchers, such as Gambäck and Sikdar (2017), used both types at the same time, while othe"
W19-3516,W16-5618,0,0.644805,"sed for character input, and so the word-based strand is likely to have a significantly higher convergence rate than the character-based one. Such a discrepancy in convergence rates may cause one of the strands to dominate the other, hampering the training and resulting in an overall suboptimal performance. This issue may also have affected the experiments on variations in layer sizes and number of layers, as changes in the size of a system component will change its rate of convergence. These variations would work to the advantage of some System P R F1 64×4, 64×3, GloVe Waseem and Hovy (2016) Waseem (2016), multiclass Waseem (2016), binary Gambäck and Sikdar (2017) Fortuna et al. (2019) 80.22 72.87 — — 85.66 — 78.28 77.75 — — 72.14 — 79.24 73.89 53.43 70.05 78.29 78 64×4, 64×3, GloVe Zhang et al. (2018) Park and Fung (2017) Founta et al. (2018a) Badjatiya et al. (2017) Mishra et al. (2018) (WS) Mishra et al. (2018) (LR) Mishra et al. (2018) (HS) 84.14 — 82.7 84 83.9 82.86 84.07 83.50 84.14 — 82.7 83 84.0 83.10 84.31 83.71 84.14 82 82.7 83 83.9 82.37 83.81 83.54 Table 6: System performance comparison configurations and the disadvantage of others. The results indicate that this may be the case. H"
W19-3516,S19-2010,0,0.0373621,"he Greek sports site Gazzetta. However, the labels in this data set are based on which comments the site’s moderators found to be inappropriate in some way, including, but not restricted to, hate speech. The Twitter data set from Davidson et al. (2017) is also reasonably large and could have been an interesting option, but also somewhat lacks justifications for how each sample has been labelled: Davidson et al. attempted to differentiate between hate speech and other offensive content, but relied heavily on the crowd-sourced (CrowdFlower) annotators to make the distinction. On the other hand, Zampieri et al. (2019a), Golbeck et al. (2017), Founta et al. (2018b), and Waseem and Hovy (2016), all used extensive sets of rules when labelling their data. However, the first of those is aimed at offensive language, while the second is not straight-forwardly available. Furthermore, although the data set of Founta et al. (2018b) is substantially larger, the older one by Waseem and Hovy (2016) has been used in more previous research, and was thus taken as the basis here, too, for reasons of easier comparison to previous results. Others have tried combining deep learners with more traditional methods: Badjatiya et"
W19-3516,N16-2013,0,0.0873114,"atures and the presence of certain words. Later, these kinds of features were exchanged for more general text representations. Specifically, the approaches got more directed towards word- and character models, and in various alternations. Some researchers, such as Gambäck and Sikdar (2017), used both types at the same time, while others, e.g., Waseem and Hovy 146 Proceedings of the Third Workshop on Abusive Language Online, pages 146–156 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Several hate speech detection systems have been tested on the data set from Waseem and Hovy (2016) and can thus be compared more directly. Although the SVM-Naïve Bayes classifier of Mehdad and Tetreault (2016) outperformed their RNN-based system, deep learners seem to in general perform better than purely traditional machine learning classifiers on this dataset, with the CNN-based system of Gambäck and Sikdar (2017) outperforming the Logistic Regressionbased system of Waseem and Hovy (2016). Notably, Badjatiya et al. (2017) claimed outstanding results for a hybrid system combining an LSTM with a GBDT. However, other researchers have failed to reproduce the experiments by Badjatiya et al.,"
W93-0406,W93-0405,0,0.0423358,"Missing"
W93-0406,A92-1018,0,0.0860547,"Missing"
W93-0406,C90-3038,0,0.0347617,"Missing"
W93-0406,C88-2111,0,0.0203121,"Missing"
W93-0406,W93-0420,0,0.0865367,"Missing"
W93-0408,P92-1005,0,0.0300018,"tered up from the verb-affix to the verb phrase. The arguments of v e r b will be explained further on; first, however, we should note that the choice of this functor is rather (but not completely) arbitrary. For a language such as Finnish where the aspect information is carried on the object rather than the predicate some other functor name of course should be chosen. Also, the number of arguments and their interpretation could certainly vary between languages (or between linguists and linguistic theories treating the same language), but in general we need a strategy as the one suggested by (Alshawi & Crouch, 1992): first a way to packet the tense-aspect information declaratively in the compositional semantics and then a way to unpack this information later on to determine the implicit points in time, etc., not shown in the surface form of the sentence. This “packaging” is the main function of the functor v e rb , whose arguments are in order: Taise the relation of the event to the present time of the speaker: past, present, or future. Aspect the relation of the event to the action time of the verb: perfective or imperfective. Acticn the way in which an event happens: progressive or non-progressive. lyb"
W93-0408,P91-1021,1,0.833109,"pe with declarative bidirectional rules, that is, the grammar can be used both for language analysis and for generation (it is just compiled in different ways depending on in what direction it is to be used) A natural-language sentence that is input to the S-CLE is analysed to a logical-form like representation called QLF, Quasi-Logical Form, a conservative representation of the meaning of an input sentence based on purely linguistic evidence. The English and Swedish versions of the CLE have been used together to form a bidirectional translation system, transfer taking place at the QLF level (Alshawi et al, 1991), but the QLF can also be used as the basis for further (deeper-level) processing. Deriving a QLF from an NL-sentence involves the processing steps shown in Figure 1. NL Morphology —&gt; Syntax —&gt; Semantics —&gt; QLF Figure 1: The analysis steps of the S-CLE First morphological analysis locate the correct word-senses and inflected forms of the input string, then syntactic parsing and (compositional) semantic analysis derive the parse tree(s) and its corresponding QLFrepresentation. Later processing steps (e.g., reference resolution and quantifier scoping) will try to further instantiate the QLF, aim"
W93-0412,A92-1023,0,0.0157728,"the sentence space, we distributed twelve randomly chosen sentences from a corpus of 4021 spoken English sentences to 1100 Swedish computer scientists. We 144 received 73 answers. The translations were inspected by a professional Swedish translator, and all but a few were considered quite acceptable in a situation corresponding to the one in which they were given. The sentences distributed are shown in table 1 below. They were all in the air traffic information domain, or ATIS, the corpus used by the US government to evaluate the performance of different spoken language understanding systems (Boisen & Bates, 1992). T a b le 1: Sentences distributed 1 2 3 4 5 6 7 8 9 10 11 12 Atlanta to Oakland Thursday. Give me flights from Denver to Baltimore. Which companies fly between Boston and Oakland. Show me all flights from Pittsburgh to Dallas. Show me the names of airlines in Atlanta. What&apos;s the cheapest flight from Atlanta to Baltimore. I want to fly from Baltimore to Dallas round trip. Show all flights and fares from Denver to San Francisco. List round trip flights between Boston and Oakland using T WA . What are the flights from Dallas to Boston for the next day. And the ground what is the ground transpor"
W93-0412,C92-2067,0,0.012401,"logical evaluation is a test suite, a set of sentences which individually represent specified constructions and hence constitute performance probes. Most work on MT-system evaluation has been concerned with how such a test-suite should be composed, e.g. (King & Falkedal, 1990, and Gamback et al, 1991a, 1991b); however, the methods outlined in this paper follow the declarative evaluation track. Previous methods along this path have normally been “hand-crafted”, or based on existing (labour-intensive) methods for the evaluation of human translators’ work (Balkan, 1991). Both Thompson (1991) and Su et al (1992) have independently worked on automating the process. They present methods for evaluating translation quality based on statistical measurements of a candidate translation against a standard set using simple string-matching algorithms, i.e., ideas quite akin to the ones below. The rest of the paper is outlined as follows: in the section following we describe an experiment with obtaining a set of translated sentences from a large group of informants. In section 3 we discuss what conclusions can be drawn from the experiment, the key questions being what the structure of the sentence set is and if"
W93-0412,E93-1046,0,0.0537159,"Missing"
Y96-1006,P92-1005,0,0.0238718,"lexical ambiguity. Another example is the referential ambiguity introduced by the anaphoric or deictic uses of pronouns. The second level of underspecification refers to global ambiguities. The primary example is scopal ambiguities. In addition, for example ambiguities introduced by collective-distributive readings belong to this group. These phenomena are the ones that most of the present work in the area aim at treating, and that mainly at the representational level. Several representations for this type of underspecification have thus been introduced, for example, Quasi Logical Form, QLF (Alshawi and Crouch, 1992) and Underspecified Discourse Representation Theory, UDRT (Reyle, 1993). Our representation for tackling this type of phenomena, Language for Underspecified Discourse representations, LUD, was introduced in (Bos et al., 1996). In the present paper, we will only give a brief account of it (Section 4). Finally, (Pinkal, 1995) discusses a third level of underspecified semantic phenomena. These are the ones caused by ambiguous syntactic information (e.g., PP-attachment), 54 or even by incoherent non-semantic information. An example is number disagreement introducing a range of different possible i"
Y96-1006,C92-1017,0,0.0387682,"Missing"
Y96-1006,C96-2133,1,0.865108,"Missing"
Y96-1006,1993.mtsummit-1.11,0,0.0628441,"Missing"
Y96-1006,C96-1024,1,\N,Missing
Y96-1006,P98-1072,1,\N,Missing
Y96-1006,C98-1069,1,\N,Missing
