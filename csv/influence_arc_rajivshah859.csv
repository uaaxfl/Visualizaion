2020.aacl-main.71,D15-1075,0,0.070922,"ontradictory) Table 1: Example illustrating context (c) - hypothesis (h) pairs for the task of textual entailment. Textual entailment (TE) is the task of determining if a hypothesis sentence can be inferred from a given context sentence. Figure 1 shows examples of context-hypothesis pairs for TE. Previous works (Wang and Zhang, 2009; Tatu and Moldovan, 2005; Sammons et al., 2010) investigated several semantic approaches for TE and demonstrated how they can be used to evaluate inference-related tasks such as QuesResearchers have curated many resources3 and benchmark datasets for TE in English (Bowman et al., 2015; Williams et al., 2018; Khot et al., 2018). However, to our knowledge, there is only one TE dataset (XNLI) in Hindi, which was created by translating English data (Conneau et al., 2018) and another in HindiEnglish code-switched setting (Khanuja et al., 2020). Hindi is the language with the fourth most native speakers in the world4 . Despite its wide prevalence, Hindi is still considered a low-resource language by NLP practitioners because there are a rather limited number of publicly available annotated datasets. Developing models that can accurately process text from low-resource languages,"
2020.aacl-main.71,P19-4007,0,0.172184,"for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 706–719 c December 4 - 7, 2020. 2020 Association for Computational Linguistics tated sentence with each of the template hypotheses to create TE samples. Unlike XNLI, our dataset is based on the original Hindi text and is not translated. Furthermore, the multiple annotation artefacts (Tan et al., 2019) present in the original classification data are leveled out for the Textual entailment task on the recasted data due to label balance 5 . We evaluated state-of-the-art language models (Conneau et al., 2019) performance on the recasted TE data. We then combine the predictions of related pairs (same premise) from TE task to predict the classification labels of the original data (premise sentence), a twostep classification. We observed that a better TE performance on the recasted data leads to higher accuracy on the followed classification task. We also observed that TE models can make inconsistent predictions across samples derived from the same context sentence. Driven by these observations, we propose two improvements to TE and classification modeling. First, we introduce a regularisation constr"
2020.aacl-main.71,D17-1070,0,0.0302848,"tive not only ensures that the model predictions are correct but also ensures that they are correct for the right reasons. The true classification label can be retrieved from the entailment vector only when the model draws necessary inferences correctly. Otherwise the multi-class classification would fail. Furthermore, combining the joint objective (Equation 2) with consistency regulariser (Equation 1) for the intermediate TE prediction further force pairwise-consistency between prediction of related TE pairs. Baselines - For evaluating our approach, we use the following baselines: InferSent (Conneau et al., 2017), Sent2Vec (Pagliardini et al., 2018), Bag-of-words (BoW) and XLMRoBERTa (Conneau et al., 2019) which is state-of-the-art for multilingual language modelling. Also, we evaluate a hypothesis-only analogue for each one of them as well. For experiments with recasted data, we use embeddings of context-hypothesis pair for baselines whereas for the hypothesis-only (Poliak et al., 2018b) models, we only use embeddings of the hypothesis sentence, keeping it blind to the context. 711 Hypothesis only Baselines - Evaluating hypothesis-only models is motivated by irregularities and biases presented in ent"
2020.aacl-main.71,2020.calcs-1.2,0,0.0221868,"ples of context-hypothesis pairs for TE. Previous works (Wang and Zhang, 2009; Tatu and Moldovan, 2005; Sammons et al., 2010) investigated several semantic approaches for TE and demonstrated how they can be used to evaluate inference-related tasks such as QuesResearchers have curated many resources3 and benchmark datasets for TE in English (Bowman et al., 2015; Williams et al., 2018; Khot et al., 2018). However, to our knowledge, there is only one TE dataset (XNLI) in Hindi, which was created by translating English data (Conneau et al., 2018) and another in HindiEnglish code-switched setting (Khanuja et al., 2020). Hindi is the language with the fourth most native speakers in the world4 . Despite its wide prevalence, Hindi is still considered a low-resource language by NLP practitioners because there are a rather limited number of publicly available annotated datasets. Developing models that can accurately process text from low-resource languages, such as Hindi, is critical for the proliferation and broader adoption of NLP technologies. Creating a high-quality labeled corpus for TE in Hindi through crowd-sourcing could be challenging. In this paper, we employ a recasting technique from Poliak et al. (2"
2020.aacl-main.71,W18-3504,1,0.827641,"or resides in spoken dialect than texts. There have been recent efforts using curriculum learning for making pretrained language models for several multi-lingual tasks (Conneau et al., 2018, 2019). However, many such languages give rise to creoles, building new mixed languages at the interface of existing languages. One such example is Hinglish (Hindi + English) that has widely been taken over in the form of tweets and social media messages. Attempts have been made to study linguistic tasks like language identification, NER (Singh et al., 2018) and detection of hate speech from social media (Mathur et al., 2018). (Sitaram et al., 2019) looks at the challenges and opportunities of code-switching. Joshi et al. (2019) compares the current deep learning methods for classification tasks in Hindi and concludes the need of more efficient models for the same. Apart from that, low-resource languages also challenge us to shift from data-driven modelling to intelligent neural modelling. This improves language understanding from limited available data and also diminishes the need of hand-engineered feature representations similar to generative modelling. Some such efforts have been put forth by Kumar et al. (201"
2020.aacl-main.71,2020.lrec-1.149,1,0.72254,"marked as ‘non-entailed’. This process is summarized with an example in Figure 1. For more detailed recasting illustration, see Appendix Section A.1 Figure 5. BHAAV - The second dataset BHAAV (BH ) (Kumar et al., 2019) contains 20,304 sentences from Hindi short stories annotated for one of the following five emotion categories: joy, anger, suspense, sad, and neutral. We used a similar process as PR to recast BH using the following templates to create the hypothesis: ‘It is a matter of great &lt;label&gt;’ and ‘It is not a matter of great &lt;label&gt;’. Hindi Discourse Modes Dataset (HDA) - This dataset (Dhanwal et al., 2020) consists of 10, 472 sentences from Hindi short stories annotated for five different discourse modes argumentative, narrative, descriptive, dialogic and informative. Recasting Classification Datasets One of the main challenges for TE evaluation for low-resource languages is the lack of labeled data. In this work, we employ recasting to convert annotated classification datasets in Hindi to labeled TE samples. As in (Poliak et al., 2018a), we selected four different datasets for recasting thus introducing linguistic diversity in the resulting TE dataset. Product Review - The first dataset (PR) c"
2020.aacl-main.71,N18-1049,0,0.0150654,"del predictions are correct but also ensures that they are correct for the right reasons. The true classification label can be retrieved from the entailment vector only when the model draws necessary inferences correctly. Otherwise the multi-class classification would fail. Furthermore, combining the joint objective (Equation 2) with consistency regulariser (Equation 1) for the intermediate TE prediction further force pairwise-consistency between prediction of related TE pairs. Baselines - For evaluating our approach, we use the following baselines: InferSent (Conneau et al., 2017), Sent2Vec (Pagliardini et al., 2018), Bag-of-words (BoW) and XLMRoBERTa (Conneau et al., 2019) which is state-of-the-art for multilingual language modelling. Also, we evaluate a hypothesis-only analogue for each one of them as well. For experiments with recasted data, we use embeddings of context-hypothesis pair for baselines whereas for the hypothesis-only (Poliak et al., 2018b) models, we only use embeddings of the hypothesis sentence, keeping it blind to the context. 711 Hypothesis only Baselines - Evaluating hypothesis-only models is motivated by irregularities and biases presented in entailment Context (Hindi): वह रोया जब उ"
2020.aacl-main.71,2020.acl-main.560,0,0.0207709,"revolved around English language. Our approach is motivated by such studies to analyse NLU using current embeddings for low-resource languages like Hindi. Bhattacharyya (2012) discusses some of the key challenges associated with Hindi, for example, grammatical constraints for most words to be masculine/feminine (similar to French and unlike English), which makes 707 semantic tasks like pronoun resolution, paraphrasing tough. 2.2 NLP for Low-Resource Languages In a plethora of diverse languages, only a handful of them have plenty of labeled resources for data-driven analysis and advancements (Joshi et al., 2020). Data in low-resource languages is either unlabeled or resides in spoken dialect than texts. There have been recent efforts using curriculum learning for making pretrained language models for several multi-lingual tasks (Conneau et al., 2018, 2019). However, many such languages give rise to creoles, building new mixed languages at the interface of existing languages. One such example is Hinglish (Hindi + English) that has widely been taken over in the form of tweets and social media messages. Attempts have been made to study linguistic tasks like language identification, NER (Singh et al., 20"
2020.aacl-main.71,H05-1047,0,0.0616048,"together. We therefore highlight the benefits of data recasting and our approach 2 with supporting experimental results. Context-Hypothesis p : The kid exclaimed with joy. h : The kid is happy. p : I am feeling happy. h : I am angry. Label entailed not-entailed (contradictory) Table 1: Example illustrating context (c) - hypothesis (h) pairs for the task of textual entailment. Textual entailment (TE) is the task of determining if a hypothesis sentence can be inferred from a given context sentence. Figure 1 shows examples of context-hypothesis pairs for TE. Previous works (Wang and Zhang, 2009; Tatu and Moldovan, 2005; Sammons et al., 2010) investigated several semantic approaches for TE and demonstrated how they can be used to evaluate inference-related tasks such as QuesResearchers have curated many resources3 and benchmark datasets for TE in English (Bowman et al., 2015; Williams et al., 2018; Khot et al., 2018). However, to our knowledge, there is only one TE dataset (XNLI) in Hindi, which was created by translating English data (Conneau et al., 2018) and another in HindiEnglish code-switched setting (Khanuja et al., 2020). Hindi is the language with the fourth most native speakers in the world4 . Desp"
2020.aacl-main.71,D09-1082,0,0.0387386,"tual entailment tasks together. We therefore highlight the benefits of data recasting and our approach 2 with supporting experimental results. Context-Hypothesis p : The kid exclaimed with joy. h : The kid is happy. p : I am feeling happy. h : I am angry. Label entailed not-entailed (contradictory) Table 1: Example illustrating context (c) - hypothesis (h) pairs for the task of textual entailment. Textual entailment (TE) is the task of determining if a hypothesis sentence can be inferred from a given context sentence. Figure 1 shows examples of context-hypothesis pairs for TE. Previous works (Wang and Zhang, 2009; Tatu and Moldovan, 2005; Sammons et al., 2010) investigated several semantic approaches for TE and demonstrated how they can be used to evaluate inference-related tasks such as QuesResearchers have curated many resources3 and benchmark datasets for TE in English (Bowman et al., 2015; Williams et al., 2018; Khot et al., 2018). However, to our knowledge, there is only one TE dataset (XNLI) in Hindi, which was created by translating English data (Conneau et al., 2018) and another in HindiEnglish code-switched setting (Khanuja et al., 2020). Hindi is the language with the fourth most native spea"
2020.aacl-main.71,I17-1100,0,0.0509973,"Missing"
2020.aacl-main.71,N18-1101,0,0.0257341,": Example illustrating context (c) - hypothesis (h) pairs for the task of textual entailment. Textual entailment (TE) is the task of determining if a hypothesis sentence can be inferred from a given context sentence. Figure 1 shows examples of context-hypothesis pairs for TE. Previous works (Wang and Zhang, 2009; Tatu and Moldovan, 2005; Sammons et al., 2010) investigated several semantic approaches for TE and demonstrated how they can be used to evaluate inference-related tasks such as QuesResearchers have curated many resources3 and benchmark datasets for TE in English (Bowman et al., 2015; Williams et al., 2018; Khot et al., 2018). However, to our knowledge, there is only one TE dataset (XNLI) in Hindi, which was created by translating English data (Conneau et al., 2018) and another in HindiEnglish code-switched setting (Khanuja et al., 2020). Hindi is the language with the fourth most native speakers in the world4 . Despite its wide prevalence, Hindi is still considered a low-resource language by NLP practitioners because there are a rather limited number of publicly available annotated datasets. Developing models that can accurately process text from low-resource languages, such as Hindi, is criti"
2020.acl-srw.3,P18-1198,0,0.0124705,"bedding should have a core distance of 0. Hence if the utterance satisfies top1 probability ≥ α then it is classified as a correct word. We use α = 0.75 in our experiments. Now, if a word utterance is phonemic paraphasia, HDBSCAN returns near similar cluster membership probabilities for 2 to 3 clusters (eg. lat will be clustered close to correct words bat, late etc.) Probing Tasks Unsupervised word embeddings can be improved further and geared specifically for aphasic speech, but in order to understand what these embeddings are capturing it is important to probe them. Taking inspiration from (Conneau et al., 2018), we create probing tasks specifically for paraphasia. Probing tasks are simple classification tasks for embeddings. We detail three probing tasks specifically for phonemic and neologistic paraphasia. 1. Phoneme-Movement: Phonemic paraphasia is often characterized with phoneme movement, usually involving a shift in the position of one or two phonemes. In this binary classification task, the embeddings are used to determine if a phoneme shift took place or not. 2. Phoneme-Add/Delete: The addition or deletion of a phoneme is seen in phonemic paraphasia. We use the generated embeddings to determi"
2020.coling-main.426,L18-1659,0,0.370345,"nts of political representatives over critical societal factors (Abercrombie and Batista-Navarro, 2020b), and also for assessing political candidates and basing voting decisions (Utych, 2019). Analyzing sentiment in such political discourse is propelled by the developments in fields like behavioral economics that bring the psychological aspects of parliamentary decision-making to the forefront (Rheault, 2016). However, under Parliament’s Rules of Behavior,1 the language used in political debates is complex, laden with domain-specific procedural jargon used in the political realm, and obscure (Abercrombie and Batista-Navarro, 2018a). This esoteric and tedious nature of political debates makes their analysis complex, forming a barrier to ordinary citizen’s insights into political stances and wide-ranging consequences they entail (Edelman, 1985). The good news is that natural language processing (NLP) shows promise for analyzing voluminous political debates and breaking the understanding barrier towards political ideology to help make informed voting decisions (Davoodi et al., 2020; Eidelman et al., 2018). However, conventional language models (Hasan and Ng, 2013) may not generalize well on understanding the obscure ling"
2020.coling-main.426,W18-6241,0,0.307693,"nts of political representatives over critical societal factors (Abercrombie and Batista-Navarro, 2020b), and also for assessing political candidates and basing voting decisions (Utych, 2019). Analyzing sentiment in such political discourse is propelled by the developments in fields like behavioral economics that bring the psychological aspects of parliamentary decision-making to the forefront (Rheault, 2016). However, under Parliament’s Rules of Behavior,1 the language used in political debates is complex, laden with domain-specific procedural jargon used in the political realm, and obscure (Abercrombie and Batista-Navarro, 2018a). This esoteric and tedious nature of political debates makes their analysis complex, forming a barrier to ordinary citizen’s insights into political stances and wide-ranging consequences they entail (Edelman, 1985). The good news is that natural language processing (NLP) shows promise for analyzing voluminous political debates and breaking the understanding barrier towards political ideology to help make informed voting decisions (Davoodi et al., 2020; Eidelman et al., 2018). However, conventional language models (Hasan and Ng, 2013) may not generalize well on understanding the obscure ling"
2020.coling-main.426,2020.lrec-1.624,0,0.334629,"associated with the governance of a country or a region. It involves various aspects that influence critical decisions having national importance. One such aspect is the conduct of parliamentary debates between political parties having ruling and opposition power. These debates discuss matters affecting the future development of a nation, such as economic and societal growth, policy reforms, and budget revisions. Records of such debates act as a valuable language resource as they provide a wealth of information regarding viewpoints of political representatives over critical societal factors (Abercrombie and Batista-Navarro, 2020b), and also for assessing political candidates and basing voting decisions (Utych, 2019). Analyzing sentiment in such political discourse is propelled by the developments in fields like behavioral economics that bring the psychological aspects of parliamentary decision-making to the forefront (Rheault, 2016). However, under Parliament’s Rules of Behavior,1 the language used in political debates is complex, laden with domain-specific procedural jargon used in the political realm, and obscure (Abercrombie and Batista-Navarro, 2018a). This esoteric and tedious nature of political debates makes t"
2020.coling-main.426,K19-1024,0,0.0140242,"2019). Political Stance and Sentiment Analysis NLP has seen a growth in analyzing and mining opinions from political discourse (Cabrio and Villata, 2018; Rheault, 2016; Fiˇser et al., 2020). Word embeddings have shown remarkable progress in analyzing political debates and text (Onyimadu et al., 2013; Abercrombie and Batista-Navarro, 2018a; Rheault and Cochrane, 2020). (Rudkowsky et al., 2018) conducted a textual analysis of Austrian parliamentary speeches, demonstrating the effectiveness of using word embeddings instead of conventional approaches such as Bag-of-Words. More recent approaches (Abercrombie et al., 2019; Abercrombie and Batista-Navarro, 2020a) show the ability of pre-trained transformers such as BERT in capturing domain-specific jargon better for feature extraction from debates. A promising new direction at the intersection of Politics and NLP is the inclusion of context such as political party affiliations and engagement in social circles. (Boutyline and Willer, 2017; Lai et al., 2019) study the linguistic patterns of politically engaged users on Twitter by inferring a user’s political inclination through the politicians and policy nonprofits they follow. They show that more conservative us"
2020.coling-main.426,2020.acl-main.632,0,0.0862526,"Missing"
2020.coling-main.426,W19-1909,0,0.0225039,"Missing"
2020.coling-main.426,P19-2039,1,0.933927,"umanities, and computational linguistics (Vilares and He, 2017; Sim et al., 2013; Slembrouck, 1992). A developing body of research at the intersection of Politics and Linguistics spans agreement detection (Menini and Tonelli, 2016; M. and M., 2018; Duthie and Budzynska, 2018), emotion analysis (Rheault, 2016; Dzieciatko, 2019), topic-opinion analysis (Nguyen et al., 2015; Abercrombie and Batista-Navarro, 2018b) and, debate stance classification (Proksch et al., 2019). Existing work focuses on these tasks through legislative speeches from the US Congress (Chen et al., 2017), the UK Parliament (Bhavan et al., 2019), and the EU Parliament (Glavaˇs et al., 2017; Frid-Nielsen, 2018) and through social media such as Twitter (Trilling, 2014; Boutyline and Willer, 2017). Recently, some tools for extracting and annotating political data have also been developed (Haddadan et al., 2019). Political Stance and Sentiment Analysis NLP has seen a growth in analyzing and mining opinions from political discourse (Cabrio and Villata, 2018; Rheault, 2016; Fiˇser et al., 2020). Word embeddings have shown remarkable progress in analyzing political debates and text (Onyimadu et al., 2013; Abercrombie and Batista-Navarro, 20"
2020.coling-main.426,Q17-1010,0,0.012043,"t natural language processing (NLP) shows promise for analyzing voluminous political debates and breaking the understanding barrier towards political ideology to help make informed voting decisions (Davoodi et al., 2020; Eidelman et al., 2018). However, conventional language models (Hasan and Ng, 2013) may not generalize well on understanding the obscure linguistic styles of political debates. This complexity arises due to the lack of political context and procedural parliamentary jargon in generic corpora over which traditional text representation models are trained (Pennington et al., 2014; Bojanowski et al., 2017). For instance, in the political context, Red State is a state that primarily votes for Republicans, whereas a Blue State votes primarily for Democrats. Leveraging the success of unsupervised pre-training in NLP (Devlin et al., 2019; Liu et al., 2019), fine-tuning pre-trained models over the voluminous debate transcripts can lead to drastic advances in analyzing political debates. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 https://www.parliament.uk/documents/rules-of-behaviour.pdf License details: http:// 4847 P"
2020.coling-main.426,2020.acl-main.476,0,0.128526,"used in political debates is complex, laden with domain-specific procedural jargon used in the political realm, and obscure (Abercrombie and Batista-Navarro, 2018a). This esoteric and tedious nature of political debates makes their analysis complex, forming a barrier to ordinary citizen’s insights into political stances and wide-ranging consequences they entail (Edelman, 1985). The good news is that natural language processing (NLP) shows promise for analyzing voluminous political debates and breaking the understanding barrier towards political ideology to help make informed voting decisions (Davoodi et al., 2020; Eidelman et al., 2018). However, conventional language models (Hasan and Ng, 2013) may not generalize well on understanding the obscure linguistic styles of political debates. This complexity arises due to the lack of political context and procedural parliamentary jargon in generic corpora over which traditional text representation models are trained (Pennington et al., 2014; Bojanowski et al., 2017). For instance, in the political context, Red State is a state that primarily votes for Republicans, whereas a Blue State votes primarily for Democrats. Leveraging the success of unsupervised pre"
2020.coling-main.426,N19-1423,0,0.344093,"2018). However, conventional language models (Hasan and Ng, 2013) may not generalize well on understanding the obscure linguistic styles of political debates. This complexity arises due to the lack of political context and procedural parliamentary jargon in generic corpora over which traditional text representation models are trained (Pennington et al., 2014; Bojanowski et al., 2017). For instance, in the political context, Red State is a state that primarily votes for Republicans, whereas a Blue State votes primarily for Democrats. Leveraging the success of unsupervised pre-training in NLP (Devlin et al., 2019; Liu et al., 2019), fine-tuning pre-trained models over the voluminous debate transcripts can lead to drastic advances in analyzing political debates. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 https://www.parliament.uk/documents/rules-of-behaviour.pdf License details: http:// 4847 Proceedings of the 28th International Conference on Computational Linguistics, pages 4847–4859 Barcelona, Spain (Online), December 8-13, 2020 Politicians Speech Transcripts Motions Peter Aldous U1 U2 Conservative Party Intra-Party Co"
2020.coling-main.426,C18-1013,0,0.0213803,"tes is complex, laden with domain-specific procedural jargon used in the political realm, and obscure (Abercrombie and Batista-Navarro, 2018a). This esoteric and tedious nature of political debates makes their analysis complex, forming a barrier to ordinary citizen’s insights into political stances and wide-ranging consequences they entail (Edelman, 1985). The good news is that natural language processing (NLP) shows promise for analyzing voluminous political debates and breaking the understanding barrier towards political ideology to help make informed voting decisions (Davoodi et al., 2020; Eidelman et al., 2018). However, conventional language models (Hasan and Ng, 2013) may not generalize well on understanding the obscure linguistic styles of political debates. This complexity arises due to the lack of political context and procedural parliamentary jargon in generic corpora over which traditional text representation models are trained (Pennington et al., 2014; Bojanowski et al., 2017). For instance, in the political context, Red State is a state that primarily votes for Republicans, whereas a Blue State votes primarily for Democrats. Leveraging the success of unsupervised pre-training in NLP (Devlin"
2020.coling-main.426,E17-2109,0,0.052777,"Missing"
2020.coling-main.426,I13-1191,0,0.0770471,"ed in the political realm, and obscure (Abercrombie and Batista-Navarro, 2018a). This esoteric and tedious nature of political debates makes their analysis complex, forming a barrier to ordinary citizen’s insights into political stances and wide-ranging consequences they entail (Edelman, 1985). The good news is that natural language processing (NLP) shows promise for analyzing voluminous political debates and breaking the understanding barrier towards political ideology to help make informed voting decisions (Davoodi et al., 2020; Eidelman et al., 2018). However, conventional language models (Hasan and Ng, 2013) may not generalize well on understanding the obscure linguistic styles of political debates. This complexity arises due to the lack of political context and procedural parliamentary jargon in generic corpora over which traditional text representation models are trained (Pennington et al., 2014; Bojanowski et al., 2017). For instance, in the political context, Red State is a state that primarily votes for Republicans, whereas a Blue State votes primarily for Democrats. Leveraging the success of unsupervised pre-training in NLP (Devlin et al., 2019; Liu et al., 2019), fine-tuning pre-trained mo"
2020.coling-main.426,C16-1232,0,0.0177292,"anism (Sec. 5.3) and token-level attention (Sec. 5.4) thus providing a use case for GPolS as a tool for parliamentary debate analysis. 4848 2 Related Work Politics and Linguistics Analyzing political data acts as a knowledge source that provides insights into cohesion within political parties, stances of MPs towards critical motions for both the general public, and across domains including humanities, and computational linguistics (Vilares and He, 2017; Sim et al., 2013; Slembrouck, 1992). A developing body of research at the intersection of Politics and Linguistics spans agreement detection (Menini and Tonelli, 2016; M. and M., 2018; Duthie and Budzynska, 2018), emotion analysis (Rheault, 2016; Dzieciatko, 2019), topic-opinion analysis (Nguyen et al., 2015; Abercrombie and Batista-Navarro, 2018b) and, debate stance classification (Proksch et al., 2019). Existing work focuses on these tasks through legislative speeches from the US Congress (Chen et al., 2017), the UK Parliament (Bhavan et al., 2019), and the EU Parliament (Glavaˇs et al., 2017; Frid-Nielsen, 2018) and through social media such as Twitter (Trilling, 2014; Boutyline and Willer, 2017). Recently, some tools for extracting and annotating polit"
2020.coling-main.426,N19-1221,0,0.0604814,"Missing"
2020.coling-main.426,P15-1139,0,0.0284745,"d Work Politics and Linguistics Analyzing political data acts as a knowledge source that provides insights into cohesion within political parties, stances of MPs towards critical motions for both the general public, and across domains including humanities, and computational linguistics (Vilares and He, 2017; Sim et al., 2013; Slembrouck, 1992). A developing body of research at the intersection of Politics and Linguistics spans agreement detection (Menini and Tonelli, 2016; M. and M., 2018; Duthie and Budzynska, 2018), emotion analysis (Rheault, 2016; Dzieciatko, 2019), topic-opinion analysis (Nguyen et al., 2015; Abercrombie and Batista-Navarro, 2018b) and, debate stance classification (Proksch et al., 2019). Existing work focuses on these tasks through legislative speeches from the US Congress (Chen et al., 2017), the UK Parliament (Bhavan et al., 2019), and the EU Parliament (Glavaˇs et al., 2017; Frid-Nielsen, 2018) and through social media such as Twitter (Trilling, 2014; Boutyline and Willer, 2017). Recently, some tools for extracting and annotating political data have also been developed (Haddadan et al., 2019). Political Stance and Sentiment Analysis NLP has seen a growth in analyzing and mini"
2020.coling-main.426,D14-1162,0,0.0871229,"85). The good news is that natural language processing (NLP) shows promise for analyzing voluminous political debates and breaking the understanding barrier towards political ideology to help make informed voting decisions (Davoodi et al., 2020; Eidelman et al., 2018). However, conventional language models (Hasan and Ng, 2013) may not generalize well on understanding the obscure linguistic styles of political debates. This complexity arises due to the lack of political context and procedural parliamentary jargon in generic corpora over which traditional text representation models are trained (Pennington et al., 2014; Bojanowski et al., 2017). For instance, in the political context, Red State is a state that primarily votes for Republicans, whereas a Blue State votes primarily for Democrats. Leveraging the success of unsupervised pre-training in NLP (Devlin et al., 2019; Liu et al., 2019), fine-tuning pre-trained models over the voluminous debate transcripts can lead to drastic advances in analyzing political debates. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 https://www.parliament.uk/documents/rules-of-behaviour.pdf Licen"
2020.coling-main.426,W16-5612,0,0.0998675,"nation, such as economic and societal growth, policy reforms, and budget revisions. Records of such debates act as a valuable language resource as they provide a wealth of information regarding viewpoints of political representatives over critical societal factors (Abercrombie and Batista-Navarro, 2020b), and also for assessing political candidates and basing voting decisions (Utych, 2019). Analyzing sentiment in such political discourse is propelled by the developments in fields like behavioral economics that bring the psychological aspects of parliamentary decision-making to the forefront (Rheault, 2016). However, under Parliament’s Rules of Behavior,1 the language used in political debates is complex, laden with domain-specific procedural jargon used in the political realm, and obscure (Abercrombie and Batista-Navarro, 2018a). This esoteric and tedious nature of political debates makes their analysis complex, forming a barrier to ordinary citizen’s insights into political stances and wide-ranging consequences they entail (Edelman, 1985). The good news is that natural language processing (NLP) shows promise for analyzing voluminous political debates and breaking the understanding barrier towa"
2020.coling-main.426,D13-1010,0,0.0185637,"mmons, we demonstrate GPolS’s ability for stance analysis in parliamentary debates (Sec. 5). Lastly, we visualize GPolS’s graph attention mechanism (Sec. 5.3) and token-level attention (Sec. 5.4) thus providing a use case for GPolS as a tool for parliamentary debate analysis. 4848 2 Related Work Politics and Linguistics Analyzing political data acts as a knowledge source that provides insights into cohesion within political parties, stances of MPs towards critical motions for both the general public, and across domains including humanities, and computational linguistics (Vilares and He, 2017; Sim et al., 2013; Slembrouck, 1992). A developing body of research at the intersection of Politics and Linguistics spans agreement detection (Menini and Tonelli, 2016; M. and M., 2018; Duthie and Budzynska, 2018), emotion analysis (Rheault, 2016; Dzieciatko, 2019), topic-opinion analysis (Nguyen et al., 2015; Abercrombie and Batista-Navarro, 2018b) and, debate stance classification (Proksch et al., 2019). Existing work focuses on these tasks through legislative speeches from the US Congress (Chen et al., 2017), the UK Parliament (Bhavan et al., 2019), and the EU Parliament (Glavaˇs et al., 2017; Frid-Nielsen,"
2020.coling-main.426,D17-1165,0,0.194985,"rom the UK House of Commons, we demonstrate GPolS’s ability for stance analysis in parliamentary debates (Sec. 5). Lastly, we visualize GPolS’s graph attention mechanism (Sec. 5.3) and token-level attention (Sec. 5.4) thus providing a use case for GPolS as a tool for parliamentary debate analysis. 4848 2 Related Work Politics and Linguistics Analyzing political data acts as a knowledge source that provides insights into cohesion within political parties, stances of MPs towards critical motions for both the general public, and across domains including humanities, and computational linguistics (Vilares and He, 2017; Sim et al., 2013; Slembrouck, 1992). A developing body of research at the intersection of Politics and Linguistics spans agreement detection (Menini and Tonelli, 2016; M. and M., 2018; Duthie and Budzynska, 2018), emotion analysis (Rheault, 2016; Dzieciatko, 2019), topic-opinion analysis (Nguyen et al., 2015; Abercrombie and Batista-Navarro, 2018b) and, debate stance classification (Proksch et al., 2019). Existing work focuses on these tasks through legislative speeches from the US Congress (Chen et al., 2017), the UK Parliament (Bhavan et al., 2019), and the EU Parliament (Glavaˇs et al., 2"
2020.coling-main.426,J94-2004,0,0.329429,"ipt edges Est based on speaker-self context, Speaker-Speaker edges Ess based on intra-party context, and Transcript-Motion edges Etm based on motion context. G is a heterogeneous graph as it has different types of nodes and edges. We now describe these three relations one by one that capture different contexts based on the similarities observed through Figure 3. • Speaker-self context captures the relationship between a speaker and the transcript of their speech in a given debate. Each speech is personalized and reflects the mentality of a speaker across different speeches, motions and times (Wiebe, 1994a; Layman et al., 2006). Contrastingly, self-speaker context can also capture the domain expertise of the speakers. Speaker-self context captures the similarity between speeches by the same speaker. It is formally represented as a Speaker-Transcript edge Est between a transcript t and the MP s whose speech that transcript corresponds to. • Intra-party context models the relationship between a MP and the party they belong to. We build on the hypothesis that speakers are influenced by other party members, and there exists a partisan mentality like political cohesion within parties. (Lai et al.,"
2020.coling-main.611,D18-1319,0,0.061378,"Missing"
2020.coling-main.611,D14-1181,0,0.0156671,"re denoted by S where S = {S1 , S2 , ...} where each Si ∈ [0, M ]. The layer m where mixup occurs is chosen randomly from S with equal probability given to each layer in S and sampled separately for each pair of examples that are mixed. 3 Experiments We investigated the effectiveness of EMix with five benchmark sentence classification tasks as used by Guo et al. (2019), for a fair comparison. The following datasets are used : TREC (Li and Roth, 2002), SST-1 (Socher et al., 2013), SST-2 (binary classification) and Subj (Pang and Lee, 2004). We evaluate our proposed mixup using the popular CNN (Kim, 2014) and BERT (Devlin et al., 2018) for sentence classification model. We compare with three recent text augmentation methods including EDA (Wei and Zou, 2019), wordMixup (linear interpolation is applied on the word embedding level) and senMixup (linear interpolation is conducted on the layer before the Softmax layer) (Guo et al., 2019). These augmentation strategies rely less on additional text resources or domain knowledge. In our experiments, we follow the exact implementation and settings in (Kim, 2014), (Guo et al., 2019), and (Wei and Zou, 2019). Specifically, we use filter sizes of 3, 4, an"
2020.coling-main.611,C02-1150,0,0.276038,"d forward pass after the mixed hidden representation has been generated is defined as follows: hl = gl (hl−1 , θ), l ∈ [m + 1, M ] (7) The layers chosen for mixup are denoted by S where S = {S1 , S2 , ...} where each Si ∈ [0, M ]. The layer m where mixup occurs is chosen randomly from S with equal probability given to each layer in S and sampled separately for each pair of examples that are mixed. 3 Experiments We investigated the effectiveness of EMix with five benchmark sentence classification tasks as used by Guo et al. (2019), for a fair comparison. The following datasets are used : TREC (Li and Roth, 2002), SST-1 (Socher et al., 2013), SST-2 (binary classification) and Subj (Pang and Lee, 2004). We evaluate our proposed mixup using the popular CNN (Kim, 2014) and BERT (Devlin et al., 2018) for sentence classification model. We compare with three recent text augmentation methods including EDA (Wei and Zou, 2019), wordMixup (linear interpolation is applied on the word embedding level) and senMixup (linear interpolation is conducted on the layer before the Softmax layer) (Guo et al., 2019). These augmentation strategies rely less on additional text resources or domain knowledge. In our experiments"
2020.coling-main.611,P04-1035,0,0.0556298,"lows: hl = gl (hl−1 , θ), l ∈ [m + 1, M ] (7) The layers chosen for mixup are denoted by S where S = {S1 , S2 , ...} where each Si ∈ [0, M ]. The layer m where mixup occurs is chosen randomly from S with equal probability given to each layer in S and sampled separately for each pair of examples that are mixed. 3 Experiments We investigated the effectiveness of EMix with five benchmark sentence classification tasks as used by Guo et al. (2019), for a fair comparison. The following datasets are used : TREC (Li and Roth, 2002), SST-1 (Socher et al., 2013), SST-2 (binary classification) and Subj (Pang and Lee, 2004). We evaluate our proposed mixup using the popular CNN (Kim, 2014) and BERT (Devlin et al., 2018) for sentence classification model. We compare with three recent text augmentation methods including EDA (Wei and Zou, 2019), wordMixup (linear interpolation is applied on the word embedding level) and senMixup (linear interpolation is conducted on the layer before the Softmax layer) (Guo et al., 2019). These augmentation strategies rely less on additional text resources or domain knowledge. In our experiments, we follow the exact implementation and settings in (Kim, 2014), (Guo et al., 2019), and"
2020.coling-main.611,D13-1170,0,0.00645211,"xed hidden representation has been generated is defined as follows: hl = gl (hl−1 , θ), l ∈ [m + 1, M ] (7) The layers chosen for mixup are denoted by S where S = {S1 , S2 , ...} where each Si ∈ [0, M ]. The layer m where mixup occurs is chosen randomly from S with equal probability given to each layer in S and sampled separately for each pair of examples that are mixed. 3 Experiments We investigated the effectiveness of EMix with five benchmark sentence classification tasks as used by Guo et al. (2019), for a fair comparison. The following datasets are used : TREC (Li and Roth, 2002), SST-1 (Socher et al., 2013), SST-2 (binary classification) and Subj (Pang and Lee, 2004). We evaluate our proposed mixup using the popular CNN (Kim, 2014) and BERT (Devlin et al., 2018) for sentence classification model. We compare with three recent text augmentation methods including EDA (Wei and Zou, 2019), wordMixup (linear interpolation is applied on the word embedding level) and senMixup (linear interpolation is conducted on the layer before the Softmax layer) (Guo et al., 2019). These augmentation strategies rely less on additional text resources or domain knowledge. In our experiments, we follow the exact impleme"
2020.coling-main.611,D15-1306,0,0.0203425,"ur proposed method, we eliminate the common mean vector from the embeddings. We then interpolate hidden representations at multiple layers, including the embedding layer. This mixing method is also simple and easy to implement and leads to a further improvement in performance. Related Work: Data Augmentation has been vastly used in Computer Vision (Krizhevsky et al., 2012), which employs techniques like flipping, scaling, and rotation. Data augmentation has also been explored in NLP to some extent. (Zhang et al., 2015) replaced words with their synonyms according to a geometric distribution. (Wang and Yang, 2015) used k-NN and Cosine Similarity metrics to find a similar word for replacement. (Wei and Zou, 2019) introduced EDA (Easy Data Augmentation) for NLP which includes Synonym Replacement, Random Insertion, Random Deletion, and Random Swap. These methods have the risk of completely changing the context of the sentence with the replacement of a word which alters the entire This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 6931 Proceedings of the 28th International Conference on Computational Linguisti"
2020.coling-main.611,D19-1670,0,0.228592,"s paper, we deal with the problem of sentence classification. Current state-of-the-art deep learning approaches for sentence classification have millions of parameters, thus requiring a large number of training examples, which may be time-consuming and expensive to obtain. We present a new data augmentation technique called Emix to improve the performance of current text classification models. In general, it is difficult to come up with rules for language transformation similar to image transformations; hence universal data augmentation techniques have not thoroughly been explored in NLP yet (Wei and Zou, 2019). One such technique is Mixup - it uses systematic transformations to make sure the model trains on samples from the vicinity distribution (Chapelle et al., 2001) along with the original distribution of the training data. Mixup is a data-agnostic data augmentation method that is proven to be effective in Computer Vision tasks (Zhang et al., 2017; Verma et al., 2018). Guo et al. (2019) extends Mixup to Text classification using interpolations in the embedding space. In our proposed method, we eliminate the common mean vector from the embeddings. We then interpolate hidden representations at mul"
2020.ecnlp-1.7,P14-5010,0,0.00496482,"eatures Request Model 70.1 Politeness Markers 70.4 F1-score ing to the constituent tokens. Word2Vec Clusters: We follow the same approach as in (Preot¸iuc-Pietro et al., 2015), where words are clustered using pair-wise similarities in Word2Vec space (Mikolov et al., 2013). Each tweet is then represented as a distribution over these clusters; the values are proportional to the number of tokens belonging to a cluster. These clusters have previously been demonstrated to have great interpretability (Preot¸iuc-Pietro et al., 2015, 2017; Zou et al., 2016). POS Tags: We used the Stanford POS Tagger (Manning et al., 2014) to represent tweets as a dense frequency vector over five main POS tags: nouns, adjectives, adverbs, verbs, pronouns. Pronoun Types: Pronouns are often used in complaints and suggestions to reveal personal involvement or to add intensity to an opinion (Claridge, 2007; Meinl, 2013). We identify various pronoun types (first person, second person, third person, demonstrative, indefinite) using dictionaries and use their counts as features. 0.70 0.66 0.67 0.65 0.61 0.59 0.62 0.63 0.62 0.67 0.66 0.63 Table 3: Predictive accuracy and F1-score associated with different types of features. tated them"
2020.ecnlp-1.7,S13-2053,0,0.0286673,"ashtags, user mentions, and intensifiers. (iv) The last group of features again use pre-trained models or lexicons associated with request, which is a closely related ˇ arov´a, 2008). speech act (Sv´ 4.1.1 Sentiment features We expect sentiment to contribute strongly towards the prediction of complaints. We experiment with two pre-trained models: Stanford Sentiment (Socher et al., 2013) and VADER (Hutto and Gilbert, 2014). Namely, we use the scores predicted by these models as representations of tweets. Likewise, we also experiment with two sentiment lexicons: MPQA (Wilson et al., 2005), NRC (Mohammad et al., 2013) for assigning sentiment scores to tweets. 4.1.3 Orthographic features Our first set of orthographic feature uses counts of URLs, hashtags, user mentions, and special symbols used in the post. The second set of orthographic features try to identify potential intensifiers such as capitalization and repeated use of exclamation or question marks. These types of intensifiers are often used to express anger or strong opinions Semantic features We experimented with four different semantic features: Unigrams: Each tweet (Wallach, 2006) is represented as sparse vector of tf-idf values correspond50 (Me"
2020.ecnlp-1.7,P19-1495,0,0.0327301,"Missing"
2020.ecnlp-1.7,P15-1169,0,0.0711353,"Missing"
2020.ecnlp-1.7,P17-1068,0,0.0600626,"Missing"
2020.ecnlp-1.7,P18-1187,0,0.0269888,"Missing"
2020.ecnlp-1.7,W03-0404,0,0.112002,"Missing"
2020.ecnlp-1.7,N19-2008,0,0.0445568,"Missing"
2020.ecnlp-1.7,D13-1170,0,0.00305025,"properties such as n-grams, word embeddings, and part of speech tags. (ii) The second group of features are based on pre-trained sentiment models or lexicons. (iii) The third group of features use orthographic information such as hashtags, user mentions, and intensifiers. (iv) The last group of features again use pre-trained models or lexicons associated with request, which is a closely related ˇ arov´a, 2008). speech act (Sv´ 4.1.1 Sentiment features We expect sentiment to contribute strongly towards the prediction of complaints. We experiment with two pre-trained models: Stanford Sentiment (Socher et al., 2013) and VADER (Hutto and Gilbert, 2014). Namely, we use the scores predicted by these models as representations of tweets. Likewise, we also experiment with two sentiment lexicons: MPQA (Wilson et al., 2005), NRC (Mohammad et al., 2013) for assigning sentiment scores to tweets. 4.1.3 Orthographic features Our first set of orthographic feature uses counts of URLs, hashtags, user mentions, and special symbols used in the post. The second set of orthographic features try to identify potential intensifiers such as capitalization and repeated use of exclamation or question marks. These types of intens"
2020.ecnlp-1.7,I05-1036,0,0.0265746,"Missing"
2020.ecnlp-1.7,H05-1044,0,0.103916,"aphic information such as hashtags, user mentions, and intensifiers. (iv) The last group of features again use pre-trained models or lexicons associated with request, which is a closely related ˇ arov´a, 2008). speech act (Sv´ 4.1.1 Sentiment features We expect sentiment to contribute strongly towards the prediction of complaints. We experiment with two pre-trained models: Stanford Sentiment (Socher et al., 2013) and VADER (Hutto and Gilbert, 2014). Namely, we use the scores predicted by these models as representations of tweets. Likewise, we also experiment with two sentiment lexicons: MPQA (Wilson et al., 2005), NRC (Mohammad et al., 2013) for assigning sentiment scores to tweets. 4.1.3 Orthographic features Our first set of orthographic feature uses counts of URLs, hashtags, user mentions, and special symbols used in the post. The second set of orthographic features try to identify potential intensifiers such as capitalization and repeated use of exclamation or question marks. These types of intensifiers are often used to express anger or strong opinions Semantic features We experimented with four different semantic features: Unigrams: Each tweet (Wallach, 2006) is represented as sparse vector of t"
2020.emnlp-main.619,P17-1067,0,0.0588312,"fixed size sentence embedding. ?? ??−? ???? X + + X ???? X ?(⋅) ? ? X ???? ?  ?? ?  ??−? ? ???−? ??−? Δ? ?? hi? ?? ??+? ???+? ??+? Figure 3: Architecture of a Time-aware LSTM cell. Figure is adapted from Baytas et al. (2017). [CLS] token (768-dimensional encoding) as the aggregate representation of the emotional spectrum. We define the emotion vector (Eki 2 R768 ) of each historic tweet hik as: Eki = PlutchikTransformer(hik ) (2) Modeling Historical Tweets Sequentially: The emotional historic context of tweets can be used to model progressive emotional states of the author of those tweets (Abdul-Mageed and Ungar, 2017; De Choudhury et al., 2013). This makes recurrent neural networks (RNN), and particularly LSTMs (Hochreiter and Schmidhuber, 1997), the most natural methods for encoding and learning from a sequence of a user’s historical tweets. However, the time interval between the posting of historic tweets can vary widely, from a few seconds to a few years (Wojcik and Hughes, 2019). Such variations can be an important factor in analyzing the emotional states of a user over time (Sueki, 2015). LSTM cells assume the input to be equally spaced sequences and thus are unable to model irregularities in posting"
2020.emnlp-main.619,W17-1612,0,0.0225042,"n, 2017; Broer, 2020), and non-intrusive manner. Although informed consent of each user was not sought as it may be deemed coercive, automated de-identification of the dataset was performed to reduce the risk of including any identifying data in the raw data. All tweets shown as examples in Figure 1 and Section 5.4 have been paraphrased as per the moderate disguise scheme suggested in Bruckman (2002) to protect the privacy of individuals (Fiesler and Proferes, 2018). The annotation of user data has been kept separately from raw user data on protected servers linked only through anonymous IDs (Benton et al., 2017). Assessments made by STATENet are sensitive and should be shared selectively to avoid misuse, such as Samaritan’s Radar (Hsin et al., 2016). Our work does not make any diagnostic claims related to suicide. We study the social media posts in a purely observational capacity (Norval and Henderson, 2017) and do not intervene with the user experience in any way. Limitations: We acknowledge that studying suicidality is subjective in nature (Keilp et al., 2012) and that the interpretation of the analysis presented may vary across individuals. Due to the situatedness of language, the studied data may"
2020.emnlp-main.619,W14-3207,0,0.452219,"Missing"
2020.emnlp-main.619,2020.acl-main.700,0,0.233404,"5) and lose the context of individual historic tweets by aggregating them. Approaches besides deep learning have also been explored, such as the work done by Vioules et al. (2018), which uses the martingale framework (Ho, 2005) with sentiment scores and tweet level features such as likes to study two users on Twitter. ?ො? ?? ⋮ that can give insight into the user’s mental state to improve predictive power (Venek et al., 2017). A user-dependent, personalized context can truly process the “natural” language of a user and understand the semantic context from the perspective of that specific user (Flek, 2020). User context may include the user’s emotion spectrum (Ren et al., 2016), social graph methods (Mishra et al., 2019) and temporal context (Mathur et al., 2020). Suicide risk assessment for preliminary screening has been done at both binary (suicidal intent present, suicidal intent absent) (Cao et al., 2019; De Choudhury et al., 2016; Mathur et al., 2020; Losada et al., 2019), and multiple (Zirikly et al., 2019; Vioules et al., 2018; Gaur et al., 2019) levels of risk ranging from no risk to severe risk. ⋯ ?? ?? ???−? ??−? ሬሬሬሬሬሬԦ ??? ሬሬሬሬԦ ??? ???−? ??−? ??? ?? Figure 2: STATENet: Model Archit"
2020.emnlp-main.619,P16-2096,0,0.113986,"ctively to avoid misuse, such as Samaritan’s Radar (Hsin et al., 2016). Our work does not make any diagnostic claims related to suicide. We study the social media posts in a purely observational capacity (Norval and Henderson, 2017) and do not intervene with the user experience in any way. Limitations: We acknowledge that studying suicidality is subjective in nature (Keilp et al., 2012) and that the interpretation of the analysis presented may vary across individuals. Due to the situatedness of language, the studied data may be susceptible to demographic, annotator, and mediumspecific biases (Hovy and Spruit, 2016). We recognize that suicide risk exists on a diverse spectrum, and the simplification of binary labels could lead to artificial notions of risk (Bryan and Rudd, 2006). Practical Implications: Through STATENet, we suggest a neural architecture for preliminary screening of at-risk users on social media to aid the prioritization of clinical resources. Our work observes Twitter in a non-intrusive manner and does not intervene with the user experience in any way. STATENet should form part of a distributed human-in-the-loop (de Andrade et al., 2018) system for finer interpretation of risk. Focusing"
2020.emnlp-main.619,W19-3005,0,0.636295,"uˇsic, 7685 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7685–7697, c November 16–20, 2020. 2020 Association for Computational Linguistics 2003). Analyzing the user history and emotion spectrum, as shown in Figure 1 can provide crucial context to estimate suicidal risk in a tweet authored by that user. Such an Emotional Historic Context (EHC) of a user over time can be characteristic of their mental health (Coppersmith et al., 2014). Modeling temporal user context, either as a bagof-tweets (Gaur et al., 2019), or sequentially (Cao et al., 2019; Matero et al., 2019) helps in identifying suicidal intent. However, in Figure 1, we show that the impact of varying time intervals between tweets is crucial for an accurate assessment. It is critical to model the large gap between the user’s recent tweets that are collectively indicative of suicidal intent and those three years apart. Such uneven Temporal Tweeting Irregularities (TTI) ranging from seconds to years (Wojcik and Hughes, 2019) between successive tweets influence the assessment of a user’s tweet differently. Sequential models such as Long Short Term Memory (LSTMs) networks assume that posting interval"
2020.emnlp-main.619,N19-3019,1,0.466795,"have also been explored, such as the work done by Vioules et al. (2018), which uses the martingale framework (Ho, 2005) with sentiment scores and tweet level features such as likes to study two users on Twitter. ?ො? ?? ⋮ that can give insight into the user’s mental state to improve predictive power (Venek et al., 2017). A user-dependent, personalized context can truly process the “natural” language of a user and understand the semantic context from the perspective of that specific user (Flek, 2020). User context may include the user’s emotion spectrum (Ren et al., 2016), social graph methods (Mishra et al., 2019) and temporal context (Mathur et al., 2020). Suicide risk assessment for preliminary screening has been done at both binary (suicidal intent present, suicidal intent absent) (Cao et al., 2019; De Choudhury et al., 2016; Mathur et al., 2020; Losada et al., 2019), and multiple (Zirikly et al., 2019; Vioules et al., 2018; Gaur et al., 2019) levels of risk ranging from no risk to severe risk. ⋯ ?? ?? ???−? ??−? ሬሬሬሬሬሬԦ ??? ሬሬሬሬԦ ??? ???−? ??−? ??? ?? Figure 2: STATENet: Model Architecture 3 Methodology 3.1 Notations and Problem Formulation We acknowledge that modeling suicidal intent as a binary c"
2020.emnlp-main.619,W19-3004,0,0.0137275,"houdhury et al., 2013) and that their suicidal behaviour is correlated with suicidal tweets (Sueki, 2015). Static word embeddings such as GloVe (Pennington et al., 2014) have been used to encode tweets for detecting suicide ideation (Sinha et al., 2019) in the past. However, recent 7687 studies have shown that pre-trained transformer models yield more comprehensive representations of linguistic features in a tweet (Salminen et al., 2020). We found that SentenceBERT (Reimers and Gurevych, 2019) empirically outperforms embeddings used in previous works such as FastText (Cao et al., 2019), ELMo (Mohammadi et al., 2019), etc. We use the 768-dimensional encoding obtained from SentenceBERT.3 Formally, 0 Ti = SentenceBERT(ti ) (1) where Ti0 2 R768 is linearly transformed using a dense layer to Ti 2 Rd with dimension d. 3.3 User Historical Emotion Spectrum Individual Historic Tweet Encoding: Amplification of emotional factors such as emotional reactivity (Tarrier et al., 2007), intensity (Links et al., 2008) and instability (Palmier-Claus et al., 2012) can increase suicide risk. Building on this, we extract the emotion spectrum of each historic tweet hik . Although proficient in semantic modeling of text, genera"
2020.emnlp-main.619,D14-1162,0,0.0874979,"Missing"
2020.emnlp-main.619,D19-1410,0,0.0139372,"be Assessed Studies have shown that the linguistic styles of social media users can aid in understanding their mental state (De Choudhury et al., 2013) and that their suicidal behaviour is correlated with suicidal tweets (Sueki, 2015). Static word embeddings such as GloVe (Pennington et al., 2014) have been used to encode tweets for detecting suicide ideation (Sinha et al., 2019) in the past. However, recent 7687 studies have shown that pre-trained transformer models yield more comprehensive representations of linguistic features in a tweet (Salminen et al., 2020). We found that SentenceBERT (Reimers and Gurevych, 2019) empirically outperforms embeddings used in previous works such as FastText (Cao et al., 2019), ELMo (Mohammadi et al., 2019), etc. We use the 768-dimensional encoding obtained from SentenceBERT.3 Formally, 0 Ti = SentenceBERT(ti ) (1) where Ti0 2 R768 is linearly transformed using a dense layer to Ti 2 Rd with dimension d. 3.3 User Historical Emotion Spectrum Individual Historic Tweet Encoding: Amplification of emotional factors such as emotional reactivity (Tarrier et al., 2007), intensity (Links et al., 2008) and instability (Palmier-Claus et al., 2012) can increase suicide risk. Building o"
2020.emnlp-main.619,W18-6223,1,0.905462,"depressive symptoms (Harris and Goh, 2016). NLP Methods: In recent years, social media has shown promise in providing insights into the psychological state of individuals (Paul and Dredze, 2011). Jashinsky et al. (2014) reported that Twitter is a viable tool for real-time monitoring (Braithwaite et al., 2016) of suicide risk. Early efforts in utilizing social media include the use of user features (Masuda et al., 2013) and online suicide notes (Pestian et al., 2010; Huang et al., 2007). Since then, the focus has been on using psycholinguistic lexicons such as LIWC (De Choudhury et al., 2016; Sawhney et al., 2018b) and textual features such as POS, tense, etc. for classification (Ji et al., 2018; Huang et al., 2014). Shared tasks such as CLPsych (Zirikly et al., 2019) and CLEF eRISK (Losada et al., 2019) have seen a rise in the use of deep learning for suicidality prediction. CNN based architectures (Du et al., 2018; Sawhney et al., 2018a; Shing et al., 2018; Naderi et al., 2019) and LSTM based architectures (Ji et al., 2018; Tadesse et al., 2020) utilize pre-trained word embeddings to predict suicide risk. Although these text-based methods capture the semantic nature of posts in isolation, no user as"
2020.emnlp-main.619,P18-3013,1,0.623463,"depressive symptoms (Harris and Goh, 2016). NLP Methods: In recent years, social media has shown promise in providing insights into the psychological state of individuals (Paul and Dredze, 2011). Jashinsky et al. (2014) reported that Twitter is a viable tool for real-time monitoring (Braithwaite et al., 2016) of suicide risk. Early efforts in utilizing social media include the use of user features (Masuda et al., 2013) and online suicide notes (Pestian et al., 2010; Huang et al., 2007). Since then, the focus has been on using psycholinguistic lexicons such as LIWC (De Choudhury et al., 2016; Sawhney et al., 2018b) and textual features such as POS, tense, etc. for classification (Ji et al., 2018; Huang et al., 2014). Shared tasks such as CLPsych (Zirikly et al., 2019) and CLEF eRISK (Losada et al., 2019) have seen a rise in the use of deep learning for suicidality prediction. CNN based architectures (Du et al., 2018; Sawhney et al., 2018a; Shing et al., 2018; Naderi et al., 2019) and LSTM based architectures (Ji et al., 2018; Tadesse et al., 2020) utilize pre-trained word embeddings to predict suicide risk. Although these text-based methods capture the semantic nature of posts in isolation, no user as"
2020.emnlp-main.619,W18-0603,0,0.607073,"Missing"
2020.emnlp-main.619,W19-3003,0,0.836024,"of individuals (Paul and Dredze, 2011). Jashinsky et al. (2014) reported that Twitter is a viable tool for real-time monitoring (Braithwaite et al., 2016) of suicide risk. Early efforts in utilizing social media include the use of user features (Masuda et al., 2013) and online suicide notes (Pestian et al., 2010; Huang et al., 2007). Since then, the focus has been on using psycholinguistic lexicons such as LIWC (De Choudhury et al., 2016; Sawhney et al., 2018b) and textual features such as POS, tense, etc. for classification (Ji et al., 2018; Huang et al., 2014). Shared tasks such as CLPsych (Zirikly et al., 2019) and CLEF eRISK (Losada et al., 2019) have seen a rise in the use of deep learning for suicidality prediction. CNN based architectures (Du et al., 2018; Sawhney et al., 2018a; Shing et al., 2018; Naderi et al., 2019) and LSTM based architectures (Ji et al., 2018; Tadesse et al., 2020) utilize pre-trained word embeddings to predict suicide risk. Although these text-based methods capture the semantic nature of posts in isolation, no user associated context is provided 2 Similar to the type of algorithmic model deployed for post level screening on Facebook (Card, 2018). 7686 ?? ෩1 ? ?? ෩? ? ?′?"
2020.emnlp-main.643,N19-1423,0,0.00855628,"returns’ predictability can be improved since the disclosure of informed investors influences volatility spreads (Atilgan, 2014). Although multiple sources of information are crucial, not all modalities contribute equally (Akhtar et al., 2019). Noise in one modality can be detrimental in such multimodal frameworks (Morris-Drake et al., 2016). Multimodality and Finance The Efficient Market Hypothesis (Malkiel, 2003) illustrates the success of multimodal data sources for predictive financial tasks. The more recent multimodal HTML (Yang et al., 2020) is a transformer-based model that uses BERT (Devlin et al., 2019) for textual modeling, and the same hand-crafted audio features as MDRM, in an early fusion formulation. Both MDRM and HTML assume stocks’ independence and do not exploit these relations between stock movements. Relations like the same industrial base and co-ownership also result in related stock movements (Feng et al., 2019). Recent works exploit stock relations through graph neural networks (Kipf and Welling, 2017; Veliˇckovi´c et al., 2018) for stock movement prediction (Kim et al., 2019; Sawhney et al., 2020). Building on these gaps in existing literature, we propose VolTAGE for volatility"
2020.emnlp-main.643,N09-1031,0,0.884413,"escribe next. Volatility Forecasting Forecasting stock volatility is a crucial pillar across multiple financial domains and has focused on numerous academic studies. Volatility is a key indicator of uncertainty and is a decisive variable to many investment decisions and portfolio creations. Previous work in this domain has mainly relied on numerical features (Liu and Chen, 2019; Nikou et al., 2019), such as macroeconomic indicators (Hoseinzade et al., 2019). This includes discrete (GARCH (Duan, 1995), rolling regression (Peng et al., 2018)), continuous (Andersen, 2007), and neural approaches (Kogan et al., 2009). This comprehensive work illustrates the significance of volatility in investment, security valuation, and risk management. Natural Language Processing and Finance Extensive studies incorporating related text information have proven successful in financial forecasting tasks. Mohan et al. (2019); Tan et al. (2019) utilized financial news articles to improve the accuracy of stock price predictions. Hu et al. (2018) propose a hybrid attention network to predict the stock trend based on the related sequential news articles. Researchers have also observed the influence of textual data in online me"
2020.emnlp-main.643,D14-1120,0,0.0307475,"tment, security valuation, and risk management. Natural Language Processing and Finance Extensive studies incorporating related text information have proven successful in financial forecasting tasks. Mohan et al. (2019); Tan et al. (2019) utilized financial news articles to improve the accuracy of stock price predictions. Hu et al. (2018) propose a hybrid attention network to predict the stock trend based on the related sequential news articles. Researchers have also observed the influence of textual data in online media on stock markets (Bollen et al., 2011; Mittermayer and Knolmayer, 2006). Si et al. (2014) showed sentiment analysis based on social media is predictive of each stock’s market. However, utilizing multimodal sources of information remains an underexplored avenue in financial forecasting. Speech Processing and Finance Newer studies (Qin and Yang, 2019; Yang et al., 2020) illustrate the gains obtained using vocal cues from the CEO’s earnings conference calls for volatility prediction. Yet, the majority of the current work does not utilize speech based data. The audio features add 8002 greater context and provide psycho-linguistic signaling about the speaker’s emotional state (Jiang an"
2020.emnlp-main.643,D14-1162,0,0.0844529,"Missing"
2020.emnlp-main.643,P17-1081,0,0.0317663,"ing Yahoo Finance6 from 1 January’17 till 31 December’17. The stock data for 11 earnings calls was not available on Yahoo Finance; hence we excluded these calls from our dataset. Following Qin and Yang (2019); Yang et al. (2020), we experiment with n 2 {3, 7, 15, 30} days to analyze the performance over both short and long term periods. 5.2 Baselines We compare VolTAGE with the following methods: • Vpast : Following Qin and Yang (2019), we use Vpast , the average log volatility of the past d days to predict the future d days’ average log volatility. • bc-LSTM: We also compare against bc-LSTM (Poria et al., 2017) which extracts the uni-modal features using separate contextual Bi-LSTMs and fuses them. • MDRM: Qin and Yang (2019) extract pretrained GloVe embeddings and hand-crafted acoustic features that are fed to separate BiLSTMs to get their uni-modal contextual embeddings, which are then fused and fed to a twolayer dense network. • HTML: Yang et al. (2020) is the state-of-the-art model using WWM-BERT to encode text tokens. HTML makes use of the same audio features as MDRM. The unimodal features are fused and fed to a sentence-level transformer to get the multimodal representations for each call. 5.3"
2020.emnlp-main.643,P19-1038,0,0.211355,"ech, which could indicate the emotional and affective state of the speakers, and provide insights into company performance. More recently, the use of audio processing for earnings calls has gained an interest in both financial and linguistic research (Burgoon et al., 2015; Jiang and Pell, 2017). 8001 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8001–8013, c November 16–20, 2020. 2020 Association for Computational Linguistics Multimodal approaches can extract complementary information from multiple modalities to improve financial modeling, MDRM (Qin and Yang, 2019), and HTML (Yang et al., 2020) validate the premise of such approaches for volatility forecasting. Additionally, advances in graph-based deep learning (Kipf and Welling, 2017) have led to the rise of graph neural networks (GNNs) that can model the relationships between related stocks (Feng et al., 2019). Publicly available online company information can be used to identify connections between stocks that might influence each other, such as those having the same CEO or belonging to the same industry. Financial tasks are often correlated, thus making multi-task learning a promising choice for fi"
2020.emnlp-main.643,P14-1109,0,0.303112,"inance Newer studies (Qin and Yang, 2019; Yang et al., 2020) illustrate the gains obtained using vocal cues from the CEO’s earnings conference calls for volatility prediction. Yet, the majority of the current work does not utilize speech based data. The audio features add 8002 greater context and provide psycho-linguistic signaling about the speaker’s emotional state (Jiang and Pell, 2017). Qin and Yang (2019) illustrated that late fusion of audio and text features from earnings calls could be used to forecast stock volatility following the earnings call. The verbose quarterly earnings calls (Wang and Hua, 2014) act as a medium of voluntary disclosure (Tasker, 1998), thereby resulting in significant stock movements (Ding et al., 2015), yet the majority of existing approaches do not focus on such highly volatile macro activities, where the market microstructure is highly uncertain (Rogers et al., 2009). During these macro events, the stock returns’ predictability can be improved since the disclosure of informed investors influences volatility spreads (Atilgan, 2014). Although multiple sources of information are crucial, not all modalities contribute equally (Akhtar et al., 2019). Noise in one modality"
2020.emnlp-main.645,S17-2091,0,0.0245806,"to further study the use of GANs for the task of keyphrase generation. 1 Introduction Keyphrases capture the most salient topics of a document and are often indexed in databases to help with search and information retrieval techniques. Researchers tag their scientific publications with high-quality keyphrases to ensure discoverability in scientific repositories. Automatic identification of keyphrases is of great interest to the scientific community as it helps to recommend relevant articles, suggest missing citations to authors, identify potential peer reviewers, and analyze research trends (Augenstein et al., 2017). Keyphrases could either be extractive (part of the document) or abstractive (not part of the document). Some prior works have referred to them as present and absent keyphrases, respectively. Keyphrase generation is the process of predicting both extractive and abstractive keyphrases from a given document. Most of the previous works in keyphrase domain, including both supervised and unsupervised techniques, primarily focus on extractive keyphrases (Hasan and Ng, 2014; Mahata et al., 2018; Sahrawat et al., 2020). Recent studies Meng et al. (2017); Ye and Wang (2018); Chan et al. (2019) have st"
2020.emnlp-main.645,N19-1070,0,0.0525614,"Missing"
2020.emnlp-main.676,1996.eamt-1.1,0,0.0150564,"Missing"
2020.emnlp-main.676,D18-2029,0,0.0216286,", 2002). To this end, MAN-SF uses the SMI encoder to extract a feature vector ct using tweets. The encoder shown in Figure 3 extracts social media features, ct , by first encoding tweets for a day and then over multiple days using a hierarchical attention mechanism (Yang et al., 2016). Tweet Embedding For any given tweet tw , we generate an embedding vector m ∈ Rd . We explored word and sentence level embedding methods to learn tweet representations: Global Vectors for Word Representation (GloVe) (Pennington et al., 2014), Fasttext (Joulin et al., 2017), and Universal Sentence Encoders (USE) (Cer et al., 2018). Empirically, sentence-level embeddings generated using a deep averaging network encoder variant of the USE3 gave us the most promising results. Thus, we encode each tweet tw using USE. Learning Representations for one day On any day i, a variable number tweets [tw1 , tw2 , . . . twK ] for each stock s are posted, and these capture and influence the stock trends (Fung et al., 3 Implementation used: https://tfhub.dev/ google/universal-sentence-encoder/2 exp (hTj W hm ) γj = PK T j=1 exp (hj W hm ) (6) X (7) ri = γj hj j where, hm ∈ RK×dm denotes a concatenation of all hidden states from GRUm a"
2020.emnlp-main.676,W14-4012,0,0.112836,"Missing"
2020.emnlp-main.676,D14-1148,0,0.0185024,"er prices or tweets, and further align the data across trading windows for related stocks to ensure data is available for all trading days in the window for all stocks. The hidden size of all GRUs is 64, and the USE embedding dimension is 512. We use U = 8 attention heads for both GAT layers. We use the Adam optimizer with a learning rate set to 5e−4 and train MAN-SF for 10, 000 epochs. It takes 3hrs to train and test MAN-SF on Tesla K80 GPU. We use early stopping based on Matthew’s Correlation Coefficient (MCC) taken over the validation set. 5.2 Following prior research for stock prediction (Ding et al., 2014; Xu and Cohen, 2018), we use accuracy, F1 score, MCC (implementations from sklearn6 ) for classification performance. We use MCC because, unlike the F1 score, MCC avoids bias due to data skew as it does not depend on the choice of the positive class and accounts forthe TrueNegatives. tp f n For a given confusion matrix : f p tn tp × tn − f p × f n (14) MCC = p (tp + f p)(tp + f n)(tn + f p)(tn + f n) i=1 where, Yi is the true price movement of stock i. 5 Experiments 5.1 Dataset and Training Setup We adopt the StockNet dataset (Xu and Cohen, 2018) for the training and evaluation of MAN-SF. T"
2020.emnlp-main.676,E17-2068,0,0.0110308,"ser sentiment towards stocks (Xu and Cohen, 2018; Fung et al., 2002). To this end, MAN-SF uses the SMI encoder to extract a feature vector ct using tweets. The encoder shown in Figure 3 extracts social media features, ct , by first encoding tweets for a day and then over multiple days using a hierarchical attention mechanism (Yang et al., 2016). Tweet Embedding For any given tweet tw , we generate an embedding vector m ∈ Rd . We explored word and sentence level embedding methods to learn tweet representations: Global Vectors for Word Representation (GloVe) (Pennington et al., 2014), Fasttext (Joulin et al., 2017), and Universal Sentence Encoders (USE) (Cer et al., 2018). Empirically, sentence-level embeddings generated using a deep averaging network encoder variant of the USE3 gave us the most promising results. Thus, we encode each tweet tw using USE. Learning Representations for one day On any day i, a variable number tweets [tw1 , tw2 , . . . twK ] for each stock s are posted, and these capture and influence the stock trends (Fung et al., 3 Implementation used: https://tfhub.dev/ google/universal-sentence-encoder/2 exp (hTj W hm ) γj = PK T j=1 exp (hj W hm ) (6) X (7) ri = γj hj j where, hm ∈ RK×d"
2020.emnlp-main.676,P15-1131,0,0.515539,"strategy speculates a fall in price, a short sell7 is performed. We compute the cumulative profit (Krauss, 2018) earned as: Profitt = X pti − pt−1 t−1 i (−1)Actioni t−1 p i i∈S (15) where, S denotes the set of stocks, pti denotes the price of stock i at day t. Actiont−1 is a binary i value [0, 1]. The Actiont−1 is 0 if the long position i is taken at time t for stock i; otherwise it is 1. 6 sklearn: https://scikit-learn.org Short sell: https://en.wikipedia.org/wiki/ Short_(finance) 8420 7 Model TA FA RAND ARIMA (Brown, 2004) Selvin et al. (2017) RandForest (Venkata Sasank Pagolu, 2016) TSLDA (Nguyen and Shirai, 2015) HAN (Hu et al., 2018) StockNet - TechnicalAnalyst (Xu and Cohen, 2018) StockNet - FundamentalAnalyst (Xu and Cohen, 2018) StockNet - IndependentAnalyst (Xu and Cohen, 2018) StockNet - DiscriminativeAnalyst (Xu and Cohen, 2018) StockNet - HedgeFundAnalyst (Xu and Cohen, 2018) HATS (Kim et al., 2019) Chen et al. (2018) Adversarial LSTM (Feng et al., 2019a) MAN-SF (This work) F1 ↑ Accuracy ↑ MCC ↑ 0.502 ± 8e−4 0.513 ± 1e−3 0.529 ± 5e−2 0.527 ± 2e−3 0.539 ± 6e−3 0.572 ± 4e−3 0.546 ± − 0.572 ± − 0.573 ± − 0.559 ± − 0.575 ± − 0.560 ± 2e−3 0.530 ± 7e−3 0.570 ± − 0.605 ± 2e−4 0.509 ± 8e−4 0.514 ± 1e−"
2020.emnlp-main.676,D14-1162,0,0.0853484,"ctual information but also portray user sentiment towards stocks (Xu and Cohen, 2018; Fung et al., 2002). To this end, MAN-SF uses the SMI encoder to extract a feature vector ct using tweets. The encoder shown in Figure 3 extracts social media features, ct , by first encoding tweets for a day and then over multiple days using a hierarchical attention mechanism (Yang et al., 2016). Tweet Embedding For any given tweet tw , we generate an embedding vector m ∈ Rd . We explored word and sentence level embedding methods to learn tweet representations: Global Vectors for Word Representation (GloVe) (Pennington et al., 2014), Fasttext (Joulin et al., 2017), and Universal Sentence Encoders (USE) (Cer et al., 2018). Empirically, sentence-level embeddings generated using a deep averaging network encoder variant of the USE3 gave us the most promising results. Thus, we encode each tweet tw using USE. Learning Representations for one day On any day i, a variable number tweets [tw1 , tw2 , . . . twK ] for each stock s are posted, and these capture and influence the stock trends (Fung et al., 3 Implementation used: https://tfhub.dev/ google/universal-sentence-encoder/2 exp (hTj W hm ) γj = PK T j=1 exp (hj W hm ) (6) X ("
2020.emnlp-main.676,P19-1038,0,0.15388,"9; Nguyen et al., 2019) and macroeconomic indicators like GDP (Hoseinzade et al., 2019). Such TA methods include discrete: GARCH (Bollerslev, 1986), continuous (Andersen, 2007), and neural approaches (Nguyen and Yoon, 2019; Nikou et al., 2019). Newer models based on the EMH that are categorized under fundamental analysis (FA) (Dichev and Tang, 2006), account for stock affecting factors beyond numerical ones such as investor sentiment through news, etc. Work in natural language processing (NLP) from sources such as news (Hu et al., 2018), social media data (Xu and Cohen, 2018), earnings calls (Qin and Yang, 2019; Sawhney et al., 2020b) shows the merit of FA in capturing market sentiment, surprises, mergers, acquisitions that traditional TA based methods fail to account. A limitation of existing NLP methods for stock prediction is that they assume stock movements to be independent of each other, contrary to true market function (Diebold and Yilmaz, 2014). This assumption hinders NLP centric FA’s ability to learn latent patterns for the study of interrelated stocks. Another line of FA revolves around employing graph-based methods to improve TA (e.g., pricebased models) by augmenting them with inter sto"
2020.emnlp-main.676,P18-1183,0,0.221261,"s like past prices (Ding and Qin, 2019; Nguyen et al., 2019) and macroeconomic indicators like GDP (Hoseinzade et al., 2019). Such TA methods include discrete: GARCH (Bollerslev, 1986), continuous (Andersen, 2007), and neural approaches (Nguyen and Yoon, 2019; Nikou et al., 2019). Newer models based on the EMH that are categorized under fundamental analysis (FA) (Dichev and Tang, 2006), account for stock affecting factors beyond numerical ones such as investor sentiment through news, etc. Work in natural language processing (NLP) from sources such as news (Hu et al., 2018), social media data (Xu and Cohen, 2018), earnings calls (Qin and Yang, 2019; Sawhney et al., 2020b) shows the merit of FA in capturing market sentiment, surprises, mergers, acquisitions that traditional TA based methods fail to account. A limitation of existing NLP methods for stock prediction is that they assume stock movements to be independent of each other, contrary to true market function (Diebold and Yilmaz, 2014). This assumption hinders NLP centric FA’s ability to learn latent patterns for the study of interrelated stocks. Another line of FA revolves around employing graph-based methods to improve TA (e.g., pricebased model"
2020.emnlp-main.676,N16-1174,0,0.0275549,"drive stock trends (AbuMostafa and Atiya, 1996). With the rising ubiquity of the Internet, social media platforms, such as Twitter, influence investors to follow market trends (Tetlock, 2007; Hu et al., 2018). Tweets not only convey factual information but also portray user sentiment towards stocks (Xu and Cohen, 2018; Fung et al., 2002). To this end, MAN-SF uses the SMI encoder to extract a feature vector ct using tweets. The encoder shown in Figure 3 extracts social media features, ct , by first encoding tweets for a day and then over multiple days using a hierarchical attention mechanism (Yang et al., 2016). Tweet Embedding For any given tweet tw , we generate an embedding vector m ∈ Rd . We explored word and sentence level embedding methods to learn tweet representations: Global Vectors for Word Representation (GloVe) (Pennington et al., 2014), Fasttext (Joulin et al., 2017), and Universal Sentence Encoders (USE) (Cer et al., 2018). Empirically, sentence-level embeddings generated using a deep averaging network encoder variant of the USE3 gave us the most promising results. Thus, we encode each tweet tw using USE. Learning Representations for one day On any day i, a variable number tweets [tw1"
2020.emnlp-main.676,N16-1000,0,0.223559,"Missing"
2020.lrec-1.149,J08-4004,0,0.265856,"भी उसे अब फर से नई जान-पहचान करनी पड़ेगी। Maybe it was clear to him that his daughter must have grown up now, and that he would have to reacquaint with her again. The 53 stories contained 10,472 sentences and all sentences were annotated by the three annotators. We evaluated the annotations in terms of inter-annotator agreements using Krippendorff’s alpha (K-alpha) (Krippendorff, 2011) which is more robust than simple agreement measures because it accounts for chance correction and class distributions. We observed strong inter-annotator agreements (K-alpha of 0.87) and per recommendations in (Artstein and Poesio, 2008) conclude that the annotations are of good quality. We chose a straightforward majority decision for label aggregation: if two or more annotators agreed on a discourse mode for a sentence. In cases where there was no agreement between the annotators, they met in person to discuss and assign the final label. 2.4. Dataset statistics Figure 1 shows a distribution of the number of sentences for each discourse mode. There is fair amount of class imbalance in this domain with the most prevalent class Descriptive having 3,954 samples, and the two low prevalence classes (Informative and Argumentative)"
2020.lrec-1.149,Q17-1010,0,0.0206365,"Missing"
2020.lrec-1.149,K18-2017,0,0.0221198,"Missing"
2020.lrec-1.149,W07-1428,0,0.0452706,"t they have already talked about, change of topic and relationship between states, events, beliefs etc, in a given mode of communication (Webber et al., 2012). The discourse structures could be present in the form of a single sentence or span across multiple sentences. Understanding discourse structures and relationships between them in text could be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from writt"
2020.lrec-1.149,J81-2001,0,0.470376,"ut, indicating relationship to what they have already talked about, change of topic and relationship between states, events, beliefs etc, in a given mode of communication (Webber et al., 2012). The discourse structures could be present in the form of a single sentence or span across multiple sentences. Understanding discourse structures and relationships between them in text could be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify diff"
2020.lrec-1.149,P14-2047,1,0.805639,"nd relationship between states, events, beliefs etc, in a given mode of communication (Webber et al., 2012). The discourse structures could be present in the form of a single sentence or span across multiple sentences. Understanding discourse structures and relationships between them in text could be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 of"
2020.lrec-1.149,W16-3617,1,0.84438,"und Discourse in the context of linguistics is defined as exploitation of language features by speakers to express what they are talking about, indicating relationship to what they have already talked about, change of topic and relationship between states, events, beliefs etc, in a given mode of communication (Webber et al., 2012). The discourse structures could be present in the form of a single sentence or span across multiple sentences. Understanding discourse structures and relationships between them in text could be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Racha"
2020.lrec-1.149,W09-3029,0,0.0379755,"ration (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spoken languages in the world1 . In spite of its wide usage it is still considered as one of the low resource languages by NLP practitioners, which necessitates the creation of new resources and tools for computational linguists that enables them to understand the under1 https://en.wikipedia.org/wiki/List_of_ languages_by_number_of_native_speakers lying nuances of the language using natural l"
2020.lrec-1.149,prasad-etal-2008-penn,0,0.104598,"mode of communication (Webber et al., 2012). The discourse structures could be present in the form of a single sentence or span across multiple sentences. Understanding discourse structures and relationships between them in text could be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spo"
2020.lrec-1.149,W11-0414,0,0.0355923,"2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spoken languages in the world1 . In spite of its wide usage it is still considered as one of the low resource languages by NLP practitioners, which necessitates the creation of new resources and tools for computational linguists that enables them to understand the under1 https://en.wikipedia.org/wiki/List_of_ languages_by_number_of_"
2020.lrec-1.149,P17-1011,0,0.0249855,"of these short stories were originally written in Hindi but some of them were written in other Indian languages and later translated to Hindi. We chose against crowd-sourcing the annotation process because we wanted to directly work with the annotators for qualitative feedback and to also ensure high quality annotations. We employed three native Hindi speakers with college level education for the annotation task. We first selected two random stories from our corpus and had the three annotators work on them independently and classify each sentence based on the discourse mode taxonomy used in (Song et al., 2017). Song et al (Song et al., 2017) developed their taxonomy based on prior works in linguistics (Smith, 2003). This preliminary task helped the annotators familiarize themselves with discourse modes and also understand the scope of this annotation task. More importantly, this also helped us ascertain feedback about the class labels. Based on the annotators’ feedback we first observed that of five discourse modes used in (Song et al., 2017), Emotion was extremely prevalent: most of the sentences in these short stories could be associated with some sort of an emotion. We therefore decided to elimi"
2020.lrec-1.149,tonelli-etal-2010-annotation,0,0.0196186,"as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spoken languages in the world1 . In spite of its wide usage it is still considered as one of the low resource languages by NLP practitioners, which necessitates the creation of new resources and tools for computational linguists that enables them to understand the under1 https://en.wikipedia.org/"
2020.lrec-1.149,W16-6307,0,0.0226833,"w resources and tools for computational linguists that enables them to understand the under1 https://en.wikipedia.org/wiki/List_of_ languages_by_number_of_native_speakers lying nuances of the language using natural language processing techniques. The syntax and semantics of Hindi is often different from other high resource languages like English. Dependency of the meaning of expressions on word order, morphological variations, and spelling variations makes Hindi an interesting language to study and also pose additional challenges for linguistic modelling (Kumar et al., 2019). Tripathi et al. (Tripathi et al., 2016), created a Hindi corpus of 1960 sentences extracted from children stories that are annotated with discourse modes for improving storytelling experience using TTS systems. They annotated an already existing story speech corpus (Sarkar et al., 2014) for three discourse modes - dialogue, narrative and descriptive. The main motivation of their work was to develop an automated discourse mode identification system at sentence level that could be further used for enhancing the output of a TTS system by improving the performance of the prosody models. Motivated by the efforts of (Tripathi et al., 201"
2020.lrec-1.149,W10-1844,0,0.0371131,"al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spoken languages in the world1 . In spite of its wide usage it is still considered as one of the low resource languages by NLP practitioners, which necessitates the creation of new resources and tools for computational linguists that enables them to understand the under1 https://en.wikipedia.org/wiki/List_of_ languages_by_number_of_native_speakers lying nuances o"
2020.lrec-1.149,P12-1008,0,0.027183,"ld be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spoken languages in the world1 . In spite of its wide usage it is still considered as one of the low resource languages by NLP practitioners, which necessitates the creation of new resources and tools for computational linguists that e"
2020.semeval-1.219,N19-1423,0,0.00786907,"x2 x3 ... xn Embeddings Emphasis Selection using Outputs Label Figure 1: BiLSTM architecture used for emphasis selection. 2 Methods Let d = {w1 , w2 , ..., wn } be the input text, where wi is the ith token. The problem of emphasis selection is to assign each token wi one of two possible labels E = {eI , eO }, where eI denotes emphasis on the token and eO means otherwise. We approach this problem as a sequence labeling task solved using a BiLSTM model. We first represent each token wi with a dense vector xi of a fixed size. To this end, we explore three different embedding architectures: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and XL-NET (Yang et al., 2019). Thus the given input text d is transformed into a sequence of vectors {x1 , x2 , ..., xn }. We then feed these vectors to a BiLSTM model which captures the sequential relations between the tokens. The hidden state of the BiLSTM hi is associated with the token wi . Thus hi provides a fixed-size representation for token wi while incorporating information from the surrounding tokens. In standard sequence prediction problem, we can apply an affine transformation to map hi to the class space. However, in this paper, as with (Shirani et a"
2020.semeval-1.219,P06-1068,0,0.0953821,"Missing"
2020.semeval-1.219,N18-2100,1,0.833793,"ve modeled word-level emphasis using acoustic and prosodic features. Understanding emphasis in speech is critical to many downstream applications such as text-to-speech synthesis (Nakajima et al., 2014), speech-to-speech translation (Do et al., 2015), and computer assisted pronunciation training (Felps et al., 2009). In computational linguistics, emphasis selection is very closely related to the problem of keyphrase extraction (Turney, 2002). Keyphrases typically refer nouns and noun-phrases that capture the most salient topics in long documents such as scientific articles (Sahrawat et al., ; Mahata et al., 2018; Swaminathan et al., 2020), news articles (Hulth and Megyesi, 2006), web pages (Yih et al., 2006), etc. In contrast, emphasis selection deals with very short texts (e.g. social media posts), and also emphasis could be applied to words belonging to various parts of speech. The goal of SemEval 2020 - Task 10 is to design methods for automatic emphasis selection in short texts. To this end, the organizers (Shirani et al., 2020) provided a dataset consisting of over 3,000 sentences annotated for token-level emphasis by multiple annotators. The authors employed the standard I-O tagging schema, whi"
2020.semeval-1.219,Y14-1022,0,0.224721,"color, font, or other typographic features. This type of emphasis can help with expressing an intent, providing more clarity, or drawing attention towards specific information in the text. Automatic emphasis selection is therefore useful in graphic design and presentation applications to assist users with appropriate choice of text layout. Prior works in speech processing (Mishra et al., 2012; Chen and Pan, 2017) have modeled word-level emphasis using acoustic and prosodic features. Understanding emphasis in speech is critical to many downstream applications such as text-to-speech synthesis (Nakajima et al., 2014), speech-to-speech translation (Do et al., 2015), and computer assisted pronunciation training (Felps et al., 2009). In computational linguistics, emphasis selection is very closely related to the problem of keyphrase extraction (Turney, 2002). Keyphrases typically refer nouns and noun-phrases that capture the most salient topics in long documents such as scientific articles (Sahrawat et al., ; Mahata et al., 2018; Swaminathan et al., 2020), news articles (Hulth and Megyesi, 2006), web pages (Yih et al., 2006), etc. In contrast, emphasis selection deals with very short texts (e.g. social media"
2020.semeval-1.219,P19-1112,0,0.464389,"ent the underlying text with various contextual embedding models. We also employ label distribution learning to account for annotator disagreements. We experiment with the choice of model architectures, trainability of layers, and different contextual embeddings. Our best performing architecture is an ensemble of different models, which achieved an overall matching score of 0.783, placing us 15th out of 31 participating teams. Lastly, we analyze the results in terms of parts of speech tags, sentence lengths, and word ordering. 1 Introduction Emphasis selection is an emerging research problem (Shirani et al., 2019) in the natural language processing domain, which involves automatic identification of words or phrases from a short text that would serve as good candidates for visual emphasis. This research is most relevant to visual media such as flyers, posters, ads, and motivational messages where certain words or phrases can be visually emphasized with the use of different color, font, or other typographic features. This type of emphasis can help with expressing an intent, providing more clarity, or drawing attention towards specific information in the text. Automatic emphasis selection is therefore use"
2020.semeval-1.219,2020.semeval-1.184,0,0.038936,"Missing"
2021.acl-long.526,N19-1423,0,0.0156981,"rresponding text fragment. Aeneas uses the Sakoe-Chiba Band Dynamic Time Warping (DTW) (Sakoe and Chiba, 1978) forced alignment algorithm, which has been proven to improve discrimination between words and has superior performance over other conventional algorithms. 5 Methodology 5.1 Text and Audio Encoding Text Encoding We compute an utterance’s textual encoding as the arithmetic mean of all its word vectors. BERT is well known as an effective pretrained language-based model for extracting wordembeddings (Biswas et al., 2020) for a variety of language modeling tasks. We use Uncased Base BERT (Devlin et al., 2019) to extract the word embeddings. For each call, we represent the text utterances as [t1 , t2 , . . . , tN ]. As seen from Figure 4, we embed each text utterance ti to get its corresponding 768-dimensional text encoding gi using BERT such that gi = BERT(ti ) for each i ∈ [1, N ]. Audio Encoding We use the OpenSMILE8 library to extract the audio features at a sampling rate of 10ms and choose the set of 62 geMAPS features described in (Eyben et al., 2016). This set includes features like pitch, jitter, loudness, etc., which have proven to be effective in audio analysis tasks (Chao et al., 2015)."
2021.acl-long.526,N09-1031,0,0.291606,"d by utterances and aligned with the audio. • We accompany the dataset with neural baseline architectures that use the multimodal multi-speaker input to predict stock volatility and price movement. • To the best of our knowledge, no such M&A conference call dataset exists in academia, and our proposed methodology, M3ANet is the first deep learning approach for financial predictions on M&A conference calls. 2 Related Work M&A Conference Calls Financial reports and conference calls have been shown to have a correlation with the stock market and improve financial predictions (Bowen et al., 2001; Kogan et al., 2009). Studies have also been carried out specifically for M&A calls, showing their effect on the market (Dasgupta et al., 2020; Hu et al., 2018). However, there exists a gap in leveraging neural predictive modeling on using verbal and vocal cues pertaining to M&A calls for financial forecasting. 6752 Financial Forecasting Research has shown historical pricing data to be useful in predicting financial risk modeling (Kristjanpoller et al., 2014; Zheng et al., 2019; Dumas et al., 2009). It also considers volatility as an indicator of uncertainty, which helps make decisions regarding investments (Hest"
2021.acl-long.526,A88-1002,0,0.323951,"Missing"
2021.acl-long.526,2020.emnlp-main.643,1,0.873266,"Missing"
2021.acl-long.526,D14-1162,0,0.0923349,"yer after being passed through the sentence transformer, while σ represents the final activation function and y represents the final prediction from the activation corresponding to the task. We use ReLU for the final prediction in the volatility prediction task and sigmoid for the price prediction task. We then use Mean Squared Error (MSE) and Binary Cross-Entropy (BCE) losses to train the output for volatility prediction and stock price movement prediction, respectively. 6 Experimental Setup 6.1 We compare M3ANet against modern baselines across modalities for both the tasks. We employ GloVe (Pennington et al., 2014), FinBERT (Araci, 2019) and Roberta (Liu et al., 2019) to embed the text and choose an LSTM + Dense layer architecture as a benchmark for both volatility and price movement prediction. We also use all three (text, audio, and multimodal) variants of the Multimodal Deep Regression Model (MDRM) (Qin and Yang, 2019) as baselines. 6.2 Training Setup We tune M3ANet’s hyper-parameters using Grid Search. We summarize the range of hyperparameters tuned on: size of the transformer’s feedforward layer and size of the linear layers ∈ {16, 32, 64}, dropout δ ∈ {0.0, 0.1, 0.25, 0.5}, batch size b ∈ {32, 64,"
2021.acl-long.526,P19-1038,0,0.210215,"cues play a critical role in verbal communication as they can provide support or discredit the verbal message that is being spoken (Jiang and Pell, 2017). For example, consider if the CEO of the acquiring company exhibits confidence in the statement - ”we are confident that this acquisition will bring us profits,” however, displays nervousness while justifying technical details of the deal, we may infer contradiction in the claims of a successful M&A. Vocal cues have been proven indicators of emotions like deceit and nervousness (Belin et al., 2017; Sporer and Schwandt, 2006). Past research (Qin and Yang, 2019; Sawhney et al., 2020c) shows that the addition of vocal cues has helped with the task of financial predictions and enrich the learned representations. Our contributions can be summarized as: • We curate a public dataset M3A2 (Multimodal Multi-Speaker Merger & Acquisition Call Fi2 The source code, processed features, and details on acquiring raw data are available at https://github.com/ midas-research/m3a-acl nancial Forecasting Dataset) that consists of 816 M&A conference calls spanning over 545 hours between 2016 to 2020 with their transcripts and audio recordings, segmented by utterances a"
2021.acl-long.526,P17-1157,0,0.0324318,"Missing"
2021.acl-long.526,2020.emnlp-main.676,1,0.871043,"l role in verbal communication as they can provide support or discredit the verbal message that is being spoken (Jiang and Pell, 2017). For example, consider if the CEO of the acquiring company exhibits confidence in the statement - ”we are confident that this acquisition will bring us profits,” however, displays nervousness while justifying technical details of the deal, we may infer contradiction in the claims of a successful M&A. Vocal cues have been proven indicators of emotions like deceit and nervousness (Belin et al., 2017; Sporer and Schwandt, 2006). Past research (Qin and Yang, 2019; Sawhney et al., 2020c) shows that the addition of vocal cues has helped with the task of financial predictions and enrich the learned representations. Our contributions can be summarized as: • We curate a public dataset M3A2 (Multimodal Multi-Speaker Merger & Acquisition Call Fi2 The source code, processed features, and details on acquiring raw data are available at https://github.com/ midas-research/m3a-acl nancial Forecasting Dataset) that consists of 816 M&A conference calls spanning over 545 hours between 2016 to 2020 with their transcripts and audio recordings, segmented by utterances and aligned with the au"
2021.acl-long.526,2021.naacl-main.294,1,0.860808,"Missing"
2021.acl-long.526,N10-1000,0,0.254625,"Missing"
2021.acl-long.526,2021.naacl-main.316,1,0.833153,"Missing"
2021.acl-long.526,I13-1097,0,0.0193653,"986; Engle, 1981). On the other hand, we are interested in analyzing multimodal data like text and audio, which can hold completely different information for predictive models. Natural Language Processing and Finance For any system using human interactions to determine financial risk or stock movements, it is necessary to determine the relationship between the various words to determine the speaker’s sentiment. Advances in NLP have been utilized in many approaches to show financial information significantly improving performance in forecasting tasks like volatility and stock price prediction (Wang et al., 2013; Ding et al., 2015; Mittermayer and Knolmayer, 2007). Research has also shown that social media affects the stock market (Bollen et al., 2010; Oliveira et al., 2017; Sawhney et al., 2020a). Machine learning methods using simple bag-of-words features to represent the financial documents used in previous research (Kogan et al., 2009; Rekabsaz et al., 2017) largely ignore the inter-dependencies between the sentences. To fill the gap, recent approaches have moved towards newer models such as transformers (Yang et al., 2020) and reinforcement learning (Sawhney et al., 2021b) over natural language"
2021.acl-long.526,P18-1183,0,0.0220738,"-wise trends, we see that acquisitions are consistently more frequent that mergers every year. Further, we note that mergers see a decreasing trend in the number of utterances and acquisitions have a consistent number of speakers in M&A calls. We also note that acquisitions conference calls seem to be increasing in length as the years progress. We chronologically divide our dataset into a train, validation, and test set in the ratio of 70 : 10 : 20, respectively. Such a split ensures that future data is not used for forecasting past data. 4 4.2 Formalizing price movement prediction Following (Xu and Cohen, 2018), we define price movement yd−τ,d over a period of τ days as a binary classification task. For a given stock, we employ its close price, which can either rise or fall on a day d compared to a previous day d − τ , to formulate the classification task as:  1, pd+τ &gt; pd , y[d−τ,d] = (2) 0, pd+τ ≤ pd Curating M3A: Dataset Creation 4.1 Data Acquisition We curate our dataset, M3A, by acquiring audio records and text transcripts from the Bloomberg Terminal.4 Since the conference calls were reliably available from 2016, we filter and list all M&A calls between 2016 and 2020. To limit the scope, we en"
2021.adaptnlp-1.23,2020.acl-main.692,0,0.094304,"tvya.malik, abdulwaheed1513}@gmail.com {rajivratn}@iiitd.ac.in Analysis (Maas et al., 2011), Question Answering (Zhang et al., 2018), among others. However, they do not investigate the domain invariance of PLM representations from different layers when presented with data from distinct domains. Studying the invariance of PLM representations has been useful in advancing methods for unsupervised domain adaptation. For example, in building domain adaptation models that explicitly reduce the divergence between layers of a neural network (Long et al., 2015; Shen et al., 2018a), for data selection (Aharoni and Goldberg, 2020; Ma et al., 2019) et cetera. Given the importance of PLMs, a glass-box study of the internal robustness of PLM representations is overdue. We thus study these representations, dissecting them layer by layer, to uncover their internal contributions in domain adaptation. Firstly, we use the tools of domain divergence and domain invariance, without subscribing to the performance of a model on any end task. The theory of domain adaptation (Ben-David et al., 2010), shows that reducing H-divergence between two domains results in higher performance in the target domain. Many works have since adopted"
2021.adaptnlp-1.23,W18-0701,0,0.0234151,"Missing"
2021.adaptnlp-1.23,D15-1075,0,0.0357068,"using diagnostic probes. We find that similar layers have similar amounts of linguistic information for data from an unseen domain. 1 Introduction Pretrained Language Models (PLMs) have improved the downstream performance of many natural language understanding tasks on standard data (Devlin et al., 2019).1 Recent works attest to the surprising out-of-the-box robustness of PLMs on out-of-distribution tasks (Hendrycks et al., 2020; Brown et al., 2020; Miller et al., 2020). These works measure robustness in terms of the performance invariance of PLMs on end tasks like Natural Language Inference (Bowman et al., 2015; Williams et al., 2018), Sentiment 1 We borrow the term standard data from (Plank, 2016) to refer to news and web-like text and non-standard data to refer to other text like biomedical and Twitter. 222 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 222–244 April 20, 2021. ©2021 Association for Computational Linguistics • Further, we analyze the robustness in terms of the syntactic and semantic information encoded in the representations across unseen domains and find that similar layers have similar amounts of linguistic information, which is a preliminary exposition of"
2021.adaptnlp-1.23,2020.findings-emnlp.222,0,0.042037,"Missing"
2021.adaptnlp-1.23,W19-1909,0,0.0541812,"Missing"
2021.adaptnlp-1.23,P18-1198,0,0.0575978,"Missing"
2021.adaptnlp-1.23,D11-1033,0,0.0575404,"e divergence between representations from different layers are reduced (Long et al., 2015). Recently, Aharoni and Goldberg (2020) show the final transformer layer representations cluster while Ma et al. (2019) consider penultimate layer representations. The high domain divergence of the upper layers is a plausible explanation for the clustering (Figs. 8 to 13 in Appendix E.). Clustering of representations plays a key role in downstream applications, such as data selection for machine translation and curriculum learning, data points in the source domain closest to the target domain are chosen (Axelrod et al., 2011; Moore and Lewis, 2010). BERT vs. RoBERTa: Compared to BERT, RoBERTa has uniform divergence across layers (c.f. Fig. 1 ). RoBERTa is similar to BERT, but a major difference is the amount of pre-training data used (one magnitude; 160GB vs. 16GB). We speculate that the domain-invariance is because the pretraining data is an unintended mixture of different domains. Recent works have shown the impact of training models with large and diverse datasets on the robustness of image classification models (Taori et al., 2020) and text classification models (Tu et al., 2020) with similar trends observed"
2021.adaptnlp-1.23,R13-1026,0,0.0116282,"framework (Wang et al., 2019) . Data: Following (Tenney et al., 2019b) we use the OntoNotes 5.0 corpus (Weischedel, Ralph et al., 2013) for probing. Since they are from newswire and web text, which is similar to the pretraining corpus of BERT (Devlin et al., 2019), we consider this dataset as standard data (source domain). We choose Twitter to represent non-standard data (target domain) for the probing task since our previous experiments showed a greater divergence, and thus are significantly different from the pretraining corpus used in BERT. For POS tagging, we use the dataset described by (Derczynski et al., 2013). We remove the following POS tags from the dataset: USR, URL, HT, RT, “(”, and “)”, to normalize the labels across the domains. For NER, we use the dataset released for the shared task of the Workshop on Noisy Usergenerated Text (W-NUT) (Baldwin et al., 2015). For coreference resolution, we use the dataset presented in (Aktas¸ et al., 2018), whose annotations were later modified by (Aktas¸ et al., 2020) so that they were conceptually parallel to OntoNotes 5.0 corpus (Weischedel, Ralph et al., 2013). The size of the datasets across train, development and test splits were kept similar for both"
2021.adaptnlp-1.23,W15-4319,0,0.0355881,"Missing"
2021.adaptnlp-1.23,N19-1423,0,0.518877,"main-specific data (DAPT) (Gururangan et al., 2020) exhibit more variance than their pretrained PLM counterparts; and that iii) Distilled models (e.g.,DistilBERT) also show greater domain variance. Second, we investigate the robustness of representations by analyzing the encoded syntactic and semantic information using diagnostic probes. We find that similar layers have similar amounts of linguistic information for data from an unseen domain. 1 Introduction Pretrained Language Models (PLMs) have improved the downstream performance of many natural language understanding tasks on standard data (Devlin et al., 2019).1 Recent works attest to the surprising out-of-the-box robustness of PLMs on out-of-distribution tasks (Hendrycks et al., 2020; Brown et al., 2020; Miller et al., 2020). These works measure robustness in terms of the performance invariance of PLMs on end tasks like Natural Language Inference (Bowman et al., 2015; Williams et al., 2018), Sentiment 1 We borrow the term standard data from (Plank, 2016) to refer to news and web-like text and non-standard data to refer to other text like biomedical and Twitter. 222 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 222–244 Apri"
2021.adaptnlp-1.23,2020.findings-emnlp.148,0,0.0131892,"esentations. Next, to analyze whether training with larger data scale aids in robustness, we consider RoBERTa (Liu et al., 2019b), which is similar to BERT, but trained on a magnitude larger standard data. Further, we check the effect of distillation on domaininvariance and hence, consider DistilBERT (Sanh et al., 2019). Finally, training of models on domain-specific data is known to increase their performance on domain-specific tasks. To analyze the effect of continued fine-tuning on invariance, we consider RoBERTa pretrained on non-standard Biomedical (Gururangan et al., 2020), and Twitter (Barbieri et al., 2020) domain data. We refer to this as DAPT-biomed and DAPT-tweet, respectively. For our experiments, we use the models hosted on the huggingface-transformer library (Wolf et al., 2020). Divergence Measures. We consider three different divergence measures that are widely used in the unsupervised domain adaptation literature. Correlation Alignment (CORAL) measures the difference between covariance of features – a second-order moment. Sun and Saenko (2016) reduce the distributional distance between features for unsupervised domain adaptation (UDA) in computer vision models. In contrast to CORAL, Cent"
2021.adaptnlp-1.23,D19-1371,0,0.0149487,"is non-trivial (Gretton et al., 2012b). We confirm this by plotting the PCA representations of these data points (Figs. 8 to 13 in Appendix E.), which show that the representations from the two domains are interspersed in the lower layers and separated in the upper layers, as done in many previous works (Ganin et al., 2016; Long et al., 2015). We further quantify this by performing k-means clustering where 3.3 What happens to the domain-invariance of DAPT models? To create domain-specific PLM, the simplest methods train models from scratch on domainspecific data like scientific publications (Beltagy et al., 2019), BioBERT (Lee et al., 2019), ClinicalBERT (Alsentzer et al., 2019) among others. In 225 (a) CORAL (b) CMD Figure 2: Comparing CORAL and CMD divergences for roberta-base and DAPT-biomed (Gururangan et al., 2020). Considers standard and biomedical samples. main). DistilBERT contains half the number of layers compared to BERT. At a comparable layers, DistilBERT always has higher divergence values for both CMD and CORAL. Sanh et al. (2019) show that distillation loss that mimics the teacher’s output and cosine embedding loss which aligns the student and teacher hidden states vectors, are the majo"
2021.adaptnlp-1.23,P18-1031,0,0.0281242,"8 0.092 0.695 9 0.164 0.753 10 0.312 0.772 11 0.63 0.762 12 0.588 0.68 Table 1: NMI values measuring the clustering performance at different layers of bert-base-uncased. k = 2 (the number of domains). We evaluate the clusters using Normalized Mutual Information (c.f. Table 1). Clustering quality is higher for upper layers compared to lower layers where representations are interspersed. The increasing divergence across layers has plausible implications in making decisions in many scenarios. For example, in deciding the number of layers in the gradual unfreezing of layers in transfer learning (Howard and Ruder, 2018), in unsupervised domain adaptation where divergence between representations from different layers are reduced (Long et al., 2015). Recently, Aharoni and Goldberg (2020) show the final transformer layer representations cluster while Ma et al. (2019) consider penultimate layer representations. The high domain divergence of the upper layers is a plausible explanation for the clustering (Figs. 8 to 13 in Appendix E.). Clustering of representations plays a key role in downstream applications, such as data selection for machine translation and curriculum learning, data points in the source domain c"
2021.adaptnlp-1.23,P19-1356,0,0.0793364,"also consider only one target domain — Twitter — and analyze bert-base-uncased for our probes. The availability and varying characteristics of the dataset across domains dictates our choice. For example, compared to standard coreference, biomedical text exhibits co-referring terms across sentences in long documents. Concerning which part of the model encodes syntactic information required for POS and NER, we observe that the middle layers perform the best for both tasks, invariant of domain. This result is consistent with the results reported for nondomain adaptation work (Liu et al., 2019a; Jawahar et al., 2019). For Coref, the upper layers perform better on the task for the source domain. This indicates that the models store the information required for Coref (Liu et al., 2019a), but that the lower layers perform better when it comes to the target domain. We speculate that this is due to the nature of Twitter-coref dataset (target domain). For tasks like coreference resolution, there is a need for the presence of semantic information to identify the co-referring entities. But as tweets are naturally shorter, they contain co-referring entities that are close to each other, and do not require long-ran"
2021.adaptnlp-1.23,2020.acl-main.740,0,0.0296054,"Missing"
2021.adaptnlp-1.23,2020.findings-emnlp.372,0,0.0369331,"d (Gururangan et al., 2020). Considers standard and biomedical samples. main). DistilBERT contains half the number of layers compared to BERT. At a comparable layers, DistilBERT always has higher divergence values for both CMD and CORAL. Sanh et al. (2019) show that distillation loss that mimics the teacher’s output and cosine embedding loss which aligns the student and teacher hidden states vectors, are the major contributors to the student’s performance. Yet, we find that DistilBERT still has greater variance which may affect downstream tasks like text classification. Although a few models (Jiao et al., 2020; Sanh et al., 2019) reduce some notion of geometric distance between the intermediate representations of the student and the teacher, it does not guarantee that the entire linguistic knowledge and the domain-invariance of the teacher are transferred to the student model. Recent work in NLP have tried to incorporate rich information from teacher networks using contrastive learning (Tian et al., 2020; Sun et al., 2020) and by reducing the Earth Mover’s distance between the hidden representations in the transformer architecture (Li et al., 2020). Related computer vision work also to impart adver"
2021.adaptnlp-1.23,2020.tacl-1.40,0,0.157226,"target domain are chosen (Axelrod et al., 2011; Moore and Lewis, 2010). BERT vs. RoBERTa: Compared to BERT, RoBERTa has uniform divergence across layers (c.f. Fig. 1 ). RoBERTa is similar to BERT, but a major difference is the amount of pre-training data used (one magnitude; 160GB vs. 16GB). We speculate that the domain-invariance is because the pretraining data is an unintended mixture of different domains. Recent works have shown the impact of training models with large and diverse datasets on the robustness of image classification models (Taori et al., 2020) and text classification models (Tu et al., 2020) with similar trends observed where RoBERTa is more robust. We make forward passes of 1000 samples from one pair of domains (standard–biomedical / standard–twitter) separately through the transformers, obtaining two sets of representations. We then use these to calculate divergence measures. We consider the representations of [CLS] token as the representation of a sentence, as done in other works. Note that we do not fine-tune any of our models on the non-standard data. 3.2 Results Across Layers: Overall, the divergence measures increase from the lower layers to the upper layers (Figure 1). CO"
2021.adaptnlp-1.23,2020.emnlp-main.36,0,0.0353442,"tributors to the student’s performance. Yet, we find that DistilBERT still has greater variance which may affect downstream tasks like text classification. Although a few models (Jiao et al., 2020; Sanh et al., 2019) reduce some notion of geometric distance between the intermediate representations of the student and the teacher, it does not guarantee that the entire linguistic knowledge and the domain-invariance of the teacher are transferred to the student model. Recent work in NLP have tried to incorporate rich information from teacher networks using contrastive learning (Tian et al., 2020; Sun et al., 2020) and by reducing the Earth Mover’s distance between the hidden representations in the transformer architecture (Li et al., 2020). Related computer vision work also to impart adversarial robustness, even in the student network (Goldblum et al., 2020). The benefits of such enhanced distillation techniques on the robustness of the model is an under-explored area. contrast, instead of pretraining from scratch, recent work shows impressive benefits of continuing to pretrain on domain-specific data — termed domain adaptive pretraining (DAPT) (Gururangan et al., 2020). Although there are improvements"
2021.adaptnlp-1.23,2020.findings-emnlp.125,0,0.0226931,"on PLM robustness use the notion of performance drop in a new target domain (Hendrycks et al., 2020; Tu et al., 2020; Miller et al., 2020). However, analyzing the robustness and invariance of the representations under data from different domains or adversarial examples (Zhu et al., 2020) has not received much attention thus far in domain adaptation. Concerning the robustness of linguistic information stored in representations, Merchant et al. (2020) analyze the syntactic and semantic information preserved by PLMs, both before and after fine-tuning the models on task-specific data. Similarly, Tamkin et al. (2020) analyze the role of different layers in transfer learning on end tasks. Different from their study, we are interested in the intrinsic invariance of the PLM representations under data from different domains. 6.2 Unsupervised Domain Adaptation For unsupervised domain adaptation, a popular method is use the adversarial training between a domain and a task classifier (DANN) (Ganin et al., 2016). Compared to DANN, where domainspecific peculiarities are lost, (Bousmalis et al., 2016) introduce domain-specific networks, which where domain-specific and domain-invariant representations are formed in"
2021.adaptnlp-1.23,N18-1101,0,0.078309,"Missing"
2021.adaptnlp-1.23,P19-1452,0,0.321109,"rained language models still encode for data from a different domain? Here, we evaluate the robustness of representations in bert-base-uncased. Do word-level repre226 (a) CORAL (b) CMD Figure 3: CORAL and CMD divergence measures for standard vs biomedical samples. Two encoders are considered here: bert-base-uncased and distilbert-base. where Coreference resolution (Coref) is considered a semantic task. We chose these tasks guided by the availability of similar datasets in both domains. For all our experiments involving probing, we use the jiant framework (Wang et al., 2019) . Data: Following (Tenney et al., 2019b) we use the OntoNotes 5.0 corpus (Weischedel, Ralph et al., 2013) for probing. Since they are from newswire and web text, which is similar to the pretraining corpus of BERT (Devlin et al., 2019), we consider this dataset as standard data (source domain). We choose Twitter to represent non-standard data (target domain) for the probing task since our previous experiments showed a greater divergence, and thus are significantly different from the pretraining corpus used in BERT. For POS tagging, we use the dataset described by (Derczynski et al., 2013). We remove the following POS tags from the"
2021.adaptnlp-1.23,2020.emnlp-main.239,0,0.0302919,"PubMed abstracts2 and for the Twitter domain, we sample 5000 tweets from the year 2011 made available by the archive team.3 We follow the same procedure as Nguyen et al. (2020) to preprocess tweets: we use fastText (Joulin et al., 2017) to consider only English tweets and use the emoji package 4 to translate emojis into text strings, normalize all the user mentions to @USER and URLs to HTTPURL. How Domain-Invariant are PLM Representations? Most of the current techniques in unsupervised domain adaptation explicitly reduce the divergence between different layer representations during training (Yu et al., 2020). A common posthoc analysis from such works shows the reduction of domain invariance at different layers. However, they do not pay much heed to the domaininvariance of representations that already exist in such models prior to domain-adapted training. 2 https://www.nlm.nih.gov/databases/download/ pubmed medline.html 3 https://archive.org/details/twitterstream 4 https://pypi.org/project/emoji 224 Layer NMI (standard-biomed) NMI (standard-twitter) 1 0.004 0.256 2 0.365 0.252 3 0.046 0.215 4 0.63 0.233 5 0.722 0.698 6 0.596 0.699 7 0.245 0.727 8 0.092 0.695 9 0.164 0.753 10 0.312 0.772 11 0.63 0."
2021.eacl-main.185,D19-5106,0,0.0198101,"These include discrete (Bollerslev, 1986), continuous (Andersen, 2007), and neural approaches (Zhang et al., 2017; Feng et al., 2019a). Despite their success, a limitation of these methods is that they are limited to numerical features and do not factor crucial stock influencing factors such as text (Lee et al., 2014). Contemporary Methods: Newer models based on the EMH, leverage natural language features extracted from investor sentiments (Li and Shah, 2017), financial reports (Kogan et al., 2009; Rekabsaz et al., 2017), earnings calls (Qin and Yang, 2019), online news (Peng and Jiang, 2016; Chen et al., 2019a,b; Du and Tanaka-Ishii, 2020) and social media (Si et al., 2013; Tabari et al., 2018; Sawhney et al., 2020a) for stock price regression and movement classification tasks. These methods show how NLP can complement conventional price-based methods in capturing the effect of events like market surprises and mergers over stock returns. However, these methods do not directly optimize profit, and do not factor the fine-grain irregularities in release times of stock affecting text. For stock trading, the timing of release of information across these sources plays a critical role, as price changes r"
2021.eacl-main.185,D19-5105,0,0.0485779,"Missing"
2021.eacl-main.185,N19-1423,0,0.00698682,"T as shown in Figure 3, for hierarchically and attentively learning time-aware representations of news and tweets within (§3.2) and across (§3.3) the days in the lookback period. Lastly, we optimize FAST to rank stocks in terms of expected profitability (§3.4) for daily stock trading. 3.2 Intra-Day Textual Information Encoder To model the news or tweets over a day, FAST first encodes the texts via an embedding layer. Text Embedding Layer: Owing to the success of transfer learning and pre-training of language models in NLP, we use Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) to encode the texts. BERT has shown to capture more contextual text representations as opposed to methods like word2vec (Hu et al., 2017), GloVe (Xu and Cohen, 2018), ELMO (Mohammadi et al., 2019). We encode each text t to a higher dimensional representation m = BERT(t) ∈ Rd where d = 768, obtained by averaging the token level outputs from the final layer of BERT. Learning Stock Representations for One Day: For each stock s on any given day i, a variable number of texts (news or tweets) [t1 , t2 , . . . tK ] are posted at times [k1 , k2 , . . . kK ] that may discuss news or express sentiments"
2021.eacl-main.185,2020.acl-main.307,0,0.302101,"(Bollerslev, 1986), continuous (Andersen, 2007), and neural approaches (Zhang et al., 2017; Feng et al., 2019a). Despite their success, a limitation of these methods is that they are limited to numerical features and do not factor crucial stock influencing factors such as text (Lee et al., 2014). Contemporary Methods: Newer models based on the EMH, leverage natural language features extracted from investor sentiments (Li and Shah, 2017), financial reports (Kogan et al., 2009; Rekabsaz et al., 2017), earnings calls (Qin and Yang, 2019), online news (Peng and Jiang, 2016; Chen et al., 2019a,b; Du and Tanaka-Ishii, 2020) and social media (Si et al., 2013; Tabari et al., 2018; Sawhney et al., 2020a) for stock price regression and movement classification tasks. These methods show how NLP can complement conventional price-based methods in capturing the effect of events like market surprises and mergers over stock returns. However, these methods do not directly optimize profit, and do not factor the fine-grain irregularities in release times of stock affecting text. For stock trading, the timing of release of information across these sources plays a critical role, as price changes rapidly factor all publicly avai"
2021.eacl-main.185,D15-1166,0,0.0582991,"Missing"
2021.eacl-main.185,N09-1031,0,0.0619954,"97; Lin et al., 2009), technical (Shynkevich et al., 2017), and macroeconomic indicators (Hoseinzade et al., 2019). These include discrete (Bollerslev, 1986), continuous (Andersen, 2007), and neural approaches (Zhang et al., 2017; Feng et al., 2019a). Despite their success, a limitation of these methods is that they are limited to numerical features and do not factor crucial stock influencing factors such as text (Lee et al., 2014). Contemporary Methods: Newer models based on the EMH, leverage natural language features extracted from investor sentiments (Li and Shah, 2017), financial reports (Kogan et al., 2009; Rekabsaz et al., 2017), earnings calls (Qin and Yang, 2019), online news (Peng and Jiang, 2016; Chen et al., 2019a,b; Du and Tanaka-Ishii, 2020) and social media (Si et al., 2013; Tabari et al., 2018; Sawhney et al., 2020a) for stock price regression and movement classification tasks. These methods show how NLP can complement conventional price-based methods in capturing the effect of events like market surprises and mergers over stock returns. However, these methods do not directly optimize profit, and do not factor the fine-grain irregularities in release times of stock affecting text. For"
2021.eacl-main.185,lee-etal-2014-importance,0,0.0223536,"on spans various methods, commonly framed as regression or classification tasks (Jiang, 2020). Conventional methods rely on numeric features like historical prices (Kohara et al., 1997; Lin et al., 2009), technical (Shynkevich et al., 2017), and macroeconomic indicators (Hoseinzade et al., 2019). These include discrete (Bollerslev, 1986), continuous (Andersen, 2007), and neural approaches (Zhang et al., 2017; Feng et al., 2019a). Despite their success, a limitation of these methods is that they are limited to numerical features and do not factor crucial stock influencing factors such as text (Lee et al., 2014). Contemporary Methods: Newer models based on the EMH, leverage natural language features extracted from investor sentiments (Li and Shah, 2017), financial reports (Kogan et al., 2009; Rekabsaz et al., 2017), earnings calls (Qin and Yang, 2019), online news (Peng and Jiang, 2016; Chen et al., 2019a,b; Du and Tanaka-Ishii, 2020) and social media (Si et al., 2013; Tabari et al., 2018; Sawhney et al., 2020a) for stock price regression and movement classification tasks. These methods show how NLP can complement conventional price-based methods in capturing the effect of events like market surprise"
2021.eacl-main.185,K17-1031,0,0.0190091,"ke historical prices (Kohara et al., 1997; Lin et al., 2009), technical (Shynkevich et al., 2017), and macroeconomic indicators (Hoseinzade et al., 2019). These include discrete (Bollerslev, 1986), continuous (Andersen, 2007), and neural approaches (Zhang et al., 2017; Feng et al., 2019a). Despite their success, a limitation of these methods is that they are limited to numerical features and do not factor crucial stock influencing factors such as text (Lee et al., 2014). Contemporary Methods: Newer models based on the EMH, leverage natural language features extracted from investor sentiments (Li and Shah, 2017), financial reports (Kogan et al., 2009; Rekabsaz et al., 2017), earnings calls (Qin and Yang, 2019), online news (Peng and Jiang, 2016; Chen et al., 2019a,b; Du and Tanaka-Ishii, 2020) and social media (Si et al., 2013; Tabari et al., 2018; Sawhney et al., 2020a) for stock price regression and movement classification tasks. These methods show how NLP can complement conventional price-based methods in capturing the effect of events like market surprises and mergers over stock returns. However, these methods do not directly optimize profit, and do not factor the fine-grain irregularities in rel"
2021.eacl-main.185,S19-2023,0,0.0174432,"mize FAST to rank stocks in terms of expected profitability (§3.4) for daily stock trading. 3.2 Intra-Day Textual Information Encoder To model the news or tweets over a day, FAST first encodes the texts via an embedding layer. Text Embedding Layer: Owing to the success of transfer learning and pre-training of language models in NLP, we use Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) to encode the texts. BERT has shown to capture more contextual text representations as opposed to methods like word2vec (Hu et al., 2017), GloVe (Xu and Cohen, 2018), ELMO (Mohammadi et al., 2019). We encode each text t to a higher dimensional representation m = BERT(t) ∈ Rd where d = 768, obtained by averaging the token level outputs from the final layer of BERT. Learning Stock Representations for One Day: For each stock s on any given day i, a variable number of texts (news or tweets) [t1 , t2 , . . . tK ] are posted at times [k1 , k2 , . . . kK ] that may discuss news or express sentiments towards the stock. We encode each of the K texts posted in a day, using 2166 BERT as [m1 , m2 , . . . mK ]. Often, analyzing a single text alone may not be informative enough to analyze a stock (B"
2021.eacl-main.185,P15-1131,0,0.530878,"the method to get a ranked list of the predicted return ratio for each stock. The trader then buys the top η stocks and then sells the bought stocks on the market close of the trading day τ . The IRR on any day τ is deP pτ −pτi −1 fined as, IRRτ = i∈S τ −1 i pτ −1 where, S τ −1 wavelet transform of historic prices (Bao et al., 2017). Classification (CLF) The following methods classify movements as [up, down, neutral] and trade the stocks where prices are expected to rise. • TSLDA: Topic Sentiment Latent Dirichlet Allocation, a generative model jointly exploiting topics and sentiments in text (Nguyen and Shirai, 2015). • CH-RNN: An RNN with cross-modal attention on price trends and texts across days (Wu et al., 2018). i denotes the set of stocks in the portfolio on day τ −1 and pτi , pτi −1 are the closing price of the stock i on days τ and τ − 1 respectively. We calculate SR by computing the earned return Ra in excess of the E[R −R ] risk-free return8 Rf , defined as: SR = std[Raa −Rff ] . Ranking: We also evaluate the stock ranking ability of FAST using two widely-used ranking metrics: Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG@η). MRR is the reciprocal rank of the first r"
2021.eacl-main.185,N16-1041,0,0.0164791,"inzade et al., 2019). These include discrete (Bollerslev, 1986), continuous (Andersen, 2007), and neural approaches (Zhang et al., 2017; Feng et al., 2019a). Despite their success, a limitation of these methods is that they are limited to numerical features and do not factor crucial stock influencing factors such as text (Lee et al., 2014). Contemporary Methods: Newer models based on the EMH, leverage natural language features extracted from investor sentiments (Li and Shah, 2017), financial reports (Kogan et al., 2009; Rekabsaz et al., 2017), earnings calls (Qin and Yang, 2019), online news (Peng and Jiang, 2016; Chen et al., 2019a,b; Du and Tanaka-Ishii, 2020) and social media (Si et al., 2013; Tabari et al., 2018; Sawhney et al., 2020a) for stock price regression and movement classification tasks. These methods show how NLP can complement conventional price-based methods in capturing the effect of events like market surprises and mergers over stock returns. However, these methods do not directly optimize profit, and do not factor the fine-grain irregularities in release times of stock affecting text. For stock trading, the timing of release of information across these sources plays a critical role,"
2021.eacl-main.185,P19-1038,0,0.0143364,"and macroeconomic indicators (Hoseinzade et al., 2019). These include discrete (Bollerslev, 1986), continuous (Andersen, 2007), and neural approaches (Zhang et al., 2017; Feng et al., 2019a). Despite their success, a limitation of these methods is that they are limited to numerical features and do not factor crucial stock influencing factors such as text (Lee et al., 2014). Contemporary Methods: Newer models based on the EMH, leverage natural language features extracted from investor sentiments (Li and Shah, 2017), financial reports (Kogan et al., 2009; Rekabsaz et al., 2017), earnings calls (Qin and Yang, 2019), online news (Peng and Jiang, 2016; Chen et al., 2019a,b; Du and Tanaka-Ishii, 2020) and social media (Si et al., 2013; Tabari et al., 2018; Sawhney et al., 2020a) for stock price regression and movement classification tasks. These methods show how NLP can complement conventional price-based methods in capturing the effect of events like market surprises and mergers over stock returns. However, these methods do not directly optimize profit, and do not factor the fine-grain irregularities in release times of stock affecting text. For stock trading, the timing of release of information across t"
2021.eacl-main.185,P17-1157,0,0.041048,"Missing"
2021.eacl-main.185,2020.emnlp-main.676,1,0.757369,"l., 2017; Feng et al., 2019a). Despite their success, a limitation of these methods is that they are limited to numerical features and do not factor crucial stock influencing factors such as text (Lee et al., 2014). Contemporary Methods: Newer models based on the EMH, leverage natural language features extracted from investor sentiments (Li and Shah, 2017), financial reports (Kogan et al., 2009; Rekabsaz et al., 2017), earnings calls (Qin and Yang, 2019), online news (Peng and Jiang, 2016; Chen et al., 2019a,b; Du and Tanaka-Ishii, 2020) and social media (Si et al., 2013; Tabari et al., 2018; Sawhney et al., 2020a) for stock price regression and movement classification tasks. These methods show how NLP can complement conventional price-based methods in capturing the effect of events like market surprises and mergers over stock returns. However, these methods do not directly optimize profit, and do not factor the fine-grain irregularities in release times of stock affecting text. For stock trading, the timing of release of information across these sources plays a critical role, as price changes rapidly factor all publicly available information (Norman, 2014). Firms may exploit investors’ perception of"
2021.eacl-main.185,2020.emnlp-main.619,1,0.860886,"l., 2017; Feng et al., 2019a). Despite their success, a limitation of these methods is that they are limited to numerical features and do not factor crucial stock influencing factors such as text (Lee et al., 2014). Contemporary Methods: Newer models based on the EMH, leverage natural language features extracted from investor sentiments (Li and Shah, 2017), financial reports (Kogan et al., 2009; Rekabsaz et al., 2017), earnings calls (Qin and Yang, 2019), online news (Peng and Jiang, 2016; Chen et al., 2019a,b; Du and Tanaka-Ishii, 2020) and social media (Si et al., 2013; Tabari et al., 2018; Sawhney et al., 2020a) for stock price regression and movement classification tasks. These methods show how NLP can complement conventional price-based methods in capturing the effect of events like market surprises and mergers over stock returns. However, these methods do not directly optimize profit, and do not factor the fine-grain irregularities in release times of stock affecting text. For stock trading, the timing of release of information across these sources plays a critical role, as price changes rapidly factor all publicly available information (Norman, 2014). Firms may exploit investors’ perception of"
2021.eacl-main.185,P13-2005,0,0.0295137,"07), and neural approaches (Zhang et al., 2017; Feng et al., 2019a). Despite their success, a limitation of these methods is that they are limited to numerical features and do not factor crucial stock influencing factors such as text (Lee et al., 2014). Contemporary Methods: Newer models based on the EMH, leverage natural language features extracted from investor sentiments (Li and Shah, 2017), financial reports (Kogan et al., 2009; Rekabsaz et al., 2017), earnings calls (Qin and Yang, 2019), online news (Peng and Jiang, 2016; Chen et al., 2019a,b; Du and Tanaka-Ishii, 2020) and social media (Si et al., 2013; Tabari et al., 2018; Sawhney et al., 2020a) for stock price regression and movement classification tasks. These methods show how NLP can complement conventional price-based methods in capturing the effect of events like market surprises and mergers over stock returns. However, these methods do not directly optimize profit, and do not factor the fine-grain irregularities in release times of stock affecting text. For stock trading, the timing of release of information across these sources plays a critical role, as price changes rapidly factor all publicly available information (Norman, 2014)."
2021.eacl-main.185,W18-3102,0,0.0440309,"pproaches (Zhang et al., 2017; Feng et al., 2019a). Despite their success, a limitation of these methods is that they are limited to numerical features and do not factor crucial stock influencing factors such as text (Lee et al., 2014). Contemporary Methods: Newer models based on the EMH, leverage natural language features extracted from investor sentiments (Li and Shah, 2017), financial reports (Kogan et al., 2009; Rekabsaz et al., 2017), earnings calls (Qin and Yang, 2019), online news (Peng and Jiang, 2016; Chen et al., 2019a,b; Du and Tanaka-Ishii, 2020) and social media (Si et al., 2013; Tabari et al., 2018; Sawhney et al., 2020a) for stock price regression and movement classification tasks. These methods show how NLP can complement conventional price-based methods in capturing the effect of events like market surprises and mergers over stock returns. However, these methods do not directly optimize profit, and do not factor the fine-grain irregularities in release times of stock affecting text. For stock trading, the timing of release of information across these sources plays a critical role, as price changes rapidly factor all publicly available information (Norman, 2014). Firms may exploit inv"
2021.eacl-main.185,P18-1183,0,0.395647,"often reported across financial news and social media, have shown to influence market dynamics (Laakkonen, 2004). As shown in Figure 1, prices immediately react to breaking news about the related company. Such reactions conform to the Efficient Market Hypothesis (EMH), which states that financial markets are informationally efficient and prices reflect all available information (Malkiel, 1989). The abundance of stock affecting information across news and Twitter helps investors analyze market trends and inspires the adoption of NLP to study the interplay between textual data and stock prices (Xu and Cohen, 2018; Oliveira et al., 2017). However, unlike structured numerical data, analyzing natural language poses various challenges. First, analyzing individual tweets or news headlines may not be informative enough. They often exhibit a sequential context-dependency, where analyzing them together can provide a greater unified context, 2164 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2164–2175 April 19 - 23, 2021. ©2021 Association for Computational Linguistics á Figure 2: More accurate methods M1 (higher accuracy) may not always be m"
2021.eacl-main.205,P17-1067,0,0.030009,"r historical emotional context in a temporal phaseStudies show that emotions expressed in suicidal tweets are correlated with suicidal behavior (Sueki, 2015; Spates et al., 2018; Zhang et al., 2017). As a building block, we utilize Plutchik’s wheel of emotions (Plutchik, 1980) to capture the emotions expressed by a user in their tweets. Plutchik’s wheel outlines eight primary emotions arranged as four pairs of opposing dualities: Joy - Sadness, Surprise - Anticipation, Anger - Fear, and Trust - Disgust. We utilize Plutchik Transformer (Sawhney et al., 2020), a BERT model fine-tuned on Emonet (Abdul-Mageed and Ungar, 2017), a dataset of 790,059 tweets labeled across 8 primary emotions as per Plutchik’s wheel of emotions. Owing to the success of pre-training language models in NLP, Plutchik Transformer jointly learns textual and emotion features for representation learning of user tweets for subsequent suicidal intent detection. We extract a 768-dimension encoding from the [CLS]3 token of the penultimate transformer layer, which is densely connected with an 8-dimensional output layer representative of each primary emotion. 3 Empirically, the [CLS] token performed better than taking the average of the output vect"
2021.eacl-main.205,N19-1151,0,0.0600687,"Missing"
2021.eacl-main.205,W14-3207,0,0.0968343,"Missing"
2021.eacl-main.205,W16-0311,0,0.0594097,"Missing"
2021.eacl-main.205,2020.acl-main.700,1,0.807437,"individual posts may not be sufficient to assess a user’s suicide risk, even for humans (Sisask et al., 2008; O’dea et al., 2015). Figure 1 illustrates how features such as historical posts (Matero et al., 2019) can add context for analyzing a user’s online behavior over time (Van Heeringen and Maruˇsic, 2003) to better 2415 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2415–2428 April 19 - 23, 2021. ©2021 Association for Computational Linguistics ascertain suicide risk. Despite the success of usercentric contextual models (Flek, 2020) for suicide ideation detection, they have two major limitations. First, recurrent neural networks, particularly LSTMs, that are natural methods to learn patterns from a sequence of a user’s historical tweets (Cao et al., 2019; Zeng et al., 2019, 2020), assume uniform time gaps between successive tweets. However, tweets can be posted at irregular time intervals (Lei et al., 2018), and varying time gaps can influence the assessment of a user’s suicidality progression (Chen et al., 2018), as shown in Figure 1. Second, these methods implicitly assume that a user’s mental and emotional state progr"
2021.eacl-main.205,W14-3213,0,0.0689706,"ve long-term gate f˜k , which contains older historic emotional context. Additionally, we propose a short-term gate i˜k that encodes recent historic tweets, as shown in Figure 3. We then feed the time-lapsed ∆k from the previous tweet and the historical emotional representation eik of each tweet hik to a TSE-LSTM cell. This design aids TSE-LSTM to learn two probability distributions pf˜k and pi˜k corresponding to the long-term and short-term gates, respectively. Psychological studies show that a user’s recent emotions can be more indicative of their current mental state (Fawcett et al., 1990; Homan et al., 2014). To this end, we set the update frequency of the short-term gate higher than the long-term gate to increase the influence of their more recent emotional context. To impose this natural ordering of frequency updates, we apply cumulative sum (cumsum) operation to the probability distributions pf˜k and pi˜k : ˜ ik−1 ⊕ ∆k ) + b˜) p˜f = σ(W˜f(eik ⊕ ∆k ) + U˜f(H f ˜ k−1 ⊕ ∆k ) + b˜) p˜ = σ(W˜(eik ⊕ ∆k ) + U˜(H i i i −−−−−→ ), ˜i = ← −−−−−− ) ˜ fk = − cumsum(p cumsum(p ˜ k ˜ f i i (3) (4) (5) where σ represents softmax, ⊕ denotes concate˜i nation and H k−1 is the previous hidden state. TSE-LSTM CELL"
2021.eacl-main.205,P16-2096,0,0.0218779,"and Proferes, 2018; Chancellor et al., 2019a,b). We ensure that this analysis is shared selectively and subject to IRB approval (Zimmer, 2009, 2010) to avoid misuse such as Samaritan’s Radar (Hsin et al., 2016). We acknowledge that suicidality is subjective (Keilp 2422 et al., 2012) and that the interpretation of this analysis may vary across individuals (Puschman, 2017). We further acknowledge that suicide risk exists on a diverse spectrum (Bryan and Rudd, 2006), rather than at a binary level, and that the studied data may be susceptible to demographic, annotator, and medium-specific biases (Hovy and Spruit, 2016). Finally, our work does not make any diagnostic claims related to suicide. PHASE should form part of a distributed human-in-the-loop (de Andrade et al., 2018) system for finer interpretation of risk. 7 Conclusion Motivated by the rising exhibition of suicide ideation on social media, we present PHASE. Building on psychological studies analyzing the emotional spectrum and mental health of users, PHASE adaptively learns emotional phase-aware user representations through historical tweeting activity for suicidal ideation detection. We propose multiple modeling innovations in PHASE components: co"
2021.eacl-main.205,W19-3005,0,0.331088,"cy (Chancellor et al., 2019b). of ten people disclosing their suicidal thoughts and plans on social media (Golden et al., 2009). Natural Language Processing (NLP) presents an encouraging prospect to complement social science to identify risk markers in user behavior (De Choudhury et al., 2013, 2016) to aid suicide risk assessment (Shing et al., 2018, 2020). However, suicide ideation is complex, and often, individual posts may not be sufficient to assess a user’s suicide risk, even for humans (Sisask et al., 2008; O’dea et al., 2015). Figure 1 illustrates how features such as historical posts (Matero et al., 2019) can add context for analyzing a user’s online behavior over time (Van Heeringen and Maruˇsic, 2003) to better 2415 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2415–2428 April 19 - 23, 2021. ©2021 Association for Computational Linguistics ascertain suicide risk. Despite the success of usercentric contextual models (Flek, 2020) for suicide ideation detection, they have two major limitations. First, recurrent neural networks, particularly LSTMs, that are natural methods to learn patterns from a sequence of a user’s historical"
2021.eacl-main.205,N19-3019,1,0.917923,"018b). Shared tasks such as CLPsych (Zirikly et al., 2019) and CLEF eRISK (Losada et al., 2020) have seen a rise in neural networks such as CNNs (Yates et al., 2017; Du et al., 2018; Naderi et al., 2019; Gaur et al., 2019) and LSTMs (Ji et al., 2018; Tadesse et al., 2020) to predict suicide risk. While these methods capture post semantics in isolation, no user context is leveraged, hindering insight into 2416 the user’s mental state to improve predictive power (Venek et al., 2017; Flek, 2020). User context includes the user’s emotions (Ren et al., 2016; Guntuku et al., 2017), social networks (Mishra et al., 2019) and historical posts (Mathur et al., 2020). Contextual Methods: The best performing model, the DualContextBERT (Matero et al., 2019), at CLPsych 2019 for suicidal estimation exemplifies the utility of temporal context. The DualContextBERT models post embeddings sequentially via an RNN. Such RNN-based approaches assume that users’ historical posts are equally spaced in time, hindering their ability to learn their relative importance in a time-aware manner. Recently, timeaware modeling of well defined stages in numerical time series data shows promising results in clinical tasks like patient su"
2021.eacl-main.205,2020.emnlp-main.619,1,0.620672,"PHASE jointly learns the semantics of user tweets and their historical emotional context in a temporal phaseStudies show that emotions expressed in suicidal tweets are correlated with suicidal behavior (Sueki, 2015; Spates et al., 2018; Zhang et al., 2017). As a building block, we utilize Plutchik’s wheel of emotions (Plutchik, 1980) to capture the emotions expressed by a user in their tweets. Plutchik’s wheel outlines eight primary emotions arranged as four pairs of opposing dualities: Joy - Sadness, Surprise - Anticipation, Anger - Fear, and Trust - Disgust. We utilize Plutchik Transformer (Sawhney et al., 2020), a BERT model fine-tuned on Emonet (Abdul-Mageed and Ungar, 2017), a dataset of 790,059 tweets labeled across 8 primary emotions as per Plutchik’s wheel of emotions. Owing to the success of pre-training language models in NLP, Plutchik Transformer jointly learns textual and emotion features for representation learning of user tweets for subsequent suicidal intent detection. We extract a 768-dimension encoding from the [CLS]3 token of the penultimate transformer layer, which is densely connected with an 8-dimensional output layer representative of each primary emotion. 3 Empirically, the [CLS]"
2021.eacl-main.205,W18-6223,1,0.942958,"media has shown promise in providing insights into users’ mental states (Paul and Dredze, 2011). Jashinsky et al. (2014) reported that Twitter is a viable tool for real-time monitoring (Braithwaite et al., 2016) of suicide risk. Early efforts in utilizing social media leverage user features such as their age, gender,and social network connectivity (Masuda et al., 2013) and online suicide notes (Pestian et al., 2010). Since then, the focus has been on using psycholinguistic lexicons such as LIWC and textual features such as n-grams, POS tags, etc. for classification (De Choudhury et al., 2016; Sawhney et al., 2018b). Shared tasks such as CLPsych (Zirikly et al., 2019) and CLEF eRISK (Losada et al., 2020) have seen a rise in neural networks such as CNNs (Yates et al., 2017; Du et al., 2018; Naderi et al., 2019; Gaur et al., 2019) and LSTMs (Ji et al., 2018; Tadesse et al., 2020) to predict suicide risk. While these methods capture post semantics in isolation, no user context is leveraged, hindering insight into 2416 the user’s mental state to improve predictive power (Venek et al., 2017; Flek, 2020). User context includes the user’s emotions (Ren et al., 2016; Guntuku et al., 2017), social networks (Mis"
2021.eacl-main.205,P18-3013,1,0.901871,"media has shown promise in providing insights into users’ mental states (Paul and Dredze, 2011). Jashinsky et al. (2014) reported that Twitter is a viable tool for real-time monitoring (Braithwaite et al., 2016) of suicide risk. Early efforts in utilizing social media leverage user features such as their age, gender,and social network connectivity (Masuda et al., 2013) and online suicide notes (Pestian et al., 2010). Since then, the focus has been on using psycholinguistic lexicons such as LIWC and textual features such as n-grams, POS tags, etc. for classification (De Choudhury et al., 2016; Sawhney et al., 2018b). Shared tasks such as CLPsych (Zirikly et al., 2019) and CLEF eRISK (Losada et al., 2020) have seen a rise in neural networks such as CNNs (Yates et al., 2017; Du et al., 2018; Naderi et al., 2019; Gaur et al., 2019) and LSTMs (Ji et al., 2018; Tadesse et al., 2020) to predict suicide risk. While these methods capture post semantics in isolation, no user context is leveraged, hindering insight into 2416 the user’s mental state to improve predictive power (Venek et al., 2017; Flek, 2020). User context includes the user’s emotions (Ren et al., 2016; Guntuku et al., 2017), social networks (Mis"
2021.eacl-main.205,W18-0603,0,0.0289704,"Missing"
2021.eacl-main.205,2020.acl-main.723,0,0.0286142,"Missing"
2021.eacl-main.205,D17-1322,0,0.0184847,"real-time monitoring (Braithwaite et al., 2016) of suicide risk. Early efforts in utilizing social media leverage user features such as their age, gender,and social network connectivity (Masuda et al., 2013) and online suicide notes (Pestian et al., 2010). Since then, the focus has been on using psycholinguistic lexicons such as LIWC and textual features such as n-grams, POS tags, etc. for classification (De Choudhury et al., 2016; Sawhney et al., 2018b). Shared tasks such as CLPsych (Zirikly et al., 2019) and CLEF eRISK (Losada et al., 2020) have seen a rise in neural networks such as CNNs (Yates et al., 2017; Du et al., 2018; Naderi et al., 2019; Gaur et al., 2019) and LSTMs (Ji et al., 2018; Tadesse et al., 2020) to predict suicide risk. While these methods capture post semantics in isolation, no user context is leveraged, hindering insight into 2416 the user’s mental state to improve predictive power (Venek et al., 2017; Flek, 2020). User context includes the user’s emotions (Ren et al., 2016; Guntuku et al., 2017), social networks (Mishra et al., 2019) and historical posts (Mathur et al., 2020). Contextual Methods: The best performing model, the DualContextBERT (Matero et al., 2019), at CLPsyc"
2021.eacl-main.205,2020.acl-main.305,0,0.0841575,"Missing"
2021.eacl-main.205,P19-1270,0,0.0243586,"ing a user’s online behavior over time (Van Heeringen and Maruˇsic, 2003) to better 2415 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2415–2428 April 19 - 23, 2021. ©2021 Association for Computational Linguistics ascertain suicide risk. Despite the success of usercentric contextual models (Flek, 2020) for suicide ideation detection, they have two major limitations. First, recurrent neural networks, particularly LSTMs, that are natural methods to learn patterns from a sequence of a user’s historical tweets (Cao et al., 2019; Zeng et al., 2019, 2020), assume uniform time gaps between successive tweets. However, tweets can be posted at irregular time intervals (Lei et al., 2018), and varying time gaps can influence the assessment of a user’s suicidality progression (Chen et al., 2018), as shown in Figure 1. Second, these methods implicitly assume that a user’s mental and emotional state progression is smooth in time, with an ever-increasing tendency. However, in reality, studies show that emotional (Larsen et al., 2015), and suicidality progression can vary significantly (Bryan and Rudd, 2016; Bryan, 2020), and show fluctuating phas"
2021.eacl-main.205,W19-3003,0,0.0209575,"rs’ mental states (Paul and Dredze, 2011). Jashinsky et al. (2014) reported that Twitter is a viable tool for real-time monitoring (Braithwaite et al., 2016) of suicide risk. Early efforts in utilizing social media leverage user features such as their age, gender,and social network connectivity (Masuda et al., 2013) and online suicide notes (Pestian et al., 2010). Since then, the focus has been on using psycholinguistic lexicons such as LIWC and textual features such as n-grams, POS tags, etc. for classification (De Choudhury et al., 2016; Sawhney et al., 2018b). Shared tasks such as CLPsych (Zirikly et al., 2019) and CLEF eRISK (Losada et al., 2020) have seen a rise in neural networks such as CNNs (Yates et al., 2017; Du et al., 2018; Naderi et al., 2019; Gaur et al., 2019) and LSTMs (Ji et al., 2018; Tadesse et al., 2020) to predict suicide risk. While these methods capture post semantics in isolation, no user context is leveraged, hindering insight into 2416 the user’s mental state to improve predictive power (Venek et al., 2017; Flek, 2020). User context includes the user’s emotions (Ren et al., 2016; Guntuku et al., 2017), social networks (Mishra et al., 2019) and historical posts (Mathur et al.,"
2021.emnlp-main.499,W14-3902,0,0.031231,"grammatical systems from one utterance to another within the same conversation (Gumperz, 1982). On the other hand, code-mixing refers to the use of linguistic units such as phrases, words, or morphemes of one language in the utterance of another language (Myers-Scotton, 1997; Myers-Scotton et al., 2002). In other words, code-switching is an interutterance, and code-mixing is an intra-utterance phenomenon. However, in this paper, we use the term code-switching to refer to both these concepts. Code-switching has started to gain some traction from computational linguists over the last few years (Barman et al., 2014b; Bali et al., 2014), where they developed datasets for many interesting problems such as language identification (Das and Gambäck, 2014), part of speech tagging (Barman et al., 2014a), question answering (Chandu et al., 2015), and named entity recognition (Singh et al., 2018). Additionally, researchers have also started to develop objective metrics that characterize the complexity of code-switching in a given corpus (Gambäck and Das, 2016; Guzmán et al., 2017). For a thorough review of datasets and other developments in this space, we recommend the review paper from (Sitaram et al., 2019). M"
2021.emnlp-main.499,N18-2097,0,0.0224929,"Table 2, there are only three datasets The goal of our annotation process is to build a with Hindi-English code-switched conversations, Hi-En code-switched conversation summarization but none of them contain summaries. dataset. We first considered the option of creating A large majority of research in abstractive sum- summaries for an existing code-switched conversamarization focuses on news articles (Hermann tional dataset like DSTC2 (Banerjee et al., 2018). et al., 2015; Grusky et al., 2018a; Narayan et al., Though this dataset is substantially large with over 2018a) and scientific papers (Cohan et al., 2018), 50,000 utterances, it is not open-domain and only because of the availability of large benchmark contains human-computer conversations focused datasets. The task of summarizing open-domain on restaurant reservations, thus lacking linguistic multi-party conversations has not been investigated diversity. We, therefore, chose the option of manuuntil recently with the introduction of SAMSum ally translating the SAMSum corpus (Gliwa et al., (Gliwa et al., 2019). (Chen and Yang, 2020) 2019) from En to Hi-En. obtained state-of-the-art results on this corpus Annotation Process - We hired eight annot"
2021.emnlp-main.499,W18-3210,0,0.0645678,"Missing"
2021.emnlp-main.499,W14-5152,0,0.0122164,"o the use of linguistic units such as phrases, words, or morphemes of one language in the utterance of another language (Myers-Scotton, 1997; Myers-Scotton et al., 2002). In other words, code-switching is an interutterance, and code-mixing is an intra-utterance phenomenon. However, in this paper, we use the term code-switching to refer to both these concepts. Code-switching has started to gain some traction from computational linguists over the last few years (Barman et al., 2014b; Bali et al., 2014), where they developed datasets for many interesting problems such as language identification (Das and Gambäck, 2014), part of speech tagging (Barman et al., 2014a), question answering (Chandu et al., 2015), and named entity recognition (Singh et al., 2018). Additionally, researchers have also started to develop objective metrics that characterize the complexity of code-switching in a given corpus (Gambäck and Das, 2016; Guzmán et al., 2017). For a thorough review of datasets and other developments in this space, we recommend the review paper from (Sitaram et al., 2019). Most of the code-switched datasets typically contain individual posts or comments from social media applications like Twitter and Facebook"
2021.emnlp-main.499,W18-3817,0,0.0118732,"&quot;I have been thinking for a while&quot; to &quot;Main kaafi time se soch rahi hu&quot;, where they introduced the token time. However, we observed that this was a rare phenomenon and did not sway the analysis significantly. 26,865 11,616 12,016 76,330 75,791 43,407 13,760 17,804 10.07 6.35 45,644 2,934 2,810 38,539 Table 4: Code-switching statistics of the GupShup dataset. code-switched: had a combination of Hindi and English tokens. Table 4 has more detailed codeswitching statistics of the corpus. We determine the matrix language (MyersScotton et al., 2002)4 of a sentence using the heuristics proposed in (Dhar et al., 2018). Namely, we define a sentence as Hindi if (a) the majority of tokens are Hindi, (b) we detect the use of any Romanized Hindi verbs, or (c) we detect the use of Romanized Hindi bi-grams. Per this definition on the 43,407 code-switched utterances, the matrix language of 45,644 sentences was Hindi, and 2,934 sentences were in English. Based on the matrix language and token-level language tags, we further analyzed the codeswitching complexity using the metrics Cavg (Gambäck and Das, 2016), Cc (Banerjee et al., 2018), and I-index (Guzman et al., 2016). These metrics quantify complexity in terms of"
2021.emnlp-main.499,W18-3205,0,0.0360658,"Missing"
2021.emnlp-main.499,2020.emnlp-main.751,0,0.0622945,"Missing"
2021.emnlp-main.499,2020.acl-main.560,0,0.0130445,"eractions between peers who are fluent in multiple languages. For example, in the Indian subcontinent, it is common for people to alternate between English and other regional languages like Hindi over the course of a single conversation. Code-switching is an integral part of both written and spoken conversations for various multi-lingual communities across the world (Auer, 2013). Developing models that can accurately process code-switched text is essential to the proliferation of NLP technologies to these communities and contributes towards the diversity and inclusivity of language resources (Joshi et al., 2020). However, building such models would require high-quality human-curated datasets. This paper introduces the task of abstractive summarization of open-domain code-switched written conversations. Namely, given a multi-party conversation in Hi-En, the objective is to generate a summary in English, as shown in Table 1. A multiparty code-switched conversation C is a sequence of n utterances {u1 , u2 , ..., un }, where the ith utterance ui is written by pj one of the k participants. The utterance ui is a sequence of m tokens {x1 , x2 , ..., xm }, where the tokens could either be in English or trans"
2021.emnlp-main.499,W04-1013,0,0.0711838,"d a 6-layer decoder. For the T5 model5 , we tried both fine-tuning and multitask learning approach (T5 MTL); for the latter approach, we trained the model for both summarization and translation. We ran this entire process for three experimental setups: (1) En summaries from Hi-En conversations, (2) En summaries from En conversations, and (3) Hi-En summaries from Hi-En conversations. All models were trained using Huggingface’s transformer library on Google colab GPU enabled platform. The model performances are reported in terms of the following automatic evaluation metrics: ROUGE (R1, R2, RL) (Lin, 2004), BLEURT (Sellam et al., 2020), BERT-score (Zhang et al., 2020), BLEU (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005). Results - The results are summarized in Table 5. In generating English summaries from Hi-En conversations, the mBART model obtained the best R1 and R2 scores; this is likely because it is the only model to have been trained on multiple languages and therefore better understands code-switching points in the conversation. The multi-view model obtained the best RL, BLEURT, and BLEU scores. Though this model was pre-trained only on English data, it explicitly extrac"
2021.emnlp-main.499,P14-1115,0,0.0718149,"Missing"
2021.emnlp-main.499,D18-1206,0,0.0938214,"ain why T5 has fewer MI errors. The II errors have increased significantly for all the Quantifying Abstractiveness - During our error models when summarizing Hi-En conversations, analysis, we also observed that the summaries genfurther demonstrating the inability of the models erated from English conversations were often more to understand code-switched conversations. We extractive than abstractive, where the models were noticed that even though the models select salient selecting the most salient phrases in the source conaspects of the conversations, the summaries have versations. Following (Narayan et al., 2018b), we incorrect inferences because some of the critical quantified the abstractiveness of the summaries by tokens in the conversation were code-switched. calculating the portion of new n-gram generated Summary Compression - We measured the com- by the models for English. Namely, we report the ratio of n-grams that were part of the summaries pression ratio of the summaries generated by all but not the source conversations to the total number models for both En and Hi-En conversations as show in figure 2. Per (Grusky et al., 2018b), com- of n-grams in the summaries. When compared to reference s"
2021.emnlp-main.499,P02-1040,0,0.110227,"learning approach (T5 MTL); for the latter approach, we trained the model for both summarization and translation. We ran this entire process for three experimental setups: (1) En summaries from Hi-En conversations, (2) En summaries from En conversations, and (3) Hi-En summaries from Hi-En conversations. All models were trained using Huggingface’s transformer library on Google colab GPU enabled platform. The model performances are reported in terms of the following automatic evaluation metrics: ROUGE (R1, R2, RL) (Lin, 2004), BLEURT (Sellam et al., 2020), BERT-score (Zhang et al., 2020), BLEU (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005). Results - The results are summarized in Table 5. In generating English summaries from Hi-En conversations, the mBART model obtained the best R1 and R2 scores; this is likely because it is the only model to have been trained on multiple languages and therefore better understands code-switching points in the conversation. The multi-view model obtained the best RL, BLEURT, and BLEU scores. Though this model was pre-trained only on English data, it explicitly extracts conversational structure to help with the summary generation. The T5 MTL model outperforme"
2021.emnlp-main.499,D17-1240,0,0.0250386,"Missing"
2021.emnlp-main.499,2020.tacl-1.18,0,0.0144576,"d not pursue it further. chitecture which contains only a decoder and can6182 RL BLEURT BERTScore BLEU METEOR Metric Coherence Consistency Fluency Relevance Overall R2 lation between all the automatic metrics and human evaluations. All the ROUGE-based metrics and BLEURT were strongly correlated with human evaluations, but METEOR had a very low correlation. Perhaps, as a next step, we could consider fine-tuning BLEURT on this dataset to serve as a measure of quality for summaries from codeswitching conversations. R1 not, therefore, create a meaningful representation of the source conversation (Rothe et al., 2020). Additionally, we also observed minimal variance in BERTScore values across the models, suggesting it may not be the most effective metric for measuring the quality of summaries. In the third experimental setup, we generated Hi-En summaries from Hi-En conversations. Here we observed a steep drop in performance of all the models when compared to generating English summaries. However, this drop was less prominent for BART, which obtained the best scores across most metrics. Another interesting observation is that, despite being multilingual, mBART generated poor Hi-En summaries. Overall, from t"
2021.emnlp-main.499,2020.acl-main.704,0,0.0136379,"For the T5 model5 , we tried both fine-tuning and multitask learning approach (T5 MTL); for the latter approach, we trained the model for both summarization and translation. We ran this entire process for three experimental setups: (1) En summaries from Hi-En conversations, (2) En summaries from En conversations, and (3) Hi-En summaries from Hi-En conversations. All models were trained using Huggingface’s transformer library on Google colab GPU enabled platform. The model performances are reported in terms of the following automatic evaluation metrics: ROUGE (R1, R2, RL) (Lin, 2004), BLEURT (Sellam et al., 2020), BERT-score (Zhang et al., 2020), BLEU (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005). Results - The results are summarized in Table 5. In generating English summaries from Hi-En conversations, the mBART model obtained the best R1 and R2 scores; this is likely because it is the only model to have been trained on multiple languages and therefore better understands code-switching points in the conversation. The multi-view model obtained the best RL, BLEURT, and BLEU scores. Though this model was pre-trained only on English data, it explicitly extracts conversational structure to"
2021.emnlp-main.499,W18-2405,0,0.0163003,"yers-Scotton et al., 2002). In other words, code-switching is an interutterance, and code-mixing is an intra-utterance phenomenon. However, in this paper, we use the term code-switching to refer to both these concepts. Code-switching has started to gain some traction from computational linguists over the last few years (Barman et al., 2014b; Bali et al., 2014), where they developed datasets for many interesting problems such as language identification (Das and Gambäck, 2014), part of speech tagging (Barman et al., 2014a), question answering (Chandu et al., 2015), and named entity recognition (Singh et al., 2018). Additionally, researchers have also started to develop objective metrics that characterize the complexity of code-switching in a given corpus (Gambäck and Das, 2016; Guzmán et al., 2017). For a thorough review of datasets and other developments in this space, we recommend the review paper from (Sitaram et al., 2019). Most of the code-switched datasets typically contain individual posts or comments from social media applications like Twitter and Facebook anno6178 3 https://github.com/midas-research/gupshup Dataset (Das and Gambäck, 2014) (Barman et al., 2014a) (Chandu et al., 2015) (Jamatia e"
2021.emnlp-main.499,W14-3907,0,0.0604706,"Missing"
2021.emnlp-main.499,2020.coling-main.63,0,0.0381609,"-Domain Code-Switched Conversations ∗ Laiba Mehnaz Debanjan Mahata Uma Sushmitha Gunturi Amardeep Kumar NYU, USA Moody’s Analytics, USA Virginia Tech, USA Walmart, India Rakesh Gosangi Riya Jain Gauri Gupta Isabelle Lee Bloomberg, USA IIIT-Delhi, India IIIT-Delhi, India Univ of Washington, USA Anish Acharya Rajiv Ratn Shah Univ of Texas at Austin, USA IIIT-Delhi, India Abstract the abstractive summarization of written conversations (Mehdad et al., 2014; Goo and Chen, 2018; Zhao et al., 2019). Automatic conversation summarization has potential applications in various fields such as healthcare (Song et al., 2020), call centers (Alam et al., 2016), education (Joshi and Rosé, 2007), and many other areas (Feng et al., 2021). One of the biggest challenges in conversation summarization has been the lack of large datasets with human-annotated summaries. Most researchers evaluate their summarization techniques on transcriptions of AMI (Carletta et al., 2005), or ICSI meeting corpus (Janin et al., 2003) using the meeting topics as summaries. These corpora are very useful for various speech-related research problems, but they do not represent written conversations in chat applications. Recently, (Gliwa et al.,"
2021.naacl-main.176,W14-3207,0,0.0961054,"Missing"
2021.naacl-main.176,2020.acl-main.700,1,0.901178,"lexicons like LIWC (Pennebaker et al., 2001). Deep learning models like CNNs (Naderi et al., 2019) and LSTMs (Coppersmith et al., 2018) have improved suicide ideation detection (Ji et al., 2020) thanks to a more robust semantic context to interpret the tweet in question, however, lacking user-level context, are often unable to ascertain suicide risk (Sisask et al., 2008). The best performing models (Matero et al., 2019; Naderi et al., 2019) at the CLPsych (Zirikly et al., 2019) and CLEF e-Risk (Losada et al., 2019) exemplify the promising yet underexplored direction of user context modeling (Flek, 2020) for suicide ideation detection. Although recent studies (Shing et al., 2020; Sawhney et al., 2020) explore the personal historical context of users, community-based social context has rarely been explored for this task. One of the few attempts includes SNAPBATNET (Sinha et al., 2019), a shallow embedding model to extract network structural features. fluential nodes have a large number of connections, creating social hubs, further amplifying phenomena such as the “Werther effect” (Fahey et al., 2018). Social networks with scale-free structure are subjects to major distortions when embedded int"
2021.naacl-main.176,K19-1093,0,0.131865,"3.3 Modeling Personal Historical Context To model historical emotions of a user and factor in the natural irregularities in posting time of historical tweets (Lei et al., 2018; Wojcik and 3.2 Encoding Tweets Hughes, 2019), we propose the HEAT mechanism: We build on previous studies which show that Hawkes temporal Emotion AggregraTion. HEAT the linguistic styles (De Choudhury et al., 2013, leverages Hawkes Process (Hawkes, 1971), a self2016) and emotions expressed in suicidal tweets exciting temporal point process to model the in2178 tensity of emotions whenever a tweet is posted in the past (Guo et al., 2019). Intuitively, it assumes that emotions exhibited in different historic tweets can influence one another. To obtain the final historic representation (Eij ∈ R768 ) of the tweet to be assessed ti , HEAT aggregates encoded historical emotions eik using an exponential kernel as: Eij = X 0 Poincare Ball Transformation Hyperbolic Non-linear Activation 0 (eik + eik e−β∆τk ), eik = max(eik , 0) (1) k:∆τk ≥0 where, ∆τk is the time gap between a historical tweet and the tweet to be assessed (current tweet) posted at time τk and τcurr , respectively.  and β are hyperparameters such that  &lt; β. 3.4 Soc"
2021.naacl-main.176,P16-2096,0,0.0304509,"each user was not sought as it may be deemed coercive, we perform automatic de-identification of the dataset using named entity recognition (Benton et al., 2017a,b) to reduce the risk of including any identifying data in the raw data. We paraphrase all examples shown in this work to protect user privacy (Chancellor et al., 2019a,b). All the user data is kept separately on protected servers linked to the raw text and network data only through anonymous IDs. We acknowledge that it is almost impossible to prevent abuse of released technology even when developed with good intentions (Jonas, 1984; Hovy and Spruit, 2016). Hence, we ensure that this analysis is shared only selectively and subject to IRB approval (Zimmer, 2009, 2010) to avoid misuse such as Samaritan’s Radar (Hsin et al., 2016). 8 Conclusion Acknowledgements This work has been supported by the German Federal Ministry of Education and Research (BMBF) as a part of the Junior AI Scientists program under the reference 01-S20060. We thank the anonymous reviewers for their valuable input. Limitations: We acknowledge that suicidality is subjective (Keilp et al., 2012), the interpretation of this analysis may vary across individuals on so- References c"
2021.naacl-main.176,2020.acl-main.48,0,0.0275179,"raph neural networks. To overcome this limitation, we propose to model the social relations using graph convolutions over hyperbolic space (Chami et al., 2019). Our key contributions are as follows: (i) We present the first deep graph neural frame- 2.2 Graph Neural Networks work to identify suicide ideation on social media While graph neural networks (GNNs) have made by explicitly modeling users’ social and temporal advances in enhancing NLP models for various emotional context jointly (§3). tasks (Mishra et al., 2019a; Del Tredici et al., 2019; (ii) Motivated by psychological studies and the Lu and Li, 2020), two broad shortcomings limit scale-free nature of social networks, we propose their effectiveness for suicide ideation detection. the use of Hyperbolic Graph Convolutions (§3.4). First, these methods do not capture the personal (iii) We propose a mechanism leveraging historical and social network context together, both Hawkes process to learn the historic emotional of which are strongly correlated to risk assessment spectrum of a user in a time-sensitive manner from on social media (Yang and Eisenstein, 2017). Sectheir historical posts (§3.3). ond, studies have shown that users exhibiting su"
2021.naacl-main.176,2020.findings-emnlp.213,0,0.0364284,"tions of users who interact with it (Chung and Zeng, 2020). User D presents an error case. We find that the tweet to be assessed is ambiguous, and historical activity is not informative either. Moreover, user D is isolated, highlighting that suicide ideation detection in the absence of contextual elements (historical activity, network interactions) can be highly subjective, and paves the way for future work. Future Practical Applicability In the future, we would want to focus on creating a differentially private public model that can be shared with the community while preserving user privacy (Lyu et al., 2020; Yu et al., 2019). Further, suicide ideation detection on social media can involve failure modes that could potentially incorrectly ascertain suicide risk. To this end, we focus on Hyper-SOS as a preliminary tool for prioritizing human expert, clinical psychologist-based assessment. 7 Motivated by psychological studies, we propose a framework jointly leveraging emotional history from user’s past tweets and social information from user’s neighborhood in a network to contextualize the interpretation of the latest tweet of a user. To our knowledge, this is the first deep graph neural network stu"
2021.naacl-main.176,W19-3005,0,0.177344,"leveraging NLP for suicide ideation detection on social media (De Choudhury et al., 2013, 2016; Shing et al., 2018; Sawhney et al., 2018) combine general features such as n-grams and POS tags with lexicons like LIWC (Pennebaker et al., 2001). Deep learning models like CNNs (Naderi et al., 2019) and LSTMs (Coppersmith et al., 2018) have improved suicide ideation detection (Ji et al., 2020) thanks to a more robust semantic context to interpret the tweet in question, however, lacking user-level context, are often unable to ascertain suicide risk (Sisask et al., 2008). The best performing models (Matero et al., 2019; Naderi et al., 2019) at the CLPsych (Zirikly et al., 2019) and CLEF e-Risk (Losada et al., 2019) exemplify the promising yet underexplored direction of user context modeling (Flek, 2020) for suicide ideation detection. Although recent studies (Shing et al., 2020; Sawhney et al., 2020) explore the personal historical context of users, community-based social context has rarely been explored for this task. One of the few attempts includes SNAPBATNET (Sinha et al., 2019), a shallow embedding model to extract network structural features. fluential nodes have a large number of connections, creatin"
2021.naacl-main.176,N19-1221,0,0.297081,"into the Euclidean representation space (Chen et al., 2013; Aparicio et al., 2015) by ordinary graph neural networks. To overcome this limitation, we propose to model the social relations using graph convolutions over hyperbolic space (Chami et al., 2019). Our key contributions are as follows: (i) We present the first deep graph neural frame- 2.2 Graph Neural Networks work to identify suicide ideation on social media While graph neural networks (GNNs) have made by explicitly modeling users’ social and temporal advances in enhancing NLP models for various emotional context jointly (§3). tasks (Mishra et al., 2019a; Del Tredici et al., 2019; (ii) Motivated by psychological studies and the Lu and Li, 2020), two broad shortcomings limit scale-free nature of social networks, we propose their effectiveness for suicide ideation detection. the use of Hyperbolic Graph Convolutions (§3.4). First, these methods do not capture the personal (iii) We propose a mechanism leveraging historical and social network context together, both Hawkes process to learn the historic emotional of which are strongly correlated to risk assessment spectrum of a user in a time-sensitive manner from on social media (Yang and Eisenste"
2021.naacl-main.176,N19-3019,1,0.942334,"into the Euclidean representation space (Chen et al., 2013; Aparicio et al., 2015) by ordinary graph neural networks. To overcome this limitation, we propose to model the social relations using graph convolutions over hyperbolic space (Chami et al., 2019). Our key contributions are as follows: (i) We present the first deep graph neural frame- 2.2 Graph Neural Networks work to identify suicide ideation on social media While graph neural networks (GNNs) have made by explicitly modeling users’ social and temporal advances in enhancing NLP models for various emotional context jointly (§3). tasks (Mishra et al., 2019a; Del Tredici et al., 2019; (ii) Motivated by psychological studies and the Lu and Li, 2020), two broad shortcomings limit scale-free nature of social networks, we propose their effectiveness for suicide ideation detection. the use of Hyperbolic Graph Convolutions (§3.4). First, these methods do not capture the personal (iii) We propose a mechanism leveraging historical and social network context together, both Hawkes process to learn the historic emotional of which are strongly correlated to risk assessment spectrum of a user in a time-sensitive manner from on social media (Yang and Eisenste"
2021.naacl-main.176,W18-0603,0,0.193762,"et al., ingly turn to social media rather than mental health practitioners. Personally contextualiz2009). Consequently, a growing body of work has ing the buildup of such ideation is critical for shown that natural language processing can compleaccurate identification of users at risk. In this ment social media analysis to identify risk markers work, we propose a framework jointly leverin online user behavior to aid suicide risk assessaging a user’s emotional history and social inment (McCarthy, 2010; De Choudhury et al., 2016; formation from a user’s neighborhood in a netReger et al., 2020; Shing et al., 2018). However, work to contextualize the interpretation of the analyzing individual user posts is not always suffilatest tweet of a user on Twitter. Reflecting cient to infer user’s mental state and the associated upon the scale-free nature of social network relationships, we propose the use of Hyperbolic suicide risk (Harris, 2010; Sisask et al., 2008). Graph Convolution Networks, in combination Studies suggest that suicide can be influenced with the Hawkes process to learn the historby social factors (Masuda et al., 2013; Gvion ical emotional spectrum of a user in a timeand Apter, 2012), and is"
2021.naacl-main.176,2020.acl-main.723,0,0.0131941,"ike CNNs (Naderi et al., 2019) and LSTMs (Coppersmith et al., 2018) have improved suicide ideation detection (Ji et al., 2020) thanks to a more robust semantic context to interpret the tweet in question, however, lacking user-level context, are often unable to ascertain suicide risk (Sisask et al., 2008). The best performing models (Matero et al., 2019; Naderi et al., 2019) at the CLPsych (Zirikly et al., 2019) and CLEF e-Risk (Losada et al., 2019) exemplify the promising yet underexplored direction of user context modeling (Flek, 2020) for suicide ideation detection. Although recent studies (Shing et al., 2020; Sawhney et al., 2020) explore the personal historical context of users, community-based social context has rarely been explored for this task. One of the few attempts includes SNAPBATNET (Sinha et al., 2019), a shallow embedding model to extract network structural features. fluential nodes have a large number of connections, creating social hubs, further amplifying phenomena such as the “Werther effect” (Fahey et al., 2018). Social networks with scale-free structure are subjects to major distortions when embedded into the Euclidean representation space (Chen et al., 2013; Aparicio et al., 20"
2021.naacl-main.176,2020.emnlp-main.619,1,0.820275,"al., 2019) and LSTMs (Coppersmith et al., 2018) have improved suicide ideation detection (Ji et al., 2020) thanks to a more robust semantic context to interpret the tweet in question, however, lacking user-level context, are often unable to ascertain suicide risk (Sisask et al., 2008). The best performing models (Matero et al., 2019; Naderi et al., 2019) at the CLPsych (Zirikly et al., 2019) and CLEF e-Risk (Losada et al., 2019) exemplify the promising yet underexplored direction of user context modeling (Flek, 2020) for suicide ideation detection. Although recent studies (Shing et al., 2020; Sawhney et al., 2020) explore the personal historical context of users, community-based social context has rarely been explored for this task. One of the few attempts includes SNAPBATNET (Sinha et al., 2019), a shallow embedding model to extract network structural features. fluential nodes have a large number of connections, creating social hubs, further amplifying phenomena such as the “Werther effect” (Fahey et al., 2018). Social networks with scale-free structure are subjects to major distortions when embedded into the Euclidean representation space (Chen et al., 2013; Aparicio et al., 2015) by ordinary graph n"
2021.naacl-main.176,P18-3013,1,0.89069,"l do something myself Twitter User Figure 1: Illustration of social influence and context, specifically copycat suicidal ideation, in a scale-free network setting. Such social and temporal context can contextualize a user’s state for a more accurate suicide risk assessment. We paraphrase all examples in this paper as per the moderate disguise scheme (Bruckman, 2002) to protect user privacy (Chancellor et al., 2019b). 2 2.1 Related Work Suicide Ideation Detection Early efforts in leveraging NLP for suicide ideation detection on social media (De Choudhury et al., 2013, 2016; Shing et al., 2018; Sawhney et al., 2018) combine general features such as n-grams and POS tags with lexicons like LIWC (Pennebaker et al., 2001). Deep learning models like CNNs (Naderi et al., 2019) and LSTMs (Coppersmith et al., 2018) have improved suicide ideation detection (Ji et al., 2020) thanks to a more robust semantic context to interpret the tweet in question, however, lacking user-level context, are often unable to ascertain suicide risk (Sisask et al., 2008). The best performing models (Matero et al., 2019; Naderi et al., 2019) at the CLPsych (Zirikly et al., 2019) and CLEF e-Risk (Losada et al., 2019) exemplify the promi"
2021.naacl-main.176,Q17-1021,0,0.02855,"shra et al., 2019a; Del Tredici et al., 2019; (ii) Motivated by psychological studies and the Lu and Li, 2020), two broad shortcomings limit scale-free nature of social networks, we propose their effectiveness for suicide ideation detection. the use of Hyperbolic Graph Convolutions (§3.4). First, these methods do not capture the personal (iii) We propose a mechanism leveraging historical and social network context together, both Hawkes process to learn the historic emotional of which are strongly correlated to risk assessment spectrum of a user in a time-sensitive manner from on social media (Yang and Eisenstein, 2017). Sectheir historical posts (§3.3). ond, studies have shown that users exhibiting sui(iv) Through a series of experiments (§5), we cide ideation tend to form social networks with show that our framework significantly outperforms scale-free characteristics (Jonas, 1992; Mesoudi, existing methods (§6.1) on this task, as well as 2009), which regular GNNs are unable to accustandard Graph Neural Networks (§6.2). rately capture (Chami et al., 2019) in learnt social (v) Finally, we analyze the contributions of representations.We build on these limitations by Hyper-SOS’s individual components to asses"
2021.naacl-main.176,W19-3003,0,0.17203,"dia (De Choudhury et al., 2013, 2016; Shing et al., 2018; Sawhney et al., 2018) combine general features such as n-grams and POS tags with lexicons like LIWC (Pennebaker et al., 2001). Deep learning models like CNNs (Naderi et al., 2019) and LSTMs (Coppersmith et al., 2018) have improved suicide ideation detection (Ji et al., 2020) thanks to a more robust semantic context to interpret the tweet in question, however, lacking user-level context, are often unable to ascertain suicide risk (Sisask et al., 2008). The best performing models (Matero et al., 2019; Naderi et al., 2019) at the CLPsych (Zirikly et al., 2019) and CLEF e-Risk (Losada et al., 2019) exemplify the promising yet underexplored direction of user context modeling (Flek, 2020) for suicide ideation detection. Although recent studies (Shing et al., 2020; Sawhney et al., 2020) explore the personal historical context of users, community-based social context has rarely been explored for this task. One of the few attempts includes SNAPBATNET (Sinha et al., 2019), a shallow embedding model to extract network structural features. fluential nodes have a large number of connections, creating social hubs, further amplifying phenomena such as the “Wer"
2021.naacl-main.294,2020.acl-main.307,0,0.0839005,"Missing"
2021.naacl-main.294,P19-1047,0,0.0181456,"ed models further performance and future prospects with outside ana- perpetuate stereotypes that can harm underreprelysts and investors (Qin and Yang, 2019; Sawhney sented communities, specifically in the financial et al., 2020b). They consist of two sections: a and corporate world. Novák-Tót et al. (2017) even prepared delivery of performance statistics, anal- show that female speakers have to deliver better ysis and future expectations, and a spontaneous acoustic-melodic performance to seem as charisquestion-answer session to seek additional informa- matic as men. tion not disclosed before (Keith and Stent, 2019). Taking a step towards fair risk forecasting mod1 els, we analyze gender bias by studying the erCode & Implementation: https://github.com/ midas-research/multimodal-bias-naacl ror disparity in the state-of-the-art for multimodal 3751 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3751–3757 June 6–11, 2021. ©2021 Association for Computational Linguistics volatility prediction, MDRM (Qin and Yang, 2019). 2 Background: Why Study Bias? Bias in Finance Public financial data is impacting virtually"
2021.naacl-main.294,N09-1031,0,0.507465,"elf-reference using ‘I’, ‘me’ and ‘mine’ whereas women tend to reference the team, like ‘we’, ‘our’ and ‘us’ (Investments, 2017). Although there is great progress in mitigating bias in text, understanding its presence in multimodal speech based analysis, particularly in real world scenarios like corporate earnings calls analysis remain an understudied yet promising research direction. Another study found that despite having identical credibility, female CEOs are perceived as less capable to attract growth capital (Bigelow et al., 2014). 3 Formulation and Experiments Stock volatility Following Kogan et al. (2009); Sawhney et al. (2020c), for a given stock, with a close price of pi on trading day i, we calculate the average log volatility over n days following the day of the earnings call as: s v[0,n] = ln  Pn i=1 (ri − r¯)2 n   pi pi−1 (1) − 1 and Bias in AI and Finance With the advent of AI and Big Data, companies are intelligently using data to measure performance (Newman, 2020). Volatility Prediction Consider each earnings But seldom do enterprises check on the imbal- call E, with aligned audio recordings A and text ance in gathered data. Women still represent fewer transcripts T . The earnings"
2021.naacl-main.294,2020.lrec-1.796,0,0.0393432,"5) after applying Bonferroni corless when the number of samples across genders rection (Weisstein, 2004), a multiple comparison 3754 correction when multiple statistical tests are being performed. These differences in audio features of executives’ speech can amplify the error disparity, as models may associate certain gender specific features such as Voice analysis-based features like Shimmer and Jitter. 5 Ethical Considerations Degradation in the performance of speech models could be due to discernible noise and indiscernible sources like demographic bias: age, gender, dialect, culture, etc (Meyer et al., 2020; Hashimoto et al., 2018; Tatman and Kasten, 2017). Studies also show that AI can deploy biases against black people in criminal sentencing (Angwin et al., 2016; Tatman and Kasten, 2017). Although we only account for the gender bias in our study, we acknowledge that there could exist other kinds of bias due to age, accent, culture, ethnic and regional disparities in audio cues, as the publicly available earnings calls majorly have companies belonging to the US. Moreover, only publicly available earnings calls have been used limiting the scope of the data. This also limits the availability of g"
2021.naacl-main.294,D18-1302,0,0.0607152,"Missing"
2021.naacl-main.294,P19-1038,0,0.258986,"s’ speech. Recently, new mulAudio features contextualize text and connotate timodal approaches that leverage the verbal speaker’s emotional and psychological state (Fish and vocal cues of speakers in financial discloet al., 2017; Jiang and Pell, 2017; Burgoon et al., sures significantly outperform previous state2015; Bachorowski, 1999). Hence, when used with of-the-art approaches demonstrating the bentextual features, audio features significantly deterefits of multimodality and speech. However, mine the effect of earning calls on the stock market the financial realm is still plagued with a se(Qin and Yang, 2019; Yang et al., 2020). Past revere underrepresentation of various communities spanning diverse demographics, gender, search has shown that audio features such as speakand native speech. While multimodal moders’ pitch, intensity, etc. vary greatly across genels are better risk forecasters, it is imperative ders (Mendoza et al., 1996; Burris et al., 2014; to also investigate the potential bias that these Latinus and Taylor, 2012). Moreover, female execmodels may learn from the speech signals of utives are highly underrepresented in these earnings company executives. In this work, we present calls"
2021.naacl-main.294,2020.acl-main.690,0,0.0232041,"ttributes like gender, accent, etc. and investigate the sources of this bias. Our reIt further perpetuates gender-based stereotypes and sults suggest that multimodal neural financial generalizations like female executives are less con1 models accentuate gender-based stereotypes. fident than male executives (Lonkani, 2019), men are assessed as more charismatic than female exec1 Introduction utives under identical conditions (Novák-Tót et al., Earnings calls are publicly available, quarterly con- 2017), and nurses are female and doctors are male ference calls where CEOs discuss their company’s (Saunders and Byrne, 2020). Biased models further performance and future prospects with outside ana- perpetuate stereotypes that can harm underreprelysts and investors (Qin and Yang, 2019; Sawhney sented communities, specifically in the financial et al., 2020b). They consist of two sections: a and corporate world. Novák-Tót et al. (2017) even prepared delivery of performance statistics, anal- show that female speakers have to deliver better ysis and future expectations, and a spontaneous acoustic-melodic performance to seem as charisquestion-answer session to seek additional informa- matic as men. tion not disclosed be"
2021.naacl-main.294,2020.emnlp-main.676,1,0.538971,"I’, ‘me’ and ‘mine’ whereas women tend to reference the team, like ‘we’, ‘our’ and ‘us’ (Investments, 2017). Although there is great progress in mitigating bias in text, understanding its presence in multimodal speech based analysis, particularly in real world scenarios like corporate earnings calls analysis remain an understudied yet promising research direction. Another study found that despite having identical credibility, female CEOs are perceived as less capable to attract growth capital (Bigelow et al., 2014). 3 Formulation and Experiments Stock volatility Following Kogan et al. (2009); Sawhney et al. (2020c), for a given stock, with a close price of pi on trading day i, we calculate the average log volatility over n days following the day of the earnings call as: s v[0,n] = ln  Pn i=1 (ri − r¯)2 n   pi pi−1 (1) − 1 and Bias in AI and Finance With the advent of AI and Big Data, companies are intelligently using data to measure performance (Newman, 2020). Volatility Prediction Consider each earnings But seldom do enterprises check on the imbal- call E, with aligned audio recordings A and text ance in gathered data. Women still represent fewer transcripts T . The earnings calls are divided int"
2021.naacl-main.294,2020.emnlp-main.643,1,0.578176,"I’, ‘me’ and ‘mine’ whereas women tend to reference the team, like ‘we’, ‘our’ and ‘us’ (Investments, 2017). Although there is great progress in mitigating bias in text, understanding its presence in multimodal speech based analysis, particularly in real world scenarios like corporate earnings calls analysis remain an understudied yet promising research direction. Another study found that despite having identical credibility, female CEOs are perceived as less capable to attract growth capital (Bigelow et al., 2014). 3 Formulation and Experiments Stock volatility Following Kogan et al. (2009); Sawhney et al. (2020c), for a given stock, with a close price of pi on trading day i, we calculate the average log volatility over n days following the day of the earnings call as: s v[0,n] = ln  Pn i=1 (ri − r¯)2 n   pi pi−1 (1) − 1 and Bias in AI and Finance With the advent of AI and Big Data, companies are intelligently using data to measure performance (Newman, 2020). Volatility Prediction Consider each earnings But seldom do enterprises check on the imbal- call E, with aligned audio recordings A and text ance in gathered data. Women still represent fewer transcripts T . The earnings calls are divided int"
2021.naacl-main.294,P14-1109,0,0.0520396,"Missing"
2021.naacl-main.316,N19-1358,0,0.0282089,"Missing"
2021.naacl-main.316,P18-1063,0,0.0223945,"Missing"
2021.naacl-main.316,N19-1423,0,0.00904038,"rive inspiration from Hu et al. (2017); Sawhney et al. (2020, 2021) to design the policy network. However, it is important to note that PROFIT is compatible with any general deep network that is capable of handling time-series of textual data. We specifically adopt the following network as it inherently covers a breadth of com0 0 ponents that are proved beneficial for designing yτ = rτ + γQφ (sτ +1 , µθ (sτ +1 )), (2) language-based systems for stock trading. First, PROFIT’s policy encodes the texts t corφ 2 L = E[(yτ − Q (sτ , aτ )) ] (3) responding to a stock s released in a day using BERT (Devlin et al., 2019). We tokenize and trunwhere yτ is the updated Q-value, γ is a discount cate the input text (t) for each news item or tweet factor, θ and θ0 , φ and φ0 are the two copy paand feed it to BERT. We then aggregate the final rameters of the policy µ and the value function Q, hidden states (the final-layer transformer outputs) respectively. The actor is updated using the policy of the input to get the encoded representation (m, gradient ∇θ J via backpropagation through time as: size 768) as as m = BERT(t) ∈ Rd , d=768. We also experiment with the [CLS] token and other φ θ θ pooling techniques such as"
2021.naacl-main.316,2020.acl-main.307,0,0.433158,"ver, the timing of their release plays a critical role as stock markets rapidly react to new information (Foucault et al., 2016). Furthermore, not each news story or tweet holds the potential to influence stock trends as texts have a diverse influence on prices (Hu et al., 2017). These observations suggest benefits in factoring in the time-aware dependence and diverse influence of text while analyzing natural language. Despite profitability being the prime objective of quantitative trading, existing natural language processing methods for stock prediction (Hu et al., 2017; Xu and Cohen, 2018; Du and Tanaka-Ishii, 2020) are commonly formulated as classification or regression tasks, and are not directly optimized towards profit generation. Such methods face fundamental drawbacks. First, they do not innately incorporate the decision making and strategies involved in quantitative trading, in turn limiting potential profitability. Second, they have limited practical applicability as they do not factor in the monetary resources available and financial assets (stocks) held with a trader at each trading time step. This gap presents a new research direction where profit generation can be directly optimized by modeli"
2021.naacl-main.316,P16-1153,0,0.044851,"Missing"
2021.naacl-main.316,D16-1189,0,0.0468653,"Missing"
2021.naacl-main.316,2020.emnlp-main.469,0,0.0189712,"Missing"
2021.naacl-main.316,N18-1158,0,0.0479949,"Missing"
2021.naacl-main.316,P15-1131,0,0.154576,"± 4.21 CR↑ 24.81 ± 11.56 25.72 ± 13.29 33.25 ± 15.12 40.88 ± 13.04 China & Hong Kong SR↑ StR↑ 1.02 ± 0.29 1.49 ± 0.37 1.03 ± 0.22 1.56 ± 0.43 1.19 ± 0.16 1.67 ± 0.34 1.29 ± 0.32 1.99 ± 0.61 MDD↓ 14.34 ± 5.63 15.88 ± 5.58 10.45 ± 2.36 6.78 ± 6.09 Table 1: Trading performance over different problem formulations (mean of 5 runs). All formulations use the same base architecture defined in PROFIT’s policy network to model stock affecting text over the lookback period. • TSLDA: Topic Sentiment Latent Dirichlet Allocation, a generative model jointly exploiting topics and sentiments in textual data (Nguyen and Shirai, 2015). • StockEmb: Stock embeddings acquired using prices, and dual vector (word-level vectors and context-level vectors) representation of texts (Du and Tanaka-Ishii, 2020). • SN - HFA: StockNet - HedgeFundAnalyst, a variational autoencoder with attention on texts and prices (Xu and Cohen, 2018). • MAN-SF (text only): BERT based hierarchical encoder for financial text using hierarchical temporal attention (Sawhney et al., 2020). • Chaotic: A Hierarchical Attention Network using GRU encoders with temporal attention applied on text within days, and the days in the lookback period (Hu et al., 2017)."
2021.naacl-main.316,P18-1199,0,0.0244141,"Missing"
2021.naacl-main.316,2020.emnlp-main.676,1,0.718917,"each batch B, PROFIT minimizes the following loss L with respect to φ to update the critic as: et al. (2015). Next, we define the trading policy network, which takes the observations at state sτ as input to generate stock trading actions aτ . We use the same architecture for defining the actor and the critic networks. 4.2 Trading Policy Network To generate trading actions, we first learn representations for each stock s ∈ S using the Tday market-information observation om , and the trading-account observation oτ at the time-step τ . For this study, we derive inspiration from Hu et al. (2017); Sawhney et al. (2020, 2021) to design the policy network. However, it is important to note that PROFIT is compatible with any general deep network that is capable of handling time-series of textual data. We specifically adopt the following network as it inherently covers a breadth of com0 0 ponents that are proved beneficial for designing yτ = rτ + γQφ (sτ +1 , µθ (sτ +1 )), (2) language-based systems for stock trading. First, PROFIT’s policy encodes the texts t corφ 2 L = E[(yτ − Q (sτ , aτ )) ] (3) responding to a stock s released in a day using BERT (Devlin et al., 2019). We tokenize and trunwhere yτ is the up"
2021.naacl-main.316,2021.eacl-main.185,1,0.831737,"Missing"
2021.naacl-main.316,N18-1113,0,0.0603385,"Missing"
2021.naacl-main.316,D18-1397,0,0.064681,"Missing"
2021.naacl-main.316,P18-1183,0,0.396373,"ket dynamics (Laakkonen, 2004). For instance, prices immediately react to breaking news about the related company (Busse and Green, 2002). Such reactions conform to the Efficient Market Hypothesis (EMH), a hypothesis in finance which states that financial markets are informationally efficient and prices reflect all available market information at any given time (Malkiel, 1989). The abundance of stock affecting information across news and social media online inspires the adoption of natural language processing to study the interplay between textual data and stock prices (Oliveira et al., 2017; Xu and Cohen, 2018). However, unlike numerical data, the study of natural language is more challenging. Individual tweets or news headlines may not be informative enough, and analyzing them together can provide a greater 4018 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4018–4030 June 6–11, 2021. ©2021 Association for Computational Linguistics context, as shown in Figure 1. Moreover, the timing of their release plays a critical role as stock markets rapidly react to new information (Foucault et al., 2016). Fu"
2021.naacl-main.316,P18-1104,0,0.0649706,"Missing"
2021.naacl-main.387,D16-1084,0,0.0255354,"l. (2018) proposed a multitask ensemble architecture for jointly modeling emotion, sentiment, and intensity, which gave improvements over single-label classification. 3 Problem Description We aim to analyze different perspectives of the complex narratives pertaining to the #MeToo movement on social media platforms. Specifically, given a tweet text, we formulate for it a multi-label multiclass classification problem with definitions taken from previous works (ElSherief et al., 2018) • Stance Detection: Determining the opinion of the author of a tweet, regarding a particular target of interest (Augenstein et al., 2016). Stance detection is categorized into three classes: Support for when the author favors the #MeToo movement or it’s cause; Opposition, representing opposing stance or indifference towards the movement; or Neither, when the text does not have a clear viewpoint (Mohammad and Turney, 2013). it is targeted towards a community or a section of people or Neither otherwise (Basile et al., 2019). • Sarcasm Detection: Given a tweet ti , we aim to map it to either be Sarcastic or Not Sarcastic based on the presence of implicit sarcastic tone of the post (Bamman and Smith, 2015). • Dialogue Act Classific"
2021.naacl-main.387,S19-2007,0,0.0207601,"s classification problem with definitions taken from previous works (ElSherief et al., 2018) • Stance Detection: Determining the opinion of the author of a tweet, regarding a particular target of interest (Augenstein et al., 2016). Stance detection is categorized into three classes: Support for when the author favors the #MeToo movement or it’s cause; Opposition, representing opposing stance or indifference towards the movement; or Neither, when the text does not have a clear viewpoint (Mohammad and Turney, 2013). it is targeted towards a community or a section of people or Neither otherwise (Basile et al., 2019). • Sarcasm Detection: Given a tweet ti , we aim to map it to either be Sarcastic or Not Sarcastic based on the presence of implicit sarcastic tone of the post (Bamman and Smith, 2015). • Dialogue Act Classification: These are a function of a speaker’s utterance during a conversation, for example, question, answer, suggestion, etc., and are classified into three classes, namely Allegation (when the author intends to allege an individual or group of sexual misconduct) (Hutchings, 2012), Justification (tweets where the author is justifying their actions), and Refutation (for when the author refu"
2021.naacl-main.387,D15-1085,0,0.0320617,"Missing"
2021.naacl-main.387,P19-1241,1,0.921812,"is on a publicly available dataset that is created in the backdrop of mass instances of sexual harassment disclosures and includes nuanced labels to identify accompanying linguistic behaviors. Existing literature has emphasized that the text’s emotional attributes have a high correlation with dialogue narratives describing instances of sexual harassment (Lane and Hedin, 2020). Prior works (Anzovino et al., 2018; Sharifirad et al., 2018) have mostly focused on label specific detection of linguistic narratives related to sexual harassment disclosures in isolation by exploiting lexical features (Chowdhury et al., 2019; Karlekar and Bansal, 2018). However, subtle intricacies present in the discussion of sexual abuse disclosures often reflect the speaker’s affective and psychological state, which are overlooked by feature-engineered models. For instance, part (a) of Figure 1 shows a tweet expressing support towards the #MeToo movement but in a tone that might be difficult for naive neural learning models to capture without context. Part (b) of Figure 1 presents a tweet in which the author has an initial positive outlook, which later reverses to disgust for the subject. The lack of context about the event and"
2021.naacl-main.387,P15-2139,0,0.0215124,"for the primary task). Multitask learning frameworks are generally built using either of these two approaches: hard parameter sharing or soft parameter sharing. In a hard parameter sharing model (Caruana, 1997), both the primary and auxiliary tasks have a shared encoder followed by separate task-specific network branches, and the shared encoder is updated by both the tasks alternately. On the other hand, in the soft parameter sharing approach, tasks have different encoders with independent parameters, and the distance between their parameters is regularized using a regularization constraint (Duong et al., 2015; Yang and Hospedales, 2016), to encourage the parameters to be similar. calculate the weighted summation of these two rep˜ (p) , using two learnable parameters, resentations - h α(p) and α(s) (where α(p) +α(s) = 1), as formulated in Equation 7 to regulate the information resulting from the two encoders (Figure 2). ˜ (p) = α(p) h(p) + α(s) h(s) h (7) Such an approach to aggregate information flow from two encoders has facilitated success in prior Multitask learning settings as well (Rajamanickam et al., 2020; Dankers et al., 2019). As for our auxiliary task, we pass the embeddings e(a) through"
2021.naacl-main.387,D16-1136,0,0.0219207,"various sources compensates for missing data and complements existing meta-data (Tan et al., 2013; Ding et al., 2014), thus allowing for effective sharing of task-invariant features (Caruana, 1997; Zhang and Wang, 2016; Zhang et al., 2018). MTL has been utilized for name error recognition (Cheng et al., 2015), tagging-chunking (Collobert et al., 2011), machine translation (Luong et al., 2015) and relation extraction (Gupta et al., 2016). Liu et al. (2017) used shared and private latent features leveraging multitask learning for different text classification tasks. Rajamanickam et al. (2020); Duong et al. (2016); Liu et al. (2016) proposed a joint framework for modeling abuse and emotion detection and showed improvements over STL and transfer learning. Akhtar et al. (2018) proposed a multitask ensemble architecture for jointly modeling emotion, sentiment, and intensity, which gave improvements over single-label classification. 3 Problem Description We aim to analyze different perspectives of the complex narratives pertaining to the #MeToo movement on social media platforms. Specifically, given a tweet text, we formulate for it a multi-label multiclass classification problem with definitions taken fro"
2021.naacl-main.387,D19-1227,0,0.0191049,"their parameters is regularized using a regularization constraint (Duong et al., 2015; Yang and Hospedales, 2016), to encourage the parameters to be similar. calculate the weighted summation of these two rep˜ (p) , using two learnable parameters, resentations - h α(p) and α(s) (where α(p) +α(s) = 1), as formulated in Equation 7 to regulate the information resulting from the two encoders (Figure 2). ˜ (p) = α(p) h(p) + α(s) h(s) h (7) Such an approach to aggregate information flow from two encoders has facilitated success in prior Multitask learning settings as well (Rajamanickam et al., 2020; Dankers et al., 2019). As for our auxiliary task, we pass the embeddings e(a) through only the shared encoder (h(a) = h(s) ), followed by a dropout layer. We use this architecture for Heterogeneous MTL experiments. For Homogeneous MTL ones, we employ hard parameter sharing model due to statistical out-performance in this scenario. This technique consists of a single stacked encoder that is shared and updated by both tasks related to identifying narratives related to sexual abuse disclosures within #MeToo movement, followed by task-specific branches. The shared representations from the encoder are passed through th"
2021.naacl-main.387,N19-1423,0,0.0108224,"d emotion detection as the auxiliary task. 4 4.1 Methodology Text Encoding Building on the success of transformer-based models in NLP, we chose BERTweet (Dat Quoc Nguyen and Nguyen, 2020), a pre-trained language model trained on 850 million English tweets. BERTweet • Hate Speech Identification: Detection of hate has been trained with the same training procedure speech involves labeling the tweets as Di- as RoBERTa (Liu et al., 2019) and has the same rected Hate if the comment is targeted towards model configuration as the BERT base architecan individual or an entity, Generalized Hate if ture (Devlin et al., 2019). The key component in 4883 Task Label #Samples Relevance Relevant 7,249 Support 3,074 Opposition 743 Stance Hate Speech Sarcasm Directed 419 Generalized 281 Sarcastic 220 Allegation 578 Dialogue Acts Justification 292 Refutation 216 Text Guys are pissed off at [name] for affecting the credibility of a sexual assault survivor. Only men and r*p* enablers are questioning the movement in today’s times. #Attack #BringTheChange. Thank you [name] for your courage passion and fight for #MeToo. It gives [name] strength to overcoming all this in front of people. Hope this inspires others as well to bri"
2021.naacl-main.387,C16-1239,0,0.0244124,"rning Frameworks for learning representations across two different sources within the same domain follow multitask learning (Caruana, 1997). The ability to utilize knowledge from various sources compensates for missing data and complements existing meta-data (Tan et al., 2013; Ding et al., 2014), thus allowing for effective sharing of task-invariant features (Caruana, 1997; Zhang and Wang, 2016; Zhang et al., 2018). MTL has been utilized for name error recognition (Cheng et al., 2015), tagging-chunking (Collobert et al., 2011), machine translation (Luong et al., 2015) and relation extraction (Gupta et al., 2016). Liu et al. (2017) used shared and private latent features leveraging multitask learning for different text classification tasks. Rajamanickam et al. (2020); Duong et al. (2016); Liu et al. (2016) proposed a joint framework for modeling abuse and emotion detection and showed improvements over STL and transfer learning. Akhtar et al. (2018) proposed a multitask ensemble architecture for jointly modeling emotion, sentiment, and intensity, which gave improvements over single-label classification. 3 Problem Description We aim to analyze different perspectives of the complex narratives pertaining"
2021.naacl-main.387,W17-2902,0,0.044457,"training of related tasks (Section 6.4), keeping in mind the ethical concerns of communities affected by this research (Section 7). 2 Related Work Sexual Harassment Disclosures on Social Media Several works have focused on identifying sexual violence (Leatherman, 2011), harassment and sexism (Wekerle et al., 2018; Manikonda et al., Moreover, apart from their inherent complex- 2018b) in social media posts by analyzing factors ity, conversations related to the #MeToo movement such as linguistic themes, social engagement, and also pose a challenge of emotional ambiguity. This lexical attributes. Jha and Mamidi (2017) experiwork is the first attempt at joint modeling of narra- mented with algorithms such as SVM and BiLSTM tives related to sexual abuse disclosures and emo- along with fastText to categorize hostility of sexist tion classification to learn the patterns of their inter- posts. (Parikh et al., 2019) proposed a multi-label action via parameter sharing techniques offered by CNN-based neural architecture along with word Multitask Learning (MTL). The affective features, and sentence level embeddings for identifying variwhich result from a joint learning setup through ants of sexism present in online"
2021.naacl-main.387,D18-1303,0,0.0248521,"le dataset that is created in the backdrop of mass instances of sexual harassment disclosures and includes nuanced labels to identify accompanying linguistic behaviors. Existing literature has emphasized that the text’s emotional attributes have a high correlation with dialogue narratives describing instances of sexual harassment (Lane and Hedin, 2020). Prior works (Anzovino et al., 2018; Sharifirad et al., 2018) have mostly focused on label specific detection of linguistic narratives related to sexual harassment disclosures in isolation by exploiting lexical features (Chowdhury et al., 2019; Karlekar and Bansal, 2018). However, subtle intricacies present in the discussion of sexual abuse disclosures often reflect the speaker’s affective and psychological state, which are overlooked by feature-engineered models. For instance, part (a) of Figure 1 shows a tweet expressing support towards the #MeToo movement but in a tone that might be difficult for naive neural learning models to capture without context. Part (b) of Figure 1 presents a tweet in which the author has an initial positive outlook, which later reverses to disgust for the subject. The lack of context about the event and contrasting qualifications"
2021.naacl-main.387,P17-1001,0,0.0238209,"Missing"
2021.naacl-main.387,S18-1001,0,0.030439,"k-specific and shared encoder respectively, for the primary task. Sexual Abuse Disclosures - #MeTooMA This dataset 4 has 9,973 tweets and covers different mutually non-exclusive linguistic annotations related to the #MeToo movement (Gautam et al., 2020). The distribution and statistics about various labels are present in Table 1 and Section 3. We present an instance associated with each of the proposed tasks in Table 1. For our experiments, we focus only on tweets that are annotated as relevant to the #MeToo movement. Emotions - SemEval18 This dataset5 has been taken from SemEval-2018 Task-1 (Mohammad et al., 2018) and covers emotion-specific labels representing the mental state of the authors of the tweets. It consists of 10,986 tweets distributed across 11 emotion labels – (anger, disgust, anticipation, fear, joy, love, optimism, pessimism, sadness, surprise and trust), each being a binary label to indicate the presence of a particular emotion. 5.2 Task Specific Setting detection, independently. We experiment with two distinct embedding spaces – GloVe-Twitter and BERTweet. Based on the superior performance of BERTweet with respect to GloVe-Twitter, we preferred it for further experimentation and studi"
2021.naacl-main.387,D19-1174,0,0.0118914,"le et al., 2018; Manikonda et al., Moreover, apart from their inherent complex- 2018b) in social media posts by analyzing factors ity, conversations related to the #MeToo movement such as linguistic themes, social engagement, and also pose a challenge of emotional ambiguity. This lexical attributes. Jha and Mamidi (2017) experiwork is the first attempt at joint modeling of narra- mented with algorithms such as SVM and BiLSTM tives related to sexual abuse disclosures and emo- along with fastText to categorize hostility of sexist tion classification to learn the patterns of their inter- posts. (Parikh et al., 2019) proposed a multi-label action via parameter sharing techniques offered by CNN-based neural architecture along with word Multitask Learning (MTL). The affective features, and sentence level embeddings for identifying variwhich result from a joint learning setup through ants of sexism present in online social platforms. shared parameters, will encompass the text’s emo- Chowdhury et al. (2019) emphasized the use of lintional content that is likely to be predictive of guistic themes, contextual meta-data, and semantic narratives corresponding to sexual abuse disclo- cues for evaluating human beha"
2021.naacl-main.387,D14-1162,0,0.0881677,"cially because of the involvement of [name]. This has happened far too many times in the past. Table 1: Distribution of labels and examples for all tasks in #MeTooMA dataset. The tweets have been paraphrased for anonymity reasons and personally identifiable information has been censored. We want to caution the readers that examples in this paper, though censored for profanity might contain offensive language. transformer-based models is the token level selfattention (Vaswani et al., 2017) that enables them to generate dynamic contextualized embeddings as opposed to static embeddings of GloVe (Pennington et al., 2014). Let (w1 , w2 , ..., wn ) represent the sequence of tokens from a given tweet t. These tokens are pre-processed and passed through BERTweet3 . We consider embeddings from the last layer of BERTweet and obtain an embedding ei for a given tweet ti . Embedding for each tweet is of dimension m × k, where k represents the dimension size of BERT based model and m represents the maximum length for the tweets. ei = BERT weet(ti ) (1) These representations from Equation 1 are passed through a stacked BiLSTM encoder. Dropout is then applied to these encoded representations h(t) (Equation 4 represents g"
2021.naacl-main.387,2020.acl-main.394,0,0.186733,"ty to utilize knowledge from various sources compensates for missing data and complements existing meta-data (Tan et al., 2013; Ding et al., 2014), thus allowing for effective sharing of task-invariant features (Caruana, 1997; Zhang and Wang, 2016; Zhang et al., 2018). MTL has been utilized for name error recognition (Cheng et al., 2015), tagging-chunking (Collobert et al., 2011), machine translation (Luong et al., 2015) and relation extraction (Gupta et al., 2016). Liu et al. (2017) used shared and private latent features leveraging multitask learning for different text classification tasks. Rajamanickam et al. (2020); Duong et al. (2016); Liu et al. (2016) proposed a joint framework for modeling abuse and emotion detection and showed improvements over STL and transfer learning. Akhtar et al. (2018) proposed a multitask ensemble architecture for jointly modeling emotion, sentiment, and intensity, which gave improvements over single-label classification. 3 Problem Description We aim to analyze different perspectives of the complex narratives pertaining to the #MeToo movement on social media platforms. Specifically, given a tweet text, we formulate for it a multi-label multiclass classification problem with"
2021.naacl-main.387,W18-5114,0,0.0119071,"oo movement on the basis of stance (support or opposition), hate-speech, sarcasm, and dialogue acts (allegation, refutation, or justification of sexual misconduct). We focus our analysis on a publicly available dataset that is created in the backdrop of mass instances of sexual harassment disclosures and includes nuanced labels to identify accompanying linguistic behaviors. Existing literature has emphasized that the text’s emotional attributes have a high correlation with dialogue narratives describing instances of sexual harassment (Lane and Hedin, 2020). Prior works (Anzovino et al., 2018; Sharifirad et al., 2018) have mostly focused on label specific detection of linguistic narratives related to sexual harassment disclosures in isolation by exploiting lexical features (Chowdhury et al., 2019; Karlekar and Bansal, 2018). However, subtle intricacies present in the discussion of sexual abuse disclosures often reflect the speaker’s affective and psychological state, which are overlooked by feature-engineered models. For instance, part (a) of Figure 1 shows a tweet expressing support towards the #MeToo movement but in a tone that might be difficult for naive neural learning models to capture without contex"
N18-2100,S17-2091,0,0.0907755,"of scientific articles and ranking of keyphrases extracted from them using theme-weighted PageRank. Evaluations are performed on benchmark datasets producing state-of-the-art results. 1 Introduction and Background Keyphrases are single or multi-word linguistic units that represent the salient aspects of a document. The task of ranked keyphrase extraction from scientific articles is of great interest to scientific publishers as it helps to recommend articles to readers, highlight missing citations to authors, identify potential reviewers for submissions, and analyze research trends over time (Augenstein et al., 2017). Due to its widespread use, keyphrase extraction has received significant attention from researchers (Kim et al., 2010; Augenstein et al., 2017). However, the task is far from solved and the performances of the present systems are worse in comparison to many other NLP tasks (Liu et al., 2010). Some of the major challenges are the varied length of the documents to be processed, their structural inconsistency and developing strategies that can perform well in different domains (Hasan and Ng, 2014). Methods for automatic keyphrase extraction are mainly divided into two categories: supervised and"
N18-2100,D10-1036,0,0.200263,"s of a document. The task of ranked keyphrase extraction from scientific articles is of great interest to scientific publishers as it helps to recommend articles to readers, highlight missing citations to authors, identify potential reviewers for submissions, and analyze research trends over time (Augenstein et al., 2017). Due to its widespread use, keyphrase extraction has received significant attention from researchers (Kim et al., 2010; Augenstein et al., 2017). However, the task is far from solved and the performances of the present systems are worse in comparison to many other NLP tasks (Liu et al., 2010). Some of the major challenges are the varied length of the documents to be processed, their structural inconsistency and developing strategies that can perform well in different domains (Hasan and Ng, 2014). Methods for automatic keyphrase extraction are mainly divided into two categories: supervised and unsupervised. Supervised methods approach the problem as a binary classification problem Title: Identification of states of complex systems with estimation of admissible measurement errors on the basis of fuzzy information. Abstract: The problem of identification of states of complex systems"
N18-2100,P14-1023,0,0.0159065,"TF-IDF, clustering, and graph-based ranking (Hasan and Ng, 2010; Mihalcea and Tarau, 2004). On the presence of domain-specific data, supervised methods have shown better performance. The unsupervised methods have the advantage of not requiring any training data and can produce results in any domain. With recent advancements in deep learning techniques applied to natural language processing (NLP), the trend is to represent words as dense real-valued vectors, popularly known as word embeddings. These representations of words have been shown to equal or outperform other methods (e.g. LSA, SVD) (Baroni et al., 2014). The embedding vectors, are supposed to preserve the semantic and syntactic similarities between words. They have been shown to be useful for several NLP tasks, like part-of-speech tagging, chunking, named entity recognition, semantic role labeling, syntactic parsing, and speech processing, among others (Collobert et al., 2011). Some of the most popular approaches for training word embeddings are Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014) and Fasttext (Bojanowski et al., 2016). Keyphrase extraction is a fundamental task in natural language processing that facilitates map"
N18-2100,D09-1027,0,0.203195,"Missing"
N18-2100,S10-1055,0,0.0826147,"Missing"
N18-2100,I13-1062,0,0.362339,"Missing"
N18-2100,S15-1013,0,0.277935,"Missing"
N18-2100,D14-1162,0,0.0817368,", popularly known as word embeddings. These representations of words have been shown to equal or outperform other methods (e.g. LSA, SVD) (Baroni et al., 2014). The embedding vectors, are supposed to preserve the semantic and syntactic similarities between words. They have been shown to be useful for several NLP tasks, like part-of-speech tagging, chunking, named entity recognition, semantic role labeling, syntactic parsing, and speech processing, among others (Collobert et al., 2011). Some of the most popular approaches for training word embeddings are Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014) and Fasttext (Bojanowski et al., 2016). Keyphrase extraction is a fundamental task in natural language processing that facilitates mapping of documents to a set of representative phrases. In this paper, we present an unsupervised technique (Key2Vec) that leverages phrase embeddings for ranking keyphrases extracted from scientific articles. Specifically, we propose an effective way of processing text documents for training multi-word phrase embeddings that are used for thematic representation of scientific articles and ranking of keyphrases extracted from them using theme-weighted PageRank. Ev"
N18-2100,C10-2042,0,0.52018,"Missing"
N18-2100,P14-1119,0,0.121621,"authors, identify potential reviewers for submissions, and analyze research trends over time (Augenstein et al., 2017). Due to its widespread use, keyphrase extraction has received significant attention from researchers (Kim et al., 2010; Augenstein et al., 2017). However, the task is far from solved and the performances of the present systems are worse in comparison to many other NLP tasks (Liu et al., 2010). Some of the major challenges are the varied length of the documents to be processed, their structural inconsistency and developing strategies that can perform well in different domains (Hasan and Ng, 2014). Methods for automatic keyphrase extraction are mainly divided into two categories: supervised and unsupervised. Supervised methods approach the problem as a binary classification problem Title: Identification of states of complex systems with estimation of admissible measurement errors on the basis of fuzzy information. Abstract: The problem of identification of states of complex systems on the basis of fuzzy values of informative attributes is considered. Some estimates of a maximally admissible degree of measurement error are obtained that make it possible, using the apparatus of fuzzy set"
N18-2100,P14-3006,0,0.033507,"Missing"
N18-2100,W03-1028,0,0.853726,") for Avg. F1@10 on SemEval 2010 dataset. 3 Experiments and Results respectively. In the evaluation, we check the performance over the top 5, 10 and 15 candidates returned by Key2Vec. The performance of Key2Vec on the metrics is shown in Table 2. Tables 3 and 4 shows a comparison of Key2Vec with some of the state-of-the-art systems giving best performances on the Inspec and SemEval 2010 datasets, respectively. The final ranked keyphrases obtained using the Key2Vec methodology as described in the previous section is evaluated on the popular Inspec and SemEval 2010 datasets. The Inspec dataset (Hulth, 2003) is composed of 2000 abstracts of scientific articles divided into sets of 1000, 500, and 500, as training, validation and test datasets respectively. Each document has two lists of keyphrases assigned by humans - controlled, which are assigned by the authors, and uncontrolled, which are freely assigned by the readers. The controlled keyphrases are mostly abstractive, whereas the uncontrolled ones are mostly extractive (Wang et al., 2015). The Semeval 2010 dataset (Kim et al., 2010) consists of 284 full length ACM articles divided into a test set of size 100, training set of size 144 and trial"
N18-2100,Q15-1017,0,0.0426549,"Missing"
N18-2100,S10-1004,0,0.907772,"on benchmark datasets producing state-of-the-art results. 1 Introduction and Background Keyphrases are single or multi-word linguistic units that represent the salient aspects of a document. The task of ranked keyphrase extraction from scientific articles is of great interest to scientific publishers as it helps to recommend articles to readers, highlight missing citations to authors, identify potential reviewers for submissions, and analyze research trends over time (Augenstein et al., 2017). Due to its widespread use, keyphrase extraction has received significant attention from researchers (Kim et al., 2010; Augenstein et al., 2017). However, the task is far from solved and the performances of the present systems are worse in comparison to many other NLP tasks (Liu et al., 2010). Some of the major challenges are the varied length of the documents to be processed, their structural inconsistency and developing strategies that can perform well in different domains (Hasan and Ng, 2014). Methods for automatic keyphrase extraction are mainly divided into two categories: supervised and unsupervised. Supervised methods approach the problem as a binary classification problem Title: Identification of stat"
N19-3018,E17-2068,0,0.0728069,"Missing"
N19-3018,D14-1181,0,0.00792937,"Missing"
N19-3018,P11-1015,0,0.0269,"ag-of-words approaches tend to have a high recall but lead to high rates of false positives because lexical detection methods classify all messages containing particular terms only. Following this stream of research, our work considers deep learning techniques for the detection of social media disclosures of sexual harassment. CNNs also have been able to generate state of the art results in text classification because of their ability to extract features from word embeddings (Kim, 2014). Recent approaches that concatenate embeddings derived from other tasks with the input at different layers (Maas et al. (2011)) still 5 4.3 Classification For every tweet ti ∈ D, in the dataset, a binary valued value variable yi is used, which can either be 0 or 1. The value 0 indicates that the text belongs to the Non-Disclosure category while 1 indicates Disclosure. The training has been split into three parts as shown in F igure1. • Language Model (LM) - This model is trained from a large corpus of unlabeled data. In this case, a pre-trained Wikipedia Language Model was used. github.com/ramitsawhney27/NAACLSRW19meToo 139 Disclosure # WhenIWas 15 I was molested by my best friend I was sexually assaulted by my step"
N19-3018,W18-6223,1,0.844541,"be treated as a separate language construct, with its own rules and restrictions that need to be adRelated Work Twitter is fast becoming the most widely used source for social media research, both in academia and in industry (Meghawat et al., 2018) (Shah and Zimmermann, 2017). Wekerle et al. (2018) have shown that Twitter is being used for increasing research on sexual violence. Using social media could support at-risk youth, professionals, and academics given the many strengths of employing such a knowledge mobilization tool. Previously, Twitter has been used to tackle mental health issues (Sawhney et al., 2018b) (Sawhney et al., 2018a) and for other social issues like detection of hate speech content online (Mathur et al., 2018). Mahata et al. (2018) have mobilized Twitter to detect information regarding personal intake of medicines. Social media use is free, easy to implement, available to difficult to access popu137 was assaulted raped me groped forced me #WhenIwas abusive drugged inappropriate boyfriend dressed to capture subtle nuances and understand the context better. 3 Data 3.1 Data Collection Typically, it has been difficult to extract data related to sexual harassment due to social stigma"
N19-3018,P18-3013,1,0.846286,"be treated as a separate language construct, with its own rules and restrictions that need to be adRelated Work Twitter is fast becoming the most widely used source for social media research, both in academia and in industry (Meghawat et al., 2018) (Shah and Zimmermann, 2017). Wekerle et al. (2018) have shown that Twitter is being used for increasing research on sexual violence. Using social media could support at-risk youth, professionals, and academics given the many strengths of employing such a knowledge mobilization tool. Previously, Twitter has been used to tackle mental health issues (Sawhney et al., 2018b) (Sawhney et al., 2018a) and for other social issues like detection of hate speech content online (Mathur et al., 2018). Mahata et al. (2018) have mobilized Twitter to detect information regarding personal intake of medicines. Social media use is free, easy to implement, available to difficult to access popu137 was assaulted raped me groped forced me #WhenIwas abusive drugged inappropriate boyfriend dressed to capture subtle nuances and understand the context better. 3 Data 3.1 Data Collection Typically, it has been difficult to extract data related to sexual harassment due to social stigma"
N19-3018,W18-0801,0,0.0183545,"s based on a specific current event”I believe Dr. Ford because the same thing happened to me”. The user assumes that a majority of the readers will be able to gather context from the amount of information provided. However, the system is unable to pick up this nuance because of lack of information about current events. 7 Ethical Considerations Human language processing, and human language touches many parts of life, these areas also have an ethical dimension. For example, languages define linguistic communities, so inclusion and bias become relevant topics. Based on the issues highlighted in (Schmaltz (2018)), we address these as: Error Analysis An analysis has been done to show which texts lead to erroneous and a possible explanation of why that might have been the case. • Non-Serious - ”I got raped at FIFA the last time I played lol” has a flippant tone. However, the model predicted this as Disclosure because of lack of more contextual information. • Privacy: Individual consent from users was not sought as the data was publicly available and attempts to contact the author for research participation could be deemed coercive and may change user behavior. • Third person quote - ”I was followed and"
N19-3018,W18-3504,1,0.836399,"is fast becoming the most widely used source for social media research, both in academia and in industry (Meghawat et al., 2018) (Shah and Zimmermann, 2017). Wekerle et al. (2018) have shown that Twitter is being used for increasing research on sexual violence. Using social media could support at-risk youth, professionals, and academics given the many strengths of employing such a knowledge mobilization tool. Previously, Twitter has been used to tackle mental health issues (Sawhney et al., 2018b) (Sawhney et al., 2018a) and for other social issues like detection of hate speech content online (Mathur et al., 2018). Mahata et al. (2018) have mobilized Twitter to detect information regarding personal intake of medicines. Social media use is free, easy to implement, available to difficult to access popu137 was assaulted raped me groped forced me #WhenIwas abusive drugged inappropriate boyfriend dressed to capture subtle nuances and understand the context better. 3 Data 3.1 Data Collection Typically, it has been difficult to extract data related to sexual harassment due to social stigma but now, an increasing number of people are turning to the Internet to vent their frustration, seek help and discuss sexu"
N19-3018,D15-1309,0,0.0282016,"is motivates our study to build a mediumspecific Language Model for the segregation of tweets containing disclosures of sexual harassment. While there is a developing body of literature on the topic of identifying patterns in the language used on social media that analyze sexual harassment disclosure (Manikonda et al., 2018); (Andalibi et al., 2016), very few attempts have been made to segregate texts containing discussions about sexual abuse from texts containing personal recollections of sexual harassment experiences. Efforts have been made to segregate domestic abuse stories from Reddit by Schrading et al. (2015) and Karlekar and Bansal. However, these approaches do not take into consideration the model’s domain understanding of the syntactic and semantic attributes of the specific medium in which the text is present. In that regard, our paper makes two significant contributions. 1. Generation of a labeled real-world dataset for identifying social media disclosures of sexual abuse, by manual annotation. 2. Comparison of the proposed MediumSpecific Disclosure Language Model architecture for segregation of tweets containing disclosure, with various deep learning architectures and machine learning models"
N19-3018,P15-1150,0,0.0449762,"Missing"
N19-3018,O18-1021,0,0.0249668,"Missing"
N19-3018,N10-1020,0,0.0499913,"allows lengthy submissions, unlike Twitter, and therefore the use of standard English is more common. This allows natural language processing tools trained on standard English to function better. Our method explores the merits of using a Twitter-specific Language Model which can counter the shortcomings of using pre-trained word embeddings derived from other tasks, on a medium like Twitter where the language is informal, and the grammar is often ambiguous. N-gram based Twitter Language Models (Vo et al., 2015) have been previously used to detect events and for analyzing Twitter conversations (Ritter et al., 2010). Atefeh and Khreich (2015) used Emoticon Smoothed Language Models for Twitter Sentiment Analysis. Rother and Rettberg (2018) used the ULMFiT model proposed by Howard and Ruder (2018) to detect offensive tweets in German. Manikonda et al. (2018) try to investigate social media posts discussing sexual abuse by analyzing factors such as linguistic themes, social engagement, and emotional attributes. Their work proves that Twitter is an effective source for human behavior analysis, based on several linguistic markers. Andalibi et al. (2016) attempt to characterize abuse related disclosures into d"
N19-3018,N16-1174,0,0.0730462,"Missing"
N19-3018,S18-1177,0,0.0402068,"Missing"
N19-3019,W17-3013,0,0.0467162,"Missing"
N19-3019,K15-1011,0,0.0241714,"atures: author profiling, historical stylistic features, social network graph embeddings and tweet metadata with an ablation study for validation. 2.2 3. Conducted an extensive quantitative comparison with several traditional and state-of-theart baselines along with an in-depth error analysis to highlight the challenges faced. 2 2.1 Author Profiling The inclusion of author based information has been explored in some tasks related to natural language processing. Waseem and Hovy (2016) utilized gender and location-based information along with text-based features to achieve superior performance. Johannsen et al. (2015) used similar features for syntactic parsing. While it is accepted that such demographic features may improve performance, it is often not possible to extract such features from social media websites like Twitter since this information is often unavailable and unreliable. This has spawned an exciting line of research that makes use of a social graph of interaction between users to derive information about the user. Applications extract information about each user by representing each user as a node in a social graph and creating low dimensional representations usually induced by neural archite"
N19-3019,D14-1181,0,0.00517717,"Missing"
N19-3019,K16-1017,0,0.0971776,"Missing"
N19-3019,D16-1076,0,0.046285,"Missing"
N19-3019,U12-1019,0,0.0189147,"enation layer was picked up as the embedding for the combination, and this was generated for all the users. These embeddings were then used in an LR classifier and a balanced random forest classifier. The results from the balanced random forest classifier were superior and were further used for feature stacking as mentioned in Section 6.2. SNAP-BATNET uses Follower, Mention and RepliedTo embeddings combined using the deep learning approach to generate social graph based features. 6.2 Figure 1: Combining graph embeddings. Figure 2: Feature Stacking: A meta-learning approach (figure taken from (Lui, 2012)). mation were explored. While tweet metadata is sparse, social graph based embeddings are dense in nature. Initially, concatenation was used, and several models were tried by performing ablation studies. It was observed that the performance of the classifiers did not change significantly and in some cases deteriorated as features were concatenated. Therefore, it was reasoned that the feature sets should be combined in a way that would have the ability to join them related to their relative importance and also allow learning of non-linear relationships between them. Instead of using concatenat"
N19-3019,P04-3031,0,0.164119,"by using scikit-learn’s Latent Dirichlet Allocation module (Pedregosa et al., 2011). Only those tokens were considered which occurred at least 10 times in the entire corpus. Due to the unstructured format of the text used in social media, a set of filters were employed to reduce the noise while not losing useful information. 1. A tweet-tokenizer was used 8 to parse the tweet and replace every username mentions, hashtags, and urls with <mention>, <hashtag>and <url>respectively. 2. The tokenized text then underwent stopword removal and was used as an input to WordNet Lemmatizer provided by nltk(Bird and Loper, 2004). Tweet Metadata Features: The count of hashtags, mentions, URLs, and emojis along with the retweet count and favorite count of every tweet was extracted and used as a feature to gain information about the tweets response by the authors environment. User Historical tweets: To gain information about the behavior of the author and their stylistic choices, a collection of their tweets were preprocessed, and stylistic and semantic features such as the averaged GloVe embeddings, NRC sentiment scores and Parts of Speech counts were extracted. Social Graph Features: Grover and Leskovec (2016) describ"
N19-3019,D18-2029,0,0.0218452,"Tree which incrementally builds in stage-wise fashion. It is used as a baseline in (Badjatiya et al., 2017). • GloVe + CNN: A CNN architecture inspired from (Kim, 2014; Badjatiya et al., 2017) was used with filter sizes (3,4,5). • GloVe + LSTM: An LSTM with 50 cells was used along with dropout layers (p = 0.25 and 0.5, preceding and following, respectively). • ELMo: Tensorflow Hub 9 was used to get ELMo(Peters et al., 2018) embeddings which are known to have an excellent performance in several fields including sentiment analysis and text classification. • USE - The Universal Sentence Encoder (Cer et al., 2018) encodes text into high dimensional vectors that can be used for tasks like text classification, semantic similarity, and clustering. Tensorflow Hub was used to get sentence encoding. Each tweet was converted encoded onto a dense 512 feature space. • Sawhney C-LSTM: We replicated the CLSTM architecture used in (Sawhney et al., 9 Methodology: SNAP-BATNET https://tfhub.dev/ 151 Combination Follower+Mentions (CG) Follower+RepliedTo (CG) Mentions+RepliedTo (CG) Follower+Mentions + RepliedTo (CG) Follower+Mentions + RepliedTo (CE) F1 0.808 0.806 0.803 0.807 AP 0.203 0.196 0.197 0.201 0.849 0.268 Ta"
N19-3019,J93-2004,0,0.0694003,"Missing"
N19-3019,W18-5118,1,0.781581,"ges 147–156 c Minneapolis, Minnesota, June 3 - 5, 2019. 2017 Association for Computational Linguistics year to establish a set of signals to help predict depression before its onset. Benton et al. (2017) utilized a novel multitask learning framework to predict atypical mental health conditions with a scope of predicting suicidal behavior but included only text-based features for their multi-task framework. Furthermore, there have been several forays into tweet classification that utilize a similar set of signals for other applications such as detection of abuse, cyberbullying and hate speech (Mathur et al., 2018b), (Mathur et al., 2018a). Waseem and Hovy (2016) used a public dataset and used a collection of features to show the usefulness of gender-based and location-based information in improving the effectiveness of classifiers. Gamb¨ack and Sikdar (2017) developed a CNN model that used both character n-grams and word2vec features in order to improve the classifier performance greatly. Badjatiya et al. (2017) made use of the same benchmarking dataset, provided a set of baselines and used a combination of randomly initialized embeddings along with LSTM and Gradient Boosting Decision Trees to achieve"
N19-3019,N18-2019,0,0.202962,"Missing"
N19-3019,W18-3504,1,0.843612,"ges 147–156 c Minneapolis, Minnesota, June 3 - 5, 2019. 2017 Association for Computational Linguistics year to establish a set of signals to help predict depression before its onset. Benton et al. (2017) utilized a novel multitask learning framework to predict atypical mental health conditions with a scope of predicting suicidal behavior but included only text-based features for their multi-task framework. Furthermore, there have been several forays into tweet classification that utilize a similar set of signals for other applications such as detection of abuse, cyberbullying and hate speech (Mathur et al., 2018b), (Mathur et al., 2018a). Waseem and Hovy (2016) used a public dataset and used a collection of features to show the usefulness of gender-based and location-based information in improving the effectiveness of classifiers. Gamb¨ack and Sikdar (2017) developed a CNN model that used both character n-grams and word2vec features in order to improve the classifier performance greatly. Badjatiya et al. (2017) made use of the same benchmarking dataset, provided a set of baselines and used a combination of randomly initialized embeddings along with LSTM and Gradient Boosting Decision Trees to achieve"
N19-3019,W04-3252,0,0.0818223,"er three graphs, tweets from the dataset and the historical collection were crawled through. Developing a Lexicon of Suicidal Phrases In order to scrape tweets to create the dataset, a lexicon of phrases which could indicate suicidal ideation was created. The top posts, most of which are much larger than tweets, were scraped from three different forums which have an abundance of posts with suicidal ideation. These are r/suicidalthoughts 3 (top 100), r/suicidewatch4 (top 100) and takethislife.com 5 (top 200). Pytextrank 6 is a python module which implements a ranking model for text processing (Mihalcea and Tarau, 2004). This was used to rank and gather the list of the most prominent phrases from these posts. A manual filtering pass was also done to remove posts with little or no suicidal ideation information. The resulting list had 143 phrases such as hit life, think suicide, wanting to die, suicide times, last day, feel pain point, alternate life, time to go, beautiful suicide, hate life. Furthermore, the lexicon was extended by using the lexicon shared in (Sawhney et al., 2018a). 3.2 text user mentions retweet count Data Collection 3.3 Collecting tweets: For each phrase in the curated lexicon, tweets were"
N19-3019,W18-6223,1,0.905282,"al help. It has been found that people suffering from suicidal ideation make use of social media networks to share information about their mental health online (Park et al., 2012) with many having disclosed their suicidal thoughts and plans (Prieto et al., 2014). Therefore it is a pressing issue to be able to utilize the signals available on social media in order to identify individuals who suffer from suicide ideation in an automated manner and offer them the required help and treatment. There exists an active field of research in the field of suicidal ideation detection (O’Dea et al., 2015; Sawhney et al., 2018a) that are able to extract meaningful patterns of behavior from users of social media in order to predict suicidal behavior. These have utilized the information presented in the text of the posts that were shared and utilized both traditional as well as deep learning methods. A rich body of literature exists to show the influence of social interactions of at-risk individuals for their effective detection and treatment. However, to the best of our knowledge, no advances have been made to include information from social engagement, ego networks and other user attributes Suicide is a leading cau"
N19-3019,C18-1093,0,0.148802,"o extract such features from social media websites like Twitter since this information is often unavailable and unreliable. This has spawned an exciting line of research that makes use of a social graph of interaction between users to derive information about the user. Applications extract information about each user by representing each user as a node in a social graph and creating low dimensional representations usually induced by neural architecture (Grover and Leskovec, 2016; Qiu et al., 2018). The application of such graph-based features overcomes the limitation caused by unavailability. Mishra et al. (2018); Qian et al. (2018) use such social graph based features to gain considerable improvement in the task of abuse detection. Tasks like sarcasm detection also gain improvement by Related Work Suicidal Ideation Detection There have been certain advances in the usage of social media to automatically detect cases of suicidal ideation in the past (Sawhney et al., 2018a; De Choudhury et al., 2013; Benton et al., 2017). Cavazos-Rehg et al. (2016) performed a contentbased analysis on a small number of depression related tweets to derive certain qualitative insights into the behavior of users displaying"
N19-3019,P18-3013,1,0.664875,"al help. It has been found that people suffering from suicidal ideation make use of social media networks to share information about their mental health online (Park et al., 2012) with many having disclosed their suicidal thoughts and plans (Prieto et al., 2014). Therefore it is a pressing issue to be able to utilize the signals available on social media in order to identify individuals who suffer from suicide ideation in an automated manner and offer them the required help and treatment. There exists an active field of research in the field of suicidal ideation detection (O’Dea et al., 2015; Sawhney et al., 2018a) that are able to extract meaningful patterns of behavior from users of social media in order to predict suicidal behavior. These have utilized the information presented in the text of the posts that were shared and utilized both traditional as well as deep learning methods. A rich body of literature exists to show the influence of social interactions of at-risk individuals for their effective detection and treatment. However, to the best of our knowledge, no advances have been made to include information from social engagement, ego networks and other user attributes Suicide is a leading cau"
N19-3019,N16-2013,0,0.198496,"5, 2019. 2017 Association for Computational Linguistics year to establish a set of signals to help predict depression before its onset. Benton et al. (2017) utilized a novel multitask learning framework to predict atypical mental health conditions with a scope of predicting suicidal behavior but included only text-based features for their multi-task framework. Furthermore, there have been several forays into tweet classification that utilize a similar set of signals for other applications such as detection of abuse, cyberbullying and hate speech (Mathur et al., 2018b), (Mathur et al., 2018a). Waseem and Hovy (2016) used a public dataset and used a collection of features to show the usefulness of gender-based and location-based information in improving the effectiveness of classifiers. Gamb¨ack and Sikdar (2017) developed a CNN model that used both character n-grams and word2vec features in order to improve the classifier performance greatly. Badjatiya et al. (2017) made use of the same benchmarking dataset, provided a set of baselines and used a combination of randomly initialized embeddings along with LSTM and Gradient Boosting Decision Trees to achieve state of the art performance. which we hypothesiz"
N19-3019,D14-1162,0,0.0909297,"Missing"
N19-3019,N18-1202,0,0.05327,"Bag of Words + GloVe + GBDT: A Bag of Words(BoW) corpus was generated with unigram and bigram features, the averaged pretrained GloVe embeddings were then used on a Gradient Boosting Decision Tree which incrementally builds in stage-wise fashion. It is used as a baseline in (Badjatiya et al., 2017). • GloVe + CNN: A CNN architecture inspired from (Kim, 2014; Badjatiya et al., 2017) was used with filter sizes (3,4,5). • GloVe + LSTM: An LSTM with 50 cells was used along with dropout layers (p = 0.25 and 0.5, preceding and following, respectively). • ELMo: Tensorflow Hub 9 was used to get ELMo(Peters et al., 2018) embeddings which are known to have an excellent performance in several fields including sentiment analysis and text classification. • USE - The Universal Sentence Encoder (Cer et al., 2018) encodes text into high dimensional vectors that can be used for tasks like text classification, semantic similarity, and clustering. Tensorflow Hub was used to get sentence encoding. Each tweet was converted encoded onto a dense 512 feature space. • Sawhney C-LSTM: We replicated the CLSTM architecture used in (Sawhney et al., 9 Methodology: SNAP-BATNET https://tfhub.dev/ 151 Combination Follower+Mentions ("
P19-1241,P18-1031,0,0.0636729,"extract features from word embeddings (Kim, 2014). Recent approaches that concatenate embeddings derived from other tasks with the input at different layers (Maas et al. (2011)) still train from scratch and treat pre-trained embeddings as fixed parameters, limiting their usefulness. A language model that possesses universal properties could be useful in cases where there is a lack of annotated datasets or language resources, which is prevalent in NLP research. We propose a three-part Classification method, based on the Universal Language Model Fine-tuning (ULMFiT) architecture, introduced by (Howard and Ruder, 2018) that enables robust inductive transfer learning for any NLP task, akin to fine-tuning ImageNet models: We use the 3-layer AWD-LSTM architecture proposed by Merity et al. (2017) using the same hyperparameters and no additions other than tuned dropout hyperparameters. Dropouts have been successful in feed-forward and convolutional neural networks, but applying dropouts similarly to an RNNs hidden state is ineffective as it disrupts the RNNs ability to retain long-term dependencies, and may cause overfitting. Our proposed method makes use of DropConnect (Merity et al., 2017), in which, instead o"
P19-1241,E17-2068,0,0.0244778,"Missing"
P19-1241,D18-1303,0,0.260035,"eets containing personal stories of sexual harassment. Manikonda et al. (2018) have carried out a preliminary analysis of the user engagement, discussion topics, word connotations, and sentiment concerning the #metoo movement. Andalibi et al. (2016) have explored anonymity and support seeking during the #metoo movement. However, very few studies have attempted to separate texts containing discussions about sexual harassment from texts containing personal stories of sexual harassment experiences. Efforts have been made to aggregate domestic abuse stories from Reddit by Schrading et al. (2015). Karlekar and Bansal (2018a) have attempted to categorize personal stories into categories like ogling, commenting, groping. Our study aims to help this body of research grow by automating the process of collection of tweets containing recollections of sexual harassment. 2527 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2527–2537 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 1.1 Clinical Perspective Prior research in psychology has demonstrated the importance of social support in combating depression (George et al., 1989). I"
P19-1241,D14-1181,0,0.00551414,"Missing"
P19-1241,W02-0109,0,0.0135474,"ge value of Cohen Kappas inter-annotator agreement κ = 0.83, while the rest fell into the category of Discussion. Our dataset will be made publicly available, following the guidelines mentioned in Section 7 to facilitate further research and analysis on this very pertinent issue 2 . 3.3 Preprocessing The following preprocessing steps were taken as a part of noise reduction: • Extra white spaces, newlines, and special characters were removed from the sentences. • Stopwords corpus was taken from NLTK and was used to eliminate words which provide little to no information about individual tweets (Loper and Bird, 2002). 2 4 The Social Media Language Model (SMLM) Our work considers deep learning techniques for the detection of social media disclosures of sexual harassment. The majority of methods used to study NLP problems employing shallow machine learning models and time-consuming, hand-crafted features suffer from dimensionality problems since linguistic information is usually represented with sparse representations (highdimensional features). (Khatua et al., 2018). Bagof-words approaches tend to have high recall but lead to high rates of false positives because lexical detection methods classify all mess"
P19-1241,P11-1015,0,0.00923384,"ionality problems since linguistic information is usually represented with sparse representations (highdimensional features). (Khatua et al., 2018). Bagof-words approaches tend to have high recall but lead to high rates of false positives because lexical detection methods classify all messages containing particular terms only. CNNs also have been able to generate state of the art results in text classification because of their ability to extract features from word embeddings (Kim, 2014). Recent approaches that concatenate embeddings derived from other tasks with the input at different layers (Maas et al. (2011)) still train from scratch and treat pre-trained embeddings as fixed parameters, limiting their usefulness. A language model that possesses universal properties could be useful in cases where there is a lack of annotated datasets or language resources, which is prevalent in NLP research. We propose a three-part Classification method, based on the Universal Language Model Fine-tuning (ULMFiT) architecture, introduced by (Howard and Ruder, 2018) that enables robust inductive transfer learning for any NLP task, akin to fine-tuning ImageNet models: We use the 3-layer AWD-LSTM architecture proposed"
P19-1241,N10-1020,0,0.134795,"Missing"
P19-1241,P18-3013,1,0.852454,"se inferences can then be used to create online pathways to direct people to health information and assistance and also to generate personalized interventions. Regrettably, the computational methods used to collect, process, and utilize online writing data, as well as the evaluations of these techniques, are still dispersed in the literature. Wekerle et al. (2018) have shown that Twitter is being used for increasing research on sexual violence. Using social media could support at-risk youth, professionals, and academics given the many strengths of employing such a knowledge mobilization tool. Sawhney et al. (2018) have worked on the detection of posts containing suicidal ideation on Twitter. Social media use is free, easy to implement, available to difficult to access populations (e.g., victims of sexual violence), and can reduce the gap between research and practice. Bogen et al. (2018) discusses the social reactions to disclosures of sexual victimization on Twitter. This work suggests that online forums may offer a unique context for disclosing violence and receiving support. Khatua et al. (2018) have explored deep learning techniques to classify tweets of sexual violence, but have not specifically f"
P19-1241,D15-1309,0,0.374193,"for the aggregation of tweets containing personal stories of sexual harassment. Manikonda et al. (2018) have carried out a preliminary analysis of the user engagement, discussion topics, word connotations, and sentiment concerning the #metoo movement. Andalibi et al. (2016) have explored anonymity and support seeking during the #metoo movement. However, very few studies have attempted to separate texts containing discussions about sexual harassment from texts containing personal stories of sexual harassment experiences. Efforts have been made to aggregate domestic abuse stories from Reddit by Schrading et al. (2015). Karlekar and Bansal (2018a) have attempted to categorize personal stories into categories like ogling, commenting, groping. Our study aims to help this body of research grow by automating the process of collection of tweets containing recollections of sexual harassment. 2527 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2527–2537 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 1.1 Clinical Perspective Prior research in psychology has demonstrated the importance of social support in combating depressi"
P19-1241,N16-1174,0,0.0332074,"Missing"
P19-1241,S18-1177,0,0.039417,"Missing"
P19-2038,K16-1017,0,0.0291992,"ges that use inflammatory sectarian language to promote hatred and violence against people. Our work extends on the work done by (Albadi et al., 2018) in terms of exploring the mer273 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 273–280 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics • Is one community more prone to spreading hateful content than the other? stein, 2015); (Chen and Ku, 2016)). Other tasks that have benefited from social representations are sarcasm detection (Amir et al., 2016) and political opinion prediction (Tlmcel and Leon, 2017). To our knowledge, so far there has been no substantial research on using social network graphs as features to analyze and categorize tweets in Arabic. Our work proposes a novel architecture that builds on the current state of the art and improves its performance using community graph features. • Can such information be effectively leveraged to improve the performance of the current state of the art in the detection of religious hate speech within Arabic speaking users? In this paper, we do an in-depth analysis of how adding community f"
P19-2038,C18-1093,0,0.0238951,"ury∗ Manipal Institute of Technology arijit10@gmail.com Aniket Didolkar∗ Manipal Institute of Technology adidolkar123@gmail.com Ramit Sawhney Netaji Subhas Institute of Technology ramits.co@nsit.net.in Rajiv Ratn Shah MIDAS, IIIT-Delhi rajivratn@iiitd.ac.in Abstract its of introducing community interaction as a feature in the detection of religious hate speech in Arabic. Most previous work in the area of hate speech detection has targeted mainly English content (Davidson et al., 2017) (Djuric et al., 2015) (Badjatiya et al., 2017). Author profiling using community graphs has been explored by (Mishra et al., 2018) for abuse detection on Twitter. We propose a novel Cyber Hate Detection approach using multiple twitter graphs and traditional word embeddings. The rapid widespread of social media has led to some undesirable consequences like the rapid increase of hateful content and offensive language. Religious Hate Speech, in particular, often leads to unrest and sometimes aggravates to violence against people on the basis of their religious affiliations. The richness of the Arabic morphology and the limited available resources makes this task especially challenging. The current state-of-theart approaches"
P19-2038,C16-1154,0,0.0285133,"religious minorities (Bonotti, 2017). This makes it important to develop automated tools to detect messages that use inflammatory sectarian language to promote hatred and violence against people. Our work extends on the work done by (Albadi et al., 2018) in terms of exploring the mer273 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 273–280 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics • Is one community more prone to spreading hateful content than the other? stein, 2015); (Chen and Ku, 2016)). Other tasks that have benefited from social representations are sarcasm detection (Amir et al., 2016) and political opinion prediction (Tlmcel and Leon, 2017). To our knowledge, so far there has been no substantial research on using social network graphs as features to analyze and categorize tweets in Arabic. Our work proposes a novel architecture that builds on the current state of the art and improves its performance using community graph features. • Can such information be effectively leveraged to improve the performance of the current state of the art in the detection of religious hate"
P19-2038,P15-1073,0,0.0177046,"antic) cues. Our proposed methodology contends that leveraging Community-Interaction can better help us profile hate speech content on social media. Our proposed ARHNet (Arabic Religious Hate Speech Net) model incorporates both Arabic Word Embeddings and Social Network Graphs for the detection of religious hate speech. 1 Social network graphs are increasingly being used as a powerful tool for NLP applications (Mahata et al., 2018; Shah et al., 2016b), leading to substantial improvement in performance for tasks like text categorization, sentiment analysis, and author attribute identification ((Hovy, 2015); (Yang and Eisenstein, 2015); (Yang et al., 2016). The idea of using this type of information is best explained by the concept of homophily, i.e., the phenomenon that people, both in real life as well as on the Internet, tend to associate more with those who appear similar. Here, similarity can be defined based on various parameters like location, age, language, etc. The basic idea behind leveraging community interaction is that if we have information about members of a community defined by some similarity measure, then we can infer information about a person based on which community they bel"
P19-2038,N16-2013,0,0.0897349,"Missing"
P19-2038,D16-1152,0,0.062389,"nds that leveraging Community-Interaction can better help us profile hate speech content on social media. Our proposed ARHNet (Arabic Religious Hate Speech Net) model incorporates both Arabic Word Embeddings and Social Network Graphs for the detection of religious hate speech. 1 Social network graphs are increasingly being used as a powerful tool for NLP applications (Mahata et al., 2018; Shah et al., 2016b), leading to substantial improvement in performance for tasks like text categorization, sentiment analysis, and author attribute identification ((Hovy, 2015); (Yang and Eisenstein, 2015); (Yang et al., 2016). The idea of using this type of information is best explained by the concept of homophily, i.e., the phenomenon that people, both in real life as well as on the Internet, tend to associate more with those who appear similar. Here, similarity can be defined based on various parameters like location, age, language, etc. The basic idea behind leveraging community interaction is that if we have information about members of a community defined by some similarity measure, then we can infer information about a person based on which community they belong to. For our study, knowing that members of a p"
P19-2039,P11-2099,0,0.0217238,"dural jargon of Parliament, it is tougher for the non-expert citizen to assess the standings of their elected representative. Therefore, conducting stance classification studies on such data is a challenging task with potential benefits. However, the documents tend to be highly tedious and difficult to comprehend, and thus become a barrier to information about political issues and leanings. Sentiment analysis of data from various relevant sources (social media, newspapers, transcripts, etc.) has often given several insights about public opinion, issues of contention, general trends and so on (Carvalho et al., 2011; Loukis et al., 2014). Such techniques have even been used for purposes like predicting election outcomes and the reception of newly-launched products. Since these insights have wide-ranging consequences, it becomes imperative to develop rigorous standards and state-of-the-art techniques for them. One aspect that helps with analyzing such patterns and sentiments is studying about the interconnections and networks underlying such data. Homophily, or the tendency of people to associate with like-minded individuals, is the fundamental aspect of depicting relationships between users of a social n"
P19-2039,L18-1659,0,0.266527,"s that extract information about each user by representing them as a node in the social graph and creating low dimensional representation usually induced by neural architectures (Grover and Leskovec, 2016). Mishra et al. (2018) and Qian et al. (2018) use such social graph-based features to gain considerable improvement in the task of abuse detection in social media. However there has been no work done to model the interaction between the members of the Parliament for the task of stance classification. For studying transcripts of speeches delivered in the House of Commons in the UK Parliament, Abercrombie and Batista-Navarro (2018b) curated a dataset consisting of parliamentary motions and debates as provided in the Hansard transcripts, along with other information like party affiliations and polarities of the motions being discussed. This was followed by carrying out studies on the dataset and developing a sentiment analysis model which also demonstrated the results of motionindependent (one-step) and motion-dependent classification of polarities Abercrombie and Batista-Navarro (2018a). This dataset is used for further analysis in our experiments. vides information on how a speaker is likely to respond to an issue bei"
P19-2039,W18-6241,0,0.392474,"s that extract information about each user by representing them as a node in the social graph and creating low dimensional representation usually induced by neural architectures (Grover and Leskovec, 2016). Mishra et al. (2018) and Qian et al. (2018) use such social graph-based features to gain considerable improvement in the task of abuse detection in social media. However there has been no work done to model the interaction between the members of the Parliament for the task of stance classification. For studying transcripts of speeches delivered in the House of Commons in the UK Parliament, Abercrombie and Batista-Navarro (2018b) curated a dataset consisting of parliamentary motions and debates as provided in the Hansard transcripts, along with other information like party affiliations and polarities of the motions being discussed. This was followed by carrying out studies on the dataset and developing a sentiment analysis model which also demonstrated the results of motionindependent (one-step) and motion-dependent classification of polarities Abercrombie and Batista-Navarro (2018a). This dataset is used for further analysis in our experiments. vides information on how a speaker is likely to respond to an issue bei"
P19-2039,W06-1639,0,0.171391,"pid with the proliferation of social media data and research (?Shah and Zimmermann, 2017; Shah et al., 2016b; Mahata et al., 2018; Shah et al., 2016c,a). Lauderdale and Herzog (2016) presented their method of determining political positions from legislative speech. The datasets were sourced from Irish and US Senate debates. Rheault et al. (2016) examined the emotional polarity variations in speeches delivered in the British parliament over a hundred years. They observed a correlation between the variations in emotional states of a particular period of time and the national economic situation. Thomas et al. (2006) studied the relationships between segments of speeches delivered in the Congress and the overall tone: of opposition or support. A significant amount of research exists on the political temperament across social media websites like Facebook and Twitter. Stieglitz and Dang-Xuan (2012) studied the relationship between the inherent sentiment of politically relevant tweets and the retweet activity. Ceron et al. (2014) proposed methods for determining the political alignments of citizens and tested them on French and Italian-context datasets. Many new findings based on the contemporary political l"
P19-2039,C18-1093,0,0.0290392,"owever, owing to the voluminous quantity, * Indicates equal contribution. 281 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 281–287 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics of a network to help in the task of sentiment analysis. Moreover, there have been applications that extract information about each user by representing them as a node in the social graph and creating low dimensional representation usually induced by neural architectures (Grover and Leskovec, 2016). Mishra et al. (2018) and Qian et al. (2018) use such social graph-based features to gain considerable improvement in the task of abuse detection in social media. However there has been no work done to model the interaction between the members of the Parliament for the task of stance classification. For studying transcripts of speeches delivered in the House of Commons in the UK Parliament, Abercrombie and Batista-Navarro (2018b) curated a dataset consisting of parliamentary motions and debates as provided in the Hansard transcripts, along with other information like party affiliations and polarities of the motion"
P19-2039,N18-2019,0,0.0463401,"Missing"
S19-2122,W18-3504,1,0.892346,"Missing"
S19-2122,W17-3007,0,0.0924157,"Missing"
S19-2122,W17-3013,0,0.0659399,"Missing"
S19-2122,W17-3008,0,0.108801,"Missing"
S19-2122,D18-1471,0,0.0607319,"Missing"
S19-2122,W17-3006,0,0.0757286,"Missing"
S19-2122,D11-1141,0,0.102716,"Missing"
S19-2122,W18-4401,0,0.141229,"Missing"
S19-2122,S19-2010,0,0.0939239,"ed of demeaning words. Both of them are the popular categories of offensive content, widespread in different social media channels. Interests from both academia and industry has led to the organization of related workshops such 683 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 683–690 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics as TA-COS1 , Abusive Language Online2 , and TRAC3 , along with shared tasks such as GermEval (Wiegand et al., 2018) and TRAC (Kumar et al., 2018). The task 6 of SemEval 2019 (Zampieri et al., 2019b) is one such recent effort containing short posts from tweets collected from the Twitter platform and annotated by human annotators with the objective of identifying expressions of offensive language, categorization of offensive language and identifying the target against whom the offensive language is being used, leading to three sub tasks (A, B and C). We only participate in two of them for which we define the problems. Problem Definition Sub-task A - Given a labeled dataset D of tweets, the objective of the task is to learn a classification/prediction function that can predict a label l f"
S19-2122,W17-1101,0,0.0322092,"rly used for web pages (Mahata et al., 2015). Introduction The unrestricted use of offensive language in social media is disgraceful for a progressive society as it promotes the spread of abuse, violence, hatred, and leads to other activities like trolling. Offensive text can be broadly classified as abusive and hate speech on the basis of the context and target of the offense. Hate speech is an act of offending, insulting or threatening a person or a group of similar people on the basis of religion, race, caste, sexual orientation, gender or belongingness to a specific stereotyped community (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018). Abusive speech categorically differs from hate speech because of its casual motive to hurt using general slurs composed of demeaning words. Both of them are the popular categories of offensive content, widespread in different social media channels. Interests from both academia and industry has led to the organization of related workshops such 683 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 683–690 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics as TA-COS1 , Abusive Language"
S19-2122,W17-3003,0,0.162685,"Missing"
S19-2122,W17-3012,0,0.0407375,"Missing"
S19-2122,N19-1144,0,0.133459,"ed of demeaning words. Both of them are the popular categories of offensive content, widespread in different social media channels. Interests from both academia and industry has led to the organization of related workshops such 683 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 683–690 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics as TA-COS1 , Abusive Language Online2 , and TRAC3 , along with shared tasks such as GermEval (Wiegand et al., 2018) and TRAC (Kumar et al., 2018). The task 6 of SemEval 2019 (Zampieri et al., 2019b) is one such recent effort containing short posts from tweets collected from the Twitter platform and annotated by human annotators with the objective of identifying expressions of offensive language, categorization of offensive language and identifying the target against whom the offensive language is being used, leading to three sub tasks (A, B and C). We only participate in two of them for which we define the problems. Problem Definition Sub-task A - Given a labeled dataset D of tweets, the objective of the task is to learn a classification/prediction function that can predict a label l f"
S19-2213,W10-0509,0,0.0808879,"Missing"
S19-2213,D15-1258,0,0.015734,"ction of understanding online opinions and reactions have been limited to developing methods for areas like sentiment analysis and opinion mining (Medhat et al., 2014; Baghel et al., 2018; Kapoor et al., 2018; Mahata et al., 2018a,b; Jangid et al., 2018; Meghawat et al., 2018; Shah and Zimmermann, 2017). Mining and understanding suggestions can open new areas to study consumer behavior and tapping nuggets of information that could be directly linked with the development and enhancement of products (Brun and Hagege, 2013; Dong et al., 2013; Ramanand et al., 2010), improve customer experiences (Negi and Buitelaar, 2015), and aid in understanding the linguistic nuances of giving advice (Wicaksono and Myaeng, 2013). Suggestion mining is a relatively new domain and is challenged by problems such as ambiguity in task formulation and manual annotation, understanding sentence level semantics, figurative expressions, handling long and complex sentences, context dependency, and highly imbalanced class distribution, as already mentioned by (Negi et al., 2018). Similar problems are also observed in the dataset shared by the organizers for the SemEval task, as it is obtained from a real-world application comprising of"
S19-2213,S19-2151,0,0.0498033,"Missing"
S19-2213,W10-0207,0,0.0388025,"as industry. Most of the previous efforts in the direction of understanding online opinions and reactions have been limited to developing methods for areas like sentiment analysis and opinion mining (Medhat et al., 2014; Baghel et al., 2018; Kapoor et al., 2018; Mahata et al., 2018a,b; Jangid et al., 2018; Meghawat et al., 2018; Shah and Zimmermann, 2017). Mining and understanding suggestions can open new areas to study consumer behavior and tapping nuggets of information that could be directly linked with the development and enhancement of products (Brun and Hagege, 2013; Dong et al., 2013; Ramanand et al., 2010), improve customer experiences (Negi and Buitelaar, 2015), and aid in understanding the linguistic nuances of giving advice (Wicaksono and Myaeng, 2013). Suggestion mining is a relatively new domain and is challenged by problems such as ambiguity in task formulation and manual annotation, understanding sentence level semantics, figurative expressions, handling long and complex sentences, context dependency, and highly imbalanced class distribution, as already mentioned by (Negi et al., 2018). Similar problems are also observed in the dataset shared by the organizers for the SemEval task, as it"
W18-3504,W17-3008,0,0.108751,"Missing"
W18-3504,W17-3001,0,0.0916195,"Missing"
W18-3504,W17-3006,0,0.115229,"Missing"
W18-3504,D14-1162,0,0.0812481,"∈ X and yi ∈ Y . In the experiments, Y is the set of all labels for a multi-class classification task, and yi is one of three class labels. Figure 1 shows the architecture of convolutional neural network used in the experiments throughout the paper. CNN models pre-trained on English dataset learn low-level features of the English language. The last few layers are removed and then replaced with fresh layers keeping the initial convolutional layers frozen and retrained on dataset HEOT where it learns to ex7. Transformation of pre-processed tweets into a word vector representation through Glove (Pennington et al., 2014) pre-trained vector embeddings. The version of Glove pre trained word vectors used in our case was Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 200d, 1.42 GB download). 8. The final step in tweet transformation was the creation a word vector sequences that can be fed into the neural network architecture. 4.2 Transfer Learning Transfer learning (Pan and Yang, 2010) is a machine learning paradigm that refers to knowledge transfer from one domain of interest to another, with the aim to reuse already learned features in learning a specialized task. The task from which the system extracts k"
W18-3504,W17-3013,0,0.089175,"Missing"
W18-3504,W17-1101,0,0.024023,". Further, we approach the problem of classification of the tweets in HEOT dataset using transfer learning wherein the proposed model employing Convolutional Neural Networks is pre-trained on tweets in English followed by retraining on Hinglish tweets. 1 Introduction The rampant use of offensive content on social media is destructive to a progressive society as it tends to promote abuse, violence and chaos and severely impacts individuals at different levels. Offensive text can be broadly classified as abusive and hate speech on the basis of the context and target of the offense. Hate speech (Schmidt and Wiegand, 2017) is an act of offending, insulting or threatening a person or a group of similar 18 Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media , pages 18–26, c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics informal language to postcolonial Indian society. Several work like that done by Dwivedi and Sukhadeve (2010) attempted to translate HindiEnglish language into pure English. However, the major challenge in this case is that the grammatical rules of Hinglish are gravely uncertain and user dependent. One of the earliest"
W18-3504,P16-2037,0,0.0183848,"freezing and retraining the models from source to target tasks. The success of transfer learning for analyzing complex cross linguistic textual structures can be extended to include many more tasks involving code-switched and code-mixed data. The future efforts can be directed towards finetuning the neural network models using boosting methods such as gradient boosting (Badjatiya et al., 2017). The experiments here used CNN models for primary training, but other types of deep learning models like LSTM have also been known to show a high affinity for semantic tasks such as sentiment analysis (Wang et al., 2016) and sentence translation (Sutskever et al., 2014). Another possible approach to fine-tune the classification can be to use a stacked ensemble of shallow convolutional neural network (CNN) models as shown by Friedrichs et al. (2018). In recent years, leveraging multimodal information in several multimedia analytics problem has shown great success (Shah, 2016a,b; Shah and Zimmermann, 2017). Thus, in the future, we plan to exploit multimodal information in offensive language detection since the most of existing systems work in unimodal settings. Moreover, since offensive language is closely rela"
W18-5118,W14-3914,0,0.0289597,"Dataset Table 2 spells out tweet distributions across EOT and HOT datasets. HOT is a manually annotated dataset that was created using the Twitter Streaming API3 by selecting tweets having more than 3 Hinglish words. The tweets were collected during the interval of 4 months of November 2017 to February 2018. The tweets were mined by imposing geo-location restriction such that tweets originating only in the Indian subcontinent were made part of the corpus. Inspired by the work of Rudra et al. (2016), tweets were mined from popular Twitter hashtags of viral topics popular across the news feed. Bali et al. (2014) pointed out that Indian social media users have high activity on Facebook pages of a few listed prominent public entities. Hence, we crawled tweets and responses from Twitter handles of sports-persons, political figures, news channels and movie stars. The collected corpus of tweets initially had 25667 tweets which was filtered down to remove tweets containing only URL’s, only images and videos, having less than 3 words, non-English and nonHinglish scripts and duplicates. The annotation of • Sentiment Score (SS): We have used tweet sentiment score evaluated using SentiWordNet (Baccianella et a"
W18-5118,W17-3013,0,0.060505,"Missing"
W18-5118,W15-4322,0,0.0547543,"Missing"
W18-5118,W17-3006,0,0.0344538,"ng only the last dense layers as trainable. The models are finally then tested on the testing section of HOT and results compiled in Table 7. We hypothesize that the performance of both CNN and LSTM should comparatively enhance due to transfer learning as compared to the benchmark due to syntactical degradation of tweets during the pre-processing step. If this process leads to an overall enhancement of model performance on HOT dataset, then the intuition to use transfer learning for transferring pre-learnt semantic features between two syntactically obscure language would hold ground. As per (Park and Fung, 2017), proposed CNN and LSTM architecture for these experiments were designed to have shallow layers as the small size of our dataset runs the risk of overfitting on the data. NLTK was used to eliminate most unproductive words which provide little information about individual tweets. This was followed by transliteration and then translation of each word in Hinglish tweet into the corresponding English word using the Hinglish-English dictionary mentioned in Section 4.1. At this step, the syntax and grammatical notions of the target language were ignored and the resultant tweet was treated as an asso"
W18-5118,C16-1234,0,0.0286175,"ons of our work can be summarized as follows: (2008) demonstrating cross-linguistic interaction on a semantic level. Several attempts to translate the Hindi-English mixed language into pure English have been made previously, but a major hindrance to this progress has been the fact that the structure of language varies due to relative discrepancies in grammatical features (Bhargava et al., 1988). Ravi and Ravi (2016) proved that a combination of TF-IDF features, gain ratio based feature selection, and Radial Basis Function Neural Network work best for sentiment classification in Hinglish text. Joshi et al. (2016) used sub-word level LSTM models for Hinglish sentiment analysis. Efforts to detect offensive text in online textual content have been undertaken previously for other languages as well like German (Ross et al., 2017) and Arabic (Mubarak et al., 2017). Gamb¨ack and Sikdar (2017) used a multichannel HybridCNN architecture to arrive at promising results for hate speech detection in English tweets. Badjatiya et al. (2017) presented a gradient boosted LSTM model with random embeddings to outperform state of the art hate speech detection techniques. Vo et al. (2017) demonstrated the use of multi-cha"
W18-5118,D14-1162,0,0.0815195,"s used to eliminate most unproductive words which provide little information about individual tweets. This was followed by transliteration and then translation of each word in Hinglish tweet into the corresponding English word using the Hinglish-English dictionary mentioned in Section 4.1. At this step, the syntax and grammatical notions of the target language were ignored and the resultant tweet was treated as an assortment of isolated words and phrases to make them eligible for conversion into word vector representation. Step 2: We used multiple word embedding representations such as Glove (Pennington et al., 2014), Twitter word2vec (Godin et al., 2015), and FastText (Bojanowski et al., 2016) embeddings for creating the word embedding layers and to obtain the word sequence vector representations of the processed tweets. Finally, the train-test split of both the datasets was been kept in the ratio of 80:20 for all experiments described in this paper. 3.2 Transfer Learning based Offensive Text Classification Recently, Badjatiya et al. (2017) performed state of the art classification of tweets in English language as racist, sexist or neither using multiple deep learning techniques motivating exploration of"
W18-5118,khapra-etal-2014-transliteration,0,0.0408382,"Missing"
W18-5118,D16-1121,0,0.0208785,"ails of baseline, transfer learning and MIMCT model in Section 4.2 followed by results analysis in Section 4.3. 4.1 Dataset Table 2 spells out tweet distributions across EOT and HOT datasets. HOT is a manually annotated dataset that was created using the Twitter Streaming API3 by selecting tweets having more than 3 Hinglish words. The tweets were collected during the interval of 4 months of November 2017 to February 2018. The tweets were mined by imposing geo-location restriction such that tweets originating only in the Indian subcontinent were made part of the corpus. Inspired by the work of Rudra et al. (2016), tweets were mined from popular Twitter hashtags of viral topics popular across the news feed. Bali et al. (2014) pointed out that Indian social media users have high activity on Facebook pages of a few listed prominent public entities. Hence, we crawled tweets and responses from Twitter handles of sports-persons, political figures, news channels and movie stars. The collected corpus of tweets initially had 25667 tweets which was filtered down to remove tweets containing only URL’s, only images and videos, having less than 3 words, non-English and nonHinglish scripts and duplicates. The annot"
W18-5118,P18-3013,1,0.577753,"ical figures, news channels and movie stars. The collected corpus of tweets initially had 25667 tweets which was filtered down to remove tweets containing only URL’s, only images and videos, having less than 3 words, non-English and nonHinglish scripts and duplicates. The annotation of • Sentiment Score (SS): We have used tweet sentiment score evaluated using SentiWordNet (Baccianella et al., 2010) as a feature to stress on polarity of the tweets. The SS input will be a unidimensional vector denoted by +1 for positive, 0 for neutral and -1 for negative sentiment. • LIWC Features: Inspired by (Sawhney et al., 2018a), Linguistic Inquiry and Word Count (Pennebaker et al., 2007) throws light on various language modalities expressing the linguistic statistical make-up of each text. Table 1 portrays the cumulative linguistic attributes calculated by LIWC2007 to form a LIWC attribute vector of 67 dimension (67D). Moreover, we have excluded numbers 3 141 https://developer.twitter.com/ LIWC Categories Attributes word count (mean), words per sentence, dictionary words, total pronouns, words >6 letters, total functional words, personal pronouns 1st person singular, 1st person plural,2nd person, 3rd person singul"
W18-5118,W18-5907,1,0.851212,"Missing"
W18-5118,W18-3504,1,0.881342,"outperform state of the art hate speech detection techniques. Vo et al. (2017) demonstrated the use of multi-channel CNN-LSTM model for Vietnamese sentiment analysis. The use of transfer learning enables the application of feature-based knowledge transference in domains with disparate feature spaces and data distribution (Pan and Yang, 2010). Pan et al. (2012) gave a detailed explanation about the application of transfer learning for cross-domain, instance-based and feature-based text classification. An important work in this direction of Hinglish offensive text classification was done by by Mathur et al. (2018b) by effectively employing transfer learning. • Building an annotated Hinglish Offensive Tweet (HOT) dataset2 . • We ascertain the usefulness of transfer learning for classifying offensive Hinglish tweets. • We build a novel MIMCT model that outperforms the baseline models on HOT. 3 The remainder of this paper is organized as follows. Sections 2 and 3 discuss the related work and methodologies in detail, respectively. Discussions and evaluations are done in Section 4 followed by conclusion and future work in Section 5. 2 3.1 Methodology Pre-processing The tweets obtained from data sources wer"
W18-5118,W17-3008,0,0.0321251,"o this progress has been the fact that the structure of language varies due to relative discrepancies in grammatical features (Bhargava et al., 1988). Ravi and Ravi (2016) proved that a combination of TF-IDF features, gain ratio based feature selection, and Radial Basis Function Neural Network work best for sentiment classification in Hinglish text. Joshi et al. (2016) used sub-word level LSTM models for Hinglish sentiment analysis. Efforts to detect offensive text in online textual content have been undertaken previously for other languages as well like German (Ross et al., 2017) and Arabic (Mubarak et al., 2017). Gamb¨ack and Sikdar (2017) used a multichannel HybridCNN architecture to arrive at promising results for hate speech detection in English tweets. Badjatiya et al. (2017) presented a gradient boosted LSTM model with random embeddings to outperform state of the art hate speech detection techniques. Vo et al. (2017) demonstrated the use of multi-channel CNN-LSTM model for Vietnamese sentiment analysis. The use of transfer learning enables the application of feature-based knowledge transference in domains with disparate feature spaces and data distribution (Pan and Yang, 2010). Pan et al. (2012)"
W18-5907,W18-5118,1,0.795498,"r SVM based classification. 2 Related Work Several successful attempts in health text mining have shown that social media can act as a rich source of information for public health monitoring (Broniatowski et al., 2015). MA and Eldredge developed an annotated dataset from consumer drug review posts on social media. Twitter data has been used previously for identifying mentions of medication intake (Mahata et al., 2018a,b), monitoring prescription drug abuse (Hanson et al., 2013). The domain of health text mining also extends to include mental health. Past work on identifying hateful behaviour (Mathur et al., 2018a,b) and suicidal behavior on Twitter (Sawhney et al., 2018a). 3 3. Dataset HO: (741 EBDR, 1072 non-EBDR): This represents the held-out dataset, with tweets collected using both approaches stated above and no overlap with PDR and BDC. We utilized the traditional query based tweet mining practice which made the PDR dataset more generic in nature. In addition, we employed a Twitter handle supervision technique in BDC dataset to focus on a larger tweet corpus heavily specific to the positive class. Lastly, the held out dataset was created using both the techniques in conjunction for a fair assess"
W18-5907,W18-3504,1,0.581621,"r SVM based classification. 2 Related Work Several successful attempts in health text mining have shown that social media can act as a rich source of information for public health monitoring (Broniatowski et al., 2015). MA and Eldredge developed an annotated dataset from consumer drug review posts on social media. Twitter data has been used previously for identifying mentions of medication intake (Mahata et al., 2018a,b), monitoring prescription drug abuse (Hanson et al., 2013). The domain of health text mining also extends to include mental health. Past work on identifying hateful behaviour (Mathur et al., 2018a,b) and suicidal behavior on Twitter (Sawhney et al., 2018a). 3 3. Dataset HO: (741 EBDR, 1072 non-EBDR): This represents the held-out dataset, with tweets collected using both approaches stated above and no overlap with PDR and BDC. We utilized the traditional query based tweet mining practice which made the PDR dataset more generic in nature. In addition, we employed a Twitter handle supervision technique in BDC dataset to focus on a larger tweet corpus heavily specific to the positive class. Lastly, the held out dataset was created using both the techniques in conjunction for a fair assess"
W18-5907,P18-3013,0,0.08653,"ful attempts in health text mining have shown that social media can act as a rich source of information for public health monitoring (Broniatowski et al., 2015). MA and Eldredge developed an annotated dataset from consumer drug review posts on social media. Twitter data has been used previously for identifying mentions of medication intake (Mahata et al., 2018a,b), monitoring prescription drug abuse (Hanson et al., 2013). The domain of health text mining also extends to include mental health. Past work on identifying hateful behaviour (Mathur et al., 2018a,b) and suicidal behavior on Twitter (Sawhney et al., 2018a). 3 3. Dataset HO: (741 EBDR, 1072 non-EBDR): This represents the held-out dataset, with tweets collected using both approaches stated above and no overlap with PDR and BDC. We utilized the traditional query based tweet mining practice which made the PDR dataset more generic in nature. In addition, we employed a Twitter handle supervision technique in BDC dataset to focus on a larger tweet corpus heavily specific to the positive class. Lastly, the held out dataset was created using both the techniques in conjunction for a fair assessment of real world cases. The collected corpus of tweets wa"
W18-6223,W18-5907,1,0.794844,"Missing"
W18-6223,W18-5118,1,0.83069,"cted 1 million tweets following the suicides of 26 prominent figures in Japan between 2010 and 2014 and investigated if media coverage of suicides is correlated with an increase in the actual number of suicides. The reciprocal connectivity between authors of suicidal content suggested a ripple effect in tightly-coupled virtual communities (Colombo et al., 2016) thereby concluding that Twitter is an effective source for investigation of virtual self-harm markers and appropriate intervention. Tweet mining has been successfully been applied in detecting social problems on the web as indicated by Mathur et al. (2018a,b,c) and Mahata et al. (2018). Related Work Media communication can have both positive and negative influence on suicidal ideation. A systematic review of all articles in PsycINFO, MEDLINE, EMBASE, Scopus, and CINAH from 1991 to 2011 for language constructs relating to selfharm or suicide by Daine et al. (2013) concluded that internet may be used as an intervention tool for vulnerable individuals under the age of 25. However, not all language constructs containing the word suicide indicate suicidal intent, specific semantic constructs may be used for predicting whether a sentence implies sel"
W18-6223,W18-3504,1,0.77222,"cted 1 million tweets following the suicides of 26 prominent figures in Japan between 2010 and 2014 and investigated if media coverage of suicides is correlated with an increase in the actual number of suicides. The reciprocal connectivity between authors of suicidal content suggested a ripple effect in tightly-coupled virtual communities (Colombo et al., 2016) thereby concluding that Twitter is an effective source for investigation of virtual self-harm markers and appropriate intervention. Tweet mining has been successfully been applied in detecting social problems on the web as indicated by Mathur et al. (2018a,b,c) and Mahata et al. (2018). Related Work Media communication can have both positive and negative influence on suicidal ideation. A systematic review of all articles in PsycINFO, MEDLINE, EMBASE, Scopus, and CINAH from 1991 to 2011 for language constructs relating to selfharm or suicide by Daine et al. (2013) concluded that internet may be used as an intervention tool for vulnerable individuals under the age of 25. However, not all language constructs containing the word suicide indicate suicidal intent, specific semantic constructs may be used for predicting whether a sentence implies sel"
W18-6223,P18-3013,1,0.339642,"best performance of different classifiers. Some semantic constructs are associated with lifetime suicidal ideation as compared to others. A cross-sectional study of suicidal intent in 220,848 Twitter users in their 20s in Japan (Sueki, 2015) concluded that language framing was important for identifying suicidal markers in the text. For example: want to suicide was found to be associated more frequently with a lifetime suicidal intent than want to die in similar sentences. Several of these studies emphasized the influencing power of social media and internet in the study of suicide ideation. (Sawhney et al., 2018a) demonstrated the use of ensembles to approach the detection of suicidal mentions on social media. One of the most concerning issues with suiciderelated content on Twitter is the propagation of harmful ideas through social network graphs. A study by Grandjean (2016) performed a classification of users by influence in digital communities based on graph density and vectors of cen1 3 3.1 Data Data Collection One of the foremost challenges in the domain of suicidal ideation detection is the lack of availability of a public dataset due to privacy and anonymity concerns borne out of social stigma"
W19-3223,D11-1145,0,0.0499976,"a doctor’s office (Leaman et al., 2010). Taking cognizance of this, the International Society of Drug Bulletins in 2005 said, “...patient reporting systems should periodically sample the scattered drug experiences patients reported on the internet...”. This is an upcoming branch which lies at the intersection of information systems and medicine - pharmacovigilance (Leaman et al., 2010). Detecting and tracking information about certain diseases has been the focus of quite a lot of work (Nakhasi et al., 2012; Paul and Dredze, 2011). For instance, cancer investigation (Ofran et al., 2012), flu (Aramaki et al., 2011; Lamb et al., 2013) and depression (De Choudhury et al., 2013; Yazdavar et al., 2017). There has been some work in the domain of pharmacovigilance (Mahata et al., 2018b,a,c; Mathur et al., 2018; Sarker et al., 2018), recently as well. The body of works most relevant to ours is the one which uses transfer learning on health domain. 3 Dataset The dataset for the shared tasks was collected from the social networking website, Twitter. It consists of mentions of drug effects and other health related issues. 1. For the shared task 1, a total of 25,672 tweets are made available for training, out of"
W19-3223,P18-1031,0,0.188464,"n of classes in Train and Validation datasets for Sub-Task-1 (Identifying ADR and non-ADR tweets) 3. We show the use of stacked embeddings combined with BLSTM+CRF tagger for identifying spans mentioning adverse drug reactions in tweets. Normally, data in health domain is harder to get and process. Thus, many researchers have resorted to using transfer learning in order to deal with the data paucity. The works using transfer learning generally use word embeddings in order to improve the generalization of classification to unseen textual cases. In the context of this work we heavily use ULMFit (Howard and Ruder, 2018) and BERT (Devlin et al., 2018) for our experiments and make an initial attempt on how transfer learning in the domain of health works using them for the different text classification tasks of Social Media Mining for Health Workshop. Next, we give a brief description of the datasets used in this work for the different tasks. 4. We also show the use of combining pretrained BERT embeddings with Glove embeddings fed to a BLSTM text classifier for sub-task-1 and sub-task-4. 2 Related Work In general, self reporting of drug effects by patients is a highly noisy source of data. However, even after b"
W19-3223,N13-1097,0,0.0450512,"Missing"
W19-3223,S19-2200,0,0.0246265,"Missing"
W19-3223,W10-1915,0,0.0190084,"t classification tasks of Social Media Mining for Health Workshop. Next, we give a brief description of the datasets used in this work for the different tasks. 4. We also show the use of combining pretrained BERT embeddings with Glove embeddings fed to a BLSTM text classifier for sub-task-1 and sub-task-4. 2 Related Work In general, self reporting of drug effects by patients is a highly noisy source of data. However, even after being noisy, it captures quite a lot of information which might not be available in other cleaner sources of data such as limited clinical trials or a doctor’s office (Leaman et al., 2010). Taking cognizance of this, the International Society of Drug Bulletins in 2005 said, “...patient reporting systems should periodically sample the scattered drug experiences patients reported on the internet...”. This is an upcoming branch which lies at the intersection of information systems and medicine - pharmacovigilance (Leaman et al., 2010). Detecting and tracking information about certain diseases has been the focus of quite a lot of work (Nakhasi et al., 2012; Paul and Dredze, 2011). For instance, cancer investigation (Ofran et al., 2012), flu (Aramaki et al., 2011; Lamb et al., 2013)"
W19-3223,W19-3203,0,0.117657,"Missing"
W19-3223,W18-5907,1,0.821244,"ed drug experiences patients reported on the internet...”. This is an upcoming branch which lies at the intersection of information systems and medicine - pharmacovigilance (Leaman et al., 2010). Detecting and tracking information about certain diseases has been the focus of quite a lot of work (Nakhasi et al., 2012; Paul and Dredze, 2011). For instance, cancer investigation (Ofran et al., 2012), flu (Aramaki et al., 2011; Lamb et al., 2013) and depression (De Choudhury et al., 2013; Yazdavar et al., 2017). There has been some work in the domain of pharmacovigilance (Mahata et al., 2018b,a,c; Mathur et al., 2018; Sarker et al., 2018), recently as well. The body of works most relevant to ours is the one which uses transfer learning on health domain. 3 Dataset The dataset for the shared tasks was collected from the social networking website, Twitter. It consists of mentions of drug effects and other health related issues. 1. For the shared task 1, a total of 25,672 tweets are made available for training, out of which 2,374 contain adverse drug reaction (ADR) mention and the rest (23,298) do not. Only training data was provided by the organizers. For performing our experiments we segmented the provided"
