2005.jeptalnrecital-court.10,devillers-etal-2004-french,1,0.869945,"Missing"
2005.jeptalnrecital-court.10,P01-1037,0,\N,Missing
2005.jeptalnrecital-long.29,P98-1052,0,0.0325592,"Missing"
2005.jeptalnrecital-long.29,P01-1012,0,0.0605697,"Missing"
2009.jeptalnrecital-court.26,rosset-petel-2006-ritel,1,0.885696,"Missing"
2009.jeptalnrecital-court.26,toney-etal-2008-evaluation,1,0.881957,"Missing"
2010.jeptalnrecital-long.27,P06-1034,0,0.0437718,"Missing"
2011.jeptalnrecital-long.10,galibert-etal-2010-named,1,0.875256,"Missing"
2011.jeptalnrecital-long.10,grishman-2010-impact,0,0.0333199,"Missing"
2011.jeptalnrecital-long.10,S07-1007,0,0.0617765,"Missing"
2011.jeptalnrecital-long.10,mota-grishman-2008-ne,0,0.0334153,"Missing"
2011.jeptalnrecital-long.10,P01-1055,0,0.0892782,"Missing"
2011.jeptalnrecital-long.6,2010.jeptalnrecital-long.39,0,0.0910184,"Missing"
2011.jeptalnrecital-long.6,C10-2030,0,0.0255174,"Missing"
2011.jeptalnrecital-long.6,W08-0615,0,0.0669835,"Missing"
2011.jeptalnrecital-long.6,P09-3003,0,0.0664102,"Missing"
2015.jeptalnrecital-long.3,P10-1052,0,0.066151,"Missing"
2015.jeptalnrecital-long.3,moriceau-tannier-2014-french,1,0.877846,"Missing"
2015.jeptalnrecital-long.3,S10-1071,0,0.0564766,"Missing"
2015.jeptalnrecital-long.3,P05-3021,0,0.110123,"Missing"
2016.jeptalnrecital-demo.18,P12-3007,0,0.0728341,"Missing"
2016.jeptalnrecital-demo.18,L16-1433,1,0.870596,"Missing"
2016.jeptalnrecital-demo.18,L16-1147,0,0.0196495,"Missing"
2016.jeptalnrecital-jep.15,goryainova-etal-2014-morpho,1,0.885485,"Missing"
2016.jeptalnrecital-jep.15,nemoto-etal-2008-speech,0,0.0782422,"Missing"
2016.jeptalnrecital-jep.15,vasilescu-etal-2012-cross,0,0.0605319,"Missing"
2016.jeptalnrecital-jep.31,H90-1021,0,0.804144,"Missing"
2016.jeptalnrecital-long.9,C08-2013,0,0.0770964,"Missing"
2017.jeptalnrecital-long.13,P14-1023,0,0.0994656,"Missing"
2017.jeptalnrecital-long.13,D07-1074,0,0.239996,"Missing"
2017.jeptalnrecital-long.13,K16-1026,0,0.0357843,"Missing"
2017.jeptalnrecital-long.13,Q15-1016,0,0.091516,"Missing"
2017.jeptalnrecital-long.13,Q15-1023,0,0.0405613,"Missing"
2017.jeptalnrecital-long.13,P09-1113,0,0.0726146,"Missing"
2017.jeptalnrecital-long.13,Q14-1019,0,0.0635932,"Missing"
2017.jeptalnrecital-long.13,D14-1167,0,0.0489415,"Missing"
2017.jeptalnrecital-long.13,K16-1025,0,0.0395777,"Missing"
2018.jeptalnrecital-court.22,N18-1118,1,0.874305,"Missing"
2018.jeptalnrecital-court.22,2007.mtsummit-papers.11,0,0.102828,"Missing"
2018.jeptalnrecital-court.22,2010.iwslt-papers.10,0,0.0478956,"Missing"
2018.jeptalnrecital-court.22,W15-2501,0,0.0583848,"Missing"
2018.jeptalnrecital-court.22,D17-1263,0,0.0530534,"Missing"
2018.jeptalnrecital-court.22,S13-2029,0,0.127893,"Missing"
2018.jeptalnrecital-court.22,P17-2031,0,0.0230854,"Missing"
2018.jeptalnrecital-court.22,K16-1006,0,0.0647737,"Missing"
2018.jeptalnrecital-court.22,2012.amta-papers.20,0,0.0230853,"Missing"
2018.jeptalnrecital-court.22,W13-3303,0,0.075707,"Missing"
2018.jeptalnrecital-court.22,P02-1040,0,0.100169,"Missing"
2018.jeptalnrecital-court.22,D14-1162,0,0.081919,"Missing"
2018.jeptalnrecital-court.22,W17-4702,0,0.038527,"Missing"
2018.jeptalnrecital-court.22,D17-1301,0,0.051898,"Missing"
2018.jeptalnrecital-long.6,Y09-1013,0,0.0696944,"Missing"
2018.jeptalnrecital-long.6,K16-1018,0,0.0535501,"Missing"
2018.jeptalnrecital-long.6,D17-1076,0,0.0610997,"Missing"
2018.jeptalnrecital-long.6,N16-1030,0,0.0826828,"Missing"
2018.jeptalnrecital-long.6,D17-1010,0,0.036195,"Missing"
2018.jeptalnrecital-long.6,P17-2035,0,0.0263336,"Missing"
2018.jeptalnrecital-long.6,N03-1033,0,0.107712,"Missing"
2020.coling-main.245,Q17-1010,0,0.0351521,"default parameters using three different corpora setup. The first one is a small and task-dependent corpus: training set of MEDIA corpus is used, by keeping all the words due to the small data size. A huge and out-of-domain corpus was used as second setup: the French Wikipedia dump (WIKI), which is composed of 573 million words using a vocabulary size of 923k words. Finally, both corpora (noted WIKI+MEDIA). The common parameters used to train our word embeddings are: window size=5, negative sampling=5, dimension=300. They have been selected based on previous studies (Pennington et al., 2014; Bojanowski et al., 2017). Note that the out of vocabulary (OOV) words are represented by null vectors. 1 https://github.com/XuezheMax/NeuroNLP2 the code an data needed to run the experiments are available here: https://github.com/saharghannay/MEDIA Eval 3 MEDIA is publicly available for academic use: https://catalogue.elra.info/en-us/repository/browse/ELRA-S0272/ 2 2723 3.3 Results 3.3.1 Embeddings update Those experiments aim to observe the impact of the update (noted update) of CBOW word embeddings or their freeze (noted freeze) while training of SLU module. We proposed different training setups for the word embedd"
2020.coling-main.245,bonneau-maynard-etal-2006-results,0,0.328034,"en frozen during training (are not updated), since Lebret et al. (2013) show that finetuned word embeddings show very similar performance and provide comparable results. However, the evaluation of whether updating the embeddings during SLU model training improves or not the results, for SLU task, is less studied. In addition, their SLU model is fed only with word embeddings, and they did not use any additional features, thus there are some rows for improvements. Finally, B´echet and Raymond (2019) benchmarked several SLU tasks and proposed a difficulty hierarchy in which the MEDIA evaluation (Bonneau-Maynard et al., 2006) seems to be the most difficult SLU task. Contributions: This study focuses on a French SLU task: the MEDIA evaluation, in which we firstly propose the evaluation of whether updating the word embeddings during training can improve the results, according to several scenarios. Secondly, we propose to use a BiLSTM-CNN architecture (Ma, 2016) that This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/ Licence details: 2722 Proceedings of the 28th International Conference on Computational Linguistics, pages 2722–2727 Barcelo"
2020.coling-main.245,N19-1423,0,0.166619,"identifies text spans that mention semantic concepts. SLU task can fall into three sub-tasks: domain classification, intent classification, and slotfilling (Tur and Mori, 2011). The latter is the task that interests us in this study. Over the past five years, the studies developed for SLU task are based on neural network architectures (Yao et al., 2014; Mesnil et al., 2015; Guo et al., 2014; Zhang and Wang, 2016; Dinarelli et al., 2017; Simonnet et al., 2017; Korpusik et al., 2019; Ghannay et al., 2020). Recent approaches take benefit from contextual or language model embeddings such as BERT (Devlin et al., 2019). Korpusik et al. (2019) investigated the transfer ability of a pre-trained BERT representation for English SLU tasks. But, as far as we know, there are no such studies on a French SLU task. Following Ghannay et al. (2020)’s study, many avenues can be explored. In their study, the word embeddings have been frozen during training (are not updated), since Lebret et al. (2013) show that finetuned word embeddings show very similar performance and provide comparable results. However, the evaluation of whether updating the embeddings during SLU model training improves or not the results, for SLU tas"
2020.coling-main.245,2020.lrec-1.302,0,0.0691953,"Missing"
2020.coling-main.245,D14-1162,0,0.0857811,"hich is trained using the default parameters using three different corpora setup. The first one is a small and task-dependent corpus: training set of MEDIA corpus is used, by keeping all the words due to the small data size. A huge and out-of-domain corpus was used as second setup: the French Wikipedia dump (WIKI), which is composed of 573 million words using a vocabulary size of 923k words. Finally, both corpora (noted WIKI+MEDIA). The common parameters used to train our word embeddings are: window size=5, negative sampling=5, dimension=300. They have been selected based on previous studies (Pennington et al., 2014; Bojanowski et al., 2017). Note that the out of vocabulary (OOV) words are represented by null vectors. 1 https://github.com/XuezheMax/NeuroNLP2 the code an data needed to run the experiments are available here: https://github.com/saharghannay/MEDIA Eval 3 MEDIA is publicly available for academic use: https://catalogue.elra.info/en-us/repository/browse/ELRA-S0272/ 2 2723 3.3 Results 3.3.1 Embeddings update Those experiments aim to observe the impact of the update (noted update) of CBOW word embeddings or their freeze (noted freeze) while training of SLU module. We proposed different training"
2020.coling-main.245,N18-1202,0,0.031204,"memBERT model was applied on the resulting data to extract the embeddings of 768 dimensions, for each sub-word from the last transformer layer. The token embeddings corresponds to the sum of its sub-word embeddings. A new CBOW embeddings is trained on WIKI data with dimension 768 to have comparable results. Results (last 2 lines) show that the use of CamemBERT contextual embeddings achieves competitive results in comparison to CBOW embeddings whatever the architecture used (BiLSTM or BiLSTM-CNN). Those results corroborate the results reported by Ghannay et al. (2020), in which, CBOW and ELMo (Peters et al., 2018) embeddings obtained comparable results in terms of F1 score (86.06 vs. 86.42). Last, the results with BiLSTM and BiLSTM-CNN architectures reveals the importance of character embeddings, even when they are combined with contextual embeddings. 4 Conclusions and future work The paper presented a study focuses on French Spoken Language Understanding (SLU) task using the MEDIA corpus. First we proposed the evaluation of whether updating the word embeddings during training improves or not the results, according to several scenarios.Second, we proposed to use a BiLSTMCNN architecture that integrates"
2020.jeptalnrecital-jep.8,galibert-etal-2014-etape,0,0.0354513,"Missing"
2020.jeptalnrecital-jep.8,gravier-etal-2012-etape,0,0.060583,"Missing"
2020.jeptalnrecital-jep.8,W11-0411,1,0.781368,"Missing"
2020.jeptalnrecital-jep.8,J98-4004,0,0.318247,"Missing"
2020.jeptalnrecital-jep.8,N16-1030,0,0.152007,"Missing"
2020.jeptalnrecital-jep.8,P10-1052,0,0.0882307,"Missing"
2020.jeptalnrecital-jep.8,P16-1101,0,0.0527671,"Missing"
2020.lrec-1.556,I11-1142,1,0.788645,"TAPE (Galibert et al., 2014). Since the ETAPE’s results publication in 2012, no new work were published, to the best of our knowledge, on named entity recognition from speech for Quaero-like treestructured French data. Tree-structured named entities can not be tackled as a simple sequence labeling task. At the time of the ETAPE campaign, state-of-the-art works focused on multiple processing steps before rebuilding a tree structure. Conditional Random Field (Lafferty et al., 2001) (CRF) are in the core of these previous sequence labeling approaches. Some approaches (Dinarelli and Rosset, 2012; Dinarelli and Rosset, 2011) used Probabilistic Context-Free Grammar (Johnson, 1998) (PCFG) in complement of CRF to implement a cascade model. CRF was trained on components information and PCFG was used to predict the whole entity tree. The ETAPE winning NER system (Raymond, 2013) only used CRF models with one model per base entity. Most of the typical approaches for named entity recognition from speech follows a two steps pipeline, with first an ASR system and then a NER system on automatic transcriptions produced by the ASR system. In this configuration, the NER component must deal with an imperfect transcription of sp"
2020.lrec-1.556,E12-1018,1,0.802446,"French evaluation campaign ETAPE (Galibert et al., 2014). Since the ETAPE’s results publication in 2012, no new work were published, to the best of our knowledge, on named entity recognition from speech for Quaero-like treestructured French data. Tree-structured named entities can not be tackled as a simple sequence labeling task. At the time of the ETAPE campaign, state-of-the-art works focused on multiple processing steps before rebuilding a tree structure. Conditional Random Field (Lafferty et al., 2001) (CRF) are in the core of these previous sequence labeling approaches. Some approaches (Dinarelli and Rosset, 2012; Dinarelli and Rosset, 2011) used Probabilistic Context-Free Grammar (Johnson, 1998) (PCFG) in complement of CRF to implement a cascade model. CRF was trained on components information and PCFG was used to predict the whole entity tree. The ETAPE winning NER system (Raymond, 2013) only used CRF models with one model per base entity. Most of the typical approaches for named entity recognition from speech follows a two steps pipeline, with first an ASR system and then a NER system on automatic transcriptions produced by the ASR system. In this configuration, the NER component must deal with an"
2020.lrec-1.556,galibert-etal-2014-etape,0,0.730244,"seeks to locate and classify named entity mentions in unstructured text into pre-defined categories (such as person names, organizations, locations, ...). Quaero project (Grouin et al., 2011) is at the initiative of an extended definition of named entity for French data. This extended version has a multilevel tree structure, where base entities are combined to define more complex ones. With the extended definition, named entity recognition consists in the detection, the classification and the decomposition of the entities. This new definition was used for the French evaluation campaign ETAPE (Galibert et al., 2014). Since the ETAPE’s results publication in 2012, no new work were published, to the best of our knowledge, on named entity recognition from speech for Quaero-like treestructured French data. Tree-structured named entities can not be tackled as a simple sequence labeling task. At the time of the ETAPE campaign, state-of-the-art works focused on multiple processing steps before rebuilding a tree structure. Conditional Random Field (Lafferty et al., 2001) (CRF) are in the core of these previous sequence labeling approaches. Some approaches (Dinarelli and Rosset, 2012; Dinarelli and Rosset, 2011)"
2020.lrec-1.556,giraudel-etal-2012-repere,0,0.0475205,"Missing"
2020.lrec-1.556,goryainova-etal-2014-morpho,1,0.845632,") and test (7 hours). These data have manual transcriptions and are fully manually annotated with named entities concepts. Our training data were augmented with the Quaero corpus (Grouin et al., 2011). This corpus is composed of data recorded from French radio and TV stations between 1998 and 2004. These data are made up of 100 hours of speech manually transcribed and fully annotated with named entities following the Quaero annotation guideline. 5.2. Automatic speech recognition In this study, we used several corpora (ESTER 1&2 (Galliano et al., 2009), REPERE (Giraudel et al., 2012) and VERA (Goryainova et al., 2014)) for a total of around 220 hours of speech. These data are used for the acoustic model training of the kaldi ASR system of the pipeline approach. The LM of this approach was trained using the speech transcripts augmented with several French newspapers (see section 4.2.3 in (Del´eglise et al., 2009)). For ASR parts, our pipeline system and our E2E system use the same dataset except for the speech of ETAPE train dataset which is used only with our E2E approach. 6. Experiments All our experiments are evaluated on the ETAPE test set with the Slot Error Rate (SER) metric (Makhoul et al., 1999) def"
2020.lrec-1.556,gravier-etal-2012-etape,0,0.0460721,"Missing"
2020.lrec-1.556,W11-0411,1,0.682334,"to do structured NER. Finally, we compare the performances of ETAPE’s systems (state-of-the-art systems in 2012) with the performances obtained using current technologies. The results show the interest of the E2E approach, which however remains below an updated pipeline approach. Keywords: Named Entity Recognition, Automatic Speech Recognition, Tree-structured Named Entity, End-to-End 1. Introduction Named entity recognition seeks to locate and classify named entity mentions in unstructured text into pre-defined categories (such as person names, organizations, locations, ...). Quaero project (Grouin et al., 2011) is at the initiative of an extended definition of named entity for French data. This extended version has a multilevel tree structure, where base entities are combined to define more complex ones. With the extended definition, named entity recognition consists in the detection, the classification and the decomposition of the entities. This new definition was used for the French evaluation campaign ETAPE (Galibert et al., 2014). Since the ETAPE’s results publication in 2012, no new work were published, to the best of our knowledge, on named entity recognition from speech for Quaero-like treest"
2020.lrec-1.556,J98-4004,0,0.294477,"n 2012, no new work were published, to the best of our knowledge, on named entity recognition from speech for Quaero-like treestructured French data. Tree-structured named entities can not be tackled as a simple sequence labeling task. At the time of the ETAPE campaign, state-of-the-art works focused on multiple processing steps before rebuilding a tree structure. Conditional Random Field (Lafferty et al., 2001) (CRF) are in the core of these previous sequence labeling approaches. Some approaches (Dinarelli and Rosset, 2012; Dinarelli and Rosset, 2011) used Probabilistic Context-Free Grammar (Johnson, 1998) (PCFG) in complement of CRF to implement a cascade model. CRF was trained on components information and PCFG was used to predict the whole entity tree. The ETAPE winning NER system (Raymond, 2013) only used CRF models with one model per base entity. Most of the typical approaches for named entity recognition from speech follows a two steps pipeline, with first an ASR system and then a NER system on automatic transcriptions produced by the ASR system. In this configuration, the NER component must deal with an imperfect transcription of speech. As a result, the quality of automatic transcriptio"
2020.lrec-1.556,N16-1030,0,0.0999424,"r named entity recognition from speech follows a two steps pipeline, with first an ASR system and then a NER system on automatic transcriptions produced by the ASR system. In this configuration, the NER component must deal with an imperfect transcription of speech. As a result, the quality of automatic transcriptions has a major impact on NER performances (Ben Jannet et al., 2015). In 2012, HMM-GMM implementations were still the stateof-the-art approaches for ASR technologies. Since this date, the great contribution of neural approaches for NER and ASR tasks were demonstrated. Recent studies (Lample et al., 2016; Ma and Hovy, 2016) improve the NER accuracy by using a combination of bidirectional Long Short-Term Memory (bLSTM) and CRF layers. Other studies (Tomashenko et al., 2016) are based on a combination of HMM and Deep Neural Network (DNN) to reach ASR state-of-the-art performances. Lately, some E2E approaches for Named Entity Recognition from speech have been proposed in (Ghannay et al., 2018). In this work, the E2E systems will learn an alignment between audio and manual transcription enriched with NE without tree-structure. Other works use End-to-End approach to map directly speech to intent i"
2020.lrec-1.556,P10-1052,0,0.0667031,"Missing"
2020.lrec-1.556,P16-1101,0,0.420815,"ition from speech follows a two steps pipeline, with first an ASR system and then a NER system on automatic transcriptions produced by the ASR system. In this configuration, the NER component must deal with an imperfect transcription of speech. As a result, the quality of automatic transcriptions has a major impact on NER performances (Ben Jannet et al., 2015). In 2012, HMM-GMM implementations were still the stateof-the-art approaches for ASR technologies. Since this date, the great contribution of neural approaches for NER and ASR tasks were demonstrated. Recent studies (Lample et al., 2016; Ma and Hovy, 2016) improve the NER accuracy by using a combination of bidirectional Long Short-Term Memory (bLSTM) and CRF layers. Other studies (Tomashenko et al., 2016) are based on a combination of HMM and Deep Neural Network (DNN) to reach ASR state-of-the-art performances. Lately, some E2E approaches for Named Entity Recognition from speech have been proposed in (Ghannay et al., 2018). In this work, the E2E systems will learn an alignment between audio and manual transcription enriched with NE without tree-structure. Other works use End-to-End approach to map directly speech to intent instead of map speech"
2020.lrec-1.556,E99-1023,0,0.156231,"de <loc.adm.town <name paris &gt; &gt; &gt;”. org.adm/loc.adm.town are Named Entities types with subtypes and kind/name are components. With the Quaero definition of named entity, NER consists in entity detection, classification and decomposition. Since this new definition is used for the French evaluation campaign ETAPE, the task in this study consists in Quaero named entity extraction from speech. 3. 3.1. problem by reducing all labels related to a word into a single one. Figure 1 illustrates an example of this concatenation. Pipelines systems 3-pass implementation Our NER systems use standard BIO2 (Sang and Veenstra, 1999) format. This standard consists of writing a column file with first the words column and then the labels column. There is one couple of word/label per line and two different sentences are separated by an empty line. The label of a word corresponds to the named entity concept in which the word is located. This label is prefixed by a ”B-” or an ”I-” depending on the position of the word in the concept. ”B-” (Begin) is used to prefixed the label of the first word and ”I-” (Inside) for all the others. ”O” (Outside) is the label used for words that are not inside a concept. Due to the structure of"
2020.repl4nlp-1.12,D19-1410,0,0.028734,"igure 1) can be used to redefine the model’s loss function. In the domain of face recognition, many loss functions (Schroff et al., 2015; Wen et al., 2016; Liu et al., 2017; Wang et al., 2018; Deng et al., 2019) have been proposed to learn better face representations, motivated by high intra-class variability due to lighting, position or background. Other studies have experimented with these methods in different domains with similar characteristics, like speaker verification (Bredin, 2017; Chung et al., 2018; Yadav and Rai, 2018), and even as an enhancement of BERT’s sentence representations (Reimers and Gurevych, 2019) for semantic textual similarity. A recent study (Srivastava et al., 2019) has also focused on comparing these methods on face verification, showing that angular margin losses achieve superior performance. On the other hand, the automatic misogyny identification (AMI) evaluation campaign (Fersini et al., 2018a) was proposed to address misogyny on tweets. Included tasks were identification (i.e. misogynous or not), categorization over five different misogyny types, and target identification (to an individual or a group). However, no participant has proposed a metric learning model. The best sys"
2020.repl4nlp-1.12,N19-1423,0,0.470138,"ase all our code as open source for easy reproducibility. Moreover, we find that almost every loss function performs equally well in this setting, matching the regular cross entropy loss. 1 Introduction Whether it is at the word or at the sentence level, learning robust representations allows neural networks to consolidate knowledge that can later be transferred to other tasks and domains. Many approaches have dealt with this problem in different ways, for instance with CBOW or skip-gram from word2vec (Mikolov et al., 2013) for contextindependent word embeddings, or more recently with BERT’s (Devlin et al., 2019) sentence embeddings and contextual word embeddings. In order to learn sentence representations, a neural encoder enc needs to learn a mapping from an initial representation xi to a target vector space. In a metric learning approach, the distances between each pair of sentence embeddings (enc(xi ), enc(xj )) should be low if classes yi = yj (intra-class compactness) and high if yi 6= yj (interclass separability). To achieve this objective, the angle θij separating a pair of embeddings (as depicted 89 Proceedings of the 5th Workshop on Representation Learning for NLP (RepL4NLP-2020), pages 89–9"
2020.semeval-1.172,baccianella-etal-2010-sentiwordnet,0,0.0495204,"in (Online), December 12, 2020. 2 Background Sentiment classification is the task of detecting whether a textual item (e.g., a product review, a blog post, an editorial, etc.) expresses a POSITIVE or a NEGATIVE opinion in general or about a given entity, e.g., a product, a person, a political party, or a policy (Nakov et al., 2016). Classifying tweets according to sentiment has many applications in political science, social sciences, market research, and many others (Mart´ınez-C´amara et al., 2014; Mejova et al., 2015). Although initially sentiment identification was focused on newswire text (Baccianella et al., 2010), later research turned towards social media (Rosenthal et al., 2015). Since 2013, a sentiment classification task on Twitter data have been organized in SemEval campaigns. Most of the earlier approaches to this problem were based on hand crafted features and sentiment lexicons (Pak and Paroubek, 2010; Mohammad et al., 2013). These features were then used as input to classifiers (e.g., Support Vector Machines). However, such approaches required extensive domain knowledge, were laborious to define, and can lead to incomplete or over-specific features. Recently, researchers pay their attention t"
2020.semeval-1.172,Q17-1010,0,0.0565339,"token is replaced with a TOPIC token. For example, the tokens # and LoveIsLove are merged and replaced with a TOPIC token. • Emoji’s with text were divided into two tokens. For example, he, becomes he and ,. • If a token contains more than one emoji, each emoji was considered as a token. For example, ,/ becomes , and /. Embeddings: Following Collobert et al. (2011), a lot of authors argued that word embedding plays a vital role to improve natural language task performance. Hence, we experimented the use of word embeddings to improve the performance of our proposed models. Using the fastText (Bojanowski et al., 2017), we prepared two embedding models: Skip-gram and Cbow. After empirically evaluating the performance on validation set, the embeddings‘ dimensionality was set to 300 for all the embeddings. The embeddings are trained on training data using the parameters: lr=0.05, context window=5, epochs=5, minimal number of word occurences=5, dimensionality=300. Experiment: We carried out two experiments with similar settings except different word embedding approaches: Skip-gram for SkipGRun, and Cbow for CbowRun. Hyper-parameters: After evaluating the model performance on the validation data, the optimal va"
2020.semeval-1.172,J81-4005,0,0.486121,"Missing"
2020.semeval-1.172,N19-1423,0,0.03792,"we pre-processed the CM tweets and proposed a Recurrent Convolutional Neural Network for the sentiment analysis of CM tweets. We submitted two runs and obtaining promising results: our best run obtained 0.691 of F1 averaged across the positives, negatives and the neutral. We observed that the proposed architecture occasionally strives to separate the positive and negative polarities from the neutral and vice versa. For future work, we will explore the performance of our model with larger corpora against the testing set. Also, we would like to investigate other embedding choices such as BERT (Devlin et al., 2019). Moreover, due to the impact that irony and sarcasm have on sentiment analysis (Hern´andez Farıas and Rosso, 2016) it would be interesting to apply deep learning techniques to detect irony (Zhang et al., 2019) but in a code-mixed scenario. Acknowledgements The research work of the first four authors was supported by ERA-Net CHIST-ERA LIHLITH Project funded by ANR (France) project ANR-17-CHR2-0001-03. The research work of the last author was partially funded by the Spanish MICINN under the project MISMIS-FAKEnHATE on Misinformation and Miscommunication in social media: FAKE news and HATE speec"
2020.semeval-1.172,C16-1234,0,0.0267187,"were based on hand crafted features and sentiment lexicons (Pak and Paroubek, 2010; Mohammad et al., 2013). These features were then used as input to classifiers (e.g., Support Vector Machines). However, such approaches required extensive domain knowledge, were laborious to define, and can lead to incomplete or over-specific features. Recently, researchers pay their attention to sentiment polarity detection on CM data. However, a few research work have been carried out in particular Hindi-English CM data with different approaches: lexicon lookup (Sharma et al., 2015), sub-word with CNN-LSTM (Joshi et al., 2016), Siamese networks (Choudhary et al., 2018), dual Encoder Network with features (Lal et al., 2019). Lai et al. (2015) proposed Recurrent Convolutional Network for text classification which is a foundational task in many NLP applications. We followed this model in our task. 3 System overview We are inspired by the model proposed in (Lai et al., 2015) particularly proposed for the text classification task. The proposed model takes sequence of CM words as input and provides sentiment polarity class as output. The recurrent structure of the proposed model captures the contextual information during"
2020.semeval-1.172,P19-2052,0,0.0137829,", 2013). These features were then used as input to classifiers (e.g., Support Vector Machines). However, such approaches required extensive domain knowledge, were laborious to define, and can lead to incomplete or over-specific features. Recently, researchers pay their attention to sentiment polarity detection on CM data. However, a few research work have been carried out in particular Hindi-English CM data with different approaches: lexicon lookup (Sharma et al., 2015), sub-word with CNN-LSTM (Joshi et al., 2016), Siamese networks (Choudhary et al., 2018), dual Encoder Network with features (Lal et al., 2019). Lai et al. (2015) proposed Recurrent Convolutional Network for text classification which is a foundational task in many NLP applications. We followed this model in our task. 3 System overview We are inspired by the model proposed in (Lai et al., 2015) particularly proposed for the text classification task. The proposed model takes sequence of CM words as input and provides sentiment polarity class as output. The recurrent structure of the proposed model captures the contextual information during the learning of the word representation, and the max-pooling layer identifies the key CM words. I"
2020.semeval-1.172,S13-2053,0,0.0654071,"016). Classifying tweets according to sentiment has many applications in political science, social sciences, market research, and many others (Mart´ınez-C´amara et al., 2014; Mejova et al., 2015). Although initially sentiment identification was focused on newswire text (Baccianella et al., 2010), later research turned towards social media (Rosenthal et al., 2015). Since 2013, a sentiment classification task on Twitter data have been organized in SemEval campaigns. Most of the earlier approaches to this problem were based on hand crafted features and sentiment lexicons (Pak and Paroubek, 2010; Mohammad et al., 2013). These features were then used as input to classifiers (e.g., Support Vector Machines). However, such approaches required extensive domain knowledge, were laborious to define, and can lead to incomplete or over-specific features. Recently, researchers pay their attention to sentiment polarity detection on CM data. However, a few research work have been carried out in particular Hindi-English CM data with different approaches: lexicon lookup (Sharma et al., 2015), sub-word with CNN-LSTM (Joshi et al., 2016), Siamese networks (Choudhary et al., 2018), dual Encoder Network with features (Lal et"
2020.semeval-1.172,S16-1001,0,0.0290725,"tivecommons.org/licenses/by/4.0/. The code of this work is available at https://github.com/somnath-banerjee/Code-Mixed_ SentimentAnalysis. 1 https://code-switching.github.io/2020/ 1281 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1281–1287 Barcelona, Spain (Online), December 12, 2020. 2 Background Sentiment classification is the task of detecting whether a textual item (e.g., a product review, a blog post, an editorial, etc.) expresses a POSITIVE or a NEGATIVE opinion in general or about a given entity, e.g., a product, a person, a political party, or a policy (Nakov et al., 2016). Classifying tweets according to sentiment has many applications in political science, social sciences, market research, and many others (Mart´ınez-C´amara et al., 2014; Mejova et al., 2015). Although initially sentiment identification was focused on newswire text (Baccianella et al., 2010), later research turned towards social media (Rosenthal et al., 2015). Since 2013, a sentiment classification task on Twitter data have been organized in SemEval campaigns. Most of the earlier approaches to this problem were based on hand crafted features and sentiment lexicons (Pak and Paroubek, 2010; Moha"
2020.semeval-1.172,pak-paroubek-2010-twitter,0,0.100366,"policy (Nakov et al., 2016). Classifying tweets according to sentiment has many applications in political science, social sciences, market research, and many others (Mart´ınez-C´amara et al., 2014; Mejova et al., 2015). Although initially sentiment identification was focused on newswire text (Baccianella et al., 2010), later research turned towards social media (Rosenthal et al., 2015). Since 2013, a sentiment classification task on Twitter data have been organized in SemEval campaigns. Most of the earlier approaches to this problem were based on hand crafted features and sentiment lexicons (Pak and Paroubek, 2010; Mohammad et al., 2013). These features were then used as input to classifiers (e.g., Support Vector Machines). However, such approaches required extensive domain knowledge, were laborious to define, and can lead to incomplete or over-specific features. Recently, researchers pay their attention to sentiment polarity detection on CM data. However, a few research work have been carried out in particular Hindi-English CM data with different approaches: lexicon lookup (Sharma et al., 2015), sub-word with CNN-LSTM (Joshi et al., 2016), Siamese networks (Choudhary et al., 2018), dual Encoder Networ"
2020.semeval-1.172,S15-2078,0,0.0267216,"s the task of detecting whether a textual item (e.g., a product review, a blog post, an editorial, etc.) expresses a POSITIVE or a NEGATIVE opinion in general or about a given entity, e.g., a product, a person, a political party, or a policy (Nakov et al., 2016). Classifying tweets according to sentiment has many applications in political science, social sciences, market research, and many others (Mart´ınez-C´amara et al., 2014; Mejova et al., 2015). Although initially sentiment identification was focused on newswire text (Baccianella et al., 2010), later research turned towards social media (Rosenthal et al., 2015). Since 2013, a sentiment classification task on Twitter data have been organized in SemEval campaigns. Most of the earlier approaches to this problem were based on hand crafted features and sentiment lexicons (Pak and Paroubek, 2010; Mohammad et al., 2013). These features were then used as input to classifiers (e.g., Support Vector Machines). However, such approaches required extensive domain knowledge, were laborious to define, and can lead to incomplete or over-specific features. Recently, researchers pay their attention to sentiment polarity detection on CM data. However, a few research wo"
2021.metanlp-1.5,2020.coling-main.448,0,0.0311996,"or Prototypical (Snell et al., 2017) networks belong to this category. 2.1.1 Supervised generalization In recent years, several approaches have been introduced and refined to overcome the issue of datalimited regime. As an example, the Prototypical Neural Networks (PNNs), developed by Snell et al. (2017) originally for image classification, were used to extract representative characteristics of the data by mapping data points into an embedding space where each sample will cluster around their respective prototype representation. Fort (2017) 37 3.2 ilarities. More recently, Dou et al. (2019), Bansal et al. (2020a) and Bansal et al. (2020b) applied various meta-learned models to few-shot NLU across domains and tasks. Finally, besides the approaches of Gu et al. (2018) and Zhang et al. (2020) that focus on handling new and low-resource languages for machine translation, to the best of our knowledge, there are no approaches that combine cross-lingual transfer and meta-learning methods for NLU tasks. 3 With the aim of generalizing unseen classes from zero to few training examples per class, PNNs is trained from a collection of N -way, k-shot classification tasks through an episodic training procedure (Vi"
2021.metanlp-1.5,P18-5002,0,0.0594652,"Missing"
2021.metanlp-1.5,2020.emnlp-main.38,0,0.0392379,"or Prototypical (Snell et al., 2017) networks belong to this category. 2.1.1 Supervised generalization In recent years, several approaches have been introduced and refined to overcome the issue of datalimited regime. As an example, the Prototypical Neural Networks (PNNs), developed by Snell et al. (2017) originally for image classification, were used to extract representative characteristics of the data by mapping data points into an embedding space where each sample will cluster around their respective prototype representation. Fort (2017) 37 3.2 ilarities. More recently, Dou et al. (2019), Bansal et al. (2020a) and Bansal et al. (2020b) applied various meta-learned models to few-shot NLU across domains and tasks. Finally, besides the approaches of Gu et al. (2018) and Zhang et al. (2020) that focus on handling new and low-resource languages for machine translation, to the best of our knowledge, there are no approaches that combine cross-lingual transfer and meta-learning methods for NLU tasks. 3 With the aim of generalizing unseen classes from zero to few training examples per class, PNNs is trained from a collection of N -way, k-shot classification tasks through an episodic training procedure (Vi"
2021.metanlp-1.5,D18-1398,0,0.0191822,"and refined to overcome the issue of datalimited regime. As an example, the Prototypical Neural Networks (PNNs), developed by Snell et al. (2017) originally for image classification, were used to extract representative characteristics of the data by mapping data points into an embedding space where each sample will cluster around their respective prototype representation. Fort (2017) 37 3.2 ilarities. More recently, Dou et al. (2019), Bansal et al. (2020a) and Bansal et al. (2020b) applied various meta-learned models to few-shot NLU across domains and tasks. Finally, besides the approaches of Gu et al. (2018) and Zhang et al. (2020) that focus on handling new and low-resource languages for machine translation, to the best of our knowledge, there are no approaches that combine cross-lingual transfer and meta-learning methods for NLU tasks. 3 With the aim of generalizing unseen classes from zero to few training examples per class, PNNs is trained from a collection of N -way, k-shot classification tasks through an episodic training procedure (Vinyals et al., 2016). Specifically, each episode is one mini-batch consisting of k examples from each of the N classes (both randomly sampled), used to form a"
2021.metanlp-1.5,H90-1021,0,0.467983,"., 2020; Upadhyay et al., 2018) corpus, whose description follows. 4.1 Models Training configurations We perform three sets of experiments: target only, multilingual and multilingual zero-shot. • target only: this configuration consists of using only the target language data. The MultiATIS++ corpus We also considered two cross-lingual classification tasks with a varying quantity of data between source and target languages to investigate the behaviour of different types of knowledge transfer. MultiATIS++ (Upadhyay et al., 2018; Xu et al., 2020) is the multilingual extension of the ATIS corpus (Hemphill et al., 1990), which belongs to the air travel planning domain. Originally in English (en), it has been human translated to 8 different other (distant and close) languages i.e., Spanish (es), German (de), French (fr), Portuguese (pt), Hindi (hi), Chinese (zh), Japanese (ja), and Turkish (tr). It contains 37,084 training examples and 7,859 test examples. Details of subsets statistics in terms • multilingual: where the training strategy aims to train a network on the concatenation of all of the nine languages and testing the model for each target language. • multilingual zero-shot: where the training relies"
2021.metanlp-1.5,2020.acl-main.747,0,0.0743626,"Missing"
2021.metanlp-1.5,N19-1423,0,0.658454,"from millions of users to gather data. Besides the requirement of a large amount of annotated data being available, domains, intents and slots are language dependent. Consequently, in practice, the resulting systems are hardly adaptable to expand to new languages. As a solution to this problem, cross-lingual transfer approaches were developed to leverage the knowledge from well-resourced languages, with task specific data available to under-resourced languages with little or no data. Recent efforts focused on training Transformer models multilingually such as the multilingual version of BERT (Devlin et al., 2019). While earlier work demonstrated the effectiveness of multilingual models to learn representations which are transferable across languages, they show limitations when applied to low-resource languages (Pires et al., 2019; Conneau et al., 2020). From another perspective lowshot learning such as few-shot and zero-shot, aims to transfer knowledge learned from one language to another when the training data is limited or is missing some task labels. As a core contribution, we explore the potential for cross-lingual transferability of multilingual Transformer-based model (Vaswani et al., 2017) (mBE"
2021.metanlp-1.5,N18-1202,0,0.114792,"Missing"
2021.metanlp-1.5,P19-1493,0,0.107204,"dly adaptable to expand to new languages. As a solution to this problem, cross-lingual transfer approaches were developed to leverage the knowledge from well-resourced languages, with task specific data available to under-resourced languages with little or no data. Recent efforts focused on training Transformer models multilingually such as the multilingual version of BERT (Devlin et al., 2019). While earlier work demonstrated the effectiveness of multilingual models to learn representations which are transferable across languages, they show limitations when applied to low-resource languages (Pires et al., 2019; Conneau et al., 2020). From another perspective lowshot learning such as few-shot and zero-shot, aims to transfer knowledge learned from one language to another when the training data is limited or is missing some task labels. As a core contribution, we explore the potential for cross-lingual transferability of multilingual Transformer-based model (Vaswani et al., 2017) (mBERT) combined with a few-shot learning algorithm based on prototypical representations. We also introduce a zero-shot scenario, where models are trained on multiple languages and evaluated on another. Our proposed approach"
2021.metanlp-1.5,D19-1214,0,0.0261756,"Missing"
2021.metanlp-1.5,2020.emnlp-main.410,0,0.0282928,"Missing"
2021.metanlp-1.5,W95-0107,0,0.0256567,"2 6132 # tokens dev 5445 5927 5909 5517 5769 9652 14416 1753 686 test 9164 10338 10228 9383 10511 16710 25939 9755 7683 # intents # slot types ) 18 84 17 17 75 71 Table 1: Details of the MultiATIS++ corpus. scenario, we construct mutiple episodic batches E. From the available data, we draw the task sets by sampling a subset of labels to form a support set from data in the high-resources languages and a query set from data in the low-resource languages to be evaluated. NLU data consists of utterances composed of sentence-level intent labels and sequences of slot labels annotated in BIO format (Ramshaw and Marcus, 1995) to define the boundary of slots. The N -way k-shot NLU task is then defined as follows: given an input query utterance in a new language qi and a k-shot support set S as references, find the most appropriate intent label or slot label sequence y: X argmaxE θ 4 log pθ (yi |qi , S). of the number of utterances, intents and slots are shown in Table 1. Our main concerns about this corpus are the Hindi and Turkish portions of the data, which are smaller than the other languages, covering only a subset of intents and slots and containing extremely few labeled examples. 4.2 We use the fine-tuning pr"
2021.metanlp-1.5,D15-1027,0,0.0270864,"nd diverse, only the most relevant approaches to this work are presented and we refer the reader to Vanschoren (2019) and Wang et al. (2019) for a surveys of earlier work. 2.1 Semi-supervised generalization 2.2 NLU using low-shot learning A number of different deep learning approaches have been applied to the problem of language understanding in recent years. For a thorough overview of deep learning methods in conversational language understanding, we refer the readers to Gao et al. (2018). In the context of relying on limited training resources, few-shot learning has been used for NLU tasks. Yazdani and Henderson (2015) proposes a method to leverage unlabeled data in order to find the separating hyperplanes that divide the utterances with the same label from those with different labels. Sun et al. (2019) extended PNNs for intent classification using hierarchical attention mechanisms when generating the prototype representations. Slot filling using few-shot models has also been explored. Ferreira et al. (2015) presented a zeroshot approach based on a knowledge base and on word representations learned from unlabeled data. Fritzler et al. (2019) applied PNNs to few-shot named entity recognition by training a se"
2021.metanlp-1.5,2020.acl-main.148,0,0.0264707,"me the issue of datalimited regime. As an example, the Prototypical Neural Networks (PNNs), developed by Snell et al. (2017) originally for image classification, were used to extract representative characteristics of the data by mapping data points into an embedding space where each sample will cluster around their respective prototype representation. Fort (2017) 37 3.2 ilarities. More recently, Dou et al. (2019), Bansal et al. (2020a) and Bansal et al. (2020b) applied various meta-learned models to few-shot NLU across domains and tasks. Finally, besides the approaches of Gu et al. (2018) and Zhang et al. (2020) that focus on handling new and low-resource languages for machine translation, to the best of our knowledge, there are no approaches that combine cross-lingual transfer and meta-learning methods for NLU tasks. 3 With the aim of generalizing unseen classes from zero to few training examples per class, PNNs is trained from a collection of N -way, k-shot classification tasks through an episodic training procedure (Vinyals et al., 2016). Specifically, each episode is one mini-batch consisting of k examples from each of the N classes (both randomly sampled), used to form a labeled (support S) and"
2021.metanlp-1.5,D19-1045,0,0.0261876,"ization 2.2 NLU using low-shot learning A number of different deep learning approaches have been applied to the problem of language understanding in recent years. For a thorough overview of deep learning methods in conversational language understanding, we refer the readers to Gao et al. (2018). In the context of relying on limited training resources, few-shot learning has been used for NLU tasks. Yazdani and Henderson (2015) proposes a method to leverage unlabeled data in order to find the separating hyperplanes that divide the utterances with the same label from those with different labels. Sun et al. (2019) extended PNNs for intent classification using hierarchical attention mechanisms when generating the prototype representations. Slot filling using few-shot models has also been explored. Ferreira et al. (2015) presented a zeroshot approach based on a knowledge base and on word representations learned from unlabeled data. Fritzler et al. (2019) applied PNNs to few-shot named entity recognition by training a separate model for each entity type and Hou et al. (2019) proposed a conditional random forest-based approach enhanced with transfer mechanisms that implicitly incorporate label dependencies"
2021.metanlp-1.5,P19-1519,0,0.0487147,"Missing"
antoine-etal-2002-predictive,H90-1020,0,\N,Missing
antoine-etal-2002-predictive,J99-4003,0,\N,Missing
antoine-etal-2002-predictive,antoine-etal-2000-obtaining,1,\N,Missing
ben-jannet-etal-2014-eter,E12-1018,1,\N,Missing
ben-jannet-etal-2014-eter,W11-0411,1,\N,Missing
ben-jannet-etal-2014-eter,galibert-etal-2014-etape,1,\N,Missing
ben-jannet-etal-2014-eter,I11-1058,1,\N,Missing
ben-jannet-etal-2014-eter,doddington-etal-2004-automatic,0,\N,Missing
ben-jannet-etal-2014-eter,C96-1079,0,\N,Missing
ben-jannet-etal-2014-eter,M93-1007,0,\N,Missing
C12-1055,J11-4004,0,0.141961,"o organize an annotation campaign but not to that of the annotation task per se. Note also that the analysis we propose here applies whatever the level of expertise of the annotators. Besides, the annotation guide is recognized as the keystone of annotation campaigns, as it defines what should be annotated. Here, we consider that the need is clearly defined and known from all the participants. 897 Annotation evaluation Studies concerning the evaluation of the quality of manual annotation allowed to identify some factors influencing inter- and intra-annotator agreements. (Gut and Bayerl, 2004; Bayerl and Paul, 2011) demonstrated that the inter-annotator agreement and the complexity of the annotation task are correlated (the larger the number of categories, the lower the inter-annotator agreement) and that the categories prone to confusions are in limited number. This brings out two complexity dimensions related to the number of categories and to the ambiguity between some categories. This ambiguity-related complexity dimension also appears in (Popescu-Belis, 2007). In the study of the inter-annotator agreements, (Krippendorff, 2004) identified a step of identification of the elements to annotate that is"
C12-1055,cook-stevenson-2010-automatically,0,0.0220103,"ion task can be decomposed into two or more EATs if the tagset itself is made of smaller independent ones. 2 This decomposition in EATs is formal in the sense that it is independent of the pragmatic organization of the work: the annotators can handle different EATs as separate steps on the source signal or all at once depending on the specific nature of the work and the tools they use. The decomposition in EATs does not result in a simplification of the original task as it is often the case for the Human Intelligence Tasks (HITs) to be performed by Turkers (workers) in Amazon Mechanical Turk (Cook and Stevenson, 2010). To take a simple example, the annotation of gene renaming relations can be analyzed as a combination of two EATs. The first one identifies the gene names in the source signal. The second one relies on the first level of annotation and indicates which of the genes hold in a renaming relation. Obviously, the annotators can add both types of annotations at the same time, but the tagsets are independent and it is easier, from a formal point of view, to analyze the annotation task as a combination of two EATs than as a unique, but complex one. 3.2 What to annotate? Localizing the units to be anno"
C12-1055,W09-3002,0,0.33341,"plexity. In the simplest case, the choice is boolean and annotating amounts to assigning the discriminated units into two categories:5 sentences may be marked as relevant or not; the occurrences of the it pronoun are marked as anaphoric or impersonal. However, the choice is often more open, for instance for representing the diversity of morphosyntactic units, annotating syntactic dependencies or typing named entities. For the richest annotations, structured labels are often proposed: the annotator adds several labels on a single given unit, the combination of which forms the final annotation (Dandapat et al., 2009). Finally, there are annotation tasks for which the choice of a label is entirely left to the annotator, as in speech transcription, where there may be as many labels as words. In such cases, we consider that we have a huge tagset, even though the annotation effort is probably of a slightly different nature for the annotator. If an annotation A is formed of a sequence of m labels (A = E1 E2 . . . Em ) and each label Ei can take ni different values, the complete tagset theoretically contains n different labels, with n = n1 ∗ n2 ∗ . . . ∗ nm . However, in practice, constraints are defined which"
C12-1055,P03-1068,0,0.0166699,"2.1 Discrimination In some annotation experiments, the question of “what to annotate” is straightforward, for instance when the units to annotate have already been marked in an automatic pre-annotation phase or when all the units are to be annotated, as in a POS-tagging task. However, for the annotators, the corpus is often a haystack within which they must find what to annotate, and discriminating what should be annotated from what should not is a complex task. Identifying the units on which the annotation work should focus is all the more complex as the units to consider are heterogeneous. (Erk et al., 2003) emphasizes that semantic role annotation and discourse annotation mix several levels of segmentation (from less than a word to more than a sentence). As a simple example, it is easier to identify in a text negatively connoted adverbs, in particular, than all the negative expressions, as the latter can be words, phrases, or even entire parts of a text. In the second case several segmentation levels are actually to be considered. This gives a first scale of difficulty. An annotation task is considered difficult to the extent that the discrimination factor, defined by the following formula, is h"
C12-1055,2010.jeptalnrecital-long.35,1,0.839263,"gene names, in our example, are simple tokens. On the contrary, the characterizing factors are low: the tagset is boolean (Dimension=0), a type language is used (Expressiveness=0.25) and ambiguity is very low as only few gene names are also common names (theoretical ambiguity can be approximated at 0.019 and residual ambiguity is on average of 0.04 for two annotators). In this case, the context is the highest factor as it is often necessary to read the whole PubMed abstract to understand the role of a mentioned entity and as annotators sometimes consult external resources (context weights 1) (Fort et al., 2010). This first EAT is represented on the same graph as the pronoun classification, thus enabling the comparison of the two tasks (see Figure 1a). If they both show little complexity on three dimensions (delimitation, expressiveness, tagset dimension), the first one (pronouns) presents a high ambiguity dimension and no discrimination problem, while the second one (gene names) shows high complexity levels on the discrimination and context dimensions and no ambiguity. The solutions to alleviate the costs of these campaigns should therefore be adapted (pre-annotation and easy access to context for g"
C12-1055,W10-1807,1,0.881481,"0.02 for the two annotators), but the context is high, as in the previous case. Figure 1b shows how the two EATs are combined to provide an analysis of the complexity of the whole task, which proves to be focused on discrimination and context. 4 Validation and illustration 4.1 Experimental validation Although, as we showed in section 2, this grid of analysis has never been identified as such, some existing results or experiments confirm its applicability. One first example of this is related to discrimination. In experiments led on the effects of pre-annotation on POS-tagging, the authors of (Fort and Sagot, 2010) showed that if the automatic pre-annotation is of very good quality, i.e. if only few tokens disseminated within the text have to be corrected, very good annotators can end up with less good annotation results, due to lapses in their concentration. This corresponds directly to the discrimination dimension we present here, which tends to get higher if the annotations to perform are submerged within the text. Also, it has been observed that the quality of the annotation decreases with the number of tags involved (Bayerl and Paul, 2011). Structuring the tagset allows to reduce the degree of free"
C12-1055,W11-0411,1,0.885766,"oner in the process. In the context of the gene renaming annotation campaign, we discovered that in some cases, the annotators needed the whole text to make their final decisions, and not only the abstract they had to annotate, as it was initially planned. If this need could have been identified beforehand, it would have been taken into account and the annotation tool would have been parameterized to give an easy access to the whole documents to annotators. 4.2 Example: structured named entities We applied this analysis on a structured named entity annotation campaign for French described in (Grouin et al., 2011). Structured named entities Within the Quaero program10 , a new definition of structured named entities was proposed. This new structure relies on two principles: the entities are both hierarchical and compositional. An entity is then composed of two kinds of elements: the 7 types (and 32 subtypes) which refer to a general segmentation of the world into major categories and the 31 components which allow to tag every word of the entity expression. Following this definition, the annotation campaign can be decomposed into two EATs: types and components. Figure 2 illustrates this definition on a F"
C12-1055,ide-romary-2006-representing,0,0.024176,"y have been, in particular, inherited from corpus linguistics. (Leech, 1993, 2005) present a list of what a specification for annotation should contain. It represents an effort toward the identification of several levels of difficulty encountered when making decisions during the manual annotation of a corpus: segmentation, inclusion of units (words or phrases) within other units (clauses or sentences), assigning categories to some textual fragments. Annotation formats Recommendations were published within the framework of the standardization effort of the annotation formats, in particular in (Ide and Romary, 2006), that finally gave birth to the ISO 24612 standard. In this view, the authors are little concerned by manual annotation difficulties as such but, to represent annotations they identify complex structuring elements which are of interest here : segmentation (identification of continuous or discontinuous sequences of characters), several layers of annotations (for example morpho-syntactic, then syntactic), relations between these layers, overlapping issues. Organization of annotation campaigns An overall schema of the organization of annotation campaigns has also emerged. It involves several act"
C12-1055,W11-1810,1,0.882881,"Missing"
C12-1055,kaplan-etal-2010-annotation,0,0.0361697,"Missing"
C12-1055,J93-2004,0,0.0603872,"e of NLP technologies on some oral or written discourse. This corresponds to a great diversity of phenomena, as these annotations vary in nature (phonetic, morpho-syntactic, semantic or task-oriented labels), in the range they cover (they can concern a couple of characters, a word, a paragraph or a whole text), in their degree of coverage (all the text is annotated or only a part of it) and in their form (atomic value, complex feature structures or relations and even cross-document alignment relations). It has been a long road since the big pioneer annotation campaigns like the Penn Treebank (Marcus et al., 1993), but one problem remains: manual annotation is expensive. Various strategies have been implemented to reduce or control annotation costs. Tools have been developed to assist and guide the work of annotators. Automatic annotation methods, sometimes based on machine learning, have been introduced to relieve the annotator of the most trivial and repetitive work and to allow him/her to focus on the hardest annotation tasks where human interpretation is critical. For simple tasks, the use of crowdsourcing is developed with the idea of dividing up tedious work and exploiting the number of annotator"
C12-1055,J05-1004,0,0.0345415,"ity of the annotation task are correlated (the larger the number of categories, the lower the inter-annotator agreement) and that the categories prone to confusions are in limited number. This brings out two complexity dimensions related to the number of categories and to the ambiguity between some categories. This ambiguity-related complexity dimension also appears in (Popescu-Belis, 2007). In the study of the inter-annotator agreements, (Krippendorff, 2004) identified a step of identification of the elements to annotate that is called “unitizing”. Similarly, in the Proposition Bank project (Palmer et al., 2005), the organizers separated role “identification” from role “classification” to compute the inter-annotator agreement, in order to “isolate the role classification decisions” from the (supposedly easier) identification. 2.3 Insights from Cognitive Science Few publications focus on the difficulties of manual annotation. In (Tomanek et al., 2010), the authors used an eye-tracking device to analyze and model the annotation cognitive complexity. Their experiment was carried out on a simple named-entity annotation task (persons, locations and organizations), with some pre-identification of complex n"
C12-1055,W12-3606,1,0.757456,"necessary to validate a choice by exploring external data (such as Wikipedia). Delimitation Discrimination Expressiveness Context Weight Tagset dimension Ambiguity Figure 3: Synthesis of the complexity for the structured named entities campaign (2 EATs, double scale) This analysis validates the choice of the hierarchical tagset that has been done for the annotation campaign described in(Grouin et al., 2011). Had a flat tagset been chosen, the dimension score would have been 1 (ν = 61). Moreover, this analysis is in line with what was observed concerning the context weight within the campaign(Rosset et al., 2012). Of course, it presents some limits as it has been shown for the ambiguity score computation. Conclusion The grid of analysis we propose here should be used as part of the preparatory work of any annotation campaign, large or small. Obviously, when done a priori, on a small sample of annotations, the results will be approximate, but we believe that the analysis itself helps asking the right questions and finding the appropriate solutions. Obviously, this pre-campaign work should be supported by an appropriate tool, so that this grid of analysis is computed more or less automatically. We are w"
C12-1055,P10-1118,0,0.0263219,"y dimension also appears in (Popescu-Belis, 2007). In the study of the inter-annotator agreements, (Krippendorff, 2004) identified a step of identification of the elements to annotate that is called “unitizing”. Similarly, in the Proposition Bank project (Palmer et al., 2005), the organizers separated role “identification” from role “classification” to compute the inter-annotator agreement, in order to “isolate the role classification decisions” from the (supposedly easier) identification. 2.3 Insights from Cognitive Science Few publications focus on the difficulties of manual annotation. In (Tomanek et al., 2010), the authors used an eye-tracking device to analyze and model the annotation cognitive complexity. Their experiment was carried out on a simple named-entity annotation task (persons, locations and organizations), with some pre-identification of complex noun phrases containing at least one potential named entity. They measured the influence of the syntactic and semantic complexities1 as well as the size of context that was used by the annotators. The results show that the annotation performance tends on average to “correlate with the [semantic] complexity of the annotation phrase” and less so"
C12-2079,J08-4004,0,0.650981,"us (a “gold-standard”) can obviously bias an evaluation performed using this corpus as a reference. Finally, a bad quality annotation would lead to misleading clues in a linguistic analysis used to create rule-based systems. However, it is not possible to directly evaluate the validity of manual annotations. Instead, interannotator agreement measures are used: at least two annotators are asked to annotate the same sample of text in parallel, their annotations are compared and a coefficient is computed. The latter can be of many types and the well-known Kappa-family is described in details in (Artstein and Poesio, 2008). However, as pointed out by the authors of this article, the obtained results are difficult to interpret. Kappa coefficients, for example, are difficult to compare, even within the same annotation task, as they imply a definition of the markables that can vary from one campaign to the other (Grouin et al., 2011). More generally, we lack clues to know if a Kappa of 0.75 is a “good” result, or if a Kappa of 0.8 is twice as good as one of 0.4 or if a result of 0.6 obtained using one coefficient is better than 0.5 with another one, and for which annotation task. We first briefly present the state"
C12-2079,J11-4004,0,0.0379275,"e obtained results. However, their analyses lack robustness, as they only apply to similar campaigns. Other studies concerning the evaluation of the quality of manual annotation identified some factors that influence inter- and intra-annotator agreements, thereby giving clues on their behavior. Gut and Bayerl (2004) thus demonstrated that the inter-annotator agreement and the complexity of the annotation task are correlated: the larger the number of categories, the lower the inter-annotator agreement. However, categories prone to confusion are in limited number. The meta-analysis presented by Bayerl and Paul (2011) extends this research on the factors influencing agreement results, identifying 8 such factors and proposing useful recommendations to improve manual annotation reliability. However, neither of these studies provides a clear picture of the behavior of the agreement coefficients 0 This work has been partially financed by OSEO, the French State Agency for Innovation, under the Quaero program. 810 or of their meanings. The experiments detailed in (Reidsma and Carletta, 2008) constitute an interesting step in this direction, focusing on the effect of annotation errors on machine learning systems"
C12-2079,fort-etal-2012-analyzing,1,0.700025,"present the pros and cons of these methods, from the statistical and mathematical points of view, with some hints about specific issues raised in some annotation campaigns, like the prevalence of one category. A section of their article is dedicated to various attempts at providing an interpretation scale for the Kappa family coefficients and how they failed to converge. Works such as (Gwet, 2012) are also to be mentioned. They present various inter-rater reliability coefficients and insist on benchmarking issues related to their interpretation. Many authors, among whom (Grouin et al., 2011; Fort et al., 2012), tried to obtain a more precise assessment of the quality of the annotation in their campaigns by computing different coefficients and analyzing the obtained results. However, their analyses lack robustness, as they only apply to similar campaigns. Other studies concerning the evaluation of the quality of manual annotation identified some factors that influence inter- and intra-annotator agreements, thereby giving clues on their behavior. Gut and Bayerl (2004) thus demonstrated that the inter-annotator agreement and the complexity of the annotation task are correlated: the larger the number o"
C12-2079,W11-0411,1,0.934731,"nterannotator agreement measures are used: at least two annotators are asked to annotate the same sample of text in parallel, their annotations are compared and a coefficient is computed. The latter can be of many types and the well-known Kappa-family is described in details in (Artstein and Poesio, 2008). However, as pointed out by the authors of this article, the obtained results are difficult to interpret. Kappa coefficients, for example, are difficult to compare, even within the same annotation task, as they imply a definition of the markables that can vary from one campaign to the other (Grouin et al., 2011). More generally, we lack clues to know if a Kappa of 0.75 is a “good” result, or if a Kappa of 0.8 is twice as good as one of 0.4 or if a result of 0.6 obtained using one coefficient is better than 0.5 with another one, and for which annotation task. We first briefly present the state of the art (Section 2), then detail the principles of our method to benchmark measures (Section 3) and show on some examples how different coefficients can be compared (Section 4). We finally discuss current limitations and point out future developments. 2 State of the art A quite detailed analysis of the most c"
C12-2079,J02-1002,0,0.272818,"on the contrary, a reference element is missing (false negative). All of these error paradigms tend to damage the annotations, so each of them should be taken into account by agreement measures. We propose here to apply each measure to a set of corpora, each of which embeds errors from one or more paradigms, and with a certain magnitude (the higher the magnitude, the higher the number of errors). This experiment should allow us to observe how the measures behave w.r.t. the different paradigms, and with a full range of magnitudes. The idea of creating artificial damaged corpora is inspired by Pevzner and Hearst (2002), then Bestgen (2009) in thematic segmentation, but our goal (giving meaning to measures) and our method (e.g. applying progressive magnitudes) are very different. 3.2 Protocol Reference. A reference annotation set (called reference) is provided to the system: a true Gold Standard or an automatically generated set based on a statistical model. It is assumed to correspond exactly to what annotations should be, with respect to the annotation guidelines. Shuffling. A shuffling process is an algorithm that automatically generates a multi-annotated corpus given three parameters: a reference annotat"
C12-2079,J08-3001,0,0.379365,", and the obtained results are used to model the behavior of these measures and understand their actual meaning. KEYWORDS: inter-annotator agreement, manual corpus annotation, evaluation. Proceedings of COLING 2012: Posters, pages 809–818, COLING 2012, Mumbai, December 2012. 809 1 Introduction The quality of manual annotations has a direct impact on the applications using them. For example, it was demonstrated that machine learning tools learn to make the same mistakes as the human annotators, if these mistakes follow a certain regular pattern and do not correspond to simple annotation noise (Reidsma and Carletta, 2008; Schluter, 2011). Furthermore, errors in a manually annotated reference corpus (a “gold-standard”) can obviously bias an evaluation performed using this corpus as a reference. Finally, a bad quality annotation would lead to misleading clues in a linguistic analysis used to create rule-based systems. However, it is not possible to directly evaluate the validity of manual annotations. Instead, interannotator agreement measures are used: at least two annotators are asked to annotate the same sample of text in parallel, their annotations are compared and a coefficient is computed. The latter can"
D11-1102,W03-0419,0,0.0587783,"Missing"
D11-1102,W05-0620,0,0.131108,"Missing"
D11-1102,J05-1003,0,0.034987,"elect hypotheses generated by the baseline model; second, a strategy to decide, after the reranking phase, if it is more likely that the baseline best hypothesis is more accurate than the best reranked hypothesis and allowing to recover the mistake. Similar ideas have been proposed in (Dinarelli et al., 2010), here we propose a significant evolution and we give a much wider description and evaluation. 4.1 Hypotheses Selection via Attribute Value Extraction (AVE) In previous reranking approaches (Collins, 2000; Collins and Duffy, 2002; Shen et al., 2003a; Shen et al., 2003b; Shen et al., 2004; Collins and Koo, 2005; Kudo et al., 2005; Dinarelli et al., 2009b), few hypotheses are generated with the baseline model, ranked by the model probability. These are then used for the reranking model. An interesting strategy to improve reranking performance is the selection of the best set of hypotheses to be reranked. In this work we propose a semantic inconsistency metric (SIM) based on the attribute-value extraction phase that allows to select better n-best hypotheses. We combine the scores provided by the rule based approach and the CRF approach for AVE, computing a confidence measure. The rule-based approach f"
D11-1102,P10-1052,0,0.0472305,"Missing"
D11-1102,J98-4004,0,0.219191,"Missing"
D11-1102,galibert-etal-2010-named,1,0.887135,"Missing"
D11-1102,doddington-etal-2004-automatic,0,0.0281018,"Missing"
D11-1102,bonneau-maynard-etal-2006-results,0,0.386126,"Missing"
D11-1102,D09-1112,1,0.943452,"our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1 Introduction Discriminative reranking is a widely used approach for several Natural Language Processing (NLP) tasks: Syntactic Parsing (Collins, 2000), Named Entity Recognition (Collins, 2000; Collins and Duffy, 2001), Semantic Role Labelling (Moschitti et al., 2008), Machine Translation (Shen et al., 2004), Question Answering (Moschitti et al., 2007). Recently reranking approaches have been successfully applied also to Spoken Language Understanding (SLU) (Dinarelli et al., 2009b). Discriminative Reranking combines two models: a first SLU model is used to generate a ranked list of n-best hypotheses; a reranking model sorts the list based on a different score and the final result is the new top ranked hypothesis. The advantage of reranking approaches is in the possibility to learn directly complex dependencies in the output domain, as this is provided in the hypotheses generated by the baseline model. In previous approaches complex features are extracted from the hypotheses for both training and classification phase, but there are very few studies on approaches that c"
D11-1102,P02-1034,0,0.0728663,"he following reranking kernel: KR (e1 , e2 ) = P T K(t1,1 , t1,2 ) + P T K(t2,1 , t2,2 ) − P T K(t1,1 , t2,2 ) − P T K(t2,1 , t1,2 ), (4) where e1 and e2 are two pairs of trees to be compared. The reranking kernel in equation 4, consisting in summing four different kernels, has been proposed in (Shen et al., 2003b) for syntactic parsing reranking, where the basic kernel was a Tree Kernel, and the idea was taken in turn from (Heibrich et al., 2000), where pairs where used to learn preference ranking. The same idea appears also, in a slightly different form, in early work about reranking, e.g. (Collins and Duffy, 2002). The same reranking schema has been used also in (Shen et al., 2004) for reranking different candidate hypotheses for machine translation. For classification, observing that the model is symmetric and exploiting kernel properties, we can use, as classification instances, simple hypotheses instead of pairs. More precisely we use pairs where the second hypothesis is empty, i.e. (Hi , 0), for i ∈ [1..n]. This simplification allow a relatively fast classification phase, since only n instances are generated for each sentence, instead of n2 . This simplification has been proposed in (Shen et al., 2"
D11-1102,W03-1012,0,0.177106,"metric. In particular, we compute the edit distance of each hypothesis in the list, with respect to the manual annotation taken from the corpus. The best hypothesis Hb is used to build positive instances for the reranker as pairs (Hb , Hi ) for i ∈ [1..n] and i 6= b, negative instances are built as (Hi , Hb ), with same constraints on index i. This means that, if n hypotheses are generated for a sentence, 2 · n instances are generated from them. Note that by construction of pairs the model is symmetric, this provides a property that will be exploited at classification phase, as described in (Shen et al., 2003b). Hypotheses are then converted into trees like the one shown in figure 1. Pairs of trees ek = (ti,k , tj,k ), for k varying along all the training or classification instances, are given as input to the SVM model to train the reranker using the following reranking kernel: KR (e1 , e2 ) = P T K(t1,1 , t1,2 ) + P T K(t2,1 , t2,2 ) − P T K(t1,1 , t2,2 ) − P T K(t2,1 , t1,2 ), (4) where e1 and e2 are two pairs of trees to be compared. The reranking kernel in equation 4, consisting in summing four different kernels, has been proposed in (Shen et al., 2003b) for syntactic parsing reranking, where"
D11-1102,W03-0402,0,0.0644372,"Missing"
D11-1102,N04-1023,0,0.408899,"the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1 Introduction Discriminative reranking is a widely used approach for several Natural Language Processing (NLP) tasks: Syntactic Parsing (Collins, 2000), Named Entity Recognition (Collins, 2000; Collins and Duffy, 2001), Semantic Role Labelling (Moschitti et al., 2008), Machine Translation (Shen et al., 2004), Question Answering (Moschitti et al., 2007). Recently reranking approaches have been successfully applied also to Spoken Language Understanding (SLU) (Dinarelli et al., 2009b). Discriminative Reranking combines two models: a first SLU model is used to generate a ranked list of n-best hypotheses; a reranking model sorts the list based on a different score and the final result is the new top ranked hypothesis. The advantage of reranking approaches is in the possibility to learn directly complex dependencies in the output domain, as this is provided in the hypotheses generated by the baseline m"
D11-1102,P05-1024,0,0.022853,"ted by the baseline model; second, a strategy to decide, after the reranking phase, if it is more likely that the baseline best hypothesis is more accurate than the best reranked hypothesis and allowing to recover the mistake. Similar ideas have been proposed in (Dinarelli et al., 2010), here we propose a significant evolution and we give a much wider description and evaluation. 4.1 Hypotheses Selection via Attribute Value Extraction (AVE) In previous reranking approaches (Collins, 2000; Collins and Duffy, 2002; Shen et al., 2003a; Shen et al., 2003b; Shen et al., 2004; Collins and Koo, 2005; Kudo et al., 2005; Dinarelli et al., 2009b), few hypotheses are generated with the baseline model, ranked by the model probability. These are then used for the reranking model. An interesting strategy to improve reranking performance is the selection of the best set of hypotheses to be reranked. In this work we propose a semantic inconsistency metric (SIM) based on the attribute-value extraction phase that allows to select better n-best hypotheses. We combine the scores provided by the rule based approach and the CRF approach for AVE, computing a confidence measure. The rule-based approach for AVE is defined b"
D11-1102,D10-1001,0,0.0736898,"Missing"
D11-1102,J08-2003,0,0.0197147,"hypothesis provided by the re-ranker. We show the effectiveness of these solutions presenting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1 Introduction Discriminative reranking is a widely used approach for several Natural Language Processing (NLP) tasks: Syntactic Parsing (Collins, 2000), Named Entity Recognition (Collins, 2000; Collins and Duffy, 2001), Semantic Role Labelling (Moschitti et al., 2008), Machine Translation (Shen et al., 2004), Question Answering (Moschitti et al., 2007). Recently reranking approaches have been successfully applied also to Spoken Language Understanding (SLU) (Dinarelli et al., 2009b). Discriminative Reranking combines two models: a first SLU model is used to generate a ranked list of n-best hypotheses; a reranking model sorts the list based on a different score and the final result is the new top ranked hypothesis. The advantage of reranking approaches is in the possibility to learn directly complex dependencies in the output domain, as this is provided in t"
D11-1102,P07-1098,0,0.0213139,"senting comparative results obtained reranking hypotheses generated by a very accurate Conditional Random Field model. We evaluate our approach on the French MEDIA corpus. The results show significant improvements with respect to current state-of-the-art and previous 1 Introduction Discriminative reranking is a widely used approach for several Natural Language Processing (NLP) tasks: Syntactic Parsing (Collins, 2000), Named Entity Recognition (Collins, 2000; Collins and Duffy, 2001), Semantic Role Labelling (Moschitti et al., 2008), Machine Translation (Shen et al., 2004), Question Answering (Moschitti et al., 2007). Recently reranking approaches have been successfully applied also to Spoken Language Understanding (SLU) (Dinarelli et al., 2009b). Discriminative Reranking combines two models: a first SLU model is used to generate a ranked list of n-best hypotheses; a reranking model sorts the list based on a different score and the final result is the new top ranked hypothesis. The advantage of reranking approaches is in the possibility to learn directly complex dependencies in the output domain, as this is provided in the hypotheses generated by the baseline model. In previous approaches complex features"
D11-1102,C00-2137,0,0.152267,"Missing"
D11-1102,P05-1022,0,\N,Missing
D11-1102,W04-3223,0,\N,Missing
devillers-etal-2002-annotations,H01-1015,0,\N,Missing
devillers-etal-2002-annotations,bonneau-maynard-etal-2000-predictive,0,\N,Missing
devillers-etal-2004-french,H92-1003,0,\N,Missing
devillers-etal-2004-french,P01-1066,0,\N,Missing
devillers-etal-2004-french,antoine-etal-2002-predictive,1,\N,Missing
devillers-etal-2004-french,antoine-etal-2000-obtaining,1,\N,Missing
dinarelli-rosset-2012-tree-structured,J98-4004,0,\N,Missing
dinarelli-rosset-2012-tree-structured,grover-etal-2008-named,0,\N,Missing
dinarelli-rosset-2012-tree-structured,D09-1112,1,\N,Missing
dinarelli-rosset-2012-tree-structured,A00-1044,0,\N,Missing
dinarelli-rosset-2012-tree-structured,E09-1024,1,\N,Missing
dinarelli-rosset-2012-tree-structured,I11-1142,1,\N,Missing
dinarelli-rosset-2012-tree-structured,D11-1102,1,\N,Missing
dinarelli-rosset-2012-tree-structured,W11-0411,1,\N,Missing
dinarelli-rosset-2012-tree-structured,I11-1058,1,\N,Missing
dinarelli-rosset-2012-tree-structured,galibert-etal-2012-extended,1,\N,Missing
dinarelli-rosset-2012-tree-structured,sekine-nobata-2004-definition,0,\N,Missing
dinarelli-rosset-2012-tree-structured,C96-1079,0,\N,Missing
E12-1018,C96-1079,0,0.629693,"of the entities, trees are not as complex as syntactic trees, thus, before designing an ad-hoc solution for the task, which require a remarkable effort and yet it doesn’t guarantee better performances, we designed a solution providing good results and which required a limited development effort. Introduction Named Entity Recognition is a traditinal task of the Natural Language Processing domain. The task aims at mapping words in a text into semantic classes, such like persons, organizations or localizations. While at first the NER task was quite simple, involving a limited number of classes (Grishman and Sundheim, 1996), along the years the task complexity increased as more complex class taxonomies were defined (Sekine and Nobata, 2004). The interest in the task is related to its use in complex frameworks for (semantic) content extraction, such like Relation Extraction applications (Doddington et al., 2004). This work presents research on a Named Entity Recognition task defined with a new set of named entities. The characteristic of such set is in that named entities have a tree structure. As concequence the task cannot be tackled as a sequence • Conditional Random Fields are models robust to noisy data, lik"
E12-1018,sekine-nobata-2004-definition,0,0.0143859,"h require a remarkable effort and yet it doesn’t guarantee better performances, we designed a solution providing good results and which required a limited development effort. Introduction Named Entity Recognition is a traditinal task of the Natural Language Processing domain. The task aims at mapping words in a text into semantic classes, such like persons, organizations or localizations. While at first the NER task was quite simple, involving a limited number of classes (Grishman and Sundheim, 1996), along the years the task complexity increased as more complex class taxonomies were defined (Sekine and Nobata, 2004). The interest in the task is related to its use in complex frameworks for (semantic) content extraction, such like Relation Extraction applications (Doddington et al., 2004). This work presents research on a Named Entity Recognition task defined with a new set of named entities. The characteristic of such set is in that named entities have a tree structure. As concequence the task cannot be tackled as a sequence • Conditional Random Fields are models robust to noisy data, like automatic transcriptions of ASR systems (Hahn et al., 2010), thus it is the best choice to deal with transcriptions o"
E12-1018,doddington-etal-2004-automatic,0,0.131386,"Introduction Named Entity Recognition is a traditinal task of the Natural Language Processing domain. The task aims at mapping words in a text into semantic classes, such like persons, organizations or localizations. While at first the NER task was quite simple, involving a limited number of classes (Grishman and Sundheim, 1996), along the years the task complexity increased as more complex class taxonomies were defined (Sekine and Nobata, 2004). The interest in the task is related to its use in complex frameworks for (semantic) content extraction, such like Relation Extraction applications (Doddington et al., 2004). This work presents research on a Named Entity Recognition task defined with a new set of named entities. The characteristic of such set is in that named entities have a tree structure. As concequence the task cannot be tackled as a sequence • Conditional Random Fields are models robust to noisy data, like automatic transcriptions of ASR systems (Hahn et al., 2010), thus it is the best choice to deal with transcriptions of broadcast data. Once words have been annotated with basic entity constituents, the tree structure of named entities is simple enough to be reconstructed with relatively sim"
E12-1018,W11-0411,1,0.809504,"Missing"
E12-1018,J98-4004,0,0.17081,"parsing algorithm. We analyse the effect of using several tree representations. Our system outperforms the best system of the evaluation campaign by a significant margin. 1 Sophie Rosset LIMSI-CNRS Orsay, France rosset@limsi.fr labelling approach. Additionally, the use of noisy data like transcriptions of French broadcast data, makes the task very challenging for traditional NLP solutions. To deal with such problems, we adopt a two-steps approach, the first being realized with Conditional Random Fields (CRF) (Lafferty et al., 2001), the second with a Probabilistic Context-Free Grammar (PCFG) (Johnson, 1998). The motivations behind that are: • Since the named entities have a tree structure, it is reasonable to use a solution coming from syntactic parsing. However preliminary experiments using such approaches gave poor results. • Despite the tree-structure of the entities, trees are not as complex as syntactic trees, thus, before designing an ad-hoc solution for the task, which require a remarkable effort and yet it doesn’t guarantee better performances, we designed a solution providing good results and which required a limited development effort. Introduction Named Entity Recognition is a traditi"
E12-1018,J96-1002,0,0.0382849,"Missing"
E12-1018,P10-1052,0,0.0685079,"Missing"
E12-1018,W04-3223,0,0.0307236,"Missing"
E12-1018,A00-2018,0,0.644565,"ether in a linear combination implement the elastic net regularizer (Zou and Hastie, 2005). As mentioned in (Lavergne et al., 2010), this kind of regularizers are very effective for feature selection at training time, which is a very good point when dealing with noisy data and big set of features. 176 4 Models for Parsing Trees The models used in this work for parsing entity trees refer to the models described in (Johnson, 1998), in (Charniak, 1997; Caraballo and Charniak, 1997) and (Charniak et al., 1998), and which constitutes the basis of the maximum entropy model for parsing described in (Charniak, 2000). A similar lexicalized model has been proposed also by Collins (Collins, 1997). All these models are based on a PCFG trained from data and used in a chart parsing algorithm to find the best parse for the given input. The PCFG model of (Johnson, 1998) is made of rules of the form: Figure 4: Baseline tree representations used in the PCFG parsing model • Xi ⇒ Xj Xk Figure 5: Filler-parent tree representations used in the PCFG pars• Xi ⇒ w ing model where X are non-terminal entities and w are terminal symbols (words in our case).1 The probability associated to these rules are: pi→j,k = P (Xi ⇒ Xj"
E12-1018,P97-1003,0,0.715042,"tie, 2005). As mentioned in (Lavergne et al., 2010), this kind of regularizers are very effective for feature selection at training time, which is a very good point when dealing with noisy data and big set of features. 176 4 Models for Parsing Trees The models used in this work for parsing entity trees refer to the models described in (Johnson, 1998), in (Charniak, 1997; Caraballo and Charniak, 1997) and (Charniak et al., 1998), and which constitutes the basis of the maximum entropy model for parsing described in (Charniak, 2000). A similar lexicalized model has been proposed also by Collins (Collins, 1997). All these models are based on a PCFG trained from data and used in a chart parsing algorithm to find the best parse for the given input. The PCFG model of (Johnson, 1998) is made of rules of the form: Figure 4: Baseline tree representations used in the PCFG parsing model • Xi ⇒ Xj Xk Figure 5: Filler-parent tree representations used in the PCFG pars• Xi ⇒ w ing model where X are non-terminal entities and w are terminal symbols (words in our case).1 The probability associated to these rules are: pi→j,k = P (Xi ⇒ Xj , Xk ) P (Xi ) have all rules in the form of 4 and 5, is straightforward and c"
E12-1018,W98-1115,0,0.217723,"+ ρ1 kλk1 + ρ2 kλk22 2 (3) kλk1 and kλk22 are the l1 and l2 regularizers (Riezler and Vasserman, 2004), and together in a linear combination implement the elastic net regularizer (Zou and Hastie, 2005). As mentioned in (Lavergne et al., 2010), this kind of regularizers are very effective for feature selection at training time, which is a very good point when dealing with noisy data and big set of features. 176 4 Models for Parsing Trees The models used in this work for parsing entity trees refer to the models described in (Johnson, 1998), in (Charniak, 1997; Caraballo and Charniak, 1997) and (Charniak et al., 1998), and which constitutes the basis of the maximum entropy model for parsing described in (Charniak, 2000). A similar lexicalized model has been proposed also by Collins (Collins, 1997). All these models are based on a PCFG trained from data and used in a chart parsing algorithm to find the best parse for the given input. The PCFG model of (Johnson, 1998) is made of rules of the form: Figure 4: Baseline tree representations used in the PCFG parsing model • Xi ⇒ Xj Xk Figure 5: Filler-parent tree representations used in the PCFG pars• Xi ⇒ w ing model where X are non-terminal entities and w are t"
E12-1018,allauzen-bonneau-maynard-2008-training,0,0.0131082,"d in (Lavergne et al., 2010), named wapiti.3 We didn’t optimize parameters ρ1 and ρ2 of the elastic net (see section 3.1), although this improves significantly the performances and leads to more compact models, default values lead in most cases to very accurate models. We used a wide set of features in CRF models, in a window of [−2, +2] around the target word: • A set of standard features like word prefixes and suffixes of length from 1 to 6, plus some Yes/No features like Does the word start with capital letter?, etc. • Morpho-syntactic features extracted from the output of the tool tagger (Allauzen and Bonneau-Maynard, 2008) • Features extracted from the output of the semantic analyzer (Rosset et al., (2009)) provided by the tool WMatch (Galibert, 2009). This analysis morpho-syntactic information as well as semantic information at the same level of named entities. Using two different sets of morpho-syntactic features results in more effective models, as they create a kind of agreement for a given word in case of match. Concerning the PCFG model, grammars, tree binarization and the different tree representations are created with our own scripts, while entity tree parsing is performed with the chart parsing algorit"
E12-1018,J05-1003,0,0.0253665,"o this model makes use of head words, like those described in section 4, thus the same considerations hold, moreover it seems quite complex for real applications, as it involves the use of four different models together. The models described in (Johnson, 1998), (Charniak, 1997; Caraballo and Charniak, 1997), (Charniak et al., 1998), (Charniak, 2000), (Collins, 1997) and (Ratnaparkhi, 1999), constitute the main individual models proposed for constituent-based syntactic parsing. Later other approaches based on models combination have been proposed, like e.g. the reranking approach described in (Collins and Koo, 2005), among many, and also evolutions or improvements of these models. More recently, approaches based on log-linear models have been proposed (Clark and Curran, 2007; Finkel et al., 2008) for parsing, called also “Tree CRF”, using also different training criteria (Auli and Lopez, 2011). Using such models in our work has basically two problems: one related to scaling issues, since our data present a large number of labels, which makes CRF training problematic, even more when using “Tree CRF”; another problem is related to the difference between syntactic parsing and named entity detection tasks, a"
E12-1018,J07-4004,0,0.0110332,"as it involves the use of four different models together. The models described in (Johnson, 1998), (Charniak, 1997; Caraballo and Charniak, 1997), (Charniak et al., 1998), (Charniak, 2000), (Collins, 1997) and (Ratnaparkhi, 1999), constitute the main individual models proposed for constituent-based syntactic parsing. Later other approaches based on models combination have been proposed, like e.g. the reranking approach described in (Collins and Koo, 2005), among many, and also evolutions or improvements of these models. More recently, approaches based on log-linear models have been proposed (Clark and Curran, 2007; Finkel et al., 2008) for parsing, called also “Tree CRF”, using also different training criteria (Auli and Lopez, 2011). Using such models in our work has basically two problems: one related to scaling issues, since our data present a large number of labels, which makes CRF training problematic, even more when using “Tree CRF”; another problem is related to the difference between syntactic parsing and named entity detection tasks, as mentioned in sub-section 4.2. Adapting “Tree CRF” to our task is thus a quite complex work, it constitutes an entire work by itself, we leave it as feature work"
E12-1018,P08-1109,0,0.0155595,"of four different models together. The models described in (Johnson, 1998), (Charniak, 1997; Caraballo and Charniak, 1997), (Charniak et al., 1998), (Charniak, 2000), (Collins, 1997) and (Ratnaparkhi, 1999), constitute the main individual models proposed for constituent-based syntactic parsing. Later other approaches based on models combination have been proposed, like e.g. the reranking approach described in (Collins and Koo, 2005), among many, and also evolutions or improvements of these models. More recently, approaches based on log-linear models have been proposed (Clark and Curran, 2007; Finkel et al., 2008) for parsing, called also “Tree CRF”, using also different training criteria (Auli and Lopez, 2011). Using such models in our work has basically two problems: one related to scaling issues, since our data present a large number of labels, which makes CRF training problematic, even more when using “Tree CRF”; another problem is related to the difference between syntactic parsing and named entity detection tasks, as mentioned in sub-section 4.2. Adapting “Tree CRF” to our task is thus a quite complex work, it constitutes an entire work by itself, we leave it as feature work. Concerning linear-ch"
E12-1018,D11-1031,0,0.0149087,"allo and Charniak, 1997), (Charniak et al., 1998), (Charniak, 2000), (Collins, 1997) and (Ratnaparkhi, 1999), constitute the main individual models proposed for constituent-based syntactic parsing. Later other approaches based on models combination have been proposed, like e.g. the reranking approach described in (Collins and Koo, 2005), among many, and also evolutions or improvements of these models. More recently, approaches based on log-linear models have been proposed (Clark and Curran, 2007; Finkel et al., 2008) for parsing, called also “Tree CRF”, using also different training criteria (Auli and Lopez, 2011). Using such models in our work has basically two problems: one related to scaling issues, since our data present a large number of labels, which makes CRF training problematic, even more when using “Tree CRF”; another problem is related to the difference between syntactic parsing and named entity detection tasks, as mentioned in sub-section 4.2. Adapting “Tree CRF” to our task is thus a quite complex work, it constitutes an entire work by itself, we leave it as feature work. Concerning linear-chain CRF models, the one we use is a state-of-the-art implementation (Lavergne et al., 2010), as it"
E12-1018,I11-1142,1,0.81477,"Missing"
E12-1018,J98-2004,0,\N,Missing
E12-1018,I11-1058,1,\N,Missing
F12-2028,bonneau-maynard-etal-2006-results,0,0.0343222,"Missing"
F12-2028,garnier-rizet-etal-2008-callsurf,0,0.0337718,"Missing"
F12-2028,W11-0411,1,0.887978,"Missing"
F12-2028,sekine-nobata-2004-definition,0,0.0299042,"Missing"
F12-2028,sekine-etal-2002-extended,0,0.106932,"Missing"
F12-2028,2009.jeptalnrecital-court.23,0,0.0625935,"Missing"
F13-1035,evert-2008-lightweight,0,0.062009,"Missing"
F13-1035,falco-etal-2012-kitten,0,0.025662,"Missing"
F13-1035,R11-1104,1,0.89772,"Missing"
F13-1035,W08-1804,0,0.0510508,"Missing"
F13-1035,quintard-etal-2010-question,1,0.869565,"Missing"
F13-1035,toney-etal-2008-evaluation,1,0.865525,"Missing"
galibert-etal-2010-hybrid,grover-etal-2000-lt,0,\N,Missing
galibert-etal-2010-hybrid,galibert-etal-2010-named,1,\N,Missing
galibert-etal-2010-named,W04-1213,0,\N,Missing
galibert-etal-2010-named,galibert-etal-2010-hybrid,1,\N,Missing
galibert-etal-2010-named,C96-1079,0,\N,Missing
galibert-etal-2010-named,P05-1045,0,\N,Missing
galibert-etal-2012-extended,grover-etal-2008-named,0,\N,Missing
galibert-etal-2012-extended,A00-1044,0,\N,Missing
galibert-etal-2012-extended,C02-1130,0,\N,Missing
galibert-etal-2012-extended,W11-0411,1,\N,Missing
galibert-etal-2012-extended,I11-1058,1,\N,Missing
galibert-etal-2012-extended,doddington-etal-2004-automatic,0,\N,Missing
galibert-etal-2012-extended,bick-2004-named,0,\N,Missing
galibert-etal-2012-extended,galibert-etal-2010-named,1,\N,Missing
galibert-etal-2012-extended,C96-1079,0,\N,Missing
garcia-fernandez-etal-2010-macaq,toney-etal-2008-evaluation,1,\N,Missing
goryainova-etal-2014-morpho,vasilescu-etal-2012-cross,1,\N,Missing
goryainova-etal-2014-morpho,luzzati-etal-2014-human,1,\N,Missing
goryainova-etal-2014-morpho,gravier-etal-2012-etape,0,\N,Missing
I11-1058,P98-1031,0,0.164757,"(2009) and many others have focused on speech data. Named Entity detection evaluation over French spoken data has been proposed within the Ester II project, as described by Galliano et al. (2009). Within the framework of the Quaero project,we proposed an extended named entity definition with compositional and hierarchical structure. This extension raises new issues and challenges in NER evaluation. First, as we shall explain below in more detail, the usual evaluation methods cannot compute the Slot Error Rate (SER) metric when named entities are compositional and recursive. Second, following Burger et al. (1998) and Hirschman et al. (1999), we consider that the evaluation of named entity recognition on noisy text output by automatic speech recognition (ASR) systems should take as reference the named entities found in the human annotation of a humantranscribed text: what should have been there in the ASR output. This requires to project the clean reference to the noisy text, which is made all the more difficult because of the compositional and hierarchical structure of the named entities. Introduction Named Entity Detection has been studied since the MUC conferences in 1987. The notion has been extend"
I11-1058,C02-1130,0,0.144389,"are hierarchical (Section 2.3) and compositional (Section 2.4). Section 2.5 provides a discussion of the issues they raise in the evaluation of named entity recognition from speech transcripts. 2.1 Named Entity Types Named Entity recognition was initially defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities are proper names categorized into three major classes: persons, locations and organizations. Proposals have been made to sub-divide these entities into finer-grained classes. For example, politicians for the person class by Fleischman and Hovy (2002) or cities for the location class by Fleischman (2001) as well as Lee and Lee (2005). The CONLL conference added a miscellaneous type which includes proper names outside the previous classes. Some classes are sometimes added, e.g. product by Bick (2004). Some numerical types are also often described and used in the literature: date, time, and amounts (money and percents in most cases). Specific entities have been proposed and handled for some tasks, e.g. language and shape by Rosset et al. (2007), or email address and phone number (Maynard et al., 2001). In specific domains, entities such as g"
I11-1058,galibert-etal-2010-named,1,0.873075,"Missing"
I11-1058,moreau-etal-2010-evaluation,1,0.810616,"ed to project the clean reference on the noisy text in order to build a new reference. That new reference then allows us to apply the clean text methodology. This projection method consists in finding new positions for the frontiers through either a dynamic programming alignment (standard sclitetype ASR evaluation alignment) or a phone-level dynamic programming alignment using canonical phonetizations. They noticed the result was too strict frontier-wise and required reducing the weight of frontier errors to obtain significant results. In Question Answering from speech transcripts evaluation, Moreau et al. (2010) required that QA systems extract answers to natural language questions from ASR outputs of broadcast news shows. The inherent application was to replay the sound segment containing the answer, with a time interval as an answer; it tolerated a time interval around the boundaries. The results were satisfactory. We thus decided to project the clean reference on the noisy text following five steps: pers.ind name.first recevrons Benoît name.last Majimel 167.5s recevrons l' acteur 168s Benoît magie name.first 168.5s mais l' acteur name.last pers.ind Figure 4: Example of a fuzzy reference built by t"
I11-1058,C96-1079,0,0.792768,"al nature of the extended named entities imply a specific method when evaluating system outputs (see Section 3). In this section, we present our extension to named entities, starting with related work (Section 2.1) and specifying their scope (Section 2.2). Our entities are hierarchical (Section 2.3) and compositional (Section 2.4). Section 2.5 provides a discussion of the issues they raise in the evaluation of named entity recognition from speech transcripts. 2.1 Named Entity Types Named Entity recognition was initially defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities are proper names categorized into three major classes: persons, locations and organizations. Proposals have been made to sub-divide these entities into finer-grained classes. For example, politicians for the person class by Fleischman and Hovy (2002) or cities for the location class by Fleischman (2001) as well as Lee and Lee (2005). The CONLL conference added a miscellaneous type which includes proper names outside the previous classes. Some classes are sometimes added, e.g. product by Bick (2004). Some numerical types are also often described and used in the literature: date"
I11-1058,W11-0411,1,0.350134,"om news data, we chose to support new kinds of entities (time, function, etc.) in order to extract a maximum of information from the corpus we processed. Compared to existing named entity structuration, our approach is more general than the extensions that have been done for specific domains, and is simpler than the complete hierarchy defined by Sekine (2004). This structure allows us to cover a large amount of named entities with a basic categorization so as to be quickly suitable for all further annotation work. The extended named entities we defined are both hierarchical and compositional (Grouin et al., 2011). This hierarchical and compositional nature of the extended named entities imply a specific method when evaluating system outputs (see Section 3). In this section, we present our extension to named entities, starting with related work (Section 2.1) and specifying their scope (Section 2.2). Our entities are hierarchical (Section 2.3) and compositional (Section 2.4). Section 2.5 provides a discussion of the issues they raise in the evaluation of named entity recognition from speech transcripts. 2.1 Named Entity Types Named Entity recognition was initially defined as recognizing proper names (Co"
I11-1058,W03-0419,0,0.0376895,"Missing"
I11-1058,sekine-nobata-2004-definition,0,0.68687,"of proper names (e.g., phrases built around substantives). In this work, we decided to extend the coverage of the named entities rather than sub-dividing the existing classes as it has been done in previous work. As we aimed to build a fact database from news data, we chose to support new kinds of entities (time, function, etc.) in order to extract a maximum of information from the corpus we processed. Compared to existing named entity structuration, our approach is more general than the extensions that have been done for specific domains, and is simpler than the complete hierarchy defined by Sekine (2004). This structure allows us to cover a large amount of named entities with a basic categorization so as to be quickly suitable for all further annotation work. The extended named entities we defined are both hierarchical and compositional (Grouin et al., 2011). This hierarchical and compositional nature of the extended named entities imply a specific method when evaluating system outputs (see Section 3). In this section, we present our extension to named entities, starting with related work (Section 2.1) and specifying their scope (Section 2.2). Our entities are hierarchical (Section 2.3) and c"
I11-1058,I05-1058,0,0.0287484,"ussion of the issues they raise in the evaluation of named entity recognition from speech transcripts. 2.1 Named Entity Types Named Entity recognition was initially defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities are proper names categorized into three major classes: persons, locations and organizations. Proposals have been made to sub-divide these entities into finer-grained classes. For example, politicians for the person class by Fleischman and Hovy (2002) or cities for the location class by Fleischman (2001) as well as Lee and Lee (2005). The CONLL conference added a miscellaneous type which includes proper names outside the previous classes. Some classes are sometimes added, e.g. product by Bick (2004). Some numerical types are also often described and used in the literature: date, time, and amounts (money and percents in most cases). Specific entities have been proposed and handled for some tasks, e.g. language and shape by Rosset et al. (2007), or email address and phone number (Maynard et al., 2001). In specific domains, entities such as gene, protein, DNA etc. are also addressed (Ohta, 2002) and campaigns are organized f"
I11-1058,W04-1213,0,\N,Missing
I11-1058,bonneau-maynard-etal-2006-results,0,\N,Missing
I11-1058,C98-1031,0,\N,Missing
I11-1058,bick-2004-named,0,\N,Missing
I11-1142,W05-0620,0,0.0372637,"(Tjong Kim Sang and De Meulder, 2003), where only four named entities were involved: Person, Organization, Location and Other. The latter was used for proper names belonging to any other entity different from the first three types. After this task, named entity detection has been refined to include other entities, e.g. describing time expressions, events, quantity, currency etc. Current named entity detection tasks provide a fine-grained semantic representation level of the lexical surface form, sometimes comparable to other semantic representations like those used in Semantic Role Labeling (Carreras and Marquez, 2005) or Spoken Language Understanding (De Mori et al., 2008). The increasing complexity of named entities definition reflects the change of needs in computer science activities, that in turn is the consequence of the tremendous growth of the amount of information to deal with nowadays. The set of named entities used in this work has been recently defined in (Grouin et al., 2011) and presents an important difference with respect to previous sets. Beyond the presence of subtypes, named entities have a tree structure. For example, the Organization type can be characterized by the subtypes administrat"
I11-1142,I11-1058,1,0.866669,"Missing"
I11-1142,P05-1022,0,0.27713,"with this procedure provides the same prediction accuracy than what we could have training the model directly with all features. 3.2 Structured Named Entities Reconstruction Models described in this section are well-known solutions for syntactic parsing. We report them to provide a self-contained and complet work, our main contribution in this respect is to have implemented and adapted algorithms to our task. The model we use for entity tree reconstruction is PCFG (Booth and Thomson, 1973; Krenn and Samuelsson, 1997). There are more accurate models for the same purpose, e.g. the one used in (Charniak and Johnson, 2005). In practice, the first annotation step being carried out with CRF, which provide a high robustness on noisy data, there is no need for complex and expensive models. Moreover PCFG are quite accurate and very fast for parsing (Collins and Koo, 2005). The input of the CRF model described in the previous section is a sentence like the one reported in section 24 . The output is the sequence of entities corresponding to the leaves of the tree in Figure 2, i.e. entity tree components. For example the chunk Nations unies (United Nations) is annotated by CRF as org.adm-B{Nations} org.adm-I{unies} whe"
I11-1142,J98-4004,0,0.732782,"ted: S ⇒ amount loc.adm.town ... org.adm amount ⇒ val object time.date.rel ⇒ name time-modifier object ⇒ func.coll func.coll ⇒ kind org.adm org.adm ⇒ name where the first production as been cut to keep readability and corresponds to the children of the tree root S. Once the rules have been generated from all trees in the training set, probabilities are estimated with simple maximum likelihood estimation as the probability of a production given the right-hand side (RHS) of the rule. The parsing algorithm using the PCFG generated from entity trees is the Cocke-Younger-Kasami (CYK) described in (Johnson, 1998). In order to use this algorithm production rules must be in Chomsky Normal Form (CNF), i.e. rules must have one of the two forms: i) Xi ⇒ Xj Xk ; ii) Xi ⇒ w, where X are non-terminal symbols and w are terminal symbols. The corresponding probabilities are pi→j,k = P (Xi ⇒ Xj , Xk ) P (Xi ) (6) P (Xi ⇒ w) P (Xi ) (7) pi→w = 4 90 personnes toujours pr´esentes a` Atambua c’ est l`a qu’ hier matin ont e´ t´e tu´es 3 employ´es du haut commissariat des Nations unies aux r´efugi´es , le HCR 1273 There are well-known algorithms to convert a grammar into CNF, e.g. (Krenn and Samuelsson, 1997). Probabil"
I11-1142,galibert-etal-2010-named,1,0.876245,"Missing"
I11-1142,J05-1003,0,0.207749,". We report them to provide a self-contained and complet work, our main contribution in this respect is to have implemented and adapted algorithms to our task. The model we use for entity tree reconstruction is PCFG (Booth and Thomson, 1973; Krenn and Samuelsson, 1997). There are more accurate models for the same purpose, e.g. the one used in (Charniak and Johnson, 2005). In practice, the first annotation step being carried out with CRF, which provide a high robustness on noisy data, there is no need for complex and expensive models. Moreover PCFG are quite accurate and very fast for parsing (Collins and Koo, 2005). The input of the CRF model described in the previous section is a sentence like the one reported in section 24 . The output is the sequence of entities corresponding to the leaves of the tree in Figure 2, i.e. entity tree components. For example the chunk Nations unies (United Nations) is annotated by CRF as org.adm-B{Nations} org.adm-I{unies} where suffixes -B and -I (for Begin and Inside) are used to have a one-to-one correspondence between words and entities. This makes the NER task a sequence segmentation and labeling problem, without having to deal with alignment issues. From the annota"
I11-1142,P10-1052,0,0.00949819,"Missing"
I11-1142,doddington-etal-2004-automatic,0,0.0632438,"ta for the annotation: transcriptions of French broadcast data. We propose an original and effective system to tackle this new task, putting together the strengths of solutions for sequence labeling approaches and syntactic parsing via cascading of different models. Our system was evaluated in the 2011 Quaero named entity detection evaluation campaign and ranked first, with results far better than those of the other participating systems. 1 Introduction Named Entity Detection is a well-known NLP task used to extract semantic information in other more complex tasks such as Relation Extraction (Doddington et al., 2004) or Question Answering (Voorhees, 2001). After its definition in MUC-6 (Grishman and Sundheim, 1996), the NER task evolved increasing its complexity. Current definitions provide a fine-grained semantic information level with a broad coverage (Sekine, 2004). This has increased the interests in developing named entity detection systems. In this paper we describe a new set of named entities defined recently(Grouin et al., 2011). These named entities have a multilevel tree structure where components are combined to define more Sophie Rosset LIMSI-CNRS / Orsay Cedex, France rosset@limsi.fr complex"
I11-1142,C96-1079,0,0.901603,"tive system to tackle this new task, putting together the strengths of solutions for sequence labeling approaches and syntactic parsing via cascading of different models. Our system was evaluated in the 2011 Quaero named entity detection evaluation campaign and ranked first, with results far better than those of the other participating systems. 1 Introduction Named Entity Detection is a well-known NLP task used to extract semantic information in other more complex tasks such as Relation Extraction (Doddington et al., 2004) or Question Answering (Voorhees, 2001). After its definition in MUC-6 (Grishman and Sundheim, 1996), the NER task evolved increasing its complexity. Current definitions provide a fine-grained semantic information level with a broad coverage (Sekine, 2004). This has increased the interests in developing named entity detection systems. In this paper we describe a new set of named entities defined recently(Grouin et al., 2011). These named entities have a multilevel tree structure where components are combined to define more Sophie Rosset LIMSI-CNRS / Orsay Cedex, France rosset@limsi.fr complex and general entity structures. This definition increases significantly the complexity of the NER tas"
I11-1142,C02-1130,0,0.0232295,"ng to sentence words have been removed to keep readability. describe and comment the results. Finally in section 5 we draw our conclusions and propose some perspectives for future work. 2 Towards Tree Structured Named Entities Named Entity Recognition was first defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996) named entities are proper names falling into three major classes: persons, locations and organizations. There are some propositions to sub-divide these entities into fine-grained classes. For example, politicians for the person class (Fleischman and Hovy, 2002) or cities for the location class (Fleischman, 2001). Some are sometimes added like product (Bick, 2004; Galliano et al., 2009). Recently some extensions of named entity have been proposed. For example, (Sekine, 2004) defined a complete hierarchy of named entities containing about 200 types. A well-known task of named entity detection is the one proposed for the CoNLL shared task 2003, described in (Tjong Kim Sang and De Meulder, 2003), where only four named entities were involved: Person, Organization, Location and Other. The latter was used for proper names belonging to any other entity diff"
I11-1142,bick-2004-named,0,0.0315399,"raw our conclusions and propose some perspectives for future work. 2 Towards Tree Structured Named Entities Named Entity Recognition was first defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996) named entities are proper names falling into three major classes: persons, locations and organizations. There are some propositions to sub-divide these entities into fine-grained classes. For example, politicians for the person class (Fleischman and Hovy, 2002) or cities for the location class (Fleischman, 2001). Some are sometimes added like product (Bick, 2004; Galliano et al., 2009). Recently some extensions of named entity have been proposed. For example, (Sekine, 2004) defined a complete hierarchy of named entities containing about 200 types. A well-known task of named entity detection is the one proposed for the CoNLL shared task 2003, described in (Tjong Kim Sang and De Meulder, 2003), where only four named entities were involved: Person, Organization, Location and Other. The latter was used for proper names belonging to any other entity different from the first three types. After this task, named entity detection has been refined to include o"
I11-1142,sekine-nobata-2004-definition,0,0.176644,"Our system was evaluated in the 2011 Quaero named entity detection evaluation campaign and ranked first, with results far better than those of the other participating systems. 1 Introduction Named Entity Detection is a well-known NLP task used to extract semantic information in other more complex tasks such as Relation Extraction (Doddington et al., 2004) or Question Answering (Voorhees, 2001). After its definition in MUC-6 (Grishman and Sundheim, 1996), the NER task evolved increasing its complexity. Current definitions provide a fine-grained semantic information level with a broad coverage (Sekine, 2004). This has increased the interests in developing named entity detection systems. In this paper we describe a new set of named entities defined recently(Grouin et al., 2011). These named entities have a multilevel tree structure where components are combined to define more Sophie Rosset LIMSI-CNRS / Orsay Cedex, France rosset@limsi.fr complex and general entity structures. This definition increases significantly the complexity of the NER task, even more due to the type of data used for the annotation: manual and automatic transcriptions of French broadcast data. Given such a definition, it is n"
I11-1142,allauzen-bonneau-maynard-2008-training,0,0.0621669,"re for incremental training of CRF models is realized with our own software. We didn’t optimize parameters ρ1 and ρ2 of the elastic net (see section 3.1), default values lead in most cases to very accurate models. We used a wide set of features in CRF models, in a window of [-2,+2] around the target word: • A set of standard features like word prefixes and suffixes of length from 1 to 5, plus some Yes/No features like “Does the word start with capital letter ?” “Does the word contain non alphanumeric characters ?”, etc. • Morpho-syntactic features extracted from the output of the tool tagger (Allauzen and Bonneau-Maynard, 2008) • Features extracted from the output of the tool WMatch (Galibert, 2009; Rosset et al., 2008). The output provided by WMatch contains detailed motpho-syntactic information as well as semantic information at the same level of named entities. Concerning the PCFG model, for preliminary studies we used our own implementation, but for this work we used the much faster implementation described in (Johnson, 1998).6 Concerning data, for preliminary studies carried out to validate our incremental training procedure, we used the same data used in the ESTER2 named entity detection evaluation campaign, T"
I11-1142,W11-0411,1,0.861081,"g systems. 1 Introduction Named Entity Detection is a well-known NLP task used to extract semantic information in other more complex tasks such as Relation Extraction (Doddington et al., 2004) or Question Answering (Voorhees, 2001). After its definition in MUC-6 (Grishman and Sundheim, 1996), the NER task evolved increasing its complexity. Current definitions provide a fine-grained semantic information level with a broad coverage (Sekine, 2004). This has increased the interests in developing named entity detection systems. In this paper we describe a new set of named entities defined recently(Grouin et al., 2011). These named entities have a multilevel tree structure where components are combined to define more Sophie Rosset LIMSI-CNRS / Orsay Cedex, France rosset@limsi.fr complex and general entity structures. This definition increases significantly the complexity of the NER task, even more due to the type of data used for the annotation: manual and automatic transcriptions of French broadcast data. Given such a definition, it is not possible to tackle the task with traditional sequence labeling approaches. At the same time, solutions able to reconstruct tree structures from flat sequences, like synt"
I11-1142,W03-0419,0,\N,Missing
I11-1142,W04-3223,0,\N,Missing
L16-1226,piperidis-2012-meta,0,0.0307233,"at were not explicitly labeled. 4. Conclusion The purpose of the CAMOMILE project has been to explore new practices around collaborative annotation and test it on specific use cases with dedicated prototypes. The developed framework can be summarized as a remote repository of annotations which are metadata attached to fragments of the media from a corpus, along with the associated RESTful API. It is thus compatible with other abstraction layers, e.g., annotation graphs (Bird and Liberman, 2001), and the metadata can follow standards 1424 in the domain as proposed in the META-SHARE initiative (Piperidis, 2012)7 . This simple framework was robust enough to support the active learning scenario described in this paper, as long as the organization of a MediaEval task with 20 annotators involved (73426 annotations) (Poignant et al., 2016). Its source code is freely available on GitHub. Further developments would improve the platform, like a direct communication between the clients through WebSockets or a flexible historization of the annotations. 5. Acknowledgements We thank the members of the CAMOMILE international advisory committee for their time and their precious advices and proposals. This work wa"
L16-1226,auer-etal-2010-elan,0,0.0761345,"Missing"
L16-1226,giraudel-etal-2012-repere,0,0.0770122,"Missing"
L16-1297,I11-1142,1,0.753925,"n downstream modules. This studies helped to better understand the origine of errors in order to build more robust ASR syst`emes. Despite all progresses, ASR systems still not perfect but their performances allows their use in many application case. In the same time the impact of the residual errors still miss understanded, mainly because we dont know how to measure or how to estimate the seriousness of transcription error for modules using ASR output. ASR errors seriousness can vary with respect to the application (see for example (Comas and Turmo, 2009) for question-answering on speech, or (Dinarelli and Rosset, 2011) for named entity recognition). We place ourselves in the context of Automatic Speech Recognition (ASR) and Named Entity Recognition (NER) combined for a task of NER in speech. Various evaluation metrics for ASR outputs can be found in the literature. Our hypothesis is that an evaluation metric, besides giving a performance score, is able to provide information about the individual errors produced by an ASR system. We expect a metric to be able to give information about the seriousness of the errors given a task. Thus we are interested by generating a ranking of ASR errors according to differe"
L16-1297,galibert-etal-2014-etape,1,0.893589,"Missing"
L16-1297,goryainova-etal-2014-morpho,1,0.85429,". They pointed out that although today the best ASR speech models are quite efficient, they have not yet reached the status of being able to perfectly take into account all the observed acoustic variation. Human listeners are still outperforming them by a factor of 5 to 6 (Vasilescu et al., 2012). The taxonomy of errors pointed out that some words are frequently victims of ASR errors: in particular short, acoustically poor and frequent items lead to local ambiguity (Adda-Decker, 2006). Other work was done studying or classifying errors given the type of words which are involved. For example, (Goryainova et al., 2014) studied ASR errors given the Part of Speech of the associated word. Most of the studies done on ASR error analysis focus on the cause of errors more then on the possible impact that it can have on downstream modules. This studies helped to better understand the origine of errors in order to build more robust ASR syst`emes. Despite all progresses, ASR systems still not perfect but their performances allows their use in many application case. In the same time the impact of the residual errors still miss understanded, mainly because we dont know how to measure or how to estimate the seriousness"
L16-1297,nemoto-etal-2008-speech,0,0.0821717,"Missing"
L16-1297,vasilescu-etal-2012-cross,0,0.0300428,"ge about them in order to improve ASR systems (Boh´acˇ et al., 2012; Dufour and Esteve, 2008) or trying to automatically detect them in ASR output (Ghannay et al., 2015). ASR errors have been mainly investigated in the framework of comparisons between automatic vs. human decoding of speech (Scharenborg, 2007; Lippmann, 1997). They pointed out that although today the best ASR speech models are quite efficient, they have not yet reached the status of being able to perfectly take into account all the observed acoustic variation. Human listeners are still outperforming them by a factor of 5 to 6 (Vasilescu et al., 2012). The taxonomy of errors pointed out that some words are frequently victims of ASR errors: in particular short, acoustically poor and frequent items lead to local ambiguity (Adda-Decker, 2006). Other work was done studying or classifying errors given the type of words which are involved. For example, (Goryainova et al., 2014) studied ASR errors given the Part of Speech of the associated word. Most of the studies done on ASR error analysis focus on the cause of errors more then on the possible impact that it can have on downstream modules. This studies helped to better understand the origine of"
L16-1366,I13-1077,0,0.0232293,"o rank; Terminology 1. Introduction The difference between the language used by health care professionals and that used by patients is cited as a source of miscommunication (Elhadad and Sutaria, 2007) or difficulty when mining patient forums (Nikfarjam et al., 2015). For example, lay people tend to use idiomatic expressions such as “mal de chien”(Fr) [literally, “dog pain” (En)] to refer to “douleur intense”(Fr) [severe pain, En]. Dictionaries which differentiate familiar terms from specialized terms are valuable for many applications of Natural Language Processing such as sentiment analysis (Ali et al., 2013) and paraphrase acquisition (Elhadad and Sutaria, 2007; Deléger and Zweigenbaum, 2009; van der Plas and Tiedemann, 2010). In the context of an e-learning system for training medical students, we are creating a virtual patient who must answer student questions in a natural way, hence using patient language (Campillos-Llanos et al., 2015; Campillos-Llanos et al., 2016). This is yet another application which requires a way to assess which term among a set of equivalent expressions would be more likely to be used by a patient. Nevertheless, such resources remain scarce and thus far, only one, the"
L16-1366,W15-4660,1,0.410631,"such as “mal de chien”(Fr) [literally, “dog pain” (En)] to refer to “douleur intense”(Fr) [severe pain, En]. Dictionaries which differentiate familiar terms from specialized terms are valuable for many applications of Natural Language Processing such as sentiment analysis (Ali et al., 2013) and paraphrase acquisition (Elhadad and Sutaria, 2007; Deléger and Zweigenbaum, 2009; van der Plas and Tiedemann, 2010). In the context of an e-learning system for training medical students, we are creating a virtual patient who must answer student questions in a natural way, hence using patient language (Campillos-Llanos et al., 2015; Campillos-Llanos et al., 2016). This is yet another application which requires a way to assess which term among a set of equivalent expressions would be more likely to be used by a patient. Nevertheless, such resources remain scarce and thus far, only one, the Consumer Health Vocabulary (CHV, Keselman et al. (2007)), aims at estimating how familiar a term is. However, the CHV only covers the English language, and limited attempts have been made to cover other languages such as French and Portuguese. Tapi Nzali et al. (2015) mention the creation of a French CHV, but actually address a differe"
L16-1366,L16-1505,1,0.819925,"terally, “dog pain” (En)] to refer to “douleur intense”(Fr) [severe pain, En]. Dictionaries which differentiate familiar terms from specialized terms are valuable for many applications of Natural Language Processing such as sentiment analysis (Ali et al., 2013) and paraphrase acquisition (Elhadad and Sutaria, 2007; Deléger and Zweigenbaum, 2009; van der Plas and Tiedemann, 2010). In the context of an e-learning system for training medical students, we are creating a virtual patient who must answer student questions in a natural way, hence using patient language (Campillos-Llanos et al., 2015; Campillos-Llanos et al., 2016). This is yet another application which requires a way to assess which term among a set of equivalent expressions would be more likely to be used by a patient. Nevertheless, such resources remain scarce and thus far, only one, the Consumer Health Vocabulary (CHV, Keselman et al. (2007)), aims at estimating how familiar a term is. However, the CHV only covers the English language, and limited attempts have been made to cover other languages such as French and Portuguese. Tapi Nzali et al. (2015) mention the creation of a French CHV, but actually address a different problem: the identification o"
L16-1366,W09-3102,1,0.945102,"sed by health care professionals and that used by patients is cited as a source of miscommunication (Elhadad and Sutaria, 2007) or difficulty when mining patient forums (Nikfarjam et al., 2015). For example, lay people tend to use idiomatic expressions such as “mal de chien”(Fr) [literally, “dog pain” (En)] to refer to “douleur intense”(Fr) [severe pain, En]. Dictionaries which differentiate familiar terms from specialized terms are valuable for many applications of Natural Language Processing such as sentiment analysis (Ali et al., 2013) and paraphrase acquisition (Elhadad and Sutaria, 2007; Deléger and Zweigenbaum, 2009; van der Plas and Tiedemann, 2010). In the context of an e-learning system for training medical students, we are creating a virtual patient who must answer student questions in a natural way, hence using patient language (Campillos-Llanos et al., 2015; Campillos-Llanos et al., 2016). This is yet another application which requires a way to assess which term among a set of equivalent expressions would be more likely to be used by a patient. Nevertheless, such resources remain scarce and thus far, only one, the Consumer Health Vocabulary (CHV, Keselman et al. (2007)), aims at estimating how fami"
L16-1366,W07-1007,0,0.0400165,"luation of this approach is conducted on 134 terms from the UMLS Metathesaurus and 868 terms from the Eugloss thesaurus. The Normalized Discounted Cumulative Gain obtained by our system is over 0.8 on both test sets. Besides, thanks to the learning-to-rank approach, adding morphological features to the language model features improves the results on the Eugloss thesaurus. Keywords: Technicality of Medical Terms; Learning to rank; Terminology 1. Introduction The difference between the language used by health care professionals and that used by patients is cited as a source of miscommunication (Elhadad and Sutaria, 2007) or difficulty when mining patient forums (Nikfarjam et al., 2015). For example, lay people tend to use idiomatic expressions such as “mal de chien”(Fr) [literally, “dog pain” (En)] to refer to “douleur intense”(Fr) [severe pain, En]. Dictionaries which differentiate familiar terms from specialized terms are valuable for many applications of Natural Language Processing such as sentiment analysis (Ali et al., 2013) and paraphrase acquisition (Elhadad and Sutaria, 2007; Deléger and Zweigenbaum, 2009; van der Plas and Tiedemann, 2010). In the context of an e-learning system for training medical s"
L16-1366,W10-3304,0,0.0619834,"Missing"
L16-1433,P12-3007,0,0.226857,"Missing"
L16-1433,P05-1045,0,0.00891247,"Missing"
L16-1433,W13-4039,0,0.0534255,"Missing"
L16-1433,D13-1036,0,0.0262914,"Missing"
L16-1505,L16-1366,1,0.819925,"terms over more technical terms. For this purpose, each set of terms sharing the same UMLS CUI is sorted by degree of technicality: e.g., for concept C0036973 (‘shiver’), grelottements is the less technical term, and frissonnement is the most technical. This degree of technicality was computed by comparing the probabilities of a term according to two language models respectively trained on a technical corpus of medical articles (CRTT)2 and on a non-technical corpus of online medical forums3 . The degree of technicality of a term is computed as the likelihood ratio of these two probabilities (Bouamor et al., 2016). To generate a lay variant of a term, its CUI is determined and the least technical term for this CUI is chosen. Additionally, a manually created list of {technical, lay} term pairs is used for terms lacking a UMLS CUI, or terms for which no degree of technicality could be computed because they were unseen in our training corpora: e.g., naus´ees et vomissements (‘nausea and vomiting’) refers to NVPO (‘PONV’, ‘Postoperative Nausea and Vomiting’). 5. Results and Evaluation The system has been tested on three patient cases with project partners and during public demonstrations, and a first evalu"
L16-1505,W15-4660,1,0.545287,"ly been addressed recently (McCray et al., 2000; Zeng-Treitler et al., 2007). Virtual patients (VP) are interactive systems and require managing terms—e.g. by formalizing ontological concepts (Nirenburg et al., 2008)—and a Natural Language Understanding (NLU) module. The NLU component may rely on text meaning representations for resolving paraphrases (Nirenburg et al., 2009) or a corpus of questions and answers curated by an expert (Kenny et al., 2008). We are developing a conversational agent to be used in a simulated consultation with a VP, where the system aims at training medical doctors (Campillos-Llanos et al., 2015). Users (medical students or doctors) interact with the VP to collect information that allows them to provide a correct diagnosis. Medical trainers define each e-learning case beforehand by entering the VP profile data in a clinical record (e.g. symptoms or medical history). Managing linguistic and terminological variation is crucial to match a user’s question to a term in the clinical record and to select suitable terms for answer generation. This paper gives an overview of the difficulties (Section 2.) and strategies applied in both analysis (Section 3.) and generation (Section 4.). We also"
L16-1505,W08-1507,0,0.07858,"Missing"
L16-1534,benikova-etal-2014-nosta,0,0.0719773,"irst introduced during the 6th Message Understanding Conference (Grishman and Sundheim, 1995), it corresponds to the recognition and classification of entities of interest in texts, generally of type Person, Organisation and Location. During subsequent evaluation campaigns and research programs, the task quickly broadened and became more complex, with the extension and refinement of typologies (Sekine et al., 2002; Galibert et al., 2011), the diversification of languages taken into account (Tjong Kim Sang and De Meulder, 2003; Santos et al., 2006; Magnini et al., 2008a; Galliano et al., 2009; Benikova et al., 2014), and the expansion of the linguistic scope with, along proper names, the consideration of nominal phrases and pronouns as candidate lexical units (Doddington et al., 2004). Later on, as recognition and classification were reaching satisfying performances (at least for well-covered languages and main stream texts such as news articles), attention focused on finer-grained processing, e.g. metonymy recognition (Markert and Nissim, 2009), and on the next logical step, namely disambiguation. Entity resolution has first been defined as a clustering problem, where different mentions in texts referri"
L16-1534,calzolari-etal-2012-lre,0,0.0303346,"NE resources (section 5.). Finally, we discuss needs and priorities for named entity resources (section 6.) and conclude (section 7.). 2. Related work Language resources (LR) have long been acknowledged as a cornerstone of NLP processes and the number of published resources is constantly growing. Although extremely valuable, the resulting set of language data is however difficult to handle and several initiatives have emerged to facilitate the discovery, the search and the documentation of LRs. Initiated in 2010, the LRE map intends to enhance the availability of information about resources (Calzolari et al., 2012) while large-scale projects such as CLARIN and META-SHARE aim at indexing LR repositories into farreaching networks. Despite these efforts, the discovery of resources remains a difficult process for metadata vocabularies differ from each other. Taking advantage of RDF, a recent work by (McCrae et al., 2015) attempts to harmonize heterogeneous descriptions of language resources. Beyond discoverability, the Open Linguistics Working Group1 concentrates on linguistic data interoperability by considering the Linked Data paradigm as a way to reconcile data and metadata descriptions (Chiarcos et al.,"
L16-1534,chiarcos-etal-2012-open,0,0.0720853,"ri et al., 2012) while large-scale projects such as CLARIN and META-SHARE aim at indexing LR repositories into farreaching networks. Despite these efforts, the discovery of resources remains a difficult process for metadata vocabularies differ from each other. Taking advantage of RDF, a recent work by (McCrae et al., 2015) attempts to harmonize heterogeneous descriptions of language resources. Beyond discoverability, the Open Linguistics Working Group1 concentrates on linguistic data interoperability by considering the Linked Data paradigm as a way to reconcile data and metadata descriptions (Chiarcos et al., 2012; Heath and Bizer, 2011). This community-based effort has initiated the creation of the Linguistic Linked Open Data Cloud and recently launched the metadata repository LingHub2 . While these enterprises are interested in linguistic resources as a whole, the present work focuses instead on resources related to named entities. The objective is not only to describe what exists, but especially to explain what serves what and to assess the extent to which today’s NE processing requirements are met in terms of resources. The study presented by (Nadeau and Sekine, 2007) discusses named entity recogni"
L16-1534,W15-3904,0,0.0262989,"Missing"
L16-1534,W09-3025,1,0.810929,"there is no complete description and assessment of NE resources. 3. • classification: categorising named entities according to a set of pre-defined semantic categories, • disambiguation/linking: linking named entity mentions to a unique reference, and • relation extraction: discovering relations between named entities. There exists two main usages of named entities depending on whether the application focuses on referential entities (e.g. indexing and knowledge integration), or whether it focuses on mentions (e.g. knowledge base population, anonymisation and information extraction at large) (Fort et al., 2009). Usual NE task combinations reflect these usages: recognizing and classifying are part of named entity recognition (suitable when focusing on mentions) and recognizing and disambiguating are part of entity linking (suitable when focusing on referents). For most NE systems and algorithms, resources are crucial for the achievement of these tasks. Three main types of resources may be distinguished, each playing a specific role. Typologies are used to define a semantic framework for the entities under consideration and are required for classification. They can be multi-purpose or domain-specific"
L16-1534,I11-1058,1,0.840253,"evolutions since then. Named entity processing is representative of the evolution of information extraction from a document to a semanticcentric view point (Rao et al., 2013). As first introduced during the 6th Message Understanding Conference (Grishman and Sundheim, 1995), it corresponds to the recognition and classification of entities of interest in texts, generally of type Person, Organisation and Location. During subsequent evaluation campaigns and research programs, the task quickly broadened and became more complex, with the extension and refinement of typologies (Sekine et al., 2002; Galibert et al., 2011), the diversification of languages taken into account (Tjong Kim Sang and De Meulder, 2003; Santos et al., 2006; Magnini et al., 2008a; Galliano et al., 2009; Benikova et al., 2014), and the expansion of the linguistic scope with, along proper names, the consideration of nominal phrases and pronouns as candidate lexical units (Doddington et al., 2004). Later on, as recognition and classification were reaching satisfying performances (at least for well-covered languages and main stream texts such as news articles), attention focused on finer-grained processing, e.g. metonymy recognition (Marker"
L16-1534,galibert-etal-2014-etape,0,0.132647,"tion switched from clustering mentions to aligning mentions to unique identifiers in a KB. Thereupon, the task of entity linking gained strong impetus (Ji and Grishman, 2011; Shen et al., 2015) and is now at the core of many knowledge extraction tools for the Semantic Web (Gangemi, 2013). In addition to this task-related vertical evolution, NE processing also branched out into several directions. Besides the general domain of well-written news wire data, work is carried out on specific domains, particularly bio-medical (Kim et al., 2003), and on more noisy input such as speech transcriptions (Galibert et al., 2014) and tweets (Ritter et al., 2011). Likewise, NE processing is called on to contribute in other research areas such as Digital Humanities, with OCRed documents (Rosset et al., 2012a) and languages or documents of earlier stages (Rodriquez et al., 2012; Brando et al., 2015). NE processing encompasses therefore different tasks which can be performed within a large variety of contexts. Accordingly, different methods and algorithms are used which, regardless of their nature, all require resources encoding linguistic knowledge about these units. Over the last decades, many named entity resources hav"
L16-1534,M95-1001,0,0.660462,"crucial for most, if not all, text mining applications. Indeed, referential units such as name of persons, places and organisations underlie the semantics of texts and guide their interpretation. Acknowledged some twenty years ago, named entity (NE) mining is an operation of ever increasing importance for many NLP applications which has undergone major evolutions since then. Named entity processing is representative of the evolution of information extraction from a document to a semanticcentric view point (Rao et al., 2013). As first introduced during the 6th Message Understanding Conference (Grishman and Sundheim, 1995), it corresponds to the recognition and classification of entities of interest in texts, generally of type Person, Organisation and Location. During subsequent evaluation campaigns and research programs, the task quickly broadened and became more complex, with the extension and refinement of typologies (Sekine et al., 2002; Galibert et al., 2011), the diversification of languages taken into account (Tjong Kim Sang and De Meulder, 2003; Santos et al., 2006; Magnini et al., 2008a; Galliano et al., 2009; Benikova et al., 2014), and the expansion of the linguistic scope with, along proper names, t"
L16-1534,D11-1072,0,0.12755,"Missing"
L16-1534,P11-1115,0,0.0343039,"onymy recognition (Markert and Nissim, 2009), and on the next logical step, namely disambiguation. Entity resolution has first been defined as a clustering problem, where different mentions in texts referring to the same entity must be grouped together (Mann and Yarowsky, 2003; Artiles et al., 2008). Next, with the advent of knowledge bases (KB) containing plenty of entities along with detailed information (Hovy et al., 2013), entity disambiguation switched from clustering mentions to aligning mentions to unique identifiers in a KB. Thereupon, the task of entity linking gained strong impetus (Ji and Grishman, 2011; Shen et al., 2015) and is now at the core of many knowledge extraction tools for the Semantic Web (Gangemi, 2013). In addition to this task-related vertical evolution, NE processing also branched out into several directions. Besides the general domain of well-written news wire data, work is carried out on specific domains, particularly bio-medical (Kim et al., 2003), and on more noisy input such as speech transcriptions (Galibert et al., 2014) and tweets (Ritter et al., 2011). Likewise, NE processing is called on to contribute in other research areas such as Digital Humanities, with OCRed do"
L16-1534,P79-1000,0,0.346156,"Missing"
L16-1534,magnini-etal-2008-evaluation,0,0.130078,"ticcentric view point (Rao et al., 2013). As first introduced during the 6th Message Understanding Conference (Grishman and Sundheim, 1995), it corresponds to the recognition and classification of entities of interest in texts, generally of type Person, Organisation and Location. During subsequent evaluation campaigns and research programs, the task quickly broadened and became more complex, with the extension and refinement of typologies (Sekine et al., 2002; Galibert et al., 2011), the diversification of languages taken into account (Tjong Kim Sang and De Meulder, 2003; Santos et al., 2006; Magnini et al., 2008a; Galliano et al., 2009; Benikova et al., 2014), and the expansion of the linguistic scope with, along proper names, the consideration of nominal phrases and pronouns as candidate lexical units (Doddington et al., 2004). Later on, as recognition and classification were reaching satisfying performances (at least for well-covered languages and main stream texts such as news articles), attention focused on finer-grained processing, e.g. metonymy recognition (Markert and Nissim, 2009), and on the next logical step, namely disambiguation. Entity resolution has first been defined as a clustering pr"
L16-1534,W03-0405,0,0.0578689,", along proper names, the consideration of nominal phrases and pronouns as candidate lexical units (Doddington et al., 2004). Later on, as recognition and classification were reaching satisfying performances (at least for well-covered languages and main stream texts such as news articles), attention focused on finer-grained processing, e.g. metonymy recognition (Markert and Nissim, 2009), and on the next logical step, namely disambiguation. Entity resolution has first been defined as a clustering problem, where different mentions in texts referring to the same entity must be grouped together (Mann and Yarowsky, 2003; Artiles et al., 2008). Next, with the advent of knowledge bases (KB) containing plenty of entities along with detailed information (Hovy et al., 2013), entity disambiguation switched from clustering mentions to aligning mentions to unique identifiers in a KB. Thereupon, the task of entity linking gained strong impetus (Ji and Grishman, 2011; Shen et al., 2015) and is now at the core of many knowledge extraction tools for the Semantic Web (Gangemi, 2013). In addition to this task-related vertical evolution, NE processing also branched out into several directions. Besides the general domain of"
L16-1534,W15-4205,0,0.0247922,"remely valuable, the resulting set of language data is however difficult to handle and several initiatives have emerged to facilitate the discovery, the search and the documentation of LRs. Initiated in 2010, the LRE map intends to enhance the availability of information about resources (Calzolari et al., 2012) while large-scale projects such as CLARIN and META-SHARE aim at indexing LR repositories into farreaching networks. Despite these efforts, the discovery of resources remains a difficult process for metadata vocabularies differ from each other. Taking advantage of RDF, a recent work by (McCrae et al., 2015) attempts to harmonize heterogeneous descriptions of language resources. Beyond discoverability, the Open Linguistics Working Group1 concentrates on linguistic data interoperability by considering the Linked Data paradigm as a way to reconcile data and metadata descriptions (Chiarcos et al., 2012; Heath and Bizer, 2011). This community-based effort has initiated the creation of the Linguistic Linked Open Data Cloud and recently launched the metadata repository LingHub2 . While these enterprises are interested in linguistic resources as a whole, the present work focuses instead on resources rel"
L16-1534,W09-1119,0,0.167243,"Missing"
L16-1534,D11-1141,0,0.130908,"Missing"
L16-1534,E12-2015,0,0.0807776,"Missing"
L16-1534,W12-3606,1,0.936307,"al., 2015) and is now at the core of many knowledge extraction tools for the Semantic Web (Gangemi, 2013). In addition to this task-related vertical evolution, NE processing also branched out into several directions. Besides the general domain of well-written news wire data, work is carried out on specific domains, particularly bio-medical (Kim et al., 2003), and on more noisy input such as speech transcriptions (Galibert et al., 2014) and tweets (Ritter et al., 2011). Likewise, NE processing is called on to contribute in other research areas such as Digital Humanities, with OCRed documents (Rosset et al., 2012a) and languages or documents of earlier stages (Rodriquez et al., 2012; Brando et al., 2015). NE processing encompasses therefore different tasks which can be performed within a large variety of contexts. Accordingly, different methods and algorithms are used which, regardless of their nature, all require resources encoding linguistic knowledge about these units. Over the last decades, many named entity resources have been built, addressing different needs, for different languages and for various input data. In this regard, the above mentioned development of NE-related tasks (e.g. entity link"
L16-1534,santos-etal-2006-harem,0,0.0942998,"Missing"
L16-1534,sekine-etal-2002-extended,0,0.627715,"h has undergone major evolutions since then. Named entity processing is representative of the evolution of information extraction from a document to a semanticcentric view point (Rao et al., 2013). As first introduced during the 6th Message Understanding Conference (Grishman and Sundheim, 1995), it corresponds to the recognition and classification of entities of interest in texts, generally of type Person, Organisation and Location. During subsequent evaluation campaigns and research programs, the task quickly broadened and became more complex, with the extension and refinement of typologies (Sekine et al., 2002; Galibert et al., 2011), the diversification of languages taken into account (Tjong Kim Sang and De Meulder, 2003; Santos et al., 2006; Magnini et al., 2008a; Galliano et al., 2009; Benikova et al., 2014), and the expansion of the linguistic scope with, along proper names, the consideration of nominal phrases and pronouns as candidate lexical units (Doddington et al., 2004). Later on, as recognition and classification were reaching satisfying performances (at least for well-covered languages and main stream texts such as news articles), attention focused on finer-grained processing, e.g. meto"
L16-1534,R11-1015,0,0.0629811,"Missing"
L16-1534,W03-0419,0,0.403652,"Missing"
L16-1534,doddington-etal-2004-automatic,0,\N,Missing
L18-1619,N13-1014,0,0.0243344,"nguages considered. Then, a tagset has to be created or adapted to the language, which requires linguistic expertise. Finally, the annotation also requires annotators with linguistic expertise. Crowdsourcing can be used for part-of-speech annotation (Hovy et al., 2014), and was even used for Alsatian (Millour et al., 2017). Yet, crowdsourcing necessitates an adapted platform, and communication to possible speakers, who for example in the case of Picard, are rare. A possible direction for POS tagging could be to create a minimum tag dictionary for the most frequent word types, such as used by (Garrette and Baldridge, 2013). This kind of approach still requires a test corpus to evaluate the tagger; and the performance remains low compared to more resourced languages. 6. Conclusion We have presented our methodology for producing corpora with POS annotations for three regional languages of France, namely Alsatian, Occitan and Picard. The tagsets are based on an extended version of the Universal POS tags, with some language-specific additions to account for particular linguistic phenomena. The annotation guidelines as well as the manually annotated corpora are freely available. We plan to use these corpora to devel"
L18-1619,P14-2062,0,0.0289724,"difficulties. First, it requires assembling a large textual corpus, which can be a challenge for these languages which have few electronic resources. Work on part-of-speech tagging for under-resourced languages is often based on parallel corpora, following (Yarowsky et al., 2001), but there are no such existing electronic corpora for the three languages considered. Then, a tagset has to be created or adapted to the language, which requires linguistic expertise. Finally, the annotation also requires annotators with linguistic expertise. Crowdsourcing can be used for part-of-speech annotation (Hovy et al., 2014), and was even used for Alsatian (Millour et al., 2017). Yet, crowdsourcing necessitates an adapted platform, and communication to possible speakers, who for example in the case of Picard, are rare. A possible direction for POS tagging could be to create a minimum tag dictionary for the most frequent word types, such as used by (Garrette and Baldridge, 2013). This kind of approach still requires a test corpus to evaluate the tagger; and the performance remains low compared to more resourced languages. 6. Conclusion We have presented our methodology for producing corpora with POS annotations fo"
L18-1619,C94-1097,0,0.819681,"Missing"
L18-1619,L16-1262,0,0.116016,"Missing"
L18-1619,W14-5303,1,0.882391,"Missing"
L18-1619,H01-1035,0,0.160961,"potlights that they lit the pit . Table 10: Annotation example for Picard The annotation guidelines and the corpora are available for all three languages on the Zenodo platform, in the RESTAURE project community (see Section 9. for the corpus list).17 5. Related Work Creating annotated corpora for under-resourced languages presents several difficulties. First, it requires assembling a large textual corpus, which can be a challenge for these languages which have few electronic resources. Work on part-of-speech tagging for under-resourced languages is often based on parallel corpora, following (Yarowsky et al., 2001), but there are no such existing electronic corpora for the three languages considered. Then, a tagset has to be created or adapted to the language, which requires linguistic expertise. Finally, the annotation also requires annotators with linguistic expertise. Crowdsourcing can be used for part-of-speech annotation (Hovy et al., 2014), and was even used for Alsatian (Millour et al., 2017). Yet, crowdsourcing necessitates an adapted platform, and communication to possible speakers, who for example in the case of Picard, are rare. A possible direction for POS tagging could be to create a minimu"
lamel-etal-2008-question,gravier-etal-2004-ester,0,\N,Missing
lamel-etal-2008-question,galliano-etal-2006-corpus,0,\N,Missing
luzzati-etal-2014-human,nemoto-etal-2008-speech,1,\N,Missing
luzzati-etal-2014-human,gravier-etal-2012-etape,0,\N,Missing
moreau-etal-2010-evaluation,gravier-etal-2004-ester,0,\N,Missing
moreau-etal-2010-evaluation,mostefa-etal-2006-evaluation,1,\N,Missing
moreau-etal-2010-evaluation,galliano-etal-2006-corpus,0,\N,Missing
P14-3005,P13-1006,0,0.0221479,"ighly abstract schemes. In the problematic of interactive continuous learning, Artzi and Zettlemoyer (2011) build by learning a semantic NL parser based on combinatory categorial grammars (CCG). Kushman and Barzilay (2013) also use CCG in order to generate regular expressions corresponding to their NL descriptions. This constructive approach by translation allows to generalize over learning examples, while the expressive power of regular expressions correspond to the type-3 grammars of the Chomsky hierarchy. This is not the case for the programming languages since they are at least of type-2. Yu and Siskind (2013) use hidden Markov models to learn a mapping between object tracks from a video sequence and predicates extracted from a NL description. The goal of their approach is different from ours but the underlying problem of finding a map between objects can be compared. The matched objects constitute here a FL expression instead of a video sequence track. 2.2 abstraction, and then encoding the obtained object into the target language. While following a different goal, one of the tasks of the XLike project (Marko Tadi´c et al., 2012) was to examine the possibility of translating statements from NL (En"
P14-3005,D11-1039,0,0.0135452,"e tokens in bold font are linked with the commands parameters, cf. section 4. Note that the relation between utterances and commands is a n to n. Several utterances can be associated to the same command and conversely. 2010) uses reinforcement learning to map English NL instructions to a sequence of FL commands. The mapping takes high-level instructions and their constitution into account. The scope of usable commands is yet limited to graphical interaction possibilities. As a result, the learning does not produce highly abstract schemes. In the problematic of interactive continuous learning, Artzi and Zettlemoyer (2011) build by learning a semantic NL parser based on combinatory categorial grammars (CCG). Kushman and Barzilay (2013) also use CCG in order to generate regular expressions corresponding to their NL descriptions. This constructive approach by translation allows to generalize over learning examples, while the expressive power of regular expressions correspond to the type-3 grammars of the Chomsky hierarchy. This is not the case for the programming languages since they are at least of type-2. Yu and Siskind (2013) use hidden Markov models to learn a mapping between object tracks from a video sequen"
P14-3005,P10-1129,0,0.0589199,"Missing"
P14-3005,P09-1010,0,0.0914624,"Missing"
P14-3005,N13-1103,0,0.0165164,"ces and commands is a n to n. Several utterances can be associated to the same command and conversely. 2010) uses reinforcement learning to map English NL instructions to a sequence of FL commands. The mapping takes high-level instructions and their constitution into account. The scope of usable commands is yet limited to graphical interaction possibilities. As a result, the learning does not produce highly abstract schemes. In the problematic of interactive continuous learning, Artzi and Zettlemoyer (2011) build by learning a semantic NL parser based on combinatory categorial grammars (CCG). Kushman and Barzilay (2013) also use CCG in order to generate regular expressions corresponding to their NL descriptions. This constructive approach by translation allows to generalize over learning examples, while the expressive power of regular expressions correspond to the type-3 grammars of the Chomsky hierarchy. This is not the case for the programming languages since they are at least of type-2. Yu and Siskind (2013) use hidden Markov models to learn a mapping between object tracks from a video sequence and predicates extracted from a NL description. The goal of their approach is different from ours but the underl"
P14-3005,P02-1040,0,0.0910512,"rance of Table 1 looks like: where at last f (w, s) is the frequency of the word w in the sentence s. As we did for the Jaccard index, we performed the measures on both raw and lemmatized words. On the other hand, getting rid of the function words and closed class words is not here mandatory since the tf-idf measure already takes the global word frequency into account. 4.1.3 The BLEU measure &lt;_operation&gt; &lt;_action&gt; charge|_˜V &lt;/_action&gt; &lt;_det&gt; les &lt;/_det&gt; &lt;_subs&gt; donn´ ees|_˜N &lt;/_subs&gt; &lt;_prep&gt; depuis &lt;/_prep&gt; &lt;_unk&gt; &quot;res.csv&quot; &lt;/_unk&gt; &lt;/_operation&gt; The bilingual evaluation understudy algorithm (Papineni et al., 2002) focuses on n-grams cooccurrences. This algorithm can be used to discard examples where the words ordering is too far from the candidate. It computes a modified precision based on the ratio of the co-occurring ngrams within candidate and reference sentences, on the total size of the candidate normalized by n. PBLEU (si , S) = X maxs ∈S occ(grn , sc ) c grams(s i , n) gr ∈s n Words tagged as unknown are considered as potential variable or function names. We also added a preliminary rule to identify character strings and count them among the possibly linked features of the utterance. The command"
P14-3005,toney-etal-2008-evaluation,1,\N,Missing
P14-3005,P13-1164,0,\N,Missing
quintard-etal-2010-question,ayache-etal-2006-equer,1,\N,Missing
quintard-etal-2010-question,bernard-etal-2010-question,1,\N,Missing
R11-1104,R09-1019,0,0.0179842,"g whether a document is intrinsically relevant or not, and is totally compliant with previous and further analysis in the standard QA strategy. Statistical language modeling (SLM) seems suitable for such a task. SLM (Jelinek et al., 1990; Rosenfeld, 2000) provides an easy way to cope with the complexity of natural language by expressing various language phenomena in terms of simple parameters in a statistical model. If SLMs have not been used extensively in pure QA, although they have shown promizing results e.g. to evaluate the intrinsic relevancy of documents estimated for ranking passages (Ganesh and Varma, 2009), they are classically used to help solving tasks closely related to the QA one, especially when topic modeling is worth e.g. entity linking and guided summarization 2 (Varma et al., 2010). 3 3.1 3.2 The first step is to build a 3-gram LM based on a 500k words dictionary obtained from a large corpus of French newspapers articles. Then, OOV and PPX scores are calculated to each DEV documents using the LM and we estimate the distribution (assuming they are Gaussian) related to each parameter by calculation of the mean and standard deviation. Finally, we define a GMM which combines the OOV and PP"
R11-1104,quintard-etal-2010-question,1,0.581375,"nch5G documents. All the lists were used to feed a filter we plugged in our QA chain to refine the original documents selection made by the system during the IR step. To reduce the number of eligible documents for searching answers we intersect the list of documents retrieved by the system during the IR step with one of the 43 lists. The objective of this filter is to help the QA system to choose the best documents given an estimation of their quality and the question. For the tuning of answer selection parameters of our QA system, we use a set of 722 factoid questions and answers references (Quintard et al., 2010) as well as the 43 document lists provided by the filtering module. For all the possible configurations of parameters, the system provides results for the complete QA chain. These results after tuning serve as a basis for selecting the best document lists. We defined two different list selection methodologies. In the first one (methodology-1), each question class is associated to the same list: the list for which the QA system obtains the best global success rate. In the second one (methodology-2), the best per-class list is selected, for each of the most frequent question class found througho"
toney-etal-2008-evaluation,walker-etal-2000-evaluation,0,\N,Missing
vasilescu-etal-2010-role,toney-etal-2008-evaluation,1,\N,Missing
vasilescu-etal-2010-role,C90-2044,0,\N,Missing
vasilescu-etal-2010-role,J06-3004,0,\N,Missing
vasilescu-etal-2010-role,J99-4003,0,\N,Missing
vasilescu-etal-2010-role,rosset-petel-2006-ritel,1,\N,Missing
W03-2802,antoine-etal-2002-predictive,1,0.891043,"Missing"
W03-2802,bonneau-maynard-etal-2000-predictive,0,0.0581338,"Missing"
W03-2802,J96-2004,0,0.137079,"Missing"
W03-2802,devillers-etal-2002-annotations,1,0.918558,"re of dialog. Consequently to these shortcomings, researchers are often unable to provide principled design and system capabilities for technology transfer. In other research areas, such as speech recognition and information retrieval, common reference tasks have been highly effective in sharing research costs and efforts. A similar development is highly needed in the dialog community. In this contribution which addresses only a part of the SLDS evaluation problem, a paradigm for evaluating the context-sensitive understanding capability of any spoken language dialog system is proposed. PEACE (Devillers et al., 2002a) described in section 3, is based on test sets extracted from real corpora, and has three main aspects: it is generic, contextual and it offers diagnostic capabilities. Here genericity is envisaged in a context of information dialogs access. The diagnostic aspect is important in order to determine the different qualities of the systems under test. The contextual aspect of evaluation is a crucial point since dialog is dynamic by nature. We propose to simulate/synthesize the contextual information. The PEACE paradigm will be tested in the French Technolangue MEDIA project and will serve as bas"
W03-2802,geoffrois-etal-2000-transcribing,0,0.0141776,"xtracted from real corpus. For dialog system diagnosis, it is also crucial to build test sets labeled with the linguistic phenomena and dialogic functions. Thus, the capabilities of system’s contextual understanding can be assessed for the main linguistic and dialogic difficulties such as, for instance, anaphora or ellipsis resolution. 3.1.4 A data structuring method Two types of units, one for literal understanding (LU), the other for contextual understanding (CU) are defined. The format of the annotated data will be adapted to language resource standard annotations implemented in XML, e.g. (Geoffrois et al., 2000), (Ide and Romary, 2002). Each unit is extracted from a real dialog corpus. LU units are composed of the user query, the corresponding audio signal, an automatic transcription obtained with a recognition system, and finally the literal semantic representation of the utterance (see Figure 1). CU units are composed of Context paraphrase (LU) AVR User query (LU) AVR (CU) AVR je voudrais un hˆotel 4 e´ toiles dans le neuvi`eme I would like a 4 category hotel in the ninth (+, argument, hotel) (+, district, 9) (+, category, 4) la mˆeme cat´egorie dans un autre arrondissement the same category in ano"
W03-2802,H92-1003,0,0.0449057,"Missing"
W03-2802,H94-1022,0,0.0374573,"test set) and finally how to restrict and organize the language phenomena used in the test set. 3 The PEACE paradigm We first describe the paradigm and relate preliminary experiments with PEACE. This paradigm which is as basement for the MEDIA project will be refined by all the partners and use for an evaluation campaign between seven systems of industrial and academic sites. 3.1 Description The PEACE paradigm relies on the idea that for database querying tasks, it is possible to define a common semantic representation, onto which all the systems are able to convert their own representation (Moore, 1994). The paradigm based on data extracted from real corpus, includes both literal and contextual understanding test sets. More precisely, it provides:  the definition of a semantic representation (see 3.1.1),  the definition of a model for dialogic contexts (see 3.1.2),  the definition and typology of linguistic phenomena and dialogic functions used to selectively diagnoze the system language capabilities (anaphora resolution, constraints relaxation, etc.) (see 3.1.3),  a data structuring method. The format of the annotated data will be adapted to language resource standard annotations implem"
W03-2802,P01-1066,0,\N,Missing
W03-2802,J98-3008,0,\N,Missing
W11-0411,W10-1804,0,0.10543,"ation realized within the Sonar project, where Named Entity are clearly simpler. They follow the MUC Named Entity definition with the subtypes as proposed by ACE. The agreement computed over the Sonar Dutch corpus ranges from 0.91 to 0.97 (kappa values) depending of the emphasized elements (span, main type, subtype, etc.). 3 3.1 Taxonomy Guidelines production Having in mind the objective of building a fact database through the extraction of named entities from texts, we defined a richer taxonomy than those used in other information extraction works. 93 Following (Bonneau-Maynard et al., 2005; Alex et al., 2010), the annotation guidelines were first written from December 2009 to May 2010 by three researchers managing the manual annotation campaign. During guidelines production, we evaluated the feasibility of this specific annotation task and the usefulness of the guidelines by annotating a small part of the target corpus. Then, these guidelines were delivered to the annotators. They consist of a description of the objects to annotate, general annotation rules and principles, and more than 250 prototypical and real examples extracted from the corpus (Rosset et al., 2010). Rules are important to set t"
W11-0411,J08-4004,0,0.16077,"rics Because human annotation is an interpretation process (Leech, 1997), there is no “truth” to rely on. It is therefore impossible to really evaluate the validity of an annotation. All we can and should do is to evaluate its reliability, i.e. the consistency of the annotation across annotators, which is achieved through computation of the inter-annotator agreement (IAA). The best way to compute it is to use one of the Kappa family coefficients, namely Cohen’s Kappa (Cohen, 1960) or Scott’s Pi (Scott, 1955), also known as Carletta’s Kappa (Carletta, 1996),11 as they take chance into account (Artstein and Poesio, 2008). However, these coefficients imply a comparison with a “random baseline” to establish whether the correlation between annotations is statistically significant. This baseline depends on the number of “markables”, i.e. all the units that could be annotated. In the case of named entities, as in many others, this “random baseline” is known to be difficult—if not impossible—to identify (Alex et al., 2010). We wish to analyze this in more detail, to see how we could actually compute these coefficients and what information it would give us about the annotation. Markables Annotators Both institutes F"
W11-0411,bick-2004-named,0,0.312622,"undheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; Galliano et al., 2009). 92 Proceedings of the Fifth Law Workshop (LAW V), pages 92–100, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics Specific entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific domains (Rosset et al., 2007), “email address” or “phone number” to process electronic messages (Maynard et al., 2001). Numeric types are also often described and used. They include “date”, “time”, and “amount” types (“amount” generally covers money and percentage). In specific domains, entities such as"
W11-0411,J96-2004,0,0.23313,"inally, we merged the results with the anno97 5.2 Metrics Because human annotation is an interpretation process (Leech, 1997), there is no “truth” to rely on. It is therefore impossible to really evaluate the validity of an annotation. All we can and should do is to evaluate its reliability, i.e. the consistency of the annotation across annotators, which is achieved through computation of the inter-annotator agreement (IAA). The best way to compute it is to use one of the Kappa family coefficients, namely Cohen’s Kappa (Cohen, 1960) or Scott’s Pi (Scott, 1955), also known as Carletta’s Kappa (Carletta, 1996),11 as they take chance into account (Artstein and Poesio, 2008). However, these coefficients imply a comparison with a “random baseline” to establish whether the correlation between annotations is statistically significant. This baseline depends on the number of “markables”, i.e. all the units that could be annotated. In the case of named entities, as in many others, this “random baseline” is known to be difficult—if not impossible—to identify (Alex et al., 2010). We wish to analyze this in more detail, to see how we could actually compute these coefficients and what information it would give"
W11-0411,W09-3002,0,0.0410686,"we presented an extension of the traditional named entity categories to new types (functions, civilizations) and new coverage (expressions built over a substantive). We created guidelines that were used by graduate annotators to annotate a broadcast news corpus. The organizers also annotated a small part of the corpus to build a mini reference corpus. We evaluated the human annotations with our mini-reference corpus: the actual computed κ is between 0.71 et 0.85 which, given the complexity of the task, seems to indicate a good annotation quality. Our results are consistent with other studies (Dandapat et al., 2009) in demonstrating that human annotators’ training is a key asset to produce quality annotations. 99 We also saw that guidelines are never fixed, but evolve all along the annotation process due to feedback between annotators and organizers; the relationship between guidelines producers and human annotators evolved from “parent” to “peer” (Akrich and Boullier, 1991). This evolution was observed during the annotation development, beyond our expectations. These data have been used for the 2011 Quaero Named Entity evaluation campaign. Extensions and revisions are planned. Our first goal is to add a"
W11-0411,desmet-hoste-2010-towards,0,0.0216477,"rent classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be annotated or the trigger “Mr” too?) and (3) annotating metonymies (“France” can be a sports team, a country, etc.). In the ACE Named Entity task (Doddington et al., 2004), a complex task, the obtained inter-annotator agreement was 0.86 in 2002 and 0.88 in 2003. Some tasks obtain better agreement. Desmet and Hoste (2010) described the Named Entity annotation realized within the Sonar project, where Named Entity are clearly simpler. They follow the MUC Named Entity definition with the subtypes as proposed by ACE. The agreement computed over the Sonar Dutch corpus ranges from 0.91 to 0.97 (kappa values) depending of the emphasized elements (span, main type, subtype, etc.). 3 3.1 Taxonomy Guidelines production Having in mind the objective of building a fact database through the extraction of named entities from texts, we defined a richer taxonomy than those used in other information extraction works. 93 Followin"
W11-0411,doddington-etal-2004-automatic,0,0.157033,", some aspects are known to lead to difficulties in obtaining coherence in the manual annotation process (Ehrmann, 2008; Fort et al., 2009). Three different classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be annotated or the trigger “Mr” too?) and (3) annotating metonymies (“France” can be a sports team, a country, etc.). In the ACE Named Entity task (Doddington et al., 2004), a complex task, the obtained inter-annotator agreement was 0.86 in 2002 and 0.88 in 2003. Some tasks obtain better agreement. Desmet and Hoste (2010) described the Named Entity annotation realized within the Sonar project, where Named Entity are clearly simpler. They follow the MUC Named Entity definition with the subtypes as proposed by ACE. The agreement computed over the Sonar Dutch corpus ranges from 0.91 to 0.97 (kappa values) depending of the emphasized elements (span, main type, subtype, etc.). 3 3.1 Taxonomy Guidelines production Having in mind the objective of building a fact databa"
W11-0411,C02-1130,0,0.0613942,"cations and organizations), we decided to extend the coverage of our campaign to new types of entities and to broaden their main parts-of-speech from proper Related work Named entity definitions Named Entity recognition was first defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; Galliano et al., 2009). 92 Proceedings of the Fifth Law Workshop (LAW V), pages 92–100, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics Specific entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific do"
W11-0411,W09-3025,1,0.778972,"ed. They include “date”, “time”, and “amount” types (“amount” generally covers money and percentage). In specific domains, entities such as gene, protein, are also handled (Ohta, 2002), and campaigns are organized for gene detection (Yeh et al., 2005). At the same time, extensions of named entities have been proposed: (Sekine, 2004) defined a complete hierarchy of named entities containing about 200 types. 2.2 Named Entities and Annotation As for any other kind of annotation, some aspects are known to lead to difficulties in obtaining coherence in the manual annotation process (Ehrmann, 2008; Fort et al., 2009). Three different classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be annotated or the trigger “Mr” too?) and (3) annotating metonymies (“France” can be a sports team, a country, etc.). In the ACE Named Entity task (Doddington et al., 2004), a complex task, the obtained inter-annotator agreement was 0.86 in 2002 and 0.88 in 2003. Some tasks obtain bette"
W11-0411,C96-1079,0,0.902879,"luation campaign on named entity extraction aiming at building a fact database in the news domain, the first step being to define what kind of entities are needed. This campaign focused on broadcast news corpora in French. While traditional named entities include three major classes (persons, locations and organizations), we decided to extend the coverage of our campaign to new types of entities and to broaden their main parts-of-speech from proper Related work Named entity definitions Named Entity recognition was first defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; G"
W11-0411,I05-1058,0,0.0948165,"roaden their main parts-of-speech from proper Related work Named entity definitions Named Entity recognition was first defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; Galliano et al., 2009). 92 Proceedings of the Fifth Law Workshop (LAW V), pages 92–100, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics Specific entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific domains (Rosset et al., 2007), “email address” or “phone number” to process electronic messages (Maynard"
W11-0411,sekine-nobata-2004-definition,0,0.926265,"entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific domains (Rosset et al., 2007), “email address” or “phone number” to process electronic messages (Maynard et al., 2001). Numeric types are also often described and used. They include “date”, “time”, and “amount” types (“amount” generally covers money and percentage). In specific domains, entities such as gene, protein, are also handled (Ohta, 2002), and campaigns are organized for gene detection (Yeh et al., 2005). At the same time, extensions of named entities have been proposed: (Sekine, 2004) defined a complete hierarchy of named entities containing about 200 types. 2.2 Named Entities and Annotation As for any other kind of annotation, some aspects are known to lead to difficulties in obtaining coherence in the manual annotation process (Ehrmann, 2008; Fort et al., 2009). Three different classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be a"
W11-0411,vilnat-etal-2010-passage,0,\N,Missing
W12-0512,O08-3002,0,0.0241167,"Missing"
W12-0512,P07-1098,0,0.0237748,"nd r the rank. The equality bonus, found empirically, is given for each systems pair. The value is 3 if the two answers are equal, 2 if an answer is included in the other and 1 otherwise. When an answer is found by two or more systems, the higher confidence score is kept. The result of this method is that the answers extracted by more than one system are favored. An answer found by only one system, even with a very high confidence score, may be downgraded. 6 Machine-learning-based method for answer re-ranking To solve a re-ranking problem, machine learning approaches can be used (for example (Moschitti et al., 2007)). But in most of the cases, the objective is to re-rank answers provided by one system, that means to re-rank multiple hypotheses from one system. In our case, we want to re-rank multiple answers from different systems. We decided to use an SVM-based approach, namely SVMrank (Joachims, 2006), which is well adapted to our problem. An important aspect is then to choose the pertinent features for such a task. Our objective is to consider robust enough features to deal with different systems’ answers without introducing biases. Two classes of characteristic should be able to give a useful represe"
W12-0512,quintard-etal-2010-question,1,0.888898,"Missing"
W12-0512,toney-etal-2008-evaluation,1,0.833409,"verage distance between the answer and each of the question words in the passage. Other criteria are the passage rank given by using results of the passage analysis, the question category, i.e. definition, characterization of an entity, verb modifier or verb complement, etc. 3.2 The RITEL systems 3.3 General overview The RITEL system (see Figure 1) which we used in these experiments is fully described in (Bernard et al., 2009). This system has been developed within the framework of the Ritel project which aimed at building a human-machine dialogue system for question-answering in open domain (Toney et al., 2008). The same multilevel analysis is carried out on both queries and documents. The objective of this analysis is to find the bits of information that may be of use for search and extraction, called pertinent information chunks. These can be of different categories: named entities, linguistic entities (e.g., verbs, prepositions), or specific entities (e.g., scores). All words that do not fall into such chunks are automatically grouped into chunks via a longest-match strategy. The analysis is hierarchical, resulting in a set of trees. Both answers and important elements of the questions are suppos"
W12-3606,bick-2004-named,0,0.0325063,"(CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of named entities containing about 200 types. Numerous investigations concern named entities in historical data (Miller et al., 2000; Crane and Jones, 2006; Proceedings of the 6th Linguistic Annotation Workshop, pages 40–48, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics Byrne, 2007; Grover et al., 2008). In most cases, the definition of named entity follows the classical definition. Nevertheless, in some cases, new categories were added. For example, the Virginia Banks project (Crane and Jones, 20"
W12-3606,doddington-etal-2004-automatic,0,0.0405426,"self (Section 4.1) and the annotation results (Section 4.2). 2 1 Introduction Named Entity Recognition (NER), and its evaluation methods, constitute an active field of research. NER can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location"
W12-3606,C02-1130,0,0.0265156,"et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of named entities containing about 200 types. Numerous investigations concern named entities in historical data (Miller et al., 2000; Crane and Jones, 2006; Proceedings of the 6th Linguistic Annotation Workshop, pages 40–48, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics Byrne, 2007; Grover et al., 2008). In most cases, the de"
W12-3606,galibert-etal-2010-named,1,0.859086,"can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub"
W12-3606,I11-1058,1,0.836346,"tities on spoken data and the resulting corpus. The training part of the corpus is only composed of broadcast news data while the test corpus is composed of both broadcast news and broadcast conversations data. In order to build a minireference corpus for this annotation campaign (a “gold” corpus), we randomly extracted a sub-corpus from the training one. This sub-corpus was annotated by 6 different annotators following a 4-step procedure. Table 1 gives statistics about training, test and gold corpora. These corpora (“BN” in the remainder of the paper) has been used in an evaluation campaign (Galibert et al., 2011). PP PP Data Training PP Inf. PP P # shows 188 # lines 43,289 # tokens 1,291,225 # entity types 113,885 # distinct types 41 # components 146,405 # distinct comp. 29 Test Gold 18 5,637 108,010 5,523 32 8,902 22 398 11,532 1,161 29 1,778 22 Table 1: Statistics on the annotated BN corpora Structured Named Entities in Old Newspapers We performed the same annotations on a corpus composed of OCRed press archives, henceforth the Old Press (OP) corpus. Human annotation was subcontracted to the same team of annotators as for the BN corpus, thus facilitating the consistency of annotations across corpora"
W12-3606,galibert-etal-2012-extended,1,0.588515,"née e t a p p a r e i l l e r a e n s u i t e nour &lt; loc.adm.town &gt; Toulon. &lt; / loc.adm.town &gt; Figure 4: Example annotated text block Component noisy-entities. When a character recognition error involves an entity boundary, a segmentation error occurs, either between an entity and other tokens, or between several entities and possibly other tokens. To allow the annotators to annotate the entity in that character span, we defined a new component noisy-entities which indicates that an entity is present in the noisy span of characters. A complete description of these adaptations can be found in (Galibert et al., 2012). 3.3 Global corpus extraction Unannotated sub-corpus Global annotated corpus Scientist 1 Scientist 2 Scientist 4 Scientist 3 Adjudication extraction Inter-Annotator Agreement To evaluate the manual annotations of the annotation team (“Global annotated corpus” in Figure 5), we built a mini reference corpus by selecting 255 blocks from the training corpus. We followed the same procedure as the one used for the BN corpus, as illustrated in Figure 5: Adjudication Institute 1 Institute 2 Adjudication Annotated sub-corpus IAA Institutes Adjudication IAA Mini-reference 1. The corpus is annotated ind"
W12-3606,C96-1079,0,0.645168,"he point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of"
W12-3606,W11-0411,1,0.930226,", as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and"
W12-3606,grover-etal-2008-named,0,0.179562,"corpus of old newspapers. This comparison is performed at two levels: the annotation process itself (Section 4.1) and the annotation results (Section 4.2). 2 1 Introduction Named Entity Recognition (NER), and its evaluation methods, constitute an active field of research. NER can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since"
W12-3606,A00-1044,0,0.0706869,"ora: the pre-existing broadcast news corpus and this new corpus of old newspapers. This comparison is performed at two levels: the annotation process itself (Section 4.1) and the annotation results (Section 4.2). 2 1 Introduction Named Entity Recognition (NER), and its evaluation methods, constitute an active field of research. NER can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described"
W12-3606,sekine-nobata-2004-definition,0,0.07436,"992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of named entities containing about 200 types. Numerous investigations concern named entities in historical data (Miller et al., 2000; Crane and Jones, 2006; Proceedings of the 6th Linguistic Annotation Workshop, pages 40–48, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics Byrne, 2007; Grover et al., 2008). In most cases, the definition of named entity follows the classical definition. Nevertheless, in some cases, new categories were added. For example, the Virginia Banks project (Crane and Jones, 2006) added categories"
W13-2321,J93-2004,0,0.0481147,"n all cases, both for speed and for accuracy, and that the subjective assessment of the annotators does not always match the actual benefits measured in the annotation outcome. 1 • can a system trained on data from one specific domain be useful on data from another domain in a pre-annotation task? Introduction • does this pre-annotation help human annotators or bias them? Human corpus annotation is a difficult, timeconsuming, and hence costly process. This motivates research into methods which reduce this cost (Leech, 1997). One such method consists of automatically pre-annotating the corpus (Marcus et al., 1993; Dandapat et al., 2009) using an existing system, e.g., a POS tagger, syntactic parser, named entity recognizer, according to the task for which the annotations aim to provide a gold standard. The pre-annotations are then corrected by the human annotators. The underlying hypothesis is that this should reduce annotation time while possibly at the same time increasing annotation completeness and consistency. We study here corpus pre-annotation in a specific setting, out-of-domain named entity annotation, in which we examine specific questions that we present below. We produced corpora and annot"
W13-2321,W09-3002,0,0.126125,"speed and for accuracy, and that the subjective assessment of the annotators does not always match the actual benefits measured in the annotation outcome. 1 • can a system trained on data from one specific domain be useful on data from another domain in a pre-annotation task? Introduction • does this pre-annotation help human annotators or bias them? Human corpus annotation is a difficult, timeconsuming, and hence costly process. This motivates research into methods which reduce this cost (Leech, 1997). One such method consists of automatically pre-annotating the corpus (Marcus et al., 1993; Dandapat et al., 2009) using an existing system, e.g., a POS tagger, syntactic parser, named entity recognizer, according to the task for which the annotations aim to provide a gold standard. The pre-annotations are then corrected by the human annotators. The underlying hypothesis is that this should reduce annotation time while possibly at the same time increasing annotation completeness and consistency. We study here corpus pre-annotation in a specific setting, out-of-domain named entity annotation, in which we examine specific questions that we present below. We produced corpora and annotation guidelines for nam"
W13-2321,I11-1142,1,0.822824,"Leixa , Olivier Galibertγ , Pierre Zweigenbaumα . α LIMSI–CNRS β Universit´e Paris-Sud γ LNE δ LPP, Universit´e Sorbonne Nouvelle  ELDA {rosset,grouin,lavergne,ben-jannet,pz}@limsi.fr leixa@elda.org, olivier.galibert@lne.fr 2011),1 and which we used in contrastive studies of news texts in French (Rosset et al., 2012). We want to rely on the same named entity definitions for studies on two types of data we did not cover: parliament debates (Europarl corpus) and regional, contemporary written news (L’Est R´epublicain), both in French. To help the annotation process we could reuse our system (Dinarelli and Rosset, 2011), but needed first to examine whether a system trained on one type of text (our first Broadcast News data) could be used to produce a useful pre-annotation for different types of text (our two corpora). We therefore set up the present study in which we aim to answer the following questions linked to this point and to related annotation issues: Abstract Automatic pre-annotation is often used to improve human annotation speed and accuracy. We address here out-of-domain named entity annotation, and examine whether automatic pre-annotation is still beneficial in this setting. Our study design incl"
W13-2321,W10-1807,0,0.523869,"want to answer these questions taking into account these two levels. We first examine related work on pre-annotation (Section 2), then present our corpora and annotation task (Section 3). We describe and discuss experiments in Section 4, and make subjective and 1 Corpora, guidelines and tools are available through ELRA under references ELRA-S0349 and ELRA-W0073. 168 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 168–177, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics the pre-annotation (Rehbein et al., 2009; Fort and Sagot, 2010; South et al., 2011). In their framesemantic argument structure annotation, Rehbein et al. (2009) addressed a specific question considering a two-level annotation scheme: is the preannotation of frame assignment (low-level annotation) useful for annotating semantic roles (highlevel annotation)? Although for the low-level annotation task they observed a significant difference in quality of final annotation, for the high-level task they found no difference. Most of these studies used a pre-annotation system trained on the same kind of data as those which were to be annotated manually. Neverthel"
W13-2321,C12-1055,1,0.796018,"ould you say that one has been easier to annotate than the other? The Europarl corpus is more difficult to annotate in the sense that the existing types and components do not always match the realities found in the corpus, either because their definitions 4. Concerning the annotation manual, are there topics that you would like to change, or correct? In the same way, which named entities caused you the most difficulties to deal with? All 8 annotators answered these questions. We summarize below what we found in their answers. 2 This feeling is supported by results about ambiguity presented in Fort et al. (2012). 172 there has been a difference between novice and expert annotators. Both groups agreed on the same difficulties, pointed at the same errors, and criticized the same entities, saying that their definitions needed to be clarified. cannot apply exactly, or because the required types and components are missing (mainly for frequencies: “five times per year”). The other half of the annotators did not feel any specific difficulties in annotating one corpus or the other. According to them, both corpora are the same in terms of register and sentence structure. 6 In this section we provide results o"
W13-2321,W09-3003,0,0.0351183,"es and components), we want to answer these questions taking into account these two levels. We first examine related work on pre-annotation (Section 2), then present our corpora and annotation task (Section 3). We describe and discuss experiments in Section 4, and make subjective and 1 Corpora, guidelines and tools are available through ELRA under references ELRA-S0349 and ELRA-W0073. 168 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 168–177, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics the pre-annotation (Rehbein et al., 2009; Fort and Sagot, 2010; South et al., 2011). In their framesemantic argument structure annotation, Rehbein et al. (2009) addressed a specific question considering a two-level annotation scheme: is the preannotation of frame assignment (low-level annotation) useful for annotating semantic roles (highlevel annotation)? Although for the low-level annotation task they observed a significant difference in quality of final annotation, for the high-level task they found no difference. Most of these studies used a pre-annotation system trained on the same kind of data as those which were to be annotat"
W13-2321,I11-1058,1,0.890374,"Missing"
W13-2321,W07-1516,0,0.0591516,"Missing"
W13-2321,W11-0411,1,0.821738,"eement and that full pre-annotation yields the best result. We observe that, as expected, pre-annotation leads human annotators to obtain higher consistency. Table 2: F-measure and Slot Error Rate achieved by the automatic system on each kind of annotation and on in-domain broadcast data We also computed inter-annotator agreement (IAA) for each corpus considering two groups of annotators, experts and novices. We consider that the inter-annotator agreement is somewhere between the F-measure and the standard IAA considering as markables all the units annotated by at least one of the annotators (Grouin et al., 2011). We computed Scott’s Pi (Scott, 1955), and Cohen’s Kappa (Cohen, 1960). The former considers 5 Subjective assessment An important piece of information in any annotation campaign is the feelings of the annotators about the task. This can give interesting clues about the expected quality of their work and on the usefulness of the pre-annotation step. We asked the annotators a few questions concerning several features of this project, such as the annotation 171 1 5.1.1 Press: Cohen&apos;s kappa Press: F-measure Europarl: Cohen&apos;s kappa Europarl: F-measure 0.9 Most of the annotators preferred the corpo"
W13-2321,W12-3606,1,0.874172,"Missing"
W17-2343,L16-1505,1,0.831443,"ght to have specific patterns (e.g. recurrent ngrams, question roots or domain semantic labels), which make it possible to formalise rules. In our system, rules are formalised based on the semantic annotation of questions.1 For example, a rule processing the combination of S YMPTOM and F REQUENCY labels interprets the input as a query on the frequency of a symptom. Accordingly, the VP agent will answer with a fixed type 1 4 Data sources and preparation 4.1 The semantic labels we use encode domain data (DISEASE), miscellanea (e.g. time or quantity) and question type or tense: e.g. Q PASTYESNO (Campillos et al., 2016). Data sources We collected French language questions from books aimed at medical consultation and clinical examination (Bates and Bickley, 2014; Epstein et al., 2015), as well as resources for medical translation (Coud´e et al., 2011; Pastore, 2015).2 We also collected questions from 25 transcribed doctor-patient interactions performed by human standardized patients (i.e. actors simulating medical consultations). 4.2 Task description Example of questions Do you cough every day ? Are your parents still alive ? of reply instantiated with the corresponding data in the record. We hypothesize that"
W17-2343,W16-2922,0,0.0134934,"utational complexity). As our data were scarce, we used word vectors pretrained in a large domain corpus from the European Medicines Agency,3 which amounts to more than 16 million tokens after tokenization. Several parameter values were tested: window size of 2, 4, 6, 8 and 10, vector dimension of 50, 100 and 300, use of 3-grams or 3-character-grams, number of negative samples (5, 10 or 20), learning rate (0.1 and 0.05) and sub-sampling threshold (1e-3 and 1e-4). We only tested the skip-gram architecture since it has 3 http://opus.lingfil.uu.se/EMEA.php/ been observed to yield better results (Chiu et al., 2016). The minimum word count was fixed to 1, given the scarcity of our labelled data. We did not use semantic annotation to create word vectors. 6 Results and discussion Table 3 breaks down our results (reported as F1score) in the training set (top of the table) with different parameter combinations and non-neural classifiers. The weighted average F1-score was computed based upon both F1-scores of classifying RBPS and OPS types of questions. The best combinations of parameters found in the training set were applied to the test set; their results are placed at the bottom of the table. Note that a b"
W17-2343,J07-1005,0,0.133932,"assifier achieved promising results without it. 1 Introduction Previous work on question classification has mostly been undertaken within the framework of question answering (hereafter, QA) tasks, where classification is but one step of the overall process. Other steps are linguistic/semantic question processing, answer retrieval and generation by integrating data; indeed, these make QA a different task to that of standard information retrieval. Biomedical QA (Zweigenbaum, 2003) has mostly focused on questions that aim to obtain knowledge to help diagnose or cure diseases, by medical doctors (Demner-Fushman and Lin, 2007) or by patients (Roberts et al., 2014b), or to obtain knowledge on biology (Neves and Leser, 2015). Clinical questions to obtain data from patient records have also been addressed (Patrick and Li, 2012). Herein, we address a question classification task from a different perspective to existing research. Our task is set in a simulated consultation scenario where a user (a medical doctor trainee) asks questions to a virtual patient (hereafter, VP) (Jaffe et al., 2015; Talbot et al., 2016) during the anamnesis stage, i.e. the interview to the patient to obtain diagnostic information. Question typ"
W17-2343,W01-1203,0,0.149194,"be handled by a different strategy. What is needed in this context is a way to determine whether a given question should be transmitted to the rule-based system or to a fallback strategy. This is the goal of the present research, which is tackled as a binary classification task. Figure 1 is a schema of the processing steps we address in this work (note that we do not represent other stages such as dialogue management). Guiding the processing of input questions is a common step in QA systems. Questions may be filtered through an upfront classifier based on machine-learning techniques, parsing (Hermjakob, 2001), regular expressions and syntactic rules, or hybrid methods (Lally et al., 2012). To achieve that, a question analysis process might precede, which may involve detecting lexical answer types, question targets or the question focus. Our VP system relies on named entity recognition and domain semantic labels in the question analysis. The results we report seem to show that leveraging this semantic information was beneficial for the classification step. We also tested a neural method without the semantic information, and indeed did not achieve the best performance (despite having promising resul"
W17-2343,W15-0611,0,0.413282,"03) has mostly focused on questions that aim to obtain knowledge to help diagnose or cure diseases, by medical doctors (Demner-Fushman and Lin, 2007) or by patients (Roberts et al., 2014b), or to obtain knowledge on biology (Neves and Leser, 2015). Clinical questions to obtain data from patient records have also been addressed (Patrick and Li, 2012). Herein, we address a question classification task from a different perspective to existing research. Our task is set in a simulated consultation scenario where a user (a medical doctor trainee) asks questions to a virtual patient (hereafter, VP) (Jaffe et al., 2015; Talbot et al., 2016) during the anamnesis stage, i.e. the interview to the patient to obtain diagnostic information. Question types need accurate classification to search the data in the clinical record. In this context, question classification has aimed at identifying detailed question types (Jaffe et al., 2015). In contrast, we consider a situation where we already have a rule-based question analysis system that classifies questions according to the semantic function or content (in order to restrict the search for data in the patient record and reply coherently). This strategy works well a"
W17-2343,D14-1181,0,0.00501679,"presentations of words—allow the prediction of a word according to the surrounding context, and vice-versa. New research questions are being raised with regard to current architectures (Mikolov et al., 2013; Pennington et al., 2014; Goldberg, 2016), parameters (e.g. vector dimension or window size), hyperparameters or the effect of input data. The latest models include subword information in word embeddings, encoding both n-grams of characters and the standard occurrence of words (Bojanowski et al., 2016). There is a growing interest in research on word embeddings for sentence classification (Kim, 2014; Zhang et al., 2016) and question classification (Mou et al., 2015). However, a far as we know, a neural network classifier using subword information has not yet been tested on a medical question classification task. This is another point we explore herein. 3 We classify questions into those that a rule-based dialogue system can process, and those needing a supplementary method. Table 1 gives examples of these two classes of questions, and shows the semantic annotation performed in our task. A rulebased system is to be favoured to maximize precision, but developing rules for any question type"
W17-2343,W06-1303,0,0.0482284,"antic level (Slaughter et al., 2006; Roberts and Demner-Fushman, 2016). We refer to (Athenikos and Han, 2010; Neves and Leser, 2015), respectively, for state-of-the-art reviews of QA for biomedicine and biology. Questions are generally classified into Yes/No, Factoid/List and Definition/summary. Questions to a virtual patient have been addressed by mapping the user input to a set of predefined questions (Jaffe et al., 2015), as is done in a large subset of recent general-domain QA work which queries lists of frequently asked questions (FAQs) and returns their associated predetermined answers (Leuski et al., 2006; Nakov et al., 2016). Our setting is different in two ways: first, we do not rely on a FAQ but instead generate answers based on the question and on the contents of the virtual patient’s record; second, we already perform fine-grained question classification with a rule-based system (Campillos et al., 2015), and aim to determine whether a given question should be referred to this rule-based strategy or deserves to be handled by a fallback strategy. 2.2 Approaches Across the mentioned tasks, machine-learning methods for classifying questions range from hierarchical classifiers (Li and Roth, 20"
W17-2343,C02-1150,0,0.235043,"ki et al., 2006; Nakov et al., 2016). Our setting is different in two ways: first, we do not rely on a FAQ but instead generate answers based on the question and on the contents of the virtual patient’s record; second, we already perform fine-grained question classification with a rule-based system (Campillos et al., 2015), and aim to determine whether a given question should be referred to this rule-based strategy or deserves to be handled by a fallback strategy. 2.2 Approaches Across the mentioned tasks, machine-learning methods for classifying questions range from hierarchical classifiers (Li and Roth, 2002) to linear support vector machines (SVM, hereafter) (Zhang and Lee, 2003). The benefit of using semantic features to improve question classification varies across experiments. For example, (Roberts et al., 2014a) reported improvements when classifying a dataset of consumer-related topics. They used an SVM with combinations of features including semantic information, namely Unified MediR (Bodenreider, 2004) Secal Language System mantic Types and Concept Unique Identifiers. For their part, (Patrick and Li, 2012) used SNOMED categories. They reported improvements in classification through models"
W17-2343,J07-1004,0,0.0238553,"Missing"
W17-2343,D15-1279,0,0.0260311,"ing to the surrounding context, and vice-versa. New research questions are being raised with regard to current architectures (Mikolov et al., 2013; Pennington et al., 2014; Goldberg, 2016), parameters (e.g. vector dimension or window size), hyperparameters or the effect of input data. The latest models include subword information in word embeddings, encoding both n-grams of characters and the standard occurrence of words (Bojanowski et al., 2016). There is a growing interest in research on word embeddings for sentence classification (Kim, 2014; Zhang et al., 2016) and question classification (Mou et al., 2015). However, a far as we know, a neural network classifier using subword information has not yet been tested on a medical question classification task. This is another point we explore herein. 3 We classify questions into those that a rule-based dialogue system can process, and those needing a supplementary method. Table 1 gives examples of these two classes of questions, and shows the semantic annotation performed in our task. A rulebased system is to be favoured to maximize precision, but developing rules for any question type is not feasible in the long term. Thus, we need a classifier to dis"
W17-2343,S16-1083,0,0.0725338,"Missing"
W17-2343,D14-1162,0,0.0816632,"type of the semantic information used in each task might explain these results. The impact of using semantic features is a point we explore in the present work in the context of questions to a virtual patient. Neural network representations and classifiers are more and more applied to natural language processing (Bengio et al., 2003; Collobert et al., 2011). Word embeddings—i.e. vector representations of words—allow the prediction of a word according to the surrounding context, and vice-versa. New research questions are being raised with regard to current architectures (Mikolov et al., 2013; Pennington et al., 2014; Goldberg, 2016), parameters (e.g. vector dimension or window size), hyperparameters or the effect of input data. The latest models include subword information in word embeddings, encoding both n-grams of characters and the standard occurrence of words (Bojanowski et al., 2016). There is a growing interest in research on word embeddings for sentence classification (Kim, 2014; Zhang et al., 2016) and question classification (Mou et al., 2015). However, a far as we know, a neural network classifier using subword information has not yet been tested on a medical question classification task. This"
W17-2343,W14-3405,0,0.341942,". 1 Introduction Previous work on question classification has mostly been undertaken within the framework of question answering (hereafter, QA) tasks, where classification is but one step of the overall process. Other steps are linguistic/semantic question processing, answer retrieval and generation by integrating data; indeed, these make QA a different task to that of standard information retrieval. Biomedical QA (Zweigenbaum, 2003) has mostly focused on questions that aim to obtain knowledge to help diagnose or cure diseases, by medical doctors (Demner-Fushman and Lin, 2007) or by patients (Roberts et al., 2014b), or to obtain knowledge on biology (Neves and Leser, 2015). Clinical questions to obtain data from patient records have also been addressed (Patrick and Li, 2012). Herein, we address a question classification task from a different perspective to existing research. Our task is set in a simulated consultation scenario where a user (a medical doctor trainee) asks questions to a virtual patient (hereafter, VP) (Jaffe et al., 2015; Talbot et al., 2016) during the anamnesis stage, i.e. the interview to the patient to obtain diagnostic information. Question types need accurate classification to se"
W17-2343,W16-6106,0,0.0267548,"ng rate; SAMP: sampling threshold Table 4: Results of the best tested models (neural approach) fier) and also used a neural model to classify questions. However, our results agree with the observation that restricted-domain QA is less affected by data-intensive methods, but depend on refined language processing methods (Moll´a and Vicedo, 2007)—in this type of system, accurate semantic annotation. On the other hand, the neural method seems promising in this kind of classification task, and how to use domain semantic information with it requires further exploration, in line with current works (Yu et al., 2016). We also need to pretrain vectors on domain data of different nature (e.g. clinical records) to confirm our results. Finally, other methods for computing vector representations of sentences deserve to be explored. 7 Conclusions For the task of optimizing question processing in a VP natural language system, we reported the improvement of using the semantic information in the question analysis step as a feature for question classification. This is likely due to the idiosyncrasy of our task, where the dialogue system makes use of semantic rules for processing input questions. We are nonetheless"
W17-2343,N16-1178,0,0.0131972,"ns of words—allow the prediction of a word according to the surrounding context, and vice-versa. New research questions are being raised with regard to current architectures (Mikolov et al., 2013; Pennington et al., 2014; Goldberg, 2016), parameters (e.g. vector dimension or window size), hyperparameters or the effect of input data. The latest models include subword information in word embeddings, encoding both n-grams of characters and the standard occurrence of words (Bojanowski et al., 2016). There is a growing interest in research on word embeddings for sentence classification (Kim, 2014; Zhang et al., 2016) and question classification (Mou et al., 2015). However, a far as we know, a neural network classifier using subword information has not yet been tested on a medical question classification task. This is another point we explore herein. 3 We classify questions into those that a rule-based dialogue system can process, and those needing a supplementary method. Table 1 gives examples of these two classes of questions, and shows the semantic annotation performed in our task. A rulebased system is to be favoured to maximize precision, but developing rules for any question type is not feasible in t"
