2020.acl-main.346,D16-1257,0,0.139868,"taev et al., 2019) on the performance of the self-attentive parser. We aim to answer two questions about the state-of-the-art self-attentive parser: • Does self-training improve the performance of the self-attentive parser on disfluency detection? Self-training is a semi-supervised technique for incorporating unlabeled data into a new model, where an existing model trained on manually labeled (i.e. gold) data is used to label unlabeled data. The automatically (i.e. silver) labeled data are treated as truth and combined with the gold labeled data to re-train a new model (McClosky et al., 2006; Choe and Charniak, 2016). Since neural models use rich representations of language pre-trained on a large amount of unlabeled data (Peters et al., 2018; Devlin et al., 2019), we might expect that self-training adds no new information to the self-attentive parser. Surprisingly, however, we find that self-training improves disfluency detection f-score of the BERT-based self-attentive parser, demonstrating that self-training provides benefits orthogonal to the pre-trained contextualized embeddings. • Does ensembling improve disfluency detection in speech transcripts? Ensembling is a commonly used technique for improving"
2020.acl-main.346,D18-1217,0,0.0257154,"studies have leveraged additional data by using: (i) contextualized embeddings pre-trained on enormous amount of unlabeled data (Jamshid Lou et al., 2019; Tran et al., 2019; Bach and Huang, 2019) and (ii) synthetic data generated by adding noise in the form of disfluencies to fluent sentences (e.g. repeating, deleting or inserting words in a sentence) (Wang et al., 2018; Bach and Huang, 2019; Dong et al., 2019). By contrast, this paper focuses on self-training, which is a simple semisupervised technique that has been effective in different NLP tasks, including parsing (McClosky et al., 2006; Clark et al., 2018; Droganova et al., 2018). To our best knowledge, this is the first work that investigates self-training a neural disfluency detection model. Another technique commonly used for improving parsing is ensembling. Ensembling is a model combination method, where scores of multiple models (they can be the same or different models, trained on the same or different data, with different random initializations) are combined in some way (Dyer et al., 2016; Choe and Charniak, 2016; Fried et al., 2017). The state-of-the-art for parsing written text is an ensemble of four BERT-based self-attentive parsers,"
2020.acl-main.346,N19-1423,0,0.237765,"es self-training improve the performance of the self-attentive parser on disfluency detection? Self-training is a semi-supervised technique for incorporating unlabeled data into a new model, where an existing model trained on manually labeled (i.e. gold) data is used to label unlabeled data. The automatically (i.e. silver) labeled data are treated as truth and combined with the gold labeled data to re-train a new model (McClosky et al., 2006; Choe and Charniak, 2016). Since neural models use rich representations of language pre-trained on a large amount of unlabeled data (Peters et al., 2018; Devlin et al., 2019), we might expect that self-training adds no new information to the self-attentive parser. Surprisingly, however, we find that self-training improves disfluency detection f-score of the BERT-based self-attentive parser, demonstrating that self-training provides benefits orthogonal to the pre-trained contextualized embeddings. • Does ensembling improve disfluency detection in speech transcripts? Ensembling is a commonly used technique for improving parsing where scores of multiple instances of the same model trained on the same or different data are combined at inference time (Dyer et al., 2016"
2020.acl-main.346,W18-6006,0,0.0183589,"ged additional data by using: (i) contextualized embeddings pre-trained on enormous amount of unlabeled data (Jamshid Lou et al., 2019; Tran et al., 2019; Bach and Huang, 2019) and (ii) synthetic data generated by adding noise in the form of disfluencies to fluent sentences (e.g. repeating, deleting or inserting words in a sentence) (Wang et al., 2018; Bach and Huang, 2019; Dong et al., 2019). By contrast, this paper focuses on self-training, which is a simple semisupervised technique that has been effective in different NLP tasks, including parsing (McClosky et al., 2006; Clark et al., 2018; Droganova et al., 2018). To our best knowledge, this is the first work that investigates self-training a neural disfluency detection model. Another technique commonly used for improving parsing is ensembling. Ensembling is a model combination method, where scores of multiple models (they can be the same or different models, trained on the same or different data, with different random initializations) are combined in some way (Dyer et al., 2016; Choe and Charniak, 2016; Fried et al., 2017). The state-of-the-art for parsing written text is an ensemble of four BERT-based self-attentive parsers, where the parsers are co"
2020.acl-main.346,N16-1024,0,0.14472,"lin et al., 2019), we might expect that self-training adds no new information to the self-attentive parser. Surprisingly, however, we find that self-training improves disfluency detection f-score of the BERT-based self-attentive parser, demonstrating that self-training provides benefits orthogonal to the pre-trained contextualized embeddings. • Does ensembling improve disfluency detection in speech transcripts? Ensembling is a commonly used technique for improving parsing where scores of multiple instances of the same model trained on the same or different data are combined at inference time (Dyer et al., 2016; Fried et al., 2017; Kitaev et al., 2019). We expect ensembling parsers to improve the performance of the model on disfluency detection, too. We show ensembling four self-trained parsers (using different BERT word representations) via averaging their span label scores increases disfluency detection f-score in comparison with a single self-trained parser. 2 Related Work Parsing speech transcripts is challenging for conventional syntactic parsers, mainly due to the presence of disfluencies. In disfluent sentences, the relation between reparandum and repair is different from other words in the s"
2020.acl-main.346,P19-1031,0,0.0167173,"ng their span label scores (Kitaev et al., 2019). While ensembling is widely used in parsing, it has not been investigated for disfluency detection. In this paper, we also explore the impact of ensembling several parsing based disfluency detection models on disfluency detection performance. 3 Model Following Jamshid Lou et al. (2019), we use a self-attentive constituency parser for joint disfluency detection and syntactic parsing2 . The parsing model is based on the architecture introduced by Kitaev and Klein (2018), which is state-ofthe-art for (i) parsing written texts (Kitaev et al., 2019; Fried et al., 2019), (ii) parsing transcribed speech (Tran et al., 2019), and (iii) joint parsing and disfluency detection (Jamshid Lou et al., 2019). The self-attentive parser assigns a score s(T ) to each tree T by calculating the sum of the potentials 2 The code is available at: https://github.com/pariajm/ joint-disfluency-detection-and-parsing on its labeled constituent spans: X s(T ) = s(i, j, l) (1) (i,j,l)∈T where s(i, j, l) is the score of a constituent beginning at string position i ending at position j with label l. The input to the parser is a sequence of vectors corresponding to the sequence of words"
2020.acl-main.346,N01-1016,1,0.780688,"an, 1993; Marcus et al., 1999). Using the trained model, we parse unlabeled data and add the silver parse trees to the gold Switchboard training data and re-train the self-attentive parser using the enlarged training set. The unlabeled data we use include Fisher Speech Transcripts Part 1 (Cieri 3756 et al., 2004) and Part 2 (Cieri et al., 2005). Table 1 summarizes the different datasets used to train the self-attentive parser. Dataset SWB Fisher Labels # Sents # Words gold silver 98k 835k 733k 14m Table 1: Summary of the datasets used to train the selfattentive parser. 4 Experiments Following Charniak and Johnson (2001), we split the Switchboard into training, dev and test sets as follows: training data consists of the sw[23]∗.mrg files, dev data consists of the sw4[5, 6, 7, 8, 9]∗.mrg files and test data consists of the sw4[0, 1]∗.mrg files. All partial words3 and punctuations are removed from the data, as they are not available in realistic ASR applications (Johnson and Charniak, 2004). 4.1 Baseline Our baseline is the self-attentive parser trained on the gold Switchboard corpus with BERT word representations. The BERT-based parser is the current state-of-the-art, providing a very strong baseline for our w"
2020.acl-main.346,Q14-1011,1,0.93747,"Missing"
2020.acl-main.346,D18-1490,1,0.81593,"Missing"
2020.acl-main.346,P17-2087,1,0.883586,"Missing"
2020.acl-main.346,N19-1282,1,0.845904,"mean) is an optional part of the disfluency, and the repair the first type of privacy replaces the reparandum. The fluent version is obtained by removing the reparandum and the interregnum. EDITED INTJ PRN NP UH S NP DT JJ PP NN IN NP NP NP VP PRP VBP NP PP NN IN The first kind of invasion of uh NP DT JJ VP PP NN IN VBD NP VP VBN NN PP IN NP PRP I mean the first type of privacy seemed invaded to me Figure 1: A parse tree from the Switchboard corpus, where reparandum The first kind of invasion of, filled pause uh and discourse marker I mean are dominated by EDITED, INTJ and PRN nodes. Jamshid Lou et al. (2019) showed that a selfattentive constituency parser achieves state-of-theart results for joint parsing and disfluency detection. They observed that because the Switchboard 3754 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3754–3763 c July 5 - 10, 2020. 2020 Association for Computational Linguistics trees include both syntactic constituency nodes and EDITED nodes that indicate disfluency, training a parser to predict the Switchboard trees can be regarded as multi-task learning (where the tasks are syntactic parsing and identifying disfluencies). In"
2020.acl-main.346,N06-1020,1,0.803304,"etection. They observed that because the Switchboard 3754 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3754–3763 c July 5 - 10, 2020. 2020 Association for Computational Linguistics trees include both syntactic constituency nodes and EDITED nodes that indicate disfluency, training a parser to predict the Switchboard trees can be regarded as multi-task learning (where the tasks are syntactic parsing and identifying disfluencies). In this paper, we extend the multi-task learning in Jamshid Lou et al. (2019) to explore the impact of self-training (McClosky et al., 2006) and ensembling (Kitaev et al., 2019) on the performance of the self-attentive parser. We aim to answer two questions about the state-of-the-art self-attentive parser: • Does self-training improve the performance of the self-attentive parser on disfluency detection? Self-training is a semi-supervised technique for incorporating unlabeled data into a new model, where an existing model trained on manually labeled (i.e. gold) data is used to label unlabeled data. The automatically (i.e. silver) labeled data are treated as truth and combined with the gold labeled data to re-train a new model (McCl"
2020.acl-main.346,N18-1202,0,0.0694547,"ttentive parser: • Does self-training improve the performance of the self-attentive parser on disfluency detection? Self-training is a semi-supervised technique for incorporating unlabeled data into a new model, where an existing model trained on manually labeled (i.e. gold) data is used to label unlabeled data. The automatically (i.e. silver) labeled data are treated as truth and combined with the gold labeled data to re-train a new model (McClosky et al., 2006; Choe and Charniak, 2016). Since neural models use rich representations of language pre-trained on a large amount of unlabeled data (Peters et al., 2018; Devlin et al., 2019), we might expect that self-training adds no new information to the self-attentive parser. Surprisingly, however, we find that self-training improves disfluency detection f-score of the BERT-based self-attentive parser, demonstrating that self-training provides benefits orthogonal to the pre-trained contextualized embeddings. • Does ensembling improve disfluency detection in speech transcripts? Ensembling is a commonly used technique for improving parsing where scores of multiple instances of the same model trained on the same or different data are combined at inference t"
2020.acl-main.346,D13-1013,0,0.444219,"Missing"
2020.acl-main.346,N18-1007,0,0.0166594,"UNK S UNK VP UNK UNK UNK UNK VP UNK UNK we we also my wife and i uh uh volunteer to we go we also my wife and i uh uh volunteer to go (b) Self-trained (a) Baseline Figure 3: A sentence from the Switchboard dev set parsed by the baseline model (left) and by the self-trained model (right). The parse tree obtained by the self-trained model is the same as the gold parse tree. 5 Results We compare the performance of our best model with previous work on the Switchboard test set. As demonstrated in Table 7, our model outperforms prior work in parsing. The parsing result for our model is higher than Tran et al. (2018) which utilizes prosodic cues, as well as text based features. Parsing (S) Tran et al. (2018) ∗ Tran et al. (2018) Jamshid Lou et al. (2019) Tran et al. (2019) Tran et al. (2019)∗ This work (single model) This work (ensemble of 4) P R F − − 92.4 − − 93.2 93.6 − − 92.9 − − 93.8 94.2 87.9 88.5 92.7 92.8 93.0 93.5 93.9 Disfluency (E) P R F Tran et al. (2018) − − − 80.0 80.6 − 82.3 92.8 84.1 84.1 91.9 93.8 76.7 77.5 84.5 85.9 86.8 86.7 86.9 87.5 89.0 89.2 90.6 ∗ Tran et al. (2018) − Jamshid Lou et al. (2018)5 89.5 91.8 − 91.6 81.7 91.1 94.5 86.7 87.5 Zayats et al. (2016) Jamshid Lou and Johnson (2"
2020.acl-main.346,H05-1030,1,0.529423,"Missing"
2020.acl-main.346,P19-1340,0,0.251721,"e Switchboard 3754 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3754–3763 c July 5 - 10, 2020. 2020 Association for Computational Linguistics trees include both syntactic constituency nodes and EDITED nodes that indicate disfluency, training a parser to predict the Switchboard trees can be regarded as multi-task learning (where the tasks are syntactic parsing and identifying disfluencies). In this paper, we extend the multi-task learning in Jamshid Lou et al. (2019) to explore the impact of self-training (McClosky et al., 2006) and ensembling (Kitaev et al., 2019) on the performance of the self-attentive parser. We aim to answer two questions about the state-of-the-art self-attentive parser: • Does self-training improve the performance of the self-attentive parser on disfluency detection? Self-training is a semi-supervised technique for incorporating unlabeled data into a new model, where an existing model trained on manually labeled (i.e. gold) data is used to label unlabeled data. The automatically (i.e. silver) labeled data are treated as truth and combined with the gold labeled data to re-train a new model (McClosky et al., 2006; Choe and Charniak,"
2020.acl-main.346,C18-1299,0,0.358916,"d are disfluent (Charniak and Johnson, 2001). 1 For example in Figure 1, the reparandum The first kind of invasion of and the repair the first type of privacy are “rough copies” of each other. 3755 To mitigate the scarcity of labeled data, some studies have leveraged additional data by using: (i) contextualized embeddings pre-trained on enormous amount of unlabeled data (Jamshid Lou et al., 2019; Tran et al., 2019; Bach and Huang, 2019) and (ii) synthetic data generated by adding noise in the form of disfluencies to fluent sentences (e.g. repeating, deleting or inserting words in a sentence) (Wang et al., 2018; Bach and Huang, 2019; Dong et al., 2019). By contrast, this paper focuses on self-training, which is a simple semisupervised technique that has been effective in different NLP tasks, including parsing (McClosky et al., 2006; Clark et al., 2018; Droganova et al., 2018). To our best knowledge, this is the first work that investigates self-training a neural disfluency detection model. Another technique commonly used for improving parsing is ensembling. Ensembling is a model combination method, where scores of multiple models (they can be the same or different models, trained on the same or diff"
2020.acl-main.346,P18-1249,0,0.0855835,"xt is an ensemble of four BERT-based self-attentive parsers, where the parsers are combined by averaging their span label scores (Kitaev et al., 2019). While ensembling is widely used in parsing, it has not been investigated for disfluency detection. In this paper, we also explore the impact of ensembling several parsing based disfluency detection models on disfluency detection performance. 3 Model Following Jamshid Lou et al. (2019), we use a self-attentive constituency parser for joint disfluency detection and syntactic parsing2 . The parsing model is based on the architecture introduced by Kitaev and Klein (2018), which is state-ofthe-art for (i) parsing written texts (Kitaev et al., 2019; Fried et al., 2019), (ii) parsing transcribed speech (Tran et al., 2019), and (iii) joint parsing and disfluency detection (Jamshid Lou et al., 2019). The self-attentive parser assigns a score s(T ) to each tree T by calculating the sum of the potentials 2 The code is available at: https://github.com/pariajm/ joint-disfluency-detection-and-parsing on its labeled constituent spans: X s(T ) = s(i, j, l) (1) (i,j,l)∈T where s(i, j, l) is the score of a constituent beginning at string position i ending at position j wit"
2020.acl-main.346,C16-1027,0,0.371635,"Missing"
2020.acl-main.346,N06-2019,1,0.690849,"Missing"
2020.acl-main.346,D17-1296,0,0.394959,"Missing"
2020.acl-main.346,D16-1109,0,0.369851,"Missing"
2020.acl-main.346,P17-2025,0,\N,Missing
2020.findings-emnlp.186,N01-1016,1,0.727644,"disfluency detection model. finds a constituency parse tree and detects disfluencies in speech transcripts. Different versions of the parser are available; we use the parser trained on the Penn Treebank Release 3 Switchboard corpus with partial words kept in the data for which they reported an f-score of 94.4 on the SWBD dev set. We remove all disfluent words (tagged as “EDITED” and “INTJ”), as well as partial words (words tagged “XX” and words ending in “-”) and punctuation from the SWBD and Fisher data. We use the standard data splits for training our models as well as the language models (Charniak and Johnson, 2001): training data consists of the sw[23].text files11 and fe 03 ∗.txt, dev data consists of the sw4[5, 6, 7, 8, 9].text files and test data consists of the sw4[0, 1].text files. We consider a pipeline approach as our baseline and apply the “off-the-shelf” disfluency detection model to the output of the baseline ASR models. As our evaluation metrics, we report WER, FER and DER for the end-to-end and the pipeline models. Since the goal of an integrated system is to find only the fluent words, we evaluate WER only on fluent words. For calculating FER and DER, we align the output of the integrated m"
2020.findings-emnlp.186,Q14-1011,1,0.622213,"lude filled pauses (e.g. um and uh), repetitions (e.g. the the), corrections (e.g. Show me the flights . . . the early flights), parenthetical asides (e.g. you know), interjections (e.g. well and like), restarts (e.g. There’s a . . . Let’s go) and partial words (e.g. wou- and oper-) which frequently occur in spontaneous speech1 and reduce the readability of speech transcripts (Liu et al., 2006). They also pose a major challenge to downstream tasks relying on the output of speech recognition systems, such as parsing and machine translation models (Johnson and Charniak, 2004; Wang et al., 2010; Honnibal and Johnson, 2014). Since these models are usually trained on fluent clean corpora, the mismatch between the training data and the actual use 1 Shriberg (1994) observed disfluencies once in every 20 words. I want a flight to Boston uh I mean to Denver. |{z } |{z } |{z } reparandum interregnum repair Disfluency detection is usually an intermediate step between an ASR model and a downstream task. This pipeline approach is complex to implement and leads to higher inference latency. It also has the potential problem of errors compounding between components, e.g. recognition errors lead to larger disfluency detectio"
2020.findings-emnlp.186,D18-1490,1,0.866612,"model? The existing evaluation metrics are designed to measure the performance of a single task, namely speech recognition or disfluency detection, but not both. We introduce two new metrics measuring the disfluency removal and word recognition performance of an end-to-end model. 2 Related Work Disfluency removal is typically performed by training a specialized disfluency detection model on disfluency labeled data and applying it as a separate component following an ASR model and prior to a downstream task. The specialized disfluency detectors (Zayats et al., 2016; Wang et al., 2016; Jamshid Lou et al., 2018) are usually trained on the Switchboard corpus (Marcus et al., 1999) which is the largest available dataset with gold (i.e. human-annotated) disfluency labels. State-ofthe-art disfluency detectors use Transformer models with pretrained contextualised word embeddings (e.g. BERT) (Tran et al., 2019; Jamshid Lou et al., 2019; Dong et al., 2019; Wang et al., 2019a; Jamshid Lou and Johnson, 2020). Multi-task learning has been effective for disfluency detection, for example, a Transformer trained to jointly detect disfluencies and find constituency parse trees would leverage syntactic information an"
2020.findings-emnlp.186,2020.acl-main.346,1,0.84038,"el on disfluency labeled data and applying it as a separate component following an ASR model and prior to a downstream task. The specialized disfluency detectors (Zayats et al., 2016; Wang et al., 2016; Jamshid Lou et al., 2018) are usually trained on the Switchboard corpus (Marcus et al., 1999) which is the largest available dataset with gold (i.e. human-annotated) disfluency labels. State-ofthe-art disfluency detectors use Transformer models with pretrained contextualised word embeddings (e.g. BERT) (Tran et al., 2019; Jamshid Lou et al., 2019; Dong et al., 2019; Wang et al., 2019a; Jamshid Lou and Johnson, 2020). Multi-task learning has been effective for disfluency detection, for example, a Transformer trained to jointly detect disfluencies and find constituency parse trees would leverage syntactic information and detect disfluencies more accurately (Jamshid Lou et al., 2019). Self-training and ensembling have also shown to provide benefit to disfluency detection (Jamshid Lou and Johnson, 2020). Selftraining on disfluent data provides benefits orthogonal to the pretrained contextualized embeddings and mitigates the scarcity of gold disfluency labeled data. The BERT-based self-attentive parser introd"
2020.findings-emnlp.186,N19-1282,1,0.937539,"Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2051–2061 c November 16 - 20, 2020. 2020 Association for Computational Linguistics we address the task of end-to-end speech recognition and disfluency removal. Specifically, we investigate whether it is possible to train an ASR model end-to-end to directly map disfluent speech into fluent transcripts, without an intermediate disfluency detection step. Some previous work has attempted disfluency detection as part of another task in an end-to-end manner, e.g. joint disfluency detection and constituency parsing (Jamshid Lou et al., 2019) and direct translation from disfluent Spanish speech to fluent English transcripts (Salesky et al., 2019). However, to the best of our knowledge, this is the first work that systematically investigates the task of end-to-end ASR and disfluency removal, serving as a starting point for future research into end-toend disfluency removal systems. In this paper, we aim to answer the following questions: • Can an ASR model directly generate fluent transcripts from disfluent speech? We might expect an end-to-end ASR model (without an explicit disfluency detection component) not to effectively detect"
2020.findings-emnlp.186,2020.iwslt-1.22,0,0.302541,"to the pretrained contextualized embeddings and mitigates the scarcity of gold disfluency labeled data. The BERT-based self-attentive parser introduced in Jamshid Lou et al. (2019) is the current state-of-the-art in disfluency detection; thus, we use it as the “off-the-shelf” disfluency detector in our pipeline approach, as explained in Section 5. With the rise of end-to-end models, the conversational speech translation models that directly translate disfluent speech into fluent texts have recently attracted increasing attention (Salesky et al., 2019; Ansari et al., 2020; Fukuda et al., 2020; Saini et al., 2020). The most similar previous work to ours is Salesky et al. (2019). They train a sequence-tosequence model (called fluent model) to directly translate from disfluent Spanish speech to fluent English transcripts without a separate disfluency detection step. As a baseline, they train a model (called disfluent model) on disfluent speech and disfluent translations. To compare the performance of the fluent and disfluent models, they score the outputs against the fluent references using BLEU and METEOR. Similar METEOR scores are reported for both models, but BLEU scores are lower with the disfluent m"
2020.findings-emnlp.186,N19-1285,0,0.193018,"20, 2020. 2020 Association for Computational Linguistics we address the task of end-to-end speech recognition and disfluency removal. Specifically, we investigate whether it is possible to train an ASR model end-to-end to directly map disfluent speech into fluent transcripts, without an intermediate disfluency detection step. Some previous work has attempted disfluency detection as part of another task in an end-to-end manner, e.g. joint disfluency detection and constituency parsing (Jamshid Lou et al., 2019) and direct translation from disfluent Spanish speech to fluent English transcripts (Salesky et al., 2019). However, to the best of our knowledge, this is the first work that systematically investigates the task of end-to-end ASR and disfluency removal, serving as a starting point for future research into end-toend disfluency removal systems. In this paper, we aim to answer the following questions: • Can an ASR model directly generate fluent transcripts from disfluent speech? We might expect an end-to-end ASR model (without an explicit disfluency detection component) not to effectively detect disfluencies. However, we show that end-to-end ASR models do learn to directly generate fluent transcripts"
2020.findings-emnlp.186,C18-1299,0,0.340906,"Missing"
2020.findings-emnlp.186,C16-1027,0,0.113794,"ASR and disfluency removal model? The existing evaluation metrics are designed to measure the performance of a single task, namely speech recognition or disfluency detection, but not both. We introduce two new metrics measuring the disfluency removal and word recognition performance of an end-to-end model. 2 Related Work Disfluency removal is typically performed by training a specialized disfluency detection model on disfluency labeled data and applying it as a separate component following an ASR model and prior to a downstream task. The specialized disfluency detectors (Zayats et al., 2016; Wang et al., 2016; Jamshid Lou et al., 2018) are usually trained on the Switchboard corpus (Marcus et al., 1999) which is the largest available dataset with gold (i.e. human-annotated) disfluency labels. State-ofthe-art disfluency detectors use Transformer models with pretrained contextualised word embeddings (e.g. BERT) (Tran et al., 2019; Jamshid Lou et al., 2019; Dong et al., 2019; Wang et al., 2019a; Jamshid Lou and Johnson, 2020). Multi-task learning has been effective for disfluency detection, for example, a Transformer trained to jointly detect disfluencies and find constituency parse trees would levera"
2020.findings-emnlp.186,N19-1008,0,0.364084,"higher inference latency. It also has the potential problem of errors compounding between components, e.g. recognition errors lead to larger disfluency detection errors. End-to-end models, on the other hand, are less prone to such problems. More importantly, end-to-end models can leverage paralinguistic features in speech signal that are not available in pipeline systems. Speech carries extra information beyond the words which might provide useful cues to disfluency detection2 . In this paper, 2 Prosodic cues (e.g. pause) signal disfluencies by marking the interruption point (Shriberg, 1994; Zayats and Ostendorf, 2019). 2051 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2051–2061 c November 16 - 20, 2020. 2020 Association for Computational Linguistics we address the task of end-to-end speech recognition and disfluency removal. Specifically, we investigate whether it is possible to train an ASR model end-to-end to directly map disfluent speech into fluent transcripts, without an intermediate disfluency detection step. Some previous work has attempted disfluency detection as part of another task in an end-to-end manner, e.g. joint disfluency detection and constituency parsing (J"
2020.textgraphs-1.7,P11-1062,0,0.644967,"pitfall of learning non-entailments such as win 6→ lose. We evaluate our model on a manually constructed dataset, showing that incorporating time intervals and applying a temporal window around them, are effective strategies. 1 Introduction Recognising textual entailment and paraphrases is core to many downstream NLP applications such as question answering and semantic parsing. In the case of open-domain question answering over unstructured data, the answer to a question may not be explicitly stated in the text, but may be recovered via paraphrases and/or entailment rules. Entailment graphs (Berant et al., 2011; Berant et al., 2015; Hosseini et al., 2018), in which nodes represent predicates and edges are entailment relations, have been proposed as a means to answer such questions. They can be mined using unsupervised methods applied over large collections of text, by keeping track of which entity pairs occur with which predicates. One common error made by these graphs, however, is that they assert spurious associations between similar but temporally distinct events that occur with the same entity pairs. For example, both the predicates beat and lost against will apply to sports team entity pairs su"
2020.textgraphs-1.7,J15-2003,0,0.271857,"non-entailments such as win 6→ lose. We evaluate our model on a manually constructed dataset, showing that incorporating time intervals and applying a temporal window around them, are effective strategies. 1 Introduction Recognising textual entailment and paraphrases is core to many downstream NLP applications such as question answering and semantic parsing. In the case of open-domain question answering over unstructured data, the answer to a question may not be explicitly stated in the text, but may be recovered via paraphrases and/or entailment rules. Entailment graphs (Berant et al., 2011; Berant et al., 2015; Hosseini et al., 2018), in which nodes represent predicates and edges are entailment relations, have been proposed as a means to answer such questions. They can be mined using unsupervised methods applied over large collections of text, by keeping track of which entity pairs occur with which predicates. One common error made by these graphs, however, is that they assert spurious associations between similar but temporally distinct events that occur with the same entity pairs. For example, both the predicates beat and lost against will apply to sports team entity pairs such as (Arsenal, Man U"
2020.textgraphs-1.7,D15-1075,0,0.0410157,"ombines the desirable behaviors of symmetric and directional measures. We adapt and examine each of these similarity measures using our evaluation dataset. Alternatively, Hosseini et al. (2019) performed link prediction on the set of relation triples extracted from the text, and showed improvements over BInc by augmenting the data with additional predicted triples. We consider this link prediction model to be beyond the scope of this work. 2.2 Evaluating Entailment Graphs The construction of entailment datasets has been framed as a number of manual annotation tasks including image captioning (Bowman et al., 2015) and question answering (Levy and Dagan, 2016). The dataset creation method used by Levy and Dagan (2016) aims to address the bias towards real world knowledge. They ask human annotators to mark possible answers to questions as True/False (entailment/non-entailment), with entities in the answer replaced using tokens representing their type (e.g. London becomes city). The method also aims to address the bias of other datasets such as Zeichner’s dataset (Zeichner et al., 2012) and the SherLIic dataset (Schmitt and Sch¨utze, 2019), in which candidate entailments were automatically pre-selected fo"
2020.textgraphs-1.7,chang-manning-2012-sutime,0,0.0261532,"Missing"
2020.textgraphs-1.7,P05-1014,0,0.112325,"(Berant et al., 2011; Hosseini et al., 2018), and eventualities (Yu et al., 2020). In this work we use typed predicates, leveraging the second level in the FIGER hierarchy (Ling and Weld, 2012), to enable a close examination of events that take place between two sports teams. Whether predicates in the graph entail each other may be determined using a variety of similarity measures. These are inspired by the Distributional Inclusion Hypothesis, which states that a predicate p entails another predicate q if for any context in which p can be used, q may be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005). They include the symmetric Lin’s similarity measure (Lin, 1998), the directional Weeds’ precision and recall measures (Weeds and Weir, 2003), and the Balanced Inclusion score (BInc) (Szpektor and Dagan, 2008). BInc, the geometric mean of Lin’s similarity and Weed’s precision, combines the desirable behaviors of symmetric and directional measures. We adapt and examine each of these similarity measures using our evaluation dataset. Alternatively, Hosseini et al. (2019) performed link prediction on the set of relation triples extracted from the text, and showed improvements over BInc by augment"
2020.textgraphs-1.7,Q18-1048,1,0.155495,"as win 6→ lose. We evaluate our model on a manually constructed dataset, showing that incorporating time intervals and applying a temporal window around them, are effective strategies. 1 Introduction Recognising textual entailment and paraphrases is core to many downstream NLP applications such as question answering and semantic parsing. In the case of open-domain question answering over unstructured data, the answer to a question may not be explicitly stated in the text, but may be recovered via paraphrases and/or entailment rules. Entailment graphs (Berant et al., 2011; Berant et al., 2015; Hosseini et al., 2018), in which nodes represent predicates and edges are entailment relations, have been proposed as a means to answer such questions. They can be mined using unsupervised methods applied over large collections of text, by keeping track of which entity pairs occur with which predicates. One common error made by these graphs, however, is that they assert spurious associations between similar but temporally distinct events that occur with the same entity pairs. For example, both the predicates beat and lost against will apply to sports team entity pairs such as (Arsenal, Man United). This is likely t"
2020.textgraphs-1.7,P19-1468,1,0.846673,"cate p entails another predicate q if for any context in which p can be used, q may be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005). They include the symmetric Lin’s similarity measure (Lin, 1998), the directional Weeds’ precision and recall measures (Weeds and Weir, 2003), and the Balanced Inclusion score (BInc) (Szpektor and Dagan, 2008). BInc, the geometric mean of Lin’s similarity and Weed’s precision, combines the desirable behaviors of symmetric and directional measures. We adapt and examine each of these similarity measures using our evaluation dataset. Alternatively, Hosseini et al. (2019) performed link prediction on the set of relation triples extracted from the text, and showed improvements over BInc by augmenting the data with additional predicted triples. We consider this link prediction model to be beyond the scope of this work. 2.2 Evaluating Entailment Graphs The construction of entailment datasets has been framed as a number of manual annotation tasks including image captioning (Bowman et al., 2015) and question answering (Levy and Dagan, 2016). The dataset creation method used by Levy and Dagan (2016) aims to address the bias towards real world knowledge. They ask hum"
2020.textgraphs-1.7,W19-0409,1,0.894684,"Missing"
2020.textgraphs-1.7,P16-2041,0,0.250603,"and directional measures. We adapt and examine each of these similarity measures using our evaluation dataset. Alternatively, Hosseini et al. (2019) performed link prediction on the set of relation triples extracted from the text, and showed improvements over BInc by augmenting the data with additional predicted triples. We consider this link prediction model to be beyond the scope of this work. 2.2 Evaluating Entailment Graphs The construction of entailment datasets has been framed as a number of manual annotation tasks including image captioning (Bowman et al., 2015) and question answering (Levy and Dagan, 2016). The dataset creation method used by Levy and Dagan (2016) aims to address the bias towards real world knowledge. They ask human annotators to mark possible answers to questions as True/False (entailment/non-entailment), with entities in the answer replaced using tokens representing their type (e.g. London becomes city). The method also aims to address the bias of other datasets such as Zeichner’s dataset (Zeichner et al., 2012) and the SherLIic dataset (Schmitt and Sch¨utze, 2019), in which candidate entailments were automatically pre-selected for manual annotation according to a similarity"
2020.textgraphs-1.7,W14-1610,0,0.0248504,"ences (left) and their resulting (collapsed) entailment/non-entailment graph (right) The contributions of this work are: 1) a model for incorporating relation-level time intervals into an entailment graph mining procedure, outperforming non-temporal models, and 2) a manually constructed evaluation dataset of sports domain predicates. To our knowledge this is the first attempt to incorporate temporal information for learning entailment graphs. 2 2.1 Related Work Entailment Graphs Entailment graphs have been constructed for a range of domains, including newswire (Hosseini et al., 2018), health (Levy et al., 2014), and commonsense (Yu et al., 2020). In order to leverage temporal information, our work focuses on the news domain, in which each article has a known publication date and temporal expressions are commonly used. A range of node representations have been explored, including Open-IE propositions (Levy et al., 2014), typed predicates (Berant et al., 2011; Hosseini et al., 2018), and eventualities (Yu et al., 2020). In this work we use typed predicates, leveraging the second level in the FIGER hierarchy (Ling and Weld, 2012), to enable a close examination of events that take place between two spor"
2020.textgraphs-1.7,Q13-1015,1,0.848439,"Missing"
2020.textgraphs-1.7,P19-1086,0,0.026677,"Missing"
2020.textgraphs-1.7,N19-1020,1,0.770989,"Missing"
2020.textgraphs-1.7,C08-1107,0,0.911416,"close examination of events that take place between two sports teams. Whether predicates in the graph entail each other may be determined using a variety of similarity measures. These are inspired by the Distributional Inclusion Hypothesis, which states that a predicate p entails another predicate q if for any context in which p can be used, q may be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005). They include the symmetric Lin’s similarity measure (Lin, 1998), the directional Weeds’ precision and recall measures (Weeds and Weir, 2003), and the Balanced Inclusion score (BInc) (Szpektor and Dagan, 2008). BInc, the geometric mean of Lin’s similarity and Weed’s precision, combines the desirable behaviors of symmetric and directional measures. We adapt and examine each of these similarity measures using our evaluation dataset. Alternatively, Hosseini et al. (2019) performed link prediction on the set of relation triples extracted from the text, and showed improvements over BInc by augmenting the data with additional predicted triples. We consider this link prediction model to be beyond the scope of this work. 2.2 Evaluating Entailment Graphs The construction of entailment datasets has been fram"
2020.textgraphs-1.7,S13-2001,0,0.0691019,"Missing"
2020.textgraphs-1.7,W03-1011,0,0.368862,"level in the FIGER hierarchy (Ling and Weld, 2012), to enable a close examination of events that take place between two sports teams. Whether predicates in the graph entail each other may be determined using a variety of similarity measures. These are inspired by the Distributional Inclusion Hypothesis, which states that a predicate p entails another predicate q if for any context in which p can be used, q may be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005). They include the symmetric Lin’s similarity measure (Lin, 1998), the directional Weeds’ precision and recall measures (Weeds and Weir, 2003), and the Balanced Inclusion score (BInc) (Szpektor and Dagan, 2008). BInc, the geometric mean of Lin’s similarity and Weed’s precision, combines the desirable behaviors of symmetric and directional measures. We adapt and examine each of these similarity measures using our evaluation dataset. Alternatively, Hosseini et al. (2019) performed link prediction on the set of relation triples extracted from the text, and showed improvements over BInc by augmenting the data with additional predicted triples. We consider this link prediction model to be beyond the scope of this work. 2.2 Evaluating Ent"
2020.textgraphs-1.7,P12-2031,0,0.0194155,"The construction of entailment datasets has been framed as a number of manual annotation tasks including image captioning (Bowman et al., 2015) and question answering (Levy and Dagan, 2016). The dataset creation method used by Levy and Dagan (2016) aims to address the bias towards real world knowledge. They ask human annotators to mark possible answers to questions as True/False (entailment/non-entailment), with entities in the answer replaced using tokens representing their type (e.g. London becomes city). The method also aims to address the bias of other datasets such as Zeichner’s dataset (Zeichner et al., 2012) and the SherLIic dataset (Schmitt and Sch¨utze, 2019), in which candidate entailments were automatically pre-selected for manual annotation according to a similarity measure. Entailments that exist, but are not captured by these similarity measures will therefore be excluded. There has been very little work on the specific problem of evaluating entailment of a temporal nature. The FraCas test suite (Cooper et al., 1996) contains a small section of which only a few examples are 61 entailments between predicates. The TEA dataset (Kober et al., 2019) consists of pairs of sentences in which tempo"
2020.textgraphs-1.7,D13-1183,0,0.641764,"Missing"
2021.acl-long.9,D17-1098,1,0.925029,"9) require the models to mention all or some of the input keywords, key-value pairs and image object labels (respectively), potentially with linguistic variants, in the generated outputs. Large (pre-trained) Transformer-based S2S models such as T5 (Raffel et al., 2019) can be trained (fine-tuned) to perform this task. However, they only learn to copy the surface tokens from encoder inputs to the decoder outputs and there is no underlying mechanism guaranteeing good constraint satisfaction (the ratio of satisfied lexical constraints to given lexical constraints). Constrained Beam Search (CBS) (Anderson et al., 2017) and related algorithms can guarantee outputs satisfying all constraints, however they are much slower than the standard beam search algorithm. In addition, as they are all inference-based algorithms, their corresponding models are not aware of the 103 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 103–113 August 1–6, 2021. ©2021 Association for Computational Linguistics constraint words or phrases, the resulting generation could be poor. Ideally, a method for producing co"
2021.acl-long.9,W17-3518,0,0.0188628,"example, the Mention Flag for flower is set (indicated by orange dots) from the third token because it is generated at the second step. Both token and Mention Flag embeddings are the input to the decoder, but Mention Flags are injected into the decoder in a different way to the tokens (see Fig. 3). Note that task specific encoder inputs have been omitted for brevity. Introduction This paper focuses on Seq2Seq (S2S) constrained text generation where a set of encoder input tokens are required to be present in the generated outputs. For example, Keyword-to-Text (Lin et al., 2020), Data-to-Text (Gardent et al., 2017; Duˇsek et al., 2020) and Image-to-Text (Lin et al., 2014; 1 The source code for this paper is released at https: //github.com/GaryYufei/ACL2021MF Agrawal et al., 2019) require the models to mention all or some of the input keywords, key-value pairs and image object labels (respectively), potentially with linguistic variants, in the generated outputs. Large (pre-trained) Transformer-based S2S models such as T5 (Raffel et al., 2019) can be trained (fine-tuned) to perform this task. However, they only learn to copy the surface tokens from encoder inputs to the decoder outputs and there is no un"
2021.acl-long.9,P16-1154,0,0.0386084,"k well in low-resource settings. Our MF models set a new Background Training S2S Models S2S models can implicitly capture the co-occurrence between encoder and decoder sequences, particularly pre-trained ones such as T5 (Raffel et al., 2019) and BART (Lewis et al., 2020). Wen et al. (2015) uses a special gate to control what information will be generated in the following steps. Kale and Rastogi (2020) have shown that the T5 models achieve state-of-the-art results in various Data-to-Text tasks, requiring copying from encoder to decoder, after fine-tuning. As an alternative, the Copy Mechanism (Gu et al., 2016) explicitly learns where to copy the input constraints into the output by adding an extra copy pathway to the models. However, these approaches cannot control or guarantee their constraint satisfaction. Lin et al. (2020) also have observed lower constraint satisfaction in the above methods, compared to the constrained decoding approaches. Constrained Decoding These algorithms, including Constrained Beam Search (CBS) (Anderson et al., 2017) and Grid Beam Search (GBS) (Hokamp and Liu, 2017), maintain a set of states which have their own size-k beams and only allow hypotheses satisfying specific"
2021.acl-long.9,P17-1141,0,0.021949,"ext tasks, requiring copying from encoder to decoder, after fine-tuning. As an alternative, the Copy Mechanism (Gu et al., 2016) explicitly learns where to copy the input constraints into the output by adding an extra copy pathway to the models. However, these approaches cannot control or guarantee their constraint satisfaction. Lin et al. (2020) also have observed lower constraint satisfaction in the above methods, compared to the constrained decoding approaches. Constrained Decoding These algorithms, including Constrained Beam Search (CBS) (Anderson et al., 2017) and Grid Beam Search (GBS) (Hokamp and Liu, 2017), maintain a set of states which have their own size-k beams and only allow hypotheses satisfying specific constraints to be considered during inference. Each CBS state corresponds to the hypotheses satisfying different constraints (exponential in the number of constraints) and the GBS states correspond to the hypotheses satisfying the same number of constraints (linear to constraint number). Balakrishnan et al. (2019); Juraska et al. (2018); Duˇsek and Jurˇc´ıcˇ ek (2016) also modify their inference algorithm in a similar way to fulfill specific output requirements. However, they significantl"
2021.acl-long.9,P19-1080,0,0.0168116,"s, compared to the constrained decoding approaches. Constrained Decoding These algorithms, including Constrained Beam Search (CBS) (Anderson et al., 2017) and Grid Beam Search (GBS) (Hokamp and Liu, 2017), maintain a set of states which have their own size-k beams and only allow hypotheses satisfying specific constraints to be considered during inference. Each CBS state corresponds to the hypotheses satisfying different constraints (exponential in the number of constraints) and the GBS states correspond to the hypotheses satisfying the same number of constraints (linear to constraint number). Balakrishnan et al. (2019); Juraska et al. (2018); Duˇsek and Jurˇc´ıcˇ ek (2016) also modify their inference algorithm in a similar way to fulfill specific output requirements. However, they significantly increase the inference run-time and memory and can produce sub-optimal outputs. 3 Method This section first formulates constrained text generation tasks, then introduces Mention Flags and their 104 integration with Transformer-based text generators. 3.1 S2S Constrained Text Generation 7 3 7 3 3 In the S2S constrained text generation tasks, we are given encoder inputs x = [x1 , . . . , xlx ] ∈ X that describe the task"
2021.acl-long.9,W05-0909,0,0.0976559,"Missing"
2021.acl-long.9,W19-8652,0,0.0315495,"Missing"
2021.acl-long.9,N18-1014,0,0.0124725,"ed decoding approaches. Constrained Decoding These algorithms, including Constrained Beam Search (CBS) (Anderson et al., 2017) and Grid Beam Search (GBS) (Hokamp and Liu, 2017), maintain a set of states which have their own size-k beams and only allow hypotheses satisfying specific constraints to be considered during inference. Each CBS state corresponds to the hypotheses satisfying different constraints (exponential in the number of constraints) and the GBS states correspond to the hypotheses satisfying the same number of constraints (linear to constraint number). Balakrishnan et al. (2019); Juraska et al. (2018); Duˇsek and Jurˇc´ıcˇ ek (2016) also modify their inference algorithm in a similar way to fulfill specific output requirements. However, they significantly increase the inference run-time and memory and can produce sub-optimal outputs. 3 Method This section first formulates constrained text generation tasks, then introduces Mention Flags and their 104 integration with Transformer-based text generators. 3.1 S2S Constrained Text Generation 7 3 7 3 3 In the S2S constrained text generation tasks, we are given encoder inputs x = [x1 , . . . , xlx ] ∈ X that describe the task, where some xi corresp"
2021.acl-long.9,2020.inlg-1.14,0,0.0748271,"he non-pre-trained and pre-trained S2S Transformer-based models. Furthermore, our experiments show that the MF models can satisfy novel constraints (i.e, involving words or phrases not seen during training) and they work well in low-resource settings. Our MF models set a new Background Training S2S Models S2S models can implicitly capture the co-occurrence between encoder and decoder sequences, particularly pre-trained ones such as T5 (Raffel et al., 2019) and BART (Lewis et al., 2020). Wen et al. (2015) uses a special gate to control what information will be generated in the following steps. Kale and Rastogi (2020) have shown that the T5 models achieve state-of-the-art results in various Data-to-Text tasks, requiring copying from encoder to decoder, after fine-tuning. As an alternative, the Copy Mechanism (Gu et al., 2016) explicitly learns where to copy the input constraints into the output by adding an extra copy pathway to the models. However, these approaches cannot control or guarantee their constraint satisfaction. Lin et al. (2020) also have observed lower constraint satisfaction in the above methods, compared to the constrained decoding approaches. Constrained Decoding These algorithms, includin"
2021.acl-long.9,P16-2008,0,0.0438952,"Missing"
2021.acl-long.9,2020.acl-main.703,0,0.212736,"atural Language Processing, pages 103–113 August 1–6, 2021. ©2021 Association for Computational Linguistics constraint words or phrases, the resulting generation could be poor. Ideally, a method for producing constrained text should: a) generate high-quality text; b) achieve high constraint satisfaction; c) have an efficient inference procedure. state-of-the-art in these three tasks. 2 In this paper, we focus on constraining transformerbased text generation models due to their popularity and success in various domains, especially in largescale pre-trained language models (Raffel et al., 2019; Lewis et al., 2020). Previous work can be roughly categorized into two streams: S2S training approaches and Constrained decoding approaches: To this end, we propose Mention Flags (MF), which trace whether a lexical constraint has been realized in partial decoder outputs. Specifically, each decoder input token is provided with a set of flags indicating which constraints have been satisfied up to that token. As shown in Fig 1, the Mention Flags for flower is set from the third step, because flower is generated at the second step. We represent the three possible Mention Flags as separate trainable embeddings and in"
2021.acl-long.9,2020.findings-emnlp.165,0,0.427516,"surface tokens from encoders to decoders, but they cannot guarantee constraint satisfaction. Constrained decoding algorithms always produce hypotheses satisfying all constraints. However, they are computationally expensive and can lower the generated text quality. In this paper, we propose Mention Flags (MF), which trace whether lexical constraints are satisfied in the generated outputs of an S2S decoder. The MF models are trained to generate tokens until all constraints are satisfied, guaranteeing high constraint satisfaction. Our experiments on the Common Sense Generation task (CommonGen) (Lin et al., 2020), End2end Data-to-Text task (E2ENLG) (Duˇsek et al., 2020) and Novel Object Captioning task (nocaps) (Agrawal et al., 2019) show that the MF models maintain higher constraint satisfaction and text quality than the baseline models and other constrained text generation algorithms, achieving state-of-the-art performance on all three tasks. These results are achieved with a much lower run-time than constrained decoding algorithms. We also show that the MF models work well in the low-resource setting. 1 1 Figure 1: An overview of the Mention Flag mechanism for Transformer-based S2S models. Here, th"
2021.acl-long.9,N03-1020,0,0.0525378,"Missing"
2021.acl-long.9,P02-1040,0,0.119361,"Missing"
2021.acl-long.9,P16-1162,0,0.0190521,"in y:t . We use conventions from the relevant data set to determine whether a constraint is a multi-word constraint. This avoids false update when the models only generate the prefix of the constraints, rather than the full constraints. For example, given constraint “washing machine”, the output could be “I put my washing in the new washing machine.” The situation becomes more complicated when both washing and washing machine are given lexical constraints. When we find this case, we delay the value 2 update for washing until the word in is generated. Modern tokenization methods, such as BPE (Sennrich et al., 2016), make this situation frequent. Definition of Mentions We deliberately allow a flexible notion of mentions in the Function m(). We can define various types of mentions to fulfill the requirements of different applications and tasks. With this flexibility, the end-users can use Mention Flags in many constraint scenarios. For tasks with strict constraints, we define mentions to be the exact string match in y:t . Otherwise, inflectional variants or synonyms of words in the lexical constraints are allowed when checking for mentions. Our Mention Flag mechanism thus supports lexical constraints with"
2021.acl-long.9,N18-2074,0,0.0218551,"e that the CA module in the Transformer decoder already uses y:t as query and he as key. The resulting query-key similarity matrix has the same size of our Mention Flag matrix, making it suitable to incorporate F . Standard S2S Transformer Model The encoder input tokens x is fed into the Transformer Encoder he = Enc(x) where he ∈ Rlx ×d and d is the model hidden size. In the Transformer decoder, there are two self-attention modules, Self MultiHead Attention (SA) which handles the current decoder input sequence y:t , and Cross Multi-Head 106 Mention Flag Matrix as Relative Position Inspired by Shaw et al. (2018) which incorporates token relative positions into the SA module, we propose to inject Mention Flags as the “relative positions” between encoder output he and current decoder input y:t in the CA module. In each decoder layer, we represent F as two sets of trainable embeddings Mention Flag key mk = Ek (F ) and Mention Flag Value mv = Ev (F ) where Ek , Ev ∈ R3×d are the Mention Flag embedding tables. mk and mv ∈ Rlx ×t×d . We have separated Mention Flags representations for each decoder layer. Eq. 4 is changed to: layer of the T5 decoder. This parameters freezing technology is applied to both T5"
2021.acl-long.9,N19-1410,0,0.0383923,"Missing"
2021.acl-long.9,2020.acl-main.325,0,0.0391574,"Missing"
2021.acl-long.9,2021.eacl-main.104,1,0.896614,"objects jointly (Anderson et al., 2018). However, Puduppully et al. (2019) shows the benefits of separating content selection and text planning steps for general data-to-text tasks. Following this, we propose to first select salient objects and incorporate the selected objects into the description using Mention Flags. m(C, ε) = [0, 0, · · · , 1, · · · , 0, 0, · · · , 1] where only salient object labels receive value 1. m() allows inflectional variants to satisfy lexical constraints. We use T5-base model in this experiment. The T5 + C and T5 + MF + C models are constrained with CBS. Following Wang et al. (2021), we report CIDEr and SPICE as output text quality metrics and constraint satisfaction for novel constraints (Novel) and all constraints (ALL). We present the performance for all evaluation images 108 (Overall) and for the challenging images with only novel objects (out-of-domain split). model, indicating that the MF model correctly captures more long-range relationships (calculated by the parsing trees used in SPICE) among the (novel) objects than CBS. Our T5 + MF model outperforms the existing state-of-the-art end-to-end single-stage image captioning systems (Agrawal et al., 2019; Li et al.,"
2021.acl-long.9,D15-1199,0,0.066173,"Missing"
2021.eacl-main.104,D17-1098,1,0.913959,"s. 4.1 Evaluation Metrics We use CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016a) and METEOR (Banerjee and Lavie, 2005) to evaluate the caption quality. CIDEr measures the similarity between the reference captions and generated outputs using tf-idf weighted ngram overlap. SPICE is based on the scene graphs matching between the reference captions and generated outputs. METEOR focuses on the alignment between the words in reference captions and generated outputs, with an aim of 1:1 correspondence. To measure the effectiveness of our copy encouragement approach, we report object F1 (Anderson et al., 2017) in the held-out COCO Benchmark. As the nocaps benchmark does not release its groundtruth captions, we instead report averaged number of mentioned objects (Ave. O) and CIDEr score for dummy captions that only contain copied object words (Object CIDEr, OC., details see Appendix). 1226 in-domain CIDEr SPICE Method near-domain CIDEr SPICE out-of-domain CIDEr SPICE Meteor Overall CIDEr SPICE Up-Down + BS Up-Down + ELMo + CBS NBT + BS NBT + CBS OSCARL + CBS + SCST 73.7 76.0 62.8 61.9 - 11.6 11.8 10.3 10.4 - 57.2 74.2 51.9 57.3 - 10.3 11.5 9.4 9.6 - 30.4 66.7 48.9 61.8 - 8.1 9.7 8.4 8.6 - 22.9 24.4"
2021.eacl-main.104,2020.findings-emnlp.165,0,0.21247,"nally, the ECOL model concatenates the above refined probabilities as follows: (8) P (ytli ) = ct,i · P (ytli |li ) P (yt ) = P (ytv ) P (ytl1 ) · · · (9) l f P (ytk ) Model Application Scope In this paper, we focus on the Novel Object Captioning task. However, in general, our copy mechanism is capable of copying any type of information. The Abstract Label approach is general to zero shot learning problems where novel items share characteristics with training instances. The Morphological Selector is also applicable to linguistic copy mechanisms in other contexts such as Commonsense Reasoning (Lin et al., 2020) where copied terms may require linguistic alignment with the generated text. 3.2 (10) where represents concatenation. Some novel object labels are included in the GPT2 vocabulary. Copying More Object Labels In this paper, we encourage the copying of object labels by using a suitable reward function in the Self-Critical Sequence Training (SCST) framework, which has proven effective for image captioning tasks. Compared with injecting additional loss terms together with the standard XE loss, using the SCST framework allows us to design arbitrary encouragement signals based on the inference outpu"
2021.eacl-main.104,Y18-1049,0,0.235449,"object labels. With these innovations, the ECOL-R model outperforms a SCST baseline and a strong inference encouragement baseline by a large margin. Our copy mechanism and caption generator incorporate two enhancements to better choose and incorporate novel objects: a) Abstract Labels which correspond to hypernyms of the object labels and facilitate knowledge transfer between objects appearing in training captions and novel objects; b) a Morphological Selector which determines the correct inflected form of the copied task specific object labels which is similar in purpose to that proposed in (Lu et al., 2018b). We evaluate the ECOL-R model on the novel object captioning benchmark nocaps (Agrawal et al., 2019) and held-out COCO (Hendricks et al., 2016). The ECOL-R model achieves a new state of the art on both benchmarks and generalizes well to in-domain images. 2 Related Work Popular Image Captioning models include LSTMbased (Anderson et al., 2018b) and Transformerbased decoders (Herdade et al., 2019; Cornia et al., 2020). The visual encoders are often neural object detectors (Anderson et al., 2018b; Wang et al., 2019) producing Region-of-Interest (ROI) vectors. To train the model to copy novel ob"
2021.emnlp-main.840,J15-2003,0,0.35235,"Missing"
2021.emnlp-main.840,P10-1124,0,0.671559,"es. Previous models learn predicates of a single valency, the number and types of arguments controlled by the predicate. Commonly these are binary graphs, which cannot model single-argument predicates like the entity states “is dead” or “is an author.” This means they miss a variety of entailments in text that could be used to answer questions such as our example. The Distributional Inclusion Hypothesis (DIH) (Dagan et al., 1999; Kartsaklis and Sadrzadeh, 2016) is a theory which has been used effectively in unsupervised learning of these same-valency entailment graphs (Geffet and Dagan, 2005; Berant et al., 2010; Hosseini, 2021). In this work the DIH is reinterpreted in a way which supports learning entailments between predicates of different valencies such as KILL(Mustard, Boddy)  DIE(Boddy). We extend the work of Hosseini et al. (2018) and develop a new Multivalent Entailment Graph (MGraph) where vertices may be predicates of different valencies. This results in new kinds of entailments that answer a broader range of questions including entity state. We further pose a true-false question answering task generated automatically from news text. Our model draws inferences across propositions of differ"
2021.emnlp-main.840,2021.case-1.6,1,0.681848,"Missing"
2021.emnlp-main.840,N19-1300,0,0.0279143,"ents of unary predicates (UU). WIN . IN (:person, BE . WINNER (:person) 5 Evaluation: Question Answering We pose an automatically generated QA task to evaluate our model explicitly for directional inference between binary and unary predicates, as we are not aware of any standard datasets for this probBE . WINNER (:person1) lem. Our task is to answer true-false questions about real events that are discussed in the news, for example, “Was Biden elected?” These types of OBLITERATE(:person1, :person2) questions are surprisingly difficult and frequently Bivalent Graphs require inference to answer (Clark et al., 2019). For this, entailment is especially useful: we must decide if the question (hypothesis) is true given a list Univalent Graphs of propositions from limited news text (premises), which are all likely to be phrased differently. Person Graph Person Person Graph Graph This task is designed independently of the BE . WINNER (:person) MGraph as a challenge in information retrieval. Positive questions made from binary and unary BE . CHAMPION (:person) predicates are selected directly from the news text using special criteria, and are then removed. From Figure 2: Bivalent graphs model entailments from"
2021.emnlp-main.840,N19-1423,0,0.029097,"the kill/die example, which are easy for humans. We generalize the DIH to learn entailments within and across valencies. Typing is very helpful for entailment graph learning (Berant et al., 2010; Lewis and Steedman, 2013; Hosseini et al., 2018). Inducing a type for each entity such as “person,” “location,” etc. enables generalized learning across instances and disambiguates word sense, e.g. “running a company” has different entailments than “running code.” We compare our model to several baselines, including strong pretrained language models in an unsupervised setting using similarity. BERT (Devlin et al., 2019) generates impressive word representations, even unsupervised (Petroni et al., 2019), which we compare with on a task of predicate inference. We further test RoBERTa (Liu et al., 2019) to show the impact of robust in-domain pretraining on the same architecture. These non-directional similarity models provide a strong baseline for evaluating directional entailment graphs. 3 Multivalent Distributional Inclusion Hypothesis tailments about one or more of the arguments arise from their roles in this eventuality. We may infer that “Mr. Boddy died” due to his role as a direct object in the killing/mu"
2021.emnlp-main.840,Q18-1048,1,0.901754,"dead” or “is an author.” This means they miss a variety of entailments in text that could be used to answer questions such as our example. The Distributional Inclusion Hypothesis (DIH) (Dagan et al., 1999; Kartsaklis and Sadrzadeh, 2016) is a theory which has been used effectively in unsupervised learning of these same-valency entailment graphs (Geffet and Dagan, 2005; Berant et al., 2010; Hosseini, 2021). In this work the DIH is reinterpreted in a way which supports learning entailments between predicates of different valencies such as KILL(Mustard, Boddy)  DIE(Boddy). We extend the work of Hosseini et al. (2018) and develop a new Multivalent Entailment Graph (MGraph) where vertices may be predicates of different valencies. This results in new kinds of entailments that answer a broader range of questions including entity state. We further pose a true-false question answering task generated automatically from news text. Our model draws inferences across propositions of different valencies to answer more questions than using same-valence entailment graphs. We also compare with several baselines, including unsupervised pretrained language models, and show that our directional entailment graphs succeed ov"
2021.emnlp-main.840,P19-1468,1,0.693194,"2010; Hosseini et al., 2018). These graphs frequently rely on the DIH for the local learning step to learn initial predicate entailments. The DIH states that for some predicates p and q, if the contextual features of p are included in those of q, then p entails q (Geffet and Dagan, 2005). In previous work predicate arguments are successfully used as these contextual features, but only predicates of the same valency are considered (e.g. binary predicates entail binary; unary entail unary), and further research computes additional edges in these same-valency graphs such as with link prediction (Hosseini et al., 2019). However, this leaves out crucial inferences that cross valencies such as the kill/die example, which are easy for humans. We generalize the DIH to learn entailments within and across valencies. Typing is very helpful for entailment graph learning (Berant et al., 2010; Lewis and Steedman, 2013; Hosseini et al., 2018). Inducing a type for each entity such as “person,” “location,” etc. enables generalized learning across instances and disambiguates word sense, e.g. “running a company” has different entailments than “running code.” We compare our model to several baselines, including strong pret"
2021.emnlp-main.840,C16-1268,0,0.0596099,"Missing"
2021.emnlp-main.840,P16-2041,0,0.337834,"Missing"
2021.emnlp-main.840,Q13-1015,1,0.905296,"previous work predicate arguments are successfully used as these contextual features, but only predicates of the same valency are considered (e.g. binary predicates entail binary; unary entail unary), and further research computes additional edges in these same-valency graphs such as with link prediction (Hosseini et al., 2019). However, this leaves out crucial inferences that cross valencies such as the kill/die example, which are easy for humans. We generalize the DIH to learn entailments within and across valencies. Typing is very helpful for entailment graph learning (Berant et al., 2010; Lewis and Steedman, 2013; Hosseini et al., 2018). Inducing a type for each entity such as “person,” “location,” etc. enables generalized learning across instances and disambiguates word sense, e.g. “running a company” has different entailments than “running code.” We compare our model to several baselines, including strong pretrained language models in an unsupervised setting using similarity. BERT (Devlin et al., 2019) generates impressive word representations, even unsupervised (Petroni et al., 2019), which we compare with on a task of predicate inference. We further test RoBERTa (Liu et al., 2019) to show the impa"
2021.emnlp-main.840,P98-2127,0,0.623102,"he argument pair ai ∈ {(em , en ) |em ∈ Et1 , en ∈ Et2 }. Here t1 , t2 ∈ T , and Et is the subset of entities of type t. For example, the predicate BUILD(:company, :thing) might have some feature f37 , the PMI of “build” with argument pair (Apple, iPhone). A Balanced Inclusion (BInc) score is calculated for the directed entailment from one predicate to another (Szpektor and Dagan, 2008). BInc is the geometric mean of two subscores: a directional score, Weeds Precision (Weeds and Weir, 2003), measuring how much one vector’s features “cover” the other’s; and a symmetrical score, Lin Similarity (Lin, 1998), which downweights infrequent predicates that cause spurious false positives. In this work we compute local binary graphs following Hosseini et al. (2018) and leverage the new MDIH to compute additional entailments for unaries and between valencies. To do this we compute a vector for each argument slot respecting its position in the predicate. For a predicate p, a slot vector (s) v(s) for s ∈ {1, 2} consists of features fi . We define τ (p, s) = t, the type of slot s in predicate p. (s) Each fi is the PMI of p and the argument in slot (s) s, ai ∈ Et . Slot vectors are computed for the slot in"
2021.emnlp-main.840,P05-1045,0,0.0198899,"G argument position which corresponds to its case (e.g. 1 for nominative, 2 for accusative), is appended to the predicate. Passive predicates are mapped to active ones. Modifiers such as negation and predicates like “planned to” as in “Professor Plum planned to attend” are also extracted in the predicate. We pay special attention to copular constructions, which always introduce stative predicates, rather than events (Vendler, 1967). These are interesting for modeling the properties of entities. 4.2 Learning Local Graphs Identified by the CoreNLP Named Entitiy Recogniser (Manning et al., 2014; Finkel et al., 2005). Here we number the typed arguments for demonstration to show which :person argument is in the entailment. 10761 includes Bivalent Graphs which contain the entailments of binary predicates (BB and BU edges), and separate Univalent Graphs which contain the entailments of unary predicates (only UU edges, since we do not allow a unary to entail a binary). We follow previous research and learn separate disjoint subgraphs for each typing, up to |T |2 bivalent and |T |univalent subgraphs given enough data. For example, we learn a bivalent (:person, :location) graph containing binary predicates such"
2021.emnlp-main.840,P05-1014,0,0.474229,"resented as directed edges. Previous models learn predicates of a single valency, the number and types of arguments controlled by the predicate. Commonly these are binary graphs, which cannot model single-argument predicates like the entity states “is dead” or “is an author.” This means they miss a variety of entailments in text that could be used to answer questions such as our example. The Distributional Inclusion Hypothesis (DIH) (Dagan et al., 1999; Kartsaklis and Sadrzadeh, 2016) is a theory which has been used effectively in unsupervised learning of these same-valency entailment graphs (Geffet and Dagan, 2005; Berant et al., 2010; Hosseini, 2021). In this work the DIH is reinterpreted in a way which supports learning entailments between predicates of different valencies such as KILL(Mustard, Boddy)  DIE(Boddy). We extend the work of Hosseini et al. (2018) and develop a new Multivalent Entailment Graph (MGraph) where vertices may be predicates of different valencies. This results in new kinds of entailments that answer a broader range of questions including entity state. We further pose a true-false question answering task generated automatically from news text. Our model draws inferences across p"
2021.emnlp-main.840,P14-5010,0,0.00376382,"s are stripped. The CCG argument position which corresponds to its case (e.g. 1 for nominative, 2 for accusative), is appended to the predicate. Passive predicates are mapped to active ones. Modifiers such as negation and predicates like “planned to” as in “Professor Plum planned to attend” are also extracted in the predicate. We pay special attention to copular constructions, which always introduce stative predicates, rather than events (Vendler, 1967). These are interesting for modeling the properties of entities. 4.2 Learning Local Graphs Identified by the CoreNLP Named Entitiy Recogniser (Manning et al., 2014; Finkel et al., 2005). Here we number the typed arguments for demonstration to show which :person argument is in the entailment. 10761 includes Bivalent Graphs which contain the entailments of binary predicates (BB and BU edges), and separate Univalent Graphs which contain the entailments of unary predicates (only UU edges, since we do not allow a unary to entail a binary). We follow previous research and learn separate disjoint subgraphs for each typing, up to |T |2 bivalent and |T |univalent subgraphs given enough data. For example, we learn a bivalent (:person, :location) graph containing"
2021.emnlp-main.840,P15-2070,0,0.0732372,"Missing"
2021.emnlp-main.840,D19-1250,0,0.0350132,"tailments within and across valencies. Typing is very helpful for entailment graph learning (Berant et al., 2010; Lewis and Steedman, 2013; Hosseini et al., 2018). Inducing a type for each entity such as “person,” “location,” etc. enables generalized learning across instances and disambiguates word sense, e.g. “running a company” has different entailments than “running code.” We compare our model to several baselines, including strong pretrained language models in an unsupervised setting using similarity. BERT (Devlin et al., 2019) generates impressive word representations, even unsupervised (Petroni et al., 2019), which we compare with on a task of predicate inference. We further test RoBERTa (Liu et al., 2019) to show the impact of robust in-domain pretraining on the same architecture. These non-directional similarity models provide a strong baseline for evaluating directional entailment graphs. 3 Multivalent Distributional Inclusion Hypothesis tailments about one or more of the arguments arise from their roles in this eventuality. We may infer that “Mr. Boddy died” due to his role as a direct object in the killing/murdering event. No other information is needed, including who murdered Mr. Boddy, whe"
2021.emnlp-main.840,D09-1001,0,0.0603039,"d the news text within this 3-day window is used as evidence to answer them. We ask questions as if happening presently in this time window to control for the variable of time, so we can ask ambiguous questions like “Did the Patriots win the Superbowl?” which may be “true” or not depending on the date and timespan. The small 3-day window size was chosen so multiple news stories about an event appear together, increasing the chances of finding question answers. Within each partition we do relation extraction in a process mirroring §4.1. 2. Selecting Positives. We adapt a selection process from Poon and Domingos (2009) to choose good questions which are interesting to a human and answerable from the partition text. First, we identify repeated entities that star in the events of the articles; these will yield interesting questions as well as ample textual evidence for answering them. In each partition we count the mentions of each entity pair (from binary propositions) and single 5.2 Question Answering Models entities (from unary and binary ones). The most frequent entities and pairs mentioned more than In each partition, models receive factual proposi5 times in the partition are selected. Predicates tions e"
2021.emnlp-main.840,N19-1020,1,0.858231,"Missing"
2021.emnlp-main.840,C08-1107,0,0.822054,"reading T would infer that H is most likely true.” From here, research has moved in ∗ Now at Google Research. 1 several directions. We study predicates, including The murder mystery board game Clue (also known as Cluedo) lends inspiration to this project. verbs and phrases that apply to arguments. 10758 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10758–10768 c November 7–11, 2021. 2021 Association for Computational Linguistics Research in predicate entailment graphs has evolved from “local” learning of entailment rules (Geffet and Dagan, 2005; Szpektor and Dagan, 2008) to later work on joint learning of “globalized” rules, overcoming sparsity in local graphs (Berant et al., 2010; Hosseini et al., 2018). These graphs frequently rely on the DIH for the local learning step to learn initial predicate entailments. The DIH states that for some predicates p and q, if the contextual features of p are included in those of q, then p entails q (Geffet and Dagan, 2005). In previous work predicate arguments are successfully used as these contextual features, but only predicates of the same valency are considered (e.g. binary predicates entail binary; unary entail unary)"
2021.emnlp-main.840,W03-1011,0,0.47742,"predicate p with corresponding vector v, v consists of features fi which are the pointwise mutual information (PMI) of p and the argument pair ai ∈ {(em , en ) |em ∈ Et1 , en ∈ Et2 }. Here t1 , t2 ∈ T , and Et is the subset of entities of type t. For example, the predicate BUILD(:company, :thing) might have some feature f37 , the PMI of “build” with argument pair (Apple, iPhone). A Balanced Inclusion (BInc) score is calculated for the directed entailment from one predicate to another (Szpektor and Dagan, 2008). BInc is the geometric mean of two subscores: a directional score, Weeds Precision (Weeds and Weir, 2003), measuring how much one vector’s features “cover” the other’s; and a symmetrical score, Lin Similarity (Lin, 1998), which downweights infrequent predicates that cause spurious false positives. In this work we compute local binary graphs following Hosseini et al. (2018) and leverage the new MDIH to compute additional entailments for unaries and between valencies. To do this we compute a vector for each argument slot respecting its position in the predicate. For a predicate p, a slot vector (s) v(s) for s ∈ {1, 2} consists of features fi . We define τ (p, s) = t, the type of slot s in predicate"
2021.emnlp-main.840,D13-1183,0,0.470293,"that the former entails being an author, while the latter entails being a programmer. 4 binary is a window into its higher-valency predicate, allowing higher-valency predicates to entail lower binaries and unaries. To learn these new kinds of connections we develop a method of local entailment rule learning using the MDIH. As in §2, the local step learns the initial directed edges of the entailment graph, which are further improved with global learning. Our local step learns entailments by machine-reading the NewsSpike corpus (2.3GB), which contains 550K news articles, or over 20M sentences (Zhang and Weld, 2013). NewsSpike consists of multi-source news articles collected within a fixed timeframe, and due to these properties the articles frequently discuss the same events but phrased in different ways, providing appropriate training evidence. Learning Multivalent Graphs We define an Entailment Graph as a directed graph of predicates and their entailments, G = (V, E). The vertices V are the set of predicates, where each argument has a type from the set of 49 FIGER base types T , e.g. TRAVEL . TO(:person, :location) ∈ V , and :person, :location ∈ T . The directed edges are E = {(v1 , v2 ) |v1 , v2 ∈ V i"
2021.findings-emnlp.238,D19-1522,0,0.13997,"ph because the corresponding triple was not found in the text (Hosseini et al., 2019; Broscheit et al., 2020). Figure 1a shows part of an example open-domain KG, in which the triple (Apple, own, Beats) is missing, but can be inferred using link prediction over all entities in the complete KG. 2790 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2790–2802 November 7–11, 2021. ©2021 Association for Computational Linguistics 's pu Previous work has applied standard link prediction methods such as TransE (Bordes et al., 2013), ConvE (Dettmers et al., 2018), or TuckER (Balazevic et al., 2019) to open-domain triples. These methods have been shown to be effective in learning the KG structure, but they are sub-optimal for open-domain link prediction because they ignore the textual context of the triples. Since the triples are extracted from text, they can be automatically grounded back to their contexts. Hence, in addition to the KG structure, the triple contexts can be used as input to the link prediction task. Figure 1b shows the context sentences that have given rise to the partial KG in Figure 1a.1 There are multiple clues in the contexts such as deal, $, cash, stock, and Financi"
2021.findings-emnlp.238,J15-2003,0,0.0282748,"Missing"
2021.findings-emnlp.238,P10-1124,0,0.0462528,"dd missing relations to the KG (e.g., own in in Figure 1a) by predicting the relations that hold between the entities of triple mentions in context (e.g., the context c1 in Figure 1b). Our experiments show that the proposed model for the contextual link prediction task significantly outperforms standard link prediction in open-domain KG completion. In addition, we investigate the interplay between contextual link prediction and context-independent entailments between relations, in the form of entailment graphs (EG). An EG has typed relations as nodes and entailment relation as directed edges (Berant et al., 2010, 2011, 2015; Hosseini et al., 2018; Hosseini, 2021). The type of each relation is determined by the types of its two entities. EGs are by definition context-independent, but they use relation types as a proxy of the context. Figure 1c shows a fragment of an EG showing that for example acquire entails own. Similar to opendomain KGs, EGs are constructed based on extracted triples from text. The entailment between two relations is predicted by computing a directional entailment score between them. It has been recently shown that the two tasks of open-domain link prediction and EG learning are co"
2021.findings-emnlp.238,P11-1062,0,0.0326858,"r training, but has two main differences: First, our contextual link prediction model outputs a directional score between relations in context (e.g., acquire in Figure 1) and hypothesis relations (e.g., own), while MTB learns a symmetric similarity score. Second, we can predict a score for any hypothesis relation as long as the relation is previously observed somewhere in the corpus with any other entities (§3.2). Relational Entailment Graphs. Earlier attempts take a local approach and predict entailment relations independently from each other (Lin and Pantel, 2001; Szpektor and Dagan, 2008). Berant et al. (2011, 2015) and Hosseini et al. (2018) propose a global approach where the dependencies between the entailment relations are taken into account. They first build a local typed EG for any plausible type pair. They then build global EGs that satisfy soft or hard constraints such as the transitivity of entailment. The constraints consider the structures both across typed EGs and inside each graph. In this work, we improve the local entailment scores, which in turn improves the global EGs. Hosseini et al. (2019) perform standard link prediction to add more coverage to the EGs by adding 3 Contextual Li"
2021.findings-emnlp.238,D19-1651,0,0.0538611,"Missing"
2021.findings-emnlp.238,2020.acl-main.209,0,0.0364414,"knowledge graph (KG) is constituted by a set of (subject, relation, object) triples such as (Apple, acquire, Beats). KGs have entities (subjects and objects) as nodes and relations as labeled edges. Manually-built KGs such as Freebase (Bollacker et al., 2008), Wikidata (Vrandeˇci´c and Krötzsch, 2014), or DBPedia (Lehmann et al., 2015) have a known set of hand-built relations. In contrast, the relation-labels of open-domain KGs are obtained from text rather than fixed. Open-domain KGs can be constructed by applying parsers or openinformation extraction methods to text (Hosseini et al., 2019; Broscheit et al., 2020). ∗ (a) Figure 1: a) Part of an example KG. The relation own is missing, but can be predicted from the rest of the KG and the triple contexts using contextual link prediction. b) The contexts c1 and c2 from which we have extracted the KG triples. The relation tokens are boldfaced and entities are italic. The contextual link prediction task predicts relations that hold between the entitypair in a grounded triple. For example, we predict that the relation own should be added between Apple and Beats. c) An example EG of type Organization, Organization. The contextual link prediction and EG learni"
2021.findings-emnlp.238,N19-1423,0,0.0397277,"is because these clues could have been seen around occurrences of other entity-pairs of the same type (e.g., Facebook and Whatsapp) that are connected by acquire, ’s purchase of, and own relations. In this paper, we propose the new task of contextual link prediction for such open-domain graphs: Given a triple (e1 , r, e2 ) grounded in context with the relation r holding between the entities e1 and e2 , our goal is to predict all the other relations that hold between the two entities. We present a model that uses contextualized relation embeddings to predict new relations. We start with BERT (Devlin et al., 2019) pre-trained embeddings and fine-tune them with a novel unsupervised contextual link prediction objective function. After training the contextual link prediction model, we can add missing relations to the KG (e.g., own in in Figure 1a) by predicting the relations that hold between the entities of triple mentions in context (e.g., the context c1 in Figure 1b). Our experiments show that the proposed model for the contextual link prediction task significantly outperforms standard link prediction in open-domain KG completion. In addition, we investigate the interplay between contextual link predic"
2021.findings-emnlp.238,2020.textgraphs-1.7,1,0.829589,"Missing"
2021.findings-emnlp.238,2021.eacl-main.316,0,0.0236904,"Missing"
2021.findings-emnlp.238,P19-1468,1,0.820203,"arch. headphone Apple A knowledge graph (KG) is constituted by a set of (subject, relation, object) triples such as (Apple, acquire, Beats). KGs have entities (subjects and objects) as nodes and relations as labeled edges. Manually-built KGs such as Freebase (Bollacker et al., 2008), Wikidata (Vrandeˇci´c and Krötzsch, 2014), or DBPedia (Lehmann et al., 2015) have a known set of hand-built relations. In contrast, the relation-labels of open-domain KGs are obtained from text rather than fixed. Open-domain KGs can be constructed by applying parsers or openinformation extraction methods to text (Hosseini et al., 2019; Broscheit et al., 2020). ∗ (a) Figure 1: a) Part of an example KG. The relation own is missing, but can be predicted from the rest of the KG and the triple contexts using contextual link prediction. b) The contexts c1 and c2 from which we have extracted the KG triples. The relation tokens are boldfaced and entities are italic. The contextual link prediction task predicts relations that hold between the entitypair in a grounded triple. For example, we predict that the relation own should be added between Apple and Beats. c) An example EG of type Organization, Organization. The contextual link"
2021.findings-emnlp.238,2020.tacl-1.28,0,0.0471479,"kens, whereas we use with unequal types, we do not need the flag as the the natural text associated with the triples. order is obvious and set o(r) = 0. For relations Extracting Factual Knowledge from Prewith identical types, we set o(r) = 0 if the entities Trained Language Models. These works form are in the original order and o(r) = 1, otherwise. a prompt where an entity is missing (e.g., Apple A triple mention is a triple grounded in its texacquire [MASK] ), and ask the language models tual context. We define a triple mention as a tuto predict the masked entity (Petroni et al., 2019, 2020; Jiang et al., 2020; Bouraoui et al., 2020; Ha- 3 For brevity, we drop the types when they are obvious. 2792 Model ple m = (e1 , r, e2 , c, s), where r ∈ R is a relation and e1 , e2 ∈ E are entities. The sub-word token sequence c = [c0 , . . . , cn ] is the textual context of the triple including the surface form of the relation and entity-pair.4 The pair s = (s1 , s2 ) indicates the indices of the first and last relation tokens. An example triple mention in Figure 1b is (Apple,acquire,Beats,c   2 ,[9, 11]). We denote by D= (ei,1 , ri , ei,2 , ci , si ) i∈{1,...,N } the set of all triple mentions. We define th"
2021.findings-emnlp.238,W19-4002,0,0.0628762,"Missing"
2021.findings-emnlp.238,2021.eacl-main.108,0,0.0439871,"Missing"
2021.findings-emnlp.238,P16-2041,0,0.0517452,"Missing"
2021.findings-emnlp.238,P19-1279,0,0.0277664,"es as well as predicted ones from contextual link prediction. We build state-of-the-art EGs. • We show that EGs in turn improve contextual link prediction. • We release a dataset containing the extracted triples grounded in context, for future research. 2791 Our code and data are available at https://github. com/mjhosseini/open_contextual_link_ pred. 2 Related Work viv et al., 2021). These models do not probe for relations because a) They face technical challenges in processing multi-token relations; and b) Relations can be expressed in many different ways. The matching-the-blank (MTB) model (Soares et al., 2019) learns relation embeddings by encouraging relations that share the same entity-pairs to have similar embeddings. This is similar to our training, but has two main differences: First, our contextual link prediction model outputs a directional score between relations in context (e.g., acquire in Figure 1) and hypothesis relations (e.g., own), while MTB learns a symmetric similarity score. Second, we can predict a score for any hypothesis relation as long as the relation is previously observed somewhere in the corpus with any other entities (§3.2). Relational Entailment Graphs. Earlier attempts"
2021.findings-emnlp.238,N15-1098,0,0.0298638,"upervised R(t1 , t2 ) as the set of relations with types t1 , t2 , model that fine-tunes pre-trained LMs directly on a or t2 , t1 . For example, R(Person, Location) intraining portion of entailment datasets. They report better results than EGs, but our focus is different. cludes born in(Person,Location), birthplace of (Location,Person), etc. Similarly, we define R(e1 , e2 ) Unlike their method, our approach is unsupervised as the set of relations r ∈ R such that (e1 , r, e2 ) is and is not capable of learning potential artifacts a valid (extracted) triple. For example, R(Barack from datasets (Levy et al., 2015). In addition, we Obama, Hawaii) includes born in3 , visit, etc. explicitly build EGs by doing machine-reading over Link prediction and entailment can hold between large text corpora, and hence can explain the basis relations with the same entity order or the reverse for the beliefs captured in them. order. When the two entity types are identical, we Pre-trained LMs for Link Prediction. KGkeep two copies of the relations one for each entity BERT (Yao et al., 2019) uses contextual represenorder. For example, acquire(Org1 ,Org2 ) predicts tations for KG completion. However, they form be part of"
2021.findings-emnlp.238,P98-2127,0,0.0947191,"Missing"
2021.findings-emnlp.238,2021.ccl-1.108,0,0.0396328,"Missing"
2021.findings-emnlp.238,2021.emnlp-main.840,1,0.762734,"Missing"
2021.findings-emnlp.238,C08-1107,0,0.294776,"ings. This is similar to our training, but has two main differences: First, our contextual link prediction model outputs a directional score between relations in context (e.g., acquire in Figure 1) and hypothesis relations (e.g., own), while MTB learns a symmetric similarity score. Second, we can predict a score for any hypothesis relation as long as the relation is previously observed somewhere in the corpus with any other entities (§3.2). Relational Entailment Graphs. Earlier attempts take a local approach and predict entailment relations independently from each other (Lin and Pantel, 2001; Szpektor and Dagan, 2008). Berant et al. (2011, 2015) and Hosseini et al. (2018) propose a global approach where the dependencies between the entailment relations are taken into account. They first build a local typed EG for any plausible type pair. They then build global EGs that satisfy soft or hard constraints such as the transitivity of entailment. The constraints consider the structures both across typed EGs and inside each graph. In this work, we improve the local entailment scores, which in turn improves the global EGs. Hosseini et al. (2019) perform standard link prediction to add more coverage to the EGs by a"
2021.findings-emnlp.238,D13-1183,0,0.0319067,"Missing"
2021.findings-emnlp.238,D19-1250,0,0.0573115,"Missing"
2021.insights-1.16,J15-2003,0,0.0551277,"Missing"
2021.insights-1.16,P11-1062,0,0.0408107,"lity-unaware: the model learns from both asserted and modal predications. Our contributions are 1) a comparison of Entailment Graphs learned from modal and nonmodal data, showing (counterintuitively) that ignoring modal distinctions in fact improves Entailment Graph-learning, and 2) insights as to whether this effect applies uniformly across different subdomains. 2 Background Entailment rules specify directional inferences between linguistic predicates (Szpektor and Dagan, 2008), and can be stored in an Entailment Graph, whose global structural properties can be used to learn more accurately (Berant et al., 2011, 2015). * Equal contribution They are defined as a directed graph G = {N, E}, 1 Assuming a democratic election. We use the typical defin which the nodes N are typed predicates and inition of the premise most likely entailing the hypothesis (Dagan et al., 2006) edges E represent the entailment relation. The lex110 Proceedings of the Second Workshop on Insights from Negative Results in NLP, pages 110–116 November 10, 2021. ©2021 Association for Computational Linguistics ISBN 978-1-954085-93-0 Category ∅ Modal operator Conditional Counterfactual Propositional attitude Example Protesters attacked"
2021.insights-1.16,2021.case-1.6,1,0.845788,"Missing"
2021.insights-1.16,S14-1009,0,0.0215041,"2016; Adel and Schütze, 2017). Recently, Bijl de Vroe et al. (2021) designed a parser similar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect on learning Entailment Graphs. ical entailment knowledge stored within them is useful for Question Answering (McKenna et al., 2021), as well as other tasks such as email categorisation (Eichler et al., 2014), relation extraction (Eichler et al., 2017) and link prediction (Hosseini et al., 2019). A subgraph containing predicates of a type-pair (e.g. PERSON-LOCATION) can be learned in an unsupervised way from collections of multiplysourced text. A vector of argument-pair counts for every predicate is first machine read from the 3 Methods corpus. Typically, relation extraction systems used for reading these corpora ignore modal modifiers, We extend relation extraction to pay attention to possibly introducing noise in the graph. Next, a modality, so that we can distinguish modal and non(directed) sim"
2021.insights-1.16,S17-1026,0,0.0239934,"l de Vroe et al. (2021) designed a parser similar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect on learning Entailment Graphs. ical entailment knowledge stored within them is useful for Question Answering (McKenna et al., 2021), as well as other tasks such as email categorisation (Eichler et al., 2014), relation extraction (Eichler et al., 2017) and link prediction (Hosseini et al., 2019). A subgraph containing predicates of a type-pair (e.g. PERSON-LOCATION) can be learned in an unsupervised way from collections of multiplysourced text. A vector of argument-pair counts for every predicate is first machine read from the 3 Methods corpus. Typically, relation extraction systems used for reading these corpora ignore modal modifiers, We extend relation extraction to pay attention to possibly introducing noise in the graph. Next, a modality, so that we can distinguish modal and non(directed) similarity score (e.g. DIRT (Lin and Pan- modal"
2021.insights-1.16,Q18-1048,1,0.686663,"scriptions of eventualities in the news, observing directional co-occurrences of typed predicates and their arguments. For example, we expect to observe all the arguments of being president, such as Biden and Obama, also to be encountered in a sufficiently large multiply-sourced body of text as arguments of running for president, but not the other way around (Hillary Clinton will run but not be president). However, if all the reports of Clinton might be president are extracted as be_president(Clinton), one might expect the learning signal to be confusing to the algorithm. We use the method of Hosseini et al. (2018) combined with a modality parser (Bijl de Vroe et al., 2021) to construct typed Entailment Graphs from raw text corpora under two different settings. Modality-aware: modal predications are removed from the data entirely, and modality-unaware: the model learns from both asserted and modal predications. Our contributions are 1) a comparison of Entailment Graphs learned from modal and nonmodal data, showing (counterintuitively) that ignoring modal distinctions in fact improves Entailment Graph-learning, and 2) insights as to whether this effect applies uniformly across different subdomains. 2 Bac"
2021.insights-1.16,P19-1468,1,0.825632,"imilar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect on learning Entailment Graphs. ical entailment knowledge stored within them is useful for Question Answering (McKenna et al., 2021), as well as other tasks such as email categorisation (Eichler et al., 2014), relation extraction (Eichler et al., 2017) and link prediction (Hosseini et al., 2019). A subgraph containing predicates of a type-pair (e.g. PERSON-LOCATION) can be learned in an unsupervised way from collections of multiplysourced text. A vector of argument-pair counts for every predicate is first machine read from the 3 Methods corpus. Typically, relation extraction systems used for reading these corpora ignore modal modifiers, We extend relation extraction to pay attention to possibly introducing noise in the graph. Next, a modality, so that we can distinguish modal and non(directed) similarity score (e.g. DIRT (Lin and Pan- modal relations in the Entailment Graph mining te"
2021.insights-1.16,W08-0607,0,0.0666705,"NLP, pages 110–116 November 10, 2021. ©2021 Association for Computational Linguistics ISBN 978-1-954085-93-0 Category ∅ Modal operator Conditional Counterfactual Propositional attitude Example Protesters attacked the police Protesters may have attacked the police If protesters attack the police... Had protesters attacked the police... Journalists said that protesters attacked the police Table 1: Modality categories Answering and Knowledge Base Population (Karttunen and Zaenen, 2005; Morante and Daelemans, 2012b). Early approaches to detecting modality focused on lexicon design (Szarvas, 2008; Kilicoglu and Bergler, 2008; Baker et al., 2010), with later approaches using machine learning over annotated corpora (Morante and Daelemans, 2009; Rei and Briscoe, 2010; Jean et al., 2016; Adel and Schütze, 2017). Recently, Bijl de Vroe et al. (2021) designed a parser similar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect on learning Entailment Graphs"
2021.insights-1.16,P16-2041,0,0.0303899,"Missing"
2021.insights-1.16,2021.emnlp-main.840,1,0.759046,"Missing"
2021.insights-1.16,P07-1125,0,0.0738546,"Missing"
2021.insights-1.16,W09-1304,0,0.0511293,"Modal operator Conditional Counterfactual Propositional attitude Example Protesters attacked the police Protesters may have attacked the police If protesters attack the police... Had protesters attacked the police... Journalists said that protesters attacked the police Table 1: Modality categories Answering and Knowledge Base Population (Karttunen and Zaenen, 2005; Morante and Daelemans, 2012b). Early approaches to detecting modality focused on lexicon design (Szarvas, 2008; Kilicoglu and Bergler, 2008; Baker et al., 2010), with later approaches using machine learning over annotated corpora (Morante and Daelemans, 2009; Rei and Briscoe, 2010; Jean et al., 2016; Adel and Schütze, 2017). Recently, Bijl de Vroe et al. (2021) designed a parser similar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect on learning Entailment Graphs. ical entailment knowledge stored within them is useful for Question Answering (McKenna et al., 2021), as well as othe"
2021.insights-1.16,W10-3008,0,0.0218177,"ounterfactual Propositional attitude Example Protesters attacked the police Protesters may have attacked the police If protesters attack the police... Had protesters attacked the police... Journalists said that protesters attacked the police Table 1: Modality categories Answering and Knowledge Base Population (Karttunen and Zaenen, 2005; Morante and Daelemans, 2012b). Early approaches to detecting modality focused on lexicon design (Szarvas, 2008; Kilicoglu and Bergler, 2008; Baker et al., 2010), with later approaches using machine learning over annotated corpora (Morante and Daelemans, 2009; Rei and Briscoe, 2010; Jean et al., 2016; Adel and Schütze, 2017). Recently, Bijl de Vroe et al. (2021) designed a parser similar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect on learning Entailment Graphs. ical entailment knowledge stored within them is useful for Question Answering (McKenna et al., 2021), as well as other tasks such as email c"
2021.insights-1.16,N06-1005,0,0.102783,"rante and Daelemans, 2012b). Early approaches to detecting modality focused on lexicon design (Szarvas, 2008; Kilicoglu and Bergler, 2008; Baker et al., 2010), with later approaches using machine learning over annotated corpora (Morante and Daelemans, 2009; Rei and Briscoe, 2010; Jean et al., 2016; Adel and Schütze, 2017). Recently, Bijl de Vroe et al. (2021) designed a parser similar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect on learning Entailment Graphs. ical entailment knowledge stored within them is useful for Question Answering (McKenna et al., 2021), as well as other tasks such as email categorisation (Eichler et al., 2014), relation extraction (Eichler et al., 2017) and link prediction (Hosseini et al., 2019). A subgraph containing predicates of a type-pair (e.g. PERSON-LOCATION) can be learned in an unsupervised way from collections of multiplysourced text. A vector of argument-pair counts for every predicate is first"
2021.insights-1.16,N19-1020,1,0.868453,"Missing"
2021.insights-1.16,P08-1033,0,0.0120881,"ive Results in NLP, pages 110–116 November 10, 2021. ©2021 Association for Computational Linguistics ISBN 978-1-954085-93-0 Category ∅ Modal operator Conditional Counterfactual Propositional attitude Example Protesters attacked the police Protesters may have attacked the police If protesters attack the police... Had protesters attacked the police... Journalists said that protesters attacked the police Table 1: Modality categories Answering and Knowledge Base Population (Karttunen and Zaenen, 2005; Morante and Daelemans, 2012b). Early approaches to detecting modality focused on lexicon design (Szarvas, 2008; Kilicoglu and Bergler, 2008; Baker et al., 2010), with later approaches using machine learning over annotated corpora (Morante and Daelemans, 2009; Rei and Briscoe, 2010; Jean et al., 2016; Adel and Schütze, 2017). Recently, Bijl de Vroe et al. (2021) designed a parser similar to that by Baker et al. (2010), to cover a wider range of phenomena, including conditionality and propositional attitude. While modality annotation is clearly useful for recognising entailment from a given text (Snow et al., 2006; De Marneffe et al., 2006), to our knowledge no research has been conducted on its effect"
2021.insights-1.16,C08-1107,0,0.204037,"ent Graphs from raw text corpora under two different settings. Modality-aware: modal predications are removed from the data entirely, and modality-unaware: the model learns from both asserted and modal predications. Our contributions are 1) a comparison of Entailment Graphs learned from modal and nonmodal data, showing (counterintuitively) that ignoring modal distinctions in fact improves Entailment Graph-learning, and 2) insights as to whether this effect applies uniformly across different subdomains. 2 Background Entailment rules specify directional inferences between linguistic predicates (Szpektor and Dagan, 2008), and can be stored in an Entailment Graph, whose global structural properties can be used to learn more accurately (Berant et al., 2011, 2015). * Equal contribution They are defined as a directed graph G = {N, E}, 1 Assuming a democratic election. We use the typical defin which the nodes N are typed predicates and inition of the premise most likely entailing the hypothesis (Dagan et al., 2006) edges E represent the entailment relation. The lex110 Proceedings of the Second Workshop on Insights from Negative Results in NLP, pages 110–116 November 10, 2021. ©2021 Association for Computational Li"
2021.insights-1.16,W03-1011,0,0.0155796,"predicates of a type-pair (e.g. PERSON-LOCATION) can be learned in an unsupervised way from collections of multiplysourced text. A vector of argument-pair counts for every predicate is first machine read from the 3 Methods corpus. Typically, relation extraction systems used for reading these corpora ignore modal modifiers, We extend relation extraction to pay attention to possibly introducing noise in the graph. Next, a modality, so that we can distinguish modal and non(directed) similarity score (e.g. DIRT (Lin and Pan- modal relations in the Entailment Graph mining tel, 2001), Weed’s score (Weeds and Weir, 2003) algorithm. This allows us to investigate the impact or BInc (Szpektor and Dagan, 2008)) is computed of modalised predicate data on the accuracy of between the vectors, producing a local entailment learned entailment edges. score between each predicate pair. Then a globaliWe extract binary relations of the form arg1sation process such as the soft constraints algorithm predicate-arg2 using M O NTEE, an open-domain of Hosseini et al. (2018), which transfers informa- modality-aware relation extraction system (Bijl de tion both within and between type-pair subgraphs, Vroe et al., 2021). M O NTEE u"
2021.insights-1.16,D13-1183,0,0.0742463,"Missing"
2021.naacl-main.268,D11-1142,0,0.738976,"over a system without this information. https://github.com/drevicko/OpenKI 1 Introduction Mark Johnson Oracle Digital Assistant mark.mj.johnson@ oracle.com text and implied by the text in combination with existing knowledge. Previous work following this approach draws on patterns in the curated knowledge graph in combination with the graph of entity mentions in texts, allowing prediction of new knowledge base relations (Riedel et al., 2013; Verga et al., 2015, 2017). Zhang et al. (2019) extend this work by incorporating text predicates connecting entity mentions extracted using OpenIE tools (Fader et al., 2011; Lockard et al., 2019) and introducing the concept of “entity neighbourhoods” consisting of the binary OpenIE predicates and knowledge base relations1 that occur with a given entity as their subject or object. Drawing on the success of text based representations incorporated into entity recognition tasks (Gillick et al., 2019), we extend Zhang et.al.’s model by incorporating text based embeddings of entities and relations into the entity neighbourhood representations. Texts are drawn from knowledge base metadata and occurrences in source texts. We use fasttext (Mikolov et al., 2018) word embe"
2021.naacl-main.268,K19-1049,0,0.0193845,"n with the graph of entity mentions in texts, allowing prediction of new knowledge base relations (Riedel et al., 2013; Verga et al., 2015, 2017). Zhang et al. (2019) extend this work by incorporating text predicates connecting entity mentions extracted using OpenIE tools (Fader et al., 2011; Lockard et al., 2019) and introducing the concept of “entity neighbourhoods” consisting of the binary OpenIE predicates and knowledge base relations1 that occur with a given entity as their subject or object. Drawing on the success of text based representations incorporated into entity recognition tasks (Gillick et al., 2019), we extend Zhang et.al.’s model by incorporating text based embeddings of entities and relations into the entity neighbourhood representations. Texts are drawn from knowledge base metadata and occurrences in source texts. We use fasttext (Mikolov et al., 2018) word embeddings and BERT (Devlin et al., 2018) to obtain text embeddings. The resulting models achieve state of the art results on two knowledge base extension data sets. Curated knowledge repositories such as knowledge bases and relational databases provide powerful tools for many practical knowledge related tasks. They require, howeve"
2021.naacl-main.268,P11-1055,0,0.0733193,"ctly expressed in individual texts, whereas we seek to utilise the combined knowledge from both a collection of texts and a knowledge base, allowing implicit and automatic association between expressions in texts and knowledge base relations and inference of propositions not directly expressed in individual texts. A number of works present a distant supervision approach that utilises entity pairs in texts as a signal for the presence of propositions that may be incorporated in a knowledge base. This signal is inherently noisy, and several approaches have been devised do deal with this (e.g.: (Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016)). Closer to what we propose, Han et al. (2018) propose a neural attention mechanism between a knowlege graph and supporting texts, outperforming previous approaches. These approaches do not utilise graph 2 https://paperswithcode.com/sota/ relation-extraction-on-tacred information in the form of connections between the texts and can only extract relations explicitly mentioned in the texts. We note that the OpenKI model (Zhang et al., 2019), which we use as a baseline, outperforms these models (see Table 3). 3 Enhanced Entity Neighbourhood Model We build on"
2021.naacl-main.268,W12-3016,0,0.0193446,"rd scoring component that combines aggregated neighbour representations (Equation 2) with a “query attention mechanism” similar to (Verga et al., 2017) — see (Zhang et al., 2019) for details. For text enhanced models we replace the neighbour representations with Equation 3. 4 Data Sets Following (Zhang et al., 2019) we test our models on two data sets: 1) English language extractions from the New Your Times (NYT) (Riedel et al., 2010) consisting of sentences with named entities identified and linked to FreeBase (FB) and 2) REVERB (Fader et al., 2011) (an OpenIE tool) extractions from ClueWeb (Lin et al., 2012) (English language web texts) as preprocessed by OpenKI authors3 also with entities linked to FreeBase. For the NYT data, we use sentences as proxies for text predicates and for predicate texts we use whole sentences (including the entity mentions). Texts for Freebase relations are derived 3 https://github.com/zhangdongxu/ relation-inference-naacl19 3431 subj/obj Table 1: Data Statistics OpenIE NYT Training data # entity pairs # without KB relations # KB relation types # Predicate types 40,878 0 250 124,836 377,013 359,197 57 320,711 Test data # test triples 4,938 1,761 from their identifiers,"
2021.naacl-main.268,P16-1200,0,0.149227,"s we seek to utilise the combined knowledge from both a collection of texts and a knowledge base, allowing implicit and automatic association between expressions in texts and knowledge base relations and inference of propositions not directly expressed in individual texts. A number of works present a distant supervision approach that utilises entity pairs in texts as a signal for the presence of propositions that may be incorporated in a knowledge base. This signal is inherently noisy, and several approaches have been devised do deal with this (e.g.: (Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016)). Closer to what we propose, Han et al. (2018) propose a neural attention mechanism between a knowlege graph and supporting texts, outperforming previous approaches. These approaches do not utilise graph 2 https://paperswithcode.com/sota/ relation-extraction-on-tacred information in the form of connections between the texts and can only extract relations explicitly mentioned in the texts. We note that the OpenKI model (Zhang et al., 2019), which we use as a baseline, outperforms these models (see Table 3). 3 Enhanced Entity Neighbourhood Model We build on the Entity Neighbourhood Encoding (EN"
2021.naacl-main.268,N19-1309,0,0.087354,"ut this information. https://github.com/drevicko/OpenKI 1 Introduction Mark Johnson Oracle Digital Assistant mark.mj.johnson@ oracle.com text and implied by the text in combination with existing knowledge. Previous work following this approach draws on patterns in the curated knowledge graph in combination with the graph of entity mentions in texts, allowing prediction of new knowledge base relations (Riedel et al., 2013; Verga et al., 2015, 2017). Zhang et al. (2019) extend this work by incorporating text predicates connecting entity mentions extracted using OpenIE tools (Fader et al., 2011; Lockard et al., 2019) and introducing the concept of “entity neighbourhoods” consisting of the binary OpenIE predicates and knowledge base relations1 that occur with a given entity as their subject or object. Drawing on the success of text based representations incorporated into entity recognition tasks (Gillick et al., 2019), we extend Zhang et.al.’s model by incorporating text based embeddings of entities and relations into the entity neighbourhood representations. Texts are drawn from knowledge base metadata and occurrences in source texts. We use fasttext (Mikolov et al., 2018) word embeddings and BERT (Devlin"
2021.naacl-main.268,D19-1069,0,0.0229317,"3). Details for object entity agg representations (vobj , pink triangles) are similar to subject entity representations. The dot product of enhanced aggregate representations and enhanced representation of the query relation (eqn. 4) are pased through activation functions fs/o and summed with learnable weights αsubj/obj . Here fs and fo are sigmoid functions with trainable temperature asubj/obj and threshold bsubj/obj (eqn. 5) (e.g.: (Cohen et al., 2020; Wang et al., 2019; Peters et al., 2019)2 ) including the creation of many annotated data sets (e.g.: (Zhang et al., 2017; Alt et al., 2020; Mesquita et al., 2019; Elsahar et al., 2019)). These tasks consider only the recognition of knowledge directly expressed in individual texts, whereas we seek to utilise the combined knowledge from both a collection of texts and a knowledge base, allowing implicit and automatic association between expressions in texts and knowledge base relations and inference of propositions not directly expressed in individual texts. A number of works present a distant supervision approach that utilises entity pairs in texts as a signal for the presence of propositions that may be incorporated in a knowledge base. This signal is"
2021.naacl-main.268,L18-1008,0,0.191412,"penIE tools (Fader et al., 2011; Lockard et al., 2019) and introducing the concept of “entity neighbourhoods” consisting of the binary OpenIE predicates and knowledge base relations1 that occur with a given entity as their subject or object. Drawing on the success of text based representations incorporated into entity recognition tasks (Gillick et al., 2019), we extend Zhang et.al.’s model by incorporating text based embeddings of entities and relations into the entity neighbourhood representations. Texts are drawn from knowledge base metadata and occurrences in source texts. We use fasttext (Mikolov et al., 2018) word embeddings and BERT (Devlin et al., 2018) to obtain text embeddings. The resulting models achieve state of the art results on two knowledge base extension data sets. Curated knowledge repositories such as knowledge bases and relational databases provide powerful tools for many practical knowledge related tasks. They require, however, substantial effort to create and maintain. Many applications deal with knowledge that is continuously changing, present2 Related Work ing prohibitive maintenance costs and limiting the utility of explicit knowledge representation tech- Open information extra"
2021.naacl-main.268,D19-1005,0,0.0612142,"Missing"
2021.naacl-main.268,N13-1008,0,0.0295495,"of OpenKi that incorporates embeddings of text-based representations of the entities and the relations. We demonstrate that this results in a substantial performance increase over a system without this information. https://github.com/drevicko/OpenKI 1 Introduction Mark Johnson Oracle Digital Assistant mark.mj.johnson@ oracle.com text and implied by the text in combination with existing knowledge. Previous work following this approach draws on patterns in the curated knowledge graph in combination with the graph of entity mentions in texts, allowing prediction of new knowledge base relations (Riedel et al., 2013; Verga et al., 2015, 2017). Zhang et al. (2019) extend this work by incorporating text predicates connecting entity mentions extracted using OpenIE tools (Fader et al., 2011; Lockard et al., 2019) and introducing the concept of “entity neighbourhoods” consisting of the binary OpenIE predicates and knowledge base relations1 that occur with a given entity as their subject or object. Drawing on the success of text based representations incorporated into entity recognition tasks (Gillick et al., 2019), we extend Zhang et.al.’s model by incorporating text based embeddings of entities and relations"
2021.naacl-main.268,N18-1081,0,0.0243564,"ge related tasks. They require, however, substantial effort to create and maintain. Many applications deal with knowledge that is continuously changing, present2 Related Work ing prohibitive maintenance costs and limiting the utility of explicit knowledge representation tech- Open information extraction (OpenIE) attempts nologies. The new knowledge is often available in to find relations expressed in collections of text based formats such as reports, news items and texts through identification of entity and relation memos. In this work, we use the term “proposition” spans (Fader et al., 2011; Stanovsky et al., 2018). to describe a triple (e1 , r, e2 ) that indicates that a Our work can be taken as an approach to incorrelation r holds between two entities e1 and e2 . porate this extracted information into an existing Work in the field has largely focussed on either knowledge base. extracting propositions directly from text or inferRelation extraction, the identification of relaring missing propositions by examining knowledge tions expressed in text between given entity mengraphs. What we are interested in here combines tions, has received much attention in recent years the two in a single model, utilising"
2021.naacl-main.268,E17-1058,0,0.0191882,", αobj ∈ R. The mixing weights are passed through the ReLU function to ensure that the raw scores can only contribute positively to the final score without canceling each other out. score(s, p, o) (5) EN E = ReLU (αsubj ) · σ(asubj Ssubj (s, p) + bsubj ) EN E + ReLU (αobj ) · σ(aobj Sobj (p, o) + bobj ) The resulting score, trained with a max-margin loss, allows us to rank propositions, with true propositions ranked higher. The full OpenKi model incorporates a third scoring component that combines aggregated neighbour representations (Equation 2) with a “query attention mechanism” similar to (Verga et al., 2017) — see (Zhang et al., 2019) for details. For text enhanced models we replace the neighbour representations with Equation 3. 4 Data Sets Following (Zhang et al., 2019) we test our models on two data sets: 1) English language extractions from the New Your Times (NYT) (Riedel et al., 2010) consisting of sentences with named entities identified and linked to FreeBase (FB) and 2) REVERB (Fader et al., 2011) (an OpenIE tool) extractions from ClueWeb (Lin et al., 2012) (English language web texts) as preprocessed by OpenKI authors3 also with entities linked to FreeBase. For the NYT data, we use sente"
2021.naacl-main.268,D15-1203,0,0.0235225,"idual texts, whereas we seek to utilise the combined knowledge from both a collection of texts and a knowledge base, allowing implicit and automatic association between expressions in texts and knowledge base relations and inference of propositions not directly expressed in individual texts. A number of works present a distant supervision approach that utilises entity pairs in texts as a signal for the presence of propositions that may be incorporated in a knowledge base. This signal is inherently noisy, and several approaches have been devised do deal with this (e.g.: (Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016)). Closer to what we propose, Han et al. (2018) propose a neural attention mechanism between a knowlege graph and supporting texts, outperforming previous approaches. These approaches do not utilise graph 2 https://paperswithcode.com/sota/ relation-extraction-on-tacred information in the form of connections between the texts and can only extract relations explicitly mentioned in the texts. We note that the OpenKI model (Zhang et al., 2019), which we use as a baseline, outperforms these models (see Table 3). 3 Enhanced Entity Neighbourhood Model We build on the Entity Neighbo"
A00-2021,P99-1069,1,0.895577,"stochastic versions of HPSGs, categorial grammars and transformational grammars. 1 Introduction ""Unification-based"" Grammars (UBGs) can capture a wide variety of linguistically important syntactic and semantic constraints. However, because these constraints can be non-local or context-sensitive, developing stochastic versions of UBGs and associated estimation procedures is not as straight-forward as it is for, e.g., PCFGs. Recent work has shown how to define probability distributions over the parses of UBGs (Abney, 1997) and efficiently estimate and use conditional probabilities for parsing (Johnson et al., 1999). Like most other practical stochastic grammar estimation procedures, this latter estimation procedure requires a parsed training corpus. Unfortunately, large parsed UBG corpora are not yet available. This restricts the kinds of models one can realistically expect to be able to estimate. For example, a model incorporating lexical selectional preferences of the kind * This research was supported by NSF awards 9720368, 9870676 and 9812169. 154 described below might have tens or hundreds of thousands of parameters, which one could not reasonably a t t e m p t to estimate from a corpus with on the"
A00-2021,P99-1014,1,0.822777,"ish National Corpus (Carroll and Rooth, 1998). We based our auxiliary distribution on 3.7 million (g, r, a) tuples (belonging to 600,000 types) we extracted these parses, where g is a lexical governor (for the shallow parses, g is either a verb or a preposition), a is the head of one of its NP arguments and r is the the grammatical relationship between the governor and argument (in the shallow parses r is always O B J for prepositional governors, and r is either SUBJ or OBJ for verbal governors). In order to avoid sparse data problems we smoothed this distribution over tuples as described in (Rooth et al., 1999). We assume that governor-relation pairs (g, r) and arguments a are independently generated from 25 hidden classes C, i.e.: I'Ik=lQJ(w)A~+J eZ_,~=lAjlj(~)(6 ) v - - ~ P((g,r,a)) = ~'~ Pe((g,r)lc)~)e(alc)ee(c) cEC Note that the auxiliary distributions Qj are treated as fixed distributions for the purposes of this estimation, even though each Qj may itself be a complex model obtained via a previous estimation process. Comparing (6) with (1) on page 2, we see that the two equations become identical if the reference distribution Q in (1) is replaced by a geometric mixture of the auxiliary distribu"
A00-2021,W98-1505,0,\N,Missing
A00-2021,J97-4005,0,\N,Missing
C00-1028,W99-0901,0,0.268089,"r synset if the former has the latter as a broader concept; for example, BEVERAGE is a hyponym of LIQUID. Figure 1 depicts a portion of the hierarchy. The statistical component consists of predicate-argument pairs extracted from a corpus in which the semantic class of the words is not indicated. A trivial algorithm might get a list of words that occurred as objects of the verb and output the semantic classes the words belong to according to Wordnet. For example, if the verb drink occurred with water and water 2 LIQU ID, the model would learn that drink selects for LIQUID. As Resnik (1997) and Abney and Light (1999) have found, the main problem these systems face is the presence of ambiguous words in the training data. If the word java also occurred as an object of drink, since java 2 BEV ERAGE and java 2 ISLAN D, this model would learn that drink selects for both BEV ERAGE and ISLAN D . More complex models have been proposed. These models, though, deal with word sense ambiguity by applying an unselective strategy similar to the one above; i.e., they assume that ambiguous words provide equal evidence for all their senses. These models choose as the concepts the verb selects for those that are in common a"
C00-1028,W97-0209,0,0.299548,"hyponym of another synset if the former has the latter as a broader concept; for example, BEVERAGE is a hyponym of LIQUID. Figure 1 depicts a portion of the hierarchy. The statistical component consists of predicate-argument pairs extracted from a corpus in which the semantic class of the words is not indicated. A trivial algorithm might get a list of words that occurred as objects of the verb and output the semantic classes the words belong to according to Wordnet. For example, if the verb drink occurred with water and water 2 LIQU ID, the model would learn that drink selects for LIQUID. As Resnik (1997) and Abney and Light (1999) have found, the main problem these systems face is the presence of ambiguous words in the training data. If the word java also occurred as an object of drink, since java 2 BEV ERAGE and java 2 ISLAN D, this model would learn that drink selects for both BEV ERAGE and ISLAN D . More complex models have been proposed. These models, though, deal with word sense ambiguity by applying an unselective strategy similar to the one above; i.e., they assume that ambiguous words provide equal evidence for all their senses. These models choose as the concepts the verb selects for"
C00-1052,P99-1070,0,0.0272328,"uctions (more on this below) and are therefore applicable to more complex grammar formalisms as well as CFGs; a property which other approaches to left-recursion elimination typically lack. For example, they apply to left-recursive uni cation-based grammars (Matsumoto et al., 1983; Pereira and Shieber, 1987; Johnson, 1998a). Because the emission probability of a PCFG production can be regarded as an annotation on a CFG production, the left-corner transform can produce a CFG with weighted productions which assigns the same probabilities to strings and transformed trees as the original grammar (Abney et al., 1999). However, the transformed grammars can be much larger than the original, which is unacceptable for many applications involving large grammars. The selective left-corner transform reduces the transformed grammar size because only those productions which appear in a left-recursive cycle need be recognized left-corner in order to remove leftrecursion. A top-down parser using a grammar produced by the selective left-corner transform simulates a generalized left-corner parser (Demers, 1977; Nijholt, 1980) which recognizes a user-speci ed subset of the original productions in a left-corner fashion,"
C00-1052,P97-1003,0,0.056028,"nd Carpenter, 1997; Roark and Johnson, 1999). Parsing accuracy drops o as grammar size decreases, presumably because smaller PCFGs have fewer adjustable parameters with which to describe this non-local information. There are other kinds of non-local information which can be incorporated into a PCFG using a transform-detransform approach that result in an even greater improvement of parsing accuracy (Johnson, 1998b). Ultimately, however, it seems that a more complex approach incorporating back-o and smoothing is necessary in order to achieve the parsing accuracy achieved by Charniak (1997) and Collins (1997). 4 Conclusion This paper presented factored selective left-corner grammar transforms. These transforms preserve the primary bene ts of the left-corner grammar transform (i.e., elimination of left-recursion and preservation of annotations on productions) while dramatically ameliorating its principal problems (grammar size and sparse data problems). This should extend the applicability of left-corner techniques to situations involving large grammars. We showed how to identify the minimal set L0 of productions of a grammar that must be recognized left-corner in order for the transformed grammar"
C00-1052,H94-1109,0,0.0235541,"ld usually have a smaller search space relative to the standard left-corner transform, all else being equal. The partial parses produced during a top-down parse consist of a single connected tree fragment, while the partial parses produced produced during a left-corner parse generally consist of several disconnected tree fragments. Since these fragments are only weakly related (via the link&quot; constraint described below), the search for each fragment is relatively independent. This may be responsible for the observation that exhaustive left-corner parsing is less ecient than top-down parsing (Covington, 1994). Informally, because the selective left-corner transform recognizes only a subset of productions in a left-corner fashion, its partial parses contain fewer tree discontiguous fragments and the search may be more ecient. While this paper focuses on reducing grammar size to minimize sparse data problems in PCFG estimation, the modi ed left-corner transforms described here are generally applicable wherever the original left-corner transform is. For example, the selective left-corner transform can be used in place of the standard left-corner transform in the construction of nite-state approximat"
C00-1052,P98-1101,1,0.891134,"and 9812169. We would like to thank our colleagues in BLLIP (Brown Laboratory for Linguistic Information Processing) and Bob Moore for their helpful comments on this paper.  Brian Roark@Brown.edu Left-corner transforms are particularly useful because they can preserve annotations on productions (more on this below) and are therefore applicable to more complex grammar formalisms as well as CFGs; a property which other approaches to left-recursion elimination typically lack. For example, they apply to left-recursive uni cation-based grammars (Matsumoto et al., 1983; Pereira and Shieber, 1987; Johnson, 1998a). Because the emission probability of a PCFG production can be regarded as an annotation on a CFG production, the left-corner transform can produce a CFG with weighted productions which assigns the same probabilities to strings and transformed trees as the original grammar (Abney et al., 1999). However, the transformed grammars can be much larger than the original, which is unacceptable for many applications involving large grammars. The selective left-corner transform reduces the transformed grammar size because only those productions which appear in a left-recursive cycle need be recognize"
C00-1052,J98-4004,1,0.430007,"and 9812169. We would like to thank our colleagues in BLLIP (Brown Laboratory for Linguistic Information Processing) and Bob Moore for their helpful comments on this paper.  Brian Roark@Brown.edu Left-corner transforms are particularly useful because they can preserve annotations on productions (more on this below) and are therefore applicable to more complex grammar formalisms as well as CFGs; a property which other approaches to left-recursion elimination typically lack. For example, they apply to left-recursive uni cation-based grammars (Matsumoto et al., 1983; Pereira and Shieber, 1987; Johnson, 1998a). Because the emission probability of a PCFG production can be regarded as an annotation on a CFG production, the left-corner transform can produce a CFG with weighted productions which assigns the same probabilities to strings and transformed trees as the original grammar (Abney et al., 1999). However, the transformed grammars can be much larger than the original, which is unacceptable for many applications involving large grammars. The selective left-corner transform reduces the transformed grammar size because only those productions which appear in a left-recursive cycle need be recognize"
C00-1052,1997.iwpt-1.18,0,0.738644,"transform TL mapping parse trees of G into parse trees of LC L (G) (Johnson, 1998a; Roark and Johnson, 1999). In the empirical evaluation below, we estimate a PCFG from the trees obtained by applying TL to the trees in the Penn WSJ tree-bank, and compare it to the PCFG estimated from the original tree-bank trees. A stochastic top-down parser using the PCFG estimated from the trees produced by TL simulates a stochastic generalized left-corner parser, which is a generalization of a standard stochastic left-corner parser that permits productions to be recognized top-down as well as left-corner (Manning and Carpenter, 1997). Thus investigating the properties of PCFG estimated from trees transformed with TL is an easy way of studying stochastic push-down automata performing generalized left-corner parses. 2.3 Pruning useless productions We turn now to the problem of reducing the size of the grammars produced by left-corner transforms. Many of the productions generated by schemata 1 are useless, i.e., they never appear in any terminating derivation. While they can be removed by standard methods for deleting useless productions (Hopcroft and Ullman, 1979), the relationship between the parse trees of G and LC L(G) d"
C00-1052,A00-2033,0,0.209858,"re are generally applicable wherever the original left-corner transform is. For example, the selective left-corner transform can be used in place of the standard left-corner transform in the construction of nite-state approximations (Johnson, 1998a), often reducing the size of the intermediate automata constructed. The selective left-corner transform can be generalized to head-corner parsing (van Noord, 1997), yielding a selective head-corner parser. (This follows from generalizing the selective left-corner transform to Horn clauses). After this paper was accepted for publication we learnt of Moore (2000), which addresses the issue of grammar size using very similar techniques to those proposed here. The goals of the two papers are slightly di erent: Moore&apos;s approach is designed to reduce the total grammar size (i.e., the sum of the lengths of the productions), while our approach minimizes the number of productions. Moore (2000) does not address left-corner tree-transforms, or questions of sparse data and parsing accuracy that are covered in section 3. 2 The selective left-corner and related transforms This section introduces the selective left-corner transform and two additional factorization"
C00-1052,C92-1032,0,0.0194849,"oductions from G are recognized using LC L (G). When the se::: A ::: ) LC  A  A{A ) ::: A ::: -removal Figure 2: The recognition of a top-down production A ! by LC L (G) involves a left-corner category A{A, which immediately rewrites to . One-step -removal applied to LC L (G) produces a grammar in which each top-down production A ! corresponds to a production A ! in the transformed grammar. lective left-corner transform is followed by a onestep -removal transform (i.e., composition or partial evaluation of schema 1b with respect to schema 1d (Johnson, 1998a; Abney and Johnson, 1991; Resnik, 1992)), each top-down production from G appears unchanged in the nal grammar. Full -removal yields the grammar given by the schemata below. D ! w D{w D!w D ! D {A D! D{B ! D{C D{B ! where D )+L w where A ! 2 P , L where D )?L A; A ! 2 P , L where C ! B 2 L where D )?L C; C ! B 2 L Moore (2000) introduces a version of the leftcorner transform called LCLR , which applies only to productions with left-recursive parent and left child categories. In the context of the other transforms that Moore introduces, it seems to have the same e ect in his system as the selective left-corner transform does here."
C00-1052,P99-1054,1,0.909499,"d. Unfortunately the grammars produced by the standard left-corner transform are usually much larger than the original. The selective left-corner transform described in this paper produces a transformed grammar which simulates left-corner recognition of a user-speci ed set of the original productions, and top-down recognition of the others. Combined with two factorizations, it produces non-left-recursive grammars that are not much larger than the original. 1 Introduction Top-down parsing techniques are attractive because of their simplicity, and can often achieve good performance in practice (Roark and Johnson, 1999). However, with a left-recursive grammar such parsers typically fail to terminate. The left-corner grammar transform converts a left-recursive grammar into a non-left-recursive one: a top-down parser using a left-corner transformed grammar simulates a left-corner parser using the original grammar (Rosenkrantz and Lewis II, 1970; Aho and Ullman, 1972). However, the left-corner transformed grammar can be signi cantly larger than the original grammar, causing numerous problems. For example, we show below that a probabilistic context-free grammar (PCFG) estimated from left-corner transformed Penn"
C00-1052,J97-3004,0,0.0718526,"Missing"
C00-1052,C98-1098,1,\N,Missing
C08-1071,P05-1022,1,0.871349,"act of search errors, value of non-generative reranker features, and effects of unknown words). From these experiments, we gain a better understanding of why self-training works for parsing. Since improvements from selftraining are correlated with unknown bigrams and biheads but not unknown words, the benefit of self-training appears most influenced by seeing known words in new combinations. 1 Introduction Supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure (Bod, 2003; Charniak and Johnson, 2005; Collins and Koo, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These annotations can be used by various higher-level applications such as semantic role labeling (Pradhan et al., 2007) and machine translation (Yamada and Knight, 2001). c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. However, labeled training data is expensive to annotate. Given the large amount of unlabeled text available for many domains and languages, techniques which allow us to use bot"
C08-1071,N06-1020,1,0.72165,"pe of semi-supervised learning. In self-training, first we train a model on the labeled data and use that model to label the unlabeled data. From the combination of our original labeled data and the newly labeled data, we train a second model – our self-trained model. The process can be iterated, where the self-trained model is used to label new data in the next iteration. One can think of self-training as a simple case of cotraining (Blum and Mitchell, 1998) using a single learner instead of several. Alternatively, one can think of it as one step of the Viterbi EM algorithm. Studies prior to McClosky et al. (2006) failed to show a benefit to parsing from self-training (Charniak, 1997; Steedman et al., 2003). While the recent success of self-training has demonstrated its merit, it remains unclear why self-training helps in some cases but not others. Our goal is to better understand when and why self-training is beneficial. In Section 2, we discuss the previous applications of self-training to parsing. Section 3 describes our experimental setup. We present and test four hypotheses of why self-training helps in Section 4 and conclude with discussion and future work in Section 5. 2 Previous Work To our kno"
C08-1071,J93-2004,0,0.0369226,"Missing"
C08-1071,P06-1055,0,0.0243637,"ker features, and effects of unknown words). From these experiments, we gain a better understanding of why self-training works for parsing. Since improvements from selftraining are correlated with unknown bigrams and biheads but not unknown words, the benefit of self-training appears most influenced by seeing known words in new combinations. 1 Introduction Supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure (Bod, 2003; Charniak and Johnson, 2005; Collins and Koo, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These annotations can be used by various higher-level applications such as semantic role labeling (Pradhan et al., 2007) and machine translation (Yamada and Knight, 2001). c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. However, labeled training data is expensive to annotate. Given the large amount of unlabeled text available for many domains and languages, techniques which allow us to use both labeled and unlabeled text to train our mo"
C08-1071,N07-1070,0,0.00990236,"provements from selftraining are correlated with unknown bigrams and biheads but not unknown words, the benefit of self-training appears most influenced by seeing known words in new combinations. 1 Introduction Supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure (Bod, 2003; Charniak and Johnson, 2005; Collins and Koo, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These annotations can be used by various higher-level applications such as semantic role labeling (Pradhan et al., 2007) and machine translation (Yamada and Knight, 2001). c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. However, labeled training data is expensive to annotate. Given the large amount of unlabeled text available for many domains and languages, techniques which allow us to use both labeled and unlabeled text to train our models are desirable. These methods are called semi-supervised. Selftraining is a specific type of semi-supervised learning. In self-training, first we"
C08-1071,P07-1080,0,0.0118957,"ects of unknown words). From these experiments, we gain a better understanding of why self-training works for parsing. Since improvements from selftraining are correlated with unknown bigrams and biheads but not unknown words, the benefit of self-training appears most influenced by seeing known words in new combinations. 1 Introduction Supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure (Bod, 2003; Charniak and Johnson, 2005; Collins and Koo, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These annotations can be used by various higher-level applications such as semantic role labeling (Pradhan et al., 2007) and machine translation (Yamada and Knight, 2001). c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. However, labeled training data is expensive to annotate. Given the large amount of unlabeled text available for many domains and languages, techniques which allow us to use both labeled and unlabeled text to train our models are desirable. These me"
C08-1071,P01-1067,0,0.00770831,"h unknown bigrams and biheads but not unknown words, the benefit of self-training appears most influenced by seeing known words in new combinations. 1 Introduction Supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure (Bod, 2003; Charniak and Johnson, 2005; Collins and Koo, 2005; Petrov et al., 2006; Titov and Henderson, 2007). These annotations can be used by various higher-level applications such as semantic role labeling (Pradhan et al., 2007) and machine translation (Yamada and Knight, 2001). c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. However, labeled training data is expensive to annotate. Given the large amount of unlabeled text available for many domains and languages, techniques which allow us to use both labeled and unlabeled text to train our models are desirable. These methods are called semi-supervised. Selftraining is a specific type of semi-supervised learning. In self-training, first we train a model on the labeled data and use that mod"
C08-1071,J05-1003,0,\N,Missing
C08-1071,P07-1078,0,\N,Missing
C08-1071,E03-1005,0,\N,Missing
C10-1060,P08-1016,0,0.043077,"tics (Coling 2010), pages 528–536, Beijing, August 2010 ASCII encoding of the International Phonetic Alphabet representation of English phonemes). The input to the learner is obtained by concatenating together the phonemic representations of each utterance’s words. The learner’s task is to identify the locations of the word boundaries in this sequence, and hence identify the words (up to homophony). Brent and Cartwright (1996) pointed out the importance of both distributional information and phonotactic (e.g., syllable-structure) constraints for word segmentation (see also Swingley (2005) and Fleck (2008)). Recently there has been considerable interest in applying Bayesian inference techniques for nonparametric models to this problem. Here the term “non-parametric” does not mean that the models have no parameters, rather, it is used to distinguish these models from the usual “parametric models” that have a fixed finite vector of parameters. Goldwater et al. (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al., 2009). The unigram model, which assumes that each word is generated independently to form a sentence, turned"
C10-1060,P06-1085,1,0.870219,"and hence identify the words (up to homophony). Brent and Cartwright (1996) pointed out the importance of both distributional information and phonotactic (e.g., syllable-structure) constraints for word segmentation (see also Swingley (2005) and Fleck (2008)). Recently there has been considerable interest in applying Bayesian inference techniques for nonparametric models to this problem. Here the term “non-parametric” does not mean that the models have no parameters, rather, it is used to distinguish these models from the usual “parametric models” that have a fixed finite vector of parameters. Goldwater et al. (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al., 2009). The unigram model, which assumes that each word is generated independently to form a sentence, turned out to be equivalent to a model originally proposed by Brent (1999). The bigram model improves word segmentation accuracy by modelling bigram inter-word contextual dependencies, “explaining away” inter-word dependencies that would otherwise cause the unigram model to under-segment. Mochihashi et al. (2009) showed that segmentation accuracy could be improved by usi"
C10-1060,N09-1036,1,0.924117,"ours. Johnson et al. (2007) introduced adaptor grammars as a grammar-based framework for expressing a variety of non-parametric models, and provided a dynamic programming Markov Chain Monte Carlo (MCMC) sampling algorithm for performing Bayesian inference on these models. For example, the unigram model can be expressed as a simple adaptor grammar as shown below, and the generic adaptor grammar inference procedure provides a dynamic programming sampling algorithm for this model. Johnson (2008b) showed how a variety of different word segmentation models can be expressed as adaptor grammars, and Johnson and Goldwater (2009) described a number of extensions and specialisations to the adaptor grammar framework that improve inference speed and accuracy (we use these techniques in our work below). Previous work on unsupervised word segmentation from phonemic input has tended to concentrate on English. However, presumably children the world over segment their first language input in the same (innately-specified) way, so a correct procedure should work for all possible human languages. However, as far as we are aware there has been relatively little work on word segmentation from phonemic input except on English. John"
C10-1060,W08-0704,1,0.874991,"ntation, albeit from orthographic rather than phonemic forms, so unfortunately their results are not comparable with ours. Johnson et al. (2007) introduced adaptor grammars as a grammar-based framework for expressing a variety of non-parametric models, and provided a dynamic programming Markov Chain Monte Carlo (MCMC) sampling algorithm for performing Bayesian inference on these models. For example, the unigram model can be expressed as a simple adaptor grammar as shown below, and the generic adaptor grammar inference procedure provides a dynamic programming sampling algorithm for this model. Johnson (2008b) showed how a variety of different word segmentation models can be expressed as adaptor grammars, and Johnson and Goldwater (2009) described a number of extensions and specialisations to the adaptor grammar framework that improve inference speed and accuracy (we use these techniques in our work below). Previous work on unsupervised word segmentation from phonemic input has tended to concentrate on English. However, presumably children the world over segment their first language input in the same (innately-specified) way, so a correct procedure should work for all possible human languages. Ho"
C10-1060,P08-1046,1,0.67658,"ntation, albeit from orthographic rather than phonemic forms, so unfortunately their results are not comparable with ours. Johnson et al. (2007) introduced adaptor grammars as a grammar-based framework for expressing a variety of non-parametric models, and provided a dynamic programming Markov Chain Monte Carlo (MCMC) sampling algorithm for performing Bayesian inference on these models. For example, the unigram model can be expressed as a simple adaptor grammar as shown below, and the generic adaptor grammar inference procedure provides a dynamic programming sampling algorithm for this model. Johnson (2008b) showed how a variety of different word segmentation models can be expressed as adaptor grammars, and Johnson and Goldwater (2009) described a number of extensions and specialisations to the adaptor grammar framework that improve inference speed and accuracy (we use these techniques in our work below). Previous work on unsupervised word segmentation from phonemic input has tended to concentrate on English. However, presumably children the world over segment their first language input in the same (innately-specified) way, so a correct procedure should work for all possible human languages. Ho"
C10-1060,P09-1012,0,0.0447985,"tric models” that have a fixed finite vector of parameters. Goldwater et al. (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al., 2009). The unigram model, which assumes that each word is generated independently to form a sentence, turned out to be equivalent to a model originally proposed by Brent (1999). The bigram model improves word segmentation accuracy by modelling bigram inter-word contextual dependencies, “explaining away” inter-word dependencies that would otherwise cause the unigram model to under-segment. Mochihashi et al. (2009) showed that segmentation accuracy could be improved by using a more sophisticated “base distribution” and a dynamic programming sampling algorithm very similar to the one used with the adaptor grammars below. They also applied their algorithm to Japanese and Chinese word segmentation, albeit from orthographic rather than phonemic forms, so unfortunately their results are not comparable with ours. Johnson et al. (2007) introduced adaptor grammars as a grammar-based framework for expressing a variety of non-parametric models, and provided a dynamic programming Markov Chain Monte Carlo (MCMC) sa"
C10-1154,P01-1017,0,0.161396,"Missing"
C10-1154,P04-1005,1,0.887791,"nnotations in the corpus. An alternative means of evaluation would be to simply generate a new signal with the reparandum and filler removed, and compare this against a ‘cleaned-up’ version of the utterance; however, Core and Schubert (1999) argue that, especially in the case of speech repairs, it is important not to simply throw away the disfluent elements of an utterance, since they can carry meaning that needs to be recovered for proper interpretation of the utterance. We are therefore interested in the first instance in a model of speech error detection, rather than a model of correction. Johnson and Charniak (2004) describe such a model, using a noisy-channel based approach to the detection of the start and end points of reparanda, interregna and repairs. Since we use this model as our starting point, we provide a more detailed explanation in Section 3. The idea of using a noisy channel model to identify speech repairs has been explored for languages other than English. Honal and Schultz (2003) use such a model, comparing speech disfluency detection in spontaneous spoken Mandarin against that in English. The approach performs well in Mandarin, although better still in English. Both the models just descr"
C10-1154,J10-1001,0,0.0276848,"and Schultz (2003) use such a model, comparing speech disfluency detection in spontaneous spoken Mandarin against that in English. The approach performs well in Mandarin, although better still in English. Both the models just described operate on transcripts of completed utterances. Ideally, however, when we deal with speech we would like to process the input word by word as it is received. Being able to do this would enable tighter integration in both speech recognition 1372 and interpretation, which might in turn improve overall accuracy. The requirement for incrementality is recognised by Schuler et al. (2010), who employ an incremental Hierarchical Hidden Markov Model (HHMM) to detect speech disfluencies. The HHMM is trained on manually annotated parse trees which are transformed by a right corner transformation; the HHMM is then used in an incremental fashion on unseen data, growing the parse structure each time a new token comes in. Special subtrees in this parse can carry a marker indicating that the span of the subtree consists of tokens corresponding to a speech disfluency. Schuler et al.’s approach thus provides scope for detecting disfluencies in an incremental fashion. However, their repor"
C10-1154,C90-3045,0,0.173222,"Missing"
C10-1154,W90-0102,0,\N,Missing
C12-1021,P12-2017,1,0.832869,"ound 1,100 utterances; in contrast, our dataset contains more than 90,000 CDS utterances in total and spans a period of several months for all of the children. This makes it possible to both compare inter-child variability in word segmentation across comparable situations and to study developmental changes in individual children over a period of several months. As such, the resource will allow researchers to ask a wider range of questions than is currently the norm, in particular with respect to the study of incremental models that have recently received a lot of interest (Pearl et al., 2011; Börschinger and Johnson, 2012; Phillips and Pearl, 2012). While for our own experiments on the effects of input size we focus on one of the six sub-corpora of the dataset, we describe and make available the full data so as to enable other researchers to take advantage of this new resource as well. The contribution of this paper is two-fold. We present a new CDS corpus for computational word segmentation that is derived from the Providence Corpus (Demuth et al., 2006), comprising data from six different children that have been collected in comparable situations over several months. The second and major contribution of our"
C12-1021,boruta-jastrzebska-2012-phonemic,0,0.0400981,"sefulness of different kinds of cues or different learning strategies. Just as important as the actual models, however, is the adequacy of the input used to evaluate them — if we are interested in answering questions about human language acquisition, the data we evaluate our models on needs to be comparable to what children are likely to have access to. To this end, several datasets of phonemically transcribed child directed speech (CDS) have been constructed in several languages, ranging from English to Italian, Polish, Sesotho and Chinese (Brent and Cartwright, 1996; Gervain and Erra, 2012; Boruta and Jastrzebska, 2012; Johnson, 2008a; Johnson and Demuth, 2010). In addition to cross-linguistic variation, however, adequate computational models also need to handle language-internal variation along several dimensions, a topic that has so far received little interest. In this paper, we look at a basic point of variation, namely the actual size of the input to the learner. The longer children are exposed to language, the more data they are exposed to and the better at their language they become, something one would expect from adequate models of language acquisition as well. We run our experiments on a novel dat"
C12-1021,W08-0704,1,0.882646,"f cues or different learning strategies. Just as important as the actual models, however, is the adequacy of the input used to evaluate them — if we are interested in answering questions about human language acquisition, the data we evaluate our models on needs to be comparable to what children are likely to have access to. To this end, several datasets of phonemically transcribed child directed speech (CDS) have been constructed in several languages, ranging from English to Italian, Polish, Sesotho and Chinese (Brent and Cartwright, 1996; Gervain and Erra, 2012; Boruta and Jastrzebska, 2012; Johnson, 2008a; Johnson and Demuth, 2010). In addition to cross-linguistic variation, however, adequate computational models also need to handle language-internal variation along several dimensions, a topic that has so far received little interest. In this paper, we look at a basic point of variation, namely the actual size of the input to the learner. The longer children are exposed to language, the more data they are exposed to and the better at their language they become, something one would expect from adequate models of language acquisition as well. We run our experiments on a novel dataset that conta"
C12-1021,P08-1046,1,0.759397,"f cues or different learning strategies. Just as important as the actual models, however, is the adequacy of the input used to evaluate them — if we are interested in answering questions about human language acquisition, the data we evaluate our models on needs to be comparable to what children are likely to have access to. To this end, several datasets of phonemically transcribed child directed speech (CDS) have been constructed in several languages, ranging from English to Italian, Polish, Sesotho and Chinese (Brent and Cartwright, 1996; Gervain and Erra, 2012; Boruta and Jastrzebska, 2012; Johnson, 2008a; Johnson and Demuth, 2010). In addition to cross-linguistic variation, however, adequate computational models also need to handle language-internal variation along several dimensions, a topic that has so far received little interest. In this paper, we look at a basic point of variation, namely the actual size of the input to the learner. The longer children are exposed to language, the more data they are exposed to and the better at their language they become, something one would expect from adequate models of language acquisition as well. We run our experiments on a novel dataset that conta"
C12-1021,C10-1060,1,0.874974,"ent learning strategies. Just as important as the actual models, however, is the adequacy of the input used to evaluate them — if we are interested in answering questions about human language acquisition, the data we evaluate our models on needs to be comparable to what children are likely to have access to. To this end, several datasets of phonemically transcribed child directed speech (CDS) have been constructed in several languages, ranging from English to Italian, Polish, Sesotho and Chinese (Brent and Cartwright, 1996; Gervain and Erra, 2012; Boruta and Jastrzebska, 2012; Johnson, 2008a; Johnson and Demuth, 2010). In addition to cross-linguistic variation, however, adequate computational models also need to handle language-internal variation along several dimensions, a topic that has so far received little interest. In this paper, we look at a basic point of variation, namely the actual size of the input to the learner. The longer children are exposed to language, the more data they are exposed to and the better at their language they become, something one would expect from adequate models of language acquisition as well. We run our experiments on a novel dataset that contains longitudinal data for si"
C12-1021,N09-1036,1,0.92188,"Missing"
C12-1021,N10-1074,1,0.798142,"long. There is therefore more data for this mother and child. Lily’s mother also talked quickly; there is therefore much data from Lily’s mother as well. Recording began around one year or once the parent reported that the child was producing approximately four words. Digital audio/video recordings took place in each child’s home. Although parent and child could move freely about, the video information was useful in determining the context of what was being discussed, including possible target words. The availability of video would allow future work along the lines of Frank et al. (2009) and Jones et al. (2010) although so far, we haven’t made direct use of the video recordings. The digital audio/video recordings were downloaded onto a computer, and both adult and child speech were orthographically transcribed using CHAT conventions (cf. MacWhinney (2000)). The child data — but unfortunately not the caregivers’ — were then also transcribed in phonemic transcription. All mother and child transcriptions, as well as audio/video files, can be found on the CHILDES database http://childes.psy.cmu.edu/. We used the XML version of the data for our transcription process. 2.1 Producing a phonemically transcri"
C12-1021,J01-3002,0,0.191553,"Missing"
C12-1088,P06-2006,0,0.0253461,"reebank WSJ data to CCG derivations and dependencies (Hockenmaier and Steedman, 2007). We use the normal-form model described in Clark and Curran (2007), which models the probability of derivations. We also follow the convention of using section 00 of CCGbank as development data, sections 02-21 as training data, and section 23 for final testing. The standard evaluation metric is labeled dependency recovery, as described by Clark and Hockenmaier (2002). Clark and Curran (2007) develop a conversion from CCG dependencies to Briscoe and Carrollstyle grammatical relations (GRs) (King et al., 2003; Briscoe and Carroll, 2006). GRs provide a useful abstraction as they allow the conflation of many CCG dependencies that are semantically similar but structurally different. For example, since subcategorization information is fully specified in categories, the verb-subject relationship is expressed in many different forms in CCG depending on the transitivity of the verb. In the GR scheme, they map to a general ncsubj dependency, echoing the underlying similarity between the CCG dependencies. Rimell and Clark (2009) adapt the C&C parsing for the biomedical domain, and in the process they developed a mapping from CCG depe"
C12-1088,P05-1022,1,0.71672,"Missing"
C12-1088,J07-4004,1,0.92699,"e mediating category for long-range dependencies. 2.3 CCG parsing The C&C parser is a fast and accurate wide-coverage CCG parser. It is a two-stage system, where a supertagger assigns probable categories to words in a sentence and the parser combines them using the CKY algorithm. The parser has been found to be particularly accurate at recovering long-range dependencies (Clark et al., 2002; Rimell et al., 2009). C & C is trained on CCGbank, a conversion of the Penn Treebank WSJ data to CCG derivations and dependencies (Hockenmaier and Steedman, 2007). We use the normal-form model described in Clark and Curran (2007), which models the probability of derivations. We also follow the convention of using section 00 of CCGbank as development data, sections 02-21 as training data, and section 23 for final testing. The standard evaluation metric is labeled dependency recovery, as described by Clark and Hockenmaier (2002). Clark and Curran (2007) develop a conversion from CCG dependencies to Briscoe and Carrollstyle grammatical relations (GRs) (King et al., 2003; Briscoe and Carroll, 2006). GRs provide a useful abstraction as they allow the conflation of many CCG dependencies that are semantically similar but str"
C12-1088,P02-1042,0,0.034116,"equire co-indexation in phrase-structure parses. These dependencies have the following form: 〈to , PP/NP1 , 1, report , (NPNP)/(S[dcl]/NP)〉, which includes the head word, its category, the argument slot, argument word, and the mediating category for long-range dependencies. 2.3 CCG parsing The C&C parser is a fast and accurate wide-coverage CCG parser. It is a two-stage system, where a supertagger assigns probable categories to words in a sentence and the parser combines them using the CKY algorithm. The parser has been found to be particularly accurate at recovering long-range dependencies (Clark et al., 2002; Rimell et al., 2009). C & C is trained on CCGbank, a conversion of the Penn Treebank WSJ data to CCG derivations and dependencies (Hockenmaier and Steedman, 2007). We use the normal-form model described in Clark and Curran (2007), which models the probability of derivations. We also follow the convention of using section 00 of CCGbank as development data, sections 02-21 as training data, and section 23 for final testing. The standard evaluation metric is labeled dependency recovery, as described by Clark and Hockenmaier (2002). Clark and Curran (2007) develop a conversion from CCG dependenci"
C12-1088,W08-1301,0,0.102231,"Missing"
C12-1088,W11-2924,0,0.0347568,"Missing"
C12-1088,J07-3004,0,0.0338431,"ludes the head word, its category, the argument slot, argument word, and the mediating category for long-range dependencies. 2.3 CCG parsing The C&C parser is a fast and accurate wide-coverage CCG parser. It is a two-stage system, where a supertagger assigns probable categories to words in a sentence and the parser combines them using the CKY algorithm. The parser has been found to be particularly accurate at recovering long-range dependencies (Clark et al., 2002; Rimell et al., 2009). C & C is trained on CCGbank, a conversion of the Penn Treebank WSJ data to CCG derivations and dependencies (Hockenmaier and Steedman, 2007). We use the normal-form model described in Clark and Curran (2007), which models the probability of derivations. We also follow the convention of using section 00 of CCGbank as development data, sections 02-21 as training data, and section 23 for final testing. The standard evaluation metric is labeled dependency recovery, as described by Clark and Hockenmaier (2002). Clark and Curran (2007) develop a conversion from CCG dependencies to Briscoe and Carrollstyle grammatical relations (GRs) (King et al., 2003; Briscoe and Carroll, 2006). GRs provide a useful abstraction as they allow the confla"
C12-1088,W05-1506,0,0.0914008,"Missing"
C12-1088,W07-2416,0,0.0531777,"r our experiments. In contrast to other grammars used in this paper, this dependency scheme contains only unlabeled word-word arcs. Stanford: de Marneffe and Manning (2008) introduced the dependency scheme used in the Stanford parser2 . We used the Stanford parser’s built-in converter to transform Penn Treebank trees into dependencies. The Stanford scheme has different variants; for this work we use the basic projective tree schema. LTH: The LTH dependency scheme was developed with the aim of making better use of the linguistic information present in the Penn Treebank from version II onwards (Johansson and Nugues, 2007). We generated these dependencies using the LTH converter3 over the NPbracketed version of the Penn Treebank described by Vadas and Curran (2007). The converter was configured to produce a functional rather than lexical DG. Fanse: Another conversion of the Penn Treebank with more fine-grained labels was presented in Tratz and Hovy (2011). The Fanse scheme is linguistically rich, featuring both non-projective dependencies and shallow semantic interpretation in its analyses. We used the freely available converter4 , which also requires the Vadas and Curran (2007) NP-bracketed Penn Treebank. Figu"
C12-1088,W03-2401,0,0.0367326,"rsion of the Penn Treebank WSJ data to CCG derivations and dependencies (Hockenmaier and Steedman, 2007). We use the normal-form model described in Clark and Curran (2007), which models the probability of derivations. We also follow the convention of using section 00 of CCGbank as development data, sections 02-21 as training data, and section 23 for final testing. The standard evaluation metric is labeled dependency recovery, as described by Clark and Hockenmaier (2002). Clark and Curran (2007) develop a conversion from CCG dependencies to Briscoe and Carrollstyle grammatical relations (GRs) (King et al., 2003; Briscoe and Carroll, 2006). GRs provide a useful abstraction as they allow the conflation of many CCG dependencies that are semantically similar but structurally different. For example, since subcategorization information is fully specified in categories, the verb-subject relationship is expressed in many different forms in CCG depending on the transitivity of the verb. In the GR scheme, they map to a general ncsubj dependency, echoing the underlying similarity between the CCG dependencies. Rimell and Clark (2009) adapt the C&C parsing for the biomedical domain, and in the process they devel"
C12-1088,J93-2004,0,0.04036,"Missing"
C12-1088,P05-1012,0,0.125133,"h-malt:VB:IN Table 2: The generation process for pair-dependency features. Each feature template follows a similar pattern. parser-predicted DG dependencies. The gold experiment allowed us to investigate the upper performance bound of our reranking technique and of our DG-derived features. We evaluate using the standard CCG dependency recovery metric over section 00 of CCGbank. We use the reranker settings that Ng et al. (2010) found to provide best performance: regression learning, 10-best mode, and no feature pruning. We use the same experimental settings reported in Nivre et al. (2010) and McDonald et al. (2005) for the Maltparser and MSTparser respectively. This means that both parsers will produce a projective dependency tree for each scheme that we experimented with. 5.1 Overall Comparison section 00 (dev) C & C normal ’07 Reranker ’10 CoNLL features Stanford features Gold LTH features Fanse features CoNLL features Stanford features Malt Predicted LTH features Fanse features CoNLL features Stanford features MST Predicted LTH features Fanse features Baselines Table 3: Parsing performance for the four over section 00 of CCGbank DG LP 87.27 87.57 89.17 88.97 88.95 89.61 87.74 87.80 87.43 87.82 87.65"
C12-1088,D07-1013,0,0.0671167,"Missing"
C12-1088,P12-1052,1,0.872524,"Missing"
C12-1088,U10-1014,1,0.924719,"e in a base parser. More informative features can be considered in reranking as the entire parse tree is available, as opposed to the fragments considered in parsing. In this paper, we propose a simple method for improving the performance of the C&C Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2007). We parse sentences using the C&C n-best parser and a 1-best dependency grammar (DG) parser, and generate DG-derived features by comparing the extracted dependencies from the C&C parser with the DG dependencies. We then incorporate the DG-derived features into the CCG reranker of Ng et al. (2010) to reorder the n-best CCG parses using the external parse information. We experiment with both the Maltparser (Nivre et al., 2007b) and the MSTparser (McDonald et al., 2005) as the DG parser. This is the first cross-formalism parser combination experiment for CCG parsing that we are aware of, combining the features and strengths of two different formalisms together. Previous work has shown that dependency parsers such as the Maltparser perform better on short-range dependencies (McDonald and Nivre, 2007), whereas the C&C parser deals with long-range dependencies more reliably (Clark et al., 2"
C12-1088,C10-1094,0,0.0127589,"atch-malt:see:IN, nomatch-malt:VB:IN Table 2: The generation process for pair-dependency features. Each feature template follows a similar pattern. parser-predicted DG dependencies. The gold experiment allowed us to investigate the upper performance bound of our reranking technique and of our DG-derived features. We evaluate using the standard CCG dependency recovery metric over section 00 of CCGbank. We use the reranker settings that Ng et al. (2010) found to provide best performance: regression learning, 10-best mode, and no feature pruning. We use the same experimental settings reported in Nivre et al. (2010) and McDonald et al. (2005) for the Maltparser and MSTparser respectively. This means that both parsers will produce a projective dependency tree for each scheme that we experimented with. 5.1 Overall Comparison section 00 (dev) C & C normal ’07 Reranker ’10 CoNLL features Stanford features Gold LTH features Fanse features CoNLL features Stanford features Malt Predicted LTH features Fanse features CoNLL features Stanford features MST Predicted LTH features Fanse features Baselines Table 3: Parsing performance for the four over section 00 of CCGbank DG LP 87.27 87.57 89.17 88.97 88.95 89.61 87."
C12-1088,D09-1085,0,0.01536,"in phrase-structure parses. These dependencies have the following form: 〈to , PP/NP1 , 1, report , (NPNP)/(S[dcl]/NP)〉, which includes the head word, its category, the argument slot, argument word, and the mediating category for long-range dependencies. 2.3 CCG parsing The C&C parser is a fast and accurate wide-coverage CCG parser. It is a two-stage system, where a supertagger assigns probable categories to words in a sentence and the parser combines them using the CKY algorithm. The parser has been found to be particularly accurate at recovering long-range dependencies (Clark et al., 2002; Rimell et al., 2009). C & C is trained on CCGbank, a conversion of the Penn Treebank WSJ data to CCG derivations and dependencies (Hockenmaier and Steedman, 2007). We use the normal-form model described in Clark and Curran (2007), which models the probability of derivations. We also follow the convention of using section 00 of CCGbank as development data, sections 02-21 as training data, and section 23 for final testing. The standard evaluation metric is labeled dependency recovery, as described by Clark and Hockenmaier (2002). Clark and Curran (2007) develop a conversion from CCG dependencies to Briscoe and Carr"
C12-1088,P07-1079,0,0.223745,"endency, binary indicator features were generated based on our feature templates (depicted in Figure 5). These features represent fragments of one or more dependency arcs that the reranker learns to favour or disprefer. Each feature includes components specified by the template, a directionality marker, and all four combinations of the word and POS tag for the head and dependent. Some of our templates also generate additional features for each mismatching dependency, conjoined with a label indicating whether the dependency existed only in the CDP or the MDP. This approach differs from that of Sagae et al. (2007) since our reranker learns a separate penalty parameter for each combination of DG and CCG constructions as a feature of our regularised MaxEnt reranker model; these weights are learnt as part of the reranker training procedure. This enables our reranker to learn which DG constructions are most reliable and informative for CCG parsing, and which DG constructions should be ignored. Following are descriptions of our DG-derived feature templates, which correspond to various dependency relations shown in Figure 5: Pair-dependency features encode the head-dependent pair and a flag indicating a matc"
C12-1088,C04-1024,0,0.0699741,"Missing"
C12-1088,P07-1031,1,0.916549,"effe and Manning (2008) introduced the dependency scheme used in the Stanford parser2 . We used the Stanford parser’s built-in converter to transform Penn Treebank trees into dependencies. The Stanford scheme has different variants; for this work we use the basic projective tree schema. LTH: The LTH dependency scheme was developed with the aim of making better use of the linguistic information present in the Penn Treebank from version II onwards (Johansson and Nugues, 2007). We generated these dependencies using the LTH converter3 over the NPbracketed version of the Penn Treebank described by Vadas and Curran (2007). The converter was configured to produce a functional rather than lexical DG. Fanse: Another conversion of the Penn Treebank with more fine-grained labels was presented in Tratz and Hovy (2011). The Fanse scheme is linguistically rich, featuring both non-projective dependencies and shallow semantic interpretation in its analyses. We used the freely available converter4 , which also requires the Vadas and Curran (2007) NP-bracketed Penn Treebank. Figures 3 and 4 demonstrate some of the differences between the four dependency schemes. For instance, auxiliaries take the lexical verb as a depende"
C12-1088,C10-1011,0,\N,Missing
C12-1088,D11-1116,0,\N,Missing
C12-1088,D07-1096,0,\N,Missing
C14-1219,Q14-1008,1,0.863249,"Missing"
C14-1219,C12-1021,1,0.879906,"Missing"
C14-1219,N09-1036,1,0.947567,"Missing"
C14-1219,P08-1046,1,0.796278,". Syllables consist of onsets or codas (producing consonants), and nuclei (vowels). Onsets, nuclei and codas are adapted, thus allowing this model to memorize sequences or consonants or sequences of vowels, dependent on their position in the word. Consonants and vowels are the pre-terminals, their derivation is specified in the grammar into phonemes of the language. Sentence → Colloc+ Colloc → W ord+ W ord → StructSyll For notations purposes, all this syllabification is appended after W ord by W ord → StructSyll. All details about the collocations and syllabification grammars can be found in (Johnson, 2008). Here is an example of a (good) parse of “yuwanttusiD6bUk” with this model, skipping the StructSyll derivations: Sentence Colloc Colloc Colloc Word Word Word Word Word Word want yu 4.3 si tu D6 bUk Including topics (contexts) To allow for the model to make use of the topics (used as proxies for contexts), we modify the grammar by prefixing utterances with topic number (similarly to (Johnson et al., 2010)), ∀K ∈ #topics: Sentence → tK Colloc+ tK ColloctK → W ord+ tK For each W ordtK , we can derive it into a common adapted W ord by W ordtK → W ord. Consider this lower level adaptor (W ord): it"
C14-1219,P06-1124,0,0.052561,"⊆ N ) of adapted nonterminals, each of them (X ∈ A) having an associated adaptor (CX ∈ C). An AG defines a distribution over trees GX , ∀X ∈ N ∪ W . If X ∈ / A, then GX is defined exactly as for a PCFG: X GX = θX→Y1 ...Yn TDX (GY1 . . . GYn ) X→Y1 ...Yn ∈RX With TDX (G1 . . . Gn ) the distribution over trees with root node X and each subtree ti ∼ Gi i.i.d. If X ∈ A, then there is an additional indirection (composition) with the distribution HX : X GX = θX→Y1 ...Yn TDX (HY1 . . . HYn ) X→Y1 ...Yn ∈RX HX ∼ CX (GX ) We used CX adaptors following the Pitman-Yor process (PYP) (Perman et al., 1992; Teh, 2006) with parameters a and b. The PYP generates (Zipfian) type frequencies that are similar to those that occur in natural language (Goldwater et al., 2011). Metaphorically, if there are n customers and m tables, the n + 1th customer is assigned to table zn+1 according to (δk is the Kronecker delta function): m zn+1 |z1 . . . zn ∼ X nk − a ma + b δm+1 + δk n+b n+b k=1 For an AG, this means that adapted non-terminals (X ∈ A) either expand to a previously generated subtree (T (X)k ) with probability proportional to how often it was visited (nk ), or to a new subtree (T (X)m+1 ) generated through the"
C16-1003,D11-1131,1,0.884945,"Missing"
C16-1003,de-marneffe-etal-2006-generating,0,0.00737879,"Missing"
C16-1003,N09-1012,1,0.948894,"Missing"
C16-1003,Q14-1011,1,0.87347,"Missing"
C16-1003,P07-1022,1,0.804025,"Missing"
C16-1003,P10-1117,1,0.897652,"Missing"
C16-1003,C10-2062,0,0.0807283,"Missing"
C16-1003,P04-1061,0,0.664929,"n fact feasible when the model is provided with sufficient training data, and present two new streaming or mini-batch algorithms for PCFG inference that can learn from millions of words of training data. We compare the performance of these algorithms to a batch algorithm that learns from less data. The minibatch algorithms outperform the batch algorithm, showing that cheap inference with more data is better than intensive inference with less data. Additionally, we show that the harmonic initialiser, which previous work identified as essential when learning from small POStag annotated corpora (Klein and Manning, 2004), is not superior to a uniform initialisation. 1 Introduction How children acquire the syntax of the languages they ultimately speak is a deep scientific question of fundamental importance to linguistics and cognitive science (Chomsky, 1986). The natural language processing task of grammar induction in principle should provide models for how children do this. However, previous work on grammar induction has learned from small datasets, and has dealt with the resulting data sparsity by modifying the input and using careful search heuristics. While these techniques are useful from an engineering"
C16-1003,E12-1024,0,0.114585,"Missing"
C16-1003,N15-1067,0,0.18668,"Missing"
C16-1003,J93-2004,0,0.0564005,"Missing"
C16-1003,Q13-1006,1,0.934798,"Missing"
C16-1003,D11-1118,0,0.298188,"Missing"
C16-1003,W13-3519,0,0.0394741,"Missing"
C16-1003,P99-1059,0,\N,Missing
C16-1003,Q13-1007,0,\N,Missing
C86-1156,P83-1021,0,0.0683591,"Missing"
C86-1156,P85-1017,0,0.0114818,"ing home. b. E v e r y w o m a n w h o klssed a m a n I loved him 1. Preceding-Context [ ~t [ Following-Context Consequently, in the naive m o d e l the discourse context is determined by a series of equations relating the context which immediately precedes a lexical item to the context which immediately follows that item. For individual words, this relation is part of the lexical specification. To illustrate, the semantic contribution of w o m a n is given here by (3a), or more generally, as (3b). (4) reminiscent of the technique used in logic p r o g r a m m i n g known as difference lists (Pereira 1985) or threading. (5) shows that a universal NP does not normally act as an antecedent for pronouns in a following sentence. 4 According to the variable-binding paradigm of anaphora, this follows because a universal can only enter into an anaphorie relation with pronouns that are in its scope. For our current purposes, it is not important whether scope is determined in terms of a tree-geometrical notion like e-command (Reinhart 1983), or in terms of function-argument structure, as proposed by (Ladusaw 1980) and (Bach and Partee 1980); in either case, it is clear that the scope of the universal in"
C90-1003,J87-1005,0,0.1059,"sl:donkey Y) , ] The first reading displayed again corresponds to the quantifier-raiscd interpretation, which paraphrases as: Situation s0 contains an individual Y, and the facts that Y is a donkey and that every way of making S1 true also makes $2 tree, where S1 contains the individual X and the facts that X is a man and X owns Y, and $2 contains the fact that X beats Y. . S :-: s O : [ s 0 : s l = = > s 2 , s 2 : o w n ( X , Y ) , s l :i (X), sl:man (X), S0 : 2 (Y), sO :donkey(Y)] ; S = sO: [ s 0 : s l = = > s 2 , s 2 : o w n ( X , Y ) , s2:i(Y),s2:donkey(Y),sl:i(X), sl:man(X)] 9 25 Sheiber [7] adopt such a scheme apparently on the grounds of greater perspicuity. In any case, the modifications that need to be made to our scheme are entirely trivial, requiring only the introduction of a modest amount of symbolic computation. Basically, the idea is to use operations which, instead of returning pieces of the final logical form incrementally and nondeterministically, return expression that will exhibit this nondeterministic behavior when evaluated later. The later evaluation will, of course, be as specified be the detinitions we have given. In short, we believe that the abstractions we"
C90-1003,C86-1156,1,\N,Missing
C90-1003,C69-7001,0,\N,Missing
C90-1003,C69-6902,0,\N,Missing
C98-1098,P91-1032,0,0.604172,"c i e n c e s , B o x 1978 Brown University Mark_Johnson@3rown.edu Abstract This p a p e r describes how to construct a finite-state machine (FSM) approximating a &apos;unification-based&apos; g r a m m a r using a left-corner g r a m m a r transform. The approximation is presented as a series of gramm a r transforms, and is exact for leftqinear and rightlinear CFGs, and for trees up to a user-specified depth of center-embedding. 1 Introduction This p a p e r describes a method for approximating grammm&apos;s with finite-state machines. Unlike the method derived from the LR(k) parsing algorithm described in Pereira and Wright (1991), these methods use g r a m m a r transformations based on the left-corner g r a m m a r transform (Rosenkrantz and Lewis II, 1970; Aho and Ulhnan, 1972). One advantage of the left, corner methods is that they generalize straightforwardly to complex feature &quot;unification based&quot; grammars, unlike the LR(k) based at&gt; proach. For example, the implementation described here translates a DCG version of the example gramm a r given by Pereira and Wright (1991) directly into a FSM without constructing an approximating CFG. Left-corner based techniques are natural for this kind of application because (wit"
C98-1098,C92-1032,0,0.032283,"ext section, which minimize stack depth in grammars that contain productions of length no greater than two. 3.1 A tail-recursion optimization If G is a left-linear grammar, a top-down parser using Z:C1(G) can recognize any string generated by (7 with a constant-bounded stack size. However, the corresponding operation with right-linear grammars requires a stack of size proportional to the length of the string, since the stack fills with paired categories A - A for each non-left-corner nonterminal in the analysis tree. The &apos;tail recursion&apos; or &apos;composition&apos; optimization (Abney and Johnson, 1991; Resnik, 1992) permits right-branching structures to be parsed with bounded stack depth. It is the result of epsilon removal applied to the output of £C1, and can be described in terms of resolution or partial evaluation of the transformed grammar with respect to productions (1.c). In effect, the schema (1.b) is split into two cases, depending on whether or not the rightmost nonterminal A - B is expanded by the epsilon rules produced by schema (1.c). This expansion yields a grammar £C2(G) = ( N &apos; , T, P2, S), where P2 contains all productions of the form (2.a-2.c). (In these schemata A , B E N; a E T; X E N"
C98-1098,P85-1018,0,0.0355232,"re 1. no transitions to any stack state whose size is larger than some user-specified limit. 1 This restriction ensures that there is only a finite number of possible stack states, and hence t h a t the top down parser is an finite-state machine. The resulting finite-state machine accepts a subset of the language generated by the original grammar. The situation becomes more complicated when we move to &apos;unification-based&apos; g r a m m a r s , since there may be an unbounded number of different categories appearing in the accessible stack states. In the system implemented here we used restriction (Shieber, 1985) on the stack states to restrict attention to a finite number of distinct stack states for any given stack depth. Since the restriction operation maps a stack state to a more general one, it produces a finite-state approximation which accepts a superset of the language generated by the original unification grammar. Thus for general constraint-based grammars the language accepted by our finite-state apt)roximation is not guaranteed to be either a superset or a subset of the language generated by the input grammar. 2 The left-corner transform While conceptually simple, the top-down parsing algor"
C98-1098,J97-3004,0,0.0156011,"e &quot;unification&quot; constraints should be associated with transitions from LR state to LR state (see Nakazawa (1995) for one proposal), in contrast, extending the techniques described here to complex feature based &quot;unification&quot; g r a m m a r is straight-forward. The main complication is the filter on useless nonterminals and productions just discussed. Generalizing the left-corner closure filter on pair categories to complex feature &quot;unification&quot; g r a m m a r s in an efficient way is complicated, and is the primary difficulty in using left-corner methods with complex feature based grammars, vail Noord (1997) provides a detailed discussion of methods for using such a &quot;left-corner filter&quot; in unification-grammar parsing, and the methods he discusses are used in the implementation described below. 3 Extended left-corner transforms This section presents some simple extensions to the basic left-corner transform presented abow~&apos;. The &apos;tail-recursion&apos; optimization permits bounded-stack parsing of both left, and right linear constructions. Further manipulation of this transform puts it into a form in which we can identify precisely the tree configurations in the original g r a m m a r which cause the stac"
D07-1031,C04-1080,0,0.614884,"for each word (Merialdo, 1994; Smith and Eisner, 2005; Goldwater and Griffiths, 2007) or a small number of “prototypes” for each POS (Haghighi and Klein, 2006). In the context of semisupervised learning using a tag lexicon, Wang and Schuurmans (2005) observe discrepencies between the empirical and estimated tag frequencies similar to those observed here, and show that constraining the estimation procedure to preserve the empirical frequencies improves tagging accuracy. (This approach cannot be used in an unsupervised setting since the empirical tag distribution is not available). However, as Banko and Moore (2004) point out, the accuracy achieved by these unsupervised methods depends strongly on the precise nature of the supervised training data (in their case, the ambiguity of the tag lexicon available to the system), which makes it more difficult to understand the behaviour of such systems. 2 Evaluation All of the experiments described below have the same basic structure: an estimator is used to infer a bitag HMM from the unsupervised training corpus (the words of Penn Treebank (PTB) Wall Street Journal corpus (Marcus et al., 1993)), and then the resulting model is used to label each word of that cor"
D07-1031,N07-1018,1,0.0489161,"assed through the function f (v) = exp ψ(v), which is plotted in Figure 5. When v  0, f (v) ≈ v − 0.5, so roughly speaking, VB for multinomials involves adding α−0.5 to the expected 302 counts when they are much larger than zero, where α is the Dirichlet prior parameter. Thus VB can be viewed as a more principled version of the wellknown ad hoc technique for approximating Bayesian estimation with EM that involves adding α−1 to the expected counts. However, in the ad hoc approach the expected count plus α − 1 may be less than zero, resulting in a value of zero for the corresponding parameter (Johnson et al., 2007; Goldwater and Griffiths, 2007). VB avoids this problem because f (v) is always positive when v &gt; 0, even when v is small. Note that because the counts are passed through f , the updated values for θ˜ and φ˜ in (4) are in general not normalized; this is because the variational free energy is only an upper bound on the negative log likelihood (Beal, 2003). We found that in general VB performed much better than GS. Computationally it is very similar to EM, and each iteration takes essentially the same time as an EM iteration. Again, we experimented with annealing in the hope of speeding converg"
D07-1031,E03-1009,0,0.390951,"hile the empirical distribution of POS tags is heavily skewed towards a few high-frequency tags. Based on this, we propose a Bayesian prior that biases the system toward more skewed distributions and show that this raises the 1-to-1 accuracy significantly. Finally, we show that a similar increase in accuracy can be achieved by reducing the number of hidden states in the models estimated by EM. There is certainly much useful information that bitag HMMs models cannot capture. Toutanova et al. (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporating some of these in an unsupervised tagging model. However, bitag models are rich enough to capture at least some distributional information (i.e., the tag 296 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 296–305, Prague, June 2007. 2007 Association for Computational Linguistics for a word depends on the tags assigned to its neighbours). Moreover, more complex models add additional complicating factors that interact in ways still poorly understood; for example, smooth"
D07-1031,J93-2004,0,0.0319065,"since the empirical tag distribution is not available). However, as Banko and Moore (2004) point out, the accuracy achieved by these unsupervised methods depends strongly on the precise nature of the supervised training data (in their case, the ambiguity of the tag lexicon available to the system), which makes it more difficult to understand the behaviour of such systems. 2 Evaluation All of the experiments described below have the same basic structure: an estimator is used to infer a bitag HMM from the unsupervised training corpus (the words of Penn Treebank (PTB) Wall Street Journal corpus (Marcus et al., 1993)), and then the resulting model is used to label each word of that corpus with one of the HMM’s hidden states. This section describes how we evaluate how well these sequences of hidden states correspond to the goldstandard POS tags for the training corpus (here, the PTB POS tags). The chief difficulty is determining the correspondence between the hidden states and the gold-standard POS tags. Perhaps the most straightforward method of establishing this correspondence is to deterministically map each hidden state to the POS tag it co-occurs 297 most frequently with, and return the proportion of"
D07-1031,P07-1094,0,0.777515,"ours). Moreover, more complex models add additional complicating factors that interact in ways still poorly understood; for example, smoothing is generally regarded as essential for higher-order HMMs, yet it is not clear how to integrate smoothing into unsupervised estimation procedures (Goodman, 2001; Wang and Schuurmans, 2005). Most previous work exploiting unsupervised training data for inferring POS tagging models has focused on semi-supervised methods in the in which the learner is provided with a lexicon specifying the possible tags for each word (Merialdo, 1994; Smith and Eisner, 2005; Goldwater and Griffiths, 2007) or a small number of “prototypes” for each POS (Haghighi and Klein, 2006). In the context of semisupervised learning using a tag lexicon, Wang and Schuurmans (2005) observe discrepencies between the empirical and estimated tag frequencies similar to those observed here, and show that constraining the estimation procedure to preserve the empirical frequencies improves tagging accuracy. (This approach cannot be used in an unsupervised setting since the empirical tag distribution is not available). However, as Banko and Moore (2004) point out, the accuracy achieved by these unsupervised methods"
D07-1031,J94-2001,0,0.943634,"Sampling (GS) and Variational Bayes (VB) estimators and show that VB converges faster than GS for this task and that VB significantly improves 1-to-1 tagging accuracy over EM. We also show that EM does nearly as well as VB when the number of hidden HMM states is dramatically reduced. We also point out the high variance in all of these estimators, and that they require many more iterations to approach convergence than usually thought. 1 Introduction It is well known that Expectation-Maximization (EM) performs poorly in unsupervised induction of linguistic structure (Carroll and Charniak, 1992; Merialdo, 1994; Klein, 2005; Smith, 2006). In retrospect one can certainly find reasons to explain this failure: after all, likelihood does not appear in the wide variety of linguistic tests proposed for identifying linguistic structure (Fromkin, 2001). This paper focuses on unsupervised part-ofspeech (POS) tagging, because it is perhaps the simplest linguistic induction task. We suggest that one reason for the apparent failure of EM for POS tagging is that it tends to assign relatively equal numbers of tokens to each hidden state, while the empirical distribution of POS tags is highly skewed, like many lin"
D07-1031,N06-1041,0,0.617073,"nteract in ways still poorly understood; for example, smoothing is generally regarded as essential for higher-order HMMs, yet it is not clear how to integrate smoothing into unsupervised estimation procedures (Goodman, 2001; Wang and Schuurmans, 2005). Most previous work exploiting unsupervised training data for inferring POS tagging models has focused on semi-supervised methods in the in which the learner is provided with a lexicon specifying the possible tags for each word (Merialdo, 1994; Smith and Eisner, 2005; Goldwater and Griffiths, 2007) or a small number of “prototypes” for each POS (Haghighi and Klein, 2006). In the context of semisupervised learning using a tag lexicon, Wang and Schuurmans (2005) observe discrepencies between the empirical and estimated tag frequencies similar to those observed here, and show that constraining the estimation procedure to preserve the empirical frequencies improves tagging accuracy. (This approach cannot be used in an unsupervised setting since the empirical tag distribution is not available). However, as Banko and Moore (2004) point out, the accuracy achieved by these unsupervised methods depends strongly on the precise nature of the supervised training data (in"
D07-1031,P05-1044,0,0.465345,"s assigned to its neighbours). Moreover, more complex models add additional complicating factors that interact in ways still poorly understood; for example, smoothing is generally regarded as essential for higher-order HMMs, yet it is not clear how to integrate smoothing into unsupervised estimation procedures (Goodman, 2001; Wang and Schuurmans, 2005). Most previous work exploiting unsupervised training data for inferring POS tagging models has focused on semi-supervised methods in the in which the learner is provided with a lexicon specifying the possible tags for each word (Merialdo, 1994; Smith and Eisner, 2005; Goldwater and Griffiths, 2007) or a small number of “prototypes” for each POS (Haghighi and Klein, 2006). In the context of semisupervised learning using a tag lexicon, Wang and Schuurmans (2005) observe discrepencies between the empirical and estimated tag frequencies similar to those observed here, and show that constraining the estimation procedure to preserve the empirical frequencies improves tagging accuracy. (This approach cannot be used in an unsupervised setting since the empirical tag distribution is not available). However, as Banko and Moore (2004) point out, the accuracy achieve"
D07-1031,N03-1033,0,0.0359196,"is by observing that the distribution of hidden states to words proposed by the EMestimated HMMs is relatively uniform, while the empirical distribution of POS tags is heavily skewed towards a few high-frequency tags. Based on this, we propose a Bayesian prior that biases the system toward more skewed distributions and show that this raises the 1-to-1 accuracy significantly. Finally, we show that a similar increase in accuracy can be achieved by reducing the number of hidden states in the models estimated by EM. There is certainly much useful information that bitag HMMs models cannot capture. Toutanova et al. (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporating some of these in an unsupervised tagging model. However, bitag models are rich enough to capture at least some distributional information (i.e., the tag 296 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 296–305, Prague, June 2007. 2007 Association for Computational Linguistics for a word depends on the tags assigned to its neighbours). Moreover, more complex"
D08-1036,E03-1009,0,0.300792,"aluation measure; e.g., a tagger which assigns all words the same single part-of-speech tag does disturbingly well under Variation of Information, suggesting that a poor tagger may score well under VI. In order to avoid this problem we focus here on evaluation measures that construct an explicit mapping between the gold-standard part-of-speech tags and the HMM’s states. Perhaps the most straightforward approach is to map each HMM state to the part-of-speech tag it co-occurs with most frequently, and use this mapping to map each HMM state sequence t to a sequence of part-of-speech tags. But as Clark (2003) observes, this approach has several defects. If a system is permitted to posit an unbounded number of states (which is not the case here) it can achieve a perfect score on by assigning each word token its own unique state. We can partially address this by cross-validation. We divide the corpus into two equal parts, and from the first part we extract a mapping from HMM states to the parts-of-speech they co-occur with most frequently, and use that mapping to map the states of the second part of the corpus to parts-of-speech. We call the accuracy of the resulting tagging the crossvalidation accu"
D08-1036,P07-1094,0,0.65589,"6; MacKay, 2003). Instead, rather than commiting to a single value for the parameters θ many Bayesians often prefer to work with the full posterior distribution P(θ |d), as this naturally reflects the uncertainty in θ’s value. In all but the simplest models there is no known closed form for the posterior distribution. However, the Bayesian literature describes a number of methods for approximating the posterior P(θ |d). Monte Carlo sampling methods and Variational Bayes are two kinds of approximate inference methods that have been applied to Bayesian inference of unsupervised HMM POS taggers (Goldwater and Griffiths, 2007; Johnson, 2007). These methods can also be used to approximate other distributions that are important to us, such as the conditional distribution P(t |w) of POS tags (i.e., HMM hidden states) t given words w. This recent literature reports contradictory results about these Bayesian inference methods. John345 son (2007) compared two Bayesian inference algorithms, Variational Bayes and what we call here a point-wise collapsed Gibbs sampler, and found that Variational Bayes produced the best solution, and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM"
D08-1036,N06-1041,0,0.387997,"has several defects. If a system is permitted to posit an unbounded number of states (which is not the case here) it can achieve a perfect score on by assigning each word token its own unique state. We can partially address this by cross-validation. We divide the corpus into two equal parts, and from the first part we extract a mapping from HMM states to the parts-of-speech they co-occur with most frequently, and use that mapping to map the states of the second part of the corpus to parts-of-speech. We call the accuracy of the resulting tagging the crossvalidation accuracy. Finally, following Haghighi and Klein (2006) and Johnson (2007) we can instead insist that at most one HMM state can be mapped to any part-of-speech tag. Following these authors, we used a greedy algorithm to associate states with POS tags; the accuracy of the resulting tagging is called the greedy 1-to-1 All − 50 All − 17 120K − 50 120K − 17 24K − 50 24K − 17 EM 0.40527 0.43101 0.29303 0.35202 0.18618 0.28165 VB 0.46123 0.51379 0.34679 0.36010 0.23823 0.36599 0.43424 0.36984 0.44125 0.29953 0.36811 GSe,p 0.47826 GSe,b 0.49371 0.46568 0.38888 0.44341 0.34404 0.37032 ? 0.45028 0.42785 0.43652 0.39182 0.39164 GSc,p 0.49910 0.41162 0.42278"
D08-1036,N07-1018,1,0.547162,"pointwise sampler requires O(nm) time per iteration, while a blocked sampler requires O(nm2 ) time per iteration, where m is the number of HMM states and n is the length of the training corpus. Second, the sampler can either be explicit or collapsed. An explicit sampler represents and samples the HMM parameters θ and φ in addition to the states t, while in a collapsed sampler the HMM parameters are integrated out, and only the states t are sampled. The difference between explicit and collapsed samplers corresponds exactly to the difference between the two PCFG sampling algorithms presented in Johnson et al. (2007). An iteration of the pointwise explicit Gibbs sampler consists of resampling θ and φ given the stateto-state transition counts n and state-to-word emission counts n0 using (5), and then resampling each state ti given the corresponding word wi and the neighboring states ti−1 and ti+1 using (6). θ t |nt , α ∼ Dir(nt + α) φt |n0t , α0 ∼ Dir(n0t + α0 ) (5) P(ti |wi , t−i , θ, φ) ∝ θti |ti−1 φwi |ti θti+1 |ti (6) The Dirichlet distributions in (5) are non-uniform; nt is the vector of state-to-state transition counts in t leaving state t in the current state vector t, while n0t is the vector of sta"
D08-1036,D07-1031,1,0.666356,"er than commiting to a single value for the parameters θ many Bayesians often prefer to work with the full posterior distribution P(θ |d), as this naturally reflects the uncertainty in θ’s value. In all but the simplest models there is no known closed form for the posterior distribution. However, the Bayesian literature describes a number of methods for approximating the posterior P(θ |d). Monte Carlo sampling methods and Variational Bayes are two kinds of approximate inference methods that have been applied to Bayesian inference of unsupervised HMM POS taggers (Goldwater and Griffiths, 2007; Johnson, 2007). These methods can also be used to approximate other distributions that are important to us, such as the conditional distribution P(t |w) of POS tags (i.e., HMM hidden states) t given words w. This recent literature reports contradictory results about these Bayesian inference methods. John345 son (2007) compared two Bayesian inference algorithms, Variational Bayes and what we call here a point-wise collapsed Gibbs sampler, and found that Variational Bayes produced the best solution, and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM. On the other h"
D08-1036,P05-1044,0,0.128934,"(b) 1-to-1 accuracy as a function of running time on a 3GHz dual quad-core Pentium for the four different Gibbs samplers on all data and 50 hidden states. Each iteration took approximately 96 sec. for the collapsed blocked sampler, 7.5 sec. for the collapsed pointwise sampler, 25 sec. for the explicit blocked sampler and 4.4 sec. for the explicit pointwise sampler. 350 accuracy. The studies presented by Goldwater and Griffiths (2007) and Johnson (2007) differed in the number of states that they used. Goldwater and Griffiths (2007) evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full Penn Treebank tag set. We ran all our estimators in both conditions here (thanks to Noah Smith for supplying us with his tag set). Also, the studies differed in the size of the corpora used. The largest corpus that Goldwater and Griffiths (2007) studied contained 96,000 words, while Johnson (2007) used all of the 1,173,766 words in the full Penn WSJ treebank. For that reason we ran all our estimators on corpora containing 24,000 words and 120,000 words as well as the full treebank. We ran each estimator with the eight different combinations of"
D10-1120,W09-0106,0,0.0418509,"7) demonstrated empirical gains on a synthetic corpus, our experiments focus on unsupervised category refinement on real language data. Universal Rules in NLP Despite the recent surge of interest in multilingual learning (Kuhn, 2004; Cohen and Smith, 2009a; Snyder et al., 2009; BergKirkpatrick and Klein, 2010), there is surprisingly little computational work on linguistic universals. On the acquisition side, Daum´e III and Campbell (2007) proposed a computational technique for discovering universal implications in typological features. More closely related to our work is the position paper by Bender (2009), which advocates the use of manually-encoded cross-lingual generalizations for the development of NLP systems. She argues that a system employing such knowledge could be easily adapted to a particular language by specializing this high level knowledge based on the typological features of the language. We also argue that cross-language universals are beneficial for automatic language processing; however, our focus is on learning language-specific adaptations of these rules from data. 3 For each observed coarse symbol s: 1. Draw top-level infinite multinomial over subsymbols βs ∼ GEM(γ). 2. For"
D10-1120,P10-1131,0,0.754224,"spite surface differences, human languages exhibit striking similarities in many fundamental aspects of syntactic structure. These structural correspondences, referred to as syntactic universals, have been extensively studied in linguistics (Baker, 2001; Carnie, 2002; White, 2003; Newmeyer, 2005) and underlie many approaches in multilingual parsing. In fact, much recent work has demonstrated that learning cross-lingual correspondences from corpus data greatly reduces the ambiguity inherent in syntactic analysis (Kuhn, 2004; Burkett and Klein, 2008; Cohen and Smith, 2009a; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010). 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/dependency/ In this paper, we present an alternative grammar induction approach that exploits these structural correspondences by declaratively encoding a small set of universal dependency rules. As input to the model, we assume a corpus annotated with coarse syntactic categories (i.e., high-level part-ofspeech tags) and a set of universal rules defined over these categories, such as those in Table 1. These rules incorporate the definitional properties of syntactic categories in terms"
D10-1120,W06-2920,0,0.0604008,"of hand-crafted dependency rules designed by Michael Collins3 for deterministic parsing, shown in Table 3. Unlike the universals from Table 1, these rules alone are enough to construct a full dependency tree. Thus they allow us to judge whether the model is able to improve upon a human-engineered deterministic parser. Moreover, with this dataset we can assess the additional benefit of using rules tailored to an individual language as opposed to universal rules. 6 dencies with the Collins head finding rules (Collins, 1999); for the other languages we use data from the 2006 CoNLL-X Shared Task (Buchholz and Marsi, 2006). Each dataset provides manually annotated part-of-speech tags that are used for both training and testing. For comparison purposes with previous work, we limit the cross-lingual experiments to sentences of length 10 or less (not counting punctuation). For English, we also explore sentences of length up to 20. The final output metric is directed dependency accuracy. This is computed based on the Viterbi parses produced using the final unnormalized variational distribution q(z) over dependency structures. Hyperparameters and Training Regimes Unless otherwise stated, in experiments with rule-bas"
D10-1120,D08-1092,0,0.0186701,"n explanation of the ruleset is provided in Section 5. Introduction Despite surface differences, human languages exhibit striking similarities in many fundamental aspects of syntactic structure. These structural correspondences, referred to as syntactic universals, have been extensively studied in linguistics (Baker, 2001; Carnie, 2002; White, 2003; Newmeyer, 2005) and underlie many approaches in multilingual parsing. In fact, much recent work has demonstrated that learning cross-lingual correspondences from corpus data greatly reduces the ambiguity inherent in syntactic analysis (Kuhn, 2004; Burkett and Klein, 2008; Cohen and Smith, 2009a; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010). 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/dependency/ In this paper, we present an alternative grammar induction approach that exploits these structural correspondences by declaratively encoding a small set of universal dependency rules. As input to the model, we assume a corpus annotated with coarse syntactic categories (i.e., high-level part-ofspeech tags) and a set of universal rules defined over these categories, such as those in Table 1. These"
D10-1120,P07-1036,0,0.045162,"languages share a high-level Indo-European ancestry, they cover a diverse range of syntactic phenomenon. Our results demonstrate that universal rules greatly improve the accuracy of dependency parsing across all of these languages, outperforming current stateof-the-art unsupervised grammar induction methods (Headden III et al., 2009; Berg-Kirkpatrick and Klein, 2010). 2 Related Work Learning with Linguistic Constraints Our work is situated within a broader class of unsupervised approaches that employ declarative knowledge to improve learning of linguistic structure (Haghighi and Klein, 2006; Chang et al., 2007; Grac¸a et al., 2007; Cohen and Smith, 2009b; Druck et al., 2009; Liang et al., 2009a). The way we apply constraints is clos1235 est to the latter two approaches of posterior regularization and generalized expectation criteria. In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010). This design enables the model to reflect constraints that are difficult to encode via the model structure or as priors on its parameters. In their approach, parameters are"
D10-1120,N09-1009,0,0.505595,"set is provided in Section 5. Introduction Despite surface differences, human languages exhibit striking similarities in many fundamental aspects of syntactic structure. These structural correspondences, referred to as syntactic universals, have been extensively studied in linguistics (Baker, 2001; Carnie, 2002; White, 2003; Newmeyer, 2005) and underlie many approaches in multilingual parsing. In fact, much recent work has demonstrated that learning cross-lingual correspondences from corpus data greatly reduces the ambiguity inherent in syntactic analysis (Kuhn, 2004; Burkett and Klein, 2008; Cohen and Smith, 2009a; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010). 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/dependency/ In this paper, we present an alternative grammar induction approach that exploits these structural correspondences by declaratively encoding a small set of universal dependency rules. As input to the model, we assume a corpus annotated with coarse syntactic categories (i.e., high-level part-ofspeech tags) and a set of universal rules defined over these categories, such as those in Table 1. These rules incorporate the"
D10-1120,P09-2001,0,0.331782,"set is provided in Section 5. Introduction Despite surface differences, human languages exhibit striking similarities in many fundamental aspects of syntactic structure. These structural correspondences, referred to as syntactic universals, have been extensively studied in linguistics (Baker, 2001; Carnie, 2002; White, 2003; Newmeyer, 2005) and underlie many approaches in multilingual parsing. In fact, much recent work has demonstrated that learning cross-lingual correspondences from corpus data greatly reduces the ambiguity inherent in syntactic analysis (Kuhn, 2004; Burkett and Klein, 2008; Cohen and Smith, 2009a; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010). 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/dependency/ In this paper, we present an alternative grammar induction approach that exploits these structural correspondences by declaratively encoding a small set of universal dependency rules. As input to the model, we assume a corpus annotated with coarse syntactic categories (i.e., high-level part-ofspeech tags) and a set of universal rules defined over these categories, such as those in Table 1. These rules incorporate the"
D10-1120,P07-1009,0,0.0163131,"Missing"
D10-1120,P09-1041,0,0.178641,"a diverse range of syntactic phenomenon. Our results demonstrate that universal rules greatly improve the accuracy of dependency parsing across all of these languages, outperforming current stateof-the-art unsupervised grammar induction methods (Headden III et al., 2009; Berg-Kirkpatrick and Klein, 2010). 2 Related Work Learning with Linguistic Constraints Our work is situated within a broader class of unsupervised approaches that employ declarative knowledge to improve learning of linguistic structure (Haghighi and Klein, 2006; Chang et al., 2007; Grac¸a et al., 2007; Cohen and Smith, 2009b; Druck et al., 2009; Liang et al., 2009a). The way we apply constraints is clos1235 est to the latter two approaches of posterior regularization and generalized expectation criteria. In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010). This design enables the model to reflect constraints that are difficult to encode via the model structure or as priors on its parameters. In their approach, parameters are estimated using a modified EM algorithm, where the E-step minimiz"
D10-1120,P07-1035,0,0.0148978,"Missing"
D10-1120,P09-1042,0,0.101254,"d Work Learning with Linguistic Constraints Our work is situated within a broader class of unsupervised approaches that employ declarative knowledge to improve learning of linguistic structure (Haghighi and Klein, 2006; Chang et al., 2007; Grac¸a et al., 2007; Cohen and Smith, 2009b; Druck et al., 2009; Liang et al., 2009a). The way we apply constraints is clos1235 est to the latter two approaches of posterior regularization and generalized expectation criteria. In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010). This design enables the model to reflect constraints that are difficult to encode via the model structure or as priors on its parameters. In their approach, parameters are estimated using a modified EM algorithm, where the E-step minimizes the KL-divergence between the model posterior and the set of distributions that satisfies the constraints. Our approach also expresses constraints as expectations on the posterior; we utilize the machinery of their framework within a variational inference algorithm with a mean field approximation. Generalized exp"
D10-1120,P06-1111,0,0.0961114,"and Swedish. Though these languages share a high-level Indo-European ancestry, they cover a diverse range of syntactic phenomenon. Our results demonstrate that universal rules greatly improve the accuracy of dependency parsing across all of these languages, outperforming current stateof-the-art unsupervised grammar induction methods (Headden III et al., 2009; Berg-Kirkpatrick and Klein, 2010). 2 Related Work Learning with Linguistic Constraints Our work is situated within a broader class of unsupervised approaches that employ declarative knowledge to improve learning of linguistic structure (Haghighi and Klein, 2006; Chang et al., 2007; Grac¸a et al., 2007; Cohen and Smith, 2009b; Druck et al., 2009; Liang et al., 2009a). The way we apply constraints is clos1235 est to the latter two approaches of posterior regularization and generalized expectation criteria. In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010). This design enables the model to reflect constraints that are difficult to encode via the model structure or as priors on its parameters. In their appro"
D10-1120,N09-1012,1,0.722165,"Missing"
D10-1120,P04-1061,0,0.74553,"e is formed. Each node of the dependency tree is comprised of three random variables: an observed coarse symbol s, a hidden refined subsymbol z, and an observed word x. In the following let the parent of the current node have symbol s0 and subsymbol z 0 ; the root node is generated from separate root-specific distributions. Subsymbol refinement is an optional component of the full model and can be omitted by deterministically equating s and z. As we explain at the end of this section, without this aspect the generative story closely resembles the classic dependency model with valence (DMV) of Klein and Manning (2004). First we draw symbol s from a finite multinomial s z x θszc - βs πss0 z0 c - φsz - coarse symbol (observed) refined subsymbol word (observed) distr over child coarse symbols for each parent s and z and context c top-level distr over subsymbols for s distr over subsymbols for each s, parent s0 and z 0 , and context c distr over words for s and z Figure 1: Graphical representation of the model and a summary of the notation. There is a copy of the outer plate for each distinct symbol in the observed coarse tags. Here, node 3 is shown to be the parent of nodes 1 and 2. Shaded variables are obser"
D10-1120,P04-1060,0,0.247121,"ategories. An explanation of the ruleset is provided in Section 5. Introduction Despite surface differences, human languages exhibit striking similarities in many fundamental aspects of syntactic structure. These structural correspondences, referred to as syntactic universals, have been extensively studied in linguistics (Baker, 2001; Carnie, 2002; White, 2003; Newmeyer, 2005) and underlie many approaches in multilingual parsing. In fact, much recent work has demonstrated that learning cross-lingual correspondences from corpus data greatly reduces the ambiguity inherent in syntactic analysis (Kuhn, 2004; Burkett and Klein, 2008; Cohen and Smith, 2009a; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010). 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/dependency/ In this paper, we present an alternative grammar induction approach that exploits these structural correspondences by declaratively encoding a small set of universal dependency rules. As input to the model, we assume a corpus annotated with coarse syntactic categories (i.e., high-level part-ofspeech tags) and a set of universal rules defined over these categories, such a"
D10-1120,D07-1072,0,0.0328162,"vised counterpart. The constraints they studied are corpus- and languagespecific. Our work demonstrates that a small set of language-independent universals can also serve as effective constraints. Furthermore, we find that our method outperforms the generalized expectation approach using corpus-specific constraints. Learning to Refine Syntactic Categories Recent research has demonstrated the usefulness of automatically refining the granularity of syntactic categories. While most of the existing approaches are implemented in the supervised setting (Finkel et al., 2007; Petrov and Klein, 2007), Liang et al. (2007) propose a non-parametric Bayesian model that learns the granularity of PCFG categories in an unsupervised fashion. For each non-terminal grammar symbol, the model posits a Hierarchical Dirichlet Process over its refinements (subsymbols) to automatically learn the granularity of syntactic categories. As with their work, we also use nonparametric priors for category refinement and employ variational methods for inference. However, our goal is to apply category refinement to dependency parsing, rather than to PCFGs, requiring a substantially different model formulation. While Liang et al. (2007)"
D10-1120,J93-2004,0,0.0410905,"experiments each coarse symbol corresponds to only one refined symbol. This is easily effected during inference by setting the HDP variational approximation truncation level to one. For each experiment we run 50 iterations of variational updates; for each iteration we perform five steps of gradient search to compute the update for the variational distribution q(z) over dependency structures. Experimental Setup Datasets and Evaluation We test the effectiveness of our grammar induction approach on English, Danish, Portuguese, Slovene, Spanish, and Swedish. For English we use the Penn Treebank (Marcus et al., 1993), transformed from CFG parses into depen3 Personal communication. 1240 7 Results In the following section we present our primary cross-lingual results using universal rules (Section 7.1) before performing a more in-depth analysis of model properties such as sensitivity to ruleset selection and inference stability (Section 7.2). English Danish Portuguese Slovene Spanish Swedish DMV 47.1 33.5 38.5 38.5 28.0 45.3 PGI 62.3 41.6 63.0 48.4 58.4 58.3 No-Split 71.5 48.8 54.0 50.6 64.8 63.3 HDP-DEP 71.9 (0.3) 51.9 (1.6) 71.5 (0.5) 50.9 (5.5) 67.2 (0.4) 62.1 (0.5) Table 4: Directed dependency accuracy u"
D10-1120,P09-1009,1,0.91835,"on 5. Introduction Despite surface differences, human languages exhibit striking similarities in many fundamental aspects of syntactic structure. These structural correspondences, referred to as syntactic universals, have been extensively studied in linguistics (Baker, 2001; Carnie, 2002; White, 2003; Newmeyer, 2005) and underlie many approaches in multilingual parsing. In fact, much recent work has demonstrated that learning cross-lingual correspondences from corpus data greatly reduces the ambiguity inherent in syntactic analysis (Kuhn, 2004; Burkett and Klein, 2008; Cohen and Smith, 2009a; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010). 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/dependency/ In this paper, we present an alternative grammar induction approach that exploits these structural correspondences by declaratively encoding a small set of universal dependency rules. As input to the model, we assume a corpus annotated with coarse syntactic categories (i.e., high-level part-ofspeech tags) and a set of universal rules defined over these categories, such as those in Table 1. These rules incorporate the definitional propertie"
D10-1120,E06-1027,0,\N,Missing
D10-1120,banea-etal-2008-bootstrapping,0,\N,Missing
D10-1120,kamps-etal-2004-using,0,\N,Missing
D10-1120,C00-1044,0,\N,Missing
D10-1120,H05-1044,0,\N,Missing
D10-1120,ruppenhofer-etal-2008-finding,0,\N,Missing
D10-1120,W03-1017,0,\N,Missing
D10-1120,H05-2017,0,\N,Missing
D10-1120,H05-1043,0,\N,Missing
D10-1120,W06-1642,0,\N,Missing
D10-1120,W03-1014,0,\N,Missing
D10-1120,J03-4003,0,\N,Missing
D10-1120,P03-1054,0,\N,Missing
D10-1120,C04-1200,0,\N,Missing
D10-1120,P08-1081,0,\N,Missing
D10-1120,2007.sigdial-1.5,0,\N,Missing
D10-1120,H05-1091,0,\N,Missing
D10-1120,P05-1017,0,\N,Missing
D11-1131,P07-1035,0,0.0524797,"Missing"
D11-1131,C10-2062,0,0.075347,"ent of Computing Macquarie University Sydney, Australia Bevan K. Jones School of Informatics University of Edinburgh Edinburgh, UK Mark Johnson Department of Computing Macquarie University Sydney, Australia benjamin.borschinger@mq.edu.au b.k.jones@sms.ed.ac.uk mark.johnson@mq.edu.au 1 Abstract to assign certain meanings to the linguistic input she is confronted with. It is often assumed that ‘grounded’ learning tasks are beyond the scope of grammatical inference techniques. In this paper, we show that the grounded task of learning a semantic parser from ambiguous training data as discussed in Kim and Mooney (2010) can be reduced to a Probabilistic Context-Free Grammar learning task in a way that gives state of the art results. We further show that additionally letting our model learn the language’s canonical word order improves its performance and leads to the highest semantic parsing f-scores previously reported in the literature.1 In this sense, learning a semantic parser seems to go beyond the well-studied task of unsupervised grammar induction. It involves not only learning a grammar for the form-side of language, i.e. language expressions such as sentences, but also the ‘grounding’ of this structu"
D11-1131,D07-1072,0,0.0831386,"Missing"
D11-1131,P09-1011,0,0.0177871,"he observed forms. Essentially, it seems as if ‘grounded’ learning tasks like this require dealing with two different kinds of information, the purely formal (phonemic) and meaningful (semantic) aspects of language. Grammatical inference seems to be limited to dealing with one level of formal information (Chang and Maia, 2001). For this reason, probably, approaches to the task of learning a semantic parser employ a variety of sophisticated and task-specific techniques that go beyond (but often elaborate on) the techniques used for grammatical inference (Lu et al., 2008; Chen and Mooney, 2008; Liang et al., 2009; Kim and Mooney, 2010; Chen et al., 2010). Introduction One of the most fundamental ideas about language is that we use it to express our thoughts. Learning a natural language, then, amounts to (at least) learning a mapping between the things we utter and the things we think, and can therefore be seen as the task of learning a semantic parser, i.e. something that maps natural language expressions such as sentences into meaning representations such as logical forms. Obviously, this learning can neither take place in a fully supervised nor in a fully unsupervised fashion: the learner does not ‘"
D11-1131,D08-1082,0,0.67217,"that provides a clue to the meaning of the observed forms. Essentially, it seems as if ‘grounded’ learning tasks like this require dealing with two different kinds of information, the purely formal (phonemic) and meaningful (semantic) aspects of language. Grammatical inference seems to be limited to dealing with one level of formal information (Chang and Maia, 2001). For this reason, probably, approaches to the task of learning a semantic parser employ a variety of sophisticated and task-specific techniques that go beyond (but often elaborate on) the techniques used for grammatical inference (Lu et al., 2008; Chen and Mooney, 2008; Liang et al., 2009; Kim and Mooney, 2010; Chen et al., 2010). Introduction One of the most fundamental ideas about language is that we use it to express our thoughts. Learning a natural language, then, amounts to (at least) learning a mapping between the things we utter and the things we think, and can therefore be seen as the task of learning a semantic parser, i.e. something that maps natural language expressions such as sentences into meaning representations such as logical forms. Obviously, this learning can neither take place in a fully supervised nor in a fully u"
D12-1064,D11-1131,1,0.787979,"Missing"
D12-1064,P05-1022,1,0.597221,"Missing"
D12-1064,D10-1028,0,0.104201,"ul in NLI (Koppel et al., 2005; Tsur and Rappoport, 2007; Estival et al., 2007). The recent work of Wong and Dras (2011), motivated by ideas from Second Language Acquisition (SLA), has shown that syntactic features — potentially capturing syntactic erAdaptor grammars (Johnson, 2010), a hierarchical non-parametric extension of PCFGs (and also interpretable as an extension of LDA-based topic models), hold out some promise here. In that initial work, Johnson’s model learnt collocations of arbitrary length such as gradient descent and cost function, under a topic associated with machine learning. Hardisty et al. (2010) applied this idea to perspective classification, learning collocations such as palestinian violence and palestinian freedom, the use of which as features was demonstrated to help the classification of texts from the Bitter Lemons corpus as either Palestinian or Israeli perspective. Typically in NLI and other authorship attribution tasks, the feature sets exclude content words, to avoid unfair cues due to potentially different domains of discourse. In our context, then, what we are interested in are ‘quasi-syntactic collocations’ of either pure PoS (e.g. NN IN NN) or a mixture of PoS with func"
D12-1064,P10-1117,1,0.882795,"second language — native language identification (NLI) — has, since the seminal work of Koppel et al. (2005), been primarily tackled as a text classification task using supervised machine learning techniques. Lexical features, such as function words, character n-grams, and part-ofspeech (PoS) n-grams, have been proven to be useful in NLI (Koppel et al., 2005; Tsur and Rappoport, 2007; Estival et al., 2007). The recent work of Wong and Dras (2011), motivated by ideas from Second Language Acquisition (SLA), has shown that syntactic features — potentially capturing syntactic erAdaptor grammars (Johnson, 2010), a hierarchical non-parametric extension of PCFGs (and also interpretable as an extension of LDA-based topic models), hold out some promise here. In that initial work, Johnson’s model learnt collocations of arbitrary length such as gradient descent and cost function, under a topic associated with machine learning. Hardisty et al. (2010) applied this idea to perspective classification, learning collocations such as palestinian violence and palestinian freedom, the use of which as features was demonstrated to help the classification of texts from the Bitter Lemons corpus as either Palestinian o"
D12-1064,D07-1072,0,0.0243615,"Missing"
D12-1064,W07-0602,0,0.340293,"just lead to feature sparsity problems, but also computational efficiency issues. Some form of feature selection should then come into play. Introduction The task of inferring the native language of an author based on texts written in a second language — native language identification (NLI) — has, since the seminal work of Koppel et al. (2005), been primarily tackled as a text classification task using supervised machine learning techniques. Lexical features, such as function words, character n-grams, and part-ofspeech (PoS) n-grams, have been proven to be useful in NLI (Koppel et al., 2005; Tsur and Rappoport, 2007; Estival et al., 2007). The recent work of Wong and Dras (2011), motivated by ideas from Second Language Acquisition (SLA), has shown that syntactic features — potentially capturing syntactic erAdaptor grammars (Johnson, 2010), a hierarchical non-parametric extension of PCFGs (and also interpretable as an extension of LDA-based topic models), hold out some promise here. In that initial work, Johnson’s model learnt collocations of arbitrary length such as gradient descent and cost function, under a topic associated with machine learning. Hardisty et al. (2010) applied this idea to perspective"
D12-1064,C08-1118,0,0.152166,"Missing"
D12-1064,D11-1148,1,0.326815,"ciency issues. Some form of feature selection should then come into play. Introduction The task of inferring the native language of an author based on texts written in a second language — native language identification (NLI) — has, since the seminal work of Koppel et al. (2005), been primarily tackled as a text classification task using supervised machine learning techniques. Lexical features, such as function words, character n-grams, and part-ofspeech (PoS) n-grams, have been proven to be useful in NLI (Koppel et al., 2005; Tsur and Rappoport, 2007; Estival et al., 2007). The recent work of Wong and Dras (2011), motivated by ideas from Second Language Acquisition (SLA), has shown that syntactic features — potentially capturing syntactic erAdaptor grammars (Johnson, 2010), a hierarchical non-parametric extension of PCFGs (and also interpretable as an extension of LDA-based topic models), hold out some promise here. In that initial work, Johnson’s model learnt collocations of arbitrary length such as gradient descent and cost function, under a topic associated with machine learning. Hardisty et al. (2010) applied this idea to perspective classification, learning collocations such as palestinian violen"
D12-1064,U11-1015,1,0.926022,"r, when character n-grams are used as features, some special characters used in some ICLE texts might affect performance. For our case, this should not be of much issue since they will not appear in our collocations. the grammar of Johnson (2010) as presented in Section 2.2.2, except that the vocabulary differs: either w ∈ Vpos or w ∈ Vpos+f w . For Vpos , there are 119 distinct PoS tags based on the Brown tagset. Vpos+f w is extended with 398 function words as per Wong and Dras (2011). m = 490 is the number of documents, and t = 25 the number of topics (chosen as the best performing one from Wong et al. (2011)). Rules of the form Docj → Docj Topici that encode the possible topics that are associated with a document j are given similar α priors as used in LDA (α = 5/t where t = 25 in our experiments). Likewise, similar β priors from LDA are placed on the adapted rules expanding from Topici → Words, representing the possible sequences of words that each topic comprises (β = 0.01).3 The inference algorithm for the adaptor grammars are based on the Markov Chain Monte Carlo technique made available online by Johnson (2010).4 3.1.3 Classification models with n-gram features Based on the two adaptor gramm"
D14-1091,N13-1012,0,0.0290726,"inal consonant clusters by the stress-only grammar, not weight per se. All together, these results indicate that syllable weight Introduction One of the first skills a child must develop in the course of language acquisition is the ability to segment speech into words. Stress has long been recognized as a useful cue for English word segmentation, following the observation that words in English are predominantly stress-initial (Cutler and Carter, 1987), together with the result that 9month-old English-learning infants prefer stressinitial stimuli (Jusczyk et al., 1993). A range of statistical (Doyle and Levy, 2013; Christiansen et al., 1998; B¨orschinger and Johnson, 2014) and rule-based (Yang, 2004; Lignos and Yang, 2010) models have used stress information to improve word segmentation. However, that work uses stress-marked input prepared by marking vowels that are listed as stressed in a pronouncing dictionary. This pre-processing step glosses over the 844 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 844–853, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Among explicitly probabilistic models, Doyle and Levy"
D14-1091,N09-1036,1,0.885086,"the generalization about final and non-final codas. If we say that a monosyllable is an initial onset with a final rhyme, the reverse occurs: we can learn the final/non-final coda generalization at the expense of the initial/non-initial regularities. If we split the symbols further, we’d generalize even less: we’d essentially have to learn 846 Sentence → Collocations3+ (1) Collocations3 → Collocations2+ (2) + Collocations2 → Collocation + Collocation → Word ing input size: 100, 200, 500, 1,000, 2,000, 5,000, and 10,000 utterances. Inference closely followed B¨orschinger and Johnson (2014) and Johnson and Goldwater (2009). We set our hyperparameters to encourage onset maximization. The hyperparameter for syllable nodes to rewrite to an onset followed by a rhyme was 10, and the hyperparameter for syllable nodes to rewrite to a rhyme only was 1. Similarly, the hyperparameter for rhyme nodes to include a coda was 1, and the hyperparameter for rhyme nodes to exclude the coda was 10. All other hyperparameters specified vague priors. We ran eight chains of each model for 1,000 iterations, collecting 20 samples with a lag of 10 iterations between samples and a burn-in of 800 iterations. We used the same batchinitiali"
D14-1091,W10-2912,0,0.016962,"hat syllable weight Introduction One of the first skills a child must develop in the course of language acquisition is the ability to segment speech into words. Stress has long been recognized as a useful cue for English word segmentation, following the observation that words in English are predominantly stress-initial (Cutler and Carter, 1987), together with the result that 9month-old English-learning infants prefer stressinitial stimuli (Jusczyk et al., 1993). A range of statistical (Doyle and Levy, 2013; Christiansen et al., 1998; B¨orschinger and Johnson, 2014) and rule-based (Yang, 2004; Lignos and Yang, 2010) models have used stress information to improve word segmentation. However, that work uses stress-marked input prepared by marking vowels that are listed as stressed in a pronouncing dictionary. This pre-processing step glosses over the 844 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 844–853, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Among explicitly probabilistic models, Doyle and Levy (2013) incorporated stress into Goldwater et al.’s (2009) Bigram model. They did this by modifying the base dis"
D14-1091,W11-0304,0,0.0273493,"Missing"
D14-1091,Q14-1008,1,\N,Missing
D15-1162,J13-1002,0,0.0810244,"on is invalid if S(b) = 1, for a word b at the front of the buffer. This bit will be set if the word was previously Shifted, and then Unshifted. At worst, each word can be pushed and popped from the stack twice, so parsing is guaranteed to terminate after a maximum of 4n transitions for a sentence of length n. The terminal condition is reached when the buffer is exhausted and exactly one word remains on the stack. This word will be deemed the root of the sentence. No ‘dummy’ root token is necessary, removing the need to choose whether the token is placed at the beginning or end of the buffer (Ballesteros and Nivre, 2013). Note that if the two words each seem like the governor of the sentence, such that the parser deems all incoming arcs to these words unlikely, the transition system is guaranteed to arrive at a configuration where these two words are adjacent to each other. The model can then predict an arc between them, initiated by either word. 2.2 Dynamic Training Oracle Goldberg and Nivre (2013) describe three questions that need to be answered in order to implement their training algorithm. Exploration Policy: When do we follow an incorrect transition, and which one do we follow? We always follow the pre"
D15-1162,P15-1033,0,0.0136102,"Missing"
D15-1162,C12-1059,0,0.0203406,"multiple words that are without incoming arcs (i.e. without governors). The original arc-eager configuration outputs partial parses in this situation. Nivre and Fernandez-Gonzalez restrict their Unshift action, such that it can only be applied when the buffer is exhausted and the word on top of the stack has no incoming arc. In this configuration, the Unshift action is the only action that can be applied. The use of the new action is therefore entirely deterministic, and they do not need to produce example configurations for the Unshift action during training. They train their model with what Goldberg and Nivre (2012) term a ‘static oracle’, which can only label configurations that are consistent with the gold-standard parse. We take the Nivre and Fernandez-Gonzalez (2014) Unshift operation, and import it into the non-monotonic parsing model of Honnibal et al. (2013), which uses a dynamic oracle to determine the gold-standard actions for configurations produced by the parser. This training strategy is critical to the success of a non-monotonic transition system. The model cannot learn to recover from previous errors if the training data cannot contain configurations that result from incorrect actions. Honn"
D15-1162,J14-2002,0,0.176736,"eam search. Because the neural network approaches improve the local model that predicts which transition to apply next, while this paper suggests changes to the transition system itself, it is reasonable to expect that the improvements reported here are largely orthogonal to those obtained using the neural network techniques. In future work we would like to explore integrating such neural network models of transition prediction with the extended transition system proposed here. 2 Improved non-monotonic transition system Our transition-system is based on the treeconstrained arc-eager system of Nivre and Fernandez-Gonzalez (2014), which extends the classic arc-eager system (Nivre, 2003) with a new non-monotonic operation that they call “Unshift”. They introduce the Unshift action to repair configurations where the buffer is exhausted and the stack contains multiple words that are without incoming arcs (i.e. without governors). The original arc-eager configuration outputs partial parses in this situation. Nivre and Fernandez-Gonzalez restrict their Unshift action, such that it can only be applied when the buffer is exhausted and the word on top of the stack has no incoming arc. In this configuration, the Unshift action"
D15-1162,P15-1038,0,0.0296008,"Missing"
D15-1162,P15-1032,0,0.0260175,"Missing"
D15-1162,P11-2033,0,0.0523041,"Missing"
D15-1162,Q13-1033,0,0.0643367,"e word remains on the stack. This word will be deemed the root of the sentence. No ‘dummy’ root token is necessary, removing the need to choose whether the token is placed at the beginning or end of the buffer (Ballesteros and Nivre, 2013). Note that if the two words each seem like the governor of the sentence, such that the parser deems all incoming arcs to these words unlikely, the transition system is guaranteed to arrive at a configuration where these two words are adjacent to each other. The model can then predict an arc between them, initiated by either word. 2.2 Dynamic Training Oracle Goldberg and Nivre (2013) describe three questions that need to be answered in order to implement their training algorithm. Exploration Policy: When do we follow an incorrect transition, and which one do we follow? We always follow the predicted transition, i.e. their two hyper-parameters are set k = 1 and p = 1.0. Optimality: What constitutes an optimal transition in configurations from which the gold tree is not reachable? We follow Honnibal et al. (2013) in defining a transition as optimal if it: 1. Renders no additional arcs unreachable using the monotonic arc-eager transitions; and 2. Renders no additional arcs u"
D15-1162,W13-3518,1,0.341921,"rsing transitions to a greedy arc-eager dependency parser in this paper, in order to permit the parser to recover from attachment errors made early in the parsing process. These additional non-monotonic transitions permit the parser to modify what would have been irrevocable parsing decisions in the monotonic arc-eager system when later information justifies this action. Thus one effect of adding the non-monotonic parsing transitions is to effectively delay the location in the input where the parser must ultimately commit to a particular attachment. Our transition-system builds on the work of Honnibal et al. (2013) and Nivre and FernandezGonzalez (2014), who each present modifications 1373 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1373–1378, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. to the arc-eager transition system that introduce some non-monotonic behaviour, resulting in small improvements in accuracy. However, these systems only apply non-monotonic transitions to a relatively small number of configurations, so they can only have a small impact on parse accuracy. We introduce a non-monotonic transition"
D15-1162,P10-1001,0,0.0602387,"Missing"
D15-1162,W03-3017,0,0.0258368,"edicts which transition to apply next, while this paper suggests changes to the transition system itself, it is reasonable to expect that the improvements reported here are largely orthogonal to those obtained using the neural network techniques. In future work we would like to explore integrating such neural network models of transition prediction with the extended transition system proposed here. 2 Improved non-monotonic transition system Our transition-system is based on the treeconstrained arc-eager system of Nivre and Fernandez-Gonzalez (2014), which extends the classic arc-eager system (Nivre, 2003) with a new non-monotonic operation that they call “Unshift”. They introduce the Unshift action to repair configurations where the buffer is exhausted and the stack contains multiple words that are without incoming arcs (i.e. without governors). The original arc-eager configuration outputs partial parses in this situation. Nivre and Fernandez-Gonzalez restrict their Unshift action, such that it can only be applied when the buffer is exhausted and the word on top of the stack has no incoming arc. In this configuration, the Unshift action is the only action that can be applied. The use of the ne"
D15-1162,D14-1082,0,\N,Missing
D16-1004,N10-1083,0,0.377162,"Missing"
D16-1004,Q13-1007,0,0.303546,"by calculating the valence from indices in the rule. For example, after L-P RED, wh does not take any right dependents so θS (stop|wh , →, h = j), where j is the right span index of X[wh ], is multiplied. Improvement Though we omit the details, we can improve the time complexity of the above grammar from O(n6 ) to O(n4 ) applying the technique similar to Eisner and Satta (1999) without changing the binarization mechanism mentioned above. We implemented this improved grammar. 5 Experimental setup A sound evaluation metric in grammar induction is known as an open problem (Schwartz et al., 2011; Bisk and Hockenmaier, 2013), which essentially arises from the ambiguity in the notion of head. For example, Universal dependencies (UD) is the recent standard in annotation and prefers content words to be heads, but as shown below this is very different from the conventional style, e.g., the one in CoNLL shared tasks (Johansson and Nugues, 2007): nsbj UD Ivan C O NLL is the cop det best sbj prd nmod amod nmod The problem is that both trees are correct under some linguistic theories but the standard metric, unlabeled attachment score (UAS), only takes into account the annotation of the current gold data. Our goal in thi"
D16-1004,P15-2143,0,0.0194069,"describe below. For D EP, we use δ = 1.ξ to denote the relaxed maximum depth allowing span length up to ξ (Eq. 4). L EN is the previously explored structural bias (Smith and Eisner, 2006), which penalizes longer dependencies by modifying each attachment score: Depth bound δ Length bias γ 50 20 0 0 1 1.2 1.3 1.4 2 0.1 0.2 0.3 0.4 0.5 Parameters (upper=δ; bottom=γ) Figure 8: UAS for various settings on (UD) WSJ. Hyperparameters Selecting hyperparameters in multilingual grammar induction is difficult; some works tune values for each language based on the development set (Smith and Eisner, 2006; Bisk et al., 2015), but this violates the assumption of unsupervised learning. We instead follow many works ˇ (Mareˇcek and Zabokrtsk´ y, 2012; Naseem et al., 2010) and select the values with the English data. For this, we use the WSJ data, which we obtain in UD style from the Stanford CoreNLP (ver. 3.6.0).9 6 Experiments WSJ Figure 8 shows the result on WSJ. Both D EP and L EN have one parameter: the maximum depth δ, and γ (Eq. 5), and the figure shows the sensitivity on them. Note that x-axis = 0 represents F UNC. For L EN, we can see the optimal parameter γ is 0.1, and degrades the performance when increasin"
D16-1004,P99-1059,0,0.269584,"ned based on the structure of the binarized CFG parse. Parameterization We can encode DMV parameters into each rule. A new arc is introduced by one of {L/R}-{P RED /C OMP}, and the stop probabilities can be assigned appropriately in each rule by calculating the valence from indices in the rule. For example, after L-P RED, wh does not take any right dependents so θS (stop|wh , →, h = j), where j is the right span index of X[wh ], is multiplied. Improvement Though we omit the details, we can improve the time complexity of the above grammar from O(n6 ) to O(n4 ) applying the technique similar to Eisner and Satta (1999) without changing the binarization mechanism mentioned above. We implemented this improved grammar. 5 Experimental setup A sound evaluation metric in grammar induction is known as an open problem (Schwartz et al., 2011; Bisk and Hockenmaier, 2013), which essentially arises from the ambiguity in the notion of head. For example, Universal dependencies (UD) is the recent standard in annotation and prefers content words to be heads, but as shown below this is very different from the conventional style, e.g., the one in CoNLL shared tasks (Johansson and Nugues, 2007): nsbj UD Ivan C O NLL is the co"
D16-1004,N12-1069,0,0.676724,"e the models try to recover the syntactic structure of language without access to the syntactically annotated data, e.g., from raw or partof-speech tagged text only. In these settings, finding better syntactic regularities universal across languages is essential, as they work as a small cue to the correct linguistic structures. A preference exploited in many previous works is favoring shorter dependencies, which has been encoded in various ways, e.g., initialization of EM (Klein and Manning, 2004), or model parameters (Smith and Eisner, 2006), and this has been the key to success of learning (Gimpel and Smith, 2012). Center-embedding is difficult to process and is known as a rare syntactic construction across languages. In this paper we describe a method to incorporate this assumption into the grammar induction tasks by restricting the search space of a model to trees with limited centerembedding. The key idea is the tabulation of left-corner parsing, which captures the degree of center-embedding of a parse via its stack depth. We apply the technique to learning of famous generative model, the dependency model with valence (Klein and Manning, 2004). Cross-linguistic experiments on Universal Dependencies"
D16-1004,N09-1012,1,0.930549,"Missing"
D16-1004,W07-2416,0,0.0222205,") applying the technique similar to Eisner and Satta (1999) without changing the binarization mechanism mentioned above. We implemented this improved grammar. 5 Experimental setup A sound evaluation metric in grammar induction is known as an open problem (Schwartz et al., 2011; Bisk and Hockenmaier, 2013), which essentially arises from the ambiguity in the notion of head. For example, Universal dependencies (UD) is the recent standard in annotation and prefers content words to be heads, but as shown below this is very different from the conventional style, e.g., the one in CoNLL shared tasks (Johansson and Nugues, 2007): nsbj UD Ivan C O NLL is the cop det best sbj prd nmod amod nmod The problem is that both trees are correct under some linguistic theories but the standard metric, unlabeled attachment score (UAS), only takes into account the annotation of the current gold data. Our goal in this experiment is to assess the effect of our structural constraints. To this end, we try to eliminate such arbitrariness in our evaluation as much as possible in the following way: • We experiment on UD, in which every treebank follows the consistent UD style annotation. • We restrict the model to explore only trees that"
D16-1004,P98-1101,1,0.715457,"e largest stack depth d∗ during parsing this tree is: d∗ = λ + 1. Schuler et al. (2010) found that on English treebanks larger stack depth such as 3 or 4 rarely occurs while Noji and Miyao (2014) validated the language universality of this observation through crosslinguistic experiments. These suggest we may utilize LC parsing as a tool for exploiting universal syntactic biases as we discuss in Section 3. Historical notes Rosenkrantz and Lewis (1970) first presented the idea of LC parsing as a grammar transform. This is arc-standard, and has no relevance to center-embedding; Resnik (1992) and Johnson (1998) formulated an arc-eager variant by extending this algorithm. The presented algorithm here is the same as Schuler et al. (2010), and is slightly different from Johnson (1998). The difference is in the start and end conditions: while 2 Schuler et al. (2010) skip this subtlety by only concerning stack depth after P RED or C OMP. We do not take this approach since ours allows a flexible extension described in Section 3. our parser begins with an empty symbol, Johnson’s parser begins with the predicted start symbol, and finishes with an empty symbol. 3 Learning with structural constraints Now we d"
D16-1004,P07-1022,1,0.729021,"2) this one-to-many mapping prevents the insideoutside algorithm to work correctly (Eisner, 2000). As a concrete example, Figures 5(a) and 5(c) show two CFG parses corresponding to the dependency tree dogsx rany fast. We approach this problem by first providing a grammar transform, which generates all valid LC transformed parses (e.g., Figures 5(b) and 5(d)) and then restricting the grammar 3 Another approach might be just applying the technique in Section 3 to some PCFG that encodes DMV, e.g., Headden III et al. (2009). The problem with this approach, in particular with split-head grammars (Johnson, 2007), is that the calculated stack depth no longer reflects the degree of center-embedding in the original parse correctly. As we discuss later, instead, we can speed up inference by applying head-splitting after obtaining the LC transformed grammar. 4 Technical details including the chart algorithm for splithead grammars can be found in the Ph.D. thesis of the first author (Noji, 2016). 37 Naive method Let us begin with the grammar below, which suffers from the spurious ambiguity: S HIFT: X[wh ]d → wh S CAN : X[wh ]d → X[wh /wp ]d wp L-P RED : X[wp /wp ]d → X[wh ]d (whx wp ); R-P RED : X[wh /wp ]"
D16-1004,P04-1061,0,0.839343,"du.au Abstract 2012; Bisk and Hockenmaier, 2013) or weaklysupervised (Garrette et al., 2015) grammar induction tasks, where the models try to recover the syntactic structure of language without access to the syntactically annotated data, e.g., from raw or partof-speech tagged text only. In these settings, finding better syntactic regularities universal across languages is essential, as they work as a small cue to the correct linguistic structures. A preference exploited in many previous works is favoring shorter dependencies, which has been encoded in various ways, e.g., initialization of EM (Klein and Manning, 2004), or model parameters (Smith and Eisner, 2006), and this has been the key to success of learning (Gimpel and Smith, 2012). Center-embedding is difficult to process and is known as a rare syntactic construction across languages. In this paper we describe a method to incorporate this assumption into the grammar induction tasks by restricting the search space of a model to trees with limited centerembedding. The key idea is the tabulation of left-corner parsing, which captures the degree of center-embedding of a parse via its stack depth. We apply the technique to learning of famous generative mo"
D16-1004,D12-1028,0,0.157762,"Missing"
D16-1004,D10-1120,1,0.881222,"t, comparing the effect of several biases including the one against longer dependencies. Our main empirical finding is that though two biases, avoiding center-embedding and favoring shorter dependencies, are conceptually similar (both favor simpler grammars), often they capture different aspects of syntax, leading to different grammars. In particular our bias cooperates well with additional small syntactic cue such as the one that the sentence root tends to be a verb or a noun, with which our models compete with the strong baseline relying on a larger number of hand crafted rules on POS tags (Naseem et al., 2010). Our contributions are: the idea to utilize leftcorner parsing for a tool to constrain the models of syntax (Section 3), the formulation of this idea for DMV (Section 4), and cross-linguistic experiments across 25 languages to evaluate the universality of the proposed approach (Sections 5 and 6). 2 Left-corner parsing We first describe (arc-eager) left-corner (LC) parsing as a push-down automaton (PDA), and then reformulate it as a grammar transform. In previous work this algorithm has been called right-corner parsing (e.g., Schuler et al. (2010)); we avoid this term and instead treat it as a"
D16-1004,C14-1202,1,0.895681,"areˇcek and Zabokrtsk´ y, In this paper, we explore the utility for another universal syntactic bias that has not yet been exploited in grammar induction: a bias against centerembedding. Center-embedding is a syntactic construction on which a clause is embedded into another one. An example is “The reporter [who the senator [who Mary met] attacked] ignored the president.”, where “who Mary met” is embedded in a larger relative clause. These constructions are known to cause memory overflow (Miller and Chomsky, 1963; Gibson, 2000), and also are rarely observed crosslinguistically (Karlsson, 2007; Noji and Miyao, 2014). Our learning method exploits this universal property of language. Intuitively during learning our models explore the restricted search space, which excludes linguistically implausible trees, i.e., those with deeper levels of center-embedding. We describe how these constraints can be imposed in EM with the inside-outside algorithm. The central 33 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 33–43, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics S HIFT S CAN P RED C OMP a σ d−1 7− → σ d−1 |Ad a d−1 σ |B/Ad 7−"
D16-1004,C92-1032,0,0.36042,"Missing"
D16-1004,P80-1024,0,0.811412,"fication is necessary since C OMP for a single token occurs for building purely right-branching structures.2 Formally, then, given a tree with degree λ of center-embedding the largest stack depth d∗ during parsing this tree is: d∗ = λ + 1. Schuler et al. (2010) found that on English treebanks larger stack depth such as 3 or 4 rarely occurs while Noji and Miyao (2014) validated the language universality of this observation through crosslinguistic experiments. These suggest we may utilize LC parsing as a tool for exploiting universal syntactic biases as we discuss in Section 3. Historical notes Rosenkrantz and Lewis (1970) first presented the idea of LC parsing as a grammar transform. This is arc-standard, and has no relevance to center-embedding; Resnik (1992) and Johnson (1998) formulated an arc-eager variant by extending this algorithm. The presented algorithm here is the same as Schuler et al. (2010), and is slightly different from Johnson (1998). The difference is in the start and end conditions: while 2 Schuler et al. (2010) skip this subtlety by only concerning stack depth after P RED or C OMP. We do not take this approach since ours allows a flexible extension described in Section 3. our parser begins w"
D16-1004,J10-1001,0,0.519299,"arger number of hand crafted rules on POS tags (Naseem et al., 2010). Our contributions are: the idea to utilize leftcorner parsing for a tool to constrain the models of syntax (Section 3), the formulation of this idea for DMV (Section 4), and cross-linguistic experiments across 25 languages to evaluate the universality of the proposed approach (Sections 5 and 6). 2 Left-corner parsing We first describe (arc-eager) left-corner (LC) parsing as a push-down automaton (PDA), and then reformulate it as a grammar transform. In previous work this algorithm has been called right-corner parsing (e.g., Schuler et al. (2010)); we avoid this term and instead treat it as a variant of LC parsing following more recent studies, e.g., van Schijndel 34 D D i j A B j+1 C OMP k ===⇒ i B j C A j+1 k Figure 2: C OMP combines two subtrees on the top of the stack. i, j, k are indices of spans. and Schuler (2013). The central motivation for this technique is to detect center-embedding in a parse efficiently. We describe this mechanism after providing the algorithm itself. We then give historical notes on LC parsing at the end of this section. PDA Let us assume a CFG is given, and it is in CNF. We formulate LC parsing as a set"
D16-1004,P11-1067,0,0.0146579,"opriately in each rule by calculating the valence from indices in the rule. For example, after L-P RED, wh does not take any right dependents so θS (stop|wh , →, h = j), where j is the right span index of X[wh ], is multiplied. Improvement Though we omit the details, we can improve the time complexity of the above grammar from O(n6 ) to O(n4 ) applying the technique similar to Eisner and Satta (1999) without changing the binarization mechanism mentioned above. We implemented this improved grammar. 5 Experimental setup A sound evaluation metric in grammar induction is known as an open problem (Schwartz et al., 2011; Bisk and Hockenmaier, 2013), which essentially arises from the ambiguity in the notion of head. For example, Universal dependencies (UD) is the recent standard in annotation and prefers content words to be heads, but as shown below this is very different from the conventional style, e.g., the one in CoNLL shared tasks (Johansson and Nugues, 2007): nsbj UD Ivan C O NLL is the cop det best sbj prd nmod amod nmod The problem is that both trees are correct under some linguistic theories but the standard metric, unlabeled attachment score (UAS), only takes into account the annotation of the curre"
D16-1004,silveira-etal-2014-gold,0,0.0503099,"Missing"
D16-1004,P06-1072,0,0.829822,") or weaklysupervised (Garrette et al., 2015) grammar induction tasks, where the models try to recover the syntactic structure of language without access to the syntactically annotated data, e.g., from raw or partof-speech tagged text only. In these settings, finding better syntactic regularities universal across languages is essential, as they work as a small cue to the correct linguistic structures. A preference exploited in many previous works is favoring shorter dependencies, which has been encoded in various ways, e.g., initialization of EM (Klein and Manning, 2004), or model parameters (Smith and Eisner, 2006), and this has been the key to success of learning (Gimpel and Smith, 2012). Center-embedding is difficult to process and is known as a rare syntactic construction across languages. In this paper we describe a method to incorporate this assumption into the grammar induction tasks by restricting the search space of a model to trees with limited centerembedding. The key idea is the tabulation of left-corner parsing, which captures the degree of center-embedding of a parse via its stack depth. We apply the technique to learning of famous generative model, the dependency model with valence (Klein"
D16-1004,N13-1010,0,0.0355593,"Missing"
D16-1004,C98-1098,1,\N,Missing
D17-1098,J14-3008,0,0.050305,"Missing"
D17-1098,W14-3348,0,0.00833119,"Missing"
D17-1098,P15-2017,0,0.449201,"sier to scale to new concepts. In this paper our goal is to incorporate text fragments such as these during caption generation, to improve the quality of resulting captions. This goal poses two key challenges. First, RNNs are generally opaque, and difficult to influence at test time. Second, text fragments may include words Introduction Automatic image captioning is a fundamental task that couples visual and linguistic learning. Recently, models incorporating recurrent neural networks (RNNs) have demonstrated promising results on this challenging task (Vinyals et al., 2015; Fang et al., 2015; Devlin et al., 2015), leveraging new benchmark datasets such as the MSCOCO dataset (Lin et al., 2014). However, these datasets are generally only concerned with a relatively small number of objects and interactions. Unsur936 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 936–945 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics that are not present in the RNN vocabulary. As illustrated in Figure 1, we address the first challenge (guidance) by using constrained beam search to guarantee the inclusion of selected words or phrase"
D17-1098,P15-1005,0,0.0352831,"Missing"
D17-1098,D16-1126,0,0.00677554,"er text fragments into the learning algorithm. Instead, we incorporate text fragments during model decoding. To the best of our knowledge we are the first to consider this more loosely-coupled approach to out-of-domain image captioning, which allows the model to take advantage of information not available at training time, and avoids the need to retrain the captioning model if the source of text fragments is changed. More broadly, the problem of generating high probability output sequences using finitestate machinery has been previously explored in the context of poetry generation using RNNs (Ghazvininejad et al., 2016) and machine translation using n-gram language models (Allauzen et al., 2014). Related Work While various approaches to image caption generation have been considered, a large body of recent work is dedicated to neural network approaches (Donahue et al., 2015; Mao et al., 2015; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Devlin et al., 2015). These approaches typically use a pretrained Convolutional Neural Network (CNN) image encoder, combined with a Recurrent Neural Network (RNN) decoder trained to predict the next output word, conditioned on previous words and the image. In each case th"
D17-1098,Q14-1006,0,0.00752773,"lections contain useful annotations, and that we should seek to use this information, in this section we caption a 5,000 image subset of the ImageNet (Russakovsky et al., 2015) ILSVRC 2012 classification dataset for assessment. The dataset contains 1.2M images classified into 1,000 object categories, from which we randomly select five images from each category. For this task we use the ResNet-50 (He et al., 2016) CNN, and train the base model on a combined training set containing 155k images comprised of the MSCOCO (Chen et al., 2015) training and validation datasets, and the full Flickr 30k (Young et al., 2014) captions dataset. We use constrained beam search and vocabulary expansion to ensure that each generated caption includes a phrase from the WordNet (Fellbaum, 1998) synset representing the ground-truth image category. For synsets that contain multiple entries, we run constrained beam search separately for each phrase and select the predicted caption with the highest log probability overall. Note that even with the use of ground-truth object labels, the ImageNet captioning task remains extremely challenging as ImageNet contains a wide variety of classes, many of which are not evenly remotely re"
D17-1098,J10-4005,0,0.0362086,"utional Neural Network (CNN) image encoder, combined with a Recurrent Neural Network (RNN) decoder trained to predict the next output word, conditioned on previous words and the image. In each case the decoding process remains the same—captions are generated by searching over output sequences greedily 1 3 Approach In this section we describe the constrained beam search algorithm, the base captioning model used in experiments, and our approach to expanding the model vocabulary with pretrained word embeddings. www.panderson.me/constrained-beam-search 937 3.1 Constrained Beam Search Beam search (Koehn, 2010) is an approximate search algorithm that is widely used to decode output sequences from Recurrent Neural Networks (RNNs). We briefly describe the RNN decoding problem, before introducing constrained beam search, a multiple-beam search algorithm that enforces constraints in the sequence generation process. Let y t = (y1 , ..., yt ) denote an output sequence of length t containing words or other tokens from vocabulary V . Given an RNN modeling a probability distribution over such sequences, the RNN decoding problem is to find the output sequence with the maximum log-probability, where the log pr"
D17-1098,D14-1162,0,0.115871,"Missing"
D18-1490,N01-1016,1,0.803339,"yt = A · Xi:j + B · X (4) where yt , A, X, b, i and j are as in the convolution operator, and ˆ is a tensor of size (n, n, m) such that each vecX ˆ i,j,: is given by f (xi , xj ), tor X 4.1 Experiments Switchboard Dataset [ reparandum + {interregnum} repair ] where (+) is the interruption point marking the end of reparandum and {} indicate optional interregnum. We collapse this annotation to a binary classification scheme in which reparanda are labeled as disfluent and all other words as fluent. We disregard interregnum words as they are trivial to detect as discussed in Section 1. Following Charniak and Johnson (2001), we split the Switchboard corpus into training, dev and test set as follows: training data consists of all sw[23]∗.dff files, dev training consists of all sw4[5, 6, 7, 8, 9]∗.dff files and test data consists of all sw4[0, 1]∗.dff files. We lower-case all text and remove all partial words and punctuations from the training data to make our evaluation both harder and more realistic (Johnson and Charniak, 2004). Partial words are strong indicators of disfluency; however, speech recognition models never generate them in their outputs. 4613 Figure 2: ACNN overview for labeling the target word “bos"
D18-1490,N15-1029,0,0.619161,"the sentence and their dependencies from the stack (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016). Joint parsing and disfluency detection can compare favorably to pipelined approaches, but requires large annotated treebanks containing both disfluent and syntatic structures for training. Our proposed approach, based on an autocorrelational neural network (ACNN), belongs to the class of sequence tagging approaches. These approaches use classification techniques such as conditional random fields (Liu et al., 2006; Ostendorf and Hahn, 2013; Zayats et al., 2014; Ferguson et al., 2015), hidden Markov models (Liu et al., 2006; Schuler et al., 2010) and deep learning based models (Hough and Schlangen, 2015; Zayats et al., 2016) to label individual words as fluent or disfluent. In much of the previous work on sequence tagging approaches, improved performance has been gained by proposing increasingly complicated labeling schemes. In this case, a model with begin-inside-outside (BIO) style states which labels words as being inside or outside of edit region1 is usually used as the baseline sequence tagging model. Then in order to come up with different pattern matching lexical cu"
D18-1490,N09-2028,0,0.227608,"Missing"
D18-1490,W10-4343,0,0.0447914,"Missing"
D18-1490,Q14-1011,1,0.852061,"speech and non-speech corpora, and using the LM scores along with other features (i.e. pattern match and NCM ones) into a MaxEnt reranker (Johnson et al., 2004) improves the performance of the baseline NCM, although this creates complex runtime dependencies. Parsing-based approaches detect disfluencies while simultaneously identifying the syntactic structure of the sentence. Typically, this is achieved by augmenting a transition-based dependency parser with a new action to detect and remove the disfluent parts of the sentence and their dependencies from the stack (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016). Joint parsing and disfluency detection can compare favorably to pipelined approaches, but requires large annotated treebanks containing both disfluent and syntatic structures for training. Our proposed approach, based on an autocorrelational neural network (ACNN), belongs to the class of sequence tagging approaches. These approaches use classification techniques such as conditional random fields (Liu et al., 2006; Ostendorf and Hahn, 2013; Zayats et al., 2014; Ferguson et al., 2015), hidden Markov models (Liu et al., 2006; Schuler et al., 2010) and deep learning base"
D18-1490,P17-2087,1,0.673541,"d the repair to allocate higher probabilities to exact copy reparandum words. Using the probabilities of TAG channel model and a bigram language model (LM) derived from training data, the NCM generates n-best disfluency analyses for each sentence at test time. The analyses are then reranked using a language model which is sensitive to the global properties of the sentence, such as a syntactic parser based LM (Johnson and Charniak, 2004; Johnson et al., 2004). Some works have shown that rescoring the n-best analyses with external n-gram (Zwarts and Johnson, 2011) and deep learning LMs (Jamshid Lou and Johnson, 2017) trained on large speech and non-speech corpora, and using the LM scores along with other features (i.e. pattern match and NCM ones) into a MaxEnt reranker (Johnson et al., 2004) improves the performance of the baseline NCM, although this creates complex runtime dependencies. Parsing-based approaches detect disfluencies while simultaneously identifying the syntactic structure of the sentence. Typically, this is achieved by augmenting a transition-based dependency parser with a new action to detect and remove the disfluent parts of the sentence and their dependencies from the stack (Rasooli and"
D18-1490,H05-1030,1,0.784812,"Missing"
D18-1490,P16-2067,0,0.0570247,"Missing"
D18-1490,N13-1102,0,0.387479,"Missing"
D18-1490,D13-1013,0,0.762457,"nson, 2017) trained on large speech and non-speech corpora, and using the LM scores along with other features (i.e. pattern match and NCM ones) into a MaxEnt reranker (Johnson et al., 2004) improves the performance of the baseline NCM, although this creates complex runtime dependencies. Parsing-based approaches detect disfluencies while simultaneously identifying the syntactic structure of the sentence. Typically, this is achieved by augmenting a transition-based dependency parser with a new action to detect and remove the disfluent parts of the sentence and their dependencies from the stack (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016). Joint parsing and disfluency detection can compare favorably to pipelined approaches, but requires large annotated treebanks containing both disfluent and syntatic structures for training. Our proposed approach, based on an autocorrelational neural network (ACNN), belongs to the class of sequence tagging approaches. These approaches use classification techniques such as conditional random fields (Liu et al., 2006; Ostendorf and Hahn, 2013; Zayats et al., 2014; Ferguson et al., 2015), hidden Markov models (Liu et al., 2006; Schuler et al.,"
D18-1490,N18-1007,0,0.469927,"Missing"
D18-1490,D16-1109,0,0.381155,"a, and using the LM scores along with other features (i.e. pattern match and NCM ones) into a MaxEnt reranker (Johnson et al., 2004) improves the performance of the baseline NCM, although this creates complex runtime dependencies. Parsing-based approaches detect disfluencies while simultaneously identifying the syntactic structure of the sentence. Typically, this is achieved by augmenting a transition-based dependency parser with a new action to detect and remove the disfluent parts of the sentence and their dependencies from the stack (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016). Joint parsing and disfluency detection can compare favorably to pipelined approaches, but requires large annotated treebanks containing both disfluent and syntatic structures for training. Our proposed approach, based on an autocorrelational neural network (ACNN), belongs to the class of sequence tagging approaches. These approaches use classification techniques such as conditional random fields (Liu et al., 2006; Ostendorf and Hahn, 2013; Zayats et al., 2014; Ferguson et al., 2015), hidden Markov models (Liu et al., 2006; Schuler et al., 2010) and deep learning based models (Hough and Schla"
D18-1490,W17-4118,0,0.0548298,"Missing"
D18-1490,P11-1071,1,0.919562,"annel model uses the similarity between the reparandum and the repair to allocate higher probabilities to exact copy reparandum words. Using the probabilities of TAG channel model and a bigram language model (LM) derived from training data, the NCM generates n-best disfluency analyses for each sentence at test time. The analyses are then reranked using a language model which is sensitive to the global properties of the sentence, such as a syntactic parser based LM (Johnson and Charniak, 2004; Johnson et al., 2004). Some works have shown that rescoring the n-best analyses with external n-gram (Zwarts and Johnson, 2011) and deep learning LMs (Jamshid Lou and Johnson, 2017) trained on large speech and non-speech corpora, and using the LM scores along with other features (i.e. pattern match and NCM ones) into a MaxEnt reranker (Johnson et al., 2004) improves the performance of the baseline NCM, although this creates complex runtime dependencies. Parsing-based approaches detect disfluencies while simultaneously identifying the syntactic structure of the sentence. Typically, this is achieved by augmenting a transition-based dependency parser with a new action to detect and remove the disfluent parts of the sente"
D18-1490,J10-1001,0,0.0219997,"Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016). Joint parsing and disfluency detection can compare favorably to pipelined approaches, but requires large annotated treebanks containing both disfluent and syntatic structures for training. Our proposed approach, based on an autocorrelational neural network (ACNN), belongs to the class of sequence tagging approaches. These approaches use classification techniques such as conditional random fields (Liu et al., 2006; Ostendorf and Hahn, 2013; Zayats et al., 2014; Ferguson et al., 2015), hidden Markov models (Liu et al., 2006; Schuler et al., 2010) and deep learning based models (Hough and Schlangen, 2015; Zayats et al., 2016) to label individual words as fluent or disfluent. In much of the previous work on sequence tagging approaches, improved performance has been gained by proposing increasingly complicated labeling schemes. In this case, a model with begin-inside-outside (BIO) style states which labels words as being inside or outside of edit region1 is usually used as the baseline sequence tagging model. Then in order to come up with different pattern matching lexical cues for repetition and correction disfluencies, they extend the"
D18-1490,C90-3045,0,0.0928688,"ection, we show that the ACNN captures important properties of speech repairs including “rough copy” dependencies, and • Using the ACNN, we achieve competitive results for disfluency detection without relying on any hand-crafted features or other representations derived from the output of preexisting systems. 2 Related Work Approaches to disfluency detection task fall into three main categories: noisy channel models, parsing-based approaches and sequence tagging approaches. Noisy channel models (NCMs) (Johnson and Charniak, 2004; Johnson et al., 2004) use complex tree adjoining grammar (TAG) (Shieber and Schabes, 1990) based channel models to find the “rough copy” dependencies between words. The channel model uses the similarity between the reparandum and the repair to allocate higher probabilities to exact copy reparandum words. Using the probabilities of TAG channel model and a bigram language model (LM) derived from training data, the NCM generates n-best disfluency analyses for each sentence at test time. The analyses are then reranked using a language model which is sensitive to the global properties of the sentence, such as a syntactic parser based LM (Johnson and Charniak, 2004; Johnson et al., 2004)"
D18-1490,W90-0102,0,\N,Missing
H05-1030,W02-1007,1,0.908554,"Missing"
H05-1030,graff-bird-2000-many,0,0.311885,"et of opportunities and challenges. While new obstacles arise from the presence of speech repairs, the possibility of word errors, and the absence of punctuation and sentence boundaries, speech also presents a tremendous opportunity to leverage multi-modal input, in the form of acoustic or even visual cues. As a step in this direction, this paper identifies a set of useful prosodic features and describes how they can be effectively incorporated into a statistical parsing model, ignoring for now the problem of word errors. Evaluated on the Switchboard corpus of conversational telephone speech (Graff and Bird, 2000), our prosody-aware parser out-performs a state-of-the-art system that uses lexical and syntactic features only. While we are not the first to employ prosodic cues in a statistical parsing model, previous efforts (Gregory et al., 2004; Kahn et al., 2004) incorporated these features as word tokens and thereby suffered from the side-effect of displacing words in the n-gram models by the parser. To avoid this problem, we generate a set of candidate parses using an off-the-shelf, k-best parser, and use prosodic (and other) features to rescore the candidate parses. Our system architecture combines"
H05-1030,N04-1011,1,0.863186,"leverage multi-modal input, in the form of acoustic or even visual cues. As a step in this direction, this paper identifies a set of useful prosodic features and describes how they can be effectively incorporated into a statistical parsing model, ignoring for now the problem of word errors. Evaluated on the Switchboard corpus of conversational telephone speech (Graff and Bird, 2000), our prosody-aware parser out-performs a state-of-the-art system that uses lexical and syntactic features only. While we are not the first to employ prosodic cues in a statistical parsing model, previous efforts (Gregory et al., 2004; Kahn et al., 2004) incorporated these features as word tokens and thereby suffered from the side-effect of displacing words in the n-gram models by the parser. To avoid this problem, we generate a set of candidate parses using an off-the-shelf, k-best parser, and use prosodic (and other) features to rescore the candidate parses. Our system architecture combines earlier models proposed for parse reranking (Collins, 2000) and filtering out edit regions (Charniak and Johnson, 2001). Detecting and removing edits prior to parsing is motivated by the claim that probabilistic contextfree grammars ("
H05-1030,P83-1019,0,0.0846047,"Missing"
H05-1030,N04-4032,1,0.919297,"input, in the form of acoustic or even visual cues. As a step in this direction, this paper identifies a set of useful prosodic features and describes how they can be effectively incorporated into a statistical parsing model, ignoring for now the problem of word errors. Evaluated on the Switchboard corpus of conversational telephone speech (Graff and Bird, 2000), our prosody-aware parser out-performs a state-of-the-art system that uses lexical and syntactic features only. While we are not the first to employ prosodic cues in a statistical parsing model, previous efforts (Gregory et al., 2004; Kahn et al., 2004) incorporated these features as word tokens and thereby suffered from the side-effect of displacing words in the n-gram models by the parser. To avoid this problem, we generate a set of candidate parses using an off-the-shelf, k-best parser, and use prosodic (and other) features to rescore the candidate parses. Our system architecture combines earlier models proposed for parse reranking (Collins, 2000) and filtering out edit regions (Charniak and Johnson, 2001). Detecting and removing edits prior to parsing is motivated by the claim that probabilistic contextfree grammars (PCFGs) perform poorl"
H05-1030,P90-1003,0,0.0944145,"is also perceptual evidence that prosody provides cues to human listeners that aid in syntactic disambiguation (Price et al., 1991), and the most important of these cues seems to be the prosodic phrases (perceived groupings of words) or the boundary events marking them. However, the utility of sentence-internal prosody in parsing conversational speech is not well established. Most early work on integrating prosody in parsing was in the context of human-computer dialog systems, where parsers typically operated on isolated utterances. The primary use of prosody was to rule out candidate parses (Bear and Price, 1990; Batliner et al., 1996). Since then, parsing has advanced considerably, and the use of statistical parsers makes the candidate pruning benefits of prosody less important. This raises the question of whether prosody is useful for improving parsing accuracy for conversational speech, apart from its use in sentence a new part-of-speech tag EW. Consecutive sequences of edit words are inserted as single, flat EDITED constituents. 4. Features (syntactic and/or prosodic) are extracted for each candidate, i.e. candidates are converted to feature vector representation. 5. The candidates are rescored b"
H05-1030,N01-1016,1,0.85709,"actic features only. While we are not the first to employ prosodic cues in a statistical parsing model, previous efforts (Gregory et al., 2004; Kahn et al., 2004) incorporated these features as word tokens and thereby suffered from the side-effect of displacing words in the n-gram models by the parser. To avoid this problem, we generate a set of candidate parses using an off-the-shelf, k-best parser, and use prosodic (and other) features to rescore the candidate parses. Our system architecture combines earlier models proposed for parse reranking (Collins, 2000) and filtering out edit regions (Charniak and Johnson, 2001). Detecting and removing edits prior to parsing is motivated by the claim that probabilistic contextfree grammars (PCFGs) perform poorly at detecting edit regions. We validate this claim empirically: two state-of-the-art PCFGs (Bikel, 2004; Charniak and Johnson, 2005) are both shown to perform significantly below a state-of-the-art edit detection system (Johnson et al., 2004). 2 Previous Work As mentioned earlier, conversational speech presents a different set of challenges and opportunities than encountered in parsing text. This paper focuses on the challenges associated with disfluencies (Se"
H05-1030,P05-1022,1,0.900216,"porated into a statistical parsing model. On the Switchboard corpus of conversational speech, the system achieves improved parse accuracy over a state-of-the-art system which uses only lexical and syntactic features. Since removal of edit regions is known to improve downstream parse accuracy, we explore alternatives for edit detection and show that PCFGs are not competitive with more specialized techniques. 1 Introduction For more than a decade, the Penn Treebank’s Wall Street Journal corpus has served as a benchmark for developing and evaluating statistical parsing techniques (Collins, 2000; Charniak and Johnson, 2005). While this common benchmark has served as a valuable shared task for focusing community effort, it has unfortunately led to the relative neglect of other genres, particularly speech. Parsed speech stands to benefit from practically every application envisioned for parsed text, including machine translation, information extraction, and language modeling. In contrast to text, however, speech (in particular, conversational speech) presents a distinct set of opportunities and challenges. While new obstacles arise from the presence of speech repairs, the possibility of word errors, and the absenc"
H05-1030,P99-1053,0,0.322839,"Missing"
H05-1030,H91-1073,1,0.912342,"Missing"
H05-1030,J99-4003,0,\N,Missing
J02-1005,J97-4005,0,\N,Missing
J07-4003,P99-1070,0,0.465814,"Missing"
J07-4003,P98-1035,0,0.0112557,"ion), or both. (Note that the choice of estimation procedure (b) is in principle orthogonal to the choice of model, and conditional estimation should not be conflated with log-linear modeling.) For a given estimation criterion, weighted CFGs, and Mealy MRFs, in particular, cannot be expected to behave any differently than PCFGs and HMMs, respectively, unless they are augmented with more features. 6. Related Work Abney, McAllester, and Pereira (1999) addressed the relationship between PCFGs and probabilistic models based on push-down automaton operations (e.g., the structured language model of Chelba and Jelinek, 1998). They proved that, although the conversion may not be simple (indeed, a blow-up in the automaton’s size may be incurred), given G, PPCFG and the set of distributions expressible by shift-reduce probabilistic push-down automata are weakly equivalent. Importantly, the standard conversion of a CFG into a shift-reduce PDA, when applied in the stochastic case, does not always preserve the probability distribution over trees. Our Theorem 2 bears a resemblance to that result. Further work on the relationship between weighted CFGs and weighted PDAs is described in Nederhof and Satta (2004). MacKay (1"
J07-4003,J99-1004,0,0.0973983,"expressive power of weighted context-free grammars (WCFGs), where each rule is associated with a positive weight, to that of the corresponding PCFGs, that is, with the same rules but where the weights of the rules expanding a nonterminal must sum to one. One might expect that because normalization removes one or more degrees of freedom, unnormalized models should be more expressive than normalized, probabilistic models. Perhaps counterintuitively, previous work has shown that the classes of probability distributions defined by WCFGs and PCFGs are the same (Abney, McAllester, and Pereira 1999; Chi 1999). However, this result does not completely settle the question about the expressive power of WCFGs and PCFGs. As we show herein, a WCFG can define a conditional distribution from strings to trees even if it does not define a probability distribution over trees. Because these conditional distributions are what are used in classification tasks and related tasks such as parsing, we need to know the relationship between the classes of conditional distributions defined by WCFGs and PCFGs. In fact we extend the results of Chi and of Abney et al., and show that WCFGs and PCFGs both define the same cl"
J07-4003,P01-1042,1,0.859052,"te that even if we allow the 8 The HMM shown in Example 3 can be factored into a Moore HMM without any change to the distribution. 488 Smith and Johnson Weighted and Probabilistic CFGs MEMM to “look back” and condition on earlier symbols (or states), it cannot represent the distribution in Example 3.  Generally speaking, this limitation of MEMMs has nothing to do with the estimation procedure (we have committed to no estimation procedure in particular) but rather with the conditional structure of the model. That some model structures work better than others at real NLP tasks was discussed by Johnson (2001) and Klein and Manning (2002). Our result—that the class of distributions allowed by MEMMs is a strict subset of those allowed by Mealy HMMs—makes this unsurprising. 5. Practical Implications Our result is that weighted generalizations of classical probabilistic grammars (PCFGs and HMMs) are no more powerful than the probabilistic models. This means that, insofar as log-linear models for NLP tasks like tagging and parsing are more successful than their probabilistic cousins, it is due to either (a) additional features added to the model, (b) improved estimation procedures (e.g., maximum condit"
J07-4003,P99-1069,1,0.367445,"ic Sciences, Brown University, Providence, RI 02912, USA. E-mail: Mark Johnson@brown.edu. Submission received: 30 November 2005; revised submission received: 11 January 2007; accepted for publication: 30 March 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 treebank, maximum likelihood estimation can be applied to learn the probability values in the model. More recently, new machine learning methods have been developed or extended to handle models of grammatical structure. Notably, conditional estimation (Ratnaparkhi, Roukos, and Ward 1994; Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001), maximum margin estimation (Taskar et al. 2004), and unsupervised contrastive estimation (Smith and Eisner 2005) have been applied to structured models. Weighted grammars learned in this way differ in two important ways from traditional, generative models. First, the weights can be any positive value; they need not sum to one. Second, features can “overlap,” and it can be difficult to design a generative model that uses such features. The benefits of new features and discriminative training methods are widely documented and recognized. This article focus"
J07-4003,W02-1002,0,0.00817733,"allow the 8 The HMM shown in Example 3 can be factored into a Moore HMM without any change to the distribution. 488 Smith and Johnson Weighted and Probabilistic CFGs MEMM to “look back” and condition on earlier symbols (or states), it cannot represent the distribution in Example 3.  Generally speaking, this limitation of MEMMs has nothing to do with the estimation procedure (we have committed to no estimation procedure in particular) but rather with the conditional structure of the model. That some model structures work better than others at real NLP tasks was discussed by Johnson (2001) and Klein and Manning (2002). Our result—that the class of distributions allowed by MEMMs is a strict subset of those allowed by Mealy HMMs—makes this unsurprising. 5. Practical Implications Our result is that weighted generalizations of classical probabilistic grammars (PCFGs and HMMs) are no more powerful than the probabilistic models. This means that, insofar as log-linear models for NLP tasks like tagging and parsing are more successful than their probabilistic cousins, it is due to either (a) additional features added to the model, (b) improved estimation procedures (e.g., maximum conditional likelihood estimation o"
J07-4003,P04-1069,0,0.0116814,"e model of Chelba and Jelinek, 1998). They proved that, although the conversion may not be simple (indeed, a blow-up in the automaton’s size may be incurred), given G, PPCFG and the set of distributions expressible by shift-reduce probabilistic push-down automata are weakly equivalent. Importantly, the standard conversion of a CFG into a shift-reduce PDA, when applied in the stochastic case, does not always preserve the probability distribution over trees. Our Theorem 2 bears a resemblance to that result. Further work on the relationship between weighted CFGs and weighted PDAs is described in Nederhof and Satta (2004). MacKay (1996) proved that linear Boltzmann chains (a class of weighted models that is essentially the same as Moore MRFs) express the same set of distributions as Moore HMMs, under the condition that the Boltzmann chain has a single specific end state. MacKay avoided the divergence problem by defining the Boltzmann chain always to condition on the length of the sequence; he tacitly requires all of his models to be in GZn &lt;∞ . We have suggested a more applicable notion of model equivalence (equivalence of the conditional distribution) and our Theorem 1 generalizes to context-free models. 7. C"
J07-4003,P05-1044,1,0.807515,"eceived: 11 January 2007; accepted for publication: 30 March 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 treebank, maximum likelihood estimation can be applied to learn the probability values in the model. More recently, new machine learning methods have been developed or extended to handle models of grammatical structure. Notably, conditional estimation (Ratnaparkhi, Roukos, and Ward 1994; Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001), maximum margin estimation (Taskar et al. 2004), and unsupervised contrastive estimation (Smith and Eisner 2005) have been applied to structured models. Weighted grammars learned in this way differ in two important ways from traditional, generative models. First, the weights can be any positive value; they need not sum to one. Second, features can “overlap,” and it can be difficult to design a generative model that uses such features. The benefits of new features and discriminative training methods are widely documented and recognized. This article focuses specifically on the first of these differences. It compares the expressive power of weighted context-free grammars (WCFGs), where each rule is associ"
J07-4003,J95-2002,0,0.094771,"X→α = |α|  Zαi (Θ ) i=1 ZX (Θ ) where αi is the ith element of α and |α |is the length of α. Chi proved that GΘ is a PCFG and that PΘ (τ ) = sΘ (τ )/Z(Θ ) for all trees τ ∈ Ω(G). Chi did not describe how to compute the nonterminal-specific partition functions ZX (Θ ). The ZX (Θ ) are related by equations of the form  ZX (Θ ) = θX→α α:X→α∈R |α|  Zαi (Θ ) i=1 which constitute a set of nonlinear polynomial equations in ZX (Θ ). Although a numerical solver might be employed to find the ZX (Θ ), we have found that in practice iterative propagation of weights following the method described by Stolcke (1995, Section 4.7.1) converges quickly when Z(Θ ) is finite. 3. Classifiers and Conditional Distributions A common application of weighted grammars is parsing. One way to select a parse tree for a sentence x is to choose the maximum weighted parse that is consistent with the observation x: τ∗ (x) = argmax sΘ ( τ ) (3) τ∈Ω(G):y(τ )=x where y(τ ) is the yield of τ. Other decision criteria exist, including minimum-loss decoding and re-ranked n-best decoding. All of these classifiers use some kind of dynamic programming algorithm to optimize over trees, and they also exploit the conditional distributi"
J07-4003,P80-1024,0,0.537079,"Missing"
J07-4003,W04-3201,0,\N,Missing
J07-4003,C98-1035,0,\N,Missing
J91-2001,J90-1002,0,0.111298,"Missing"
J91-2001,P89-1003,0,0.0377018,"of that conjunction (Kasper and Rounds 1990; Johnson 1988, 1990a; Pereira 1987). The most widely known model of feature structures and constraint language is the one developed to explain disjunctive feature values by Kasper and Rounds (1986, 1990) and Kasper (1986, 1987). The Kasper-Rounds treatment resolves the difficulties in interpreting disjunctive values by developing a specialized language for expressing these constraints. Various proposals to extend the Kasper-Rounds approach to deal with negative feature values are described by Moshier and Rounds (1987), Moshier (1988), Kasper (1988), Dawar and Vijayashanker (1989, 1990), Langholm (1989); other extensions to this framework are discussed by D6rre and Rounds (1989), Smolka (1988, 1989), and Nebel and Smolka (1989); and Shieber (1989) discusses the integration of such feature systems into a variety of parsing algorithms. One difficulty with this approach is that the constraint language is ""custom built,"" so important properties, such as compactness and decidability, must be investigated from scratch. Moreover, it is often unclear if the treatment can be extended to handle other types of feature structures as well. Rounds (1988) proposes a model for set-va"
J91-2001,C90-2018,0,0.0726618,"Missing"
J91-2001,P88-1035,0,0.21851,"Missing"
J91-2001,P90-1022,1,0.94257,"f parsing algorithms. One difficulty with this approach is that the constraint language is ""custom built,"" so important properties, such as compactness and decidability, must be investigated from scratch. Moreover, it is often unclear if the treatment can be extended to handle other types of feature structures as well. Rounds (1988) proposes a model for set-valued features, but he does not provide a language for expressing constraints on such set-valued entities, or investigate the computational complexity of systems of such constraints. This paper follows an alternative strategy suggested in Johnson (1990a): axiomatize the relevant properties of feature structures in some well-understood language (here first-order logic) and translate constraints on these structures into the same language. 4 It is possible to avoid these problems by augmentingfeature structures with ""inequalityarcs,""as was first proposed (to my knowledge)by Karttunen(1984) and discussed in Johnson (1990a), Johnson (in press) and pages 67-72 of Johnson (1988). However,it is hard to justifythe existenceof such arcs if feature structures are supposed to be linguistic objects (rather than data structures that represent formulae ma"
J91-2001,C86-1156,1,0.91744,"Missing"
J91-2001,P86-1038,0,0.696315,"first unified with u' then f' is unified with v' and further unification of e' with e succeeds, since v' does satisfy f. Thus under this interpretation 136 Johnson Features and Formulae of negation and unification, the success or failure of a sequence of unifications depends on the order in which they are performed. 4 2. Feature Structures and Function-free Formulae These problems have generated a considerable body of work on the mathematical properties of feature structures and the constraints and operations that apply to them. Following Kaplan and Bresnan (1982), Pereira and Shieber (1984), Kasper and Rounds (1986, 1990), and Johnson (1988, 1990a) the constraints that determine the feature structures are regarded as formulae from a language for describing feature structures, rather than as feature structures themselves. Disjunction and negation appear only in expressions from the description language, rather than as components of the feature structures that these expressions describe. Thus the lexical entries in the examples above will be interpreted as formulae that constrain the feature structures that can be associated with these lexical items in a syntactic tree, rather than the feature structures"
J91-2001,W89-0203,0,0.0635578,"Missing"
J91-2001,P88-1029,0,0.0702914,"satisfiability of that conjunction (Kasper and Rounds 1990; Johnson 1988, 1990a; Pereira 1987). The most widely known model of feature structures and constraint language is the one developed to explain disjunctive feature values by Kasper and Rounds (1986, 1990) and Kasper (1986, 1987). The Kasper-Rounds treatment resolves the difficulties in interpreting disjunctive values by developing a specialized language for expressing these constraints. Various proposals to extend the Kasper-Rounds approach to deal with negative feature values are described by Moshier and Rounds (1987), Moshier (1988), Kasper (1988), Dawar and Vijayashanker (1989, 1990), Langholm (1989); other extensions to this framework are discussed by D6rre and Rounds (1989), Smolka (1988, 1989), and Nebel and Smolka (1989); and Shieber (1989) discusses the integration of such feature systems into a variety of parsing algorithms. One difficulty with this approach is that the constraint language is ""custom built,"" so important properties, such as compactness and decidability, must be investigated from scratch. Moreover, it is often unclear if the treatment can be extended to handle other types of feature structures as well. Rounds (19"
J91-2001,C86-1045,0,\N,Missing
J91-2001,C90-1003,1,\N,Missing
J91-2001,P87-1033,0,\N,Missing
J91-2001,P84-1027,0,\N,Missing
J91-2001,P84-1008,0,\N,Missing
J94-1001,E93-1004,0,0.0380332,"Missing"
J94-1001,P91-1002,0,0.0239242,"d in such a pure 'unification' framework. For example, the analysis of conjunctions in LFG (Kaplan and Maxwell 1988b) and the formalizations of Discourse Representation Theory (Kamp 1981) presented in Johnson and Klein (1986) and Johnson and Kay (1990) require additional mechanisms for representing and manipulating aggregates or sets of values in ways that are beyond the capability of such ""pure"" attribute-value systems. Further, sortal constraints (which also cannot be expressed as simple equality constraints) can be used to formulate simpler and more comprehensible grammars (Carpenter 1992; Carpenter and Pollard 1991; Pollard and Sag 1987, 1992). Versions of both of these kinds of constraint, as well as the familiar attribute-value constraints, can be expressed as Scho'nfinkel-Bernays'formulae (as demonstrated in Johnson 1991a, 1991b), so that the problem of determining the satisfiability of a system of such constraints is reduced to the satisfiability problem for the corresponding formula. This class of formulae (defined in Section 3.1) seems to be expressive enough for most linguistic purposes when used with an external phrase-structure backbone. That is, these formulae are used as annotations on phrase"
J94-1001,1991.iwpt-1.17,0,0.0985713,"Missing"
J94-1001,C90-2018,0,0.0376193,"Missing"
J94-1001,P88-1035,0,0.0385962,"Missing"
J94-1001,P91-1042,0,0.0692319,"Missing"
J94-1001,E91-1007,0,0.0214532,"ial time in the worst case, and unless P=NP no tractable general-purpose algorithm for determining the satisfiability of SB formulae exists. With present technology, the best we can hope for is an algorithm that performs adequately on the types of problems that we actually encounter. Sometimes disjunctive constraints can be (automatically) transformed into nondisjunctive ones, thus avoiding the problem entirely. For example, Alshawi (1992) describes a technique attributed to Colmerauer for transforming disjunctions of finitedomain feature-value constraints into conjunctions. Kasper (1988) and Hegner (1991) point out that Horn clauses, although technically disjunctions, can be handled considerably more efficiently than general disjunctive constraints. The forward-chaining mechanisms that they propose for treating these constraints appear to be special cases of the semi-naive algorithm sketched in this paper. Unfortunately, I know of no general adequate method for handling the disjunctive constraints that arise in real grammars with acceptable efficiency. The techniques discussed by Maxwell and Kaplan (1991, 1992) seem most directly compatible with the approach described in this paper, and the me"
J94-1001,P90-1022,1,0.861535,"y, some polynomial of the length of the input], and so ensure decidability.) Interestingly, a first-order logicbased approach similar to the one presented in this paper can also be developed for extended constraint formalisms capable of expressing the entire grammar, but this is not discussed further here; see Johnson (in press b) for details. In the approach developed here Sch6nfinkel-Bernays' formulae are used to express a variety of feature structure constraints. Previous work has shown that these formulae are expressive enough to define arbitrary disjunctions and negations of constraints (Johnson 1990a, 1990b), a kind of 'set-valued' entity (Johnson 1991a), and they can be used to impose useful sort constraints (Johnson 1991b). The expression of D-theory constraints on nodes in trees is discussed in this paper. This paper extends the ideas in these earlier papers with theoretical results that suggest a forward-chaining algorithm for determining the satisfiability of an arbitrary Sch6nfinkel-Bernays' formula. This generalizes the standard feature-graph unification algorithm and is closely related to the semi-naive bottom-up algorithm used in database theory. 1 For examples of this approach"
J94-1001,J91-2001,1,0.0785969,"nd Johnson and Kay (1990) require additional mechanisms for representing and manipulating aggregates or sets of values in ways that are beyond the capability of such ""pure"" attribute-value systems. Further, sortal constraints (which also cannot be expressed as simple equality constraints) can be used to formulate simpler and more comprehensible grammars (Carpenter 1992; Carpenter and Pollard 1991; Pollard and Sag 1987, 1992). Versions of both of these kinds of constraint, as well as the familiar attribute-value constraints, can be expressed as Scho'nfinkel-Bernays'formulae (as demonstrated in Johnson 1991a, 1991b), so that the problem of determining the satisfiability of a system of such constraints is reduced to the satisfiability problem for the corresponding formula. This class of formulae (defined in Section 3.1) seems to be expressive enough for most linguistic purposes when used with an external phrase-structure backbone. That is, these formulae are used as annotations on phrase structure rules in the manner described in, e.g., Kaplan and Bresnan (1982), Shieber (1986), and Johnson (1988). This paper extends the author's previous paper on the topic (Johnson 1991a) by sketching several ot"
J94-1001,C86-1156,1,0.787174,"ion Despite their simplicity, a surprisingly wide range of linguistic phenomena can be described in terms of simple equality constraints on values in attribute-value structures, which are a particularly simple kind of feature structure (see Shieber 1986; Johnson 1988; Uszkoreit 1986; and Bresnan 1982 for examples of some of these analyses). But some phenomena do not seem to be able to be described in such a pure 'unification' framework. For example, the analysis of conjunctions in LFG (Kaplan and Maxwell 1988b) and the formalizations of Discourse Representation Theory (Kamp 1981) presented in Johnson and Klein (1986) and Johnson and Kay (1990) require additional mechanisms for representing and manipulating aggregates or sets of values in ways that are beyond the capability of such ""pure"" attribute-value systems. Further, sortal constraints (which also cannot be expressed as simple equality constraints) can be used to formulate simpler and more comprehensible grammars (Carpenter 1992; Carpenter and Pollard 1991; Pollard and Sag 1987, 1992). Versions of both of these kinds of constraint, as well as the familiar attribute-value constraints, can be expressed as Scho'nfinkel-Bernays'formulae (as demonstrated i"
J94-1001,C88-1060,0,0.106856,"ay-Shanker (1990), D6rre and Eisele (1990), Johnson (1988, 1990a, 1990b, 1991a, 1991b, in press a), Karttunen (1984), Kasper (1987a, 1987b, 1988), Kasper and Rounds (1986, 1990), Langholm (1989), Pereira (1987), and Smolka (1992). 2 Examples of this approach are Carpenter, Pollard, and Franz (1991), D6rre (1991), D6rre and Eisele (1991), Johnson (in press b), Kay (1979, 1985a, 1985b), Pollard and Sag (1987), Rounds and Manaster-Ramer (1987), Smolka (1988), and Zajac (1992). 3 While it may well be that the universal recognition and parsing problems for natural language are undecidable (Chomsky [1986, 1988] points out that there is no contrary evidence), I know of no evidence that this is actually the case. It seems reasonable then to also investigate formalisms that can only express decidable systems of constraints (and for which there exist satisfiability-testing algorithms) if linguistically adequate systems can be found. 2 Mark Johnson Computing with Features as Formulae Specifically, it is s h o w n that the satisfying H e r b r a n d models of an arbitrary Sch6nfinkel-Bernays' formula are the fix points of certain functions, and that the least fixed points of these functions are all of the"
J94-1001,C88-1061,0,0.173072,"ms, further strengthening the connection between the theory of feature structures and databases. 1. Introduction Despite their simplicity, a surprisingly wide range of linguistic phenomena can be described in terms of simple equality constraints on values in attribute-value structures, which are a particularly simple kind of feature structure (see Shieber 1986; Johnson 1988; Uszkoreit 1986; and Bresnan 1982 for examples of some of these analyses). But some phenomena do not seem to be able to be described in such a pure 'unification' framework. For example, the analysis of conjunctions in LFG (Kaplan and Maxwell 1988b) and the formalizations of Discourse Representation Theory (Kamp 1981) presented in Johnson and Klein (1986) and Johnson and Kay (1990) require additional mechanisms for representing and manipulating aggregates or sets of values in ways that are beyond the capability of such ""pure"" attribute-value systems. Further, sortal constraints (which also cannot be expressed as simple equality constraints) can be used to formulate simpler and more comprehensible grammars (Carpenter 1992; Carpenter and Pollard 1991; Pollard and Sag 1987, 1992). Versions of both of these kinds of constraint, as well as"
J94-1001,P84-1008,0,0.156514,"nson 1991b). The expression of D-theory constraints on nodes in trees is discussed in this paper. This paper extends the ideas in these earlier papers with theoretical results that suggest a forward-chaining algorithm for determining the satisfiability of an arbitrary Sch6nfinkel-Bernays' formula. This generalizes the standard feature-graph unification algorithm and is closely related to the semi-naive bottom-up algorithm used in database theory. 1 For examples of this approach see Dawar and Vijay-Shanker (1990), D6rre and Eisele (1990), Johnson (1988, 1990a, 1990b, 1991a, 1991b, in press a), Karttunen (1984), Kasper (1987a, 1987b, 1988), Kasper and Rounds (1986, 1990), Langholm (1989), Pereira (1987), and Smolka (1992). 2 Examples of this approach are Carpenter, Pollard, and Franz (1991), D6rre (1991), D6rre and Eisele (1991), Johnson (in press b), Kay (1979, 1985a, 1985b), Pollard and Sag (1987), Rounds and Manaster-Ramer (1987), Smolka (1988), and Zajac (1992). 3 While it may well be that the universal recognition and parsing problems for natural language are undecidable (Chomsky [1986, 1988] points out that there is no contrary evidence), I know of no evidence that this is actually the case. I"
J94-1001,P88-1029,0,0.0207205,"s require exponential time in the worst case, and unless P=NP no tractable general-purpose algorithm for determining the satisfiability of SB formulae exists. With present technology, the best we can hope for is an algorithm that performs adequately on the types of problems that we actually encounter. Sometimes disjunctive constraints can be (automatically) transformed into nondisjunctive ones, thus avoiding the problem entirely. For example, Alshawi (1992) describes a technique attributed to Colmerauer for transforming disjunctions of finitedomain feature-value constraints into conjunctions. Kasper (1988) and Hegner (1991) point out that Horn clauses, although technically disjunctions, can be handled considerably more efficiently than general disjunctive constraints. The forward-chaining mechanisms that they propose for treating these constraints appear to be special cases of the semi-naive algorithm sketched in this paper. Unfortunately, I know of no general adequate method for handling the disjunctive constraints that arise in real grammars with acceptable efficiency. The techniques discussed by Maxwell and Kaplan (1991, 1992) seem most directly compatible with the approach described in this"
J94-1001,P86-1038,0,0.0168678,"ints on nodes in trees is discussed in this paper. This paper extends the ideas in these earlier papers with theoretical results that suggest a forward-chaining algorithm for determining the satisfiability of an arbitrary Sch6nfinkel-Bernays' formula. This generalizes the standard feature-graph unification algorithm and is closely related to the semi-naive bottom-up algorithm used in database theory. 1 For examples of this approach see Dawar and Vijay-Shanker (1990), D6rre and Eisele (1990), Johnson (1988, 1990a, 1990b, 1991a, 1991b, in press a), Karttunen (1984), Kasper (1987a, 1987b, 1988), Kasper and Rounds (1986, 1990), Langholm (1989), Pereira (1987), and Smolka (1992). 2 Examples of this approach are Carpenter, Pollard, and Franz (1991), D6rre (1991), D6rre and Eisele (1991), Johnson (in press b), Kay (1979, 1985a, 1985b), Pollard and Sag (1987), Rounds and Manaster-Ramer (1987), Smolka (1988), and Zajac (1992). 3 While it may well be that the universal recognition and parsing problems for natural language are undecidable (Chomsky [1986, 1988] points out that there is no contrary evidence), I know of no evidence that this is actually the case. It seems reasonable then to also investigate formalisms"
J94-1001,P83-1020,0,0.124151,"Missing"
J94-1001,J88-4001,0,0.0300491,"~ P(z, y). (14) It is easy to see that this definition is not equivalent to a Sch6nfinkel-Bernays' formula by expanding the equivalence into two implications and moving the embedded quantifier out. Vx Vy Vz C(x,y) + (-~D(x,y) A (P(z,x) ~ P(z,y))). (14a) Vx Vy 3z (~D(x, y) A P(z, x) ~ P(z, y) ) ~ C(x, y). (14b) Formula (14b) is not in SB because it contains an existential quantifier inside the scope of a universal quantifier. There are a number of ways to respond to this problem. First, we can abandon the attempt to work within the Sch6nfinkel-Bernays' class, and work with some other language. Rounds (1988) describes such a language called LFP, whose decidability follows from the fact that the domain of quantification is Computational Linguistics Volume 20, Number 1 restricted (just as in SB). However, it seems to be difficult to devise a decidable system capable of simultaneously expressing both tree structure and the variety of feature structure constraints that the SB approach described here can. Blackburn, Gardent, and Meyer-viol (1993) introduce a modal language LT for describing trees decorated with feature structures, whose satisfiability problem is undecidable. In the long run, such spec"
J94-1001,P87-1013,0,0.0234224,"zes the standard feature-graph unification algorithm and is closely related to the semi-naive bottom-up algorithm used in database theory. 1 For examples of this approach see Dawar and Vijay-Shanker (1990), D6rre and Eisele (1990), Johnson (1988, 1990a, 1990b, 1991a, 1991b, in press a), Karttunen (1984), Kasper (1987a, 1987b, 1988), Kasper and Rounds (1986, 1990), Langholm (1989), Pereira (1987), and Smolka (1992). 2 Examples of this approach are Carpenter, Pollard, and Franz (1991), D6rre (1991), D6rre and Eisele (1991), Johnson (in press b), Kay (1979, 1985a, 1985b), Pollard and Sag (1987), Rounds and Manaster-Ramer (1987), Smolka (1988), and Zajac (1992). 3 While it may well be that the universal recognition and parsing problems for natural language are undecidable (Chomsky [1986, 1988] points out that there is no contrary evidence), I know of no evidence that this is actually the case. It seems reasonable then to also investigate formalisms that can only express decidable systems of constraints (and for which there exist satisfiability-testing algorithms) if linguistically adequate systems can be found. 2 Mark Johnson Computing with Features as Formulae Specifically, it is s h o w n that the satisfying H e r"
J94-1001,J92-4004,0,0.0917562,"ty problem for the corresponding formula. This class of formulae (defined in Section 3.1) seems to be expressive enough for most linguistic purposes when used with an external phrase-structure backbone. That is, these formulae are used as annotations on phrase structure rules in the manner described in, e.g., Kaplan and Bresnan (1982), Shieber (1986), and Johnson (1988). This paper extends the author's previous paper on the topic (Johnson 1991a) by sketching several other linguistic applications of Sch6nfinkel-Bernays' formulae (including a version of D-theory [Marcus, Hindle, and Fleck 1983; Vijay-Shanker 1992]), and presenting a least-fixed-point theorem that serves as the theoretical basis for a ""forwardchaining"" algorithm for determining satisfiability of Sch6nfinkel-Bernays' formulae. Interestingly, this algorithm can be viewed both as a straightforward generalization * Cognitive and Linguistic Sciences, Box 1978, Brown University,Providence,RI. E-mail: Mark_Johnson@brown.edu Q 1994Association for Computational Linguistics Computational Linguistics Volume 20, Number 1 of the standard attribute-value unification algorithm and also as a nondeterministic variant of the semi-naive evaluation method"
J94-1001,J92-2002,0,0.02675,"and is closely related to the semi-naive bottom-up algorithm used in database theory. 1 For examples of this approach see Dawar and Vijay-Shanker (1990), D6rre and Eisele (1990), Johnson (1988, 1990a, 1990b, 1991a, 1991b, in press a), Karttunen (1984), Kasper (1987a, 1987b, 1988), Kasper and Rounds (1986, 1990), Langholm (1989), Pereira (1987), and Smolka (1992). 2 Examples of this approach are Carpenter, Pollard, and Franz (1991), D6rre (1991), D6rre and Eisele (1991), Johnson (in press b), Kay (1979, 1985a, 1985b), Pollard and Sag (1987), Rounds and Manaster-Ramer (1987), Smolka (1988), and Zajac (1992). 3 While it may well be that the universal recognition and parsing problems for natural language are undecidable (Chomsky [1986, 1988] points out that there is no contrary evidence), I know of no evidence that this is actually the case. It seems reasonable then to also investigate formalisms that can only express decidable systems of constraints (and for which there exist satisfiability-testing algorithms) if linguistically adequate systems can be found. 2 Mark Johnson Computing with Features as Formulae Specifically, it is s h o w n that the satisfying H e r b r a n d models of an arbitrary"
J94-1001,J90-1002,0,\N,Missing
J94-1001,C86-1045,0,\N,Missing
J94-1001,C90-1003,1,\N,Missing
J94-1001,J93-4001,0,\N,Missing
J94-1001,P87-1033,0,\N,Missing
J94-2005,C67-1009,1,0.41809,"ws this problem to be solved straightforwardly by the general constraint mechanism. It might be advantageous for the ELI to encode very specific information about a lexical item and the empty nodes that it sponsors. For example, the ELI for a WH 298 Mark Johnson and Martin Kay Parsing and Empty Nodes item might specify that the traces it sponsors are coindexed with the WH item itself. Assuming that indices are just unbound variables (thus coindexing is unification and contraindexing is an inequality constraint), an interesting technical problem arises if the basic parsing engine uses a chart (Kay 1967, 1980). Because it is fundamental to such devices that the label on an edge is copied before it is used as a component of a larger phrase, the variables representing indices will be copied or renamed and the indices on the WH item and its sponsored trace will no longer be identical. However, it is important that the sharing of variables among the components of an ELI be respected when they come together in a phrase. One way of overcoming this problem is to associate a vector of variables with each edge, in which each variable that is shared between two or more edges is assigned a unique posit"
J94-2005,W89-0206,1,0.69684,"construct the V' nodes and could therefore use its subcategorization frame to determine how many to construct. However, this would require an analysis of the grammar that is beyond the scope of standard parsing procedures. Notice that the V trace from which the subcategorization frame is projected is incorporated into the structure only after all the of V ~nodes have been constructed. Finally, the number of VP nodes is not determined by the subcategorization frame. No amount of grammar analysis will allow a top-down parser to predict the number of adjuncts attached to VP. A head-first parser (Kay 1989; van Noord 1993) seems best adapted to the treatment of empty nodes. This mixed parsing strategy in effect predicts a head top-down and builds its complements and specifiers bottom-up. The trace of the verb would be identified immediately after the I gave had been recognized, since that trace is the 290 Mark Johnson and Martin Kay Parsing and Empty Nodes head of the complement of the I. But it is not clear how such a strategy would cope with empty nodes that do not stand in a head-to-head relationship, such as the trace associated with the adjoined NP. The construction of the NP a big picture"
J94-2005,J81-4003,0,0.0632442,"Missing"
J94-2005,C92-2066,0,0.0198154,"red by some lexical or morphological item that appears in the input. By sponsoring we mean that every empty node is associated with some nonempty lexical item, which we call its sponsor, and that the number of empty nodes that a single lexical token can sponsor is fixed by the lexicon, so that the set of all empty nodes to appear in the parse can be determined directly by a simple inspection of the lexical items in the input string. Sponsorship is closely related to lexicalization in TAGs and CFGs (Schabes 1990, 1992; Schabes, AbeillG and Joshi 1988; Schabes and Waters 1993; Vijay-Shanker and Schabes 1992). In a lexicalized grammar every node in the parse tree originates from some lexical entry, so parsing becomes a jigsaw puzzle-like problem of finding a consistent way of assembling the pieces of trees associated with each lexical item. Sponsoring is a weaker notion, in that only some of the constituent structure, namely the lexical items and empty nodes, are specified in lexical entries. This seems plausible in a framework in which general principles of grammar (e.g., X~ theory, Case theory, etc.) determine the overall structure of the parse tree. In addition, finding an appropriate associati"
J94-2005,C88-2121,0,0.0967409,"Missing"
J94-2005,C92-1034,0,0.0210612,"pty node be sponsored by some lexical or morphological item that appears in the input. By sponsoring we mean that every empty node is associated with some nonempty lexical item, which we call its sponsor, and that the number of empty nodes that a single lexical token can sponsor is fixed by the lexicon, so that the set of all empty nodes to appear in the parse can be determined directly by a simple inspection of the lexical items in the input string. Sponsorship is closely related to lexicalization in TAGs and CFGs (Schabes 1990, 1992; Schabes, AbeillG and Joshi 1988; Schabes and Waters 1993; Vijay-Shanker and Schabes 1992). In a lexicalized grammar every node in the parse tree originates from some lexical entry, so parsing becomes a jigsaw puzzle-like problem of finding a consistent way of assembling the pieces of trees associated with each lexical item. Sponsoring is a weaker notion, in that only some of the constituent structure, namely the lexical items and empty nodes, are specified in lexical entries. This seems plausible in a framework in which general principles of grammar (e.g., X~ theory, Case theory, etc.) determine the overall structure of the parse tree. In addition, finding an appropriate associati"
J94-2005,W89-0208,0,\N,Missing
J94-2005,C92-1028,0,\N,Missing
J94-2005,P93-1017,0,\N,Missing
J95-3005,J91-1004,0,0.249486,"evaluation of the caller continuations corresponds to the completion steps in chart parsing. The CPS memoization described here caches such evaluations in the same w a y that the chart caches predictions, and the termination in the face of left recursive follows from the fact that no procedure PA is ever called with the same arguments twice. Thus given a CPS formalization of the parsing problem and an appropriate memoization technique, it is in fact the case that ""the maintenance of well-formed substring tables or charts can be seen as a special case of a more general technique: memoization"" (Norvig 1991), even if the grammar contains left recursion. 6. C o n c l u s i o n and Future Work This paper has shown how to generalize Norvig's application of memoization to top-down recognizers to yield terminating recognizers for left recursive grammars. Although not discussed here, the techniques used to construct the CPS recognizers can be generalized to parsers that construct parse trees, or associate categories with ""semantic values"" or ""unification-based"" feature structures. Specifically, we add extra arguments to each (caller) continuation whose value is the feature structure, parse tree a n d /"
J95-3005,P83-1021,0,0.0497004,"Missing"
J98-4004,J98-2005,0,0.0553171,"ng a single child with the same node label (i.e., which are expanded by a production X ~ X) were deleted. 2. PCFG Models of Tree Structures The theory of PCFGs is described elsewhere (e.g., Charniak [1993]), so it is only summarized here. A PCFG is a CFG in which each production A ~ o~ in the grammar&apos;s set of productions P is associated with an emission probability P(A ~ o~) that satisfies a normalization constraint P(A ~ ~) = 1 o~:A---~o~EP and a consistency or tightness constraint not discussed here, that PCFGs estimated from tree banks using the relative frequency estimator always satisfy (Chi and Geman 1998). A PCFG defines a probability distribution over the (finite) parse trees generated by the grammar, where the probability of a tree T is given by PO-) = H P(A-+ c~)cT(A~) A-*rxEP where Cr(A ~ o~) is the number of times the production A ~ oL is used in the derivation T. The PCFG that assigns maximum likelihood to the sequence ~ of trees in a treebank corpus is given by the relative frequency estimator. G ( A ~ ~) = C~(A -~ ~) ~,V~(NuT)* C~(A ~ o/) Here C~ (A ~ o~) is the number of times the production A --~ oz is used in derivations of the trees in ~. This estimation procedure can be used in a"
J98-4004,P96-1025,0,0.0461184,"cal models of natural languages. The relative frequency estimator provides a straightforward way of inducing these grammars from treebank corpora, and a broad-coverage parsing system can be obtained by using a parser to find a maximum-likelihood parse tree for the input string with respect to such a treebank gram_mar. PCFG parsing systems often perform as well as other simple broad-coverage parsing system for predicting tree structure from part-of-speech (POS) tag sequences (Charniak 1996). While PCFG models do not perform as well as models that are sensitive to a wider range of dependencies (Collins 1996), their simplicity makes them straightforward to analyze both theoretically and empirically. Moreover, since more sophisticated systems can be viewed as refinements of the basic PCFG model (Charniak 1997), it seems reasonable to first attempt to better understand the properties of PCFG models themselves. It is well known that natural language exhibits dependencies that context-free grammars (CFGs) cannot describe (Culy 1985; Shieber 1985). But the statistical independence assumptions embodied in a particular PCFG description of a particular natural language construction are in general much str"
J98-4004,J93-1005,0,0.0174838,"A Theoretical Investigation of Alternative Tree Structures We can gain some theoretical insight into the effect that different tree representations have on PCFG language models by considering several artifical corpora whose estimated PCFGs are simple enough to study analytically. PP attachment was chosen for investigation here because the alternative structures are simple and clear, but presumably the same points could be made for any construction that has several alternative tree representations. Correctly resolving PP attachment ambiguities requires information, such as lexical information (Hindle and Rooth 1993), that is simply not available to the PCFG models considered here. Still, one might hope that a PCFG model might be able to accurately reflect general statistical trends concerning attachment preferences in the training data, even if it lacks the information to correctly resolve individual cases. But as the analysis in this section makes clear, even this is not always obtained. For example, suppose our corpora only contain two trees, both of which have yields V Det N P Det N, are always analyzed as a VP with a direct object NP and a PP, and differ only as to whether the PP modifies the NP or t"
J98-4004,E91-1004,0,0.0310608,"Missing"
J98-4004,J93-2004,0,0.0571223,"Missing"
J98-4004,P98-1115,0,\N,Missing
J98-4004,C98-1111,0,\N,Missing
J98-4004,1991.iwpt-1.22,0,\N,Missing
K16-1005,D15-1034,0,0.352519,"Missing"
K16-1005,D15-1173,0,0.138573,"the TransR model, where head and tail entities are associated with their own projection matrices. The DISTMULT model (Yang et al., 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation. Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garc´ıa-Dur´an et al., 2016). Recently, Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garc´ıa-Dur´an et al. (2015), Guu et al. (2015) and Toutanova et al. (2016) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction. In fact, our new TransE-NMM model can be also viewed as a three-relation path model as it takes into account the neighborhood entity and relation information of both head and tail entities in each triple. differ in their score function f (h, r, t) and the algorithm used to optimize their margin-based objective function, e.g., SGD, AdaGrad (Duchi et al., 2011), A"
K16-1005,D15-1082,0,0.493604,"of Computing, Macquarie University, Sydney, Australia dat.nguyen@students.mq.edu.au, {kairit.sirts, mark.johnson}@mq.edu.au 2 Data61 & Australian National University lizhen.qu@data61.csiro.au Abstract et al., 2015; Nguyen et al., 2016) and generalize to large KBs (Krompa et al., 2015). Most embedding models for KB completion learn only from triples and by doing so, ignore lots of information implicitly provided by the structure of the knowledge graph. Recently, several authors have addressed this issue by incorporating relation path information into model learning (Garc´ıaDur´an et al., 2015; Lin et al., 2015a; Guu et al., 2015; Toutanova et al., 2016) and have shown that the relation paths between entities in KBs provide useful information and improve knowledge base completion. For instance, a three-relation path Knowledge bases are useful resources for many natural language processing tasks, however, they are far from complete. In this paper, we define a novel entity representation as a mixture of its neighborhood in the knowledge base and apply this technique on TransE—a well-known embedding model for knowledge base completion. Experimental results show that the neighborhood information signifi"
K16-1005,P15-1009,0,0.142407,"onal data. 4 available datasets WN11, FB13 and NELL186. For all of them, the validation and test sets containing both correct and incorrect triples have already been constructed. Statistical information about these datasets is given in Table 2. The two benchmark datasets1 , WN11 and FB13, were produced by Socher et al. (2013) for triple classification. WN11 is derived from the large lexical KB WordNet (Miller, 1995) involving 11 relation types. FB13 is derived from the large real-world fact KB FreeBase (Bollacker et al., 2008) covering 13 relation types. The NELL186 dataset2 was introduced by Guo et al. (2015) for both triple classification and entity prediction tasks, containing 186 most frequent relations in the KB of the CMU Never Ending Language Learning project (Carlson et al., 2010). 4.2 We evaluate our model on three commonly used benchmark tasks: triple classification, entity prediction and relation prediction. This subsection describes those tasks in detail. Triple classification: The triple classification task was first introduced by Socher et al. (2013), and since then it has been used to evaluate various embedding models. The aim of the task is to predict whether a triple (h, r, t) is c"
K16-1005,D15-1038,0,0.302974,"Missing"
K16-1005,D15-1191,0,0.602284,"and tail entities are associated with their own projection matrices. The DISTMULT model (Yang et al., 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation. Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garc´ıa-Dur´an et al., 2016). Recently, Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garc´ıa-Dur´an et al. (2015), Guu et al. (2015) and Toutanova et al. (2016) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction. In fact, our new TransE-NMM model can be also viewed as a three-relation path model as it takes into account the neighborhood entity and relation information of both head and tail entities in each triple. differ in their score function f (h, r, t) and the algorithm used to optimize their margin-based objective function, e.g., SGD, AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 20"
K16-1005,N13-1090,0,0.15165,"o project entity and relation vectors into a subspace. The TransH model (Wang et al., 2014) associates each relation with a relation-specific hyperplane and uses a projection vector to project entity vectors onto that hyperplane. TransD (Ji et al., 2015) and TransR/CTransR (Lin et al., 2015b) extend the TransH model by using two projection vectors and a matrix to project entity vectors into a relation-specific space, respectively. STransE 43 Luo et al. (2015) constructed relation paths between entities and viewing entities and relations in the path as pseudo-words applied Word2Vec algorithms (Mikolov et al., 2013) to produce pretrained vectors for these pseudo-words. Luo et al. (2015) showed that using these pre-trained vectors for initialization helps to improve the performance of the TransE, SME and SE models. RTransE (Garc´ıa-Dur´an et al., 2015), PTransE (Lin et al., 2015a) and TransE-COMP (Guu et al., 2015) are extensions of the TransE model. These models similarly represent a relation path by a vector which is the sum of the vectors of all relations in the path, whereas in the Bilinear-COMP model (Guu et al., 2015), each relation is a matrix and so it represents the relation path by matrix multip"
K16-1005,P15-1067,0,0.350831,"(Garc´ıa-Dur´an et al., 2016; Nickel et al., 2016). In TransE, both entities e and relations r are represented with k-dimensional vectors v e ∈ Rk and v r ∈ Rk , respectively. These vectors are chosen such that for each triple (h, r, t) ∈ G: vh + vr ≈ vt (h,r,t)∈G 0 (h0 ,r,t0 )∈G(h,r,t) + ∪ {(h, r, t0 ) |t0 ∈ E, (h, r, t0 ) ∈ / G} is the set of incorrect triples generated by corrupting the correct triple (h, r, t) ∈ G. We applied the “Bernoulli” trick to choose whether to generate the head or tail entity when sampling an incorrect triple (Wang et al., 2014; Lin et al., 2015b; He et al., 2015; Ji et al., 2015; Ji et al., 2016). We use Stochastic Gradient Descent (SGD) with RMSProp adaptive learning rate to minimize L, and impose the following hard constraints during training: kv e k2 6 1 and kv r k2 6 1. We employ alternating optimization to minimize L. We first initialize the entity and relation-specific mixing parameters α and β to zero and only learn the randomly initialized entity and relation vectors v e and v r . Then we fix the learned vectors and only optimize the mixing parameters. In the final step, we fix again the mixing parameters and fine-tune the vectors. In all experiments presente"
K16-1005,P15-1016,0,0.0526464,"l., 2016) are extensions of the TransR model, where head and tail entities are associated with their own projection matrices. The DISTMULT model (Yang et al., 2015) is based on the Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation. Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garc´ıa-Dur´an et al., 2016). Recently, Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garc´ıa-Dur´an et al. (2015), Guu et al. (2015) and Toutanova et al. (2016) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction. In fact, our new TransE-NMM model can be also viewed as a three-relation path model as it takes into account the neighborhood entity and relation information of both head and tail entities in each triple. differ in their score function f (h, r, t) and the algorithm used to optimize their margin-based objective function, e.g., SGD, Ad"
K16-1005,N16-1054,1,0.648098,"us the corresponding mixing coefficient can be close to zero, whereas it could be relevant for predicting some other relationship, such as parent or spouse, in which case the relation-specific mixing coefficient for the child of relationship could be high. The primary contribution of this paper is introducing and formalizing the neighborhood mixture model. We demonstrate its usefulness by applying it to the well-known TransE model (Bordes et al., 2013). However, it could be applied to other embedding models as well, such as Bilinear models (Bordes et al., 2012; Yang et al., 2015) and STransE (Nguyen et al., 2016). While relation path models exploit extra information using longer paths existing in the KB, the neighborhood mixture model effectively incorporates information about many paths simultaneously. Our extensive Ne = {(e0 , r)|r ∈ R ∪ R−1 , e0 ∈ Ne,r } is the set of all entity and relation pairs that are neighbors for entity e. Each entity e is associated with a k-dimensional vector v e ∈ Rk and relation-dependent vectors ue,r ∈ Rk , r ∈ R ∪ R−1 . Now we can define the neighborhood-based entity representation ϑe,r for an entity e ∈ E for predicting the relation r ∈ R as follows: ϑe,r = ae v e + X"
K16-1005,D14-1162,0,0.115248,"orted results on the WN11 and FB13 datasets. The first five rows report the performance of models that use TransE to initialize the entity and relation vectors. The last eight rows present the accuracy of models with randomly initialized parameters. Table 4 shows that our TransE-NMM model obtains the highest accuracy on WN11 and achieves the second highest result on FB13. Note that there are higher results reported for NTN (Socher et al., 2013), Bilinear-COMP (Guu et al., 2015) and TransE-COMP when entity vectors are initialized by averaging the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014). It is not surprising as many entity names in WordNet and FreeBase are lexically meaningful. It is possible for all other embedding models to utilize the pre-trained word vectors as well. However, as pointed out by Wang et al. (2014) and Guu et al. (2015), averaging the pre-trained word vectors for initializing entity vectors is an open problem and it is not always useful since entity names in many domain-specific KBs are not lexically meaningful. Table 5 compares the accuracy for triple classification, the raw mean rank and raw Hits@10 scores for entity prediction on the NELL186 dataset. The"
K16-1005,N16-1105,0,\N,Missing
K16-1005,E17-1013,0,\N,Missing
K16-1005,P16-1136,0,\N,Missing
K16-1005,P16-1124,0,\N,Missing
K16-1005,P16-1219,0,\N,Missing
K16-1005,D16-1145,0,\N,Missing
K17-1033,W14-3204,0,0.0446438,"Missing"
K17-1033,W14-3210,0,0.0211176,"Missing"
K17-1033,P16-1221,0,0.215132,"ss visible on the constrained topic DementiaBank. The contributions of this paper are the following: Semantic idea density (SID) (Ahmed et al., 2013a,b) relies on a set of pre-defined information content units (ICU). ICU is an object or action that can be seen on the picture or is told in the story and is expected to be mentioned in the narrative. For instance, assuming that the words in capital letters and square brackets in the example sentence shown in Table 1 belong to the set of pre-defined ICUs the SID is computed by normalising the ICU count with the token count: 2/9 ≈ 0.222. Recently, Yancheva and Rudzicz (2016), proposed a method for computing SID based on word embedding clusters. We use their method for computing SID as it does not rely on any pre-defined ICU inventory and thus is applicable also on free-topic datasets. PID and SID are complementary definitions of idea density with SID being naturally applicable in standardised picture description or story re-telling tasks while PID is more suitable on datasets of spontaneous speech on free topics. 1. Development of DEPID, the new dependencybased method for automatically computing PID and its version DEPID-R which enables to detect and exclude idea"
K17-1038,D15-1040,1,0.934365,"n (Tsai et al., 2016; Nothman et al., 2013) and machine translation (Zoph et al., 2016). However, as far as we know, there is no prior work on crosslingual transfer learning for semantic parsing, which is the topic of this paper. There are several common techniques for transfer learning across domains. The simplest approach is Fine Tune, where the model is first trained on the source domain and then finetuned on the target domain (Watanabe et al., 2016). Using some form of regularization (e.g. L2 ) to encourage the target model to remain similar to the source model is another common approach (Duong et al., 2015a). In this approach, the model is trained in the cascade style, where the source model is trained first and then used as in a prior when training the target model. It is often beneficial to jointly train the source and target models under a single objective function (Collobert et al., 2011; Firat et al., 2016; Zoph and Knight, 2016). Combining source and target data together into a single dataset is a simple way to jointly train for both domains. However, this approach might not work well in the crosslingual case, i.e. transfer from one language to another, because there may not be many share"
K17-1038,W13-3520,0,0.0273841,"al word embeddings across source and target languages following the approach of Duong et al. (2016), who achieve high performance on several monolingual and crosslingual evaluation metrics. Their work is essentially a multilingual extension of word2vec, where they use a context in one language to predict a target word in another language. The target words in the other language are obtained by looking up that word in a bilingual dictionary. Thus, the input to their model is monolingual data in both languages and a bilingual dictionary. We use monolingual data from pre-processed Wikipedia dump (Al-Rfou et al., 2013) with bilingual dictionary from Panlex (Kamholz et al., 2014). We initialize the seq2seq source embeddings of both languages with the crosslingual word embeddings. However, we do not update these embeddings. We apply crosslingual word embeddings (+XlingEmb) to the All model (§3.3) and the Dual encoder model (§3.2) and jointly train for the source and target language. For other models described in this paper, we initialize with monolingual word embeddings. Wo gibt es Kindergärten in Hamburg? German encoder Figure 3: Dual encoder model where each language has a separate encoder but both share th"
K17-1038,D16-1136,1,0.846628,"ddings Overcoming lexical differences is a key challenge in crosslingual domain adaptation. Prior work on domain adaptation found features that are common across languages, such as high-level linguistic features extracted from the World Atlas of Language Structures (Dryer and Haspelmath, 2013), crosslingual word clusters (T¨ackstr¨om et al., 2012) and crosslingual word embeddings (Ammar et al., 2016). Here, we extend crosslingual word embeddings as the crosslingual features for semantic parsing. We train crosslingual word embeddings across source and target languages following the approach of Duong et al. (2016), who achieve high performance on several monolingual and crosslingual evaluation metrics. Their work is essentially a multilingual extension of word2vec, where they use a context in one language to predict a target word in another language. The target words in the other language are obtained by looking up that word in a bilingual dictionary. Thus, the input to their model is monolingual data in both languages and a bilingual dictionary. We use monolingual data from pre-processed Wikipedia dump (Al-Rfou et al., 2013) with bilingual dictionary from Panlex (Kamholz et al., 2014). We initialize t"
K17-1038,Q16-1031,0,0.146584,"ify the model to adapt for both domains (or languages). Watanabe et al. (2016) propose a dual output model where each output is used for one domain. Kim et al. (2016) extend the feature augmentation approach of Daume III (2007) for deep learning by augmenting different models for each domain. In this paper we experiment with multiple encoders for the sequence-to-sequence attentional model, as described in §3.2. While some of the methods we investigate in this paper have been explored in the domain of syntactic parsing - Tiedemann (2014) used machine translation for cross-lingual transfer, and Ammar et al. (2016) show that a single parser can produce syntactic analyses in multiple languages - our work applies them to semantic parsing. 3 w1 datasets. We begin by describing the basic attentional model and then present our methods for transfer learning to different languages. 3.1 Baseline attentional model The baseline attentional seq2seq model (TGT Only) is shown in Figure 1. The source utterance is represented as a sequence of vectors S1 , S2 , . . . , Sm . Each Si is the output of an embeddings lookup. The model has two main components: an encoder and a decoder. For the encoder, we use a bidirectional"
K17-1038,N16-1101,0,0.0367122,"Missing"
K17-1038,P16-1154,0,0.0199753,"cted LF (LF-predicted) are then replaced with the unknown words (LF-lexicalised). the jointly trained model, where we instead use crosslingual word embeddings (§3.3.1). In order to handle unknown words, during training, all words that are low frequency and capitalized are replaced with the special symbol UNK in both utterance and logical form. Effectively, we target low-frequency named entities in the dataset. This is a simple but effective version of delexicalization, which does not require a named entity recognizer.3 However, unlike previous work (Jia and Liang, 2016; Gulcehre et al., 2016; Gu et al., 2016), we also retain the original sentence in the training data, which results in a substantial performance improvement. The intuition is that the model is capable of learning a useful signal even for very rare words. During test time, we replace (from left to right) the UNK in the logical form with the corresponding word in the source utterance. Figure 2 shows examples of handling unknown words during training and testing. At train time, the two words Cin´ema and Chaplin are replaced with UNK in both utterance and logical form. At test time, the first and second UNK in the logical form are replac"
K17-1038,P07-1033,0,0.444267,"Missing"
K17-1038,P16-1014,0,0.0254254,"NK symbols in the predicted LF (LF-predicted) are then replaced with the unknown words (LF-lexicalised). the jointly trained model, where we instead use crosslingual word embeddings (§3.3.1). In order to handle unknown words, during training, all words that are low frequency and capitalized are replaced with the special symbol UNK in both utterance and logical form. Effectively, we target low-frequency named entities in the dataset. This is a simple but effective version of delexicalization, which does not require a named entity recognizer.3 However, unlike previous work (Jia and Liang, 2016; Gulcehre et al., 2016; Gu et al., 2016), we also retain the original sentence in the training data, which results in a substantial performance improvement. The intuition is that the model is capable of learning a useful signal even for very rare words. During test time, we replace (from left to right) the UNK in the logical form with the corresponding word in the source utterance. Figure 2 shows examples of handling unknown words during training and testing. At train time, the two words Cin´ema and Chaplin are replaced with UNK in both utterance and logical form. At test time, the first and second UNK in the logic"
K17-1038,P16-1004,0,0.0485956,"word embeddings into a sequence-tosequence model, and apply it to semantic parsing. To the best of our knowledge, we are the first to apply crosslingual word embedding in a sequence-to-sequence model. • Our joint model allows us to also process input with code-switching. We develop a new dataset for evaluating semantic parsing on code-switching input which we make publicly available.1 2 Related work Deep learning and the sequence-to-sequence approach in particular have achieved wide success in many applications, reaching state-of-the-art performance for semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016), machine translation (Luong et al., 2015b), image caption gen1 Instead of combining data, a more sophisticated github.com/vbtagitlab/code-switching 380 GeoQuery ATIS Number of utterances 880 5410 Jia and Liang (2016) Zettlemoyer and Collins (2007) Koˇcisk´y et al. (2016) Dong and Lapata (2016) Liang et al. (2011) Kwiatkowksi et al. (2010) Zhao and Huang (2015) 89.3 86.1 87.3 87.1 91.1 88.6 88.9 83.3 84.6 84.6 82.8 84.2 TGT Only 86.1 86.1 <s&gt; wi-1 wi wn Decoder HT Ci Attention Encoder HS Representation Table 1: Performance of the baseline attentional model (TGT Only) on GeoQuery (Zettlemoyer a"
K17-1038,N16-1088,0,0.0795549,"Missing"
K17-1038,P15-2139,1,0.921834,"n (Tsai et al., 2016; Nothman et al., 2013) and machine translation (Zoph et al., 2016). However, as far as we know, there is no prior work on crosslingual transfer learning for semantic parsing, which is the topic of this paper. There are several common techniques for transfer learning across domains. The simplest approach is Fine Tune, where the model is first trained on the source domain and then finetuned on the target domain (Watanabe et al., 2016). Using some form of regularization (e.g. L2 ) to encourage the target model to remain similar to the source model is another common approach (Duong et al., 2015a). In this approach, the model is trained in the cascade style, where the source model is trained first and then used as in a prior when training the target model. It is often beneficial to jointly train the source and target models under a single objective function (Collobert et al., 2011; Firat et al., 2016; Zoph and Knight, 2016). Combining source and target data together into a single dataset is a simple way to jointly train for both domains. However, this approach might not work well in the crosslingual case, i.e. transfer from one language to another, because there may not be many share"
K17-1038,P16-1002,0,0.407721,"incorporate bilingual word embeddings into a sequence-tosequence model, and apply it to semantic parsing. To the best of our knowledge, we are the first to apply crosslingual word embedding in a sequence-to-sequence model. • Our joint model allows us to also process input with code-switching. We develop a new dataset for evaluating semantic parsing on code-switching input which we make publicly available.1 2 Related work Deep learning and the sequence-to-sequence approach in particular have achieved wide success in many applications, reaching state-of-the-art performance for semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016), machine translation (Luong et al., 2015b), image caption gen1 Instead of combining data, a more sophisticated github.com/vbtagitlab/code-switching 380 GeoQuery ATIS Number of utterances 880 5410 Jia and Liang (2016) Zettlemoyer and Collins (2007) Koˇcisk´y et al. (2016) Dong and Lapata (2016) Liang et al. (2011) Kwiatkowksi et al. (2010) Zhao and Huang (2015) 89.3 86.1 87.3 87.1 91.1 88.6 88.9 83.3 84.6 84.6 82.8 84.2 TGT Only 86.1 86.1 <s&gt; wi-1 wi wn Decoder HT Ci Attention Encoder HS Representation Table 1: Performance of the baseline attentional model (TGT Only) on"
K17-1038,P13-2017,0,0.0296699,"Missing"
K17-1038,C82-1023,0,0.488939,"el’s ability to parse utterances consisting of both English and German on our manually constructed code-switching testset.6 An example of constructed code-switching utterance is shown in Table 2. Note that our models are only trained on “pure” English and German utterances; there are no code-switching training examples in the input. Code-switching is a complex linguistic phenomenon and there are different accounts of the socio-linguistic conventions governing its use (Poplack, 2004; Isurin et al., 2009; MacSwan, 2017), as well as of the structural properties of utterances with code-switching (Joshi, 1982). Here we focus on the simple kind of code-switching where a single phrase is produced in a different language than the rest of the utterance. Our dataset was created by a fluent bilingual speaker who generated code-switching utterances for each of the 880 examples in the NLmaps test set. Approximately half of the utterances are “Denglish” (i.e., a German phrase embedded in an English matrix sentence) and half are “Gamerican” (an English phrase embedded in a German matrix sentence). NLmaps includes English and German utterances for each test example, and where possible our code-switching utter"
K17-1038,N13-1090,0,0.0407451,"Missing"
K17-1038,kamholz-etal-2014-panlex,0,0.0231767,"ng the approach of Duong et al. (2016), who achieve high performance on several monolingual and crosslingual evaluation metrics. Their work is essentially a multilingual extension of word2vec, where they use a context in one language to predict a target word in another language. The target words in the other language are obtained by looking up that word in a bilingual dictionary. Thus, the input to their model is monolingual data in both languages and a bilingual dictionary. We use monolingual data from pre-processed Wikipedia dump (Al-Rfou et al., 2013) with bilingual dictionary from Panlex (Kamholz et al., 2014). We initialize the seq2seq source embeddings of both languages with the crosslingual word embeddings. However, we do not update these embeddings. We apply crosslingual word embeddings (+XlingEmb) to the All model (§3.3) and the Dual encoder model (§3.2) and jointly train for the source and target language. For other models described in this paper, we initialize with monolingual word embeddings. Wo gibt es Kindergärten in Hamburg? German encoder Figure 3: Dual encoder model where each language has a separate encoder but both share the same decoder. Each training mini-batch only has monolingual"
K17-1038,C16-1038,0,0.0141202,"aseline attentional model (TGT Only) on GeoQuery (Zettlemoyer and Collins, 2005) and ATIS (Zettlemoyer and Collins, 2007) dataset compared with prior work. The best performance is shown in bold. Utterance S1 S2 S3 Sm How many Japanese restaurants are there in Paris ? Figure 1: The baseline attentional model as applied to our tasks. The input is the natural language utterance and the output is the logical form. approach for joint training is to modify the model to adapt for both domains (or languages). Watanabe et al. (2016) propose a dual output model where each output is used for one domain. Kim et al. (2016) extend the feature augmentation approach of Daume III (2007) for deep learning by augmenting different models for each domain. In this paper we experiment with multiple encoders for the sequence-to-sequence attentional model, as described in §3.2. While some of the methods we investigate in this paper have been explored in the domain of syntactic parsing - Tiedemann (2014) used machine translation for cross-lingual transfer, and Ammar et al. (2016) show that a single parser can produce syntactic analyses in multiple languages - our work applies them to semantic parsing. 3 w1 datasets. We begi"
K17-1038,D16-1116,0,0.0610112,"Missing"
K17-1038,D10-1119,0,0.305055,"Missing"
K17-1038,N12-1052,0,0.0796905,"Missing"
K17-1038,P11-1060,0,0.250491,"Missing"
K17-1038,C14-1175,0,0.0140642,"and the output is the logical form. approach for joint training is to modify the model to adapt for both domains (or languages). Watanabe et al. (2016) propose a dual output model where each output is used for one domain. Kim et al. (2016) extend the feature augmentation approach of Daume III (2007) for deep learning by augmenting different models for each domain. In this paper we experiment with multiple encoders for the sequence-to-sequence attentional model, as described in §3.2. While some of the methods we investigate in this paper have been explored in the domain of syntactic parsing - Tiedemann (2014) used machine translation for cross-lingual transfer, and Ammar et al. (2016) show that a single parser can produce syntactic analyses in multiple languages - our work applies them to semantic parsing. 3 w1 datasets. We begin by describing the basic attentional model and then present our methods for transfer learning to different languages. 3.1 Baseline attentional model The baseline attentional seq2seq model (TGT Only) is shown in Figure 1. The source utterance is represented as a sequence of vectors S1 , S2 , . . . , Sm . Each Si is the output of an embeddings lookup. The model has two main"
K17-1038,D15-1166,0,0.0534101,"odel, and apply it to semantic parsing. To the best of our knowledge, we are the first to apply crosslingual word embedding in a sequence-to-sequence model. • Our joint model allows us to also process input with code-switching. We develop a new dataset for evaluating semantic parsing on code-switching input which we make publicly available.1 2 Related work Deep learning and the sequence-to-sequence approach in particular have achieved wide success in many applications, reaching state-of-the-art performance for semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016), machine translation (Luong et al., 2015b), image caption gen1 Instead of combining data, a more sophisticated github.com/vbtagitlab/code-switching 380 GeoQuery ATIS Number of utterances 880 5410 Jia and Liang (2016) Zettlemoyer and Collins (2007) Koˇcisk´y et al. (2016) Dong and Lapata (2016) Liang et al. (2011) Kwiatkowksi et al. (2010) Zhao and Huang (2015) 89.3 86.1 87.3 87.1 91.1 88.6 88.9 83.3 84.6 84.6 82.8 84.2 TGT Only 86.1 86.1 <s&gt; wi-1 wi wn Decoder HT Ci Attention Encoder HS Representation Table 1: Performance of the baseline attentional model (TGT Only) on GeoQuery (Zettlemoyer and Collins, 2005) and ATIS (Zettlemoyer a"
K17-1038,K16-1022,0,0.0619101,"Missing"
K17-1038,W16-1629,0,0.318277,"s for POS tagging and Noun Phrase bracketing (Yarowsky and Ngai, 2001), dependency parsing (T¨ackstr¨om et al., 2012; McDonald et al., 2013), named entity recognition (Tsai et al., 2016; Nothman et al., 2013) and machine translation (Zoph et al., 2016). However, as far as we know, there is no prior work on crosslingual transfer learning for semantic parsing, which is the topic of this paper. There are several common techniques for transfer learning across domains. The simplest approach is Fine Tune, where the model is first trained on the source domain and then finetuned on the target domain (Watanabe et al., 2016). Using some form of regularization (e.g. L2 ) to encourage the target model to remain similar to the source model is another common approach (Duong et al., 2015a). In this approach, the model is trained in the cascade style, where the source model is trained first and then used as in a prior when training the target model. It is often beneficial to jointly train the source and target models under a single objective function (Collobert et al., 2011; Firat et al., 2016; Zoph and Knight, 2016). Combining source and target data together into a single dataset is a simple way to jointly train for b"
K17-1038,N01-1026,0,0.251484,"Missing"
K17-1038,D07-1071,0,0.179418,"Missing"
K17-1038,N15-1162,0,0.0276714,"Missing"
K17-1038,N16-1004,0,0.0281382,"Missing"
K17-1038,D16-1163,0,0.0664284,"Missing"
K17-3014,P16-1231,0,0.0892725,"Missing"
K17-3014,D15-1041,0,0.0586343,"ny external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs better than strong baselines and especially outperforms the neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016), achieving a new state of the art. 2 BiLSTM-based latent feature representations: Given an input sentence s consisting of n word tokens w1 , w2 , ..., wn , we represent each word wi in (•) s by an embedding ewi . Plank et al. (2016) and Ballesteros et al. (2015) show that character-based representations of words help improve POS tagging and dependency parsing performances. So, we also use a sequence BiLSTM (BiLSTMseq ) to compute a character-based vector representation for each word wi in s. For a word type w consisting of k characters w = c1 c2 ...ck , the input to the sequence BiLSTM consists of k character embeddings c1:k in which each embedding vector cj represents the j th character cj in w; and the output (∗) is the character-based embedding ew of the word type w, computed as: Our joint model e(∗) w = BiLSTMseq (c1:k ) In this section, we descr"
K17-3014,D12-1133,0,0.0393744,"ation for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project"
K17-3014,D15-1159,0,0.273183,"cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs better than strong baselines and especially ou"
K17-3014,A00-1031,0,0.217047,"0 73.4 75.4 79.2 82.5 10.6 81.3 79.9 79.5 81.3 80.4 79.3 3.4 80.2 80.1 79.4 80.7 81.8 81.7 9.2 78.5 76.6 76.6 77.6 78.9 79.6 4.4 74.8 69.6 72.0 73.9 73.8 75.8 4.5 Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctuation) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates. UDPipe is the trainable pipeline for processing CoNLL-U files (Straka et al., 2016). TnT denotes the second order HMM-based TnT tagger (Brants, 2000). CRF denotes the Conditional random fields-based tagger, presented in Plank et al. (2014). BiLSTM-aux refers to the state-of-the-art (SOTA) BiLSTMbased POS tagging model with an additional auxiliary loss for rare words (Plank et al., 2016). Note that the (old) language code for Hebrew “iw” is referred to as “he” as in Plank et al. (2016). [⊕]: Results are reported in Plank et al. (2016). Stack-prop refers to the SOTA Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016). 5-Chars denotes the absolute accuracy decrease of our jPTDP, when t"
K17-3014,W06-2920,0,0.112905,"to achieve the best accuracy. Alternatively, joint learning both POS tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 1 Introduction Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006, 2007 and 2017 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a; Zeman et al., 2017). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada 134 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e"
K17-3014,D14-1082,0,0.0290936,"Missing"
K17-3014,P11-1089,0,0.0526243,"142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Univers"
K17-3014,D16-1238,0,0.0563387,"Missing"
K17-3014,P14-1130,0,0.0298879,"k et al. (2016). Stack-prop refers to the SOTA Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016). 5-Chars denotes the absolute accuracy decrease of our jPTDP, when the character-based representations of words are not taken into account. B’15 denotes the character-based stack LSTM model for transition-based dependency parsing (Ballesteros et al., 2015). PipelinePtag refers to a greedy version of the approach proposed by Alberti et al. (2015). RBGParser refers to the graph-based dependency parser with tensor decomposition, presented in Lei et al. (2014). [*]: Results are reported in Zhang and Weiss (2016). 3.2 Implementation details ment set is when using 64-dimensional character embeddings, 128-dimensional word embeddings, 128-dimensional BiLSTM states, 2 BiLSTM layers and 100 hidden nodes in MLPs with one hidden layer.4 We then apply those hyper-parameters to all 18 remaining languages. Our jPTDP is implemented using DYNET v2.0 (Neubig et al., 2017).3 We optimize the objective function using Adam (Kingma and Ba, 2014) with default DYNET parameter settings and no mini-batches. We use a fixed random seed, and we do not utilize pre-trained em"
K17-3014,P13-1104,0,0.0177322,"Missing"
K17-3014,D11-1109,0,0.0242839,"to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental r"
K17-3014,I17-1007,0,0.0423582,"Missing"
K17-3014,P15-1033,0,0.0179515,"Missing"
K17-3014,P13-2109,0,0.0304432,"Missing"
K17-3014,C96-1058,0,0.755432,"g performance and ii) the the syntactic context of a parse tree could help resolve POS Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 1 Introduction Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006, 2007 and 2017 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a; Zeman et al., 2017). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada 134 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed int"
K17-3014,P05-1012,0,0.124355,"and ii) the the syntactic context of a parse tree could help resolve POS Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 1 Introduction Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006, 2007 and 2017 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a; Zeman et al., 2017). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada 134 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer percept"
K17-3014,J11-1007,0,0.0469794,"tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 1 Introduction Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006, 2007 and 2017 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a; Zeman et al., 2017). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada 134 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustrati"
K17-3014,I11-1136,0,0.040234,"endencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 language"
K17-3014,E06-1011,0,0.360014,"Missing"
K17-3014,P82-1020,0,0.835619,"Missing"
K17-3014,P16-2091,0,0.0520545,"Missing"
K17-3014,Q16-1032,0,0.0276542,"Missing"
K17-3014,U16-1017,1,0.896512,"Missing"
K17-3014,Q16-1023,0,0.513708,"07b; Bohnet, 2010; Zhang and Nivre, 2011; Martins et al., 2013; Choi and McCallum, 2013). Recent work shows that using deep learning in dependency parsing has obtained state-of-the-art performances. Several authors represent the core features with dense vector embeddings and then feed them as inputs to neural network-based classifiers (Chen and Manning, 2014; Weiss et al., 2015; Pei et al., 2015; Andor et al., 2016). In addition, others propose novel neural architectures for parsing to handle feature-engineering (Dyer et al., 2015; Cheng et al., 2016; Zhang et al., 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016a,b; Dozat and Manning, 2017; Ma and Hovy, 2017; Peng et al., 2017). We present a novel neural network model that learns POS tagging and graph-based dependency parsing jointly. Our model uses bidirectional LSTMs to learn feature representations shared for both POS tagging and dependency parsing tasks, thus handling the feature-engineering problem. Our extensive experiments, on 19 languages from the Universal Dependencies project, show that our model outperforms the state-of-the-art neural networkbased Stack-propagation model for joint POS tagging and transition-based dependency parsing, result"
K17-3014,W03-3017,0,0.1546,"Missing"
K17-3014,P10-1001,0,0.0222126,"ic context of a parse tree could help resolve POS Keywords: Neural network, POS tagging, Dependency parsing, Bidirectional LSTM, Universal Dependencies, Multilingual parsing. 1 Introduction Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the CoNLL 2006, 2007 and 2017 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a; Zeman et al., 2017). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches (Eisner, 1996; McDonald et al., 2005; Koo and Collins, 2010) and transition-based approaches (Yamada 134 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 134–142, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden laye"
K17-3014,P14-1069,0,0.0119884,"r, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs bett"
K17-3014,P15-1032,0,0.0279264,"Missing"
K17-3014,W03-3023,0,0.190146,"Missing"
K17-3014,P15-1031,0,0.0311799,"Missing"
K17-3014,P17-1186,0,0.0153844,"Missing"
K17-3014,petrov-etal-2012-universal,0,0.0164754,"t of all possible dependency trees for the input sentence s while scorearc (h, m) measures the score of the arc between the head hth word and the modifier mth word in s. Following Kiperwasser and Goldberg (2016b), we score an arc by using a MLP with one-node output layer (MLParc ) on top of the BiLSTMctx : 3 scorearc (h, m) = MLParc (v h ◦ v m ) Experiments 3.1 Experimental setup Following Zhang and Weiss (2016) and Plank et al. (2016), we conduct multilingual experiments on 19 languages from the Universal Dependencies (UD) treebanks1 v1.2 (Nivre et al., 2015), using the universal POS tagset (Petrov et al., 2012) instead of the language specific POS tagset.2 For dependency parsing, the evaluation metric is the labeled attachment score (LAS). LAS is the percentage of words which are correctly assigned both dependency arc and relation type. where v h and v m are the shared BiLSTM-based feature vectors representing the hth and mth words in s, respectively. We then compute a marginbased hinge loss Larc with loss-augmented inference to maximize the margin between the gold unlabeled parse tree and the highest scoring incorrect tree (Kiperwasser and Goldberg, 2016b). Dependency relation types are predicted i"
K17-3014,C14-1168,0,0.0255266,"8 81.7 9.2 78.5 76.6 76.6 77.6 78.9 79.6 4.4 74.8 69.6 72.0 73.9 73.8 75.8 4.5 Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctuation) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates. UDPipe is the trainable pipeline for processing CoNLL-U files (Straka et al., 2016). TnT denotes the second order HMM-based TnT tagger (Brants, 2000). CRF denotes the Conditional random fields-based tagger, presented in Plank et al. (2014). BiLSTM-aux refers to the state-of-the-art (SOTA) BiLSTMbased POS tagging model with an additional auxiliary loss for rare words (Plank et al., 2016). Note that the (old) language code for Hebrew “iw” is referred to as “he” as in Plank et al. (2016). [⊕]: Results are reported in Plank et al. (2016). Stack-prop refers to the SOTA Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016). 5-Chars denotes the absolute accuracy decrease of our jPTDP, when the character-based representations of words are not taken into account. B’15 denotes the c"
K17-3014,N15-1005,0,0.0152957,"4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs better than strong basel"
K17-3014,P16-2067,0,0.17323,"uber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs better than strong baselines and especially outperforms the neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016), achieving a new state of the art. 2 BiLSTM-based latent feature representations: Given an input sentence s consisting of n word tokens w1 , w2 , ..., wn , we represent each word wi in (•) s by an embedding ewi . Plank et al. (2016) and Ballesteros et al. (2015) show that character-based representations of words help improve POS tagging and dependency parsing performances. So, we also use a sequence BiLSTM (BiLSTMseq ) to compute a character-based vector representation for each word wi in s. For a word type w consisting of k characters w = c1 c2 ...ck , the input to the sequence BiLSTM consists of k character embeddings c1:k in which each embedding vector cj represents the j th character cj in w; and the output (∗) is the character-based embedding ew of the word type w, computed as: Our joint model e(∗) w = BiLSTMseq (c1"
K17-3014,P16-1147,0,0.0149578,"Missing"
K17-3014,D12-1046,0,0.015649,"Linguistics Vancouver, Canada, August 3-4, 2017. PRON punct cop nsubj MLP VERB LSTM MLP LSTM LSTM LSTM LSTM we MLP ADJ LSTM LSTM are PUNCT LSTM . finished nsubj cop <c> w e </c> punct we are finished . PRON VERB ADJ PUNCT Figure 1: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing. tion of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs. ambiguities (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Zhang et al., 2015; Alberti et al., 2015; Johannsen et al., 2016; Zhang and Weiss, 2016). In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTM— the bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint"
K17-3014,P11-2033,0,0.142463,"Missing"
K17-3014,P16-1131,0,0.0218431,"Missing"
K17-3014,L16-1680,0,0.129015,"3 69.5 70.3 72.4 73.6 66.8 5.4 84.5 82.4 83.6 83.9 84.7 84.9 2.3 79.4 78.0 73.4 75.4 79.2 82.5 10.6 81.3 79.9 79.5 81.3 80.4 79.3 3.4 80.2 80.1 79.4 80.7 81.8 81.7 9.2 78.5 76.6 76.6 77.6 78.9 79.6 4.4 74.8 69.6 72.0 73.9 73.8 75.8 4.5 Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctuation) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates. UDPipe is the trainable pipeline for processing CoNLL-U files (Straka et al., 2016). TnT denotes the second order HMM-based TnT tagger (Brants, 2000). CRF denotes the Conditional random fields-based tagger, presented in Plank et al. (2014). BiLSTM-aux refers to the state-of-the-art (SOTA) BiLSTMbased POS tagging model with an additional auxiliary loss for rare words (Plank et al., 2016). Note that the (old) language code for Hebrew “iw” is referred to as “he” as in Plank et al. (2016). [⊕]: Results are reported in Plank et al. (2016). Stack-prop refers to the SOTA Stack-propagation model for joint POS tagging and transition-based dependency parsing (Zhang and Weiss, 2016). 5"
K17-3014,P16-1218,0,0.0176882,"Missing"
L18-1410,neubig-mori-2010-word,0,0.024758,"ag such as B (Begin 1 In the traditional underscore-based representation in the Vietnamese word segmentation task (Nguyen et al., 2009), white space is only used to separate words while underscore is used to separate syllables inside a word. of a word) or I (Inside of a word). Another promising approach is joint word segmentation and POS tagging (Takahashi and Yamamoto, 2016; Nguyen et al., 2017b), which assigns a combined segmentation and POS tag to each syllable. Furthermore, Luu and Kazuhide (2012), Liu and Lin (2014) and Nguyen and Le (2016) proposed methods based on pointwise prediction (Neubig and Mori, 2010), where a binary classifier is trained to identify whether or not there is a word boundary between two syllables. In this paper, we propose a novel method to Vietnamese word segmentation. Our method automatically constructs a Single Classification Ripple Down Rules (SCRDR) tree (Compton and Jansen, 1990) to correct wrong segmentations given by a longest matchingbased word segmenter. On the benchmark Vietnamese treebank (Nguyen et al., 2009), experimental results show that our method obtains better accuracy and performance speed than the previous state-of-the-art methods JVnSegmenter (Nguyen et"
L18-1410,Y06-1028,0,0.813957,"elves (Thang et al., 2008; Le et al., 2008), thus creating challenges in Vietnamese word segmentation (Nguyen et al., 2012). Many approaches are proposed for the Vietnamese word segmentation task. Le et al. (2008), Pham et al. (2009) and Tran et al. (2012) applied the maximum matching strategy (NanYuan and YanBin, 1991) to generate all possible segmentations for each input sentence; then to select the best segmentation, Le et al. (2008) and Tran et al. (2012) used n-gram language models while Pham et al. (2009) employed part-ofspeech (POS) information from an external POS tagger. In addition, Nguyen et al. (2006), Dinh and Vu (2006) and Tran et al. (2010) considered this segmentation task as a sequence labeling task, using either a linear-chain CRF, SVM or MaxEnt model to assign each syllable a segmentation tag such as B (Begin 1 In the traditional underscore-based representation in the Vietnamese word segmentation task (Nguyen et al., 2009), white space is only used to separate words while underscore is used to separate syllables inside a word. of a word) or I (Inside of a word). Another promising approach is joint word segmentation and POS tagging (Takahashi and Yamamoto, 2016; Nguyen et al., 2017b)"
L18-1410,W09-3035,0,0.228624,"ll possible segmentations for each input sentence; then to select the best segmentation, Le et al. (2008) and Tran et al. (2012) used n-gram language models while Pham et al. (2009) employed part-ofspeech (POS) information from an external POS tagger. In addition, Nguyen et al. (2006), Dinh and Vu (2006) and Tran et al. (2010) considered this segmentation task as a sequence labeling task, using either a linear-chain CRF, SVM or MaxEnt model to assign each syllable a segmentation tag such as B (Begin 1 In the traditional underscore-based representation in the Vietnamese word segmentation task (Nguyen et al., 2009), white space is only used to separate words while underscore is used to separate syllables inside a word. of a word) or I (Inside of a word). Another promising approach is joint word segmentation and POS tagging (Takahashi and Yamamoto, 2016; Nguyen et al., 2017b), which assigns a combined segmentation and POS tag to each syllable. Furthermore, Luu and Kazuhide (2012), Liu and Lin (2014) and Nguyen and Le (2016) proposed methods based on pointwise prediction (Neubig and Mori, 2010), where a binary classifier is trained to identify whether or not there is a word boundary between two syllables."
L18-1410,R11-1056,1,0.803088,"OS tag of the word “anticipate” instead of the initial POS tag “VB.” To correct a wrong conclusion returned for a given case, a new node containing a new exception rule may be attached to the last node in the evaluation path. If the last node’s rule is the last satisfied rule given the case, the new node is added as its child with the “except” edge; otherwise, the new node is attached with the “if-not” edge. SCRDR has been successfully applied in NLP tasks for temporal relation extraction (Pham and Hoffmann, 2006), word lemmatization (Plisson et al., 2008), POS tagging (Xu and Hoffmann, 2010; Nguyen et al., 2011b; Nguyen et al., 2014; Nguyen et al., 2016), named entity recognition (Nguyen and Pham, 2012) and question answering (Nguyen et al., 2011a; Nguyen et al., 2013; Nguyen et al., 2017a). The works by Plisson et al. (2008), Nguyen et al. (2011b), Nguyen et al. (2014) and Nguyen et al. (2016) build the tree autoFigure 2: Diagram of our approach. matically, while others manually construct the tree. 3. Our approach This section describes our new error-driven approach to automatically construct a SCRDR tree to correct wrong segmentations produced by an initial word segmenter. Following Nguyen et al."
L18-1410,W12-5005,0,0.0366239,"Missing"
L18-1410,E14-2005,1,0.884766,"ticipate” instead of the initial POS tag “VB.” To correct a wrong conclusion returned for a given case, a new node containing a new exception rule may be attached to the last node in the evaluation path. If the last node’s rule is the last satisfied rule given the case, the new node is added as its child with the “except” edge; otherwise, the new node is attached with the “if-not” edge. SCRDR has been successfully applied in NLP tasks for temporal relation extraction (Pham and Hoffmann, 2006), word lemmatization (Plisson et al., 2008), POS tagging (Xu and Hoffmann, 2010; Nguyen et al., 2011b; Nguyen et al., 2014; Nguyen et al., 2016), named entity recognition (Nguyen and Pham, 2012) and question answering (Nguyen et al., 2011a; Nguyen et al., 2013; Nguyen et al., 2017a). The works by Plisson et al. (2008), Nguyen et al. (2011b), Nguyen et al. (2014) and Nguyen et al. (2016) build the tree autoFigure 2: Diagram of our approach. matically, while others manually construct the tree. 3. Our approach This section describes our new error-driven approach to automatically construct a SCRDR tree to correct wrong segmentations produced by an initial word segmenter. Following Nguyen et al. (2006) and Tran et al."
L18-1410,U17-1013,1,0.664336,"Nguyen et al. (2006), Dinh and Vu (2006) and Tran et al. (2010) considered this segmentation task as a sequence labeling task, using either a linear-chain CRF, SVM or MaxEnt model to assign each syllable a segmentation tag such as B (Begin 1 In the traditional underscore-based representation in the Vietnamese word segmentation task (Nguyen et al., 2009), white space is only used to separate words while underscore is used to separate syllables inside a word. of a word) or I (Inside of a word). Another promising approach is joint word segmentation and POS tagging (Takahashi and Yamamoto, 2016; Nguyen et al., 2017b), which assigns a combined segmentation and POS tag to each syllable. Furthermore, Luu and Kazuhide (2012), Liu and Lin (2014) and Nguyen and Le (2016) proposed methods based on pointwise prediction (Neubig and Mori, 2010), where a binary classifier is trained to identify whether or not there is a word boundary between two syllables. In this paper, we propose a novel method to Vietnamese word segmentation. Our method automatically constructs a Single Classification Ripple Down Rules (SCRDR) tree (Compton and Jansen, 1990) to correct wrong segmentations given by a longest matchingbased word s"
L18-1410,dinh-etal-2008-word,0,0.574669,"Missing"
ludusan-etal-2014-bridging,P12-1020,0,\N,Missing
ludusan-etal-2014-bridging,P07-1064,0,\N,Missing
ludusan-etal-2014-bridging,N09-1036,1,\N,Missing
ludusan-etal-2014-bridging,P13-2151,0,\N,Missing
ludusan-etal-2014-bridging,D10-1045,1,\N,Missing
ludusan-etal-2014-bridging,P12-1005,0,\N,Missing
N01-1016,P92-1008,0,0.0358054,"Missing"
N01-1016,A00-2018,1,0.155269,"r would add, e.g., the node dominating the EDITED node, seems much less critical. Note that of the major problems associated with transcribed speech, we choose to deal with only one of them, speech repairs, in a special fashion. Our reasoning here is based upon what one might and might not expect from a secondpass statistical parser. For example, ungrammaticality in some sense is relative, so if the training corpus contains the same kind of ungrammatical examples as the testing corpus, one would not expect ungrammaticality itself to be a show stopper. Furthermore, the best statistical parsers [3,5] do not use grammatical rules, but rather define probability distributions over all possible rules. Similarly, parentheticals and filled pauses exist in the newspaper text these parsers currently handle, albeit at a much lower rate. Thus there is no particular reason to expect these constructions to have a major impact.1 This leaves speech repairs as the one major phenomenon not present in written text that might pose a major problem for our parser. It is for that reason that we have chosen to handle it separately. The organization of this paper follows the architecture just described. Section"
N01-1016,P96-1025,0,0.100431,"Missing"
N01-1016,P97-1003,0,0.0749165,"r would add, e.g., the node dominating the EDITED node, seems much less critical. Note that of the major problems associated with transcribed speech, we choose to deal with only one of them, speech repairs, in a special fashion. Our reasoning here is based upon what one might and might not expect from a secondpass statistical parser. For example, ungrammaticality in some sense is relative, so if the training corpus contains the same kind of ungrammatical examples as the testing corpus, one would not expect ungrammaticality itself to be a show stopper. Furthermore, the best statistical parsers [3,5] do not use grammatical rules, but rather define probability distributions over all possible rules. Similarly, parentheticals and filled pauses exist in the newspaper text these parsers currently handle, albeit at a much lower rate. Thus there is no particular reason to expect these constructions to have a major impact.1 This leaves speech repairs as the one major phenomenon not present in written text that might pose a major problem for our parser. It is for that reason that we have chosen to handle it separately. The organization of this paper follows the architecture just described. Section"
N01-1016,J03-4003,0,0.0505572,"bitag tagger). We compare these results to a baseline “null” classifier, which never identifies a word as EDITED. Our basic measure of performance is the word misclassification rate (see Section 2.1). However, we also report precision and recall scores for EDITED words alone. All words are assigned one of the two possible labels, EDITED or not. However, in our evaluation we report the accuracy of only words other than punctuation and filled pauses. Our logic here is much the same as that in the statistical parsing community which ignores the location of punctuation for purposes of evaluation [3,5, 6] on the grounds that its placement is entirely conventional. The same can be said for filled pauses in the switchboard corpus. Our results are given in Table 2. They show that our classifier makes only approximately 1/3 W0 P0 , P1 , P2 , Pf T−1 , T0 , T1 , T2 , Tf Nm Nu Ni Nl Nr Ct Cw Ti Orthographic word Partial word flags POS tags Number of words in common in source and copy Number of words in source that do not appear in copy Number of words in interregnum Number of words to left edge of source Number of words to right edge of source Followed by identical tag flag Followed by identical word"
N01-1016,P99-1053,0,0.445837,"ring given to the second pass, an already existing statistical parser trained on a transcribed speech ∗ This research was supported in part by NSF grant LIS SBR 9720368 and by NSF ITR grant 20100203. corpus. (In particular, all of the research in this paper was performed on the parsed “Switchboard” corpus as provided by the Linguistic Data Consortium.) This architecture is based upon a fundamental assumption: that the semantic and pragmatic content of an utterance is based solely on the unedited words in the word sequence. This assumption is not completely true. For example, Core and Schubert [8] point to counterexamples such as “have the engine take the oranges to Elmira, um, I mean, take them to Corning” where the antecedent of “them” is found in the EDITED words. However, we believe that the assumption is so close to true that the number of errors introduced by this assumption is small compared to the total number of errors made by the system. In order to evaluate the parser’s output we compare it with the gold-standard parse trees. For this purpose a very simple third pass is added to the architecture: the hypothesized edited words are inserted into the parser output (see Section"
N01-1016,P97-1033,0,0.0433446,"Missing"
N01-1016,J99-4003,0,0.217249,"Missing"
N01-1016,P83-1019,0,0.0541695,"Missing"
N01-1016,P95-1037,0,0.117166,"Missing"
N04-1011,N01-1016,1,0.951414,"eriments below differ from the experiments of N¨oth and Kompe in many ways. First, we used speech transcripts rather than speech recognizer lattices. Second, we used a general-purpose broadcoverage statistical parser rather than a unification grammar parser with a hand-constructed grammar. 2 Method The data used for this study is the transcribed version of the Switchboard Corpus as released by the Linguistic Data Consortium. The Switchboard Corpus is a corpus of telephone conversations between adult speakers of varying dialects. The corpus was split into training and test data as described in Charniak and Johnson (2001). The training data consisted of all files in sections 2 and 3 of the Switchboard treebank. The testing corpus consists of files sw4004.mrg to sw4153.mrg, while files sw4519.mrg to sw4936.mrg were used as development corpus. 2.1 Prosodic variables Prosodic information for the corpus was obtained from forced alignments provided by Hamaker et al. (2003) and Ferrer et al. (2002). Hamaker et al. (2003) provided word alignments between the LDC parsed corpus and new alignments of the Switchboard Coprus. Most of the differences between the two alignments were individual lexical items. In cases of dif"
N04-1011,A00-2018,1,0.333122,"sodic cues improve parsing accuracy in the same way that punctuation does. Punctuation is represented in the various Penn treebank corpora as independent word-like tokens, with corresponding terminal and preterminal nodes, as shown in Figure 1 (Bies et al., 1995). Even though this seems linguistically highly unnatural (e.g., punctuation might indicate suprasegmental prosodic properties), statistical parsers generally perform significantly better when their training and test data contains punctuation represented in this way than if the punctuation is stripped out of the training and test data (Charniak, 2000; Engel et al., 2002; Johnson, 1998). On the Switchboard treebank data set using the experimental setup described below we obtained an F-score of 0.882 when using punctuation and 0.869 when punctuation was stripped out, replicating previous experiments demonstrating the importance of punctuation. (F-score is a standard measure of parse accuracy, see e.g., Manning and Sch¨utze (1999) for details). This paper investigates how prosodic cues, when encoded in the parser’s input in a manner similar to the way the Penn treebanks encode punctuation, affect parser accuracy. Our starting point is the ob"
N04-1011,W02-1007,1,0.912989,"ve parsing accuracy in the same way that punctuation does. Punctuation is represented in the various Penn treebank corpora as independent word-like tokens, with corresponding terminal and preterminal nodes, as shown in Figure 1 (Bies et al., 1995). Even though this seems linguistically highly unnatural (e.g., punctuation might indicate suprasegmental prosodic properties), statistical parsers generally perform significantly better when their training and test data contains punctuation represented in this way than if the punctuation is stripped out of the training and test data (Charniak, 2000; Engel et al., 2002; Johnson, 1998). On the Switchboard treebank data set using the experimental setup described below we obtained an F-score of 0.882 when using punctuation and 0.869 when punctuation was stripped out, replicating previous experiments demonstrating the importance of punctuation. (F-score is a standard measure of parse accuracy, see e.g., Manning and Sch¨utze (1999) for details). This paper investigates how prosodic cues, when encoded in the parser’s input in a manner similar to the way the Penn treebanks encode punctuation, affect parser accuracy. Our starting point is the observation that the P"
N04-1011,J98-4004,1,0.757271,"in the same way that punctuation does. Punctuation is represented in the various Penn treebank corpora as independent word-like tokens, with corresponding terminal and preterminal nodes, as shown in Figure 1 (Bies et al., 1995). Even though this seems linguistically highly unnatural (e.g., punctuation might indicate suprasegmental prosodic properties), statistical parsers generally perform significantly better when their training and test data contains punctuation represented in this way than if the punctuation is stripped out of the training and test data (Charniak, 2000; Engel et al., 2002; Johnson, 1998). On the Switchboard treebank data set using the experimental setup described below we obtained an F-score of 0.882 when using punctuation and 0.869 when punctuation was stripped out, replicating previous experiments demonstrating the importance of punctuation. (F-score is a standard measure of parse accuracy, see e.g., Manning and Sch¨utze (1999) for details). This paper investigates how prosodic cues, when encoded in the parser’s input in a manner similar to the way the Penn treebanks encode punctuation, affect parser accuracy. Our starting point is the observation that the Penn treebank ann"
N06-1020,P05-1022,1,0.841488,"native reranker reorders the n-best list. These components constitute two views of the data, though the reranker’s view is restricted to the parses suggested by the first-stage parser. The reranker is not able to suggest new parses and, moreover, uses the probability of each parse tree according to the parser as a feature to perform the reranking. Nevertheless, the reranker’s value comes from its ability to make use of more powerful features. 3.1 The first-stage 50-best parser The first stage of our parser is the lexicalized probabilistic context-free parser described in (Charniak, 2000) and (Charniak and Johnson, 2005). The parser’s grammar is a smoothed third-order Markov grammar, enhanced with lexical heads, their parts of speech, and parent and grandparent information. The parser uses five probability distributions, 153 one each for heads, their parts-of-speech, headconstituent, left-of-head constituents, and right-ofhead constituents. As all distributions are conditioned with five or more features, they are all heavily backed off using Chen back-off (the average-count method from Chen and Goodman (1996)). Also, the statistics are lightly pruned to remove those that are statistically less reliable/useful"
N06-1020,A00-2018,1,0.383878,"ses. Next, a discriminative reranker reorders the n-best list. These components constitute two views of the data, though the reranker’s view is restricted to the parses suggested by the first-stage parser. The reranker is not able to suggest new parses and, moreover, uses the probability of each parse tree according to the parser as a feature to perform the reranking. Nevertheless, the reranker’s value comes from its ability to make use of more powerful features. 3.1 The first-stage 50-best parser The first stage of our parser is the lexicalized probabilistic context-free parser described in (Charniak, 2000) and (Charniak and Johnson, 2005). The parser’s grammar is a smoothed third-order Markov grammar, enhanced with lexical heads, their parts of speech, and parent and grandparent information. The parser uses five probability distributions, 153 one each for heads, their parts-of-speech, headconstituent, left-of-head constituents, and right-ofhead constituents. As all distributions are conditioned with five or more features, they are all heavily backed off using Chen back-off (the average-count method from Chen and Goodman (1996)). Also, the statistics are lightly pruned to remove those that are s"
N06-1020,P96-1041,0,0.00644101,"ur parser is the lexicalized probabilistic context-free parser described in (Charniak, 2000) and (Charniak and Johnson, 2005). The parser’s grammar is a smoothed third-order Markov grammar, enhanced with lexical heads, their parts of speech, and parent and grandparent information. The parser uses five probability distributions, 153 one each for heads, their parts-of-speech, headconstituent, left-of-head constituents, and right-ofhead constituents. As all distributions are conditioned with five or more features, they are all heavily backed off using Chen back-off (the average-count method from Chen and Goodman (1996)). Also, the statistics are lightly pruned to remove those that are statistically less reliable/useful. As in (Charniak and Johnson, 2005) the parser has been modified to produce n-best parses. However, the n-best parsing algorithm described in that paper has been replaced by the much more efficient algorithm described in (Jimenez and Marzal, 2000; Huang and Chang, 2005). 3.2 The MaxEnt Reranker The second stage of our parser is a Maximum Entropy reranker, as described in (Charniak and Johnson, 2005). The reranker takes the 50-best parses for each sentence produced by the first-stage 50best pa"
N06-1020,W03-0407,0,0.52591,"Missing"
N06-1020,W01-0521,0,0.494912,"es self-training to POS-tagging and reports the same outcomes. One would assume that errors in the original model would be amplified in the new model. Parser adaptation can be framed as a semisupervised or unsupervised learning problem. In parser adaptation, one is given annotated training data from a source domain and unannotated data from a target. In some cases, some annotated data from the target domain is available as well. The goal is to use the various data sets to produce a model that accurately parses the target domain data despite seeing little or no annotated data from that domain. Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing ac152 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 152–159, c New York, June 2006. 2006 Association for Computational Linguistics curacy. The unsupervised adaptation experiment by Bacchiani et al. (2006) is the only successful instance of parsing self-training that we have found. Our work differs in that all our data is in-domain while Bacchiani et al. uses the Brown corpus as labelled data. These correspond to different scenarios. Additionally, we e"
N06-1020,P04-1013,0,0.0092553,"Missing"
N06-1020,J93-1005,0,0.073668,"Conjunctions are about the hardest things in parsing, and we have no grip on exactly what it takes to help parse them. Conversely, everyone expected improvements on unknown words, as the self-training should dras2000 0 Number of sentences 500 1000 1500 Better No change Worse 0 1 2 3 Number of CCs 4 5 Figure 5: How self-training improves performance as a function of number of conjunctions tically reduce the number of them. It is also the case that we thought PP attachment might be improved because of the increased coverage of prepositionnoun and preposition-verb combinations that work such as (Hindle and Rooth, 1993) show to be so important. Currently, our best conjecture is that unknowns are not improved because the words that are unknown in the WSJ are not significantly represented in the LA Times we used for self-training. CCs are difficult for parsers because each conjunct has only one secure boundary. This is particularly the case with longer conjunctions, those of VPs and Ss. One thing we know is that self-training always improves performance of the parsing model when used as a language model. We think CC improvement is connected with this fact and our earlier point that the probabilities of the 50-"
N06-1020,W05-1506,0,0.198641,"h, headconstituent, left-of-head constituents, and right-ofhead constituents. As all distributions are conditioned with five or more features, they are all heavily backed off using Chen back-off (the average-count method from Chen and Goodman (1996)). Also, the statistics are lightly pruned to remove those that are statistically less reliable/useful. As in (Charniak and Johnson, 2005) the parser has been modified to produce n-best parses. However, the n-best parsing algorithm described in that paper has been replaced by the much more efficient algorithm described in (Jimenez and Marzal, 2000; Huang and Chang, 2005). 3.2 The MaxEnt Reranker The second stage of our parser is a Maximum Entropy reranker, as described in (Charniak and Johnson, 2005). The reranker takes the 50-best parses for each sentence produced by the first-stage 50best parser and selects the best parse from those 50 parses. It does this using the reranking methodology described in Collins (2000), using a Maximum Entropy model with Gaussian regularization as described in Johnson et al. (1999). Our reranker classifies each parse with respect to 1,333,519 features (most of which only occur on few parses). The features consist of those descr"
N06-1020,P99-1069,1,0.452655,"Missing"
N06-1020,P02-1017,0,0.0932785,"tracting the appropriate parsing decisions from textual examples. Given sufficient labelled data, there are several “supervised” techniques of training high-performance parsers (Charniak and Johnson, 2005; Collins, 2000; Henderson, 2004). Other methods are “semi-supervised” where they use some labelled data to annotate unlabeled data. Examples of this include self-training (Charniak, 1997) and co-training (Blum and Mitchell, 1998; Steedman et al., 2003). Finally, there are “unsupervised” strategies where no data is labeled and all annotations (including the grammar itself) must be discovered (Klein and Manning, 2002). Semi-supervised and unsupervised methods are important because good labeled data is expensive, A simple method of incorporating unlabeled data into a new model is self-training. In self-training, the existing model first labels unlabeled data. The newly labeled data is then treated as truth and combined with the actual labeled data to train a new model. This process can be iterated over different sets of unlabeled data if desired. It is not surprising that self-training is not normally effective: Charniak (1997) and Steedman et al. (2003) report either minor improvements or significant damag"
N06-1020,J93-2004,0,0.0474116,"Missing"
N06-1020,N01-1023,0,0.274276,"nker. Co-training is another way to train models from unlabeled data (Blum and Mitchell, 1998). Unlike self-training, co-training requires multiple learners, each with a different “view” of the data. When one learner is confident of its predictions about the data, we apply the predicted label of the data to the training set of the other learners. A variation suggested by Dasgupta et al. (2001) is to add data to the training set when multiple learners agree on the label. If this is the case, we can be more confident that the data was labelled correctly than if only one learner had labelled it. Sarkar (2001) and Steedman et al. (2003) investigated using co-training for parsing. These studies suggest that this type of co-training is most effective when small amounts of labelled training data is available. Additionally, co-training for parsing can be helpful for parser adaptation. 3 Experimental Setup Our parsing model consists of two phases. First, we use a generative parser to produce a list of the top n parses. Next, a discriminative reranker reorders the n-best list. These components constitute two views of the data, though the reranker’s view is restricted to the parses suggested by the first-"
N06-1020,E03-1008,0,0.896701,"phenomenon. 2 Previous work 1 Introduction In parsing, we attempt to uncover the syntactic structure from a string of words. Much of the challenge of this lies in extracting the appropriate parsing decisions from textual examples. Given sufficient labelled data, there are several “supervised” techniques of training high-performance parsers (Charniak and Johnson, 2005; Collins, 2000; Henderson, 2004). Other methods are “semi-supervised” where they use some labelled data to annotate unlabeled data. Examples of this include self-training (Charniak, 1997) and co-training (Blum and Mitchell, 1998; Steedman et al., 2003). Finally, there are “unsupervised” strategies where no data is labeled and all annotations (including the grammar itself) must be discovered (Klein and Manning, 2002). Semi-supervised and unsupervised methods are important because good labeled data is expensive, A simple method of incorporating unlabeled data into a new model is self-training. In self-training, the existing model first labels unlabeled data. The newly labeled data is then treated as truth and combined with the actual labeled data to train a new model. This process can be iterated over different sets of unlabeled data if desir"
N06-1022,P04-1006,1,0.79928,"ll too slow for many applications. In some cases researchers need large quantities of parsed data and do not have the hundreds of machines necessary to parse gigaword corpora in a week or two. More pressingly, in real-time applications such as speech recognition, a parser would be only a part of a much larger system, and the system builders are not keen on giving the parser one of the ten seconds available to process, say, a thirty-word sentence. Even worse, some applications require the parsing of multiple candidate strings per sentence (Johnson and Charniak, 2004) or parsing from a lattice (Hall and Johnson, 2004), and in these applications parsing efficiency is even more important. We present here a multilevel coarse-to-fine (mlctf) PCFG parsing algorithm that reduces the complexity of the search involved in finding the best parse. It defines a sequence of increasingly more complex PCFGs, and uses the parse forest produced by one PCFG to prune the search of the next more complex PCFG. We currently use four levels of grammars in our mlctf algorithm. The simplest PCFG, which we call the level-0 grammar, contains only one nontrivial nonterminal and is so simple that minimal time is needed to parse a sent"
N06-1022,P04-1005,1,0.389081,"about a second per sentence. Unfortunately, this is still too slow for many applications. In some cases researchers need large quantities of parsed data and do not have the hundreds of machines necessary to parse gigaword corpora in a week or two. More pressingly, in real-time applications such as speech recognition, a parser would be only a part of a much larger system, and the system builders are not keen on giving the parser one of the ten seconds available to process, say, a thirty-word sentence. Even worse, some applications require the parsing of multiple candidate strings per sentence (Johnson and Charniak, 2004) or parsing from a lattice (Hall and Johnson, 2004), and in these applications parsing efficiency is even more important. We present here a multilevel coarse-to-fine (mlctf) PCFG parsing algorithm that reduces the complexity of the search involved in finding the best parse. It defines a sequence of increasingly more complex PCFGs, and uses the parse forest produced by one PCFG to prune the search of the next more complex PCFG. We currently use four levels of grammars in our mlctf algorithm. The simplest PCFG, which we call the level-0 grammar, contains only one nontrivial nonterminal and is so"
N06-1022,J98-4004,1,0.862318,"stituent type found in “S →NP VP .” is mapped into its generalization at level 1. The probabilities of all rules are computed using maximum likelihood for constituents at that level. The grammar used by the parser can best be described as being influenced by four components: 1. the nonterminals defined at that level of parsing, 2. the binarization scheme, 3. the generalizations defined over the binarization, and 171 Grammars induced in this way tend to be too specific, as the binarization introduce a very large number of very specialized phrasal categories (the Ai ). Following common practice Johnson (1998; Klein and Manning (2003b) we Markovize by replacing these nonterminals with ones that remember less of the immediate rule context. In our version we keep track of only the parent, the head constituent and the constituent immediately to the right or left, depending on which side of the constituent we are processing. With this scheme the above rules now look like this: A →Ad,c e Ad,c →Aa,c d Aa,c →a Ab,c Ab,c →b c So, for example, the rule “A →Ad,c e” would have a high probability if constituents of type A, with c as their head, often have d followed by e at their end. Lastly, we add parent an"
N06-1022,N03-1016,0,0.854396,"f the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: p(nki,j |s) = α(nki,j )β(nki,j ) . p(s) (1) It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compared directly to ours, but are roughly in the same equivalence class. Those presented in Charniak et al. (1998) are superior, but in Section 5 below we suggest that a combination of the techniques could yield better results still. Klein and Manning (2003a) describe efficient A∗ for the most likely parse, where pruning is accomplished by using Equation 1 and a true upper bound on the outside probability. While their maximum is a looser estimate of the outside probability, it is an admissible heuristic and together with an A∗ search is guaranteed to find the best parse first. One question is if the guarantee is worth the extra search required by the looser estimate of the true outside probability. Tsuruoka and Tsujii (2004) explore the framework developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap mani"
N06-1022,P05-1022,1,0.233451,"parser while the first stage is a regular-expression approximation of the grammar. Again, the second stage is constrained by the parses found in the first stage. Neither stage is smoothed. The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. The second stage explores all of the constituents not pruned out after the first stage. Related approaches are used in Hall (2004) and Charniak and Johnson (2005). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998) (and refined in Charniak et al. (1998)). Here efficiency is gained by using a standard chartparsing algorithm and pulling constituents off the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: p(nki,j |s) = α(nki,j )β(nki,j ) . p(s) (1) It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compa"
N06-1022,P03-1054,0,0.677716,"f the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: p(nki,j |s) = α(nki,j )β(nki,j ) . p(s) (1) It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compared directly to ours, but are roughly in the same equivalence class. Those presented in Charniak et al. (1998) are superior, but in Section 5 below we suggest that a combination of the techniques could yield better results still. Klein and Manning (2003a) describe efficient A∗ for the most likely parse, where pruning is accomplished by using Equation 1 and a true upper bound on the outside probability. While their maximum is a looser estimate of the outside probability, it is an admissible heuristic and together with an A∗ search is guaranteed to find the best parse first. One question is if the guarantee is worth the extra search required by the looser estimate of the true outside probability. Tsuruoka and Tsujii (2004) explore the framework developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap mani"
N06-1022,W98-1115,1,0.878022,"found in the first stage. Neither stage is smoothed. The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. The second stage explores all of the constituents not pruned out after the first stage. Related approaches are used in Hall (2004) and Charniak and Johnson (2005). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998) (and refined in Charniak et al. (1998)). Here efficiency is gained by using a standard chartparsing algorithm and pulling constituents off the agenda according to (an estimate of) their probability given the sentence. This probability is computed by estimating Equation 1: p(nki,j |s) = α(nki,j )β(nki,j ) . p(s) (1) It must be estimated because during the bottom-up chart-parsing algorithm, the true outside probability cannot be computed. The results cited in Caraballo and Charniak (1998) cannot be compared directly to ours, but are roughly in the same equivalence class. Those presented in Charniak et al. (1998) are superior, but in"
N06-1022,A00-2018,1,0.744452,"the multilevel dynamic programming algorithm needed for coarse-to-fine analysis (which they apply to decoding rather than parsing), and show how to perform exact coarse-to-fine computation, rather than the heuristic search we perform here. A paper closely related to ours is Goodman (1997). In our terminology, Goodman’s parser is a two-stage ctf parser. The second stage is a standard tree-bank parser while the first stage is a regular-expression approximation of the grammar. Again, the second stage is constrained by the parses found in the first stage. Neither stage is smoothed. The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. The second stage explores all of the constituents not pruned out after the first stage. Related approaches are used in Hall (2004) and Charniak and Johnson (2005). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998) (and refined in Charniak et al. (1998)). Here efficiency is gained by using a standard chartparsin"
N06-1022,P97-1003,0,0.216398,"equired by the heap manipulations necessary in this scheme. They describe an iterative deepening algorithm that does not require a heap. They also speed computation by precomputing more accurate upper bounds on the outside probabilities of various kinds of constituents. They are able to reduce by half the number of constituents required to find the best parse (compared to CKY). Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bilexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing — after all, there are no phrasal constituents to consider. The current paper can be thought of as a way to take the sting out of the grammar constant for PCFGs by parsing first with very few phrasal constituents and adding them only 170 Level: 0 1 © S1 S1                       HP"
N06-1022,P99-1059,0,0.0120516,"t does not require a heap. They also speed computation by precomputing more accurate upper bounds on the outside probabilities of various kinds of constituents. They are able to reduce by half the number of constituents required to find the best parse (compared to CKY). Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bilexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing — after all, there are no phrasal constituents to consider. The current paper can be thought of as a way to take the sting out of the grammar constant for PCFGs by parsing first with very few phrasal constituents and adding them only 170 Level: 0 1 © S1 S1                       HP                     P                       MP                    2 ©  S1           S"
N06-1022,J93-2004,0,0.0285293,"S1 S VP UCP SQ SBAR SBARQ SINV NP NAC NX LST X UCP FRAG ADJP QP CONJP ADVP INTJ PRN PRT PP PRT RRC WHADJP WHADVP WHNP WHPP Figure 1: The levels of nonterminal labels after most constituents have been pruned away. 3 Multilevel Course-to-fine Parsing We use as the underlying parsing algorithm a reasonably standard CKY parser, modified to allow unary branching rules. The complete nonterminal clustering is given in Figure 1. We do not cluster preterminals. These remain fixed at all levels to the standard Penn-tree-bank set Marcus et al. (1993). Level-0 makes two distinctions, the root node and everybody else. At level 1 we make one further distinction, between phrases that tend to be heads of constituents (NPs, VPs, and Ss) and those that tend to be modifiers (ADJPs, PPs, etc.). Level-2 has a total of five categories: root, things that are typically headed by nouns, those headed by verbs, things headed by prepositions, and things headed by classical modifiers (adjectives, adverbs, etc.). Finally, level 3 is the S1 S1 P HP P P PRP VBD He P . ate IN P . at DT 4. extra annotation to improve parsing accuracy. HP HP PRP VBD MP He ate IN"
N06-1022,J93-4001,0,0.0119203,"measured by the total number of constituents processed) is decreased by a factor of ten over standard CKY parsing at the final level. We also discuss some fine points of the results therein. Finally in section 5 we suggest that because the search space of mlctf algorithms is, at this point, almost totally unexplored, future work should be able to improve significantly on these results. 2 Previous Research Coarse-to-fine search is an idea that has appeared several times in the literature of computational linguistics and related areas. The 169 first appearance of this idea we are aware of is in Maxwell and Kaplan (1993), where a covering CFG is automatically extracted from a more detailed unification grammar and used to identify the possible locations of constituents in the more detailed parses of the sentence. Maxwell and Kaplan use their covering CFG to prune the search of their unification grammar parser in essentially the same manner as we do here, and demonstrate significant performance improvements by using their coarse-to-fine approach. The basic theory of coarse-to-fine approximations and dynamic programming in a stochastic framework is laid out in Geman and Kochanek (2001). This paper describes the"
N06-1022,P05-1012,0,0.0309759,"extra search required by the looser estimate of the true outside probability. Tsuruoka and Tsujii (2004) explore the framework developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap manipulations necessary in this scheme. They describe an iterative deepening algorithm that does not require a heap. They also speed computation by precomputing more accurate upper bounds on the outside probabilities of various kinds of constituents. They are able to reduce by half the number of constituents required to find the best parse (compared to CKY). Most recently, McDonald et al. (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bilexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing — after all, there are no phrasal constituents to consider. The current paper can be thought of as a way to take the sting out of the grammar cons"
N06-1022,J98-2004,1,\N,Missing
N06-1022,W97-0302,0,\N,Missing
N06-2019,N01-1016,1,0.723238,"that one? ings of the second differ in querying similarity versus exact match. Though an engaged listener rarely has difficulty distinguishing between such alternatives, studies show that deleting disfluencies from transcripts improves readability with no reduction in reading comprehension (Jones et al., 2003). The fact that disfluencies can be completely removed without compromising meaning is important. Earlier work had already made this claim regarding speech repairs1 and argued that there was consequently little value in syntactically analyzing repairs or evaluating our ability to do so (Charniak and Johnson, 2001). Moreover, this work showed that collateral damage to parse accuracy caused by repairs could be averted by deleting them prior to parsing, and this finding has been confirmed in subsequent studies (Kahn et al., 2005; Harper et al., 2005). But whereas speech repairs have received significant attention in the parsing literature, fillers have been relatively neglected. While one study has shown that the presence of interjection and parenthetical constituents in conversational speech reduces parse accuracy (Engel et al., 2002), these constituent types are defined to cover both fluent and disfluen"
N06-2019,A00-2018,0,0.0690299,"fit of deleting fillers (e.g. you know, like) early in parsing conversational speech. Readability studies have shown that disfluencies (fillers and speech repairs) may be deleted from transcripts without compromising meaning (Jones et al., 2003), and deleting repairs prior to parsing has been shown to improve its accuracy (Charniak and Johnson, 2001). We explore whether this strategy of early deletion is also beneficial with regard to fillers. Reported experiments measure the effect of early deletion under in-domain and out-of-domain parser training conditions using a state-of-the-art parser (Charniak, 2000). While early deletion is found to yield only modest benefit for in-domain parsing, significant improvement is achieved for out-of-domain adaptation. This suggests a potentially broader role for disfluency modeling in adapting text-based tools for processing conversational speech. 1 Introduction This paper evaluates the benefit of deleting fillers early in parsing conversational speech. We follow LDC (2004) conventions in using the term filler to encompass a broad set of vocalized space-fillers that can introduce syntactic (and semantic) ambiguity. For example, in the questions Did you know I"
N06-2019,W02-1007,1,0.838543,"ctically analyzing repairs or evaluating our ability to do so (Charniak and Johnson, 2001). Moreover, this work showed that collateral damage to parse accuracy caused by repairs could be averted by deleting them prior to parsing, and this finding has been confirmed in subsequent studies (Kahn et al., 2005; Harper et al., 2005). But whereas speech repairs have received significant attention in the parsing literature, fillers have been relatively neglected. While one study has shown that the presence of interjection and parenthetical constituents in conversational speech reduces parse accuracy (Engel et al., 2002), these constituent types are defined to cover both fluent and disfluent speech phenomena (Taylor, 1996), leaving the impact of fillers alone unclear. In our study, disfluency annotations (Taylor, 1995) are leveraged to identify fillers precisely, and these annotations are merged with treebank syntax. Extending the arguments of Charniak and Johnson with regard to repairs (2001), we argue there is little value in recovering the syntactic structure Readings of the first example differ in querying listener knowledge versus speaker action, while read1 See (Core and Schubert, 1999) for a prototypic"
N06-2019,graff-bird-2000-many,0,0.0448582,"ne translation, information extraction, etc.), and bleaching speech data to more closely resemble text has been shown to improve accuracy with some text-based processing tasks (Rosenfeld et al., 1995). For our study, a state-of-the-art filler detector (Johnson et al., 2004) is employed to delete fillers prior to parsing. Results show parse accuracy improves significantly, suggesting disfluency filtering may have a broad role in enabling text-based processing of speech data. 2 Disfluency in Brief In this section we give a brief introduction to disfluency, providing an excerpt from Switchboard (Graff and Bird, 2000) that demonstrates typical production of repairs and fillers in conversational speech. We follow previous work (Shriberg, 1994) in describing a repair in terms of three parts: the reparandum (the material repaired), the corrected alteration, and between these an optional interregnum (or editing term) consisting of one or more fillers. Our notion of fillers encompasses filled pauses (e.g. uh, um, ah) as well as other vocalized space-fillers annotated by LDC (Taylor, 1995), such as you know, i mean, like, so, well, etc. Annotations shown here are typeset with the following conventions: fillers a"
N06-2019,H05-1030,1,0.906299,"Missing"
N06-2019,P04-1005,1,0.893586,"Missing"
N06-2019,P02-1037,0,0.0290365,"esponding tree and then reinserted at the same position under a flat FILLER constituent, attached as highly as possible. Transforms were achieved using TSurgeon2 and Lingua::Treebank3 . For our out-of-domain training condition, the parser was trained on sections 2-21 of the Wall Street Journal (WSJ) corpus (Marcus et al., 1993). Punctuation and capitalization were removed to bleach our our textual training data to more closely resemble speech (Rosenfeld et al., 1995). We also tried automatically changing numbers, symbols, and abbreviations in the training text to match how they would be read (Roark, 2002), but this did not improve accuracy and so is not discussed further. 3.2 Evaluation Metrics Though disfluencies rarely complicate understanding for an engaged listener, deleting them from transcripts improves readability with no reduction in 74 As discussed earlier (§1), Charniak and Johnson (2001) have argued that speech repairs do not 2 3 http://nlp.stanford.edu/software/tsurgeon.shtml http://www.cpan.org contribute to meaning and so there is little value in syntactically analyzing repairs or evaluating our ability to do so. Consequently, they relaxed standard PARSEVAL (Black et al., 1991) t"
N06-2019,J93-2004,0,\N,Missing
N06-2019,P99-1053,0,\N,Missing
N07-1018,W06-1673,0,0.0467676,"Missing"
N07-1018,P04-1061,0,0.0368722,"is task is that simple PCFGs are not accurate models of English syntactic structure. We know that PCFGs 144 α = (1.0, 1.0) α = (0.5, 1.0) α = (0.1, 1.0) 5 4 P(θ1 |α) 3 2 1 0 0 0.2 0.4 0.6 0.8 Binomial parameter θ1 1 Figure 2: A Dirichlet prior α on a binomial parameter θ1 . As α1 → 0, P(θ1 |α) is increasingly concentrated around 0. that represent only major phrasal categories ignore a wide variety of lexical and syntactic dependencies in natural language. State-of-the-art systems for unsupervised syntactic structure induction system uses models that are very different to these kinds of PCFGs (Klein and Manning, 2004; Smith and Eisner, 2006).1 Our goal in this section is modest: we aim merely to provide an illustrative example of Bayesian inference using MCMC. As Figure 2 shows, when the Dirichlet prior parameter αr approaches 0 the prior probability PD (θr |α) becomes increasingly concentrated around 0. This ability to bias the sampler toward sparse grammars (i.e., grammars in which many productions have probabilities close to 0) is useful when we attempt to identify relevant productions from a much larger set of possible productions via parameter estimation. The Bantu language Sesotho is a richly agglut"
N07-1018,P06-1072,0,0.0107366,"FGs are not accurate models of English syntactic structure. We know that PCFGs 144 α = (1.0, 1.0) α = (0.5, 1.0) α = (0.1, 1.0) 5 4 P(θ1 |α) 3 2 1 0 0 0.2 0.4 0.6 0.8 Binomial parameter θ1 1 Figure 2: A Dirichlet prior α on a binomial parameter θ1 . As α1 → 0, P(θ1 |α) is increasingly concentrated around 0. that represent only major phrasal categories ignore a wide variety of lexical and syntactic dependencies in natural language. State-of-the-art systems for unsupervised syntactic structure induction system uses models that are very different to these kinds of PCFGs (Klein and Manning, 2004; Smith and Eisner, 2006).1 Our goal in this section is modest: we aim merely to provide an illustrative example of Bayesian inference using MCMC. As Figure 2 shows, when the Dirichlet prior parameter αr approaches 0 the prior probability PD (θr |α) becomes increasingly concentrated around 0. This ability to bias the sampler toward sparse grammars (i.e., grammars in which many productions have probabilities close to 0) is useful when we attempt to identify relevant productions from a much larger set of possible productions via parameter estimation. The Bantu language Sesotho is a richly agglutinative language, in whic"
N09-1012,N09-1009,0,0.689961,"selection, this performs far better than EM; however, performing parameter selection on a held-out set without the use of gold dependencies does not perform as well. Cohen et al. (2008) investigate using Bayesian Priors with DMV. The two priors they use are the Dirichlet (which we use here) and the Logistic Normal prior, which allows the model to capture correlations between different distributions. They initialize using the harmonic initializer of Klein and Manning (2004). They find that the Logistic Normal distribution performs much better than the Dirichlet with this initialization scheme. Cohen and Smith (2009), investigate (concurRule Description S → YH Select H as root YH → LH RH Move to split-head representation LH → HL STOP LH → L′H CONT L′H → L1H STOP L′H → L2H CONT L2H → YA L′H Arg A |dir = L, head = H, val = 1 L1H → YA HL Arg A |dir = L, head = H, val = 0 . . . . . . Ldog Ldog L1 dog L′dog L2 dog |dir = L, head = H, val = 0 L′dog YT he |dir = L, head = H, val = 0 TheL |dir = L, head = H, val = 1 L1 dog TheR Ybig |dir = L, head = H, val = 1 bigL bigR L′dog YT he L′dog dogL TheL L1 dog TheR Ybig bigL Figure 4: Extended Valence Grammar schema. As before, we omit rules involving the right parts o"
N09-1012,P99-1059,0,0.195221,"timates Q(t, θ), ¯ called mate of θ. the variational distribution, which approximates the ¯ α) by minimizing the posterior distribution P (t, θ|s, KL divergence of P from Q. Minimizing the KL divergence, it turns out, is equivalent to maximizing a lower bound F of the log marginal likelihood log P (s|α). and Q(t). Kurihara and Sato (2004) show that each Q(θ¯N¯ ) is a Dirichlet distribution with parameters α ˆ r = αr + EQ(t) f (t, r). 2.2 Split-head Bilexical CFGs In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bilexical CFGs (Eisner and Satta, 1999). These allow us to use the fast Eisner and Satta (1999) parsing algorithm to compute the expectations required by VB in O(m3 ) time (Eisner and Blatz, 2007; Johnson, 2007) where m is the length of the sentence.1 In the split-head bilexical CFG framework, each nonterminal in the grammar is annotated with a terminal symbol. For dependency grammars, these annotations correspond to words and/or parts-ofspeech. Additionally, split-head bilexical CFGs require that each word sij in sentence si is represented in a split form by two terminals called its left part sijL and right part sijR . The set of"
N09-1012,N07-1018,1,0.30347,"Missing"
N09-1012,P07-1022,1,0.763442,"ing the KL divergence, it turns out, is equivalent to maximizing a lower bound F of the log marginal likelihood log P (s|α). and Q(t). Kurihara and Sato (2004) show that each Q(θ¯N¯ ) is a Dirichlet distribution with parameters α ˆ r = αr + EQ(t) f (t, r). 2.2 Split-head Bilexical CFGs In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bilexical CFGs (Eisner and Satta, 1999). These allow us to use the fast Eisner and Satta (1999) parsing algorithm to compute the expectations required by VB in O(m3 ) time (Eisner and Blatz, 2007; Johnson, 2007) where m is the length of the sentence.1 In the split-head bilexical CFG framework, each nonterminal in the grammar is annotated with a terminal symbol. For dependency grammars, these annotations correspond to words and/or parts-ofspeech. Additionally, split-head bilexical CFGs require that each word sij in sentence si is represented in a split form by two terminals called its left part sijL and right part sijR . The set of these parts constitutes the terminal symbols of the grammar. This split-head property relates to a particular type of dependency grammar in which the left and right depende"
N09-1012,P02-1017,0,0.196791,") = P (t|s, θ¯(b) ) which can be calculated using the ¯ is then iniExpectation-Maximization E-Step. Q(θ) tialized using the standard VB M-step. For the Lexicalized-EVG, we modify this procedure slightly, by first running M B smoothed EVG models for 40 iterations each and selecting the best model in each cohort as before; each L-EVG distribution is initialized from its corresponding EVG distribution. The new P (A|h, H, d, v) distributions are set initially to their corresponding P (A|H, d, v) values. 6 Results We trained on the standard Penn Treebank WSJ corpus (Marcus et al., 1993). Following Klein and Manning (2002), sentences longer than 10 words after removing punctuation are ignored. We refer to this variant as WSJ10. Following Cohen et al. (2008), we train on sections 2-21, used 22 as a held-out development corpus, and present results evaluated on section 23. The models were all trained using Variational Bayes, and initialized as described in Section 5. To evaluate, we follow Cohen et al. (2008) in using the mean of the variational posterior Dirichlets as a point estimate θ¯′ . For the unsmoothed models we decode by selecting the Viterbi parse given θ¯′ , or argmaxt P (t|s, θ¯′ ). For the smoothed mo"
N09-1012,P04-1061,0,0.775008,"es and domains with minimal resources it is valuable to study methods for parsing without requiring annotated sentences. In this work, we focus on unsupervised dependency parsing. Our goal is to produce a directed graph of dependency relations (e.g. Figure 1) where each edge indicates a head-argument relation. Since the task is unsupervised, we are not given any examples of correct dependency graphs and only take words and their parts of speech as input. Most of the recent work in this area (Smith, 2006; Cohen et al., 2008) has focused on variants of the Dependency Model with Valence (DMV) by Klein and Manning (2004). DMV was the first unsupervised dependency grammar induction system to achieve accuracy above a right-branching baseline. However, DMV is not able to capture some of the more complex aspects of language. Borrowing some ideas from the supervised parsing literature, we present two new models: Extended Valence Grammar (EVG) and its lexicalized extension (L-EVG). The primary difference between EVG and DMV is that DMV uses valence information to determine the number of arguments a head takes but not their categories. In contrast, EVG allows different distributions over arguments for different vale"
N09-1012,J93-2004,0,0.0420379,"r θ¯(b) . We set the initial Q(t) = P (t|s, θ¯(b) ) which can be calculated using the ¯ is then iniExpectation-Maximization E-Step. Q(θ) tialized using the standard VB M-step. For the Lexicalized-EVG, we modify this procedure slightly, by first running M B smoothed EVG models for 40 iterations each and selecting the best model in each cohort as before; each L-EVG distribution is initialized from its corresponding EVG distribution. The new P (A|h, H, d, v) distributions are set initially to their corresponding P (A|H, d, v) values. 6 Results We trained on the standard Penn Treebank WSJ corpus (Marcus et al., 1993). Following Klein and Manning (2002), sentences longer than 10 words after removing punctuation are ignored. We refer to this variant as WSJ10. Following Cohen et al. (2008), we train on sections 2-21, used 22 as a held-out development corpus, and present results evaluated on section 23. The models were all trained using Variational Bayes, and initialized as described in Section 5. To evaluate, we follow Cohen et al. (2008) in using the mean of the variational posterior Dirichlets as a point estimate θ¯′ . For the unsmoothed models we decode by selecting the Viterbi parse given θ¯′ , or argmax"
N09-1012,P06-1072,0,0.0477532,"arts-of-speech. 104 Ldog Rdog L1 dog dogR L′dog YT he LT he RT he TheL TheR L1 dog Ybig Lbig Rbig bigL bigR L′dog dogL Figure 3: DMV split-head bilexical CFG parse of “The big dog barks.” Smith (2006) also investigates two techniques for maximizing likelihood while incorporating the locality bias encoded in the harmonic initializer for DMV. One technique, skewed deterministic annealing, ameliorates the local maximum problem by flattening the likelihood and adding a bias towards the Klein and Manning initializer, which is decreased during learning. The second technique is structural annealing (Smith and Eisner, 2006; Smith, 2006) which penalizes long dependencies initially, gradually weakening the penalty during estimation. If hand-annotated dependencies on a held-out set are available for parameter selection, this performs far better than EM; however, performing parameter selection on a held-out set without the use of gold dependencies does not perform as well. Cohen et al. (2008) investigate using Bayesian Priors with DMV. The two priors they use are the Dirichlet (which we use here) and the Logistic Normal prior, which allows the model to capture correlations between different distributions. They init"
N09-1012,W03-3023,0,0.0754888,"8 RB 0.23 DMV log normal-families 59.4* DT 0.12 NNS 0.12 DMV shared log normal-families 62.4† IN 0.11 DMV smoothed 61.2 (1.2) IN 0.78 EVG random init EVG left, 1 CC 0.35 53.3 (7.1) RB 0.27 smoothed-skip-val 62.1 (1.9) IN 0.18 EVG smoothed-skip-head 65.0 (5.7) L-EVG smoothed 68.8 (4.5) Table 1: Directed accuracy (DA) for WSJ10, section 23. *,† indicate results reported by Cohen et al. (2008), Cohen and Smith (2009) respectively. Standard deviations over 10 runs are given in parentheses dependencies for section 23, which were extracted from the phrase structure trees using the standard rules by Yamada and Matsumoto (2003). We measure the percent accuracy of the directed dependency edges. For the lexicalized model, we replaced all words that were seen fewer than 100 times with “UNK.” We ran each of our systems 10 times, and report the average directed accuracy achieved. The results are shown in Table 1. We compare to work by Cohen et al. (2008) and Cohen and Smith (2009). Looking at Table 1, we can first of all see the benefit of randomized initialization over the harmonic initializer for DMV. We can also see a large gain by adding smoothing to DMV, topping even the logistic normal prior. The unsmoothed EVG act"
N09-1012,J03-4003,0,\N,Missing
N09-1019,M98-1014,0,0.012749,"Missing"
N09-1019,E09-1018,1,0.512187,"e 3). Our pronoun information is derived from an unsupervised coreference algorithm which does not use named entity informa1 We stem modifiers with the Porter stemmer. ROOT →Modifiers 0 # NE 0 # Prepositions 0 # Pronouns 0 # ... Pronouns 0 →Pronoun 0 Pronouns 0 Pronouns 0 → Pronoun 0 →pers|loc|org|any pers →i |he|she|who|me . . . loc →where|which|it|its org →which|it|they|we . . . Figure 3: A fragment of the full grammar. The symbol # represents punctuation between different feature types. The prior for class 0 is concentrated around personal pronouns, although other types are possible. tion (Charniak and Elsner, 2009). This algorithm uses EM to learn a generative model with syntactic, number and gender parameters. Like Haghighi and Klein (2007), we give our model information about the basic types of pronouns in English. By setting up the base grammar so that each named-entity class prefers to associate to a single type of pronoun, we can also determine the correspondence between our named-entity symbols and the actual named-entity labels– for the models without pronoun information, this matching is arbitrary and must be inferred during the evaluation process. 3.4 Data Preparation To prepare data for cluste"
N09-1019,P05-1022,1,0.178168,"yntactic, number and gender parameters. Like Haghighi and Klein (2007), we give our model information about the basic types of pronouns in English. By setting up the base grammar so that each named-entity class prefers to associate to a single type of pronoun, we can also determine the correspondence between our named-entity symbols and the actual named-entity labels– for the models without pronoun information, this matching is arbitrary and must be inferred during the evaluation process. 3.4 Data Preparation To prepare data for clustering with our system, we first parse it with the parser of Charniak and Johnson (2005). We then annotate pronouns with Charniak and Elsner (2009). For the evaluation set, we use the named entity data from MUC-7. Here, we extract all strings in <ne> tags and determine their cores, plus any relevant modifiers, governing prepositions and pronouns, by examining the parse trees. In addition, we supply the system with additional data from the North American News Corpus (NANC). Here we extract all NPs headed by proper nouns. We then process our data by merging all examples with the same core; some merged examples from our dataset are shown in Figure 4. When two examples are merged, we"
N09-1019,N01-1007,1,0.896329,"the use of extremely large corpora which allow very precise, but sparse features. For instance Etzioni et al. (2005) and Pasca (2004) use web queries to count occurrences of “cities such as X” and similar phrases. Although our research makes use of a fairly large amount of data, our method is designed to make better use of relatively common contextual features, rather than searching for high-quality semantic features elsewhere. Models of the internal structure of names have been used for cross-document coreference (Li et al., 2004; Bhattacharya and Getoor, 2006) and a goal in their own right (Charniak, 2001). Li et al. (2004) take named entity classes as a given, and develops both generative and discriminative models to detect coreference between members of each class. Their generative model designates a particular mention of a name as a “representative” and generates all other mentions from it according to an editing process. Bhattacharya and Getoor (2006) operates only on authors of scientific papers. Their model accounts for a wider variety of name variants than ours, including misspellings and initials. In addition, they confirm our intuition that Gibbs sampling for inference has insufficient"
N09-1019,W99-0613,0,0.529894,"he i prior parameters for the entity-specific symbols Exk are fixed so that, with overwhelming probability, only one expansion occurs. We can represent any fixed number of entities Ek with a standard adaptor grammar, but since we do not know the correct number, we must extend the adaptor model slightly to allow for an unbounded number. We generate the Ek from a Chinese Restaurant process prior. (General grammars with infinite numbers of nonterminals were studied by (Liang et al., 2007b)). 3.3 Modifiers, Prepositions and Pronouns Next, we introduce two types of context information derived from Collins and Singer (1999): nominal modifiers and prepositional information. A nominal modifier is either the head of an appositive phrase (“Maury Cooper, a vice president”) or a non-proper prenominal (“spokesman John Smith”)1 . If the entity is the complement of a preposition, we extract the preposition and the head of the governing NP (“a federally funded sewage plant in Georgia”). These are added to the grammar at the named-entity class level (separated from the core by a special punctuation symbol). Finally, we add information about pronouns and wh-complementizers (Figure 3). Our pronoun information is derived from"
N09-1019,D07-1074,0,0.0335999,"Missing"
N09-1019,P07-1107,0,0.5112,"m (using no “seed rules” or initial heuristics); to our knowledge this is the best such system reported on the MUC-7 dataset. In addition, the model clusters the words which appear in named entities, discovering groups of words with similar roles such as first names and types of organization. Finally, the model defines a notion of consistency between different references to the same entity; this component of the model yields a significant increase in performance. The main motivation for our system is the recent success of unsupervised generative models for coreference resolution. The model of Haghighi and Klein (2007) incorporated a latent variable for named entity class. They report a named entity score 164 of 61.2 percent, well above the baseline of 46.4, but still far behind existing named-entity systems. We suspect that better models for named entities could aid in the coreference task. The easiest way to incorporate a better model is simply to run a supervised or semi-supervised system as a preprocess. To perform joint inference, however, requires an unsupervised generative model for named entities. As far as we know, this work is the best such model. Named entities also pose another problem with the"
N09-1019,N07-1018,1,0.413381,"ers) observed throughout a large input corpus while keeping the size of our input file small. To create an input file, we first add all the MUC7 examples. We then draw additional examples from NANC, ranking them by how many features they have, until we reach a specified number (larger datasets take longer, but without enough data, results tend to be poor). 3.5 Inference Our implementation of adaptor grammars is a modified version of the Pitman-Yor adaptor grammar sampler2 , altered to deal with the infinite number of entities. It carries out inference using a Metropoliswithin-Gibbs algorithm (Johnson et al., 2007), in which it repeatedly parses each input line using the CYK algorithm, samples a parse, and proposes this as the new tree. To do Gibbs sampling for our consistencyenforcing model, we would need to sample a parse for an example from the posterior over every possible entity. However, since there are thousands of entities (the number grows roughly linearly with the number of merged examples in the data file), this is not tractable. Instead, we perform a restricted Gibbs sampling search, where we enumerate the posterior only for entities which share a word in their core with the example in quest"
N09-1019,D07-1072,0,0.0410452,"ER,Clinton ∼ EP0 ER , which we intend to be a distribution over titles in general. The resulting grammar is shown in Figure 2; the i prior parameters for the entity-specific symbols Exk are fixed so that, with overwhelming probability, only one expansion occurs. We can represent any fixed number of entities Ek with a standard adaptor grammar, but since we do not know the correct number, we must extend the adaptor model slightly to allow for an unbounded number. We generate the Ek from a Chinese Restaurant process prior. (General grammars with infinite numbers of nonterminals were studied by (Liang et al., 2007b)). 3.3 Modifiers, Prepositions and Pronouns Next, we introduce two types of context information derived from Collins and Singer (1999): nominal modifiers and prepositional information. A nominal modifier is either the head of an appositive phrase (“Maury Cooper, a vice president”) or a non-proper prenominal (“spokesman John Smith”)1 . If the entity is the complement of a preposition, we extract the preposition and the head of the governing NP (“a federally funded sewage plant in Georgia”). These are added to the grammar at the named-entity class level (separated from the core by a special pu"
N09-1019,M98-1021,0,0.0138538,"Missing"
N09-1019,D08-1067,0,0.0360479,"d aid in the coreference task. The easiest way to incorporate a better model is simply to run a supervised or semi-supervised system as a preprocess. To perform joint inference, however, requires an unsupervised generative model for named entities. As far as we know, this work is the best such model. Named entities also pose another problem with the Haghighi and Klein (2007) coreference model; since it models only the heads of NPs, it will fail to resolve some references to named entities: (“Ford Motor Co.”, “Ford”), while erroneously merging others: (“Ford Motor Co.”, “Lockheed Martin Co.”). Ng (2008) showed that better features for matching named entities– exact string match and an “alias detector” looking for acronyms, abbreviations and name variants– improve the model’s performance substantially. Yet building an alias detector is nontrivial (Uryupina, 2004). English speakers know that “President Clinton” is the same person as “Bill Clinton” , not “President Bush”. But this cannot be implemented by simple substring matching. It requires some concept of the role of each word in the string. Our model attempts to learn this role information by clustering the words within named entities. 2 R"
N09-1019,uryupina-2004-evaluating,0,0.0143615,"we know, this work is the best such model. Named entities also pose another problem with the Haghighi and Klein (2007) coreference model; since it models only the heads of NPs, it will fail to resolve some references to named entities: (“Ford Motor Co.”, “Ford”), while erroneously merging others: (“Ford Motor Co.”, “Lockheed Martin Co.”). Ng (2008) showed that better features for matching named entities– exact string match and an “alias detector” looking for acronyms, abbreviations and name variants– improve the model’s performance substantially. Yet building an alias detector is nontrivial (Uryupina, 2004). English speakers know that “President Clinton” is the same person as “Bill Clinton” , not “President Bush”. But this cannot be implemented by simple substring matching. It requires some concept of the role of each word in the string. Our model attempts to learn this role information by clustering the words within named entities. 2 Related Work Supervised named entity recognition now performs almost as well as human annotation in English (Chinchor, 1998) and has excellent performance on other languages (Tjong Kim Sang and De Meulder, 2003). For a survey of the state of the art, Human Language"
N09-1019,M98-1004,0,\N,Missing
N09-1019,M98-1012,0,\N,Missing
N09-1019,W03-0419,0,\N,Missing
N09-1036,W08-2109,0,0.0320289,"ons, each of which expands to a sequence of Words. Sentence → Colloc+ Colloc → Word+ Word → Phoneme+ Because Colloc is adapted, the collocation adaptor grammar learns Collocations as well as Words. (Presumably these approximate syntactic, semantic and pragmatic interword dependencies). Johnson reported that the collocation adaptor grammar segments as well as the Goldwater et al. bigram model, which we confirm here. Recently other researchers have emphasised the utility of phonotactic constraints (i.e., modeling the allowable phoneme sequences at word onsets and endings) for word segmentation (Blanchard and Heinz, 2008; Fleck, 2008). Johnson (2008) points out that adaptor grammars that model words as sequences of syllables can learn and exploit these constraints, significantly improving segmentation accuracy. Here we present an adaptor grammar that models collocations together with these phonotactic constraints. This grammar is quite complex, permitting us to study the effects of the various model and im320 plementation choices described below on a complex hierarchical nonparametric Bayesian model. The collocation-syllable adaptor grammar generates a Sentence in terms of three levels of Collocations (enabli"
N09-1036,P08-1016,0,0.0331332,"to a sequence of Words. Sentence → Colloc+ Colloc → Word+ Word → Phoneme+ Because Colloc is adapted, the collocation adaptor grammar learns Collocations as well as Words. (Presumably these approximate syntactic, semantic and pragmatic interword dependencies). Johnson reported that the collocation adaptor grammar segments as well as the Goldwater et al. bigram model, which we confirm here. Recently other researchers have emphasised the utility of phonotactic constraints (i.e., modeling the allowable phoneme sequences at word onsets and endings) for word segmentation (Blanchard and Heinz, 2008; Fleck, 2008). Johnson (2008) points out that adaptor grammars that model words as sequences of syllables can learn and exploit these constraints, significantly improving segmentation accuracy. Here we present an adaptor grammar that models collocations together with these phonotactic constraints. This grammar is quite complex, permitting us to study the effects of the various model and im320 plementation choices described below on a complex hierarchical nonparametric Bayesian model. The collocation-syllable adaptor grammar generates a Sentence in terms of three levels of Collocations (enabling it to captu"
N09-1036,P06-1085,1,0.875941,"ceived considerable attention recently, in part because they can be composed in hierarchical fashion to form Hierarchical Dirichlet Processes (HDP) (Teh et al., 2006). Lexical acquisition is an ideal test-bed for exploring methods for inferring structure, where the features learned are the words of the language. (Even the most hard-core nativists agree that the words of a language must be learned). We use the unsupervised word segmentation problem as a test case for evaluating structural inference in this paper. Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008). In a computational linguistics setting it is natural to try to align the HDP hierarchy with the hierarchy defined by a grammar. Adaptor grammars, which are one way of doing this, make it easy to explore a wide variety of HDP grammar-based models. Given an appropriate adaptor grammar, the fea317 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 317–325, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tures learned by adaptor grammars can correspond to linguistic units"
N09-1036,N07-1018,1,0.652939,"lities θ, where θr is the probability of rule r ∈ R, A ⊆ N is the set of adapted nonterminals and C is a vector of adaptors indexed by elements of A, so CX is the adaptor for adapted nonterminal X ∈ A. Informally, an adaptor CX nondeterministically maps a stream of trees from a base distribution HX whose support is TX (the set of subtrees whose root node is X ∈ N generated by the grammar’s rules) into another stream of trees whose support is also TX . In adaptor grammars the base distributions HX are determined by the PCFG rules expanding X and the other adapted distributions, as explained in Johnson et al. (2007b). When called upon to generate another sample tree, the adaptor either generates and returns a fresh tree from HX or regenerates a tree it has previously emitted, so in general the adapted distribution differs from the base distribution. This paper uses adaptors based on Chinese Restaurant Processes (CRPs) or Pitman-Yor Processes (PYPs) (Pitman, 1995; Pitman and Yor, 1997; Ishwaran and James, 2003). CRPs and PYPs nondeterministically generate infinite sequences of nat319 ural numbers z1 , z2 , . . ., where z1 = 1 and each zn+1 ≤ m + 1 where m = max(z1 , . . . , zn ). In the “Chinese Restaura"
N09-1036,P08-1046,1,0.928247,"ecause they can be composed in hierarchical fashion to form Hierarchical Dirichlet Processes (HDP) (Teh et al., 2006). Lexical acquisition is an ideal test-bed for exploring methods for inferring structure, where the features learned are the words of the language. (Even the most hard-core nativists agree that the words of a language must be learned). We use the unsupervised word segmentation problem as a test case for evaluating structural inference in this paper. Nonparametric Bayesian methods produce state-of-the-art performance on this task (Goldwater et al., 2006a; Goldwater et al., 2007; Johnson, 2008). In a computational linguistics setting it is natural to try to align the HDP hierarchy with the hierarchy defined by a grammar. Adaptor grammars, which are one way of doing this, make it easy to explore a wide variety of HDP grammar-based models. Given an appropriate adaptor grammar, the fea317 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 317–325, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tures learned by adaptor grammars can correspond to linguistic units such as words, syllables and collocations"
N10-1004,P07-1056,0,0.0568525,"ll non-oracle baselines. We conclude with a discussion and future work (Section 7). 2 Related work The closest work to ours is Plank and Sima’an (2008), where unlabeled text is used to group sentences from WSJ into subdomains. The authors create a model for each subdomain which weights trees from its subdomain more highly than others. Given the domain specific models, they consider different parse combination strategies. Unfortunately, these methods do not yield a statistically significant improvement. 29 Multiple source domain adaptation has been done for other tasks (e.g. classification in (Blitzer et al., 2007; Daum´e III, 2007; Dredze and Crammer, 2008)) and is related to multitask learning. Daum´e III (2007) shows that an extremely simple method delivers solid performance on a number of domain adaptation classification tasks. This is achieved by making a copy of each feature for each source domain plus the “general” pseudodomain (for capturing domain independent features). This allows the classifier to directly model which features are domain-specific. Finkel and Manning (2009) demonstrate the hierarchical Bayesian extension of this where domain-specific models draw from a general base distributi"
N10-1004,W08-2102,0,0.0245555,"Missing"
N10-1004,P05-1022,1,0.862789,"tistics of those domains but quantitative measures of domain differences and how those differences affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations of parsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition"
N10-1004,A00-2018,1,0.822038,"et text) but not the identity of its domain. The challenge is determining how to best use the available 28 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 28–36, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics Test Train BNC GENIA BROWN SWBD ETT WSJ GENIA 66.3 81.0 70.8 72.7 82.5 83.6 71.5 62.9 65.3 74.9 64.6 86.3 75.5 75.4 83.8 51.6 79.0 89.0 75.2 78.5 69.0 80.9 75.9 81.9 83.4 66.6 80.6 69.1 73.2 89.0 BROWN SWBD ETT WSJ Average 67.0 79.9 73.9 73.9 82.0 Table 1: Cross-domain f-score performance of the Charniak (2000) parser. Averages are macro-averages. Performance drops as training and test domains diverge. On average, the WSJ model is the most accurate. resources from training to maximize accuracy across multiple target texts. Broadly put, we model how domain differences influence parsing accuracy. This is done by taking several computational measures of domain differences between the target text and each source domain. We use these features in a simple linear regression model which is trained to predict the accuracy of a parsing model (or, more generally, a mixture of parsing models) on a target text."
N10-1004,P07-1033,0,0.082164,"Missing"
N10-1004,D08-1072,0,0.0166981,"h a discussion and future work (Section 7). 2 Related work The closest work to ours is Plank and Sima’an (2008), where unlabeled text is used to group sentences from WSJ into subdomains. The authors create a model for each subdomain which weights trees from its subdomain more highly than others. Given the domain specific models, they consider different parse combination strategies. Unfortunately, these methods do not yield a statistically significant improvement. 29 Multiple source domain adaptation has been done for other tasks (e.g. classification in (Blitzer et al., 2007; Daum´e III, 2007; Dredze and Crammer, 2008)) and is related to multitask learning. Daum´e III (2007) shows that an extremely simple method delivers solid performance on a number of domain adaptation classification tasks. This is achieved by making a copy of each feature for each source domain plus the “general” pseudodomain (for capturing domain independent features). This allows the classifier to directly model which features are domain-specific. Finkel and Manning (2009) demonstrate the hierarchical Bayesian extension of this where domain-specific models draw from a general base distribution. This is applied to classification (named"
N10-1004,N09-1068,0,0.155731,"Missing"
N10-1004,foster-van-genabith-2008-parser,0,0.021802,"Missing"
N10-1004,W01-0521,0,0.460508,"non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general properties of a language’s syntax. Recently, Daum´e III (2007) and Finkel and Manning (2009) showed techniques for training models that attempt to separate domainspecific and general properties. However, even when given models for multiple training domains, it is not stra"
N10-1004,I08-2097,0,0.0846298,"domain independent features). This allows the classifier to directly model which features are domain-specific. Finkel and Manning (2009) demonstrate the hierarchical Bayesian extension of this where domain-specific models draw from a general base distribution. This is applied to classification (named entity recognition) as well as dependency parsing. These works describe how to train models in many different domains but sidestep the problem of domain detection. Thus, our work is orthogonal to theirs. Our domain detection strategy draws on work in parser accuracy prediction (Ravi et al., 2008; Kawahara and Uchimoto, 2008). These works aim to predict the parser performance on a given target sentence. Ravi et al. (2008) frame this as a regression problem. Kawahara and Uchimoto (2008) treat it as a binary classification task and predict whether a specific parse is at a certain level of accuracy or higher. Ravi et al. (2008) show that their system can be used to return a ranking over different parsing models which we extend to the multiple domain setting. They also demonstrate that training their model on WSJ allows them to accurately predict parsing accuracy on the BROWN corpus. In contrast, our models are traine"
N10-1004,J93-2004,0,0.0529407,"-trained corpora as in McClosky et al. (2006a) which have been shown to work well across domains). Our final set includes text from news (WSJ, NANC), broadcast news (ETT), literature (BROWN, GUTENBERG), biomedical (GENIA, MEDLINE), spontaneous speech (SWBD), and the British National Corpus (BNC). In our experiments, self-trained corpora cannot be used as target domains since we lack gold annotations and BNC is not used as a source domain due to its size. An overview of our corpora is shown in Table 3. We use news articles portion of the Wall Street Journal corpus (WSJ) from the Penn Treebank (Marcus et al., 1993) in conjunction with the self-trained North American News Text Corpus (NANC, Graff (1995)). The English Translation Treebank, ETT (Bies, 2007), is the translation6 of broadcast news in Arabic. For literature, we use the BROWN corpus (Francis and Kuˇcera, 1979) and the same division as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sentences which we downloaded from Project Gutenberg7 as a self-trained corpus. The Switchboard corpus (SWBD) consists of transcribed telephone conversations. While the original trees include disfluency information, we assume our spee"
N10-1004,N06-1020,1,0.915962,"quantitative measures of domain differences and how those differences affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations of parsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general pro"
N10-1004,P06-1043,1,0.961271,"quantitative measures of domain differences and how those differences affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations of parsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general pro"
N10-1004,plank-simaan-2008-subdomain,0,0.0501567,"Missing"
N10-1004,D08-1093,0,0.244766,"ain (for capturing domain independent features). This allows the classifier to directly model which features are domain-specific. Finkel and Manning (2009) demonstrate the hierarchical Bayesian extension of this where domain-specific models draw from a general base distribution. This is applied to classification (named entity recognition) as well as dependency parsing. These works describe how to train models in many different domains but sidestep the problem of domain detection. Thus, our work is orthogonal to theirs. Our domain detection strategy draws on work in parser accuracy prediction (Ravi et al., 2008; Kawahara and Uchimoto, 2008). These works aim to predict the parser performance on a given target sentence. Ravi et al. (2008) frame this as a regression problem. Kawahara and Uchimoto (2008) treat it as a binary classification task and predict whether a specific parse is at a certain level of accuracy or higher. Ravi et al. (2008) show that their system can be used to return a ranking over different parsing models which we extend to the multiple domain setting. They also demonstrate that training their model on WSJ allows them to accurately predict parsing accuracy on the BROWN corpus. In c"
N10-1004,E95-1020,0,0.0753869,"Missing"
N10-1004,A97-1015,0,0.185502,"utperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general properties of a language’s syntax. Recently, Daum´e III (2007) and Finkel and Manning (2009) showed techniques for training models that attempt to separate domainspecific and general properties. However, even when given models for multiple training domains,"
N10-1004,D09-1058,0,0.0117461,"affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations of parsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains. 1 Introduction In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain (Charniak and Johnson, 2005; McClosky et al., 2006a; Petrov and Klein, 2007; Carreras et al., 2008; Suzuki et al., 2009, among others). Unfortunately, the performance of these systems degrades on sentences drawn from a different domain. This issue can be seen across different parsing models (Sekine, 1997; Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). Given that some aspects of syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see Table 1). Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general properties of a language’s syntax. Recently, Daum´e III (2007) and Finke"
N10-1004,I05-2038,0,0.0361261,"he translation6 of broadcast news in Arabic. For literature, we use the BROWN corpus (Francis and Kuˇcera, 1979) and the same division as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sentences which we downloaded from Project Gutenberg7 as a self-trained corpus. The Switchboard corpus (SWBD) consists of transcribed telephone conversations. While the original trees include disfluency information, we assume our speech corpora have had speech repairs excised (e.g. using a system such as Johnson et al. (2004)). Our biomedical data comes from the GENIA treebank8 (Tateisi et al., 2005), a corpus of abstracts from the Medline database.9 We downloaded additional sentences 6 The transcription and translation were done by humans. http://gutenberg.org/ 8 http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/ 9 http://www.ncbi.nlm.nih.gov/PubMed/ 7 from Medline for our self-trained MEDLINE corpus. Unlike the other two self-trained corpora, we include two versions of MEDLINE. These differ on whether they were parsed using GENIA or WSJ as a base model to study the effect on cross-domain performance. Finally, we use a small number of sentences from the British National Corpus (BNC) (Foster an"
N10-1004,N07-1051,0,\N,Missing
N10-1004,J03-4003,0,\N,Missing
N10-1074,N09-1036,1,0.868481,"requently occurs in the presence of an object and not so frequently in its absence is likely to refer to that object (Frank et al., 2009a; Siskind, 1996; Yu and Ballard, 2007). Importantly, all these models assume words are pre-segmented in the input. While the word segmentation task relates less clearly to the communicative content, it can be formulated according to a similar objective, that of attempting to explain the sound sequences in the input in terms of some reasonably small set of reusable units, or words. Computational models have successfully addressed the problem in much this way (Johnson and Goldwater, 2009; Goldwater et al., 2009; Brent, 1999), and the general approach is consistent with experimental observations that humans are sensitive to statistics of sound sequences (Saffran et al., 1996; Frank et al., 2007). The two tasks can be integrated in a relatively seamless way, since, as we have just formulated them, they have a common objective, that of finding a minimal, consistent set of reusable units. However, the two deal with different types of information with 502 different dependencies. The basic idea is that learning a vocabulary that both meets the constraints of the word-learning task"
N10-1074,P08-1046,1,0.769627,"fruitful. Our model is relatively simple both in terms of word learning and in terms of word segmentation. For instance, social cues and shared attention, or discourse effects, might all play a role (Frank et al., 2009b). Shared features or other relationships can also potentially impact how quickly one might generalize a label to multiple instances (Tenenbaum and Xu, 2000). There are many ways to elaborate on the word learning task, with additional potential synergistic implications. We might also elaborate the linguistic structures we incorporate into the word learning model. For instance, Johnson (2008) explores synergies in syllable and morphological structures in word segmentation. Aspects of linguistic structure, such as morphology, may contribute to word meaning learning beyond its contribution to word segmentation performance. Acknowledgments This research was funded by NSF awards 0544127 and 0631667 to Mark Johnson and by NSF DDRIG 0746251 to Michael C. Frank. We would also like to thank Anne Fernald for providing the corpus and Maeve Cullinane for help in coding it. References Conclusion and Future Work We find (1) that it is possible to jointly infer both meanings and a segmentation"
N10-1095,P05-1022,1,0.35346,"troduction Syntactic parsing is the task of identifying the phrases and clauses in natural language sentences. It has been intensively studied primarily because it is generally believed that identifying syntactic structure is a first step towards semantic interpretation. This paper focuses on parsing the Wall Street Journal (WSJ) section of the University of Pennsylvania treebank corpus (Marcus et al., 1993). There are a large number of different approaches to this task. For simplicity we focus on two popular generative statistical parsing models: Charniak’s “Maximum Entropy Inspired” parser (Charniak and Johnson, 2005) and Petrov’s “split-merge” parser (Petrov et al., 2006). We follow conventional informal usage and refer to these as the “Brown” and the “Berkeley” parsers respectively. ∗ We would like to thank Eugene Charniak and the other members of BLLIP for their helpful advice on this work. Naturally all errors remain our own. This paper applies reranking (Collins and Koo, 2005) to the n-best output of both parsers individually, as well as to an n-best list consisting of the union of the outputs of both parsers. We are interested to see whether the same kinds of features improve the performance of both"
N10-1095,J05-1003,0,0.0975792,"eebank corpus (Marcus et al., 1993). There are a large number of different approaches to this task. For simplicity we focus on two popular generative statistical parsing models: Charniak’s “Maximum Entropy Inspired” parser (Charniak and Johnson, 2005) and Petrov’s “split-merge” parser (Petrov et al., 2006). We follow conventional informal usage and refer to these as the “Brown” and the “Berkeley” parsers respectively. ∗ We would like to thank Eugene Charniak and the other members of BLLIP for their helpful advice on this work. Naturally all errors remain our own. This paper applies reranking (Collins and Koo, 2005) to the n-best output of both parsers individually, as well as to an n-best list consisting of the union of the outputs of both parsers. We are interested to see whether the same kinds of features improve the performance of both the Berkeley and the Brown parsers, or whether successful reranking requires features that are specially tuned to the parser it is applied to. Finally, we are interested in the performance of the reranker trained on the union nbest lists. Combining the output of multiple parsers in other more complex ways has been previously demonstrated to improve overall accuracy, so"
N10-1095,W02-1001,0,0.0739438,"Missing"
N10-1095,J93-2004,0,0.0492121,"tter. An ablation experiment shows that different parsers benefit from different reranker features. 1 Ahmet Engin Ural Cognitive and Linguistic Sciences Brown University Providence, RI, USA aeural@gmail.com Introduction Syntactic parsing is the task of identifying the phrases and clauses in natural language sentences. It has been intensively studied primarily because it is generally believed that identifying syntactic structure is a first step towards semantic interpretation. This paper focuses on parsing the Wall Street Journal (WSJ) section of the University of Pennsylvania treebank corpus (Marcus et al., 1993). There are a large number of different approaches to this task. For simplicity we focus on two popular generative statistical parsing models: Charniak’s “Maximum Entropy Inspired” parser (Charniak and Johnson, 2005) and Petrov’s “split-merge” parser (Petrov et al., 2006). We follow conventional informal usage and refer to these as the “Brown” and the “Berkeley” parsers respectively. ∗ We would like to thank Eugene Charniak and the other members of BLLIP for their helpful advice on this work. Naturally all errors remain our own. This paper applies reranking (Collins and Koo, 2005) to the n-bes"
N10-1095,N06-1020,1,0.495984,"Combined Reranker features standard extended 91.6 91.7 91.8 91.6 91.8 91.9 Table 1: The f-scores on section 22 of rerankers trained on folds 1–18 by minimizing a regularized “MaxEnt” objective (negative log likelihood with a Gaussian regularizer) using L-BFGS. The weight of the regularizer was tuned to optimize f-score on folds 19–20. ity of the parses plus a constituent overlap feature), while we investigate models with millions of features here. They report a higher f-score than we do when they replace the generative Brown parser with the the self-trained discriminatively-reranked parser of McClosky et al. (2006), but with inputs provided by the generative Berkeley and Brown n-best parsers they report an f-score of 91.43 on section 23, which is consistent with the results reported here. 2 Experimental setup We ran both parsers in 50-best mode, and constructed 20-fold cross-validated training data as described in Collins and Koo (2005) and Charniak and Johnson (2005), i.e., the trees in sections 2–21 of the WSJ treebank were divided into 20 equalsized folds, and the parses for each fold were generated by a parser trained on the trees in the other folds. Then sections 22, 23 and 24 were parsed using the"
N10-1095,P06-1055,0,0.025137,"ases and clauses in natural language sentences. It has been intensively studied primarily because it is generally believed that identifying syntactic structure is a first step towards semantic interpretation. This paper focuses on parsing the Wall Street Journal (WSJ) section of the University of Pennsylvania treebank corpus (Marcus et al., 1993). There are a large number of different approaches to this task. For simplicity we focus on two popular generative statistical parsing models: Charniak’s “Maximum Entropy Inspired” parser (Charniak and Johnson, 2005) and Petrov’s “split-merge” parser (Petrov et al., 2006). We follow conventional informal usage and refer to these as the “Brown” and the “Berkeley” parsers respectively. ∗ We would like to thank Eugene Charniak and the other members of BLLIP for their helpful advice on this work. Naturally all errors remain our own. This paper applies reranking (Collins and Koo, 2005) to the n-best output of both parsers individually, as well as to an n-best list consisting of the union of the outputs of both parsers. We are interested to see whether the same kinds of features improve the performance of both the Berkeley and the Brown parsers, or whether successfu"
N10-1095,D09-1161,0,0.199622,"of both parsers. We are interested to see whether the same kinds of features improve the performance of both the Berkeley and the Brown parsers, or whether successful reranking requires features that are specially tuned to the parser it is applied to. Finally, we are interested in the performance of the reranker trained on the union nbest lists. Combining the output of multiple parsers in other more complex ways has been previously demonstrated to improve overall accuracy, so it is interesting to see if the relatively simple method used here improves parsing accuracy as well. The approach of Zhang et al. (2009) is closest to the work described here. They combine n-best lists produced by the same parsers as we do, but use only a relatively small set of features (the log probabil665 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 665–668, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics Trees Berkeley Brown Combined Reranker features standard extended 91.6 91.7 91.8 91.6 91.8 91.9 Table 1: The f-scores on section 22 of rerankers trained on folds 1–18 by minimizing a regularized “MaxEnt” objective (negative log"
N13-1019,A00-2004,0,0.730875,"coherent segments (Hearst, 1997; Beeferman et al., 1999). This task can be cast as an unsupervised machine learning problem: placing topic boundaries in unannotated text. Although a variety of cues in text can be used for topic segmentation, such as cue phases (Beeferman et al., 1999; Reynar, 1999; Eisenstein and Barzilay, 2008)) and discourse information (Galley et al., 2003), in this paper, we focus on lexical cohesion and use it as the primary cue in developing an unsupervised segmentation model. The effectiveness of lexical cohesion has been demonstrated by TextTiling (Hearst, 1997), c99 (Choi, 2000), MinCut (Malioutov and Barzilay, 2006), PLDA (Purver et al., 2006), Bayesseg (Eisenstein and Barzilay, 2008), TopicTiling (Riedl and Biemann, 2012), etc. Our work uses recent progress in hierarchical topic modelling with non-parametric Bayesian methods (Du et al., 2010; Chen et al., 2011; Du et al., 2012a), and is based on Bayesian segmentation methods (Goldwater et al., 2009; Purver et al., 2006; Eisenstein and Barzilay, 2008) using topic models. This can also be viewed as a multi-topic extension of hierarchical Bayesian segmentation (Eisenstein, 2009), although our use of hierarchies is use"
N13-1019,D12-1049,1,0.922951,"senstein and Barzilay, 2008)) and discourse information (Galley et al., 2003), in this paper, we focus on lexical cohesion and use it as the primary cue in developing an unsupervised segmentation model. The effectiveness of lexical cohesion has been demonstrated by TextTiling (Hearst, 1997), c99 (Choi, 2000), MinCut (Malioutov and Barzilay, 2006), PLDA (Purver et al., 2006), Bayesseg (Eisenstein and Barzilay, 2008), TopicTiling (Riedl and Biemann, 2012), etc. Our work uses recent progress in hierarchical topic modelling with non-parametric Bayesian methods (Du et al., 2010; Chen et al., 2011; Du et al., 2012a), and is based on Bayesian segmentation methods (Goldwater et al., 2009; Purver et al., 2006; Eisenstein and Barzilay, 2008) using topic models. This can also be viewed as a multi-topic extension of hierarchical Bayesian segmentation (Eisenstein, 2009), although our use of hierarchies is used to improve the performance of linear segmentation, rather than develop hierarchical segmentation. Recently, topic models are increasingly used in various text analysis tasks including topic segmentation. Previous work (Purver et al., 2006; Misra et al., 2008; Sun et al., 2008; Misra et al., 2009; Riedl"
N13-1019,D08-1035,0,0.903506,"erimental results show that our model outperforms previous unsupervised segmentation methods using only lexical information on Choi’s datasets and two meeting transcripts and has performance comparable to those previous methods on two written datasets. 1 Introduction Documents are usually comprised of topically coherent text segments, each of which contains some number of text passages (e.g., sentences or paragraphs) (Salton et al., 1996). Within each topically coherent segment, one would expect that the word usage demonstrates more consistent lexical distributions (known as lexical cohesion (Eisenstein and Barzilay, 2008)) than that across segments. A linear partition of texts into topic segments may reveal information about, for example, themes of segments and the overall thematic structure of the text, and can subsequently be useful for text analysis tasks, such as information retrieval (e.g., passage retrieval (Salton et al., 1996)), document summarisation and discourse analysis (Galley et al., 2003). In this paper we consider how to automatically find a topic segmentation. It involves identifying the most prominent topic changes in a sequence of text passages, and splits those passages into a sequence of t"
N13-1019,N09-1040,0,0.190601,"monstrated by TextTiling (Hearst, 1997), c99 (Choi, 2000), MinCut (Malioutov and Barzilay, 2006), PLDA (Purver et al., 2006), Bayesseg (Eisenstein and Barzilay, 2008), TopicTiling (Riedl and Biemann, 2012), etc. Our work uses recent progress in hierarchical topic modelling with non-parametric Bayesian methods (Du et al., 2010; Chen et al., 2011; Du et al., 2012a), and is based on Bayesian segmentation methods (Goldwater et al., 2009; Purver et al., 2006; Eisenstein and Barzilay, 2008) using topic models. This can also be viewed as a multi-topic extension of hierarchical Bayesian segmentation (Eisenstein, 2009), although our use of hierarchies is used to improve the performance of linear segmentation, rather than develop hierarchical segmentation. Recently, topic models are increasingly used in various text analysis tasks including topic segmentation. Previous work (Purver et al., 2006; Misra et al., 2008; Sun et al., 2008; Misra et al., 2009; Riedl and Biemann, 2012) has shown that using 190 Proceedings of NAACL-HLT 2013, pages 190–200, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics topic assignments or topic distributions instead of word frequency can significan"
N13-1019,P03-1071,0,0.511168,"tences or paragraphs) (Salton et al., 1996). Within each topically coherent segment, one would expect that the word usage demonstrates more consistent lexical distributions (known as lexical cohesion (Eisenstein and Barzilay, 2008)) than that across segments. A linear partition of texts into topic segments may reveal information about, for example, themes of segments and the overall thematic structure of the text, and can subsequently be useful for text analysis tasks, such as information retrieval (e.g., passage retrieval (Salton et al., 1996)), document summarisation and discourse analysis (Galley et al., 2003). In this paper we consider how to automatically find a topic segmentation. It involves identifying the most prominent topic changes in a sequence of text passages, and splits those passages into a sequence of topically coherent segments (Hearst, 1997; Beeferman et al., 1999). This task can be cast as an unsupervised machine learning problem: placing topic boundaries in unannotated text. Although a variety of cues in text can be used for topic segmentation, such as cue phases (Beeferman et al., 1999; Reynar, 1999; Eisenstein and Barzilay, 2008)) and discourse information (Galley et al., 2003),"
N13-1019,J97-1003,0,0.987894,"A linear partition of texts into topic segments may reveal information about, for example, themes of segments and the overall thematic structure of the text, and can subsequently be useful for text analysis tasks, such as information retrieval (e.g., passage retrieval (Salton et al., 1996)), document summarisation and discourse analysis (Galley et al., 2003). In this paper we consider how to automatically find a topic segmentation. It involves identifying the most prominent topic changes in a sequence of text passages, and splits those passages into a sequence of topically coherent segments (Hearst, 1997; Beeferman et al., 1999). This task can be cast as an unsupervised machine learning problem: placing topic boundaries in unannotated text. Although a variety of cues in text can be used for topic segmentation, such as cue phases (Beeferman et al., 1999; Reynar, 1999; Eisenstein and Barzilay, 2008)) and discourse information (Galley et al., 2003), in this paper, we focus on lexical cohesion and use it as the primary cue in developing an unsupervised segmentation model. The effectiveness of lexical cohesion has been demonstrated by TextTiling (Hearst, 1997), c99 (Choi, 2000), MinCut (Malioutov"
N13-1019,D10-1038,0,0.0493605,"Missing"
N13-1019,D11-1026,0,0.425019,"g., TF or TF-IDF. Work following this line includes TextTiling (Hearst, 1997), which calculates the cosine similarity between two adjacent blocks of words purely based on the word frequency; C99 (Choi, 2000), an algorithm based on divisive clustering with a matrix-ranking scheme; LSeg (Galley et al., 2003), which uses a lexical chain to identify and weight word repetitions; U00 (Utiyama and Isahara, 2001), a probalistic approach using dynamic programming to find a segmentation with a minimum cost; MinCut (Malioutov and Barzilay, 2006), which casts segmentation as a graph cut problem, and APS (Kazantseva and Szpakowicz, 2011), which uses affinity propagation to learn clustering for segmentation. The other branch of this work characterises the lexical cohesion using topic models, to which the model introduced in Section 3 belongs. Lexical cohesion in this line of research is modelled by a probabilistic generative process. PLDA presented by Purver et al. (2006) is an unsupervised topic modelling approach for segmentation. It chains a set of LDAs (Blei et al., 2003) by assuming a Markov structure on topic distributions. A binary topic shift variable is attached to each text passage (i.e., an utterance in (Purver et a"
N13-1019,P06-1004,0,0.814321,"arst, 1997; Beeferman et al., 1999). This task can be cast as an unsupervised machine learning problem: placing topic boundaries in unannotated text. Although a variety of cues in text can be used for topic segmentation, such as cue phases (Beeferman et al., 1999; Reynar, 1999; Eisenstein and Barzilay, 2008)) and discourse information (Galley et al., 2003), in this paper, we focus on lexical cohesion and use it as the primary cue in developing an unsupervised segmentation model. The effectiveness of lexical cohesion has been demonstrated by TextTiling (Hearst, 1997), c99 (Choi, 2000), MinCut (Malioutov and Barzilay, 2006), PLDA (Purver et al., 2006), Bayesseg (Eisenstein and Barzilay, 2008), TopicTiling (Riedl and Biemann, 2012), etc. Our work uses recent progress in hierarchical topic modelling with non-parametric Bayesian methods (Du et al., 2010; Chen et al., 2011; Du et al., 2012a), and is based on Bayesian segmentation methods (Goldwater et al., 2009; Purver et al., 2006; Eisenstein and Barzilay, 2008) using topic models. This can also be viewed as a multi-topic extension of hierarchical Bayesian segmentation (Eisenstein, 2009), although our use of hierarchies is used to improve the performance of linear"
N13-1019,W08-2106,0,0.0216952,"ian methods (Du et al., 2010; Chen et al., 2011; Du et al., 2012a), and is based on Bayesian segmentation methods (Goldwater et al., 2009; Purver et al., 2006; Eisenstein and Barzilay, 2008) using topic models. This can also be viewed as a multi-topic extension of hierarchical Bayesian segmentation (Eisenstein, 2009), although our use of hierarchies is used to improve the performance of linear segmentation, rather than develop hierarchical segmentation. Recently, topic models are increasingly used in various text analysis tasks including topic segmentation. Previous work (Purver et al., 2006; Misra et al., 2008; Sun et al., 2008; Misra et al., 2009; Riedl and Biemann, 2012) has shown that using 190 Proceedings of NAACL-HLT 2013, pages 190–200, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics topic assignments or topic distributions instead of word frequency can significantly improve segmentation performance. Here we consider more advanced topic models that model dependencies between (sub-)sections in a document, such as structured topic models (STMs) presented in (Du et al., 2010; Du et al., 2012b). STMs treat each text as a sequence of segments, each of which is a"
N13-1019,P12-1009,0,0.24857,"the model introduced in Section 3 belongs. Lexical cohesion in this line of research is modelled by a probabilistic generative process. PLDA presented by Purver et al. (2006) is an unsupervised topic modelling approach for segmentation. It chains a set of LDAs (Blei et al., 2003) by assuming a Markov structure on topic distributions. A binary topic shift variable is attached to each text passage (i.e., an utterance in (Purver et al., 2006)). It is sampled to indicate whether the j th text passage shares the topic distribution with the (j − 1)th passage. Using a similar Markov structure, SITS (Nguyen et al., 2012) chains a set of HDP-LDAs (Teh et al., 2006). Unlike PLDA, SITS assumes each text passage is associated with a speaker identity that is attached to the topic shift variable as supervising information. SITS further assumes speakers have different topic change probabilities that work as priors on topic shift variables. Instead of assuming documents in a dataset share the same set of topics, Bayesseg (Eisenstein and Barzilay, 2008) treats words in a segment generated from a segment specific multinomial language model, i.e., it assumes each segment is generated from one topic, and a later hierarch"
N13-1019,J02-1002,0,0.392579,"ee different kinds of corpora4 : a set of synthetic documents, two meeting transcripts and two sets of text books (see Tables 2 and 3); and compare TSM with the following methods: two baselines (the Random algorithm that places topic boundaries uniformly at random, and the Even algorithm that places a boundary after every mth text passage, where m is the average gold-standard segment length (Beeferman et al., 1999)), C99, MinCut, Bayesseg, APS (Kazantseva and Szpakowicz, 2011), and PLDA. Metrics: We evaluated the segmentation performance with PK (Beeferman et al., 1999) and WindowDiff (WDr ) (Pevzner and Hearst, 2002), which are two common metrics used in topic segmentation. Both move a sliding window of fixed size k over the document, and compare the inferred segmentation with the gold-standard segmentation for each window. The window size is usually set to the half of the average gold-standard segment size (Pevzner and Hearst, 2002). In addition, we also used an extended WindowDiff proposed by Lamprier et al. (2007), denoted by WDe . One problem of WDr is that errors near the two ends of a text are penalised less than those in the middle. To solve the problem WDe adds k fictive text passages at the begin"
N13-1019,P06-1003,0,0.720709,"Missing"
N13-1019,P99-1046,0,0.465083,"al (Salton et al., 1996)), document summarisation and discourse analysis (Galley et al., 2003). In this paper we consider how to automatically find a topic segmentation. It involves identifying the most prominent topic changes in a sequence of text passages, and splits those passages into a sequence of topically coherent segments (Hearst, 1997; Beeferman et al., 1999). This task can be cast as an unsupervised machine learning problem: placing topic boundaries in unannotated text. Although a variety of cues in text can be used for topic segmentation, such as cue phases (Beeferman et al., 1999; Reynar, 1999; Eisenstein and Barzilay, 2008)) and discourse information (Galley et al., 2003), in this paper, we focus on lexical cohesion and use it as the primary cue in developing an unsupervised segmentation model. The effectiveness of lexical cohesion has been demonstrated by TextTiling (Hearst, 1997), c99 (Choi, 2000), MinCut (Malioutov and Barzilay, 2006), PLDA (Purver et al., 2006), Bayesseg (Eisenstein and Barzilay, 2008), TopicTiling (Riedl and Biemann, 2012), etc. Our work uses recent progress in hierarchical topic modelling with non-parametric Bayesian methods (Du et al., 2010; Chen et al., 20"
N13-1019,N12-1064,0,0.110344,"pic boundaries in unannotated text. Although a variety of cues in text can be used for topic segmentation, such as cue phases (Beeferman et al., 1999; Reynar, 1999; Eisenstein and Barzilay, 2008)) and discourse information (Galley et al., 2003), in this paper, we focus on lexical cohesion and use it as the primary cue in developing an unsupervised segmentation model. The effectiveness of lexical cohesion has been demonstrated by TextTiling (Hearst, 1997), c99 (Choi, 2000), MinCut (Malioutov and Barzilay, 2006), PLDA (Purver et al., 2006), Bayesseg (Eisenstein and Barzilay, 2008), TopicTiling (Riedl and Biemann, 2012), etc. Our work uses recent progress in hierarchical topic modelling with non-parametric Bayesian methods (Du et al., 2010; Chen et al., 2011; Du et al., 2012a), and is based on Bayesian segmentation methods (Goldwater et al., 2009; Purver et al., 2006; Eisenstein and Barzilay, 2008) using topic models. This can also be viewed as a multi-topic extension of hierarchical Bayesian segmentation (Eisenstein, 2009), although our use of hierarchies is used to improve the performance of linear segmentation, rather than develop hierarchical segmentation. Recently, topic models are increasingly used in"
N13-1019,P08-2068,0,0.0185486,"l., 2010; Chen et al., 2011; Du et al., 2012a), and is based on Bayesian segmentation methods (Goldwater et al., 2009; Purver et al., 2006; Eisenstein and Barzilay, 2008) using topic models. This can also be viewed as a multi-topic extension of hierarchical Bayesian segmentation (Eisenstein, 2009), although our use of hierarchies is used to improve the performance of linear segmentation, rather than develop hierarchical segmentation. Recently, topic models are increasingly used in various text analysis tasks including topic segmentation. Previous work (Purver et al., 2006; Misra et al., 2008; Sun et al., 2008; Misra et al., 2009; Riedl and Biemann, 2012) has shown that using 190 Proceedings of NAACL-HLT 2013, pages 190–200, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics topic assignments or topic distributions instead of word frequency can significantly improve segmentation performance. Here we consider more advanced topic models that model dependencies between (sub-)sections in a document, such as structured topic models (STMs) presented in (Du et al., 2010; Du et al., 2012b). STMs treat each text as a sequence of segments, each of which is a set of text passag"
N13-1019,P01-1064,0,0.449664,"of text based on lexical cohesion. It can be characterised by how lexical cohesion is modelled. One branch of this work represents the lexical cohesion in a vector space by exploring the word cooccurrence patterns, e.g., TF or TF-IDF. Work following this line includes TextTiling (Hearst, 1997), which calculates the cosine similarity between two adjacent blocks of words purely based on the word frequency; C99 (Choi, 2000), an algorithm based on divisive clustering with a matrix-ranking scheme; LSeg (Galley et al., 2003), which uses a lexical chain to identify and weight word repetitions; U00 (Utiyama and Isahara, 2001), a probalistic approach using dynamic programming to find a segmentation with a minimum cost; MinCut (Malioutov and Barzilay, 2006), which casts segmentation as a graph cut problem, and APS (Kazantseva and Szpakowicz, 2011), which uses affinity propagation to learn clustering for segmentation. The other branch of this work characterises the lexical cohesion using topic models, to which the model introduced in Section 3 belongs. Lexical cohesion in this line of research is modelled by a probabilistic generative process. PLDA presented by Purver et al. (2006) is an unsupervised topic modelling"
N13-1019,P11-1153,0,0.0536853,"s analysing multiparty meeting transcripts, where speaker identities are available, we are interested in more general texts and assume each text has a specific topic change probability, since (1) the identity information is not always available for all kinds of texts (e.g., continuous broadcast news transcripts (Allan et al., 1998)), (2) even for the same author, topic change probabilities for his/her different articles might be different. 3 Segmentation with Topic Models In documents, topically coherent segments usually encapsulate a set of consecutive passages that are semantically related (Wang et al., 2011). However, the topic boundaries between segments are often unavailable a priori. Thus we treat all passage boundaries (e.g., sentence boundaries, paragraph boundaries or pauses between utterances) as possible topic boundaries. To recover the topic boundaries we develop a structured topic segmentation model by integrating ideas from the segmented topic model (Du et al., 2010, STM) and Bayesian segmentation models. The basic idea of our model is that each document consists of a set of segments where text passages in the same segment are generated from the same topic distribution, called segment"
N15-1006,P13-2107,1,0.847568,"nologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 53–63, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics John likes mangoes NP (SNP)/NP NP from India madly (NPNP)/NP NP (SNP)(SNP) NPNP NP SNP > &lt; > SNP S &lt; &lt; Figure 1: Normal form CCG derivation. of incremental CCG derivations and can train on the dependencies in the existing treebank. Our approach can therefore be adapted to other languages with dependency treebanks, since CCG lexical categories can be easily extracted from dependency treebanks (Cakici, 2005; Ambati et al., 2013). The rest of the paper is arranged as follows. Section 2 gives a brief introduction to related work in the areas of CCG parsing and incremental parsing. In section 3, we describe our incremental shift-reduce parsing algorithm. Details about the experiments, evaluation metrices and analysis of the results are in section 4. We conclude with possible future directions in section 5. 2 Related Work In this section, we first give a brief introduction to various available CCG parsers. Then we describe approaches towards incremental and greedy parsing. 2.1 CCG Parsers There has been a significant amo"
N15-1006,P11-1048,0,0.652721,"we describe our incremental shift-reduce parsing algorithm. Details about the experiments, evaluation metrices and analysis of the results are in section 4. We conclude with possible future directions in section 5. 2 Related Work In this section, we first give a brief introduction to various available CCG parsers. Then we describe approaches towards incremental and greedy parsing. 2.1 CCG Parsers There has been a significant amount of work on developing chart-based parsers for CCG. Both generative (Hockenmaier and Steedman, 2002) and discriminative (Clark et al., 2002; Clark and Curran, 2007; Auli and Lopez, 2011; Lewis and Steedman, 2014) models have been developed. As these parsers employ a bottom-up chart-parsing strategy and use normal-form CCGbank derivations which are rightbranching, they are not incremental in nature. In an SVO (Subject-Verb-Object) language, these parsers first attach the object to the verb and then the subject. Two major works in shift-reduce CCG parsing with accuracies competitive with the widely used Clark and Curran (2007) parser (C&C) are Zhang and Clark (2011) and Xu et al. (2014). Zhang and Clark (2011) used a global linear model trained discriminatively with the averag"
N15-1006,P05-2013,0,0.0427919,"Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 53–63, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics John likes mangoes NP (SNP)/NP NP from India madly (NPNP)/NP NP (SNP)(SNP) NPNP NP SNP > &lt; > SNP S &lt; &lt; Figure 1: Normal form CCG derivation. of incremental CCG derivations and can train on the dependencies in the existing treebank. Our approach can therefore be adapted to other languages with dependency treebanks, since CCG lexical categories can be easily extracted from dependency treebanks (Cakici, 2005; Ambati et al., 2013). The rest of the paper is arranged as follows. Section 2 gives a brief introduction to related work in the areas of CCG parsing and incremental parsing. In section 3, we describe our incremental shift-reduce parsing algorithm. Details about the experiments, evaluation metrices and analysis of the results are in section 4. We conclude with possible future directions in section 5. 2 Related Work In this section, we first give a brief introduction to various available CCG parsers. Then we describe approaches towards incremental and greedy parsing. 2.1 CCG Parsers There has"
N15-1006,C04-1041,0,0.229815,"Missing"
N15-1006,J07-4004,0,0.836905,"achine translation (SMT) and automatic speech recognition (ASR) (Roark, 2001; Wang and Harper, 2003). Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2000) is an efficiently parseable, yet linguistically expressive grammar formalism. In addition to predicate-argument structure, CCG elegantly captures the unbounded dependencies found in grammatical constructions like relativization, coordination etc. Availability of the English CCGbank (Hockenmaier and Steedman, 2007) has enabled the creation of several robust and accurate wide-coverage CCG parsers (Hockenmaier and Steedman, 2002; Clark and Curran, 2007; Zhang and Clark, 2011). While the majority of CCG parsers use chart-based approaches (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), there has been some work on developing shift-reduce This paper develops a new incremental shiftreduce algorithm for parsing CCG by building a dependency graph in addition to the CCG derivation as a representation. The dependencies in the graph are extracted from the CCG derivation. A node can have multiple parents, and hence we construct a dependency graph rather than a tree. Two new actions are introduced in the shift-reduce paradigm for “revealing”"
N15-1006,P02-1042,1,0.911926,"sing and incremental parsing. In section 3, we describe our incremental shift-reduce parsing algorithm. Details about the experiments, evaluation metrices and analysis of the results are in section 4. We conclude with possible future directions in section 5. 2 Related Work In this section, we first give a brief introduction to various available CCG parsers. Then we describe approaches towards incremental and greedy parsing. 2.1 CCG Parsers There has been a significant amount of work on developing chart-based parsers for CCG. Both generative (Hockenmaier and Steedman, 2002) and discriminative (Clark et al., 2002; Clark and Curran, 2007; Auli and Lopez, 2011; Lewis and Steedman, 2014) models have been developed. As these parsers employ a bottom-up chart-parsing strategy and use normal-form CCGbank derivations which are rightbranching, they are not incremental in nature. In an SVO (Subject-Verb-Object) language, these parsers first attach the object to the verb and then the subject. Two major works in shift-reduce CCG parsing with accuracies competitive with the widely used Clark and Curran (2007) parser (C&C) are Zhang and Clark (2011) and Xu et al. (2014). Zhang and Clark (2011) used a global linear"
N15-1006,P04-1015,0,0.295872,"ither on CCGbank derivations (Zhang and Clark, 2011) which are non-incremental, or on dependencies (Xu et al., 2014) which could be incremental in simple cases, but do not guarantee incrementality. Hassan et al. (2009) developed a semi-incremental CCG parser by transforming the English CCGbank into left branching derivation trees. The strictly incremental version performed with very low accuracy but a semi-incremental version gave a balance between incrementality and accuracy. There is also some work on incremental parsing using grammar formalisms other than CCG like phrase structure grammar (Collins and Roark, 2004) and tree substitution grammar (Sangati and Keller, 2013). 2.3 Greedy Parsers There has been a significant amount of work on greedy shift-reduce dependency parsing. The Malt parser (Nivre et al., 2007) is one of the earliest parsers based on this paradigm. Goldberg and Nivre (2012) improved learning for greedy parsers by using dynamic oracles rather than a single static transition sequence as the oracle. In all the standard shift-reduce parsers, when two trees combine, only the top node (root) of each tree participates in the action. Sartorio et al. (2013) introduced a technique where in addit"
N15-1006,W02-1001,0,0.336296,"edman, 2014) models have been developed. As these parsers employ a bottom-up chart-parsing strategy and use normal-form CCGbank derivations which are rightbranching, they are not incremental in nature. In an SVO (Subject-Verb-Object) language, these parsers first attach the object to the verb and then the subject. Two major works in shift-reduce CCG parsing with accuracies competitive with the widely used Clark and Curran (2007) parser (C&C) are Zhang and Clark (2011) and Xu et al. (2014). Zhang and Clark (2011) used a global linear model trained discriminatively with the averaged perceptron (Collins, 2002) and beam search for their shiftreduce CCG parser. Xu et al. (2014) developed a 54 dependency model for shift-reduce CCG parsing using a dynamic oracle technique. Unlike the chart parsers, both these parsers can produce fragmentary analyses when a complete spanning analysis is not found. Both these shift-reduce parsers are more incremental than standard chart based parsers. But, as they employ an arc-standard (Yamada and Matsumoto, 2003) shift-reduce strategy on CCGbank, given an SVO language, these parsers are not guaranteed to attach the subject before the object. 2.2 Incremental Parsers A s"
N15-1006,P96-1011,0,0.41504,"Missing"
N15-1006,C12-1059,0,0.0497734,"glish CCGbank into left branching derivation trees. The strictly incremental version performed with very low accuracy but a semi-incremental version gave a balance between incrementality and accuracy. There is also some work on incremental parsing using grammar formalisms other than CCG like phrase structure grammar (Collins and Roark, 2004) and tree substitution grammar (Sangati and Keller, 2013). 2.3 Greedy Parsers There has been a significant amount of work on greedy shift-reduce dependency parsing. The Malt parser (Nivre et al., 2007) is one of the earliest parsers based on this paradigm. Goldberg and Nivre (2012) improved learning for greedy parsers by using dynamic oracles rather than a single static transition sequence as the oracle. In all the standard shift-reduce parsers, when two trees combine, only the top node (root) of each tree participates in the action. Sartorio et al. (2013) introduced a technique where in addition to the root node, nodes on the right and left periphery respectively are also available for attachment in the parsing process. A non-monotonic parsing strategy was introduced by Honnibal et al. (2013), where an action taken during the parsing process is revised based on future"
N15-1006,R09-1025,0,0.0425191,"Missing"
N15-1006,P02-1043,1,0.884997,"guage modeling for statistical machine translation (SMT) and automatic speech recognition (ASR) (Roark, 2001; Wang and Harper, 2003). Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2000) is an efficiently parseable, yet linguistically expressive grammar formalism. In addition to predicate-argument structure, CCG elegantly captures the unbounded dependencies found in grammatical constructions like relativization, coordination etc. Availability of the English CCGbank (Hockenmaier and Steedman, 2007) has enabled the creation of several robust and accurate wide-coverage CCG parsers (Hockenmaier and Steedman, 2002; Clark and Curran, 2007; Zhang and Clark, 2011). While the majority of CCG parsers use chart-based approaches (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), there has been some work on developing shift-reduce This paper develops a new incremental shiftreduce algorithm for parsing CCG by building a dependency graph in addition to the CCG derivation as a representation. The dependencies in the graph are extracted from the CCG derivation. A node can have multiple parents, and hence we construct a dependency graph rather than a tree. Two new actions are introduced in the shift-reduce p"
N15-1006,J07-3004,1,0.944505,"Missing"
N15-1006,W13-3518,1,0.854971,"Nivre et al., 2007) is one of the earliest parsers based on this paradigm. Goldberg and Nivre (2012) improved learning for greedy parsers by using dynamic oracles rather than a single static transition sequence as the oracle. In all the standard shift-reduce parsers, when two trees combine, only the top node (root) of each tree participates in the action. Sartorio et al. (2013) introduced a technique where in addition to the root node, nodes on the right and left periphery respectively are also available for attachment in the parsing process. A non-monotonic parsing strategy was introduced by Honnibal et al. (2013), where an action taken during the parsing process is revised based on future context. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) S S S S S RR RR RR S RR RL [ [ [ [ [ [ [ [ [ [ [ NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn Slikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes SNPlikes SNPlikes SNPlikes NPmangoes NPmangoes NPmangoes NPmangoes NPmangoes (NPNP)/NPf rom (NPNP)/NPf rom NPNPf rom (SNP)(SNP)madly NPIndia John likes (10) (8) mangoes madly (11) (7) from (6) India Figure 2: NonInc - Sequence of actions with"
N15-1006,D10-1119,1,0.856898,"gramming language, then while we usually think of CCG combinatory rules like the following as applying with the two categories on the left X/Y and Y as inputs, say instantiated as S /NP and NP , to define the category X on the right as S, in fact instantiating any two of those categories defines the third. X/Y Y =⇒ X 56 For example, if we define X and X/Y as S and S /NP , we clearly define Y as NP . They proposed to use unification-based revealing to recover unbuilt constituents in from the result of overlygreedy incremental parsing. A related secondorder matching-based mechanism was used by (Kwiatkowski et al., 2010) to decompose logical forms for semantic parser induction. The present incremental parser uses a related revealing technique confined to the right periphery. Using CCG combinators and rules like type-raising followed by forward composition, we combine nodes in the stack if there is a dependency between them. However, this can create problems for the newly shifted node as its dependent might already have been reduced. For instance, if the object ‘mangoes’ is reduced after it is shifted to the stack, then it won’t be available for the preposition phrase (PP) ‘from India’ (of course, this goes fo"
N15-1006,D14-1107,1,0.844119,"ental shift-reduce parsing algorithm. Details about the experiments, evaluation metrices and analysis of the results are in section 4. We conclude with possible future directions in section 5. 2 Related Work In this section, we first give a brief introduction to various available CCG parsers. Then we describe approaches towards incremental and greedy parsing. 2.1 CCG Parsers There has been a significant amount of work on developing chart-based parsers for CCG. Both generative (Hockenmaier and Steedman, 2002) and discriminative (Clark et al., 2002; Clark and Curran, 2007; Auli and Lopez, 2011; Lewis and Steedman, 2014) models have been developed. As these parsers employ a bottom-up chart-parsing strategy and use normal-form CCGbank derivations which are rightbranching, they are not incremental in nature. In an SVO (Subject-Verb-Object) language, these parsers first attach the object to the verb and then the subject. Two major works in shift-reduce CCG parsing with accuracies competitive with the widely used Clark and Curran (2007) parser (C&C) are Zhang and Clark (2011) and Xu et al. (2014). Zhang and Clark (2011) used a global linear model trained discriminatively with the averaged perceptron (Collins, 200"
N15-1006,P87-1012,1,0.722198,"ce. Figure 2 also shows the dependency graph generated and the arc labels give the step ID after which the dependency is generated. 3.2 Revealing based Incremental Algorithm (RevInc) The NonInc algorithm described above is not incremental because it relies purely on the mostly rightbranching CCG derivation. In our example sentence, the verb (likes) combines with the subject (John) only at the end (step ID = 11) after all the remaining words in the sentence are processed, making the parse non-incremental. In this section we describe a new incremental algorithm based on a ‘revealing’ technique (Pareschi and Steedman, 1987) which tries to build the most incremental derivation. 3.2.1 Revealing Pareschi and Steedman (1987)’s original version of revealing was defined in terms of (implicitly higher-order) unification. It was based on the following observation. If we think of categories as terms in a logic programming language, then while we usually think of CCG combinatory rules like the following as applying with the two categories on the left X/Y and Y as inputs, say instantiated as S /NP and NP , to define the category X on the right as S, in fact instantiating any two of those categories defines the third. X/Y Y"
N15-1006,J01-2004,0,0.585174,"Missing"
N15-1006,Q13-1010,0,0.026186,"ch are non-incremental, or on dependencies (Xu et al., 2014) which could be incremental in simple cases, but do not guarantee incrementality. Hassan et al. (2009) developed a semi-incremental CCG parser by transforming the English CCGbank into left branching derivation trees. The strictly incremental version performed with very low accuracy but a semi-incremental version gave a balance between incrementality and accuracy. There is also some work on incremental parsing using grammar formalisms other than CCG like phrase structure grammar (Collins and Roark, 2004) and tree substitution grammar (Sangati and Keller, 2013). 2.3 Greedy Parsers There has been a significant amount of work on greedy shift-reduce dependency parsing. The Malt parser (Nivre et al., 2007) is one of the earliest parsers based on this paradigm. Goldberg and Nivre (2012) improved learning for greedy parsers by using dynamic oracles rather than a single static transition sequence as the oracle. In all the standard shift-reduce parsers, when two trees combine, only the top node (root) of each tree participates in the action. Sartorio et al. (2013) introduced a technique where in addition to the root node, nodes on the right and left periphe"
N15-1006,P13-1014,0,0.0707343,"CCG like phrase structure grammar (Collins and Roark, 2004) and tree substitution grammar (Sangati and Keller, 2013). 2.3 Greedy Parsers There has been a significant amount of work on greedy shift-reduce dependency parsing. The Malt parser (Nivre et al., 2007) is one of the earliest parsers based on this paradigm. Goldberg and Nivre (2012) improved learning for greedy parsers by using dynamic oracles rather than a single static transition sequence as the oracle. In all the standard shift-reduce parsers, when two trees combine, only the top node (root) of each tree participates in the action. Sartorio et al. (2013) introduced a technique where in addition to the root node, nodes on the right and left periphery respectively are also available for attachment in the parsing process. A non-monotonic parsing strategy was introduced by Honnibal et al. (2013), where an action taken during the parsing process is revised based on future context. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) S S S S S RR RR RR S RR RL [ [ [ [ [ [ [ [ [ [ [ NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn Slikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes SNPlikes"
N15-1006,P14-1021,0,0.384084,"ier and Steedman, 2002) and discriminative (Clark et al., 2002; Clark and Curran, 2007; Auli and Lopez, 2011; Lewis and Steedman, 2014) models have been developed. As these parsers employ a bottom-up chart-parsing strategy and use normal-form CCGbank derivations which are rightbranching, they are not incremental in nature. In an SVO (Subject-Verb-Object) language, these parsers first attach the object to the verb and then the subject. Two major works in shift-reduce CCG parsing with accuracies competitive with the widely used Clark and Curran (2007) parser (C&C) are Zhang and Clark (2011) and Xu et al. (2014). Zhang and Clark (2011) used a global linear model trained discriminatively with the averaged perceptron (Collins, 2002) and beam search for their shiftreduce CCG parser. Xu et al. (2014) developed a 54 dependency model for shift-reduce CCG parsing using a dynamic oracle technique. Unlike the chart parsers, both these parsers can produce fragmentary analyses when a complete spanning analysis is not found. Both these shift-reduce parsers are more incremental than standard chart based parsers. But, as they employ an arc-standard (Yamada and Matsumoto, 2003) shift-reduce strategy on CCGbank, giv"
N15-1006,W03-3023,0,0.0828955,"007) parser (C&C) are Zhang and Clark (2011) and Xu et al. (2014). Zhang and Clark (2011) used a global linear model trained discriminatively with the averaged perceptron (Collins, 2002) and beam search for their shiftreduce CCG parser. Xu et al. (2014) developed a 54 dependency model for shift-reduce CCG parsing using a dynamic oracle technique. Unlike the chart parsers, both these parsers can produce fragmentary analyses when a complete spanning analysis is not found. Both these shift-reduce parsers are more incremental than standard chart based parsers. But, as they employ an arc-standard (Yamada and Matsumoto, 2003) shift-reduce strategy on CCGbank, given an SVO language, these parsers are not guaranteed to attach the subject before the object. 2.2 Incremental Parsers A strictly incremental parser is one which computes the relationship between words as soon as they are encountered in the input. Shift-reduce CCG parsers rely either on CCGbank derivations (Zhang and Clark, 2011) which are non-incremental, or on dependencies (Xu et al., 2014) which could be incremental in simple cases, but do not guarantee incrementality. Hassan et al. (2009) developed a semi-incremental CCG parser by transforming the Engli"
N15-1006,P11-1069,0,0.488319,"and automatic speech recognition (ASR) (Roark, 2001; Wang and Harper, 2003). Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2000) is an efficiently parseable, yet linguistically expressive grammar formalism. In addition to predicate-argument structure, CCG elegantly captures the unbounded dependencies found in grammatical constructions like relativization, coordination etc. Availability of the English CCGbank (Hockenmaier and Steedman, 2007) has enabled the creation of several robust and accurate wide-coverage CCG parsers (Hockenmaier and Steedman, 2002; Clark and Curran, 2007; Zhang and Clark, 2011). While the majority of CCG parsers use chart-based approaches (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), there has been some work on developing shift-reduce This paper develops a new incremental shiftreduce algorithm for parsing CCG by building a dependency graph in addition to the CCG derivation as a representation. The dependencies in the graph are extracted from the CCG derivation. A node can have multiple parents, and hence we construct a dependency graph rather than a tree. Two new actions are introduced in the shift-reduce paradigm for “revealing” (Pareschi and Steedman,"
N15-1006,P11-2033,0,0.0382044,"[ [ [ [ [ [ [ [ [ [ NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn NPJohn Slikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes (SNP)/NPlikes SNPlikes SNPlikes SNPlikes NPmangoes NPmangoes NPmangoes NPmangoes NPmangoes (NPNP)/NPf rom (NPNP)/NPf rom NPNPf rom (SNP)(SNP)madly NPIndia John likes (10) (8) mangoes madly (11) (7) from (6) India Figure 2: NonInc - Sequence of actions with parser configuration and the corresponding dependency graph. Though the performance of these greedy parsers is less accurate than related parsers using a beam (Zhang and Nivre, 2011), greedy parsers are interesting as they are very fast and are practically useful in large-scale applications such as parsing the web and online machine translation or speech recognition. In this work, we develop a new greedy transition-based algorithm for incremental CCG parsing, which is more incremental than Zhang and Clark (2011) and Xu et al. (2014) and more accurate than Hassan et al. (2009). Our algorithm is not strictly incremental as we only produce derivations which are compatible with the Strict Competence Hypothesis (Steedman, 2000) (details in §3.2.3). 3 Algorithms We first descri"
N15-1034,N10-1083,0,0.0703101,"Missing"
N15-1034,Q14-1008,1,0.850942,"Missing"
N15-1034,C12-1021,1,0.862666,"Missing"
N15-1034,P13-1148,1,0.871168,"Missing"
N15-1034,P12-1020,0,0.0310225,"ork using Maximum Entropy techniques to learn phonological constraint weights (see esp. Hayes and Wilson (2008), as well as the review in Coetzee and Pater (2011)). Recently there has been work attempting to integrate these two approaches. The word segmentation work generally ignores pronunciation variation by assuming that the input to the learner consists of sequences of citation forms of words, which is highly unrealistic. The phonology learning work has generally assumed that the learner has access to the underlying forms of words, which is also unrealistic. In the word segmentation area, Elsner et al. (2012) and Elsner et al. (2013) generalise the Goldwater bigram model by assuming that the bigram model generates underlying forms, which a finite state transducer maps to surface forms. While this is an extremely general model, inference in such a model is very challenging, and they restrict attention to transducers where the underlying to surface mapping consists of simple substitutions, so their model cannot handle the deletion phenomena studied here. B¨orschinger et al. (2013) also generalise the Goldwater bigram model by including an underlyingto-surface mapping, but their mapping only allows w"
N15-1034,D13-1005,0,0.157607,"techniques to learn phonological constraint weights (see esp. Hayes and Wilson (2008), as well as the review in Coetzee and Pater (2011)). Recently there has been work attempting to integrate these two approaches. The word segmentation work generally ignores pronunciation variation by assuming that the input to the learner consists of sequences of citation forms of words, which is highly unrealistic. The phonology learning work has generally assumed that the learner has access to the underlying forms of words, which is also unrealistic. In the word segmentation area, Elsner et al. (2012) and Elsner et al. (2013) generalise the Goldwater bigram model by assuming that the bigram model generates underlying forms, which a finite state transducer maps to surface forms. While this is an extremely general model, inference in such a model is very challenging, and they restrict attention to transducers where the underlying to surface mapping consists of simple substitutions, so their model cannot handle the deletion phenomena studied here. B¨orschinger et al. (2013) also generalise the Goldwater bigram model by including an underlyingto-surface mapping, but their mapping only allows word-final underlying /t/"
N15-1034,W04-0105,1,0.873827,"al. (2010) extended this model by defining the unigram distribution with a MaxEnt model. The MaxEnt features can capture phonotactic generalisations about possible word shapes, and their model achieves a stateof-the-art word segmentation f-score. The phonological learning task is to learn the phonological mapping from underlying forms to surface forms. Johnson (1984) and Johnson (1992) describe a search procedure for identifying underlying forms and the phonological rules that map them to surface forms given surface forms organised into inflectional paradigms. Goldwater and Johnson (2003) and Goldwater and Johnson (2004) showed how Harmonic Grammar phonological constraint weights (Smolensky and Legendre, 2005) can be learnt using a Maximum Entropy parameter estimation procedure given data consisting of underlying and surface word form pairs. There is now a significant body of work using Maximum Entropy techniques to learn phonological constraint weights (see esp. Hayes and Wilson (2008), as well as the review in Coetzee and Pater (2011)). Recently there has been work attempting to integrate these two approaches. The word segmentation work generally ignores pronunciation variation by assuming that the input to"
N15-1034,N09-1036,1,0.930594,"discusses possible future directions. 2 Background and related work The word segmentation task is the task of segmenting utterances represented as sequences of phones into sequences of words. Elman (1990) introduced the word segmentation task as a simplified form of lexical acquisition, and Brent and Cartwright (1996) and Brent (1999) introduced the unigram model of word segmentation, which forms the basis of the model used here. Goldwater et al. (2009) described a non-parametric Bayesian model of word segmentation, and highlighted the importance of contextual dependencies. Johnson (2008) and Johnson and Goldwater (2009) showed that word segmentation accuracy improves when phonotactic constraints on word shapes are incorporated into the model. That model has been extended to also exploit stress cues (B¨orschinger and Johnson, 2014), the 304 “topics” present in the non-linguistic context (Johnson et al., 2010) and the special properties of function words (Johnson et al., 2014). Liang and Klein (2009) proposed a simple unigram model of word segmentation much like the original Brent unigram model, and introduced a “word length penalty” to avoid under-segmentation that we also use here. (As Liang et al note, with"
N15-1034,P14-1027,1,0.85061,"word segmentation, which forms the basis of the model used here. Goldwater et al. (2009) described a non-parametric Bayesian model of word segmentation, and highlighted the importance of contextual dependencies. Johnson (2008) and Johnson and Goldwater (2009) showed that word segmentation accuracy improves when phonotactic constraints on word shapes are incorporated into the model. That model has been extended to also exploit stress cues (B¨orschinger and Johnson, 2014), the 304 “topics” present in the non-linguistic context (Johnson et al., 2010) and the special properties of function words (Johnson et al., 2014). Liang and Klein (2009) proposed a simple unigram model of word segmentation much like the original Brent unigram model, and introduced a “word length penalty” to avoid under-segmentation that we also use here. (As Liang et al note, without this the maximum likelihood solution is not to segment utterances at all, but to analyse each utterance as a single word). Berg-Kirkpatrick et al. (2010) extended this model by defining the unigram distribution with a MaxEnt model. The MaxEnt features can capture phonotactic generalisations about possible word shapes, and their model achieves a stateof-the"
N15-1034,P84-1070,1,0.535976,"word length penalty” to avoid under-segmentation that we also use here. (As Liang et al note, without this the maximum likelihood solution is not to segment utterances at all, but to analyse each utterance as a single word). Berg-Kirkpatrick et al. (2010) extended this model by defining the unigram distribution with a MaxEnt model. The MaxEnt features can capture phonotactic generalisations about possible word shapes, and their model achieves a stateof-the-art word segmentation f-score. The phonological learning task is to learn the phonological mapping from underlying forms to surface forms. Johnson (1984) and Johnson (1992) describe a search procedure for identifying underlying forms and the phonological rules that map them to surface forms given surface forms organised into inflectional paradigms. Goldwater and Johnson (2003) and Goldwater and Johnson (2004) showed how Harmonic Grammar phonological constraint weights (Smolensky and Legendre, 2005) can be learnt using a Maximum Entropy parameter estimation procedure given data consisting of underlying and surface word form pairs. There is now a significant body of work using Maximum Entropy techniques to learn phonological constraint weights ("
N15-1034,P08-1046,1,0.91494,"udes the paper and discusses possible future directions. 2 Background and related work The word segmentation task is the task of segmenting utterances represented as sequences of phones into sequences of words. Elman (1990) introduced the word segmentation task as a simplified form of lexical acquisition, and Brent and Cartwright (1996) and Brent (1999) introduced the unigram model of word segmentation, which forms the basis of the model used here. Goldwater et al. (2009) described a non-parametric Bayesian model of word segmentation, and highlighted the importance of contextual dependencies. Johnson (2008) and Johnson and Goldwater (2009) showed that word segmentation accuracy improves when phonotactic constraints on word shapes are incorporated into the model. That model has been extended to also exploit stress cues (B¨orschinger and Johnson, 2014), the 304 “topics” present in the non-linguistic context (Johnson et al., 2010) and the special properties of function words (Johnson et al., 2014). Liang and Klein (2009) proposed a simple unigram model of word segmentation much like the original Brent unigram model, and introduced a “word length penalty” to avoid under-segmentation that we also use"
N15-1034,N09-1069,0,0.434168,"h forms the basis of the model used here. Goldwater et al. (2009) described a non-parametric Bayesian model of word segmentation, and highlighted the importance of contextual dependencies. Johnson (2008) and Johnson and Goldwater (2009) showed that word segmentation accuracy improves when phonotactic constraints on word shapes are incorporated into the model. That model has been extended to also exploit stress cues (B¨orschinger and Johnson, 2014), the 304 “topics” present in the non-linguistic context (Johnson et al., 2010) and the special properties of function words (Johnson et al., 2014). Liang and Klein (2009) proposed a simple unigram model of word segmentation much like the original Brent unigram model, and introduced a “word length penalty” to avoid under-segmentation that we also use here. (As Liang et al note, without this the maximum likelihood solution is not to segment utterances at all, but to analyse each utterance as a single word). Berg-Kirkpatrick et al. (2010) extended this model by defining the unigram distribution with a MaxEnt model. The MaxEnt features can capture phonotactic generalisations about possible word shapes, and their model achieves a stateof-the-art word segmentation f"
N15-1034,W12-2308,1,0.823987,"ion format). There is a large body of work in the phonological literature on inferring phonological rules mapping underlying forms to their surface realisations. While most of this work assumes that the underlying forms are available to the inference procedure, there is work that induces underlying forms as well as the phonological processes that map them to sur303 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 303–313, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics face forms (Eisenstat, 2009; Pater et al., 2012). We present a model that takes a corpus of unsegmented surface representations of sentences and infers a word segmentation and underlying forms for each hypothesised word. We test this model on data derived from the Buckeye corpus where the only phonological variation consists of word-final /d/ and /t/ deletions, and show that it outperforms a state-ofthe-art model that only handles word-final /t/ deletions. Our model is a MaxEnt or log-linear model, which means that it is formally equivalent to a Harmonic Grammar, which is a continuous version of Optimality Theory (OT) (Smolensky and Legendr"
N16-1054,W13-3515,0,0.0697858,"Missing"
N16-1054,D15-1034,0,0.611959,"t) and the algorithms used to optimize the margin-based objective function, e.g., SGD, AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012) and L-BFGS (Liu and Nocedal, 1989). DISTMULT (Yang et al., 2015) is based on a Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation. Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garcia-Duran et al., 2015b). The TransH model (Wang et al., 2014b) associates each relation with a relation-specific hyperplane and uses a projection vector to project entity vectors onto that hyperplane. TransD (Ji et al., 2015) and TransR/CTransR (Lin et al., 2015b) extend the TransH model using two projection vectors and a matrix to project entity vectors into a relation-specific space, respectively. TransD learns a relation-role specific mapping just as STransE, but represents this mapping by projection vectors rather than full matrices, as in STransE. Thus STransE can be viewed as an extension of the TransR model"
N16-1054,D15-1173,0,0.0381607,"n extension of the TransR model, where head and tail entities are associated with their own project matrices, rather than using the same matrix for both, as in TransR and CTransR. 462 #E 40,943 14,951 #R 18 1,345 #Train 141,442 483,142 #Valid 5,000 50,000 #Test 5,000 59,071 Table 2: Statistics of the experimental datasets used in this study (and previous works). #E is the number of entities, #R is the number of relation types, and #Train, #Valid and #Test are the numbers of triples in the training, validation and test sets, respectively. Recently, Lao et al. (2011), Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garcia-Duran et al. (2015a) and Guu et al. (2015) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction. Nickel et al. (2015) reviews other approaches for learning from KBs and multi-relational data. 4 Experiments For link prediction evaluation, we conduct experiments and compare the performance of our STransE model with published results on the benchmark WN18 and FB15k datasets (Bordes et al., 2013). Information about these datasets is given in Table 2. 4.1 Task and evaluation protocol The"
N16-1054,D15-1038,0,0.0696033,"Missing"
N16-1054,P15-1067,0,0.418981,"ed on a Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation. Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garcia-Duran et al., 2015b). The TransH model (Wang et al., 2014b) associates each relation with a relation-specific hyperplane and uses a projection vector to project entity vectors onto that hyperplane. TransD (Ji et al., 2015) and TransR/CTransR (Lin et al., 2015b) extend the TransH model using two projection vectors and a matrix to project entity vectors into a relation-specific space, respectively. TransD learns a relation-role specific mapping just as STransE, but represents this mapping by projection vectors rather than full matrices, as in STransE. Thus STransE can be viewed as an extension of the TransR model, where head and tail entities are associated with their own project matrices, rather than using the same matrix for both, as in TransR and CTransR. 462 #E 40,943 14,951 #R 18 1,345 #Train 141,442 483,142"
N16-1054,D11-1049,0,0.0156437,"as in STransE. Thus STransE can be viewed as an extension of the TransR model, where head and tail entities are associated with their own project matrices, rather than using the same matrix for both, as in TransR and CTransR. 462 #E 40,943 14,951 #R 18 1,345 #Train 141,442 483,142 #Valid 5,000 50,000 #Test 5,000 59,071 Table 2: Statistics of the experimental datasets used in this study (and previous works). #E is the number of entities, #R is the number of relation types, and #Train, #Valid and #Test are the numbers of triples in the training, validation and test sets, respectively. Recently, Lao et al. (2011), Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garcia-Duran et al. (2015a) and Guu et al. (2015) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction. Nickel et al. (2015) reviews other approaches for learning from KBs and multi-relational data. 4 Experiments For link prediction evaluation, we conduct experiments and compare the performance of our STransE model with published results on the benchmark WN18 and FB15k datasets (Bordes et al., 2013). Information about these datasets i"
N16-1054,D15-1082,0,0.544678,", 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to represent each relation. Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garcia-Duran et al., 2015b). The TransH model (Wang et al., 2014b) associates each relation with a relation-specific hyperplane and uses a projection vector to project entity vectors onto that hyperplane. TransD (Ji et al., 2015) and TransR/CTransR (Lin et al., 2015b) extend the TransH model using two projection vectors and a matrix to project entity vectors into a relation-specific space, respectively. TransD learns a relation-role specific mapping just as STransE, but represents this mapping by projection vectors rather than full matrices, as in STransE. Thus STransE can be viewed as an extension of the TransR model, where head and tail entities are associated with their own project matrices, rather than using the same matrix for both, as in TransR and CTransR. 462 #E 40,943 14,951 #R 18 1,345 #Train 141,442 483,142 #Valid 5,000 50,000 #Test 5,000 59,0"
N16-1054,D15-1191,0,0.118236,"el, where head and tail entities are associated with their own project matrices, rather than using the same matrix for both, as in TransR and CTransR. 462 #E 40,943 14,951 #R 18 1,345 #Train 141,442 483,142 #Valid 5,000 50,000 #Test 5,000 59,071 Table 2: Statistics of the experimental datasets used in this study (and previous works). #E is the number of entities, #R is the number of relation types, and #Train, #Valid and #Test are the numbers of triples in the training, validation and test sets, respectively. Recently, Lao et al. (2011), Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garcia-Duran et al. (2015a) and Guu et al. (2015) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction. Nickel et al. (2015) reviews other approaches for learning from KBs and multi-relational data. 4 Experiments For link prediction evaluation, we conduct experiments and compare the performance of our STransE model with published results on the benchmark WN18 and FB15k datasets (Bordes et al., 2013). Information about these datasets is given in Table 2. 4.1 Task and evaluation protocol The link prediction ta"
N16-1054,N13-1090,0,0.0119687,"t are represented by vectors h and t ∈ Rk respectively. The Unstructured model (Bordes et al., 2012) assumes that h ≈ t. As the Unstructured model does not take the relationship r into account, it cannot distinguish different relation types. The Structured Embedding (SE) model (Bordes et al., 2011) extends the unstructured model by assuming that h and t are similar only in a relation-dependent subspace. It represents each relation r with two matrices Wr,1 and Wr,2 ∈ Rk×k , which are chosen so that Wr,1 h ≈ Wr,2 t. The TransE model (Bordes et al., 2013) is inspired by models such as Word2Vec (Mikolov et al., 2013) where relationships between words often correspond to translations in latent feature space. The TransE model represents each relation r by a translation vector r ∈ Rk , which is chosen so that h + r ≈ t. The primary contribution of this paper is that two very simple relation-prediction models, SE and TransE, can be combined into a single model, which we call STransE. Specifically, we use relationspecific matrices Wr,1 and Wr,2 as in the SE model to identify the relation-dependent aspects of both h and t, and use a vector r as in the TransE model to describe the relationship between h and t in"
N16-1054,P15-1016,0,0.0842811,"STransE can be viewed as an extension of the TransR model, where head and tail entities are associated with their own project matrices, rather than using the same matrix for both, as in TransR and CTransR. 462 #E 40,943 14,951 #R 18 1,345 #Train 141,442 483,142 #Valid 5,000 50,000 #Test 5,000 59,071 Table 2: Statistics of the experimental datasets used in this study (and previous works). #E is the number of entities, #R is the number of relation types, and #Train, #Valid and #Test are the numbers of triples in the training, validation and test sets, respectively. Recently, Lao et al. (2011), Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garcia-Duran et al. (2015a) and Guu et al. (2015) showed that relation paths between entities in KBs provide richer information and improve the relationship prediction. Nickel et al. (2015) reviews other approaches for learning from KBs and multi-relational data. 4 Experiments For link prediction evaluation, we conduct experiments and compare the performance of our STransE model with published results on the benchmark WN18 and FB15k datasets (Bordes et al., 2013). Information about these datasets is given in Table 2. 4.1 Tas"
N16-1054,N13-1008,0,0.190991,"Missing"
N16-1054,W15-4007,0,0.168876,", γ = 5, and k = 50 for WN18, and λ = 0.0001, γ = 1, and k = 100 for FB15k. 4.2 Main results Table 3 compares the link prediction results of our STransE model with results reported in prior work, using the same experimental setup. The first twelve rows report the performance of models that do not exploit information about alternative paths between head and tail entities. The next two rows report results of the RTransE and PTransE models, which are extensions of the TransE model that exploit information about relation paths. The last row presents results for the log-linear model Node+LinkFeat (Toutanova and Chen, 2015) which makes use of textual mentions derived from the large external ClueWeb-12 corpus. It is clear that Node+LinkFeat with the additional external corpus information obtained best results. In future work we plan to extend the STransE model to incorporate such additional information. Table 3 also shows that models RTransE and PTransE employing path information achieve better results than models that do not use such information. In terms of models not exploiting path information or external information, the STransE model scores better than 463 WN18 MR H10 SE (Bordes et al., 2011) 985 80.5 Unstr"
N16-1054,D15-1174,0,0.0642098,"Missing"
N16-1054,D14-1167,0,0.613922,"using additional information. Third, the more complex models that exploit external information are typically extensions of these simpler models, and are often initialized with parameters estimated by such simpler models, so improvements to the simpler models should yield corresponding improvements to the more complex models as well. Embedding models for KB completion associate entities and/or relations with dense feature vectors or matrices. Such models obtain state-of-the-art performance (Nickel et al., 2011; Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014b; Guu et al., 2015) and generalize to large KBs (Krompa et al., 2015). Table 1 summarizes a number of prominent embedding models for KB completion. Let (h, r, t) represent a triple. In all of the models 460 Proceedings of NAACL-HLT 2016, pages 460–466, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Model Score function fr (h, t) Opt. k×k SE kWr,1 h − Wr,2 tk`1/2 ; Wr,1 , Wr,2 ∈ R SGD Unstructured kh − tk`1/2 SGD TransE kh + r − tk`1/2 ; r ∈ Rk > SGD k×k DISTMULT h Wr t ; Wr is a diagonal matrix ∈ R NTN > d k×k×d u> ; Wr,1 , Wr,2 ∈ Rd×k r tanh(h Mr t"
N16-1054,P16-1136,0,\N,Missing
N16-1054,N16-1105,0,\N,Missing
N16-1054,K16-1005,1,\N,Missing
N16-1054,P16-1124,0,\N,Missing
N16-1054,D16-1145,0,\N,Missing
N16-1054,C16-1062,0,\N,Missing
N16-1054,E17-1013,0,\N,Missing
N18-5012,Y06-1028,0,0.124858,"Missing"
N18-5012,N16-1031,0,0.0230142,"Missing"
N18-5012,P15-1038,0,0.0700121,"Missing"
N18-5012,U16-1017,1,0.928447,"is a need for building an NLP pipeline, such as the Stanford CoreNLP toolkit (Manning et al., 2014), for those key tasks to assist users and to support researchers and tool developers of downstream tasks. Nguyen et al. (2010) and Le et al. (2013) built Vietnamese NLP pipelines by wrapping existing word segmenters and POS taggers including: JVnSegmenter (Nguyen et al., 2006), vnTokenizer (Le et al., 2008), JVnTagger (Nguyen et al., 2010) and vnTagger (Le-Hong et al., 2010). However, these word segmenters and POS taggers are no longer considered SOTA models for Vietnamese (Nguyen and Le, 2016; Nguyen et al., 2016b). • Easy-to-use – All VnCoreNLP components are wrapped into a single .jar file, so users do not have to install external dependencies. Users can run processing pipelines from either the command-line or the Java API. • Fast – VnCoreNLP is fast, so it can be used for dealing with large-scale data. Also it benefits users suffering from limited computation resources (e.g. users from Vietnam). • Accurate – VnCoreNLP components obtain higher results than all previous published results on the same benchmark datasets. 56 Proceedings of NAACL-HLT 2018: Demonstrations, pages 56–60 c New Orleans, Louis"
N18-5012,K17-3014,1,0.886035,"Missing"
N18-5012,Q16-1023,0,0.121699,"Missing"
N18-5012,N16-1030,0,0.0236039,"ns present evaluations for the NER (ner) and dependency parsing (parse) components. 4.1 Models: We make an empirical comparison between the VnCoreNLP’s NER component and the following neural network-based models: • BiLSTM-CRF (Huang et al., 2015) is a sequence labeling model which extends the BiLSTM model with a CRF layer. • BiLSTM-CRF + CNN-char, i.e. BiLSTMCNN-CRF, is an extension of BiLSTM-CRF, using CNN to derive character-based word representations (Ma and Hovy, 2016). • BiLSTM-CRF + LSTM-char is an extension of BiLSTM-CRF, using BiLSTM to derive the character-based word representations (Lample et al., 2016). Named entity recognition We make a comparison between SOTA featurebased and neural network-based models, which, to the best of our knowledge, has not been done in any prior work on Vietnamese NER. • BiLSTM-CRF+POS is another extension to BiLSTM-CRF, incorporating embeddings of automatically predicted POS tags (Reimers and Gurevych, 2017). Dataset: The NER shared task at the 2016 VLSP workshop provides a set of 16,861 manually annotated sentences for training and development, and a set of 2,831 manually annotated sentences for test, with four NER labels PER, LOC, ORG and MISC. Note that in bo"
N18-5012,L18-1410,1,0.651006,"Ông Nguyễn Khắc Chúc đang làm việc tại Đại học Quốc gia Hà Nội.""); pipeline.annotate(annotation); String annotatedStr = annotation. toString(); • wseg – Unlike English where white space is a strong indicator of word boundaries, when written in Vietnamese white space is also used to separate syllables that constitute words. So word segmentation is referred to as the key first step in Vietnamese NLP. We have proposed a transformation rule-based learning model for Vietnamese word segmentation, which obtains better segmentation accuracy and speed than all previous word segmenters. See details in Nguyen et al. (2018). Listing 1: Minimal code for an analysis pipeline. In addition, Listing 2 provides a more realistic and complete example code, presenting key components of the toolkit. Here an annotation pipeline can be used for any text rather than just a single sentence, e.g. for a paragraph or entire news story. 3 Components This section briefly describes each component of VnCoreNLP. Note that our goal is not to develop 57 • pos – To label words with their POS tag, we apply MarMoT which is a generic CRF framework and a SOTA POS and morphological tagger (Mueller et al., 2013).1 gold POS tags are not availa"
N18-5012,U17-1013,1,0.89339,"Missing"
N18-5012,W09-3035,0,0.236133,"Missing"
N18-5012,2010.jeptalnrecital-long.36,0,0.116472,"dependency treebank was published in 2014 (Nguyen et al., 2014); and an NER dataset was released for the second VLSP campaign in 2016. So there is a need for building an NLP pipeline, such as the Stanford CoreNLP toolkit (Manning et al., 2014), for those key tasks to assist users and to support researchers and tool developers of downstream tasks. Nguyen et al. (2010) and Le et al. (2013) built Vietnamese NLP pipelines by wrapping existing word segmenters and POS taggers including: JVnSegmenter (Nguyen et al., 2006), vnTokenizer (Le et al., 2008), JVnTagger (Nguyen et al., 2010) and vnTagger (Le-Hong et al., 2010). However, these word segmenters and POS taggers are no longer considered SOTA models for Vietnamese (Nguyen and Le, 2016; Nguyen et al., 2016b). • Easy-to-use – All VnCoreNLP components are wrapped into a single .jar file, so users do not have to install external dependencies. Users can run processing pipelines from either the command-line or the Java API. • Fast – VnCoreNLP is fast, so it can be used for dealing with large-scale data. Also it benefits users suffering from limited computation resources (e.g. users from Vietnam). • Accurate – VnCoreNLP components obtain higher results than all"
N18-5012,P16-1101,0,0.0801222,"LP) tasks including word segmentation, part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing, and obtains state-of-the-art (SOTA) results for these tasks. We release VnCoreNLP to provide rich linguistic annotations to facilitate research work on Vietnamese NLP. Our VnCoreNLP is open-source and available at: https:// github.com/vncorenlp/VnCoreNLP. 1 Figure 1: In pipeline architecture of VnCoreNLP, annotations are performed on an Annotation object. Pham et al. (2017) built the NNVLP toolkit for Vietnamese sequence labeling tasks by applying a BiLSTM-CNN-CRF model (Ma and Hovy, 2016). However, Pham et al. (2017) did not make a comparison to SOTA traditional feature-based models. In addition, NNVLP is slow with a processing speed at about 300 words per second, which is not practical for real-world application such as dealing with large-scale data. In this paper, we present a Java NLP toolkit for Vietnamese, namely VnCoreNLP, which aims to facilitate Vietnamese NLP research by providing rich linguistic annotations through key NLP components of word segmentation, POS tagging, NER and dependency parsing. Figure 1 describes the overall system architecture. The following items"
N18-5012,P14-5010,0,0.0129689,"tively explored in the last decade, boosted by the successes of the 4-year KC01.01/2006-2010 national project on Vietnamese language and speech processing (VLSP). Over the last 5 years, standard benchmark datasets for key Vietnamese NLP tasks are publicly available: datasets for word segmentation and POS tagging were released for the first VLSP evaluation campaign in 2013; a dependency treebank was published in 2014 (Nguyen et al., 2014); and an NER dataset was released for the second VLSP campaign in 2016. So there is a need for building an NLP pipeline, such as the Stanford CoreNLP toolkit (Manning et al., 2014), for those key tasks to assist users and to support researchers and tool developers of downstream tasks. Nguyen et al. (2010) and Le et al. (2013) built Vietnamese NLP pipelines by wrapping existing word segmenters and POS taggers including: JVnSegmenter (Nguyen et al., 2006), vnTokenizer (Le et al., 2008), JVnTagger (Nguyen et al., 2010) and vnTagger (Le-Hong et al., 2010). However, these word segmenters and POS taggers are no longer considered SOTA models for Vietnamese (Nguyen and Le, 2016; Nguyen et al., 2016b). • Easy-to-use – All VnCoreNLP components are wrapped into a single .jar file,"
N18-5012,I17-3010,0,0.0466753,"ely VnCoreNLP—a Java NLP annotation pipeline for Vietnamese. Our VnCoreNLP supports key natural language processing (NLP) tasks including word segmentation, part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing, and obtains state-of-the-art (SOTA) results for these tasks. We release VnCoreNLP to provide rich linguistic annotations to facilitate research work on Vietnamese NLP. Our VnCoreNLP is open-source and available at: https:// github.com/vncorenlp/VnCoreNLP. 1 Figure 1: In pipeline architecture of VnCoreNLP, annotations are performed on an Annotation object. Pham et al. (2017) built the NNVLP toolkit for Vietnamese sequence labeling tasks by applying a BiLSTM-CNN-CRF model (Ma and Hovy, 2016). However, Pham et al. (2017) did not make a comparison to SOTA traditional feature-based models. In addition, NNVLP is slow with a processing speed at about 300 words per second, which is not practical for real-world application such as dealing with large-scale data. In this paper, we present a Java NLP toolkit for Vietnamese, namely VnCoreNLP, which aims to facilitate Vietnamese NLP research by providing rich linguistic annotations through key NLP components of word segmentat"
N18-5012,P05-1012,0,0.395501,"Missing"
N18-5012,D17-1035,0,0.0299672,"Missing"
N18-5012,D13-1032,0,0.135735,"Missing"
N19-1282,N01-1016,1,0.770572,"chboard treebank corpus (Mitchell et al., 1999) the reparanda, filled pauses and discourse markers are dominated by EDITED, INTJ and PRN nodes, respectively (see Figure 1). Of these disfluency nodes, EDITED nodes pose a major problem for conventional syntactic parsers, as the parsers typically fail to find any EDITED nodes at all. Conventional parsers mainly capture tree-structured dependencies between words, while the relation between reparandum and repair is quite different: the repair is often a “rough copy” of the reparandum, using the same or very similar words in roughly the same order (Charniak and Johnson, 2001; Johnson and Charniak, 2004). The “rough copy” dependencies are strong evidence of a disfluency, but conventional syntac2756 Proceedings of NAACL-HLT 2019, pages 2756–2765 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics tic parsers cannot capture them. Moreover, the reparandum and the repair do not form conventional syntactic phrases, as illustrated in Figure 1, which is an additional difficulty when integrating disfluency detection with syntactic parsing. This motivated the development of special disfluency detection systems which find and remo"
N19-1282,N15-1029,0,0.218965,"Missing"
N19-1282,W18-2501,0,0.0266779,"Missing"
N19-1282,P06-1021,0,0.120731,"Missing"
N19-1282,Q14-1011,1,0.934389,"ciation for Computational Linguistics tic parsers cannot capture them. Moreover, the reparandum and the repair do not form conventional syntactic phrases, as illustrated in Figure 1, which is an additional difficulty when integrating disfluency detection with syntactic parsing. This motivated the development of special disfluency detection systems which find and remove disfluent words from the input prior to parsing (Charniak and Johnson, 2001; Kahn et al., 2005; Lease and Johnson, 2006), and special mechanisms added to parsers specifically to handle disfluencies (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016; Tran et al., 2018). S EDITED INTJ PRN S NP UH VP PRP VBP RB we do n&apos;t uh S NP NP NP I mean a 2 Related Work Speech recognition errors, unknown sentence boundaries and disfluencies are three major problems addressed by previous work on parsing speech. In this work, we focus on the problem of disfluency detection in parsing human-transcribed speech, where we assume that sentence boundaries are given and there are no word recognition errors. This section reviews approaches that add special mechanisms to parsers to handle disfluencies as well as specialized disfluency det"
N19-1282,D18-1490,1,0.752509,"cation-aware attention mechanism is specially useful for detecting disfluencies (Tran et al., 2018). In general, parsing models are poor at detecting disfluencies, mainly due to “rough copy” dependencies in disfluent sentences, which are difficult for conventional parsers to detect. 2.2 Specialized Disfluency Detection Models Disfluency detection models often use a sequence tagging technique to assign a single label to each 2757 word of a sequence. Previous work shows that LSTMs and CNNs operating on words alone are poor at disfluency detection (Zayats et al., 2016; Wang et al., 2016; Jamshid Lou et al., 2018). The performance of state-of-the-art disfluency detection models depends heavily on hand-crafted pattern match features, which are specifically designed to find “rough copies”. One recent paper (Jamshid Lou et al., 2018) augments a CNN model with a new kind of layer called an autocorrelational layer to capture “rough copy” dependencies. The model compares the input vectors of words within a window to find identical or similar words. The addition of the auto-correlational layer to a “vanilla” CNN significantly improves the performance over the baseline CNN model. The results are competitive to"
N19-1282,P17-2087,1,0.888174,"Missing"
N19-1282,P18-1249,0,0.311676,"roach presented here is the self-attentive transformer architecture, which suggests that this architecture is capable of detecting disfluencies with very high accuracy. The work we present goes beyond the work of Wang et al. (2018) in also studying the impact of jointly predicting syntactic structure and disfluencies (so it can be understood as a kind of multi-task learning). We also investigate the impact of different ways of representing disfluency information in the context of a syntactic parsing task. 3 Neural Constituency Parser We use the self-attentive constituency parser introduced by Kitaev and Klein (2018) and train it on the Switchboard corpus of transcribed speech (we describe the training and evaluation conditions in more detail in Section 4). The self-attentive parser achieves state-of-the-art performance on WSJ data, which is why we selected it as the best “off-the-shelf” parsing model. The constituency parser uses a self-attentive transformer (Vaswani et al., 2017) as an encoder and a chart-based parser (Stern et al., 2017) as a decoder, as reviewed in the following sections. 3.1 Self-Attentive Encoder The encoder of a transformer is a stack of n identical layers, each consists of two sta"
N19-1282,N06-2019,1,0.86002,"ut conventional syntac2756 Proceedings of NAACL-HLT 2019, pages 2756–2765 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics tic parsers cannot capture them. Moreover, the reparandum and the repair do not form conventional syntactic phrases, as illustrated in Figure 1, which is an additional difficulty when integrating disfluency detection with syntactic parsing. This motivated the development of special disfluency detection systems which find and remove disfluent words from the input prior to parsing (Charniak and Johnson, 2001; Kahn et al., 2005; Lease and Johnson, 2006), and special mechanisms added to parsers specifically to handle disfluencies (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016; Tran et al., 2018). S EDITED INTJ PRN S NP UH VP PRP VBP RB we do n&apos;t uh S NP NP NP I mean a 2 Related Work Speech recognition errors, unknown sentence boundaries and disfluencies are three major problems addressed by previous work on parsing speech. In this work, we focus on the problem of disfluency detection in parsing human-transcribed speech, where we assume that sentence boundaries are given and there are no word recognition error"
N19-1282,N18-1202,0,0.0107086,"into training, dev and test sets as follows: training data consists of the sw[23]∗.mrg files, dev data consists of the sw4[5, 6, 7, 8, 9]∗.mrg files and test data consists of the sw4[0, 1]∗.mrg files. Except as explicitly noted below, we remove all partial words (words tagged XX and words ending in “-”) and punctuation from data, as they are not available in realistic ASR applications (Johnson and Charniak, 2004). 3.3 4.1 T Given the gold tagged tree T ? , we train the model by minimizing a hinge loss:   ? ? max 0, max? [s(T ) + 4(T, T )] − s(T ) (4) T 6=T External Embedding and Edited Loss Peters et al. (2018) have recently introduced a new approach for word representation called Embeddings from Language Models (ELMo) which has achieved state-of-the-art results in various NLP tasks. These embeddings are produced by a LSTM language model (LM) which inputs words and characters and generates a vector representation for each word of the sentence. The ELMo output is a concatenation of both the forward and backward LM hidden states. We found that using external ELMo embedding as the only lexical representation used by the model leads to the highest EDITED word f-score. Following Kitaev and Klein (2018),"
N19-1282,D13-1013,0,0.70739,"e 2 - June 7, 2019. 2019 Association for Computational Linguistics tic parsers cannot capture them. Moreover, the reparandum and the repair do not form conventional syntactic phrases, as illustrated in Figure 1, which is an additional difficulty when integrating disfluency detection with syntactic parsing. This motivated the development of special disfluency detection systems which find and remove disfluent words from the input prior to parsing (Charniak and Johnson, 2001; Kahn et al., 2005; Lease and Johnson, 2006), and special mechanisms added to parsers specifically to handle disfluencies (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016; Tran et al., 2018). S EDITED INTJ PRN S NP UH VP PRP VBP RB we do n&apos;t uh S NP NP NP I mean a 2 Related Work Speech recognition errors, unknown sentence boundaries and disfluencies are three major problems addressed by previous work on parsing speech. In this work, we focus on the problem of disfluency detection in parsing human-transcribed speech, where we assume that sentence boundaries are given and there are no word recognition errors. This section reviews approaches that add special mechanisms to parsers to handle disfluencies as well a"
N19-1282,P17-1076,0,0.170558,"senting disfluency information in the context of a syntactic parsing task. 3 Neural Constituency Parser We use the self-attentive constituency parser introduced by Kitaev and Klein (2018) and train it on the Switchboard corpus of transcribed speech (we describe the training and evaluation conditions in more detail in Section 4). The self-attentive parser achieves state-of-the-art performance on WSJ data, which is why we selected it as the best “off-the-shelf” parsing model. The constituency parser uses a self-attentive transformer (Vaswani et al., 2017) as an encoder and a chart-based parser (Stern et al., 2017) as a decoder, as reviewed in the following sections. 3.1 Self-Attentive Encoder The encoder of a transformer is a stack of n identical layers, each consists of two stacked sublayers: a multi-head attention mechanism, and a point-wise fully connected network. The inputs to the encoder first flow through a self-attention sublayer, which helps the encoder attends to several words in the sentence as it encodes a specific word. Because the model lacks recurrent layers, this sublayer is the only mechanism which propagates information between positions in the sentence. The self-attention maps the in"
N19-1282,N18-1007,0,0.548884,"nnot capture them. Moreover, the reparandum and the repair do not form conventional syntactic phrases, as illustrated in Figure 1, which is an additional difficulty when integrating disfluency detection with syntactic parsing. This motivated the development of special disfluency detection systems which find and remove disfluent words from the input prior to parsing (Charniak and Johnson, 2001; Kahn et al., 2005; Lease and Johnson, 2006), and special mechanisms added to parsers specifically to handle disfluencies (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016; Tran et al., 2018). S EDITED INTJ PRN S NP UH VP PRP VBP RB we do n&apos;t uh S NP NP NP I mean a 2 Related Work Speech recognition errors, unknown sentence boundaries and disfluencies are three major problems addressed by previous work on parsing speech. In this work, we focus on the problem of disfluency detection in parsing human-transcribed speech, where we assume that sentence boundaries are given and there are no word recognition errors. This section reviews approaches that add special mechanisms to parsers to handle disfluencies as well as specialized disfluency detection models. VP PP VP DT NN IN PRP VBP • M"
N19-1282,C18-1299,0,0.670617,"“rough copies”. One recent paper (Jamshid Lou et al., 2018) augments a CNN model with a new kind of layer called an autocorrelational layer to capture “rough copy” dependencies. The model compares the input vectors of words within a window to find identical or similar words. The addition of the auto-correlational layer to a “vanilla” CNN significantly improves the performance over the baseline CNN model. The results are competitive to models using complex hand-crafted features or external information sources, indicating that the auto-correlation model learns “rough copies”. One recent paper (Wang et al., 2018) introduces a semi-supervised approach to disfluency detection. Their self-attentive model is the current stateof-the-art result in disfluency detection. The common factor in Wang et al. (2018) and the approach presented here is the self-attentive transformer architecture, which suggests that this architecture is capable of detecting disfluencies with very high accuracy. The work we present goes beyond the work of Wang et al. (2018) in also studying the impact of jointly predicting syntactic structure and disfluencies (so it can be understood as a kind of multi-task learning). We also investig"
N19-1282,C16-1027,0,0.451532,"enting the parser with a location-aware attention mechanism is specially useful for detecting disfluencies (Tran et al., 2018). In general, parsing models are poor at detecting disfluencies, mainly due to “rough copy” dependencies in disfluent sentences, which are difficult for conventional parsers to detect. 2.2 Specialized Disfluency Detection Models Disfluency detection models often use a sequence tagging technique to assign a single label to each 2757 word of a sequence. Previous work shows that LSTMs and CNNs operating on words alone are poor at disfluency detection (Zayats et al., 2016; Wang et al., 2016; Jamshid Lou et al., 2018). The performance of state-of-the-art disfluency detection models depends heavily on hand-crafted pattern match features, which are specifically designed to find “rough copies”. One recent paper (Jamshid Lou et al., 2018) augments a CNN model with a new kind of layer called an autocorrelational layer to capture “rough copy” dependencies. The model compares the input vectors of words within a window to find identical or similar words. The addition of the auto-correlational layer to a “vanilla” CNN significantly improves the performance over the baseline CNN model. The"
N19-1282,D17-1296,0,0.471205,"Missing"
N19-1282,P15-1048,0,0.170112,"the constituent-based representation of disfluencies with a word-based representation of disfluencies improves the detection of disfluent words, Joint Parsing and Disfluency Detection Many speech parsers adopt a transition-based dependency approach to (i) find the relationship between head words and words modifying the heads, and (ii) detect and remove disfluent words and their dependencies from the sentence. Transition-based parsers can be augmented with new parse actions to specifically handle disfluent words (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016; Wu et al., 2015). A classifier is trained to choose between the standard and the augmented parse actions at each time step. Using pattern-match features in the classifier significantly improves disfluency detection (Honnibal and Johnson, 2014). This reflects the fact that parsing based models use pattern-matching to capture the “rough copy” dependencies that are characteristic of speech disfluencies. Speech parsing models usually use lexical features. One recent approach (Tran et al., 2018) integrates lexical and prosodic cues in an encoderdecoder constituency parser. Prosodic cues result in very small perfor"
N19-1282,D16-1109,0,0.605089,"nguistics tic parsers cannot capture them. Moreover, the reparandum and the repair do not form conventional syntactic phrases, as illustrated in Figure 1, which is an additional difficulty when integrating disfluency detection with syntactic parsing. This motivated the development of special disfluency detection systems which find and remove disfluent words from the input prior to parsing (Charniak and Johnson, 2001; Kahn et al., 2005; Lease and Johnson, 2006), and special mechanisms added to parsers specifically to handle disfluencies (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016; Tran et al., 2018). S EDITED INTJ PRN S NP UH VP PRP VBP RB we do n&apos;t uh S NP NP NP I mean a 2 Related Work Speech recognition errors, unknown sentence boundaries and disfluencies are three major problems addressed by previous work on parsing speech. In this work, we focus on the problem of disfluency detection in parsing human-transcribed speech, where we assume that sentence boundaries are given and there are no word recognition errors. This section reviews approaches that add special mechanisms to parsers to handle disfluencies as well as specialized disfluency detection models. VP PP VP"
P00-1061,P99-1035,1,0.867771,"Missing"
P00-1061,P97-1003,0,0.125027,"Missing"
P00-1061,A94-1009,0,0.0611326,"Missing"
P00-1061,A00-2021,1,0.883041,"Missing"
P00-1061,P99-1069,1,0.492297,"for estimating the parameters of the stochastic grammar from unannotated data. Our usage of EM was initiated by the current lack of large unicationbased treebanks for German. However, our experimental results also show an exception to the common wisdom of the insuciency of EM for highly accurate statistical modeling. Our approach to lexicalized stochastic modeling is based on the parametric family of loglinear probability models, which is used to dene a probability distribution on the parses of a Lexical-Functional Grammar (LFG) for German. In previous work on log-linear models for LFG by Johnson et al. (1999), pseudolikelihood estimation from annotated corpora has been introduced and experimented with on a small scale. However, to our knowledge, to date no large LFG annotated corpora of unrestricted German text are available. Fortunately, algorithms exist for statistical inference of log-linear models from unannotated data (Riezler, 1999). We apply this algorithm to estimate log-linear LFG models from large corpora of newspaper text. In our largest experiment, we used 250,000 parses which were produced by parsing 36,000 newspaper sentences with the German LFG. Experimental evaluation of our models"
P00-1061,J93-2004,0,0.0541982,"Missing"
P00-1061,P92-1017,0,0.134096,"Missing"
P00-1061,C00-2094,1,0.846315,"Missing"
P00-1061,W97-0301,0,0.0177929,"Missing"
P01-1042,1997.iwpt-1.16,0,0.0361746,"Shift-reduce parsing The previous section compared similiar joint and conditional tagging models. This section compares a pair of joint and conditional parsing models. The models are both stochastic shift-reduce parsers; they differ only in how the distribution over possible next moves are calculated. These parsers are direct simplifications of the Structured Language Model (Jelinek, 2000). Because the parsers’ moves are determined solely by the top two category labels on the stack and possibly the look-ahead symbol, they are much simpler than stochastic LR parsers (Briscoe and Carroll, 1993; Inui et al., 1997). The distribution over trees generated by the joint model is a probabilistic context-free language (Abney et al., 1999). As with the PCFG models discussed earlier, these parsers are not lexicalized; lexical items are ignored, and the POS tags are used as the terminals. These two parsers only produce trees with unary or binary nodes, so we binarized the training data before training the parser, and debinarize the trees the parsers produce before evaluating them with respect to the test data (Johnson, 1998). We binarized by inserting n − 2 additional nodes into each local tree with n &gt; 2 childr"
P01-1042,P99-1069,1,0.885601,"en used with general exponential or “maximum entropy” models because standard maximum likelihood estimation is usually computationally intractable (Berger et al., 1996; Della Pietra et al., 1997; Jelinek, 1997). Wellknown computational linguistic models such as (MLE) (MCLE) Ω Y = y i , X = xi Ω Y = y i , X = xi X = xi Figure 1: The MLE makes the training data (yi , xi ) as likely as possible (relative to Ω), while the MCLE makes (yi , xi ) as likely as possible relative to other pairs (y 0 , xi ). Maximum-Entropy Markov Models (McCallum et al., 2000) and Stochastic Unification-based Grammars (Johnson et al., 1999) are standardly estimated with conditional estimators, and it would be interesting to know whether conditional estimation affects the quality of the estimated model. It should be noted that in practice, the MCLE of a model with a large number of features with complex dependencies may yield far better performance than the MLE of the much smaller model that could be estimated with the same computational effort. Nevertheless, as this paper shows, conditional estimators can be used with other kinds of models besides MaxEnt models, and in any event it is interesting to ask whether the MLE differs f"
P01-1042,J98-4004,1,0.837771,"symbol, they are much simpler than stochastic LR parsers (Briscoe and Carroll, 1993; Inui et al., 1997). The distribution over trees generated by the joint model is a probabilistic context-free language (Abney et al., 1999). As with the PCFG models discussed earlier, these parsers are not lexicalized; lexical items are ignored, and the POS tags are used as the terminals. These two parsers only produce trees with unary or binary nodes, so we binarized the training data before training the parser, and debinarize the trees the parsers produce before evaluating them with respect to the test data (Johnson, 1998). We binarized by inserting n − 2 additional nodes into each local tree with n &gt; 2 children. We binarized by first joining the head to all of the constituents to its right, and then joining the resulting structure with constituents to the left. The label of a new node is the label of the head followed by the suffix “-1” if the head is (contained in) the right child or “-2” if the head is (contained in) the left child. Figure 3 depicts an example of this transformation. The Structured Language Model is described in detail in Jelinek (2000), so it is only reviewed here. Each parser’s stack is a"
P01-1042,P99-1070,0,0.0608238,"a pair of joint and conditional parsing models. The models are both stochastic shift-reduce parsers; they differ only in how the distribution over possible next moves are calculated. These parsers are direct simplifications of the Structured Language Model (Jelinek, 2000). Because the parsers’ moves are determined solely by the top two category labels on the stack and possibly the look-ahead symbol, they are much simpler than stochastic LR parsers (Briscoe and Carroll, 1993; Inui et al., 1997). The distribution over trees generated by the joint model is a probabilistic context-free language (Abney et al., 1999). As with the PCFG models discussed earlier, these parsers are not lexicalized; lexical items are ignored, and the POS tags are used as the terminals. These two parsers only produce trees with unary or binary nodes, so we binarized the training data before training the parser, and debinarize the trees the parsers produce before evaluating them with respect to the test data (Johnson, 1998). We binarized by inserting n − 2 additional nodes into each local tree with n &gt; 2 children. We binarized by first joining the head to all of the constituents to its right, and then joining the resulting struc"
P01-1042,J96-1002,0,0.00544452,"odel parameter θ which make the training data pairs (yi , xi ) as likely as possible relative to all other pairs (y 0 , x0 ) in Ω. The MCLE, on the other hand, selects the model parameter θ in order to make the training data pair (yi , xi ) more likely than other pairs (y 0 , xi ) in Ω, i.e., pairs with the same visible value xi as the training datum. In statistical computational linguistics, maximum conditional likelihood estimators have mostly been used with general exponential or “maximum entropy” models because standard maximum likelihood estimation is usually computationally intractable (Berger et al., 1996; Della Pietra et al., 1997; Jelinek, 1997). Wellknown computational linguistic models such as (MLE) (MCLE) Ω Y = y i , X = xi Ω Y = y i , X = xi X = xi Figure 1: The MLE makes the training data (yi , xi ) as likely as possible (relative to Ω), while the MCLE makes (yi , xi ) as likely as possible relative to other pairs (y 0 , xi ). Maximum-Entropy Markov Models (McCallum et al., 2000) and Stochastic Unification-based Grammars (Johnson et al., 1999) are standardly estimated with conditional estimators, and it would be interesting to know whether conditional estimation affects the quality of t"
P01-1042,J93-1002,0,0.0176603,"nificant statistically). 4 Shift-reduce parsing The previous section compared similiar joint and conditional tagging models. This section compares a pair of joint and conditional parsing models. The models are both stochastic shift-reduce parsers; they differ only in how the distribution over possible next moves are calculated. These parsers are direct simplifications of the Structured Language Model (Jelinek, 2000). Because the parsers’ moves are determined solely by the top two category labels on the stack and possibly the look-ahead symbol, they are much simpler than stochastic LR parsers (Briscoe and Carroll, 1993; Inui et al., 1997). The distribution over trees generated by the joint model is a probabilistic context-free language (Abney et al., 1999). As with the PCFG models discussed earlier, these parsers are not lexicalized; lexical items are ignored, and the POS tags are used as the terminals. These two parsers only produce trees with unary or binary nodes, so we binarized the training data before training the parser, and debinarize the trees the parsers produce before evaluating them with respect to the test data (Johnson, 1998). We binarized by inserting n − 2 additional nodes into each local tr"
P02-1018,A00-2018,0,0.812769,"ring empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information. The patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it. This paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus. Evaluating the algorithm on the output of Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity. 1 Introduction One of the main motivations for research on parsing is that syntactic structure provides important information for semantic interpretation; hence syntactic parsing is an important first step in a variety of I would like to thank my colleages in the Brown Laboratory for Linguistic Information Processing (BLLIP) as well as Michael Collins for their advice. This research was supported by NSF awards DM"
P02-1018,P97-1003,0,0.0296799,"tic interpretation; hence syntactic parsing is an important first step in a variety of I would like to thank my colleages in the Brown Laboratory for Linguistic Information Processing (BLLIP) as well as Michael Collins for their advice. This research was supported by NSF awards DMS 0074276 and ITR IIS 0085940. ∗ useful tasks. Broad coverage syntactic parsers with good performance have recently become available (Charniak, 2000; Collins, 2000), but these typically produce as output a parse tree that only encodes local syntactic information, i.e., a tree that does not include any “empty nodes”. (Collins (1997) discusses the recovery of one kind of empty node, viz., WH-traces). This paper describes a simple patternmatching algorithm for post-processing the output of such parsers to add a wide variety of empty nodes to its parse trees. Empty nodes encode additional information about non-local dependencies between words and phrases which is important for the interpretation of constructions such as WH-questions, relative clauses, etc. 1 For example, in the noun phrase the man Sam likes the fact the man is interpreted as the direct object of the verb likes is indicated in Penn treebank notation by empty"
P02-1018,J98-4004,1,0.824572,"y compound SBAR subtree, as explained in the text and Figure 3. SINV S-1 , VP NP VP , VBD NNS VBD changes occured NP SBAR NNP said -NONE- S Sam 0 -NONE*T*-1 Figure 3: A parse tree containing an empty compound SBAR subtree. be regarded as an instance of the Memory-Based Learning approach, where both the pattern extraction and pattern matching involve recursively visiting all of the subtrees of the tree concerned. It can also be regarded as a kind of tree transformation, so the overall system architecture (including the parser) is an instance of the “transform-detransform” approach advocated by Johnson (1998). The algorithm has two phases. The first phase of the algorithm extracts the patterns from the trees in the training corpus. The second phase of the algorithm uses these extracted patterns to insert empty nodes and index their antecedents in trees that do not contain empty nodes. Before the trees are used in the training and insertion phases they are passed through a common preproccessing step, which relabels preterminal nodes dominating auxiliary verbs and transitive verbs. 2.1 Auxiliary and transitivity annotation The preprocessing step relabels auxiliary verbs and transitive verbs in all t"
P02-1018,J93-2004,0,0.0384199,"co-indexed antecedents in phrase structure trees that do not contain this information. The patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it. This paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus. Evaluating the algorithm on the output of Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity. 1 Introduction One of the main motivations for research on parsing is that syntactic structure provides important information for semantic interpretation; hence syntactic parsing is an important first step in a variety of I would like to thank my colleages in the Brown Laboratory for Linguistic Information Processing (BLLIP) as well as Michael Collins for their advice. This research was supported by NSF awards DMS 0074276 and ITR IIS 0085940. ∗ useful task"
P02-1035,W01-0521,0,0.118829,"Missing"
P02-1035,P99-1069,1,0.740485,"ts have so far been confined to a relatively small scale for various reasons. Firstly, the rudimentary character of functional annotations in standard treebanks has hindered the direct use of such data for statistical estimation of linguistically fine-grained statistical parsing systems. Rather, parameter estimation for such models had to resort to unsupervised techniques (Bouma et al., 2000; Riezler et al., 2000), or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences (Johnson et al., 1999). Furthermore, the effort involved in coding broadcoverage grammars by hand has often led to the specialization of grammars to relatively small domains, thus sacrificing grammar coverage (i.e. the percentage of sentences for which at least one analysis is found) on free text. The approach presented in this paper is a first attempt to scale up stochastic parsing systems based on linguistically fine-grained handcoded grammars to the UPenn Wall Street Journal (henceforth WSJ) treebank (Marcus et al., 1994). The problem of grammar coverage, i.e. the fact that not all sentences receive an analysis,"
P02-1035,H94-1020,0,0.0483752,"Missing"
P02-1035,J93-4001,1,0.513311,"on LFG parses and the discriminative statistical estimation technique. Experimental results are reported in section 4. A discussion of results is in section 5. 2 2.1 Robust Parsing using LFG A Broad-Coverage LFG The grammar used for this project was developed in the ParGram project (Butt et al., 1999). It uses LFG as a formalism, producing c(onstituent)-structures (trees) and f(unctional)-structures (attribute value matrices) as output. The c-structures encode constituency. F-structures encode predicate-argument relations and other grammatical information, e.g., number, tense. The XLE parser (Maxwell and Kaplan, 1993) was used to produce packed representations, specifying all possible grammar analyses of the input. The grammar has 314 rules with regular expression right-hand sides which compile into a collection of finite-state machines with a total of 8,759 states and 19,695 arcs. The grammar uses several lexicons and two guessers: one guesser for words recognized by the morphological analyzer but not in the lexicons and one for those not recognized. As such, most nouns, adjectives, and adverbs have no explicit lexical entry. The main verb lexicon contains 9,652 verb stems and 23,525 subcategorization fra"
P02-1035,P92-1017,0,0.21467,"Missing"
P02-1035,P00-1061,1,0.777157,"High versus low attachment is indicated by property functions counting the number of recursively embedded phrases. Other property functions are designed to refer to f-structure attributes, which correspond to grammatical functions in LFG, or to atomic attributevalue pairs in f-structures. More complex property functions are designed to indicate, for example, the branching behaviour of c-structures and the (non)parallelism of coordinations on both c-structure and f-structure levels. Furthermore, properties refering to lexical elements based on an auxiliary distribution approach as presented in Riezler et al. (2000) are included in the model. Here tuples of head words, argument words, and grammatical relations are extracted from the training sections of the WSJ, and fed into a finite mixture model for clustering grammatical relations. The clustering model itself is then used to yield smoothed probabilities as values for property functions on head-argument-relation tuples of LFG parses. 3.2 maximum likelihood estimation the joint probability of the training data to best describe observations is maximized. Since the discriminative task is kept in mind during estimation, discriminative methods can yield imp"
P02-1036,J97-4005,0,0.824827,"ed by NSF awards DMS 0074276 and ITR IIS 0085940. ∗ Mark Johnson Cognitive and Linguistic Sciences Brown University Mark Johnson@Brown.edu 1 Introduction Stochastic Unification-Based Grammars (SUBGs) use log-linear models (also known as exponential or MaxEnt models and Markov Random Fields) to define probability distributions over the parses of a unification grammar. These grammars can incorporate virtually all kinds of linguistically important constraints (including non-local and non-context-free constraints), and are equipped with a statistically sound framework for estimation and learning. Abney (1997) pointed out that the non-contextfree dependencies of a unification grammar require stochastic models more general than Probabilistic Context-Free Grammars (PCFGs) and Markov Branching Processes, and proposed the use of loglinear models for defining probability distributions over the parses of a unification grammar. Unfortunately, the maximum likelihood estimator Abney proposed for SUBGs seems computationally intractable since it requires statistics that depend on the set of all parses of all strings generated by the grammar. This set is infinite (so exhaustive enumeration is impossible) and p"
P02-1036,P99-1069,1,0.92069,"eral than Probabilistic Context-Free Grammars (PCFGs) and Markov Branching Processes, and proposed the use of loglinear models for defining probability distributions over the parses of a unification grammar. Unfortunately, the maximum likelihood estimator Abney proposed for SUBGs seems computationally intractable since it requires statistics that depend on the set of all parses of all strings generated by the grammar. This set is infinite (so exhaustive enumeration is impossible) and presumably has a very complex structure (so sampling estimates might take an extremely long time to converge). Johnson et al. (1999) observed that parsing and related tasks only require conditional distributions over parses given strings, and that such conditional distributions are considerably easier to estimate than joint distributions of strings and their parses. The conditional maximum likelihood estimator proposed by Johnson et al. requires statistics that depend on the set of all parses of the strings in the training corpus. For most linguistically realistic grammars this set is finite, and for moderate sized grammars and training corpora this estimation procedure is quite feasible. However, our recent experiments in"
P04-1005,N01-1016,1,0.447866,"antage over other kinds of models that they can in principle be integrated with other probabilistic models to produce a combined model that uses all available evidence to select the globally optimal analysis. Shriberg and Stolcke (1998) studied the location and distribution of repairs in the Switchboard corpus, but did not propose an actual model of repairs. Heeman and Allen (1999) describe a noisy channel model of speech repairs, but leave “extending the model to incorporate higher level syntactic . . . processing” to future work. The previous work most closely related to the current work is Charniak and Johnson (2001), who used a boosted decision stub classifier to classify words as edited or not on a word Eugene Charniak Brown University Providence, RI 02912 ec@cs.brown.edu by word basis, but do not identify or assign a probability to a repair as a whole. There are two innovations in this paper. First, we demonstrate that using a syntactic parser-based language model Charniak (2001) instead of bi/trigram language models significantly improves the accuracy of repair detection and correction. Second, we show how Tree Adjoining Grammars (TAGs) can be used to provide a precise formal description and probabili"
P04-1005,P01-1017,1,0.898728,"describe a noisy channel model of speech repairs, but leave “extending the model to incorporate higher level syntactic . . . processing” to future work. The previous work most closely related to the current work is Charniak and Johnson (2001), who used a boosted decision stub classifier to classify words as edited or not on a word Eugene Charniak Brown University Providence, RI 02912 ec@cs.brown.edu by word basis, but do not identify or assign a probability to a repair as a whole. There are two innovations in this paper. First, we demonstrate that using a syntactic parser-based language model Charniak (2001) instead of bi/trigram language models significantly improves the accuracy of repair detection and correction. Second, we show how Tree Adjoining Grammars (TAGs) can be used to provide a precise formal description and probabilistic model of the crossed dependencies occurring in speech repairs. The rest of this paper is structured as follows. The next section describes the noisy channel model of speech repairs and the section after that explains how it can be applied to detect and repair speech repairs. Section 4 evaluates this model on the Penn 3 disfluency-tagged Switchboard corpus, and secti"
P04-1005,C90-3045,0,0.235708,"age model. On the other hand, it seems desirable to use a language model that is sensitive to more global properties of the sentence, and we do this by reranking the initial analysis, replacing the bigram language model with a syntactic parser based model. We do not need to intersect this parser based language model with our TAG channel model since we evaluate each analysis separately. 2.2 The TAG channel model The TAG channel model defines a stochastic mapping of source sentences X into observed sentences Y . There are several ways to define transducers using TAGs such as Shieber and Schabes (1990), but the following simple method, inspired by finite-state transducers, suffices for the application here. The TAG defines a language whose vocabulary is the set of pairs (Σ∪{∅})×(Σ∪{∅}), where Σ is the vocabulary of the observed sentences Y . A string Z in this language can be interpreted as a pair of strings (Y, X), where Y is the concatenation of the projection of the first components of Z and X is the concatenation of the projection of the second components. For example, the string Z = a:a flight:flight to:∅ Boston:∅ uh:∅ I:∅ mean:∅ to:to Denver:Denver on:on Friday:Friday corresponds to t"
P04-1005,J99-4003,0,\N,Missing
P04-1005,W90-0102,0,\N,Missing
P04-1006,P99-1066,0,0.893203,"n 3 we use Bayes’ rule to find the optimal string given P (A|W ), the acoustic model, and P (W ), the language model. Although the language model can be used to rescore1 the word-lattice, it is typically used to select a single hypothesis. We focus our attention in this paper to syntactic language modeling techniques that perform complete parsing, meaning that parse trees are built upon the strings in the word-lattice. 2.1 n–best list reranking Much effort has been put forth in developing efficient probabilistic models for parsing strings (Caraballo and Charniak, 1998; Goldwater et al., 1998; Blaheta and Charniak, 1999; Charniak, 2000; Charniak, 2001); an obvious solution to parsing wordlattices is to use n–best list reranking. The n–best list reranking procedure, depicted in Figure 3, utilizes an external language model that selects a set of strings from the word-lattice. These strings are analyzed by the parser which computes a language model probability. This probability is combined (2) i=1 The result of the acoustic modeling process is a set 1 To rescore a word-lattice, each arch is assigned a new score (probability) defined by a new model (in combination with the acoustic model). 12/0 is/0 4 8 man/0 ma"
P04-1006,J98-2004,0,0.60286,"max P (A, W ) = arg max P (A|W )P (W ) (3) In Equation 3 we use Bayes’ rule to find the optimal string given P (A|W ), the acoustic model, and P (W ), the language model. Although the language model can be used to rescore1 the word-lattice, it is typically used to select a single hypothesis. We focus our attention in this paper to syntactic language modeling techniques that perform complete parsing, meaning that parse trees are built upon the strings in the word-lattice. 2.1 n–best list reranking Much effort has been put forth in developing efficient probabilistic models for parsing strings (Caraballo and Charniak, 1998; Goldwater et al., 1998; Blaheta and Charniak, 1999; Charniak, 2000; Charniak, 2001); an obvious solution to parsing wordlattices is to use n–best list reranking. The n–best list reranking procedure, depicted in Figure 3, utilizes an external language model that selects a set of strings from the word-lattice. These strings are analyzed by the parser which computes a language model probability. This probability is combined (2) i=1 The result of the acoustic modeling process is a set 1 To rescore a word-lattice, each arch is assigned a new score (probability) defined by a new model (in combinat"
P04-1006,A00-2018,0,0.877348,"nd the optimal string given P (A|W ), the acoustic model, and P (W ), the language model. Although the language model can be used to rescore1 the word-lattice, it is typically used to select a single hypothesis. We focus our attention in this paper to syntactic language modeling techniques that perform complete parsing, meaning that parse trees are built upon the strings in the word-lattice. 2.1 n–best list reranking Much effort has been put forth in developing efficient probabilistic models for parsing strings (Caraballo and Charniak, 1998; Goldwater et al., 1998; Blaheta and Charniak, 1999; Charniak, 2000; Charniak, 2001); an obvious solution to parsing wordlattices is to use n–best list reranking. The n–best list reranking procedure, depicted in Figure 3, utilizes an external language model that selects a set of strings from the word-lattice. These strings are analyzed by the parser which computes a language model probability. This probability is combined (2) i=1 The result of the acoustic modeling process is a set 1 To rescore a word-lattice, each arch is assigned a new score (probability) defined by a new model (in combination with the acoustic model). 12/0 is/0 4 8 man/0 man's/1.385 early/"
P04-1006,P01-1017,0,0.116367,"d towards word-lattice subsets for which there are few or no syntactic analyses posited. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model. 1 Introduction Success in language modeling has been dominated by the linear n-gram for the past few decades. A number of syntactic language models have proven to be competitive with the n-gram and better than the most popular n-gram, the trigram (Roark, 2001; Xu et al., 2002; Charniak, 2001; Hall and Johnson, 2003). Language modeling for speech could well be the first real problem for which syntactic techniques are useful. VP:ate PP:on VB NP IN PP:with NP:plate IN NP:fork John ate the pizza on a plate with a fork . Figure 1: An incomplete parse tree with head-word annotations. One reason that we expect syntactic models to perform well is that they are capable of modeling long-distance dependencies that simple n-gram ∗ This research was supported in part by NSF grants 9870676 and 0085940. models cannot. For example, the model presented by Chelba and Jelinek (Chelba and Jelinek, 1"
P04-1006,W98-1115,1,0.855083,"W )P (W ) (3) In Equation 3 we use Bayes’ rule to find the optimal string given P (A|W ), the acoustic model, and P (W ), the language model. Although the language model can be used to rescore1 the word-lattice, it is typically used to select a single hypothesis. We focus our attention in this paper to syntactic language modeling techniques that perform complete parsing, meaning that parse trees are built upon the strings in the word-lattice. 2.1 n–best list reranking Much effort has been put forth in developing efficient probabilistic models for parsing strings (Caraballo and Charniak, 1998; Goldwater et al., 1998; Blaheta and Charniak, 1999; Charniak, 2000; Charniak, 2001); an obvious solution to parsing wordlattices is to use n–best list reranking. The n–best list reranking procedure, depicted in Figure 3, utilizes an external language model that selects a set of strings from the word-lattice. These strings are analyzed by the parser which computes a language model probability. This probability is combined (2) i=1 The result of the acoustic modeling process is a set 1 To rescore a word-lattice, each arch is assigned a new score (probability) defined by a new model (in combination with the acoustic mo"
P04-1006,W97-0302,0,0.21582,"Missing"
P04-1006,J98-4004,1,0.89628,"Missing"
P04-1006,P03-1054,0,0.0673587,"Missing"
P04-1006,J93-2004,0,0.040572,"4 Experiments The purpose of attention shifting is to reduce the amount of work exerted by the first stage PCFG parser while maintaining the same quality of language modeling (in the multi-stage system). We have performed a set of experiments on the NIST ’93 HUB–1 word-lattices. The HUB–1 is a collection of 213 word-lattices resulting from an acoustic recognizer’s analysis of speech utterances. Professional readers reading Wall Street Journal articles generated the utterances. The first stage parser is a best-first PCFG parser trained on sections 2 through 22, and 24 of the Penn WSJ treebank (Marcus et al., 1993). Prior to training, the treebank is transformed into speech-like text, removing punctuation and expanding numerals, etc.5 Overparsing is performed using an edge pop6 multiplicative factor. The parser records the number of edge pops required to reach the first complete parse. The parser continues to parse a until multiple of the number of edge pops required for the first parse are popped off the agenda. The second stage parser used is a modified version of the Charniak language modeling parser described in (Charniak, 2001). We trained this parser 5 Brian Roark of AT&T provided a tool to perfor"
P04-1006,J01-2004,0,0.199827,"e parser’s attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model. 1 Introduction Success in language modeling has been dominated by the linear n-gram for the past few decades. A number of syntactic language models have proven to be competitive with the n-gram and better than the most popular n-gram, the trigram (Roark, 2001; Xu et al., 2002; Charniak, 2001; Hall and Johnson, 2003). Language modeling for speech could well be the first real problem for which syntactic techniques are useful. VP:ate PP:on VB NP IN PP:with NP:plate IN NP:fork John ate the pizza on a plate with a fork . Figure 1: An incomplete parse tree with head-word annotations. One reason that we expect syntactic models to perform well is that they are capable of modeling long-distance dependencies that simple n-gram ∗ This research was supported in part by NSF grants 9870676 and 0085940. models cannot. For example, the model presented by Chelba a"
P04-1006,P02-1025,0,0.0674733,"tention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model. 1 Introduction Success in language modeling has been dominated by the linear n-gram for the past few decades. A number of syntactic language models have proven to be competitive with the n-gram and better than the most popular n-gram, the trigram (Roark, 2001; Xu et al., 2002; Charniak, 2001; Hall and Johnson, 2003). Language modeling for speech could well be the first real problem for which syntactic techniques are useful. VP:ate PP:on VB NP IN PP:with NP:plate IN NP:fork John ate the pizza on a plate with a fork . Figure 1: An incomplete parse tree with head-word annotations. One reason that we expect syntactic models to perform well is that they are capable of modeling long-distance dependencies that simple n-gram ∗ This research was supported in part by NSF grants 9870676 and 0085940. models cannot. For example, the model presented by Chelba and Jelinek (Chelb"
P04-1006,H91-1042,0,\N,Missing
P04-1007,P03-1006,1,0.526898,"atures Φi for i = 1 . . . d implemented by D. The second concerns the choice of parameters αi for i = 0 . . . d which assign weights to the n-gram features as well as the baseline feature Φ0 . Before describing methods for training a discriminative language model using perceptron and CRF algorithms, we give a little more detail about the structure of D, focusing on how n-gram language models can be implemented with finite-state techniques. 3.3 Representation of n-gram language models An n-gram model can be efficiently represented in a deterministic WFA, through the use of failure transitions (Allauzen et al., 2003). Every string accepted by such an automaton has a single path through the automaton, and the weight of the string is the sum of the weights of the transitions in that path. In such a representation, every state in the automaton represents an n-gram history h, e.g. wi−2 wi−1 , and there are transitions leaving the state for every word wi such that the feature hwi has a weight. There is also a failure transition leaving the state, labeled with some reserved symbol φ, which can only be traversed if the next symbol in the input does not match any transition leaving the state. This failure transit"
P04-1007,W02-1001,1,0.422692,"eter estimation methods within the framework, the perceptron algorithm and a method based on conditional random fields. The linear models we describe are general enough to be applicable to a diverse range of NLP and speech tasks – this section gives a general description of the approach. In the next section of the paper we describe how global linear models can be applied to speech recognition. In particular, we focus on how the decoding and parameter estimation problems can be implemented over lattices using finite-state techniques. 2.1 Global linear models We follow the framework outlined in Collins (2002; 2004). The task is to learn a mapping from inputs x ∈ X to outputs y ∈ Y. We assume the following components: (1) Training examples (xi , yi ) for i = 1 . . . N . (2) A function GEN which enumerates a set of candidates GEN(x) for an input x. (3) A representation Φ mapping each (x, y) ∈ X × Y to a feature vector Φ(x, y) ∈ Rd . (4) A parameter vector α ¯ ∈ Rd . The components GEN, Φ and α ¯ define a mapping from an input x to an output F (x) through F (x) = argmax Φ(x, y) · α ¯ (1) y∈GEN(x) P where Φ(x, y) · α ¯ is the inner product s αs Φs (x, y). The learning task is to set the parameter val"
P04-1007,P99-1069,1,0.599285,"training algorithm in decoding heldout and test examples in our experiments. Say α ¯ it is the parameter vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the parameters α ¯ AV G are defined P averaged as α ¯ AV G = i,t α ¯ it /N T . Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α ¯ to define a conditional distribution over the members of GEN(x) for a given input x: 1 exp (Φ(x, y) · α ¯) Z(x, α ¯) P where Z(x, α ¯) = ¯ ) is a y∈GEN(x) exp (Φ(x, y) · α normalization constant that depends on x and α ¯. Given these definitions, the log-likelihood of the training data under parameters α ¯ is pα¯ (y|x) = LL(α) ¯ = N X log pα¯ (yi |xi ) N X [Φ(xi , yi ) · α ¯ − log Z(xi , α)] ¯ i=1 = (2) i=1 2.2 The Perceptron algorithm We no"
P04-1007,W02-2018,0,0.0613897,"ctive function: LLR (α) ¯ = N X [Φ(xi , yi ) · α ¯ − log Z(xi , α)] ¯ − i=1 ||α|| ¯ 2 2σ 2 (3) The value σ dictates the relative influence of the loglikelihood term vs. the prior, and is typically estimated using held-out data. The optimal parameters under this criterion are α ¯ ∗ = argmaxα¯ LLR (¯ α). We use a limited memory variable metric method (Benson and Mor´e, 2002) to optimize LLR . There is a general implementation of this method in the Tao/PETSc software libraries (Balay et al., 2002; Benson et al., 2002). This technique has been shown to be very effective in a variety of NLP tasks (Malouf, 2002; Wallach, 2002). The main interface between the optimizer and the training data is a procedure which takes a parameter vector α ¯ as input, and in turn returns LLR (¯ α) as well as the gradient of LLR at α ¯ . The derivative of the objective function with respect to a parameter αs at parameter values α ¯ is   N X X ∂LLR αs Φs (xi , yi ) − = pα ¯ (y|xi )Φs (xi , y)− 2 ∂αs σ i=1 (4) y∈GEN(xi ) Note that LLR (¯ α) is a convex function, so that there is a globally optimal solution and the optimization method 2 will find it. The use of the Gaussian prior term ||¯ α ||/2σ 2 in the objective fun"
P04-1007,W03-0430,0,0.0804031,"r vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the parameters α ¯ AV G are defined P averaged as α ¯ AV G = i,t α ¯ it /N T . Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α ¯ to define a conditional distribution over the members of GEN(x) for a given input x: 1 exp (Φ(x, y) · α ¯) Z(x, α ¯) P where Z(x, α ¯) = ¯ ) is a y∈GEN(x) exp (Φ(x, y) · α normalization constant that depends on x and α ¯. Given these definitions, the log-likelihood of the training data under parameters α ¯ is pα¯ (y|x) = LL(α) ¯ = N X log pα¯ (yi |xi ) N X [Φ(xi , yi ) · α ¯ − log Z(xi , α)] ¯ i=1 = (2) i=1 2.2 The Perceptron algorithm We now turn to methods for training the parameters α ¯ of the model, given a set of training examples 2 Note"
P04-1007,N03-1028,0,0.0263195,"α ¯ it is the parameter vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1. Then the parameters α ¯ AV G are defined P averaged as α ¯ AV G = i,t α ¯ it /N T . Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing (Ratnaparkhi et al., 1994; Johnson et al., 1999), and tagging or segmentation tasks (Lafferty et al., 2001; Sha and Pereira, 2003; McCallum and Li, 2003; Pinto et al., 2003). CRFs use the parameters α ¯ to define a conditional distribution over the members of GEN(x) for a given input x: 1 exp (Φ(x, y) · α ¯) Z(x, α ¯) P where Z(x, α ¯) = ¯ ) is a y∈GEN(x) exp (Φ(x, y) · α normalization constant that depends on x and α ¯. Given these definitions, the log-likelihood of the training data under parameters α ¯ is pα¯ (y|x) = LL(α) ¯ = N X log pα¯ (yi |xi ) N X [Φ(xi , yi ) · α ¯ − log Z(xi , α)] ¯ i=1 = (2) i=1 2.2 The Perceptron algorithm We now turn to methods for training the parameters α ¯ of the model, given a set of tr"
P05-1022,W97-0302,0,0.0593829,"ming states according to) features of the grandparent node in addition to the parent, thus multiplying the number of possible dynamic programming states even more. Thus nobody has implemented this version. There is, however, one particular feature of the Charniak parser that mitigates the space problem: it is a “coarse-to-fine” parser. By “coarse-to-fine” we mean that it first produces a crude version of the parse using coarse-grained dynamic programming states, and then builds fine-grained analyses by splitting the most promising of coarse-grained states. A prime example of this idea is from Goodman (1997), who describes a method for producing a simple but crude approximate grammar of a standard context-free grammar. He parses a sentence using the approximate grammar, and the results are used to constrain the search for a parse with the full CFG. He finds that total parsing time is greatly reduced. A somewhat different take on this paradigm is seen in the parser we use in this paper. Here the parser first creates a parse forest based upon a much less complex version of the complete grammar. In particular, it only looks at standard CFG features, the parent and neighbor labels. Because this gramm"
P05-1022,W05-1506,0,0.736476,"mal decision must be the second-best choice at that choice point. Further, the nth-best parse can only involve at most n suboptimal parsing decisions, and all but one of these must be involved in one of the second through the n − 1th-best parses. Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on. The algorithm was originally described for hidden Markov models. Since this first draft of this paper we have become aware of two PCFG implementations of this algorithm (Jimenez and Marzal, 2000; Huang and Chang, 2005). The first was tried on relatively small grammars, while the second was implemented on top of the Bikel re-implementation of the Collins parser (Bikel, 2004) and achieved oracle results for 50-best parses similar to those we report below. Here, however, we describe how to find n-best parses in a more straight-forward fashion. Rather than storing a single best parse of each edge, one stores n of them. That is, when using dynamic programming, rather than throwing away a candidate if it scores less than the best, one keeps it if it is one of the top n analyses for this edge discovered so far. Th"
P05-1022,J04-4004,0,0.0724418,"f these must be involved in one of the second through the n − 1th-best parses. Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on. The algorithm was originally described for hidden Markov models. Since this first draft of this paper we have become aware of two PCFG implementations of this algorithm (Jimenez and Marzal, 2000; Huang and Chang, 2005). The first was tried on relatively small grammars, while the second was implemented on top of the Bikel re-implementation of the Collins parser (Bikel, 2004) and achieved oracle results for 50-best parses similar to those we report below. Here, however, we describe how to find n-best parses in a more straight-forward fashion. Rather than storing a single best parse of each edge, one stores n of them. That is, when using dynamic programming, rather than throwing away a candidate if it scores less than the best, one keeps it if it is one of the top n analyses for this edge discovered so far. This is really very straight-forward. The problem is space. Dynamic programming parsing algorithms for PCFGs require O(m2 ) dynamic programming states, where m"
P05-1022,E03-1005,0,0.0193579,"Missing"
P05-1022,A00-2018,1,0.285717,"of the top n analyses for this edge discovered so far. This is really very straight-forward. The problem is space. Dynamic programming parsing algorithms for PCFGs require O(m2 ) dynamic programming states, where m is the length of the sentence, so an n-best parsing algorithm requires O(nm2 ). However things get much worse when the grammar is bilexicalized. As shown by Eisner (Eisner and Satta, 1999) the dynamic programming algorithms for bilexicalized PCFGs require O(m3 ) states, so a n-best parser would require O(nm3 ) states. Things become worse still in a parser like the one described in Charniak (2000) because it conditions on (and hence splits the dynamic programming states according to) features of the grandparent node in addition to the parent, thus multiplying the number of possible dynamic programming states even more. Thus nobody has implemented this version. There is, however, one particular feature of the Charniak parser that mitigates the space problem: it is a “coarse-to-fine” parser. By “coarse-to-fine” we mean that it first produces a crude version of the parse using coarse-grained dynamic programming states, and then builds fine-grained analyses by splitting the most promising"
P05-1022,P97-1003,0,0.194461,"-best parsing is straight-forward in best-first search or beam search approaches that do not use dynamic programming: to generate more than one parse, one simply allows the search mechanism to create successive versions to one’s heart’s content. A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning. At the end one has a beam-width’s number of best parses (Roark, 2001). The Collins parser (Collins, 1997) does use dynamic programming in its search. That is, whenever a constituent with the same history is generated a second time, it is discarded if its probability is lower than the original version. If the opposite is true, then the original is discarded. This is fine if one only 174 wants the first-best, but obviously it does not directly enumerate the n-best parses. However, Collins (Collins, 2000; Collins and Koo, in submission) has created an nbest version of his parser by turning off dynamic programming (see the user’s guide to Bikel’s re-implementation of Collins’ parser, http://www.cis.u"
P05-1022,P99-1059,0,0.620531,"hion. Rather than storing a single best parse of each edge, one stores n of them. That is, when using dynamic programming, rather than throwing away a candidate if it scores less than the best, one keeps it if it is one of the top n analyses for this edge discovered so far. This is really very straight-forward. The problem is space. Dynamic programming parsing algorithms for PCFGs require O(m2 ) dynamic programming states, where m is the length of the sentence, so an n-best parsing algorithm requires O(nm2 ). However things get much worse when the grammar is bilexicalized. As shown by Eisner (Eisner and Satta, 1999) the dynamic programming algorithms for bilexicalized PCFGs require O(m3 ) states, so a n-best parser would require O(nm3 ) states. Things become worse still in a parser like the one described in Charniak (2000) because it conditions on (and hence splits the dynamic programming states according to) features of the grandparent node in addition to the parent, thus multiplying the number of possible dynamic programming states even more. Thus nobody has implemented this version. There is, however, one particular feature of the Charniak parser that mitigates the space problem: it is a “coarse-to-fi"
P05-1022,P99-1069,1,0.0726976,"Missing"
P05-1022,P03-1054,0,0.178434,"s of speech of the lexical head and the functional head of nodes in parse trees. WProj (158,771) The instances of this schema are preterminals together with the categories of ` of their closest maximal projection ancestors. The parameters of this schema control the number ` of maximal projections, and whether the preterminals and the ancestors are lexicalized. 178 Word (49,097) The instances of this schema are lexical items together with the categories of ` of their immediate ancestor nodes, where ` is a schema parameter (` = 2 or ` = 3 here). This feature was inspired by a similar feature in Klein and Manning (2003). HeadTree (72,171) The instances of this schema are tree fragments consisting of the local trees consisting of the projections of a preterminal node and the siblings of such projections. This schema is parameterized by the head type (lexical or functional) used to determine the projections of a preterminal, and whether the head preterminal is lexicalized. NGramTree (291,909) The instances of this schema are subtrees rooted in the least common ancestor of ` contiguous preterminal nodes. This schema is parameterized by the number ` of contiguous preterminals (` = 2 or ` = 3 here) and whether th"
P05-1022,W02-2018,0,0.0557284,"he-art quality, but the reranker further improves the f -score. f (y)Pθ (y|Y) y∈Y 6 In the experiments reported here, we used a GausP 2 sian or quadratic regularizer R(w) = c m j=1 wj , where c is an adjustable parameter that controls the amount of regularization, chosen to optimize the reranker’s f -score on the development set (section 24 of the treebank). We used the Limited Memory Variable Metric optimization algorithm from the PETSc/TAO optimization toolkit (Benson et al., 2004) to find the optimal feature weights θˆ because this method seems substantially faster than comparable methods (Malouf, 2002). The PETSc/TAO toolkit provides a variety of other optimization algorithms and flags for controlling convergence, but preliminary experiments on the Collins’ trees with different algorithms and early stopping did not show any performance improvements, so we used the default PETSc/TAO setting for our experiments here. 5 f -score 0.9102 0.9037 Experimental results We evaluated the performance of our reranking parser using the standard PARSEVAL metrics. We 179 Conclusion This paper has described a dynamic programming n-best parsing algorithm that utilizes a heuristic coarse-to-fine refinement of"
P05-1022,P02-1035,1,0.123862,"heir correct parses y? (s1 ), . . . , y? (sn ). We used the 20-fold crossvalidation technique described in Collins (2000) to compute the n-best parses Y(s) for each sentence s in D. In general the correct parse y? (s) is not a member of Y(s), so instead we train the reranker to identify one of the best parses Y+ (s) = arg maxy∈Y(s) Fy? (s) (y) in the n-best parser’s output, where Fy? (y) is the Parseval f -score of y evaluated with respect to y? . Because there may not be a unique best parse for each sentence (i.e., |Y+ (s) |> 1 for some sentences s) we used the variant of MaxEnt described in Riezler et al. (2002) for partially labelled training data. n-best trees New Collins Recall the standard MaxEnt conditional probability model for a parse y ∈ Y: exp vθ (y) , where 0 y 0 ∈Y exp vθ (y ) P Pθ (y|Y) = vθ (y) = θ · f (y) = m X θj fj (y). j=1 The loss function LD proposed in Riezler et al. (2002) is just the negative log conditional likelihood of the best parses Y+ (s) relative to the n-best parser output Y(s): n X 0 LD (θ) = − log Pθ (Y+ (si )|Y(si )), where i=1 Pθ (Y+ |Y) = X Pθ (y|Y) y∈Y+ The partial derivatives of this loss function, which are required by the numerical estimation procedure, are: ∂LD"
P05-1022,J01-2004,0,0.463729,"θ that maximizes the score vθ (y) of the parses y ∈ Y+ (s) relative to the scores of the other parses in Y(s), for each s in the training data. 2 Recovering the n-best parses using coarse-to-fine parsing The major difficulty in n-best parsing, compared to 1-best parsing, is dynamic programming. For example, n-best parsing is straight-forward in best-first search or beam search approaches that do not use dynamic programming: to generate more than one parse, one simply allows the search mechanism to create successive versions to one’s heart’s content. A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning. At the end one has a beam-width’s number of best parses (Roark, 2001). The Collins parser (Collins, 1997) does use dynamic programming in its search. That is, whenever a constituent with the same history is generated a second time, it is discarded if its probability is lower than the original version. If the opposite is true, then the original is discarded. This is fine if one only 174 want"
P06-1043,W01-0521,0,0.742981,"lone WSJ+2,500k NANC B ROWN alone B ROWN+50k NANC B ROWN+250k NANC B ROWN+500k NANC WSJ+B ROWN WSJ+B ROWN +50k NANC WSJ+B ROWN +250k NANC WSJ+B ROWN +500k NANC Parser alone 83.9 86.4 86.3 86.8 86.8 86.7 86.5 86.8 86.8 86.6 Reranking parser 85.8 87.7 87.4 88.0 88.1 87.8 88.1 88.1 88.1 87.7 Table 3: f -scores from various combinations of WSJ, NANC, and B ROWN corpora on B ROWN development. The reranking parser used the WSJ-trained reranker model. The B ROWN parsing model is naturally better than the WSJ model for this task, but combining the two training corpora results in a better model (as in Gildea (2001)). Adding small amounts of NANC further improves the models. Parser model WSJ WSJ+NANC B ROWN Parser alone 82.9 87.1 86.7 WSJ-reranker 85.2 87.8 88.2 B ROWN-reranker 85.2 87.9 88.4 Table 5: Performance of various combinations of parser and reranker models when evaluated on B ROWN test. The WSJ+NANC parser with the WSJ reranker comes close to the B ROWN-trained reranking parser. The B ROWN reranker provides only a small improvement over its WSJ counterpart, which is not statistically significant. 342 Bracketing agreement f -score Complete match Average crossing brackets POS Tagging agreement 88"
P06-1043,P99-1010,0,0.0140006,"Missing"
P06-1043,I05-1006,1,0.53592,"a different class, and data from a different domain. He also notes that different domains have very different structures by looking at frequent grammar productions. For these reasons he takes the position that we should, instead, simply create treebanks for a large number of domains. While this is a coherent position, it is far from the majority view. There are many different approaches to parser adaptation. Steedman et al. (2003) apply cotraining to parser adaptation and find that cotraining can work across domains. The need to parse biomedical literature inspires (Clegg and Shepherd, 2005; Lease and Charniak, 2005). Clegg and Shepherd (2005) provide an extensive side-by-side performance analysis of several modern statistical parsers when faced with such data. They find that techniques which combine differ338 3 Corpora cleanups on NANC to ease parsing. NANC contains news articles from various news sources including the Wall Street Journal, though for this paper, we only use articles from the LA Times portion. To use the data from NANC, we use self-training (McClosky et al., 2006). First, we take a WSJ trained reranking parser (i.e. both the parser and reranker are built from WSJ training data) and parse"
P06-1043,J93-2004,0,0.0335602,"Missing"
P06-1043,P05-1022,1,0.281883,"s treebank. Furthermore, the treebanked Brown data is mostly general non-fiction and much closer to WSJ than, e.g., medical corpora would be. Thus, most work on parser adaptation resorts to using some labeled in-domain data to fortify the larger quantity of outof-domain data. In this paper, we present some encouraging results on parser adaptation without any in-domain data. (Though we also present results with indomain data as a reference point.) In particular we note the effects of two comparatively recent techniques for parser improvement. The first of these, parse-reranking (Collins, 2000; Charniak and Johnson, 2005) starts with a “standard” generative parser, but uses it to generate the n-best parses rather than a single parse. Then a reranking phase uses more detailed features, features which would (mostly) be impossible to incorporate in the initial phase, to reorder Statistical parsers trained and tested on the Penn Wall Street Journal (WSJ) treebank have shown vast improvements over the last 10 years. Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typically) the WSJ treebank data. This has led to concern that such parsers may be too finely tun"
P06-1043,W05-1102,0,0.0458518,"the same class, data from a different class, and data from a different domain. He also notes that different domains have very different structures by looking at frequent grammar productions. For these reasons he takes the position that we should, instead, simply create treebanks for a large number of domains. While this is a coherent position, it is far from the majority view. There are many different approaches to parser adaptation. Steedman et al. (2003) apply cotraining to parser adaptation and find that cotraining can work across domains. The need to parse biomedical literature inspires (Clegg and Shepherd, 2005; Lease and Charniak, 2005). Clegg and Shepherd (2005) provide an extensive side-by-side performance analysis of several modern statistical parsers when faced with such data. They find that techniques which combine differ338 3 Corpora cleanups on NANC to ease parsing. NANC contains news articles from various news sources including the Wall Street Journal, though for this paper, we only use articles from the LA Times portion. To use the data from NANC, we use self-training (McClosky et al., 2006). First, we take a WSJ trained reranking parser (i.e. both the parser and reranker are built from WS"
P06-1043,N06-1020,1,0.773237,"d find that cotraining can work across domains. The need to parse biomedical literature inspires (Clegg and Shepherd, 2005; Lease and Charniak, 2005). Clegg and Shepherd (2005) provide an extensive side-by-side performance analysis of several modern statistical parsers when faced with such data. They find that techniques which combine differ338 3 Corpora cleanups on NANC to ease parsing. NANC contains news articles from various news sources including the Wall Street Journal, though for this paper, we only use articles from the LA Times portion. To use the data from NANC, we use self-training (McClosky et al., 2006). First, we take a WSJ trained reranking parser (i.e. both the parser and reranker are built from WSJ training data) and parse the sentences from NANC with the 50-best (Charniak and Johnson, 2005) parser. Next, the 50-best parses are reordered by the reranker. Finally, the 1-best parses after reranking are combined with the WSJ training set to retrain the firststage parser.1 McClosky et al. (2006) find that the self-trained models help considerably when parsing WSJ. We primarily use three corpora in this paper. Selftraining requires labeled and unlabeled data. We assume that these sets of data"
P06-1043,A97-1015,0,0.114939,"se a “model-merging” (Bacchiani et al., 2006) approach. The different corpora are, in effect, concatenated together. However, (Bacchiani et al., 2006) achieve a larger gain by weighting the in-domain (Brown) data more heavily than the out-of-domain WSJ data. One can imagine, for instance, five copies of the Brown data concatenated with just one copy of WSJ data. 2 Related Work Work in parser adaptation is premised on the assumption that one wants a single parser that can handle a wide variety of domains. While this is the goal of the majority of parsing researchers, it is not quite universal. Sekine (1997) observes that for parsing a specific domain, data from that domain is most beneficial, followed by data from the same class, data from a different class, and data from a different domain. He also notes that different domains have very different structures by looking at frequent grammar productions. For these reasons he takes the position that we should, instead, simply create treebanks for a large number of domains. While this is a coherent position, it is far from the majority view. There are many different approaches to parser adaptation. Steedman et al. (2003) apply cotraining to parser ad"
P06-1043,E03-1008,0,0.736667,"esearchers, it is not quite universal. Sekine (1997) observes that for parsing a specific domain, data from that domain is most beneficial, followed by data from the same class, data from a different class, and data from a different domain. He also notes that different domains have very different structures by looking at frequent grammar productions. For these reasons he takes the position that we should, instead, simply create treebanks for a large number of domains. While this is a coherent position, it is far from the majority view. There are many different approaches to parser adaptation. Steedman et al. (2003) apply cotraining to parser adaptation and find that cotraining can work across domains. The need to parse biomedical literature inspires (Clegg and Shepherd, 2005; Lease and Charniak, 2005). Clegg and Shepherd (2005) provide an extensive side-by-side performance analysis of several modern statistical parsers when faced with such data. They find that techniques which combine differ338 3 Corpora cleanups on NANC to ease parsing. NANC contains news articles from various news sources including the Wall Street Journal, though for this paper, we only use articles from the LA Times portion. To use t"
P06-1085,P98-2206,0,0.0943693,"Missing"
P06-1085,J01-3002,0,0.142733,"s independent of its local context, while the second incorporates bigram dependencies between adjacent words. The algorithms we use to search for likely segmentations do differ, but so long as the segmentations they produce are close to optimal we can be confident that any differences in the segmentations reflect differences in the probabilistic models, i.e., in the kinds of dependencies between words. We are not the first to propose explicit probabilistic models of word segmentation. Two successful word segmentation systems based on explicit probabilistic models are those of Brent (1999) and Venkataraman (2001). Brent’s ModelBased Dynamic Programming (MBDP) system assumes a unigram word distribution. Venkataraman uses standard unigram, bigram, and trigram language models in three versions of his system, which we refer to as n-gram Segmentation (NGS). Despite their rather different generative structure, the MBDP and NGS segmentation accuracies are very similar. Moreover, the segmentation accuracy of the NGS unigram, bigram, and trigram models hardly differ, suggesting that contextual dependencies are irrelevant to word segmentation. HowDeveloping better methods for segmenting continuous text into wor"
P06-1085,J04-1004,0,0.104648,"Missing"
P06-1085,C98-2201,0,\N,Missing
P07-1022,J99-1004,0,0.0553987,"by a PBDG is the set of strings that have projective dependency parses generated by the grammar. The following dependency grammar generates the dependency parse in Figure 1. 0 gave Sandy gave gave dog the dog a bone gave bone This paper does not consider stochastic dependency grammars directly, but see Section 8 for an application involving them. However, it is straightforward to associate weights with dependencies, and since the dependencies are preserved by the transformations, obtain a weighted CFG. Standard methods for converting weighted CFGs to equivalent PCFGs can be used if required (Chi, 1999). Alternatively, one can transform a corpus of dependency parses into a corpus of the corresponding CFG parses, and estimate CFG production probabilities directly from that corpus. 3 A naive encoding of PBDGs There is a well-known method for encoding a PBDG as a CFG in which each terminal u ∈ Σ is associated with a corresponding nonterminal Xu that expands to u and all of u’s descendants. The nonterminals of the naive encoding CFG consist of the start symbol S and symbols Xu for each terminal u ∈ Σ, and S the productions of the CFG are the instances of the following schemata: S Xu Xu Xu → → →"
P07-1022,P99-1059,0,0.834017,"techniques developed for CFGs available to PBDGs. We demonstrate this by describing a maximum posterior parse decoder for PBDGs. 1 Introduction Projective Bilexical Dependency Grammars (PBDGs) have attracted attention recently for two reasons. First, because they capture bilexical head-tohead dependencies they are capable of producing extremely high-quality parses: state-of-the-art discriminatively trained PBDG parsers rival the accuracy of the very best statistical parsers available today (McDonald, 2006). Second, Eisner-Satta O(n3 ) PBDG parsing algorithms are extremely fast (Eisner, 1996; Eisner and Satta, 1999; Eisner, 2000). This paper investigates the relationship between Context-Free Grammar (CFG) parsing and the Eisner/Satta PBDG parsing algorithms, including their extension to second-order PBDG parsing (McDonald, 2006; McDonald and Pereira, 2006). Specifically, we show how to use an off-line preprocessing 168 step, the Unfold-Fold transformation, to transform a PBDG into an equivalent CFG that can be parsed in O(n3 ) time using a version of the CKY algorithm with suitable indexing (Younger, 1967), and extend this transformation so that it captures second-order PBDG dependencies as well. The tr"
P07-1022,C96-1058,0,0.045423,"kes all of the techniques developed for CFGs available to PBDGs. We demonstrate this by describing a maximum posterior parse decoder for PBDGs. 1 Introduction Projective Bilexical Dependency Grammars (PBDGs) have attracted attention recently for two reasons. First, because they capture bilexical head-tohead dependencies they are capable of producing extremely high-quality parses: state-of-the-art discriminatively trained PBDG parsers rival the accuracy of the very best statistical parsers available today (McDonald, 2006). Second, Eisner-Satta O(n3 ) PBDG parsing algorithms are extremely fast (Eisner, 1996; Eisner and Satta, 1999; Eisner, 2000). This paper investigates the relationship between Context-Free Grammar (CFG) parsing and the Eisner/Satta PBDG parsing algorithms, including their extension to second-order PBDG parsing (McDonald, 2006; McDonald and Pereira, 2006). Specifically, we show how to use an off-line preprocessing 168 step, the Unfold-Fold transformation, to transform a PBDG into an equivalent CFG that can be parsed in O(n3 ) time using a version of the CKY algorithm with suitable indexing (Younger, 1967), and extend this transformation so that it captures second-order PBDG depe"
P07-1022,P96-1024,0,0.0349847,"ve a bone . O(n3 ) time using the CKY algorithm. S Lu Lu L M v u L M v u R u R u R M u v R M u v M x y → → → → → → → → → → Lu u R ul L Lv v Mu vr Lu R M M v w w u ur R M R u v v R vl u L M M u w w v R Ly x where 0 u where v where v where v u u w where u where u where u where x, y v v w u ∈Σ u 8 Maximum posterior decoding As noted in the introduction, one consequence of the PBDG to CFG reductions presented in this paper is that CFG parsing and estimation techniques are now available for PBDGs as well. As an example application, this section describes Maximum Posterior Decoding (MPD) for PBDGs. Goodman (1996) observed that the Viterbi parse is in general not the optimal parse for evaluation metrics such as f-score that are based on the number of correct constituents in a parse. He showed that MPD improves f-score modestly relative to Viterbi decoding for PCFGs. Since dependency parse accuracy is just the proportion of dependencies in the parse that are correct, Goodman’s observation should hold for PBDG parsing as well. MPD for PBDGs selects the parse that maximizes the sum of the marginal probabilities of 174 each of the dependencies in the parse. Such a decoder might plausibly produce parses tha"
P07-1022,P05-1010,0,0.0392547,"ion so that it captures second-order PBDG dependencies as well. The transformations are ambiguity-preserving, i.e., there is a one-toone mapping between dependency parses and CFG parses, so it is possible to map the CFG parses back to the PBDG parses they correspond to. The PBDG to CFG reductions make techniques developed for CFGs available to PBDGs as well. For example, incremental CFG parsing algorithms can be used with the CFGs produced by this transform, as can the Inside-Outside estimation algorithm (Lari and Young, 1990) and more exotic methods such as estimating adjoined hidden states (Matsuzaki et al., 2005; Petrov et al., 2006). As an example application, we describe a maximum posterior parse decoder for PBDGs in Section 8. The Unfold-Fold transformation is a calculus for transforming functional and logic programs into equivalent but (hopefully) faster programs (Burstall and Darlington, 1977). We use it here to transform CFGs encoding dependency grammars into other CFGs that are more efficiently parsable. Since CFGs can be expressed as Horn-clause logic programs (Pereira and Shieber, 1987) and the UnfoldFold transformation is provably correct for such programs (Sato, 1992; Pettorossi and Proeit"
P07-1022,E06-1011,0,0.231418,"ns. First, because they capture bilexical head-tohead dependencies they are capable of producing extremely high-quality parses: state-of-the-art discriminatively trained PBDG parsers rival the accuracy of the very best statistical parsers available today (McDonald, 2006). Second, Eisner-Satta O(n3 ) PBDG parsing algorithms are extremely fast (Eisner, 1996; Eisner and Satta, 1999; Eisner, 2000). This paper investigates the relationship between Context-Free Grammar (CFG) parsing and the Eisner/Satta PBDG parsing algorithms, including their extension to second-order PBDG parsing (McDonald, 2006; McDonald and Pereira, 2006). Specifically, we show how to use an off-line preprocessing 168 step, the Unfold-Fold transformation, to transform a PBDG into an equivalent CFG that can be parsed in O(n3 ) time using a version of the CKY algorithm with suitable indexing (Younger, 1967), and extend this transformation so that it captures second-order PBDG dependencies as well. The transformations are ambiguity-preserving, i.e., there is a one-toone mapping between dependency parses and CFG parses, so it is possible to map the CFG parses back to the PBDG parses they correspond to. The PBDG to CFG reductions make techniques de"
P07-1022,P06-1055,0,0.0402298,"second-order PBDG dependencies as well. The transformations are ambiguity-preserving, i.e., there is a one-toone mapping between dependency parses and CFG parses, so it is possible to map the CFG parses back to the PBDG parses they correspond to. The PBDG to CFG reductions make techniques developed for CFGs available to PBDGs as well. For example, incremental CFG parsing algorithms can be used with the CFGs produced by this transform, as can the Inside-Outside estimation algorithm (Lari and Young, 1990) and more exotic methods such as estimating adjoined hidden states (Matsuzaki et al., 2005; Petrov et al., 2006). As an example application, we describe a maximum posterior parse decoder for PBDGs in Section 8. The Unfold-Fold transformation is a calculus for transforming functional and logic programs into equivalent but (hopefully) faster programs (Burstall and Darlington, 1977). We use it here to transform CFGs encoding dependency grammars into other CFGs that are more efficiently parsable. Since CFGs can be expressed as Horn-clause logic programs (Pereira and Shieber, 1987) and the UnfoldFold transformation is provably correct for such programs (Sato, 1992; Pettorossi and Proeitti, 1992), it follows"
P07-1104,W06-1655,1,0.814278,"Missing"
P07-1104,A00-2018,0,0.0713209,"ion (1) is used to discriminatively re-rank the candidate list using additional features which may or may not be included in the baseline model. Since 828 We follow the experimental paradigm of parse re-ranking outlined in Charniak and Johnson (2005), and fed the features extracted by their program to the five rerankers we developed. Each uses a linear model trained using one of the five estimators. These rerankers attempt to select the best parse ? for a sentence ? from the 50-best list of possible parses ??? ? for the sentence. The linear model combines the log probability calculated by the Charniak (2000) parser as a feature with 1,219,272 additional features. We trained the feaBaseline ME/L2 ME/L1 AP Boosting BLasso F-Score 0.8986 0.9176 0.9165 0.9164 0.9131 0.9133 # features time (min) # train iter 1,211,026 19,121 939,248 6,714 8,085 62 37 2 495 239 129 174 8 92,600 56,500 Table 1: Performance summary of estimators on parsing re-ranking (ME/L2: ME with L2 regularization; ME/L1: ME with L1 regularization) ME/L2 ME/L2 ME/L1 AP Boost Blasso &lt;&lt; ~ &lt;&lt; &lt;&lt; ME/L1 &gt;&gt; ~ &lt; ~ AP ~ ~ &lt;&lt; &lt; Boost &gt;&gt; &gt; &gt;&gt; BLasso &gt;&gt; ~ &gt; ~ ~ Table 2: Statistical significance test results (“&gt;&gt;” or “&lt;&lt;” means P-value &lt; 0.01; &gt;"
P07-1104,P05-1022,1,0.17373,"6), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularizat"
P07-1104,W02-1001,0,0.121044,"ures are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularization, outperformed Boosting in the LM adaptation task. Ng (2004) showed that for logistic regression, L1 regularization outperforms L2 regularization on artificial datasets which contain many completely irrelevant features. Goodman (2003) showed that in two out of three tasks, an ME estimator with a one-sided Laplacian prior ("
P07-1104,P06-1029,1,0.803398,"ely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularization, outperformed Boosting in the LM adaptation task. Ng (2004) showed that for logistic regression, L1 regularization outperforms L2 regularization on artificial datasets which contain many completely irrelevant features. Goodman (2003) showed that in two out of three tasks, an ME estimator with a one-sided Laplacian prior (i.e., L1 regularization with the constraint that all feature weights are positive) outperformed a comparable estimator using a Gaussian prior (i.e., L2 regularization). Riezler and Vasserman (2004) showed that a"
P07-1104,N04-1039,0,0.300415,"Missing"
P07-1104,P99-1069,1,0.742056,"n into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularization, outperformed Boos"
P07-1104,W03-1018,0,0.505913,"ith L1 regularization This estimator also minimizes the negative conditional log-likelihood, but uses an L1 (or Lasso) penalty. That is, ?(?) in Equation (2) is defined according to ? ? = ? ? ?? . L1 regularization typically leads to sparse solutions in which many feature weights are exactly zero, so it is a natural candidate when feature selection is desirable. By contrast, L2 regularization produces solutions in which most weights are small but non-zero. Optimizing the L1-regularized objective function is challenging because its gradient is discontinuous whenever some parameter equals zero. Kazama and Tsujii (2003) described an estimation method that constructs an equivalent constrained optimization problem with twice the number of variables. However, we found that this method is impractically slow for large-scale NLP tasks. In this work we use the orthant-wise limited-memory quasi-Newton algorithm (OWL-QN), which is a modification of L-BFGS that allows it to effectively handle the discontinuity of the gradient (Andrew and Gao 2007). We provide here a high-level description of the algorithm. A quasi-Newton method such as L-BFGS uses first order information at each iterate to build an approximation to th"
P07-1104,W02-2018,0,0.215148,"the parameters. Here,  is a parameter that controls the amount of regularization, optimized on held-out data. This is one of the most popular estimators, largely due to its appealing computational properties: both ? ? and ?(?) are convex and differentiable, so gradient-based numerical algorithms can be used to find the global minimum efficiently. In our experiments, we used the limited memory quasi-Newton algorithm (or L-BFGS, Nocedal and Wright 1999) to find the optimal ? because this method has been shown to be substantially faster than other methods such as Generalized Iterative Scaling (Malouf 2002). Because for some sentences there are multiple best parses (i.e., parses with the same F-Score), we used the variant of ME estimator described in Riezler et al. (2002), where ? ? is defined as the likelihood of the best parses ? ∈ ?(?) relative to the n-best parser output ??? ? , (i.e., ? ? ⊑ ???(?)): ? ? = − ??=1 log ? ? ∈?(? ? ) ?(?? |?? ). We applied this variant in our experiments of parse re-ranking and LM adaptation, and found that on both tasks it leads to a significant improvement in performance for the L2-regularied ME estimator but not for the L1-regularied ME estimator. 2.2 ME esti"
P07-1104,W04-3223,0,0.0632296,"ver a large number of weakly informative features. The first intuition motivates feature selection methods such as Boosting and BLasso (e.g., Collins 2000; Zhao and Yu, 2004), which usually work best when many features are completely irrelevant. L1 or Lasso regularization of linear models, introduced by Tibshirani (1996), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low comput"
P07-1104,P02-1035,1,0.821172,"gely due to its appealing computational properties: both ? ? and ?(?) are convex and differentiable, so gradient-based numerical algorithms can be used to find the global minimum efficiently. In our experiments, we used the limited memory quasi-Newton algorithm (or L-BFGS, Nocedal and Wright 1999) to find the optimal ? because this method has been shown to be substantially faster than other methods such as Generalized Iterative Scaling (Malouf 2002). Because for some sentences there are multiple best parses (i.e., parses with the same F-Score), we used the variant of ME estimator described in Riezler et al. (2002), where ? ? is defined as the likelihood of the best parses ? ∈ ?(?) relative to the n-best parser output ??? ? , (i.e., ? ? ⊑ ???(?)): ? ? = − ??=1 log ? ? ∈?(? ? ) ?(?? |?? ). We applied this variant in our experiments of parse re-ranking and LM adaptation, and found that on both tasks it leads to a significant improvement in performance for the L2-regularied ME estimator but not for the L1-regularied ME estimator. 2.2 ME estimation with L1 regularization This estimator also minimizes the negative conditional log-likelihood, but uses an L1 (or Lasso) penalty. That is, ?(?) in Equation (2) is"
P07-1104,N03-1033,1,0.14229,"ontext are ME models. Following previous work (Ratnaparkhi, 1996), we assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two words (i.e., ?=2 in the equation above). The local models at each position include features of the current word, the previous word, the next word, and features of the previous two tags. In addition to lexical identity of the words, we used features of word suffixes, capitalization, and number/special character signatures of the words. We used the standard splits of the Penn Treebank from the tagging literature (Toutanova et al. 2003) for training, development and test sets. The training set comprises Sections 0-18, the development set — Sections 19-21, and the test set — Sections 22-24. We compared training the ME models using L1 and L2 regularization. For each of the two types of regularization we selected the best value of the regularization constant using grid search to optimize the accuracy on the development set. We report final accuracy measures on the test set in Table 6. The results on this task confirm the trends we have seen so far. There is almost no difference in 3 Only the L2 vs. AP comparison is significant"
P07-1104,J05-1003,0,\N,Missing
P08-1046,W05-0615,1,0.9265,"rammar can capture is not common in the Brent corpus. It is possible that a more sophisticated model of morphology, or even a careful tuning of the Bayesian prior parameters α and β, would produce better results. 3.3 Unigram syllable adaptor grammar PCFG estimation procedures have been used to model the supervised and unsupervised acquisition of syllable structure (M¨uller, 2001; M¨uller, 2002); and the best performance in unsupervised acquisition is obtained using a grammar that encodes linguistically detailed properties of syllables whose rules are inferred using a fairly complex algorithm (Goldwater and Johnson, 2005). While that work studied the acquisition of syllable structure from isolated words, here we investigate whether learning syllable structure together with word segmentation improves word segmentation accuracy. Modeling syllable structure is a natural application of adaptor grammars, since the grammar can learn the possible onset and coda clusters, rather than requiring them to be stipulated in the grammar. In the unigram syllable adaptor grammar shown in Figure 7, Consonant expands to any consonant and Vowel expands to any vowel. This grammar defines a Word to consist of up to three Syllables,"
P08-1046,P06-1085,1,0.167583,"to models of unsupervised word segmentation. We follow previous work in using the Brent corpus consists of 9790 transcribed utterances (33,399 words) of childdirected speech from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) in the CHILDES database (MacWhinney and Snow, 1985). The utterances have been converted to a phonemic representation using a phonemic dictionary, so that each occurrence of a word has the same phonemic transcription. Utterance boundaries are given in the input to the system; other word boundaries are not. We evaluated the f-score of the recovered word constituents (Goldwater et al., 2006b). Using the adaptor grammar software available on the author’s web site, samplers were run for 10,000 epochs (passes through the training data). We scored the parses assigned to the training data at the end of sampling, and for the last two epochs we annealed at temperature 0.5 (i.e., squared the probability) during sampling in or1 10 100 1000 U word 0.55 0.55 0.55 0.53 U morph 0.46 0.46 0.42 0.36 0.52 0.51 0.49 0.46 U syll C word 0.53 0.64 0.74 0.76 C morph 0.56 0.63 0.73 0.63 0.77 0.77 0.78 0.74 C syll Sentence → Words Words → Word Words → Word Words Word → Phonemes Phonemes → Phoneme Phon"
P08-1046,N07-1018,1,0.783027,"leaves, but unlike a tree substitution grammar, in which the subtrees are specified in advance, in an adaptor grammar the subtrees, as well as their probabilities, are learnt from the training data. In order to make parsing and inference tractable we require the leaves of these subtrees to be terminals, as explained in section 2. Thus adaptor grammars are simple models of structure learning, where the subtrees that constitute the units of generalization are in effect new context-free rules learnt during the inference process. (In fact, the inference procedure for adaptor grammars described in Johnson et al. (2007b) relies on a PCFG approximation that contains a rule for each subtree generalization in the adaptor grammar). This paper applies adaptor grammars to word segmentation and morphological acquisition. Linguistically, these exhibit considerable cross-linguistic variation, and so are likely to be learned by human learners. It’s also plausible that semantics and contextual information is less important for their acquisition than, say, syntax. 399 2 From PCFGs to Adaptor Grammars This section introduces adaptor grammars as an extension of PCFGs; for a more detailed exposition see Johnson et al. (20"
P08-1046,D07-1072,0,0.377553,"models are not characterized by a finite vector of parameters, so the complexity of the model can vary depending on the data it describes). Adaptor grammars are a framework for specifying a wide range of such models for grammatical inference. They can be viewed as a nonparametric extension of PCFGs. Informally, there seem to be at least two natural ways to construct non-parametric extensions of a PCFG. First, we can construct an infinite number of more specialized PCFGs by splitting or refining the PCFG’s nonterminals into increasingly finer states; this leads to the iPCFG or “infinite PCFG” (Liang et al., 2007). Second, we can generalize over arbitrary subtrees rather than local trees in much the way done in DOP or tree substitution grammar (Bod, 1998; Joshi, 2003), which leads to adaptor grammars. Informally, the units of generalization of adaptor grammars are entire subtrees, rather than just local trees, as in PCFGs. Just as in tree substitution grammars, each of these subtrees behaves as a new context-free rule that expands the subtree’s root node to its leaves, but unlike a tree substitution grammar, in which the subtrees are specified in advance, in an adaptor grammar the subtrees, as well as"
P08-1046,P01-1053,0,0.0544288,"Missing"
P08-1046,W02-0608,0,0.0495472,"Missing"
P09-2085,D08-1033,0,0.0317874,"Missing"
P09-2085,P07-1035,0,0.0288854,"present two nonparametric Bayesian language models: a DP unigram model and an HDP bigram model. Under the DP model, words in a corpus w = w1 . . . wn are generated as follows: Unsupervised learning of natural language is one of the most challenging areas in NLP. Recently, methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation (Teh et al., 2006; Goldwater et al., 2006a; Goldwater et al., 2006b; Liang et al., 2007; Finkel et al., 2007; DeNero et al., 2008). These models are often based on the Dirichlet process (DP) (Ferguson, 1973) or hierarchical Dirichlet process (HDP) (Teh et al., 2006), with Gibbs sampling as a method of inference. Exact implementation of such sampling methods requires considerable bookkeeping of various counts, which motivated Goldwater et al. (2006a) (henceforth, GGJ06) to develop an approximation using expected counts. However, we show here that their approximation is flawed in two respects: 1) It omits an important factor in the expectation, and 2) Even after G|α0 , P0 wi |G ∼ DP(α0 , P0 ) ∼G where"
P09-2085,P06-1085,1,0.89977,"we present an efficient method for sampling from the HDP (and related models, such as the hierarchical PitmanYor process) that considerably decreases the memory footprint of such models as compared to the naive implementation. As we have noted, the issues described in this paper apply to models for various kinds of NLP tasks; for concreteness, we will focus on n-gram language modeling for the remainder of the paper, closely following the presentation in GGJ06. The implementation of collapsed Gibbs samplers for non-parametric Bayesian models is non-trivial, requiring considerable book-keeping. Goldwater et al. (2006a) presented an approximation which significantly reduces the storage and computation overhead, but we show here that their formulation was incorrect and, even after correction, is grossly inaccurate. We present an alternative formulation which is exact and can be computed easily. However this approach does not work for hierarchical models, for which case we present an efficient data structure which has a better space complexity than the naive approach. 1 2 Introduction The Chinese Restaurant Process GGJ06 present two nonparametric Bayesian language models: a DP unigram model and an HDP bigram"
P09-2085,D07-1072,0,0.0157343,"aurant Process GGJ06 present two nonparametric Bayesian language models: a DP unigram model and an HDP bigram model. Under the DP model, words in a corpus w = w1 . . . wn are generated as follows: Unsupervised learning of natural language is one of the most challenging areas in NLP. Recently, methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation (Teh et al., 2006; Goldwater et al., 2006a; Goldwater et al., 2006b; Liang et al., 2007; Finkel et al., 2007; DeNero et al., 2008). These models are often based on the Dirichlet process (DP) (Ferguson, 1973) or hierarchical Dirichlet process (HDP) (Teh et al., 2006), with Gibbs sampling as a method of inference. Exact implementation of such sampling methods requires considerable bookkeeping of various counts, which motivated Goldwater et al. (2006a) (henceforth, GGJ06) to develop an approximation using expected counts. However, we show here that their approximation is flawed in two respects: 1) It omits an important factor in the expectation, and 2) Even after G|α0 , P0 wi |G ∼"
P10-1117,N09-1019,1,0.856187,"ot indicate any structure internal to base noun phrases (i.e., it presents “flat” analyses of the pre-head NP elements). For many applications it would be extremely useful to have a more elaborated analysis of this kind of NP structure. For example, in an NP coreference application, if we could determine that Bill and Hillary are both first 1154 names then we could infer that Bill Clinton and Hillary Clinton are likely to refer to distinct individuals. On the other hand, because Mr in Mr Clinton is not a first name, it is possible that Mr Clinton and Bill Clinton refer to the same individual (Elsner et al., 2009). Here we present an adaptor grammar based on the insights of the PCFG encoding of LDA topic models that learns some of the structure of proper names. The key idea is that elements in proper names typically appear in a fixed order; we expect honorifics to appear before first names, which appear before middle names, which in turn appear before surnames, etc. Similarly, many company names end in fixed phrases such as Inc. Here we think of first names as a kind of topic, albeit one with a restricted positional location. One of the challenges is that some of these structural elements can be filled"
P10-1117,N09-1036,1,0.824344,"R as potentially unbounded, and try to learn the rules required to describe a training corpus as well as their probabilities. Adaptor grammars are an example of this approach (Johnson et al., 2007b), where entire subtrees generated by a “base grammar” can be viewed as distinct rules (in that we learn a separate probability for each subtree). The inference task is non-parametric if there are an unbounded number of such subtrees. We review the adaptor grammar generative process below; for an informal introduction see Johnson (2008) and for details of the adaptor grammar inference procedure see Johnson and Goldwater (2009). An adaptor grammar (N, W, R, S, θ, A, C) consists of a PCFG (N, W, R, S, θ) in which a subset A ⊆ N of the nonterminals are adapted, and where each adapted nonterminal X ∈ A has an associated adaptor CX . An adaptor CX for X is a function that maps a distribution over trees TX to a distribution over distributions over TX (we give examples of adaptors below). Just as for a PCFG, an adaptor grammar defines distributions GX over trees TX for each X ∈ N ∪ W . If X ∈ W or X 6∈ A then GX is defined just as for a PCFG above, i.e., using (1). However, if X ∈ A then GX is defined in terms of an addit"
P10-1117,N07-1018,1,0.788545,"ite NP coreference models and other applications. The rest of this paper is structured as follows. The next section reviews Latent Dirichlet Allocation (LDA) topic models, and the following section reviews Probabilistic Context-Free Grammars (PCFGs). Section 4 shows how an LDA topic model can be expressed as a PCFG, which provides the fundamental connection between LDA and PCFGs that we exploit in the rest of the paper, and shows how it can be used to define a “sticky topic” version of LDA. The following section reviews Adaptor Grammars (AGs), a non-parametric extension of PCFGs introduced by Johnson et al. (2007b). Section 6 exploits the connection between LDA and PCFGs to propose an AG-based topic model that extends LDA by defining distributions over collocations rather than individual words, and section 7 applies this extension to the problem of finding the structure of proper names. 2 Latent Dirichlet Allocation Models Latent Dirichlet Allocation (LDA) was introduced as an explicit probabilistic counterpart to Latent Semantic Indexing (LSI) (Blei et al., 2003). Like LSI, LDA is intended to produce a lowdimensional characterisation or summary of a doc1148 Proceedings of the 48th Annual Meeting of t"
P10-1117,P08-1046,1,0.915017,"2007). In the second approach, which we adopt here, we regard the set of rules R as potentially unbounded, and try to learn the rules required to describe a training corpus as well as their probabilities. Adaptor grammars are an example of this approach (Johnson et al., 2007b), where entire subtrees generated by a “base grammar” can be viewed as distinct rules (in that we learn a separate probability for each subtree). The inference task is non-parametric if there are an unbounded number of such subtrees. We review the adaptor grammar generative process below; for an informal introduction see Johnson (2008) and for details of the adaptor grammar inference procedure see Johnson and Goldwater (2009). An adaptor grammar (N, W, R, S, θ, A, C) consists of a PCFG (N, W, R, S, θ) in which a subset A ⊆ N of the nonterminals are adapted, and where each adapted nonterminal X ∈ A has an associated adaptor CX . An adaptor CX for X is a function that maps a distribution over trees TX to a distribution over distributions over TX (we give examples of adaptors below). Just as for a PCFG, an adaptor grammar defines distributions GX over trees TX for each X ∈ N ∪ W . If X ∈ W or X 6∈ A then GX is defined just as"
P10-1117,D07-1072,0,0.0758782,"the focus of intense research in machine learning recently. In the topicmodelling community this has lead to work on Dirichlet Processes and Chinese Restaurant Processes, which can be used to estimate the number of topics as well as their distribution across documents (Teh et al., 2006). There are two obvious non-parametric extensions to PCFGs. In the first we regard the set of nonterminals N as potentially unbounded, and try to learn the set of nonterminals required to describe the training corpus. This approach goes under the name of the “infinite HMM” or “infinite PCFG” (Beal et al., 2002; Liang et al., 2007; Liang et al., 2009). Informally, we are given a set of “basic categories”, say NP, VP, etc., and a set of rules that use these basic categories, say S → NP VP. The inference task is to learn a set of refined categories and rules (e.g., S7 → NP2 VP5 ) as well as their probabilities; this approach can therefore be viewed as a Bayesian version of the “split-merge” approach to grammar induction (Petrov and Klein, 2007). In the second approach, which we adopt here, we regard the set of rules R as potentially unbounded, and try to learn the rules required to describe a training corpus as well as t"
P10-1117,J93-2004,0,0.0426253,"ic11 studied) (Topic19 pattern recognition algorithms) 4 (Topic2 feedforward neural network) (Topic1 implementation) 5 (Topic11 single) (Topic10 ocular dominance stripe) (Topic12 low) (Topic3 ocularity) (Topic12 drift rate) 7 Finding the structure of proper names Grammars offer structural and positional sensitivity that is not exploited in the basic LDA topic models. Here we explore the potential for using Bayesian inference for learning linear ordering constraints that hold between elements within proper names. The Penn WSJ treebank is a widely used resource within computational linguistics (Marcus et al., 1993), but one of its weaknesses is that it does not indicate any structure internal to base noun phrases (i.e., it presents “flat” analyses of the pre-head NP elements). For many applications it would be extremely useful to have a more elaborated analysis of this kind of NP structure. For example, in an NP coreference application, if we could determine that Bill and Hillary are both first 1154 names then we could infer that Bill Clinton and Hillary Clinton are likely to refer to distinct individuals. On the other hand, because Mr in Mr Clinton is not a first name, it is possible that Mr Clinton an"
P10-1117,N07-1051,0,0.0124965,"unded, and try to learn the set of nonterminals required to describe the training corpus. This approach goes under the name of the “infinite HMM” or “infinite PCFG” (Beal et al., 2002; Liang et al., 2007; Liang et al., 2009). Informally, we are given a set of “basic categories”, say NP, VP, etc., and a set of rules that use these basic categories, say S → NP VP. The inference task is to learn a set of refined categories and rules (e.g., S7 → NP2 VP5 ) as well as their probabilities; this approach can therefore be viewed as a Bayesian version of the “split-merge” approach to grammar induction (Petrov and Klein, 2007). In the second approach, which we adopt here, we regard the set of rules R as potentially unbounded, and try to learn the rules required to describe a training corpus as well as their probabilities. Adaptor grammars are an example of this approach (Johnson et al., 2007b), where entire subtrees generated by a “base grammar” can be viewed as distinct rules (in that we learn a separate probability for each subtree). The inference task is non-parametric if there are an unbounded number of such subtrees. We review the adaptor grammar generative process below; for an informal introduction see Johns"
P10-2040,W00-0717,0,0.0136914,"Missing"
P10-2040,E03-1009,0,0.737372,"Missing"
P10-2040,W02-1001,0,0.0336461,"Missing"
P10-2040,D08-1036,1,0.871197,"criptors of the k most frequent words in the corpus. The final tag of any token in the corpus is the cluster number of its type. 3 Data and Evaluation We ran the SVD2 algorithm described above on the full Wall Street Journal part of the Penn Treebank (1,173,766 tokens). Capitalization was ignored, resulting in Ntypes = 43,766, with only a minor effect on accuracy. Evaluation was done against the POS-tag annotations of the 45-tag PTB tagset (hereafter PTB45), and against the Smith and Eisner (2005) coarse version of the PTB tagset (hereafter PTB17). We selected the three evaluation criteria of Gao and Johnson (2008): M-to-1, 1-to-1, and VI. M-to-1 and 1-to1 are the tagging accuracies under the best manyto-one map and the greedy one-to-one map respectively; VI is a map-free informationtheoretic criterion—see Gao and Johnson (2008) for details. Although we find M-to-1 to be the most reliable criterion of the three, we include the other two criteria for completeness. In addition to the best M-to-1 map, we also employ here, for large values of k2, a prototypebased M-to-1 map. To construct this map, we first find, for each induced tag t, the word type with which it co-occurs most frequently; we call this word"
P10-2040,P07-1094,0,0.658256,"Missing"
P10-2040,N06-1041,0,0.67836,"Missing"
P10-2040,C08-1042,0,0.0853855,"he SVD2 algorithm is its ability to produce a fine-grained labeling of the data, using a number of clusters much larger than the number of tags 218 in a syntax-motivated POS-tag system. Such fine-grained labelings can capture additional linguistic features. To achieve a fine-grained labeling, only the final clustering step in the SVD2 algorithm needs to be changed; the computational cost this entails is negligible. A high-quality fine-grained labeling, such as achieved by the SVD2 approach, may be of practical interest as an input to various types of unsupervised grammar-induction algorithms (Headden et al. 2008). This application is left for future work. Prototype-based tagging. One potentially important practical application of a high-quality fine-grained labeling is its use for languages which lack any kind of annotated data. By first applying the SVD2 algorithm, word types are grouped together into a few hundred clusters. Then, a prototype word is automatically extracted from each cluster. This produces, in a completely unsupervised way, a list of only a few hundred words that need to be hand-tagged by an expert. The results shown in Fig. 1 indicate that these prototype tags can then be used to ta"
P10-2040,D07-1031,1,0.770504,"the VI scores for SVD2 are 0.938 for PTB17 and 0.885 for PTB45. To examine the sensitivity of the algorithm to its four parameters, w1, r1, k1, and r2, we changed each of these parameters separately by a multiplicative factor of either 0.5 or 2; in neither case did M-to-1 accuracy drop by more than 0.014. This performance was achieved despite the fact that the SVD2 tagger is mathematically much simpler than the other models. Our MATLAB implementation of SVD2 takes only a few minutes to run on a desktop computer, in contrast to HMM training times of several hours or days (Gao and Johnson 2008; Johnson 2007). High-k performance. Not suffering from the same computational limitations as other models, SVD2 can easily accommodate high numbers of induced tags, resulting in fine-grained labelings. The value of this flexibility is discussed in the next section. Figure 1 shows, as a function of k2, the tagging accuracy of SVD2 under both the best and the prototype-based M-to-1 maps (see Section 3), for both the PTB45 and the PTB17 tagsets. The horizontal one-tag-per-word-type line in each panel is the theoretical upper limit for tagging accuracy in non-disambiguating models (such as SVD2). This limit is"
P10-2040,W09-1121,0,0.0103311,"nder the best M-to-1 map, the greedy 1-to-1 map, and VI, for the full PTB45 tagset and the reduced PTB17 tagset. HMM-EM, HMM-VB and HMM-GS show the best results from Gao and Johnson (2008); HMM-Sparse(32) and VEM (10-1,10-1) show the best results from Graça et al. (2009). The performance of SVD2 compares favorably to the HMM models. Note that SVD2 is a deterministic algorithm. The table shows, in parentheses, the standard deviations reported in Graça et al. (2009). For the sake of comparison with Graça et al. (2009), we also note that, with k2 = 45, SVD2 scores 0.659 on PTB45. The NVI scores (Reichart and Rappoport 2009) corresponding to the VI scores for SVD2 are 0.938 for PTB17 and 0.885 for PTB45. To examine the sensitivity of the algorithm to its four parameters, w1, r1, k1, and r2, we changed each of these parameters separately by a multiplicative factor of either 0.5 or 2; in neither case did M-to-1 accuracy drop by more than 0.014. This performance was achieved despite the fact that the SVD2 tagger is mathematically much simpler than the other models. Our MATLAB implementation of SVD2 takes only a few minutes to run on a desktop computer, in contrast to HMM training times of several hours or days (Gao"
P10-2040,E95-1020,0,0.924745,"effort than the best currently available models. Third, it demonstrates that state-of-the-art accuracy can be realized without disambiguation, i.e., without attempting to assign different tags to different tokens of the same type. Finally, with no significant increase in computational cost, SVD2 can create much finer-grained labelings than typically produced by other algorithms. When combined with some minimal supervision in postprocessing, this makes the approach useful for tagging languages that lack the resources required by fully supervised models. 2 Methods Following the original work of Schütze (1995), we begin by constructing a right context matrix, R, and a left context matrix, L. Rij counts the number of times in the corpus a token of word type i is immediately followed by a token of word type j. Similarly, Lij counts the number of times a token of type i is preceded by a token of type j. We truncate these matrices, including, in the right and left contexts, only the w1 most frequent word types. The resulting L and R are of dimension Ntypes×w1, where Ntypes is the number of word types (spelling forms) in the corpus, and w1 is set to 1000. (The full Ntypes× Ntypes context matrices satisf"
P10-2040,P05-1044,0,0.218985,"d tags. We use the same weighted k-means algorithm as in the first pass, again placing the k initial centroids on the descriptors of the k most frequent words in the corpus. The final tag of any token in the corpus is the cluster number of its type. 3 Data and Evaluation We ran the SVD2 algorithm described above on the full Wall Street Journal part of the Penn Treebank (1,173,766 tokens). Capitalization was ignored, resulting in Ntypes = 43,766, with only a minor effect on accuracy. Evaluation was done against the POS-tag annotations of the 45-tag PTB tagset (hereafter PTB45), and against the Smith and Eisner (2005) coarse version of the PTB tagset (hereafter PTB17). We selected the three evaluation criteria of Gao and Johnson (2008): M-to-1, 1-to-1, and VI. M-to-1 and 1-to1 are the tagging accuracies under the best manyto-one map and the greedy one-to-one map respectively; VI is a map-free informationtheoretic criterion—see Gao and Johnson (2008) for details. Although we find M-to-1 to be the most reliable criterion of the three, we include the other two criteria for completeness. In addition to the best M-to-1 map, we also employ here, for large values of k2, a prototypebased M-to-1 map. To construct t"
P10-2040,N03-1033,0,0.122559,"Missing"
P11-1071,P06-2101,0,0.21064,"t. grounded and provide a large performance increase. Third, we utilise a loss function, approximate expected f-score, that explicitly targets the asymmetric 1 Introduction evaluation metrics used in the disfluency detection Most spontaneous speech contains disfluencies such task. We explain how to optimise this loss funcas partial words, filled pauses (e.g., “uh”, “um”, tion, and show that this leads to a marked improve“huh”), explicit editing terms (e.g., “I mean”), par- ment in disfluency detection. This is consistent with enthetical asides and repairs. Of these, repairs Jansche (2005) and Smith and Eisner (2006), who pose particularly difficult problems for parsing and observed similar improvements when using approxrelated Natural Language Processing (NLP) tasks. imate f-score loss for other problems. Similarly we This paper presents a model of disfluency detec- introduce a loss function based on the edit-f-score in tion based on the noisy channel framework, which703 our domain. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 703–711, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Together, these three improvements a"
P11-1071,H05-1030,1,0.885332,"Missing"
P11-1071,J10-1001,0,0.120729,"Missing"
P11-1071,H94-1016,0,0.149122,"ble as the upper bound on a model using the first 25 best analyses. We therefore use the top 25 analyses from the noisy channel model in the remainder of this paper and use a reranker to choose the most suitable candidate among these. 6 Corpora for language modelling We would like to use additional data to model the fluent part of spoken language. However, the Switchboard corpus is one of the largest widelyavailable disfluency-annotated speech corpora. It is reasonable to believe that for effective disfluency detection Switchboard is not large enough and more text can provide better analyses. Schwartz et al. (1994), although not focusing on disfluency detection, show that using written language data for modelling spoken language can improve performance. We turn to three other bodies of text and investigate the use of these corpora for our task, disfluency detection. We will describe these corpora in detail here. The predictions made by several language models are likely to be strongly correlated, even if the language models are trained on different corpora. This motivates the choice for log-linear learners, which are built to handle features which are not necessarily independent. We incorporate informat"
P11-1071,N01-1016,1,\N,Missing
P11-1071,H01-1052,0,\N,Missing
P11-1071,N04-4040,0,\N,Missing
P11-1071,P04-1005,1,\N,Missing
P11-1071,H05-1087,0,\N,Missing
P11-1071,P06-1071,0,\N,Missing
P12-1051,D11-1131,1,0.826426,"Missing"
P12-1051,N10-1068,0,0.0115772,"g by presenting a tree transducer model and drawing connections to other similar systems. We make a further contribution by bringing to tree transducers the benefits of the Bayesian framework for principled handling of data sparsity and 488 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 488–496, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics prior knowledge. Graehl et al. (2008) present an EM training procedure for top down tree transducers, but while there are Bayesian approaches to string transducers (Chiang et al., 2010) and PCFGs (Kurihara and Sato, 2006), there has yet to be a proposal for Bayesian inference in tree transducers. Our variational algorithm produces better semantic parses than EM while remaining general to a broad class of transducers appropriate for other domains. In short, our contributions are three-fold: we present a new state-of-the-art semantic parsing model, propose a broader theory for tree transformation based semantic parsing, and present a general inference algorithm for the tree transducer framework. We recommend the last of these as just one benefit of working within a general the"
P12-1051,C69-0101,0,0.627125,"). We indicate states using all capital letters: NUM → population(PLACE). Intuitively, an RTG is a CFG where the yield of every parse is itself a tree. In fact, for any CFG G, it 1 See Liang et al. (2011) for work in representing lambda calculus expressions with trees. 489 is straightforward to produce a corresponding RTG that generates the set of parses of G. Consequently, while we assume we have an RTG for the MR language, there is no loss of generality if the MR language is actually context free. 3 Weighted root-to-frontier, linear, non-deleting tree-to-string transducers Tree transducers (Rounds, 1970; Thatcher, 1970) are generalizations of finite state machines that operate on trees. Mirroring the branching nature of its input, the transducer may simultaneously transition to several successor states, assigning a separate state to each subtree. There are many classes of transducer with different formal properties (Knight and Greahl, 2005; Maletti et al., 2009). Figure 1(c) is an example of a root-to-frontier, linear, non-deleting tree-to-string transducer. It is defined using rules where the left hand side identifies a state of the transducer and a fragment of the input tree, and the right"
P12-1051,N04-1035,0,0.0212804,"ransformative machine that takes one as input and produces another as output. Different semantic parsing approaches have taken one or the other view, and both can be captured in this single framework. WASP (Wong and Mooney, 2006) is an example of the former perspective, coupling the generation of the MR and NL with a synchronous grammar, a formalism closely related to tree transducers. The most significant difference from our approach is that they use machine translation techniques for automatically extracting rules from parallel corpora; similar techniques can be applied to tree transducers (Galley et al., 2004). In fact, synchronous grammars and tree transducers can be seen as instances of the same more general class of automata (Shieber, 3 The addition of W symbols is a convenience; it is easier to design transducer rules where every substring on the right side corresponds to a subtree on the left. Figure 3: Coupled derivation of an (MR, NL) pair. At each step an MR grammar rule is chosen to expand the MR and the corresponding portion of the NL is then generated. Symbols W stand for locations in the tree corresponding to substrings of the output and are removed in a post-processing step. (a) The (M"
P12-1051,W04-3312,0,0.198227,"Missing"
P12-1051,W05-0602,0,0.620725,"e mapping. Introduction Semantic parsing is the task of mapping natural language sentences to a formal representation of meaning. Typically, a system is trained on pairs of natural language sentences (NLs) and their meaning representation expressions (MRs), as in figure 1(a), and the system must generalize to novel sentences. Most semantic parsing models rely on an assumption of structural similarity between MR and NL. Since strict isomorphism is overly restrictive, this assumption is often relaxed by applying transformations. Several approaches assume a tree structure to the NL, MR, or both (Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; B¨orschinger et al., 2011), and often involve tree transformations either between two trees or a tree and a string. The tree transducer, a formalism from automata theory which has seen interest in machine translation (Yamada and Knight, 2001; Graehl et al., 2008) and has potential applications in many other areas, is well suited to formalizing such tree transformation based models. Yet, while many semantic parsing systems resemble the formalism, each was proposed as an independent model requiring custom algorithms, leaving it unc"
P12-1051,J08-3004,0,0.145628,"Most semantic parsing models rely on an assumption of structural similarity between MR and NL. Since strict isomorphism is overly restrictive, this assumption is often relaxed by applying transformations. Several approaches assume a tree structure to the NL, MR, or both (Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; B¨orschinger et al., 2011), and often involve tree transformations either between two trees or a tree and a string. The tree transducer, a formalism from automata theory which has seen interest in machine translation (Yamada and Knight, 2001; Graehl et al., 2008) and has potential applications in many other areas, is well suited to formalizing such tree transformation based models. Yet, while many semantic parsing systems resemble the formalism, each was proposed as an independent model requiring custom algorithms, leaving it unclear how developments in one line of inquiry relate to others. We argue for a unifying theory of tree transformation based semantic parsing by presenting a tree transducer model and drawing connections to other similar systems. We make a further contribution by bringing to tree transducers the benefits of the Bayesian framewor"
P12-1051,P06-1115,0,0.788705,"on Semantic parsing is the task of mapping natural language sentences to a formal representation of meaning. Typically, a system is trained on pairs of natural language sentences (NLs) and their meaning representation expressions (MRs), as in figure 1(a), and the system must generalize to novel sentences. Most semantic parsing models rely on an assumption of structural similarity between MR and NL. Since strict isomorphism is overly restrictive, this assumption is often relaxed by applying transformations. Several approaches assume a tree structure to the NL, MR, or both (Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; B¨orschinger et al., 2011), and often involve tree transformations either between two trees or a tree and a string. The tree transducer, a formalism from automata theory which has seen interest in machine translation (Yamada and Knight, 2001; Graehl et al., 2008) and has potential applications in many other areas, is well suited to formalizing such tree transformation based models. Yet, while many semantic parsing systems resemble the formalism, each was proposed as an independent model requiring custom algorithms, leaving it unclear how developments i"
P12-1051,D10-1119,1,0.506978,"ith the most probable derivation. 8 Experimental setup and evaluation We evaluate the system on GeoQuery (Wong and Mooney, 2006), a parallel corpus of 880 English questions and database queries about United States geography, 250 of which were translated into Spanish, Japanese, and Turkish. We present here additional translations of the full 880 sentences into 4 Because of the resemblance to EM, this procedure has been called VBEM. Unlike EM, however, this procedure alternates between two estimation steps and has no maximization step. 494 German, Greek, and Thai. For evaluation, following from Kwiatkowski et al. (2010), we reserve 280 sentences for test and train on the remaining 600. During development, we use cross-validation on the 600 sentence training set. At test, we run once on the remaining 280 and perform 10 fold cross-validation on the 250 sentence sets. To judge correctness, we follow standard practice and submit each parse as a GeoQuery database query, and say the parse is correct only if the answer matches the gold standard. We report raw accuracy (the percentage of sentences with correct answers), as well as F1: the harmonic mean of precision (the proportion of correct answers out of sentences"
P12-1051,P01-1067,0,0.0752881,"alize to novel sentences. Most semantic parsing models rely on an assumption of structural similarity between MR and NL. Since strict isomorphism is overly restrictive, this assumption is often relaxed by applying transformations. Several approaches assume a tree structure to the NL, MR, or both (Ge and Mooney, 2005; Kate and Mooney, 2006; Wong and Mooney, 2006; Lu et al., 2008; B¨orschinger et al., 2011), and often involve tree transformations either between two trees or a tree and a string. The tree transducer, a formalism from automata theory which has seen interest in machine translation (Yamada and Knight, 2001; Graehl et al., 2008) and has potential applications in many other areas, is well suited to formalizing such tree transformation based models. Yet, while many semantic parsing systems resemble the formalism, each was proposed as an independent model requiring custom algorithms, leaving it unclear how developments in one line of inquiry relate to others. We argue for a unifying theory of tree transformation based semantic parsing by presenting a tree transducer model and drawing connections to other similar systems. We make a further contribution by bringing to tree transducers the benefits of"
P12-1051,N06-1056,0,\N,Missing
P12-1051,D08-1082,0,\N,Missing
P12-1051,P11-1060,0,\N,Missing
P12-1093,D11-1131,1,0.696411,"Missing"
P12-1093,N10-1081,0,0.0946776,"Processes. Our adaptor grammars are actually based on the more general Pitman-Yor Processes, as described in Johnson and Goldwater (2009). 887 Word Word.pig Word ... wheres the piggie Figure 6: Sample parse generated by the collocation adaptor grammar. The adapted nonterminals Colloct and Wordt are shown underlined; the subtrees they dominate are “cached” by the adaptor grammar. The prefix (not shown here) is parsed exactly as in the Unigram PCFG. mars to generalise over subtrees of arbitrary size. Generic software is available for adaptor grammar inference, based either on Variational Bayes (Cohen et al., 2010) or Markov Chain Monte Carlo (Johnson and Goldwater, 2009). We used the latter software because it is capable of performing hyper-parameter inference for the PCFG rule probabilities and the Pitman-Yor Process parameters. We used the “outof-the-box” settings for this software, i.e., uniform priors on all PCFG rule parameters, a Beta(2, 1) prior on the Pitman-Yor a parameters and a “vague” Gamma(100, 0.01) prior on the Pitman-Yor b parameters. (Presumably performance could be improved if the priors were tuned, but we did not explore this here). Here we explore a simple “collocation” extension to"
P12-1093,D10-1028,0,0.123438,"capable of performing hyper-parameter inference for the PCFG rule probabilities and the Pitman-Yor Process parameters. We used the “outof-the-box” settings for this software, i.e., uniform priors on all PCFG rule parameters, a Beta(2, 1) prior on the Pitman-Yor a parameters and a “vague” Gamma(100, 0.01) prior on the Pitman-Yor b parameters. (Presumably performance could be improved if the priors were tuned, but we did not explore this here). Here we explore a simple “collocation” extension to the unigram PCFG which associates multiword collocations, rather than individual words, with topics. Hardisty et al. (2010) showed that this significantly improved performance in a sentiment analysis task. The collocation adaptor grammar in Figure 5 generates the words of the utterance as a sequence of collocations, each of which is a sequence of words. Each collocation is either associated with the sentence topic or with the None topic, just like words in the unigram model. Figure 6 shows a sample parse generated by the collocation adaptor grammar. We also experimented with a variant of the unigram and collocation grammars in which the topicspecific word distributions Wordt for each t ∈ T Model unigram unigram co"
P12-1093,N09-1036,1,0.952235,"each utterance, and the rules of the grammar encode relationships between these objects and specific words. These rules permit every object to map to every word (including function words; i.e., there is no “stop word” list), and the learning process decides which of these rules will have a non-trivial probability (these encode the object-word mappings the system has learned). This reduction of grounded learning to grammatical inference allows us to use standard grammatical inference procedures to learn our models. Here we use the adaptor grammar package described in Johnson et al. (2007) and Johnson and Goldwater (2009) with “out of the box” default settings; no parameter tuning whatsoever was done. Adaptor grammars are a framework for specifying hierarchical non-parametric models that has been previously used to model language acquisition (Johnson, 2008). 883 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 883–891, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Social cue child.eyes child.hands mom.eyes mom.hands mom.point Value objects child is looking at objects child is touching objects care-giver is looking at obje"
P12-1093,P08-1046,1,0.901809,"ides which of these rules will have a non-trivial probability (these encode the object-word mappings the system has learned). This reduction of grounded learning to grammatical inference allows us to use standard grammatical inference procedures to learn our models. Here we use the adaptor grammar package described in Johnson et al. (2007) and Johnson and Goldwater (2009) with “out of the box” default settings; no parameter tuning whatsoever was done. Adaptor grammars are a framework for specifying hierarchical non-parametric models that has been previously used to model language acquisition (Johnson, 2008). 883 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 883–891, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Social cue child.eyes child.hands mom.eyes mom.hands mom.point Value objects child is looking at objects child is touching objects care-giver is looking at objects care-giver is touching objects care-giver is pointing to ing information: Figure 1: The 5 social cues in the Frank et al. (to appear) corpus. The value of a social cue for an utterance is a subset of the available topics (i.e., the obje"
P12-1093,P10-1117,1,0.874872,"2, and they do so by parsing the prefix and the words of the utterance separately; the top-level rules of the grammar force the same topic to be associated with both the prefix and the words of the utterance (see Figure 3). 2.1 Topic models and the unigram PCFG As Johnson et al. (2010) observe, this kind of grounded learning can be viewed as a specialised kind of topic inference in a topic model, where the utterance topic is constrained by the available objects (possible topics). We exploit this observation here using a reduction based on the reduction of LDA topic models to PCFGs proposed by Johnson (2010). This leads to our first model, the unigram grammar, which is a PCFG.1 1 In fact, the unigram grammar is equivalent to a HMM, but the PCFG parameterisation makes clear the relationship 886 Sentence → Topict Wordst TopicNone → ## Topict → Tt TopicNone Topict → TNone Topict Tt → t Topicalc1 Topicalci → (ci ) Topicalci+1 Topicalc` → (c` ) # TNone → t NotTopicalc1 NotTopicalci → (ci ) NotTopicalci+1 NotTopicalc` → (c` ) # Wordst → WordNone (Wordst ) Wordst → Wordt (Wordst ) Wordt → w ∀t ∈ T 0 ∀t ∈ T 0 ∀t ∈ T ∀t ∈ T i = 1, . . . , ` − 1 ∀t ∈ T i = 1, . . . , ` − 1 ∀t ∈ T 0 ∀t ∈ T ∀t ∈ T 0 , w ∈ W"
P12-2017,U11-1004,1,0.705944,"Missing"
P12-2017,N09-1036,1,0.894733,"rticles without rejuvenation. Comparing the 1-particle learner with 1600 rejuvenation steps to the 16-particle learner with 100 rejuvenation steps, for both models the former outperforms the latter in both log-probability and token fscore. This suggests that if one has to trade-off particle number against rejuvenation steps, one may be better off favouring the latter. Despite the dramatic improvement over not using rejuvenation, there is still a considerable gap between all the incremental learners and the batch sampling algorithm in terms of log-probability. A similar observation was made by Johnson and Goldwater (2009) for incremental initialisation in word segmentation using adaptor grammars. Their batch sampler converged on higher token f-score but lower probability solutions in some settings when initialized in an incremental fashion as opposed to randomly. We agree with their suggestion that this may be due to the “greedy” character of an incremental learner. 6 Conclusion and outlook We have shown that adding rejuvenation to a particle filter improves segmentation scores and logprobabilities. Yet, our incremental algorithm still finds lower probability but high quality token fscores compared to its batc"
P12-2017,N07-1018,1,0.84796,"α, W1:n ) that does not take into account the intra-sentential word dependencies for a segmenta87 tion of o. It is precisely because we ignore these dependencies that an efficient dynamic programming algorithm is possible, but because Q is different from the target conditional distribution P , our algorithm that uses Q instead of P needs to correct for this. In a particle filter, this is done when the particle weights are calculated (B¨orschinger and Johnson, 2011). For an MCMC algorithm or our rejuvenation step, a Metropolis-Hastings accept/reject step is required, as described in detail by Johnson et al. (2007) in the context of grammatical inference.1 In our case, during rejuvenation an utterance u with current segmentation s is reanalyzed as follows: • remove all the words contained in s from the particle’s current state L, yielding state L∗ • sample a proposal segmentation s0 for u from Q(·|u, L∗, α), using Mochihashi et al. (2009)’s dynamic programming algorithm 0 |L∗,α)Q(s|L∗,α) • calculate m = min{1, PP (s (s|L∗,α)Q(s0 |L∗,α) } • with probability m, accept the new sample and update L∗ accordingly, else keep the original segmentation and set the particle’s state back to L This completes the des"
P12-2017,P09-1012,0,0.779196,"over the data to non-deterministically explore the state space of possible segmentations. If an MCMC algorithm runs long enough, the probability of it visiting any specific segmentation is the probability of that segmentation under the target posterior distribution, here, the distribution over segmentations given the observed data. The MCMC algorithm of Goldwater et al. (2009) is a Gibbs sampler that makes very small moves through the state space by changing individual word boundaries one at a time. An alternative MCMC algorithm that samples segmentations for entire utterances was proposed by Mochihashi et al. (2009). Below, we correct a minor error in the algorithm, recasting it as a Metropolis-within-Gibbs sampler. Moving beyond MCMC algorithms, Pearl et al. (2010) describe an algorithm that can be seen as a degenerate limiting case of a particle filter with only one particle. Their Dynamic Programming Sampling algorithm makes a single pass through the data, processing one utterance at a time by sampling a segmentation given the choices made for all previous utterances. While their algorithm comes with 86 no guarantee that it converges on the intended posterior distribution, B¨orschinger and Johnson (20"
P13-1102,J98-2005,0,0.0546119,"on a mean-field approximation, and Johnson et al. (2007) proposed MCMC samplers for the posterior distribution over rule probabilities and the parse trees of the training data strings. PCFGs have the interesting property (which we expect most linguistically more realistic models to also possess) that the distributions they define are not always properly normalized or “tight”. In a non-tight PCFG the partition function (i.e., sum of the “probabilities” of all the trees generated by the PCFG) is less than one. (Booth and Thompson, 1973, called such non-tight PCFGs “inconsistent”, but we follow Chi and Geman (1998) in calling them “non-tight” to avoid confusion with the consistency of statistical estimators). Chi (1999) showed that renormalized nontight PCFGs (which he called “Gibbs CFGs”) define the same class of distributions over trees as do tight PCFGs with the same rules, and provided an algorithm for mapping any PCFG to a tight PCFG with the same rules that defines the same distribution over trees. An obvious question is then: how does tightness affect the inference of PCFGs? Chi and Geman (1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML estimates are always t"
P13-1102,J99-1004,0,0.0589929,"rule probabilities and the parse trees of the training data strings. PCFGs have the interesting property (which we expect most linguistically more realistic models to also possess) that the distributions they define are not always properly normalized or “tight”. In a non-tight PCFG the partition function (i.e., sum of the “probabilities” of all the trees generated by the PCFG) is less than one. (Booth and Thompson, 1973, called such non-tight PCFGs “inconsistent”, but we follow Chi and Geman (1998) in calling them “non-tight” to avoid confusion with the consistency of statistical estimators). Chi (1999) showed that renormalized nontight PCFGs (which he called “Gibbs CFGs”) define the same class of distributions over trees as do tight PCFGs with the same rules, and provided an algorithm for mapping any PCFG to a tight PCFG with the same rules that defines the same distribution over trees. An obvious question is then: how does tightness affect the inference of PCFGs? Chi and Geman (1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML estimates are always tight for both the supervised case (where the input consists of parse trees) and the unsupervised case (wher"
P13-1102,J12-3003,1,0.849779,"ngineering perspectives. Cognitively it is implausible that children can perceive the parse trees of the language they are learning, but it is more reasonable to assume that they can obtain the terminal strings or yield of these trees. Unsupervised methods for learning a grammar from terminal strings alone is also interesting from an engineering perspective because such training data is cheap and plentiful, while Mark Johnson Department of Computing Macquarie University mark.johnson@mq.edu.au the manually parsed data required by supervised methods are expensive to produce and relatively rare. Cohen and Smith (2012) show that inferring PCFG rule probabilities from strings alone is computationally intractable, so we should not expect to find an efficient, general-purpose algorithm for the unsupervised problem. Instead, approximation algorithms are standardly used. For example, the InsideOutside (IO) algorithm efficiently implements the Expectation-Maximization (EM) procedure for approximating a Maximum Likelihood estimator (Lari and Young, 1990). Bayesian estimators for PCFG rule probabilities have also been attracting attention because they provide a theoretically-principled way of incorporating prior in"
P13-1102,N07-1018,1,0.807749,"Columbia University scohen@cs.columbia.edu Abstract Probabilistic context-free grammars have the unusual property of not always defining tight distributions (i.e., the sum of the “probabilities” of the trees the grammar generates can be less than one). This paper reviews how this non-tightness can arise and discusses its impact on Bayesian estimation of PCFGs. We begin by presenting the notion of “almost everywhere tight grammars” and show that linear CFGs follow it. We then propose three different ways of reinterpreting non-tight PCFGs to make them tight, show that the Bayesian estimators in Johnson et al. (2007) are correct under one of them, and provide MCMC samplers for the other two. We conclude with a discussion of the impact of tightness empirically. 1 Introduction Probabilistic Context-Free Grammars (PCFGs) play a special role in computational linguistics because they are perhaps the simplest probabilistic models of hierarchical structures. Their simplicity enables us to mathematically analyze their properties to a detail that would be difficult with linguistically more accurate models. Such analysis is useful because it is reasonable to expect more complex models to exhibit similar properties"
P13-1102,P04-1061,0,0.0302015,"the state-of-the-art in unsupervised grammar induction, but to try to measure empirical differences in the estimates produced by the three different approaches to tightness just described. The bottom line of our experiments is that we could not detect any significant difference in the estimates produced by samplers for these three different approaches. In our experiments we used the English Penn treebank (Marcus et al., 1993). We use the part-ofspeech tag sequences of sentences shorter than 11 words in sections 2–21. The grammar we use is the PCFG version of the dependency model with valence (Klein and Manning, 2004), as it appears in Smith (2006). We used a symmetric Dirichlet prior with hyperparameter α = 0.1. For each of the three approaches for handling tightness, we ran 100 times the samplers in §7, each for 1,000 iterations. We discarded the first 900 sweeps of each run, and calculated the F1 -scores of the sampled trees every 10th sweep from the last 100 sweeps. For each run we calculated the average F1 -score over the 10 sweeps we evaluated. We thus have 100 average F1 -scores for each of the samplers. Figure 1 plots the density of F1 scores (compared to the gold standard) resulting from the Gibbs"
P13-1102,J93-2004,0,0.0415775,"in unsupervised grammar induction In this section we present experiments using the three samplers just described in an unsupervised grammar induction problem. Our goal here is not to improve the state-of-the-art in unsupervised grammar induction, but to try to measure empirical differences in the estimates produced by the three different approaches to tightness just described. The bottom line of our experiments is that we could not detect any significant difference in the estimates produced by samplers for these three different approaches. In our experiments we used the English Penn treebank (Marcus et al., 1993). We use the part-ofspeech tag sequences of sentences shorter than 11 words in sections 2–21. The grammar we use is the PCFG version of the dependency model with valence (Klein and Manning, 2004), as it appears in Smith (2006). We used a symmetric Dirichlet prior with hyperparameter α = 0.1. For each of the three approaches for handling tightness, we ran 100 times the samplers in §7, each for 1,000 iterations. We discarded the first 900 sweeps of each run, and calculated the F1 -scores of the sampled trees every 10th sweep from the last 100 sweeps. For each run we calculated the average F1 -sc"
P13-1148,P12-1020,0,0.131774,"ts performance for recovering deleted /t/s. We look at both a situation where word boundaries are pre-specified and only inference for underlying forms has to be performed; and the problem of jointly finding the word boundaries and recovering deleted underlying /t/s. Section 5 discusses our findings, and section 6 concludes with directions for further research. 1508 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1508–1516, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Background and related work The work of Elsner et al. (2012) is most closely related to our goal of building a model that handles variation. They propose a pipe-line architecture involving two separate generative models, one for word-segmentation and one for phonological variation. They model the mapping to surface forms using a probabilistic finite-state transducer. This allows their architecture to handle virtually arbitrary pronunciation variation. However, as they point out, combining the segmentation and the variation model into one joint model is not straight-forward and usual inference procedures are infeasible, which requires the use of several"
P13-1148,N09-1036,1,0.910574,"but we don’t tell it whether or not these words end in an underlying /t/. Even in this simple example, there are 5 possible positions for the model to posit an underlying /t/. We evaluate the model in terms of F-score, the harmonic mean of recall (the fraction of underlying /t/s the model correctly recovered) and precision (the fraction of underlying /t/s the model predicted that were correct). In these experiments, we ran a total of 2500 iterations with a burnin of 2000. We collect samples with a lag of 10 for the last 500 iterations and perform maximum marginal decoding over these samples (Johnson and Goldwater, 2009), as well as running two chains so as to get an idea of the variance.5 We are also interested in how well the model can infer the rule probabilities from the data, that is, whether it can learn values for the different ρc parameters. We compare two settings, one where we perform inference for these parameters assuming a uniform Beta prior on each ρc (LEARN -ρ) and one where we provide the model with the empirical probabilities for each ρc as estimated off the gold-data (GOLD -ρ), e.g., for the uniform condition 0.29. The results are shown in Table 2. Best performance for both the Unigram and t"
P13-1148,P08-1046,1,0.830797,"nderlying form, a word surface realization of Ui,j , a word /t/-deletion probability in context c observed segments for ith utterance Table 1: Key for the variables in Figure 1 and Figure 2. See Figure 3 for the definition of B. parameter α0 , and the word type specific distributions Lw are drawn from a DP (L, α1 ), resulting in a hierarchical DP model (Teh et al., 2006). The base distribution B functions as a lexical generator, defining a prior distribution over possible words. In principle, B can incorporate arbitrary prior knowledge about possible words, for example syllable structure (cf. Johnson (2008)). Inspired by Norris et al. (1997), we use a simpler possible word constraint that only rules out sequences that lack a vowel (see Figure 3). While this is clearly a simplification it is a plausible assumption for English data. Instead of generating the observed sequence of segments W directly by concatenating the underlying forms as in Goldwater et al. (2009), we map each Ui,j to a corresponding surface-form Si,j by a probabilistic rule component PR . The values over which the Si,j range are determined by the available phonological processes. In the model we study here, the phonological proc"
P14-1027,N09-1036,1,0.966591,"ne or more Words, while the second rule (3) states that a Word consists of a sequence of one or more Phones; we assume that there are rules expanding Phone into all possible phones. Because Word is an adapted nonterminal, the adaptor grammar memoises Word subtrees, which corresponds to learning the phone sequences for the words of the language. The more sophisticated Adaptor Grammars discussed below can be understood as specialising either the first or the second of the rules in (2–3). The next two subsections review the Adaptor Grammar word segmentation models presented in Johnson (2008) and Johnson and Goldwater (2009): section 2.1 reviews how phonotactic syllable-structure constraints can be expressed with Adaptor Grammars, while section 2.2 reviews how phrase-like units called “collocations” capture inter-word dependencies. Section 2.3 presents the major novel contribution of this paper 285 Grammar: (4) is replaced with (10–11) and (12– 17) are added to the grammar. Word → SyllableIF designed to correspond to syntactic phrases, by examining the sample parses induced by the Adaptor Grammar we noticed that the collocations often correspond to noun phrases, prepositional phrases or verb phrases. This motivat"
P14-1027,N07-1018,1,0.854867,"Missing"
P14-1027,P12-1093,1,0.726801,"estion by Hochmann et al. (2010) that human learners use frequency cues to identify function words, it might be interesting to develop computational models that do the same thing. In an Adaptor Grammar the frequency distribution of function words might be modelled by specifying the prior for the Pitman-Yor Process parameters associated with the function words’ adapted nonterminals so that it prefers to generate a small number of high-frequency items. It should also be possible to develop models which capture the fact that function words tend not to be topic-specific. Johnson et al. (2010) and Johnson et al. (2012) show how Adaptor Grammars can model the association between words and non-linguistic “topics”; perhaps these models could be extended to capture some of the semantic properties of function words. It would also be interesting to further explore the extent to which Bayesian model selection is a useful approach to linguistic “parameter setting”. In order to do this it is imperative to develop better methods than the problematic “Harmonic Mean” estimator used here for calculating the evidence (i.e., the marginal probability of the data) that can handle the combination of discrete and continuous h"
P14-1027,P08-1046,1,0.951068,"e power is specified by the aX parameter of the PYP). The PCFG rules expanding an adapted nonterminal X define the “base distribution” of the associated DP or PYP, and the aX and bX parameters determine how much mass is reserved for “new” trees. There are several different procedures for inferring the parse trees and the rule probabilities given a corpus of strings: Johnson et al. (2007b) describe a MCMC sampler and Cohen et al. (2010) describe a Variational Bayes procedure. We use the MCMC procedure here since this has been successfully applied to word segmentation problems in previous work (Johnson, 2008). by explaining how we modify these adaptor grammars to capture some of the special properties of function words. 2.1 Syllable structure and phonotactics The rule (3) models words as sequences of independently generated phones: this is what Goldwater et al. (2009) called the “monkey model” of word generation (it instantiates the metaphor that word types are generated by a monkey randomly banging on the keys of a typewriter). However, the words of a language are typically composed of one or more syllables, and explicitly modelling the internal structure of words typically improves word segmenta"
P14-1027,N10-1081,0,\N,Missing
P15-1141,P09-2085,1,0.884586,"Missing"
P15-1141,P13-1148,1,0.893639,"Missing"
P15-1141,N13-1019,1,0.941764,"e meaning of text by identifying a set of latent topics from a collection of documents and assigning each word in these documents to one of the latent topics. A document is modelled as a mixture of latent topics, and each topic is a distribution over a finite vocabulary of words. It is common for topic models to treat documents as bags-of-words, ignoring any internal structure. While this simplifies posterior inference, it also ignores the information encoded in, for example, syntactic relationships (Boyd-Graber and Blei, 2009), word order (Wallach, 2006) and the topic structure of documents (Du et al., 2013). Here we are interested in topic models that capture dependencies between adjacent words in a topic dependent way. For example, the phrase “white house” can be interpreted compositionally in a real-estate context, but not in a political context. Several extensions of LDA have been proposed that assign topics not only to individual words but also to multi-word phrases, which we call topical collocations. However, as we will discuss in section 2, most of those extensions either rely on a pre-processing step to identify potential collocations (e.g., bigrams and trigrams) or limit attention to bi"
P15-1141,P10-1117,1,0.84281,"Most existing topic models make the bagof-words assumption that words are generated independently, and so ignore potentially useful information about word order. Previous attempts to use collocations (short sequences of adjacent words) in topic models have either relied on a pipeline approach, restricted attention to bigrams, or resulted in models whose inference does not scale to large corpora. This paper studies how to simultaneously learn both collocations and their topic assignments. We present an efficient reformulation of the Adaptor Grammar-based topical collocation model (AG-colloc) (Johnson, 2010), and develop a point-wise sampling algorithm for posterior inference in this new formulation. We further improve the efficiency of the sampling algorithm by exploiting sparsity and parallelising inference. Experimental results derived in text classification, information retrieval and human evaluation tasks across a range of datasets show that this reformulation scales to hundreds of thousands of documents while maintaining the good performance of the AG-colloc model. 1 Introduction Probabilistic topic models like Latent Dirichlet Allocation (LDA) (Blei et al., 2003) are commonly used to study"
P15-1141,Q14-1036,0,0.0454692,"Missing"
P15-1141,P06-1124,0,0.0273164,"allowed TNG to outperform LDACOL on a standard information retrieval task. However, both LDACOL and TNG do not require words within a sequence to share the same topic, which can result in semantically incoherent collocations. Subsquent models have sought to encourage topically coherent collocations, including PhraseDiscovering LDA (Lindsey et al., 2012), the timebased topical n-gram model (Jameel and Lam, 2013a) and the n-gram Hierarchical Dirichlet Process (HDP) model (Jameel and Lam, 2013b). Phrase-Discovering LDA is a non-parametric ex1461 tension of TNG inspired by Bayesian N-gram models Teh (2006) that incorporate a Pitman-Yor Process prior. The n-gram HDP is a nonparametric extension of LDA-colloc, putting an HDP prior on the per-document topic distribution. Both of these non-parametric extensions use the Chinese Franchise representation for posterior inference. Our work here is based on the AG-colloc model proposed by Johnson (2010). He showed how Adaptor Grammars can generalise LDA to learn topical collocations of unbounded length while jointly identifying the topics that occur in each document. Unfortunately, because the Adaptor Grammar inference algorithm uses Probabilistic Contex"
P15-1141,E14-1056,0,0.0309651,"rd tokens which are grouped into D documents. We sample from the posterior distribution over segmentations of documents into collocations, and assignments of topics to collocations. Let each document d be a sequence of Nd words wd,1 , . . . , wd,Nd . We introduce a set of auxiliary random variables bd,1 , . . . , bd,Nd . The value 2 In the TCM, the vocabulary differs from topic to topic. Given a sequence of adjacent words, it is hard to tell if it is a collocation without knowing the topic of its context. Therefore, the Pointwise Mutual Information (PMI) (Newman et al., 2010) and its variant (Lau et al., 2014) are not applicable to our TCM in evaluation. of bd,j indicates whether there is a collocation boundary between wd,j and wd,j+1 , and, if there is, the topic of the collocation to the left of the boundary. If there is no boundary then bd,j = 0. Otherwise, there is a collocation to the left of the boundary consisting of the words wd,l+1 , . . . , wd,j where l = max {i |1 ≤ i ≤ j − 1 ∧ bd,i 6= 0}, and bd,j = k (1 ≤ k ≤ K) is the topic of the collocation. Note that bd,Nd must not be 0 as the end of a document is always a collocation boundary. For example, consider the document consisting of the w"
P15-1141,D12-1020,0,0.0195177,"lso the first word’s topic assignment, proposing the topical N-gram (TNG) model. In other words, whereas LDACOL only adds a distribution for every word-type to LDA, TNG adds a distribution for every possible word-topic pair. Wang et al. found that this modification allowed TNG to outperform LDACOL on a standard information retrieval task. However, both LDACOL and TNG do not require words within a sequence to share the same topic, which can result in semantically incoherent collocations. Subsquent models have sought to encourage topically coherent collocations, including PhraseDiscovering LDA (Lindsey et al., 2012), the timebased topical n-gram model (Jameel and Lam, 2013a) and the n-gram Hierarchical Dirichlet Process (HDP) model (Jameel and Lam, 2013b). Phrase-Discovering LDA is a non-parametric ex1461 tension of TNG inspired by Bayesian N-gram models Teh (2006) that incorporate a Pitman-Yor Process prior. The n-gram HDP is a nonparametric extension of LDA-colloc, putting an HDP prior on the per-document topic distribution. Both of these non-parametric extensions use the Chinese Franchise representation for posterior inference. Our work here is based on the AG-colloc model proposed by Johnson (2010)."
P16-1192,W13-2322,0,0.0419457,"e, which means the terms are much more complex than for the PCFG and there are much fewer terminal symbols with the same homomorphic term. As a consequence, condensing the invhom is much less helpful. However, the sibling-finder algorithm excels at maintaining state information within each elementary tree, yielding a 1000x speedup over the naive bottom-up algorithm when it was cancelled. Graphs. Finally, we parsed a corpus of graphs instead of strings, using the 13681-rule graph grammar of Groschwitz et al. (2015) to parse the 1258 graphs with up to 10 nodes from the “Little Prince” AMR-Bank (Banarescu et al., 2013). The top-down algorithms are slow in this experiment, confirming Groschwitz et al.’s findings. Again, the sibling-finder algorithm outperforms all other algorithms. Note that Groschwitz et al.’s parser (“GKT 15” in Fig. 7) shares much code with our system. It uses the same decomposition automata, but a less mature version of the sibling-finder method which fully computes the invhom automaton. Our new system achieves a 9x speedup for parsing the whole corpus, compared to GKT 15. 7 Related Work Describing parsing algorithms at a high level of abstraction has a long tradition in computational li"
P16-1192,2000.iwpt-1.9,0,0.148422,"word that did not occur in the sentence). PCFG. We extracted a binarized context-free grammar with 6929 rules from Section 00 of the Penn Treebank, and parsed the sentences of Section 00 with it. The homorphism in the corresponding IRTG assigns every terminal symbol a constant or the term ∗(x1 , x2 ), as in Fig. 1. As a consequence, the condensed automaton optimization from Section 5 outperforms all other algo2049 rithms, achieving a 100x speedup over the naive bottom-up algorithm when it was cancelled. TAG. We also extracted a tree-adjoining grammar from Section 00 of the PTB as described by Chen and Vijay-Shanker (2000), converted it to an IRTG as described by Koller and Kuhlmann (2012), and binarized it, yielding an IRTG with 26652 rules. Each term h(r) in this grammar represents an entire TAG elementary tree, which means the terms are much more complex than for the PCFG and there are much fewer terminal symbols with the same homomorphic term. As a consequence, condensing the invhom is much less helpful. However, the sibling-finder algorithm excels at maintaining state information within each elementary tree, yielding a 1000x speedup over the naive bottom-up algorithm when it was cancelled. Graphs. Finally,"
P16-1192,P13-1091,0,0.189945,"Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. This comes with challenges that are both practical and theoretical. From a theoretical perspective, many of these algorithms are basically the same, in that they rest upon a CKY-style parsing algorithm which recursively explores substructures of the input object and assigns them nonterminal symbols, but their exact relationship is rarely made explicit. On the practical side, this parsing algorithm and its extensions (e.g. to EM training)"
P16-1192,J07-2003,0,0.0686948,"ext-free, TAG, and graph parsing. For graph parsing, we obtain the best runtimes in the literature. 1 Introduction Grammar formalisms that go beyond context-free grammars have recently enjoyed renewed attention throughout computational linguistics. Classical grammar formalisms such as TAG (Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. This comes with challenges that are both practical and theoretical. From a theoretical perspective, many of these algorithms are basically the s"
P16-1192,J07-4004,0,0.0800609,"e present techniques that speed up tree-automata-based parsing, to the point that it becomes practically feasible on realistic data when applied to context-free, TAG, and graph parsing. For graph parsing, we obtain the best runtimes in the literature. 1 Introduction Grammar formalisms that go beyond context-free grammars have recently enjoyed renewed attention throughout computational linguistics. Classical grammar formalisms such as TAG (Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. T"
P16-1192,P01-1033,0,0.189005,"Missing"
P16-1192,N04-1035,0,0.15297,"rsing, we obtain the best runtimes in the literature. 1 Introduction Grammar formalisms that go beyond context-free grammars have recently enjoyed renewed attention throughout computational linguistics. Classical grammar formalisms such as TAG (Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. This comes with challenges that are both practical and theoretical. From a theoretical perspective, many of these algorithms are basically the same, in that they rest upon a CKY-style parsing algo"
P16-1192,J08-3004,0,0.0781866,"best runtimes in the literature. 1 Introduction Grammar formalisms that go beyond context-free grammars have recently enjoyed renewed attention throughout computational linguistics. Classical grammar formalisms such as TAG (Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. This comes with challenges that are both practical and theoretical. From a theoretical perspective, many of these algorithms are basically the same, in that they rest upon a CKY-style parsing algorithm which recursive"
P16-1192,J13-1006,0,0.0898079,"ased parsing, to the point that it becomes practically feasible on realistic data when applied to context-free, TAG, and graph parsing. For graph parsing, we obtain the best runtimes in the literature. 1 Introduction Grammar formalisms that go beyond context-free grammars have recently enjoyed renewed attention throughout computational linguistics. Classical grammar formalisms such as TAG (Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. This comes with challenges that are both practical and"
P16-1192,W11-2902,1,0.472049,"erspective, many of these algorithms are basically the same, in that they rest upon a CKY-style parsing algorithm which recursively explores substructures of the input object and assigns them nonterminal symbols, but their exact relationship is rarely made explicit. On the practical side, this parsing algorithm and its extensions (e.g. to EM training) have to be implemented and optimized from scratch for each new grammar formalism. Thus, development time is spent on reinventing wheels that are slightly different from previous ones, and the resulting implementations still tend to underperform. Koller and Kuhlmann (2011) introduced Interpreted Regular Tree Grammars (IRTGs) in order to address this situation. An IRTG represents a language by describing a regular language of derivation trees, each of which is mapped to a term over some algebra and evaluated there. Grammars from a wide range of monolingual and synchronous formalisms can be mapped into IRTGs by using different algebras: Context-free and treeadjoining grammars use string algebras of different kinds, graph grammars can be captured by using graph algebras, and so on. In addition, IRTGs come with a universal parsing algorithm based on closure results"
P16-1192,W12-4616,1,0.749415,"string is an element of L(G). We assume that no two rules of M use the same terminal symbol; this is generally not required in tree automata, but every IRTG can be brought into this convenient form. Furthermore, we focus (but only for simplicity of presentation) on IRTGs that use a single string-algebra interpretation, as in Fig. 1. Such grammars capture context-free grammars. However, IRTGs can capture a wide variety of grammar formalisms by using different algebras. For instance, an interpretation that uses a TAG string algebra (or TAG derived-tree algebra) models a tree-adjoining grammar (Koller and Kuhlmann, 2012), and an interpretation into an s-graph algebra models a hyperedge replacement graph grammar (HRG, Groschwitz et al. (2015)). By using multiple algebras, IRTGs can also represent synchronous grammars and (bottom-up) treeto-tree and tree-to-string transducers. In general, any grammar formalism whose grammars describe derivations in terms of a finite set of states can typically be converted into IRTG. 2.3 Parsing IRTGs Koller and Kuhlmann (2011) present a uniform parsing algorithm for IRTGs based on tree automata. The (monolingual) parsing problem of IRTG consists in determining, for an IRTG G a"
P16-1192,D14-1107,0,0.0236714,"t speed up tree-automata-based parsing, to the point that it becomes practically feasible on realistic data when applied to context-free, TAG, and graph parsing. For graph parsing, we obtain the best runtimes in the literature. 1 Introduction Grammar formalisms that go beyond context-free grammars have recently enjoyed renewed attention throughout computational linguistics. Classical grammar formalisms such as TAG (Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. This comes with challenges"
P16-1192,P15-1079,0,0.017374,"literature. 1 Introduction Grammar formalisms that go beyond context-free grammars have recently enjoyed renewed attention throughout computational linguistics. Classical grammar formalisms such as TAG (Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. This comes with challenges that are both practical and theoretical. From a theoretical perspective, many of these algorithms are basically the same, in that they rest upon a CKY-style parsing algorithm which recursively explores substructur"
P16-1192,N06-1056,0,0.0924091,"Missing"
P16-1192,P15-1143,1,0.727586,"in tree automata, but every IRTG can be brought into this convenient form. Furthermore, we focus (but only for simplicity of presentation) on IRTGs that use a single string-algebra interpretation, as in Fig. 1. Such grammars capture context-free grammars. However, IRTGs can capture a wide variety of grammar formalisms by using different algebras. For instance, an interpretation that uses a TAG string algebra (or TAG derived-tree algebra) models a tree-adjoining grammar (Koller and Kuhlmann, 2012), and an interpretation into an s-graph algebra models a hyperedge replacement graph grammar (HRG, Groschwitz et al. (2015)). By using multiple algebras, IRTGs can also represent synchronous grammars and (bottom-up) treeto-tree and tree-to-string transducers. In general, any grammar formalism whose grammars describe derivations in terms of a finite set of states can typically be converted into IRTG. 2.3 Parsing IRTGs Koller and Kuhlmann (2011) present a uniform parsing algorithm for IRTGs based on tree automata. The (monolingual) parsing problem of IRTG consists in determining, for an IRTG G and an input object a ∈ A, a representation of the set parses(a) = {τ ∈ L(M ) |h(τ )A = a}, i.e. of the derivation trees tha"
P17-1134,P15-2082,0,0.030673,"e as an example the case that in Hebrew there are seven synonyms for the word fear, and that different authors may choose consistently from among them. Then, having constructed their own synsets using available biblical resources and annotations, they represent texts by vectors of synonyms and apply a modified cosine similarity measure to compare and cluster these vectors. While the general task is relevant to this paper, the particular notion of synonymy here means the approach is specific to this problem, although their approach is extended to other kinds of text in Akiva and Koppel (2013). Aldebei et al. (2015) proposed a new approach motivated by this work, similarly clustering sentences, then using a Naive Bayes classifier with modified prior probabilities to classify sentences. Poetry Voice Detection Brooke et al. (2012) perform stylistic segmentation of a well-known poem, The Waste Land by T.S. Eliot. This poem is renowned for the great number of voices that appear throughout the text and has been the subject of much literary analysis (Bedient and Eliot, 1986; Cooper, 1987). These distinct voices, conceived of as representing different characters, have differing tones, lexis and grammatical styl"
P17-1134,N10-1083,0,0.0984604,"Missing"
P17-1134,W12-2504,0,0.33007,"our task, indicating its difficulty, a Bayesian model that incorporates appropriately compact language models and alternating asymmetric priors can achieve scores on the standard metrics around halfway to perfect segmentation. 1 Introduction Most work on automatically segmenting text has been on the basis of topic: segment boundaries correspond to topic changes (Hearst, 1997). There are various contexts, however, in which it is of interest to identify changes in other characteristics; for example, there has been work on identifying changes in authorship (Koppel et al., 2011) and poetic voice (Brooke et al., 2012). In this paper we investigate text segmentation on the basis of change in the native language of the writer. Two illustrative contexts where this task might be of interest are patchwriting detection and literary analysis. Patchwriting is the heavy use of text from a different source with some modification and insertion of additional words and sentences to form a new text. Pecorari (2003) notes that this is a kind of textual plagiarism, but is a strategy for learning to write in an appropriate language and style, rather than for deception. Keck (2006), Gilmore et al. (2010) and Vieyra et al. ("
P17-1134,W13-1406,0,0.0505659,"Missing"
P17-1134,C14-1185,0,0.247824,"Missing"
P17-1134,N13-1019,1,0.877831,"lexical cohesion — expressed in this context as topic segments having compact and consistent lexical distributions — and implements this within a probabilistic framework by modelling words within each segment as draws from a multinomial language model associated with that segment. Much other subsequent work either uses this as a baseline, or extends it in some way: Jeong and Titov (2010), for example, who propose a model for joint discourse segmentation and alignment for documents with parallel structures, such as a text with commentaries or presenting alternative views on the same topic; or Du et al. (2013), who use hierarchical topic structure to improve the linear segmentation. Bible Authorship Koppel et al. (2011) consider the task of decomposing a document into its authorial components based on their stylistic properties and propose an unsupervised method for doing so. The authors use as their data two biblical books, Jeremiah and Ezekiel, that are generally believed to be single-authored: their task was to segment a single artificial text constructed by interleaving chapters of the two books. Their most successful method used work in biblical scholarship on lexical choice: they give as an e"
P17-1134,N09-1040,0,0.176558,"hat incorporating knowledge about which features are associated with which L1 could potentially help improve the results. One approach to do this is the use of asymmetric priors. We note that features associated with an L1 often dominate in a segment. Accordingly, priors can represent evidence external to the data that some some aspect should be weighted more strongly: for us, this is evidence from the NLI classification task. The segmentation models discussed so far only make use of a symmetric prior but later work mentions that it would be possible to modify this to use an asymmetric prior (Eisenstein, 2009). Given that priors are effective for incorporating external information, recent work has highlighted the importance of optimizing over such priors, and in particular, the use of asymmetric priors. Key work on this is by Wallach et al. (2009) on LDA, who report that “an asymmetric Dirichlet prior over the document-topic distributions has substantial advantages over a symmetric prior”, 1462 with prior values being determined through hyperparameter optimization. Such methods have since been applied in other tasks such as sentiment analysis (Lin and He, 2009; Lin et al., 2012) to achieve substant"
P17-1134,D08-1035,0,0.195164,"ns, sequences of related words (defined via thesaurus), to break up a text into topical segments: breaks in lexical chains indicate breaks in topic. The TextTiling algorithm (Hearst, 1994, 1997) took a related approach, defining a function over lexical frequency and distribution information to determine topic boundaries, and assuming that each topic has its own vocabulary and that large shifts in this vocabulary usage correspond to topic shifts. There have been many approaches since that time. A key one, which is the basis for our own work, is the unsupervised Bayesian technique BAYES S EG of Eisenstein and Barzilay (2008), based on a generative model that assumes that each segment has its own language model. Under this assumption the task can be framed as predicting boundaries at points which maximize the probability of a text being generated by a given language model. Their method is based on lexical cohesion — expressed in this context as topic segments having compact and consistent lexical distributions — and implements this within a probabilistic framework by modelling words within each segment as draws from a multinomial language model associated with that segment. Much other subsequent work either uses t"
P17-1134,W13-1729,0,0.0166827,"ate the importance of inducing a compact distribution, which we did here by reducing the vocabulary size by stripping non-informative features. 5.4 Applying Two Asymmetric Priors Our final model, L1S EG -A SYM P, assesses whether setting different priors for each L1 can improve performance. Our grid search over two priors gives 900 possible prior combinations. These combinations also include cases where θa and θb are symmetric, which is equivalent to the L1S EG -C OMP model. We observe (Table 1) that 8 In §2 we noted the comparison of PTB and CLAWS2 tagsets in Malmasi and Cahill (2015); also, Gyawali et al. (2013) compared Penn Treebank and Universal POS tagsets and found that the more fine-grained PTB ones did better. 1464 L1 A native L1s. There isn’t yet a topic-balanced corpus like T OEFL11 which includes native speaker writing for evaluation, although we expect (given recent results on distinguishing native from nonnative text in Malmasi and Dras (2015)) that the techniques should carry over. For the literary analysis, as well, to bridge the gap between work like Morzinski (1994) and a computational application, it remains to be seen how precise an annotation is possible for this task. Additionally"
P17-1134,P94-1002,0,0.847302,"esion (Halliday and Hasan, 1976) is an important concept here: the principle that text is not formed by a random set of words and sentences but rather logically ordered sets of related words that together form a topic. In addition to the semantic relation between words, other methods such as back-references and conjunctions also help achieve cohesion. Based on this, Morris and Hirst (1991) proposed the use of lexical chains, sequences of related words (defined via thesaurus), to break up a text into topical segments: breaks in lexical chains indicate breaks in topic. The TextTiling algorithm (Hearst, 1994, 1997) took a related approach, defining a function over lexical frequency and distribution information to determine topic boundaries, and assuming that each topic has its own vocabulary and that large shifts in this vocabulary usage correspond to topic shifts. There have been many approaches since that time. A key one, which is the basis for our own work, is the unsupervised Bayesian technique BAYES S EG of Eisenstein and Barzilay (2008), based on a generative model that assumes that each segment has its own language model. Under this assumption the task can be framed as predicting boundarie"
P17-1134,J97-1003,0,0.895698,"er, stylistically expressed characteristics such as change of authorship or native language. We propose a Bayesian unsupervised text segmentation approach to the latter. While baseline models achieve essentially random segmentation on our task, indicating its difficulty, a Bayesian model that incorporates appropriately compact language models and alternating asymmetric priors can achieve scores on the standard metrics around halfway to perfect segmentation. 1 Introduction Most work on automatically segmenting text has been on the basis of topic: segment boundaries correspond to topic changes (Hearst, 1997). There are various contexts, however, in which it is of interest to identify changes in other characteristics; for example, there has been work on identifying changes in authorship (Koppel et al., 2011) and poetic voice (Brooke et al., 2012). In this paper we investigate text segmentation on the basis of change in the native language of the writer. Two illustrative contexts where this task might be of interest are patchwriting detection and literary analysis. Patchwriting is the heavy use of text from a different source with some modification and insertion of additional words and sentences to"
P17-1134,D14-1142,0,0.143057,"Missing"
P17-1134,P10-2028,0,0.0212724,"t each segment has its own language model. Under this assumption the task can be framed as predicting boundaries at points which maximize the probability of a text being generated by a given language model. Their method is based on lexical cohesion — expressed in this context as topic segments having compact and consistent lexical distributions — and implements this within a probabilistic framework by modelling words within each segment as draws from a multinomial language model associated with that segment. Much other subsequent work either uses this as a baseline, or extends it in some way: Jeong and Titov (2010), for example, who propose a model for joint discourse segmentation and alignment for documents with parallel structures, such as a text with commentaries or presenting alternative views on the same topic; or Du et al. (2013), who use hierarchical topic structure to improve the linear segmentation. Bible Authorship Koppel et al. (2011) consider the task of decomposing a document into its authorial components based on their stylistic properties and propose an unsupervised method for doing so. The authors use as their data two biblical books, Jeremiah and Ezekiel, that are generally believed to"
P17-1134,P06-1004,0,0.539462,"j} |θ0 ) = Segmentation Models For all of our segmentation we use as a starting point the unsupervised Bayesian method of Eisenstein and Barzilay (2008); see §2.3 We recap the important technical definitions here. In Equation 1 of their work they define the observation likelihood as, p(X |z, Θ) = T Y t p(xt |θzt ), (1) where X is the set of all T sentences, z is the vector of segment assignments for each sentence, xt is the bag of words drawn from the language model and Θ is the set of all K language models Θ1 . . . ΘK . As is standard in segmentation work, K is assumed to be fixed and known (Malioutov and Barzilay, 2006); it is set to the actual number of segments. The authors also impose an additional constraint, that zt must be equal to either zt−1 (the previous sentence’s segment) or zt−1 + 1 (the next segment), in order to ensure a linear segmentation. This segmentation model has two parameters: the set of language models Θ and the segment assignment indexes z. The authors note that since this task is only concerned with the segment assignments, searching in the space of language models is not desirable. They offer two alternatives to overcome this: (1) taking point estimates of the language models, which"
P17-1134,W15-0606,1,0.938516,"of 5 segments each for the single L1 pair, and from all language pairs (L1S EG -PTB-A LL D EV, L1S EG -PTB-A LL -T EST). We would expect that these datasets should not be segmentable by topic, as all the segments are on the same topic; the segments should however, differ in stylistic characteristics related to the L1. L1S EG -CLAWS2 This dataset is generated using the same methodology as L1S EG -PTB, with the exception that the essays are tagged using the RASP tagger which uses the more fine-grained CLAWS2 tagset, noting that the CLAWS2 tagset performed better in the NLI classification task (Malmasi and Cahill, 2015). 3.3 Evaluation We use the standard Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) metrics, which (broadly speaking) select sentences using a moving window of size k and determines whether these sentences correctly or incorrectly fall into the same or different reference segmentations. Pk and WD scores range between 0 and 1, with a lower score indicating better performance, and 0 a perfect segmentation. It has been noted that some “degenerate” algorithms — such as placing boundaries randomly or at every possible position — can score 0.5 (Pevzner and Hearst, 2002)."
P17-1134,D14-1144,1,0.910428,"Missing"
P17-1134,J91-1002,0,0.270624,"ation based on authorial characteristics, applied to native language. 2 Related Work Topic Segmentation The most widelyresearched text segmentation task has as its goal to divide a text into topically coherent segments. Lexical cohesion (Halliday and Hasan, 1976) is an important concept here: the principle that text is not formed by a random set of words and sentences but rather logically ordered sets of related words that together form a topic. In addition to the semantic relation between words, other methods such as back-references and conjunctions also help achieve cohesion. Based on this, Morris and Hirst (1991) proposed the use of lexical chains, sequences of related words (defined via thesaurus), to break up a text into topical segments: breaks in lexical chains indicate breaks in topic. The TextTiling algorithm (Hearst, 1994, 1997) took a related approach, defining a function over lexical frequency and distribution information to determine topic boundaries, and assuming that each topic has its own vocabulary and that large shifts in this vocabulary usage correspond to topic shifts. There have been many approaches since that time. A key one, which is the basis for our own work, is the unsupervised"
P17-1134,J02-1002,0,0.450824,"L -T EST). We would expect that these datasets should not be segmentable by topic, as all the segments are on the same topic; the segments should however, differ in stylistic characteristics related to the L1. L1S EG -CLAWS2 This dataset is generated using the same methodology as L1S EG -PTB, with the exception that the essays are tagged using the RASP tagger which uses the more fine-grained CLAWS2 tagset, noting that the CLAWS2 tagset performed better in the NLI classification task (Malmasi and Cahill, 2015). 3.3 Evaluation We use the standard Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) metrics, which (broadly speaking) select sentences using a moving window of size k and determines whether these sentences correctly or incorrectly fall into the same or different reference segmentations. Pk and WD scores range between 0 and 1, with a lower score indicating better performance, and 0 a perfect segmentation. It has been noted that some “degenerate” algorithms — such as placing boundaries randomly or at every possible position — can score 0.5 (Pevzner and Hearst, 2002). WD scores are typically similar to Pk , correcting for differential penalties between false positive boundaries"
P17-1134,N13-1009,0,0.041223,"Missing"
P17-1134,W13-1706,0,0.105765,"Missing"
P17-1134,C12-1158,0,0.107886,"Missing"
P17-1134,D12-1064,1,0.884455,"Missing"
P17-2087,N01-1016,1,0.737851,"ndum to detect disfluencies. In other words, that the disfluency is observed “after” the fluent repair in a backward language model is helpful for recognizing disfluencies. Table 1: The features used in the reranker. They, except for the first and second one, were applied by Zwarts and Johnson (2011). weight related to the LM features extracted from Switchboard. This is because the fluent sentence itself is part of the language model (Zwarts and Johnson, 2011). As a solution, we apply a k-fold cross-validation (k = 20) to train the LSTM language models when using Switchboard corpus. We follow Charniak and Johnson (2001) in splitting Switchboard corpus into training, development and test set. The training data consists of all sw[23]∗.dps files, development training consists of all sw4[5, 6, 7, 8, 9]∗.dps files and test data consists of all sw4[0, 1]∗.dps files. Following Johnson and Charniak (2004), we remove all partial words and punctuation from the training data. Although partial words are very strong indicators of disfluency, standard speech recognizers never produce them in their outputs, so this makes our evaluation both harder and more realistic. 5 baseline corpus Switchboard Fisher forward 86.1 86.2 8"
P17-2087,N15-1029,0,0.764033,"ork Approaches to disfluency detection task fall into three main categories: sequence tagging, parsingbased and noisy channel model. The sequence is sometimes called edit. 547 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 547–553 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2087 tagging models label words as fluent or disfluent using a variety of different techniques, including conditional random fields (Ostendorf and Hahn, 2013; Zayats et al., 2014; Ferguson et al., 2015), hidden Markov models (Liu et al., 2006; Schuler et al., 2010) or recurrent neural networks (Hough and Schlangen, 2015; Zayats et al., 2016). Although sequence tagging models can be easily generalized to a wide range of domains, they require a specific state space for disfluency detection, such as begin-inside-outside (BIO) style states that label words as being inside or outside of a reparandum word sequence. The parsing-based approaches refer to parsers that detect disfluencies, as well as identifying the syntactic structure of the sentence (Rasooli and Tetreault, 2013; Honnibal and Johnson"
P17-2087,N13-1102,0,0.712403,"Missing"
P17-2087,Q14-1011,1,0.83285,"rguson et al., 2015), hidden Markov models (Liu et al., 2006; Schuler et al., 2010) or recurrent neural networks (Hough and Schlangen, 2015; Zayats et al., 2016). Although sequence tagging models can be easily generalized to a wide range of domains, they require a specific state space for disfluency detection, such as begin-inside-outside (BIO) style states that label words as being inside or outside of a reparandum word sequence. The parsing-based approaches refer to parsers that detect disfluencies, as well as identifying the syntactic structure of the sentence (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016). Training a parsing-based model requires large annotated tree-banks that contain both disfluencies and syntactic structures. Noisy channel models (NCMs) use the similarity between reparandum and repair as an indicator of disfluency. However, applying an effective language model (LM) inside an NCM is computationally complex. To alleviate this problem, some researchers use more effective LMs to rescore the NCM disfluency analyses. Johnson and Charniak (2004) applied a syntactic parsing-based LM trained on the fluent version of the Switchboard corpus to rescore the disfl"
P17-2087,D13-1013,0,0.685682,"2013; Zayats et al., 2014; Ferguson et al., 2015), hidden Markov models (Liu et al., 2006; Schuler et al., 2010) or recurrent neural networks (Hough and Schlangen, 2015; Zayats et al., 2016). Although sequence tagging models can be easily generalized to a wide range of domains, they require a specific state space for disfluency detection, such as begin-inside-outside (BIO) style states that label words as being inside or outside of a reparandum word sequence. The parsing-based approaches refer to parsers that detect disfluencies, as well as identifying the syntactic structure of the sentence (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016). Training a parsing-based model requires large annotated tree-banks that contain both disfluencies and syntactic structures. Noisy channel models (NCMs) use the similarity between reparandum and repair as an indicator of disfluency. However, applying an effective language model (LM) inside an NCM is computationally complex. To alleviate this problem, some researchers use more effective LMs to rescore the NCM disfluency analyses. Johnson and Charniak (2004) applied a syntactic parsing-based LM trained on the fluent version of the Switchboard"
P17-2087,P04-1005,1,0.894395,"ugh to reduce the readability of speech transcripts. Moreover, disfluencies pose a major challenge to natural language processing tasks, such as dialogue systems, that rely on speech transcripts (Ostendorf et al., 2008). Since such systems are usually trained on fluent, clean corpora, it is important to apply a speech disfluency detection system as a pre-processor to find and remove disfluencies from input data. By disfluency detection, we usually mean identifying and deleting reparandum words. Filled pauses and discourse markers belong to a closed set of words, so they are trivial to detect (Johnson and Charniak, 2004). In this paper, we introduce a new model for detecting restart and repair disfluencies in spontaneous speech transcripts called LSTM Noisy Channel Model (LSTM-NCM). The model uses a Noisy Channel Model (NCM) to generate n-best candidate disfluency analyses, and a Long Short-Term Memory (LSTM) language model to rescore the NCM analyses. The language model scores are used as features in a MaxEnt reranker to select the most plausible analysis. We show that this novel approach improves the current state-of-the-art. Introduction Disfluency is a characteristic of spontaneous speech which is not pre"
P17-2087,J10-1001,0,0.0850232,"categories: sequence tagging, parsingbased and noisy channel model. The sequence is sometimes called edit. 547 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 547–553 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2087 tagging models label words as fluent or disfluent using a variety of different techniques, including conditional random fields (Ostendorf and Hahn, 2013; Zayats et al., 2014; Ferguson et al., 2015), hidden Markov models (Liu et al., 2006; Schuler et al., 2010) or recurrent neural networks (Hough and Schlangen, 2015; Zayats et al., 2016). Although sequence tagging models can be easily generalized to a wide range of domains, they require a specific state space for disfluency detection, such as begin-inside-outside (BIO) style states that label words as being inside or outside of a reparandum word sequence. The parsing-based approaches refer to parsers that detect disfluencies, as well as identifying the syntactic structure of the sentence (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016). Training a parsing-based model"
P17-2087,D16-1109,0,0.454458,"Markov models (Liu et al., 2006; Schuler et al., 2010) or recurrent neural networks (Hough and Schlangen, 2015; Zayats et al., 2016). Although sequence tagging models can be easily generalized to a wide range of domains, they require a specific state space for disfluency detection, such as begin-inside-outside (BIO) style states that label words as being inside or outside of a reparandum word sequence. The parsing-based approaches refer to parsers that detect disfluencies, as well as identifying the syntactic structure of the sentence (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016). Training a parsing-based model requires large annotated tree-banks that contain both disfluencies and syntactic structures. Noisy channel models (NCMs) use the similarity between reparandum and repair as an indicator of disfluency. However, applying an effective language model (LM) inside an NCM is computationally complex. To alleviate this problem, some researchers use more effective LMs to rescore the NCM disfluency analyses. Johnson and Charniak (2004) applied a syntactic parsing-based LM trained on the fluent version of the Switchboard corpus to rescore the disfluency analyses. Zwarts an"
P17-2087,P11-1071,1,0.933477,"l., 2016). Training a parsing-based model requires large annotated tree-banks that contain both disfluencies and syntactic structures. Noisy channel models (NCMs) use the similarity between reparandum and repair as an indicator of disfluency. However, applying an effective language model (LM) inside an NCM is computationally complex. To alleviate this problem, some researchers use more effective LMs to rescore the NCM disfluency analyses. Johnson and Charniak (2004) applied a syntactic parsing-based LM trained on the fluent version of the Switchboard corpus to rescore the disfluency analyses. Zwarts and Johnson (2011) trained external n-gram LMs on a variety of large speech and non-speech corpora to rank the analyses. Using the external LM probabilities as features into the reranker improved the baseline NCM (Johnson and Charniak, 2004). The idea of applying external language models in the reranking process of the NCM motivates our model in this work. 3 LSTM Noisy Channel Model We follow Johnson and Charniak (2004) in using an NCM to find the n-best candidate disfluency analyses for each sentence. The NCM, however, lacks an effective language model to capture more complicated language structures. To overco"
P18-1170,W13-2322,0,0.605465,"mlinde|mfowlie|koller}@coli.uni-saarland.de mark.johnson@mq.edu.au Abstract We present a semantic parser for Abstract Meaning Representations which learns to parse strings into tree representations of the compositional structure of an AMR graph. This allows us to use standard neural techniques for supertagging and dependency tree parsing, constrained by a linguistically principled type system. We present two approximative decoding algorithms, which achieve state-of-the-art accuracy and outperform strong baselines. 1 Introduction Over the past few years, Abstract Meaning Representations (AMRs, Banarescu et al. (2013)) have become a popular target representation for semantic parsing. AMRs are graphs which describe the predicate-argument structure of a sentence. Because they are graphs and not trees, they can capture reentrant semantic relations, such as those induced by control verbs and coordination. However, it is technically much more challenging to parse a string into a graph than into a tree. For instance, grammar-based approaches (Peng et al., 2015; Artzi et al., 2015) require the induction of a grammar from the training corpus, which is hard because graphs can be decomposed into smaller pieces in fa"
P18-1170,S17-2157,0,0.230807,"Missing"
P18-1170,P13-2131,0,0.343584,"Missing"
P18-1170,E17-1051,0,0.661226,"connect these fragments), and solves the former with a supertagging-style method and the latter with a graph-based dependency parser. Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results. We go beyond these approaches by explicitly modeling the compositional structure of the AMR, which allows the dependency parser to combine AMRs for the words using a small set of universal operations, guided by the types of these AMRs. Other recent methods directly implement a dependency parser for AMRs, e.g. the transitionbased model of Damonte et al. (2017), or postprocess the output of a dependency parser by adding missing edges (Du et al., 2014; Wang et al., 2015). In contrast to these, our model makes no strong assumptions on the dependency parsing algorithm that is used; here we choose that of Kiperwasser and Goldberg (2016). The commitment of our parser to derive AMRs compositionally mirrors that of grammar-based AMR parsers (Artzi et al., 2015; Peng et al., 2015). In particular, there are parallels between the types we use in the AM algebra and CCG categories (see Section 3 for details). As a neural system, our parser struggles less with c"
P18-1170,S14-2080,0,0.0299105,"with a graph-based dependency parser. Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results. We go beyond these approaches by explicitly modeling the compositional structure of the AMR, which allows the dependency parser to combine AMRs for the words using a small set of universal operations, guided by the types of these AMRs. Other recent methods directly implement a dependency parser for AMRs, e.g. the transitionbased model of Damonte et al. (2017), or postprocess the output of a dependency parser by adding missing edges (Du et al., 2014; Wang et al., 2015). In contrast to these, our model makes no strong assumptions on the dependency parsing algorithm that is used; here we choose that of Kiperwasser and Goldberg (2016). The commitment of our parser to derive AMRs compositionally mirrors that of grammar-based AMR parsers (Artzi et al., 2015; Peng et al., 2015). In particular, there are parallels between the types we use in the AM algebra and CCG categories (see Section 3 for details). As a neural system, our parser struggles less with coverage issues than these, and avoids the complex grammar induction process these models re"
P18-1170,P99-1059,0,0.0174222,"Skip rules allow us to extend a substring such that it covers tokens which do not correspond to a graph fragment (i.e., their AM term is ⊥), introducing IGNORE edges. After all possible items have been derived, we extract the best well-typed tree from the item of the form ([1, n], r, τ ) with the highest score, where τ = [ ]. Because we keep track of the head indices, the projective decoder is a bilexical parsing algorithm, and shares a parsing complexity of O(n5 ) with other bilexical algorithms such as the Collins parser. It could be improved to a complexity of O(n4 ) using the algorithm of Eisner and Satta (1999). 6.2 Fixed-tree decoder The fixed-tree decoder computes the best unlabeled dependency tree tr for w, using the edge scores ω(i → k), and then computes the best AM dependency tree for w whose unlabeled version is tr . The Chu-Liu-Edmonds algorithm produces a forest of dependency trees, which we want to combine into tr . We choose the tree whose root r has the highest score for being the root of the AM dependency tree and make the roots of all others children of r. At this point, the shape of tr is fixed. We choose 1836 s = ω(G[i]) Init (i, ∅, τ (G)) : s cal phenomena such as control and coordi"
P18-1170,S16-1186,0,0.141531,"Missing"
P18-1170,P14-1134,0,0.571487,"ore idea of this paper is to parse a string into a graph by instead parsing a string into a dependencystyle tree representation of the graph’s compositional structure, represented as terms of the ApplyModify (AM) algebra (Groschwitz et al., 2017). The values of the AM algebra are annotated so[s] person ARG0 s m manner sleep ARG0 G0 AR s write G1 3 want AR scale annotated data (Banarescu et al., 2013) and two successful SemEval Challenges (May, 2016; May and Priyadarshi, 2017). Methods from dependency parsing have been shown to be very successful for AMR parsing. For instance, the JAMR parser (Flanigan et al., 2014, 2016) distinguishes concept identification (assigning graph fragments to words) from relation identification (adding graph edges which connect these fragments), and solves the former with a supertagging-style method and the latter with a graph-based dependency parser. Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results. We go beyond these approaches by explicitly modeling the compositional structure of the AMR, which allows the dependency parser to combine AMRs for the words using a small set of universal operations, gui"
P18-1170,P17-1043,0,0.250353,"o[s] person ARG0 s m manner sleep ARG0 G0 AR s write G1 3 want AR scale annotated data (Banarescu et al., 2013) and two successful SemEval Challenges (May, 2016; May and Priyadarshi, 2017). Methods from dependency parsing have been shown to be very successful for AMR parsing. For instance, the JAMR parser (Flanigan et al., 2014, 2016) distinguishes concept identification (assigning graph fragments to words) from relation identification (adding graph edges which connect these fragments), and solves the former with a supertagging-style method and the latter with a graph-based dependency parser. Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results. We go beyond these approaches by explicitly modeling the compositional structure of the AMR, which allows the dependency parser to combine AMRs for the words using a small set of universal operations, guided by the types of these AMRs. Other recent methods directly implement a dependency parser for AMRs, e.g. the transitionbased model of Damonte et al. (2017), or postprocess the output of a dependency parser by adding missing edges (Du et al., 2014; Wang et al., 2015). In contrast to these, our"
P18-1170,W17-6810,1,0.676651,"ree. For instance, grammar-based approaches (Peng et al., 2015; Artzi et al., 2015) require the induction of a grammar from the training corpus, which is hard because graphs can be decomposed into smaller pieces in far more ways than trees. Neural sequence-to-sequence models, which do very well on string-to-tree parsing (Vinyals et al., 2014), can be applied to AMRs but face the challenge that graphs cannot easily be represented as sequences (van Noord and Bos, 2017a,b). In this paper, we tackle this challenge by making the compositional structure of the AMR explicit. As in our previous work, Groschwitz et al. (2017), we view an AMR as consisting of atomic graphs representing the meanings of the individual words, which were combined compositionally using linguistically motivated operations for combining a head with its arguments and modifiers. We represent this structure as terms over the AM algebra as defined in Groschwitz et al. (2017). This previous work had no parser; here we show that the terms of the AM algebra can be viewed as dependency trees over the string, and we train a dependency parser to map strings into such trees, which we then evaluate into AMRs in a postprocessing step. The dependency p"
P18-1170,Q16-1023,0,0.513701,"algebra as defined in Groschwitz et al. (2017). This previous work had no parser; here we show that the terms of the AM algebra can be viewed as dependency trees over the string, and we train a dependency parser to map strings into such trees, which we then evaluate into AMRs in a postprocessing step. The dependency parser relies on type information, which encodes the semantic valencies of the atomic graphs, to guide its decisions. More specifically, we combine a neural supertagger for identifying the elementary graphs for the individual words with a neural dependency model along the lines of Kiperwasser and Goldberg (2016) for identifying the operations of the algebra. One key challenge is that the resulting term of the AM algebra must be semantically well-typed. This makes the decoding problem NP-complete. We present two approximation algorithms: one which takes the unlabeled dependency tree as given, and one which assumes that all dependencies are projective. We evaluate on two data sets, achieving state-of-the-art results on one and near state-of-theart results on the other (Smatch f-scores of 71.0 and 70.2 respectively). Our approach clearly outperforms strong but non-compositional baselines. Plan of the pa"
P18-1170,D17-1160,0,0.0348753,"that is used; here we choose that of Kiperwasser and Goldberg (2016). The commitment of our parser to derive AMRs compositionally mirrors that of grammar-based AMR parsers (Artzi et al., 2015; Peng et al., 2015). In particular, there are parallels between the types we use in the AM algebra and CCG categories (see Section 3 for details). As a neural system, our parser struggles less with coverage issues than these, and avoids the complex grammar induction process these models require. More generally, our use of semantic types to restrict our parser is reminiscent of Kwiatkowski et al. (2010), Krishnamurthy et al. (2017) and Zhang et al. (2017), and the idea of deriving semantic representations from dependency trees is also present in Reddy et al. (2017). sound Figure 1: Elementary as-graphs Gwant , Gwriter , Gsleep , and Gsound for the words “want”, “writer”, “sleep”, and “soundly” respectively. graphs, or as-graphs: directed graphs with node and edge labels in which certain nodes have been designated as sources (Courcelle and Engelfriet, 2012) and annotated with type information. Some examples of as-graphs are shown in Fig. 1. Each as-graph has exactly one root, indicated by the bold outline. The sources ar"
P18-1170,D10-1119,0,0.0525177,"ependency parsing algorithm that is used; here we choose that of Kiperwasser and Goldberg (2016). The commitment of our parser to derive AMRs compositionally mirrors that of grammar-based AMR parsers (Artzi et al., 2015; Peng et al., 2015). In particular, there are parallels between the types we use in the AM algebra and CCG categories (see Section 3 for details). As a neural system, our parser struggles less with coverage issues than these, and avoids the complex grammar induction process these models require. More generally, our use of semantic types to restrict our parser is reminiscent of Kwiatkowski et al. (2010), Krishnamurthy et al. (2017) and Zhang et al. (2017), and the idea of deriving semantic representations from dependency trees is also present in Reddy et al. (2017). sound Figure 1: Elementary as-graphs Gwant , Gwriter , Gsleep , and Gsound for the words “want”, “writer”, “sleep”, and “soundly” respectively. graphs, or as-graphs: directed graphs with node and edge labels in which certain nodes have been designated as sources (Courcelle and Engelfriet, 2012) and annotated with type information. Some examples of as-graphs are shown in Fig. 1. Each as-graph has exactly one root, indicated by the"
P18-1170,P18-1037,0,0.156382,"thods from supertagging and dependency parsing to map a string into a well-typed AM term, which it then evaluates into an AMR. The AM term represents the compositional semantic structure of the AMR explicitly, allowing us to use standard treebased parsing techniques. The projective parser currently computes the complete parse chart. In future work, we will speed it up through the use of pruning techniques. We will also look into more principled methods for splitting the AMRs into elementary as-graphs to replace our hand-crafted heuristics. In particular, advanced methods for alignments, as in Lyu and Titov (2018), seem promising. Overcoming the need for heuristics also seems to be a crucial ingredient for applying our method to other semantic representations. Acknowledgements We would like to thank the anonymous reviewers for their comments. We thank Stefan Gr¨unewald for his contribution to our PyTorch implementation, and want to acknowledge the inspiration obtained from Nguyen et al. (2017). We also extend our thanks to the organizers and participants of the Oslo CAS Meaning Construction workshop on Universal Dependencies. This work was supported by the DFG grant KO 2916/2-1 and a Macquarie Universi"
P18-1170,P14-5010,0,0.00502875,"spell out the tokens of “Agatha Christie”, and a link to a wiki entry. Before training, we replace each “name” node, its children, and the corresponding span in the sentence with a special NAME token, and we completely remove wiki edges. In this example, this leaves us with only a “person” and a NAME node. Further, we replace numbers and some date patterns with NUMBER and DATE tokens. On the training data this is straightforward, since names and dates are explicitly annotated in the AMR. At evaluation time, we detect dates and numbers with regular expressions, and names with Stanford CoreNLP (Manning et al., 2014). We also use Stanford CoreNLP for our POS tags. Each elementary as-graph generated by the procedure of Section 4.2 has a unique node whose label corresponds most closely to the aligned word (e.g. the “want” node in Gwant and the “write” node in Gwriter ). We replace these node labels with LEX in preprocessing, reducing the number of different elementary as-graphs from 28730 to 2370. We factor the supertagger model of Section 5.1 such that the unlexicalized version of G[i] and the label for LEX are predicted separately. At evaluation, we re-lexicalize all LEX nodes in the predicted AMR. For wo"
P18-1170,D10-1004,0,0.0352221,"· p(τ |pi , pi−1 ) for any τ 6= τi and δ is a hyperparameter controlling the bias towards the aligned supertag. We train the model using K&G’s original DyNet implementation. Their algorithm uses a hinge loss function, which maximizes the score difference between the gold dependency tree and the best predicted dependency tree, and therefore requires parsing each training instance in each iteration. Because the AM dependency trees are highly non-projective, we replaced the projective parser used in the off-the-shelf implementation by the Chu-Liu-Edmonds algorithm implemented in the TurboParser (Martins et al., 2010), improving the LAS on the development set by 30 points. 5.3 ω(G[i]) = log softmax(W · vi + b) ... ... Local edge model We also trained a local edge score model, which uses a cross-entropy rather than a hinge loss and therefore avoids the repeated parsing at training 1835 time. Instead, we follow the intuition that every node in a dependency tree has at most one incoming edge, and train the model to score the correct incoming edge as high as possible. This model takes inputs xi = (wi , pi ). We define the edge and edge label scores as in Section 5.2, with tanh replaced by ReLU. We further add"
P18-1170,S16-1166,0,0.0538152,"onal Linguistics (Long Papers), pages 1831–1841 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics The AM algebra A core idea of this paper is to parse a string into a graph by instead parsing a string into a dependencystyle tree representation of the graph’s compositional structure, represented as terms of the ApplyModify (AM) algebra (Groschwitz et al., 2017). The values of the AM algebra are annotated so[s] person ARG0 s m manner sleep ARG0 G0 AR s write G1 3 want AR scale annotated data (Banarescu et al., 2013) and two successful SemEval Challenges (May, 2016; May and Priyadarshi, 2017). Methods from dependency parsing have been shown to be very successful for AMR parsing. For instance, the JAMR parser (Flanigan et al., 2014, 2016) distinguishes concept identification (assigning graph fragments to words) from relation identification (adding graph edges which connect these fragments), and solves the former with a supertagging-style method and the latter with a graph-based dependency parser. Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results. We go beyond these approaches by ex"
P18-1170,S17-2090,0,0.0416879,"stics (Long Papers), pages 1831–1841 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics The AM algebra A core idea of this paper is to parse a string into a graph by instead parsing a string into a dependencystyle tree representation of the graph’s compositional structure, represented as terms of the ApplyModify (AM) algebra (Groschwitz et al., 2017). The values of the AM algebra are annotated so[s] person ARG0 s m manner sleep ARG0 G0 AR s write G1 3 want AR scale annotated data (Banarescu et al., 2013) and two successful SemEval Challenges (May, 2016; May and Priyadarshi, 2017). Methods from dependency parsing have been shown to be very successful for AMR parsing. For instance, the JAMR parser (Flanigan et al., 2014, 2016) distinguishes concept identification (assigning graph fragments to words) from relation identification (adding graph edges which connect these fragments), and solves the former with a supertagging-style method and the latter with a graph-based dependency parser. Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results. We go beyond these approaches by explicitly modeling the compos"
P18-1170,K17-3014,1,0.895076,"Missing"
P18-1170,K15-1004,0,0.300822,"h achieve state-of-the-art accuracy and outperform strong baselines. 1 Introduction Over the past few years, Abstract Meaning Representations (AMRs, Banarescu et al. (2013)) have become a popular target representation for semantic parsing. AMRs are graphs which describe the predicate-argument structure of a sentence. Because they are graphs and not trees, they can capture reentrant semantic relations, such as those induced by control verbs and coordination. However, it is technically much more challenging to parse a string into a graph than into a tree. For instance, grammar-based approaches (Peng et al., 2015; Artzi et al., 2015) require the induction of a grammar from the training corpus, which is hard because graphs can be decomposed into smaller pieces in far more ways than trees. Neural sequence-to-sequence models, which do very well on string-to-tree parsing (Vinyals et al., 2014), can be applied to AMRs but face the challenge that graphs cannot easily be represented as sequences (van Noord and Bos, 2017a,b). In this paper, we tackle this challenge by making the compositional structure of the AMR explicit. As in our previous work, Groschwitz et al. (2017), we view an AMR as consisting of atom"
P18-1170,D14-1162,0,0.0796083,"Missing"
P18-1170,D17-1009,0,0.0379769,"Missing"
P18-1170,W17-7306,0,0.0482189,"Missing"
P18-1170,N15-1040,0,0.276438,"d dependency parser. Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results. We go beyond these approaches by explicitly modeling the compositional structure of the AMR, which allows the dependency parser to combine AMRs for the words using a small set of universal operations, guided by the types of these AMRs. Other recent methods directly implement a dependency parser for AMRs, e.g. the transitionbased model of Damonte et al. (2017), or postprocess the output of a dependency parser by adding missing edges (Du et al., 2014; Wang et al., 2015). In contrast to these, our model makes no strong assumptions on the dependency parsing algorithm that is used; here we choose that of Kiperwasser and Goldberg (2016). The commitment of our parser to derive AMRs compositionally mirrors that of grammar-based AMR parsers (Artzi et al., 2015; Peng et al., 2015). In particular, there are parallels between the types we use in the AM algebra and CCG categories (see Section 3 for details). As a neural system, our parser struggles less with coverage issues than these, and avoids the complex grammar induction process these models require. More generall"
P18-1170,D17-1125,0,0.0470735,"Missing"
P18-2008,D17-1063,0,0.0220855,"r Computational Linguistics (Short Papers), pages 43–48 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics belled training data required to achieve state-ofthe-art semantic parsing results. Active learning has been applied to a variety of machine learning and NLP tasks (Thompson et al., 1999; Tang et al., 2002; Chenguang Wang, 2017) employing various algorithms such as least confidence score (Culotta and McCallum, 2005), large margin (Settles and Craven, 2008), entropy based sampling, density weighting method (Settles, 2012), and reinforcement learning (Fang et al., 2017). Nevertheless, there has been limited work applying active learning for deep semantic parsing with the exception of Iyer et al. (2017). Different from conventional active learning, they used crowd workers to select what data to annotate for traditional semantic parsing data collection. In this paper, we apply active learning for both traditional and overnight data collection with the focus on overnight approach. In addition, a limitation of prior active learning work is that the hyperparameters are usually predefined in some way, mostly from different work on the same or similar dataset, or f"
P18-2008,N16-1088,0,0.0618996,"” data collection approaches. We show that it is possible to obtain good training hyperparameters from seed data which is only a small fraction of the full dataset. We show that uncertainty sampling based on least confidence score is competitive in traditional data collection but not applicable for overnight collection. We evaluate several active learning strategies for overnight data collection and show that different example selection strategies per domain perform best. 1 Introduction Semantic parsing maps a natural language query to a logical form (LF) (Zettlemoyer and Collins, 2005, 2007; Haas and Riezler, 2016; Kwiatkowksi et al., 2010). Producing training data for semantic parsing is slow and costly. Active learning is effective in reducing costly data requirements for many NLP tasks. In this work, we apply active learning to deep semantic parsing and show that we can substantially reduce the data required to achieve state-of-the-art results. There are two main methods for generating semantic parsing training data. The traditional approach first generates the input natural language utterances and then labels them with output LFs. We show that active learning based on uncertainty sampling works wel"
P18-2008,P17-1089,0,0.0631421,"Linguistics belled training data required to achieve state-ofthe-art semantic parsing results. Active learning has been applied to a variety of machine learning and NLP tasks (Thompson et al., 1999; Tang et al., 2002; Chenguang Wang, 2017) employing various algorithms such as least confidence score (Culotta and McCallum, 2005), large margin (Settles and Craven, 2008), entropy based sampling, density weighting method (Settles, 2012), and reinforcement learning (Fang et al., 2017). Nevertheless, there has been limited work applying active learning for deep semantic parsing with the exception of Iyer et al. (2017). Different from conventional active learning, they used crowd workers to select what data to annotate for traditional semantic parsing data collection. In this paper, we apply active learning for both traditional and overnight data collection with the focus on overnight approach. In addition, a limitation of prior active learning work is that the hyperparameters are usually predefined in some way, mostly from different work on the same or similar dataset, or from the authors experience (Wang et al., 2017; Fang et al., 2017). In this paper, we investigate how to efficiently set the hyperparame"
P18-2008,P16-1002,0,0.0551682,"ve state-of-the-art results. There are two main methods for generating semantic parsing training data. The traditional approach first generates the input natural language utterances and then labels them with output LFs. We show that active learning based on uncertainty sampling works well for this approach. The “overnight” annotation approach (Wang et al., 2015) generates output LFs from a grammar, and uses crowd workers to paraphrase these LFs into input natural language queries. This approach 2 Related work Sequence-to-sequence models are currently the state-of-the-art for semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016; Duong et al., 2017). In this paper, we also exploit a sequenceto-sequence model to minimise the amount of la43 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 43–48 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics belled training data required to achieve state-ofthe-art semantic parsing results. Active learning has been applied to a variety of machine learning and NLP tasks (Thompson et al., 1999; Tang et al., 2002; Chenguang Wang, 2017) employing various algorithms"
P18-2008,D16-1116,0,0.0845485,"Missing"
P18-2008,D10-1119,0,0.0336806,"ches. We show that it is possible to obtain good training hyperparameters from seed data which is only a small fraction of the full dataset. We show that uncertainty sampling based on least confidence score is competitive in traditional data collection but not applicable for overnight collection. We evaluate several active learning strategies for overnight data collection and show that different example selection strategies per domain perform best. 1 Introduction Semantic parsing maps a natural language query to a logical form (LF) (Zettlemoyer and Collins, 2005, 2007; Haas and Riezler, 2016; Kwiatkowksi et al., 2010). Producing training data for semantic parsing is slow and costly. Active learning is effective in reducing costly data requirements for many NLP tasks. In this work, we apply active learning to deep semantic parsing and show that we can substantially reduce the data required to achieve state-of-the-art results. There are two main methods for generating semantic parsing training data. The traditional approach first generates the input natural language utterances and then labels them with output LFs. We show that active learning based on uncertainty sampling works well for this approach. The “o"
P18-2008,P16-1004,0,0.0155599,"esults. There are two main methods for generating semantic parsing training data. The traditional approach first generates the input natural language utterances and then labels them with output LFs. We show that active learning based on uncertainty sampling works well for this approach. The “overnight” annotation approach (Wang et al., 2015) generates output LFs from a grammar, and uses crowd workers to paraphrase these LFs into input natural language queries. This approach 2 Related work Sequence-to-sequence models are currently the state-of-the-art for semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016; Duong et al., 2017). In this paper, we also exploit a sequenceto-sequence model to minimise the amount of la43 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 43–48 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics belled training data required to achieve state-ofthe-art semantic parsing results. Active learning has been applied to a variety of machine learning and NLP tasks (Thompson et al., 1999; Tang et al., 2002; Chenguang Wang, 2017) employing various algorithms such as least confidenc"
P18-2008,D08-1112,0,0.514901,"t a sequenceto-sequence model to minimise the amount of la43 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 43–48 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics belled training data required to achieve state-ofthe-art semantic parsing results. Active learning has been applied to a variety of machine learning and NLP tasks (Thompson et al., 1999; Tang et al., 2002; Chenguang Wang, 2017) employing various algorithms such as least confidence score (Culotta and McCallum, 2005), large margin (Settles and Craven, 2008), entropy based sampling, density weighting method (Settles, 2012), and reinforcement learning (Fang et al., 2017). Nevertheless, there has been limited work applying active learning for deep semantic parsing with the exception of Iyer et al. (2017). Different from conventional active learning, they used crowd workers to select what data to annotate for traditional semantic parsing data collection. In this paper, we apply active learning for both traditional and overnight data collection with the focus on overnight approach. In addition, a limitation of prior active learning work is that the h"
P18-2008,K17-1038,1,0.940876,"ain methods for generating semantic parsing training data. The traditional approach first generates the input natural language utterances and then labels them with output LFs. We show that active learning based on uncertainty sampling works well for this approach. The “overnight” annotation approach (Wang et al., 2015) generates output LFs from a grammar, and uses crowd workers to paraphrase these LFs into input natural language queries. This approach 2 Related work Sequence-to-sequence models are currently the state-of-the-art for semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016; Duong et al., 2017). In this paper, we also exploit a sequenceto-sequence model to minimise the amount of la43 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 43–48 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics belled training data required to achieve state-ofthe-art semantic parsing results. Active learning has been applied to a variety of machine learning and NLP tasks (Thompson et al., 1999; Tang et al., 2002; Chenguang Wang, 2017) employing various algorithms such as least confidence score (Culotta and"
P18-2008,P15-1129,0,0.0937653,"Missing"
P18-2008,D07-1071,0,0.0885854,"Missing"
P18-2072,D17-1156,0,0.0582927,"Missing"
P18-2072,H01-1052,0,0.176018,"ects in a document classification task. We believe this is an important first step in developing methods for estimating the resources required to meet specific engineering performance targets. 1 Introduction An engineering discipline should be able to predict the cost of a project before the project is started. Because training data is often the most expensive part of an NLP or ML project, it is important to estimate how much training data required for a system to achieve a target accuracy. Unfortunately our field only offers fairly impractical advice, e.g., that more data increases accuracy (Banko and Brill, 2001); we currently have no practical methods for estimating how much data or what quality of data is required to achieve a target accuracy goal. Imagine if bridge construction was planned the way we build our systems! 2 Related work Power analysis (Cohen, 1992) is widely-used statistical technique (e.g., in biomedical trials) for predicting the number of measurements required in an experimental design; we aim to develop sim450 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 450–455 c Melbourne, Australia, July 15 - 20, 2018. 2018 Associ"
P19-1468,J15-2003,0,0.306359,"taset. For the link prediction task, we compare the ConvE model with our proposed link prediction score. We test how MC and Aug MC entailment scores can improve the link prediction scores in both local and global settings. 5 Results and Discussion We first compare our proposed entailment score with the previous state-of-the-art results (§5.1) and then show that we can use entailment decisions to improve the link prediction task (§5.2). 9 Higher values of K was not feasible on our machines. We performed our experiments on a 32-core 2.3 GHz machine with 256GB of RAM. 10 The entailment graphs of Berant et al. (2015) yield similar results. 5.1 Entailment Scores based on Link Prediction In this section, we compare the variants of our method to the previous state-of-the-art results on the Levy/Holt’s dataset. We compute similarity scores and report precision-recall curve by changing the threshold for entailment between 0 and 1. In order to have a fair comparison with Berant’s ILP method, we first test a set of rule-based constraints proposed by them (Berant et al., 2011). We also apply the lemma baseline heuristic process of Levy and Dagan (2016) before testing the methods. Figure 3 shows the precision-reca"
P19-1468,P12-1013,0,0.0227334,"Missing"
P19-1468,P11-1062,0,0.162317,"how improvements over the raw link prediction scores. ele cte dp Abstract (B) run for presidency of be elected president of be nominated for presidency of Introduction Figure 1: A link prediction knowledge graph (A) and an Link prediction and entailment graph induction are often treated as different problems. The former (Figure 1A) is used to infer missing relations between entities in existing knowledge graphs (Socher et al., 2013; Bordes et al., 2013; Riedel et al., 2013). The latter (Figure 1B) constructs entailment graphs with relations as nodes and entailment rules as edges between them (Berant et al., 2011, 2015; Hosseini et al., 2018) for the task of answering questions from text. In this paper, we show that these two problems are complementary by demonstrating how link prediction can help identify entailments and how discovered entailments can help predict missing links. Methods to learn entailment graphs (Berant et al., 2011, 2015; Hosseini et al., 2018) process large text corpora to find local entailment scores between relations based on the Distributional Inclusion Hypothesis which states that a word (relation) r entails another word (relation) q if and only if in any context that r can be"
P19-1468,D16-1146,0,0.0607028,"Missing"
P19-1468,P18-1011,0,0.112941,"ity scores for grounded logical rules as well as triples and learning entity and relation embeddings that score positive examples higher than negative ones. Guo et al. (2018) take an iterative approach where in each iteration a set of unseen triples are scored according to the current link prediction model and a small set of precomputed logical rules. The new triples and their scores are then used to update the current link prediction model. The above models need grounding of logical rules. A few recent works do not need grounding and are more space and time efficient (Demeester et al., 2016; Ding et al., 2018). They incorporate logical rules into distributed representations of relations. These models constrain entity or entity-pair vector representations to be nonnegative. They encourage partial ordering over relation embeddings based on implication rules; however, their methods can be only applied to (multi-)linear link prediction models such as ComplEx (Trouillon et al., 2016). In contrast, our method can be applied to any type of link prediction model. All these methods require entailment rules as their input. In most cases (Wang et al., 2015; Demeester et al., 2016; Guo et al., 2016), the entai"
P19-1468,P05-1014,0,0.431468,"g questions from text. In this paper, we show that these two problems are complementary by demonstrating how link prediction can help identify entailments and how discovered entailments can help predict missing links. Methods to learn entailment graphs (Berant et al., 2011, 2015; Hosseini et al., 2018) process large text corpora to find local entailment scores between relations based on the Distributional Inclusion Hypothesis which states that a word (relation) r entails another word (relation) q if and only if in any context that r can be used, q can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Kartsaklis and Sadrzadeh, 2016). They entailment graph (B) for entities of types politician,country. The solid lines are discovered correctly, but the dashed ones are missing. However, evidence from the link prediction model can be used to add the missing entailment rule in the entailment graph (B). Similarly, the entailment graph can be used to add the missing link in the knowledge graph (A). use types such as person, location and time, to disambiguate polysemous relations (e.g., person born in location and person born in time). Entailment graphs are then formed by imposing global constrain"
P19-1468,D16-1019,0,0.142714,"same patterns of entailments. Our method, in contrast, learns a new entailment score to improve local decisions, which in turn improves the entailment graphs. Entailment Rule Injection for link prediction. There are some attempts in recent years to improve link prediction by injecting entailment rules. Wang et al. (2015) incorporate various set of heuristic rules, including entailment rules, into embedding models for knowledge base completion. They formulate inference as an ILP problem, with the objective function generated from embeddings models and the constraints translated from the rules. Guo et al. (2016) extend the TransE model by defining plausibility scores for grounded logical rules as well as triples and learning entity and relation embeddings that score positive examples higher than negative ones. Guo et al. (2018) take an iterative approach where in each iteration a set of unseen triples are scored according to the current link prediction model and a small set of precomputed logical rules. The new triples and their scores are then used to update the current link prediction model. The above models need grounding of logical rules. A few recent works do not need grounding and are more spac"
P19-1468,C16-1268,0,0.26025,"Missing"
P19-1468,P16-2041,0,0.210562,"2.3 GHz machine with 256GB of RAM. 10 The entailment graphs of Berant et al. (2015) yield similar results. 5.1 Entailment Scores based on Link Prediction In this section, we compare the variants of our method to the previous state-of-the-art results on the Levy/Holt’s dataset. We compute similarity scores and report precision-recall curve by changing the threshold for entailment between 0 and 1. In order to have a fair comparison with Berant’s ILP method, we first test a set of rule-based constraints proposed by them (Berant et al., 2011). We also apply the lemma baseline heuristic process of Levy and Dagan (2016) before testing the methods. Figure 3 shows the precision-recall curve of all the methods in both local (A) and global (B) settings. From the SBOW methods, we only show the BInc score in the graphs as it got the best results on the development set. For Berant’s ILP method, we only have one point of precision and recall, as we had access to their entailment graphs for only one sparsity level. In both settings, Aug MC works better than all the other methods. This confirms that the link prediction method is indeed useful for finding entailment relations. Aug MC consistently outperforms MC suggest"
P19-1468,D14-1107,1,0.76789,"y of the triple being correct. We de2 note by S ∈ [0, 1]|R|×|E |the matrix containing triple probability scores. We define S(t1 , t2 ) ∈ 2 [0, 1]|R(t1 ,t2 )|×|E (t1 ,t2 ) |the submatrix of S with R(t1 , t2 ) as rows and E 2 (t1 , t2 ) as columns. We apply a link prediction model to a knowledge graph of predicate-argument structures extracted from text (§4.2). 2.2 Entailment Prediction The goal is to find entailment scores between all relations with the same types, where the 4737 3 For example by applying the Sigmoid function. entities can be in the same or opposite order (Berant et al., 2011; Lewis and Steedman, 2014b; Hosseini et al., 2018). We denote by W (t1 , t2 ) ∈ [0, 1]|R(t1 ,t2 )|×|R(t1 ,t2 ) |the (sparse) matrix containing all similarity scores Wr,q between relations r, q ∈ R(t1 , t2 ). We define W the (block diagonal) matrix consisting of all the similarity matrices W (t1 , t2 ). For a δ > 0, we define typed entailment  graphs as Gδ (t1 , t2 ) = R(t1 , t2 ), Eδ (t1 , t2 ) , where R(t1 , t2 ) are the nodes and E(t1 , t2 ) = {(r, q)|r, q ∈ R(t1 , t2 ), Wr,q ≥ δ} are the edges of the entailment graphs. Existing entailment similarity measures for relation entailment such as Weeds (Weeds and Weir, 2"
P19-1468,W14-2406,1,0.906373,"y of the triple being correct. We de2 note by S ∈ [0, 1]|R|×|E |the matrix containing triple probability scores. We define S(t1 , t2 ) ∈ 2 [0, 1]|R(t1 ,t2 )|×|E (t1 ,t2 ) |the submatrix of S with R(t1 , t2 ) as rows and E 2 (t1 , t2 ) as columns. We apply a link prediction model to a knowledge graph of predicate-argument structures extracted from text (§4.2). 2.2 Entailment Prediction The goal is to find entailment scores between all relations with the same types, where the 4737 3 For example by applying the Sigmoid function. entities can be in the same or opposite order (Berant et al., 2011; Lewis and Steedman, 2014b; Hosseini et al., 2018). We denote by W (t1 , t2 ) ∈ [0, 1]|R(t1 ,t2 )|×|R(t1 ,t2 ) |the (sparse) matrix containing all similarity scores Wr,q between relations r, q ∈ R(t1 , t2 ). We define W the (block diagonal) matrix consisting of all the similarity matrices W (t1 , t2 ). For a δ > 0, we define typed entailment  graphs as Gδ (t1 , t2 ) = R(t1 , t2 ), Eδ (t1 , t2 ) , where R(t1 , t2 ) are the nodes and E(t1 , t2 ) = {(r, q)|r, q ∈ R(t1 , t2 ), Wr,q ≥ δ} are the edges of the entailment graphs. Existing entailment similarity measures for relation entailment such as Weeds (Weeds and Weir, 2"
P19-1468,P98-2127,0,0.0642417,"et al., 2018). We denote by W (t1 , t2 ) ∈ [0, 1]|R(t1 ,t2 )|×|R(t1 ,t2 ) |the (sparse) matrix containing all similarity scores Wr,q between relations r, q ∈ R(t1 , t2 ). We define W the (block diagonal) matrix consisting of all the similarity matrices W (t1 , t2 ). For a δ > 0, we define typed entailment  graphs as Gδ (t1 , t2 ) = R(t1 , t2 ), Eδ (t1 , t2 ) , where R(t1 , t2 ) are the nodes and E(t1 , t2 ) = {(r, q)|r, q ∈ R(t1 , t2 ), Wr,q ≥ δ} are the edges of the entailment graphs. Existing entailment similarity measures for relation entailment such as Weeds (Weeds and Weir, 2003), Lin (Lin, 1998), and Balanced Inclusion (BInc; Szpektor and Dagan, 2008) are typically defined on feature vectors consisting of entitypairs (e.g., Obama-Hawaii), where the values are frequencies or pointwise mutual information (PMI) between the relations and the features (Berant et al., 2011, 2012, 2015). While these methods currently hold state-of-the-art results on relation entailment datasets (Hosseini et al., 2018), they suffer from low recall because the feature vectors are usually sparse and do not have high overlap with each other. The link prediction models, on the other hand, can predict the probabi"
P19-1468,N19-1226,0,0.0219182,"28.43 Table 2: Link prediction results on the test set of NewsSpike for all entities (top) and infrequent entities (below). We test the effect of refining ConvE scores with entailment relations. scores (>0.95). 6 Related Work Link Prediction. In recent years, many link prediction models have been proposed that learn vector or matrix representations for relations and entities (Socher et al., 2013; Bordes et al., 2013; Riedel et al., 2013; Wang et al., 2014; Lin et al., 2015; Toutanova et al., 2016; Nguyen et al., 2016; Trouillon et al., 2016; Dettmers et al., 2018; Schlichtkrull et al., 2018; Nguyen et al., 2019). These models are trained by assigning higher plausibility scores to correct facts than incorrect ones. For example, the well-known TransE model (Bordes et al., 2013) captures relational similarity between entity pairs by considering a translation vector for the relations connecting them. In particular, it learns embeddings for entities and relations such that e~2 − e~1 ≈ ~r for any correct triple (r, e1 , e2 ). In our experiments we have used ConvE (Dettmers et al., 2018), however, our proposed score can be computed based on any link prediction model and the discovered entailment relations m"
P19-1468,N16-1054,1,0.846058,"9.26 46.10 1303.56 28.25 19.30 46.36 1154.06 28.33 19.29 46.60 1154.28 28.41 19.28 46.66 1118.09 28.43 Table 2: Link prediction results on the test set of NewsSpike for all entities (top) and infrequent entities (below). We test the effect of refining ConvE scores with entailment relations. scores (>0.95). 6 Related Work Link Prediction. In recent years, many link prediction models have been proposed that learn vector or matrix representations for relations and entities (Socher et al., 2013; Bordes et al., 2013; Riedel et al., 2013; Wang et al., 2014; Lin et al., 2015; Toutanova et al., 2016; Nguyen et al., 2016; Trouillon et al., 2016; Dettmers et al., 2018; Schlichtkrull et al., 2018; Nguyen et al., 2019). These models are trained by assigning higher plausibility scores to correct facts than incorrect ones. For example, the well-known TransE model (Bordes et al., 2013) captures relational similarity between entity pairs by considering a translation vector for the relations connecting them. In particular, it learns embeddings for entities and relations such that e~2 − e~1 ≈ ~r for any correct triple (r, e1 , e2 ). In our experiments we have used ConvE (Dettmers et al., 2018), however, our proposed s"
P19-1468,Q14-1030,1,0.839153,"nek et al., 2007); however, we chose to experiment on assertions extracted from raw text. This is because we can then evaluate the predicted entailments on existing entailment datasets with examples stated in natural language (§4.3). We use the multiple-source NewsSpike corpus of Zhang and Weld (2013). The NewsSpike corpus includes 550K news articles and is well-suited for finding entailment and paraphrasing relations as it includes different articles from different sources describing identical news stories. We use the triples released by Hosseini et al. (2018)6 who run the semantic parser of Reddy et al. (2014), GraphParser, to extract binary relations between a predicate and its arguments. GraphParser uses Combinatorial Categorial Grammer (CCG) syntactic derivations by running EasyCCG (Lewis and Steedman, 2014a). The parser converts sentences to neo-Davisonian semantics, a first order logic that uses event identifiers and extracts one binary relation for each event and pair of arguments (Parsons, 1990). The entities are typed by first linking to Freebase (Bollacker et al., 2008) and then selecting the most notable type of the entity from Freebase and mapping it to FIGER types (Ling 4739 6 Accessed"
P19-1468,N13-1008,0,0.475509,"her. The new entailment score outperforms prior state-of-the-art results on a standard entialment dataset and the new link prediction scores show improvements over the raw link prediction scores. ele cte dp Abstract (B) run for presidency of be elected president of be nominated for presidency of Introduction Figure 1: A link prediction knowledge graph (A) and an Link prediction and entailment graph induction are often treated as different problems. The former (Figure 1A) is used to infer missing relations between entities in existing knowledge graphs (Socher et al., 2013; Bordes et al., 2013; Riedel et al., 2013). The latter (Figure 1B) constructs entailment graphs with relations as nodes and entailment rules as edges between them (Berant et al., 2011, 2015; Hosseini et al., 2018) for the task of answering questions from text. In this paper, we show that these two problems are complementary by demonstrating how link prediction can help identify entailments and how discovered entailments can help predict missing links. Methods to learn entailment graphs (Berant et al., 2011, 2015; Hosseini et al., 2018) process large text corpora to find local entailment scores between relations based on the Distributi"
P19-1468,C08-1107,0,0.904373,"∈ [0, 1]|R(t1 ,t2 )|×|R(t1 ,t2 ) |the (sparse) matrix containing all similarity scores Wr,q between relations r, q ∈ R(t1 , t2 ). We define W the (block diagonal) matrix consisting of all the similarity matrices W (t1 , t2 ). For a δ > 0, we define typed entailment  graphs as Gδ (t1 , t2 ) = R(t1 , t2 ), Eδ (t1 , t2 ) , where R(t1 , t2 ) are the nodes and E(t1 , t2 ) = {(r, q)|r, q ∈ R(t1 , t2 ), Wr,q ≥ δ} are the edges of the entailment graphs. Existing entailment similarity measures for relation entailment such as Weeds (Weeds and Weir, 2003), Lin (Lin, 1998), and Balanced Inclusion (BInc; Szpektor and Dagan, 2008) are typically defined on feature vectors consisting of entitypairs (e.g., Obama-Hawaii), where the values are frequencies or pointwise mutual information (PMI) between the relations and the features (Berant et al., 2011, 2012, 2015). While these methods currently hold state-of-the-art results on relation entailment datasets (Hosseini et al., 2018), they suffer from low recall because the feature vectors are usually sparse and do not have high overlap with each other. The link prediction models, on the other hand, can predict the probability of any triple being in the knowledge graph. Using pr"
P19-1468,P16-1136,0,0.379252,"elations that entail each other in both directions are regarded as paraphrases. 4736 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4736–4746 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics near can be used to answer such questions. 2 On the other hand, link prediction (or knowledge base completion) models are based on distributional methods and directly predict the source data. These models have received much attention in the recent years (Socher et al., 2013; Bordes et al., 2013; Riedel et al., 2013; Toutanova et al., 2016; Trouillon et al., 2016; Dettmers et al., 2018). The current methods learn embeddings for all entities and relations and a function to score any potential relation between the entities. One of the main capabilities of these models is that they implicitly exploit entailment relations such as person born in country entails person be from country (Riedel et al., 2013). However, entailment relations are not learned explicitly. For example, we cannot simply compute the cosine similarity of the vector representations of the two relations to detect the entailment between them, because cosine similar"
P19-1468,W03-1011,0,0.830383,"nd Steedman, 2014b; Hosseini et al., 2018). We denote by W (t1 , t2 ) ∈ [0, 1]|R(t1 ,t2 )|×|R(t1 ,t2 ) |the (sparse) matrix containing all similarity scores Wr,q between relations r, q ∈ R(t1 , t2 ). We define W the (block diagonal) matrix consisting of all the similarity matrices W (t1 , t2 ). For a δ > 0, we define typed entailment  graphs as Gδ (t1 , t2 ) = R(t1 , t2 ), Eδ (t1 , t2 ) , where R(t1 , t2 ) are the nodes and E(t1 , t2 ) = {(r, q)|r, q ∈ R(t1 , t2 ), Wr,q ≥ δ} are the edges of the entailment graphs. Existing entailment similarity measures for relation entailment such as Weeds (Weeds and Weir, 2003), Lin (Lin, 1998), and Balanced Inclusion (BInc; Szpektor and Dagan, 2008) are typically defined on feature vectors consisting of entitypairs (e.g., Obama-Hawaii), where the values are frequencies or pointwise mutual information (PMI) between the relations and the features (Berant et al., 2011, 2012, 2015). While these methods currently hold state-of-the-art results on relation entailment datasets (Hosseini et al., 2018), they suffer from low recall because the feature vectors are usually sparse and do not have high overlap with each other. The link prediction models, on the other hand, can pr"
P19-1468,D13-1183,0,0.342652,"We then describe the details of the link prediction model (§4.2), the datasets used to test the models (§4.3) and the baseline systems (§4.4). 4.1 Text Corpus Link prediction models are often applied to existing knowledge graphs such as Freebase (Bollacker et al., 2008), DBPedia (Lehmann et al., 2015) and Yago (Suchanek et al., 2007); however, we chose to experiment on assertions extracted from raw text. This is because we can then evaluate the predicted entailments on existing entailment datasets with examples stated in natural language (§4.3). We use the multiple-source NewsSpike corpus of Zhang and Weld (2013). The NewsSpike corpus includes 550K news articles and is well-suited for finding entailment and paraphrasing relations as it includes different articles from different sources describing identical news stories. We use the triples released by Hosseini et al. (2018)6 who run the semantic parser of Reddy et al. (2014), GraphParser, to extract binary relations between a predicate and its arguments. GraphParser uses Combinatorial Categorial Grammer (CCG) syntactic derivations by running EasyCCG (Lewis and Steedman, 2014a). The parser converts sentences to neo-Davisonian semantics, a first order lo"
P19-1529,D18-1162,0,0.0369491,"Missing"
P19-1529,W05-0620,0,0.0543505,"Missing"
P19-1529,C18-1251,0,0.0609233,"Missing"
P19-1529,P18-2058,0,0.047851,"ress three questions: 1) How should syntactic information be encoded as word-level features? 2) What is the best way of integrating syntactic information? and 3) What effect does the choice of syntactic representation have on the performance? We study these questions in the context of Semantic Role Labelling (SRL). A SRL system extracts the predicate-argument structure of a sentence.2 Syntax was an essential component of early SRL systems (Xue and Palmer, 2004; Punyakanok et al., 2008). The state-of-the-art neural SRL systems use a neural sequence labelling model without any syntax knowledge (He et al., 2018, 2017; Tan et al., 2018). We show below that injecting external syntactic knowledge into a neural SRL sequence labelling model can improve the performance, and our best model sets a new stateof-the-art for a non-ensemble SRL system. In this paper we express the external syntactic information as vectors of discrete features, because this enables us to explore different ways of injecting the syntactic information into the neural SRL model. Specifically, we propose three different syntax encoding methods: a) a full constituency tree representation (Full-C); b) an SRLspecific span representation"
P19-1529,P17-1044,0,0.127535,"Missing"
P19-1529,P18-1249,0,0.0693605,"Missing"
P19-1529,P17-1064,0,0.0311314,"n can be used most effectively in the Semantic Role Labeling (SRL) task. We evaluate three different ways of encoding syntactic parses and three different ways of injecting them into a state-of-the-art neural ELMo-based SRL sequence labelling model. We show that using a constituency representation as input features improves performance the most, achieving a new state-of-the-art for non-ensemble SRL models on the in-domain CoNLL’05 and CoNLL’12 benchmarks.1 1 Introduction Properly integrating external information into neural networks has received increasing attention recently (Wu et al., 2018; Li et al., 2017; Strubell et al., 2018). Previous research on this topic can be roughly categorized into three classes: i) Input: The external information are presented as additional input features (i.e., dense real-valued vectors) to the neural network (Collobert et al., 2011). ii) Output: The neural network is trained to predict the main task and the external information in a multi-task approach (Changpinyo et al., 2018). iii) Auto-encoder: This approach, recently proposed by Wu et al. (2018), simultaneously combines the Input and Output during neural models training. The simplicity of these methods allow"
P19-1529,D17-1159,0,0.0523409,"dhan et al. (2013) incorporate constituent-structure span-based information, while Hajiˇc et al. (2009) incorporate dependency-structure information. This information can be incorporated into an SRL system in several different ways. Swayamdipta et al. (2018) use span information from constituency parse trees as an additional training target in a multi-task learning approach, similar to one of the approaches we evaluate here. Roth and Lapata (2016) use an LSTM model to represent the dependency paths between predicates and arguments and feed the output as the input features to their SRL system. Marcheggiani and Titov (2017) use Graph Convolutional Network (Niepert et al., 2016) to encode the dependency parsing trees into their LSTM-based SRL system. Xia et al. (2019) represent dependency parses using position-based categorical features of tree structures in a neural model. Strubell et al. (2018) use dependency trees as a supervision signal to train one of attention heads in a self-attentive neural model. 3 Syntactic Representation This section introduces our representations of constituency and dependency syntax trees. 3.1 Full-C: Full Constituency Representation G´omez-Rodr´ıguez and Vilares (2018) propose a ful"
P19-1529,D18-1191,0,0.407935,"Missing"
P19-1529,J05-1004,0,0.0853794,"eedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5338–5343 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics the SRL CoNLL’05 and CoNLL’12 benchmarks. We show that using either of the constituency representations in either the Input or the AutoEncoder configurations produces the best performance. These results are noticeably better than a strong baseline and set a new state-of-the-art for non-ensemble SRL systems. 2 Related Work Semantic Role Labeling (SRL) generally refers to the PropBank style of annotation (Palmer et al., 2005). Broadly speaking, prior work on SRL makes use of syntactic information in two different ways. Carreras and M`arquez (2005); Pradhan et al. (2013) incorporate constituent-structure span-based information, while Hajiˇc et al. (2009) incorporate dependency-structure information. This information can be incorporated into an SRL system in several different ways. Swayamdipta et al. (2018) use span information from constituency parse trees as an additional training target in a multi-task learning approach, similar to one of the approaches we evaluate here. Roth and Lapata (2016) use an LSTM model t"
P19-1529,N18-1202,0,0.109135,"Missing"
P19-1529,D18-1310,0,0.113834,"tactic information can be used most effectively in the Semantic Role Labeling (SRL) task. We evaluate three different ways of encoding syntactic parses and three different ways of injecting them into a state-of-the-art neural ELMo-based SRL sequence labelling model. We show that using a constituency representation as input features improves performance the most, achieving a new state-of-the-art for non-ensemble SRL models on the in-domain CoNLL’05 and CoNLL’12 benchmarks.1 1 Introduction Properly integrating external information into neural networks has received increasing attention recently (Wu et al., 2018; Li et al., 2017; Strubell et al., 2018). Previous research on this topic can be roughly categorized into three classes: i) Input: The external information are presented as additional input features (i.e., dense real-valued vectors) to the neural network (Collobert et al., 2011). ii) Output: The neural network is trained to predict the main task and the external information in a multi-task approach (Changpinyo et al., 2018). iii) Auto-encoder: This approach, recently proposed by Wu et al. (2018), simultaneously combines the Input and Output during neural models training. The simplicity of the"
P19-1529,N19-1075,0,0.0560144,"ormation can be incorporated into an SRL system in several different ways. Swayamdipta et al. (2018) use span information from constituency parse trees as an additional training target in a multi-task learning approach, similar to one of the approaches we evaluate here. Roth and Lapata (2016) use an LSTM model to represent the dependency paths between predicates and arguments and feed the output as the input features to their SRL system. Marcheggiani and Titov (2017) use Graph Convolutional Network (Niepert et al., 2016) to encode the dependency parsing trees into their LSTM-based SRL system. Xia et al. (2019) represent dependency parses using position-based categorical features of tree structures in a neural model. Strubell et al. (2018) use dependency trees as a supervision signal to train one of attention heads in a self-attentive neural model. 3 Syntactic Representation This section introduces our representations of constituency and dependency syntax trees. 3.1 Full-C: Full Constituency Representation G´omez-Rodr´ıguez and Vilares (2018) propose a full representation of constituency parsing trees where the string position between wi and wi+1 is associated with the pair (n(wi ) − n(wi−1 ), l(wi"
P19-1529,W04-3212,0,0.116622,"source code is available in https:// github.com/GaryYufei/bestParseSRL this gap by integrating syntactic information to the sequence labelling task. We address three questions: 1) How should syntactic information be encoded as word-level features? 2) What is the best way of integrating syntactic information? and 3) What effect does the choice of syntactic representation have on the performance? We study these questions in the context of Semantic Role Labelling (SRL). A SRL system extracts the predicate-argument structure of a sentence.2 Syntax was an essential component of early SRL systems (Xue and Palmer, 2004; Punyakanok et al., 2008). The state-of-the-art neural SRL systems use a neural sequence labelling model without any syntax knowledge (He et al., 2018, 2017; Tan et al., 2018). We show below that injecting external syntactic knowledge into a neural SRL sequence labelling model can improve the performance, and our best model sets a new stateof-the-art for a non-ensemble SRL system. In this paper we express the external syntactic information as vectors of discrete features, because this enables us to explore different ways of injecting the syntactic information into the neural SRL model. Specif"
P19-1529,W13-3516,0,0.0539215,"Missing"
P19-1529,J08-2005,0,0.0516893,"ble in https:// github.com/GaryYufei/bestParseSRL this gap by integrating syntactic information to the sequence labelling task. We address three questions: 1) How should syntactic information be encoded as word-level features? 2) What is the best way of integrating syntactic information? and 3) What effect does the choice of syntactic representation have on the performance? We study these questions in the context of Semantic Role Labelling (SRL). A SRL system extracts the predicate-argument structure of a sentence.2 Syntax was an essential component of early SRL systems (Xue and Palmer, 2004; Punyakanok et al., 2008). The state-of-the-art neural SRL systems use a neural sequence labelling model without any syntax knowledge (He et al., 2018, 2017; Tan et al., 2018). We show below that injecting external syntactic knowledge into a neural SRL sequence labelling model can improve the performance, and our best model sets a new stateof-the-art for a non-ensemble SRL system. In this paper we express the external syntactic information as vectors of discrete features, because this enables us to explore different ways of injecting the syntactic information into the neural SRL model. Specifically, we propose three d"
P19-1529,P16-1113,0,0.0344688,"style of annotation (Palmer et al., 2005). Broadly speaking, prior work on SRL makes use of syntactic information in two different ways. Carreras and M`arquez (2005); Pradhan et al. (2013) incorporate constituent-structure span-based information, while Hajiˇc et al. (2009) incorporate dependency-structure information. This information can be incorporated into an SRL system in several different ways. Swayamdipta et al. (2018) use span information from constituency parse trees as an additional training target in a multi-task learning approach, similar to one of the approaches we evaluate here. Roth and Lapata (2016) use an LSTM model to represent the dependency paths between predicates and arguments and feed the output as the input features to their SRL system. Marcheggiani and Titov (2017) use Graph Convolutional Network (Niepert et al., 2016) to encode the dependency parsing trees into their LSTM-based SRL system. Xia et al. (2019) represent dependency parses using position-based categorical features of tree structures in a neural model. Strubell et al. (2018) use dependency trees as a supervision signal to train one of attention heads in a self-attentive neural model. 3 Syntactic Representation This s"
P19-1529,D18-1548,0,0.0953321,"Missing"
P19-1529,D18-1412,0,0.0760596,"These results are noticeably better than a strong baseline and set a new state-of-the-art for non-ensemble SRL systems. 2 Related Work Semantic Role Labeling (SRL) generally refers to the PropBank style of annotation (Palmer et al., 2005). Broadly speaking, prior work on SRL makes use of syntactic information in two different ways. Carreras and M`arquez (2005); Pradhan et al. (2013) incorporate constituent-structure span-based information, while Hajiˇc et al. (2009) incorporate dependency-structure information. This information can be incorporated into an SRL system in several different ways. Swayamdipta et al. (2018) use span information from constituency parse trees as an additional training target in a multi-task learning approach, similar to one of the approaches we evaluate here. Roth and Lapata (2016) use an LSTM model to represent the dependency paths between predicates and arguments and feed the output as the input features to their SRL system. Marcheggiani and Titov (2017) use Graph Convolutional Network (Niepert et al., 2016) to encode the dependency parsing trees into their LSTM-based SRL system. Xia et al. (2019) represent dependency parses using position-based categorical features of tree stru"
P85-1015,P83-1021,0,0.409533,"r unrestricted permutation of the words of a sentence, and capture any word order regularities the language may have by adding restrictions to the system. The extremely free word order of nonconfigurational languages is described by allowing constituents to have discontinuous [ocatio,ls. To demonstrate that it is possible to parse with such discontinuous constituents. I show how they can be incorporated into a variant of definite clause grammars, and that these grammars can be used in conjunction with a proof procedure. such as Ear[ey deduction, to coestruct a parser, a.s shown in Pereira and Warren (1983). are explicitly represented in the grammar formalism. Given this, it will be easy to generalize the notion of location so that it can describe the discontinuous constituents of non-configurational languages. The formalism [ use here is the Definite Clause Grammar formalism described in Clocksin and Mellish (1084). To fatalliarize the reader with the DCG notation, I discuss a fragment for English in this section. In fact, the DCG representation is even more general than is brought out here: as Pereira and Warren (1083) demonstrated, one can view parsin K algorithms highly specialized proof pro"
P88-1030,P83-1021,0,0.0393463,"ecursive descent Prolog parser for the fragment. INTRODUCTION This paper reports on several deductive parsers for a fragment of Chomsky's Government and Binding theory (Chomsky 1981, 1986; Van Riemsdijk and Williams 1984). These parsers were constructed to illustrate the 'Parsing as Deduction' approach, which views a parser as a specialized theorem-prover which uses knowledge of a language (i.e.its grammar) as a set of axioms from which information about the utterances of that language (e.g. their structural descriptions) can be deduced. This approach directly inspired by the seminal paper by Pereira and Warren (1983). Johnson (1988a) motivates the Parsing as Deduction approach in more detail than is possible here, and Johnson (1988b) extends the techniques presented in this paper to deal with a more complex fragment. The PAD parsers are designed to directly mirror the deductive structure of GB theory. Intuitively, it seems that deductive parsers should be able to mirror theories with a rich internal deductive structure; these parsers show that to a first approximation this is in fact the case. For example, the PAD parsers have no direct specification of a 'rule' of Passive, rather they deduce the relevant"
P90-1022,P89-1003,0,0.216305,"Missing"
P90-1022,P86-1038,0,0.298611,"Missing"
P90-1022,W89-0203,0,0.117956,"Missing"
P90-1022,P84-1027,0,0.200332,"Missing"
P90-1022,J88-4001,0,\N,Missing
P90-1022,C86-1045,0,\N,Missing
P90-1022,J88-1001,0,\N,Missing
P95-1010,J92-2002,0,0.0261653,"truction is well-formed iff these constraints are consistent. However, in the LCG treatment of agreement proposed here agreement is inherently asymmetric, in 3 Coordination asymmetries and agreement Interestingly, the analysis of coordination is the one place where most 'unification-based' accounts abandon the symmetric consistency-based treatment of agreement and adopt an asymmetric subsumptionbased account. Working in the GPSG framework Sag et. al. (1985) proposed that the features on a conjunction must be the most specific category which subsumes each conjunct (called the generalization by Shieber (1992)). Shieber (1986) proposed a weaker condition, namely that the features on the conjunction must subsume the features on each conjunct, as expressed in the annotated phrase struc1Because conjunction and disjunction are the only connectives we permit, it does not matter whether we use the classical or intuitionistic propositional calculus here. In fact, if categories such as np and ap are 'decomposed' into the conjunctions of atomic features +nounA--verb and q-noun^+verb respectively as in the Sag et. at. (1985) analysis discussed below, disjunction is not required in any of the LCG analyses bel"
P95-1010,C88-1061,0,\N,Missing
P95-1010,P90-1025,0,\N,Missing
P95-1010,P94-1021,0,\N,Missing
P95-1014,P94-1021,0,0.0796771,"Missing"
P95-1014,P83-1021,0,0.0385268,"by any clause which uses this lemma. . Subgoals can be selected in any order (Earley Deduction always selects goals in left-to-right order). This allows constraint eoroutining within a memoized subcomputation. In the categorial grammar example, a category becomes more instantiated when it combines with arguments, allowing eventually the a d d _ a d j u n c t s / 2 and d i v i s i o n / 2 to be deterministically resolved. Thus we use the flexibility Here the Xl are vectors of variables and the T/ are vectors of terms. Now consider a standard memoizing proof procedure such as Earley Deduction (Pereira and Warren 1983) or the memoizing procedures described by Tamaki and Sato (1986), Vieille (1989) or Warren (1992) from this perspective. Each memoized goal is associated with a set of bindings for its arguments; so in CLP terms each memoized goal is a 1This essentially means that basic constraints can be recast as first-order predicates. 102 in the selection of goals to run constraints whenever their arguments are sufficiently instantiated, and delay them otherwise. 4. Memoization can be selectively applied (Earley Deduction memoizes every computational step). This can significantly improve overall efficiency"
P95-1014,P85-1018,0,0.116429,"rst-order predicates. 102 in the selection of goals to run constraints whenever their arguments are sufficiently instantiated, and delay them otherwise. 4. Memoization can be selectively applied (Earley Deduction memoizes every computational step). This can significantly improve overall efficiency. In the categorial grammar example only x/3 goals are memoized (and thus only these goals incur the cost of table management). The 'abstraction' step, which is used in most memoizing systems (including complex feature grammar chart parsers where it is somewhat confusingly called 'restriction', as in Shieber 1985), receives an elegant treatment in a CLP approach; an 'abstracted' goal is merely one in which not all of the equality constraints associated with the variables appearing in the goal are selected with that goal. 2 For example, because of the backward application rule and the left-to-right evaluation our parser uses, eventually it will search at every left string position for an uninstantiated category (the variable Y in the clause), we might as well abstract all memoized goals of the form x(C, L, R) to x(_, L, _), i.e., goals in which the category and right string position are uninstantinted."
P98-1101,P91-1032,0,0.580202,"-linear grammars. Section 4 discusses how these techniques can be combined in an implementation. This paper describes how to construct a finite-state machine (FSM) approximating a 'unification-based' grammar using a left-corner grammar transform. The approximation is presented as a series of grammar transforms, and is exact for left-linear and rightlinear CFGs, and for trees up to a user-specified depth of center-embedding. 1 Introduction This paper describes a method for approximating grammars with finite-state machines. Unlike the method derived from the LR(k) parsing algorithm described in Pereira and Wright (1991), these methods use grammar transformations based on the left-corner grammar transform (Rosenkrantz and Lewis II, 1970; Aho and Ullman, 1972). One advantage of the left corner methods is that they generalize straightforwardly to complex feature ""unification based"" grammars, unlike the LR(k) based approach. For example, the implementation described here translates a DCG version of the example grammar given by Pereira and Wright (1991) directly into a FSM without constructing an approximating CFG. Left-corner based techniques are natural for this kind of application because (with the simple opti"
P98-1101,C92-1032,0,0.0304585,"ext section, which minimize stack depth in grammars that contain productions of length no greater than two. 3.1 A tail-recursion optimization If G is a left-linear grammar, a top-down parser using £.C1 (G) can recognize any string generated by G with a constant-bounded stack size. However, the corresponding operation with right-linear grammars requires a stack of size proportional to the length of the string, since the stack fills with paired categories A - A for each non-left-corner nonterminal in the analysis tree. The 'tail recursion' or 'composition' optimization (Abney and Johnson, 1991; Resnik, 1992) permits right-branching structures to be parsed with bounded stack depth. It is the result of epsilon removal applied to the output of £C1, and can be described in terms of resolution or partial evaluation of the transformed grammar with respect to productions (1.c). In effect, the schema (1.b) is split into two cases, depending on whether or not the rightmost nonterminal A - B is expanded by the epsilon rules produced by schema (1.c). This expansion yields a grammar L:C2(G) = (N', T, P2, S), where P2 contains all productions of the form (2.a-2.c). (In these schemata A , B E N; a E T; X E N U"
P98-1101,P85-1018,0,0.0611086,"Missing"
P98-1101,J97-3004,0,0.0282118,"Missing"
P99-1054,W98-1115,1,0.831692,"lly, an LC grammar performs best with our parser when right binarized, for the same reasons outlined above. We use transform composition to apply first one transform, then another to the o u t p u t of the first. We denote this A o B where (A o B) (t) = B (A (t)). After applying the left-corner transform, we then binarize the resulting grammar 5, i.e. LC o RB. Another probabilistic LC parser investigated (Manning and Carpenter, 1997), which utilized an LC parsing architecture (not a transformed grammar), also got a performance boost 4The very efficient bottom-up statistical parser detailed in Charniak et al. (1998) measured efficiency in terms of total edges popped. An edge (or, in our case, a parser state) is considered when a probability is calculated for it, and we felt that this was a better efficiency measure than simply those popped. As a baseline, their parser considered an average of 2216 edges per sentence in section 22 of the WSJ corpus (p.c.). 5Given that the LC transform involves nullary productions, the use of RB0 is not needed, i.e. nullary productions need only be introduced from one source. Thus binarization with left corner is always to unary (RB1). Transform Rules in Grammar Left Corne"
P99-1054,J93-2004,0,0.0254098,"4 Percent of Sentences Parsed* 34.16 33.99 91.27 97.37 Avg. States Considered 19270 96813 10140 13868 *Length ~ 40 (2245 sentences in F23 Avg. Labelled Precision and Recall t .65521 .65539 .71616 .73207 Avg. MLP Labelled Prec/Rec t .76427 .76095 .72712 .72327 Avg. length -- 21.68) Ratio of Avg. Prob to Avg. MLP Prob t .001721 .001440 .340858 .443705 t o f those sentences parsed Table 1: The effect of different approaches to binarization ing the effect of different binarization approaches on parser performance. The grammars were induced from sections 2-21 of the Penn Wall St. Journal Treebank (Marcus et al., 1993), and tested on section 23. For each transform tested, every tree in the training corpus was transformed before grammar induction, resulting in a transformed PCFG and lookahead probabilities estimated in the standard way. Each parse returned by the parser was detransformed for evaluation 3. The parser used in each trial was identical, with a base b e a m factor c~ = 10 -4. The performance is evaluated using these measures: (i) the percentage of candidate sentences for which a parse was found (coverage); (ii) the average number of states (i.e. rule expansions) considered per candidate sentence"
P99-1054,P80-1024,0,0.77665,"Missing"
P99-1054,J98-4004,1,\N,Missing
P99-1069,J96-1002,0,0.0368621,"udo-likelihood: (7) i=l,...,n Instead of maximizing the likelihood of the syntactic analyses over the training corpus, we maximize the conditional likelihood of these analyses given the observed yields. In our experiments, we have used a conjugate-gradient optimization program adapted from the one presented in Press et al. (1992). Regardless of the pragmatic (computational) motivation, one could perhaps argue that the conditional probabilities Po(wly ) are as useful (if not more useful) as the full probabilities P0(w), at least in those cases for which the ultimate goal is syntactic analysis. Berger et al. (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument. The problem of estimating parameters for log-linear models is not new. It is especially difficult in cases, such as ours, where a large sample space makes the direct computation of expectations infeasible. Many applications in spatial statistics, involving Markov random fields (MRF), are of this nature as well. In his seminal development of the MRF approach to spatial statistics, Besag introduced a &quot;pseudolikelihood&quot; estimator to address these difficulties (Besag, 1974; Besag,"
P99-1069,J97-4005,0,\N,Missing
P99-1069,C90-3029,0,\N,Missing
Q13-1026,Q13-1005,0,0.0141445,", 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Goldwasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to make it"
Q13-1026,W05-0613,0,0.190121,"ch utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded Learning Task Our focus in this paper is to develop computational models that help us better understand children’s language acquisition. The goal is to learn both the long term lexicon of mappings between words and objects (language learning) as well as the intended 317 topic of individual utterances (language comprehension). We consider a corpus of child-directed spe"
Q13-1026,H05-1042,0,0.0118451,"007; Kwiatkowski et al., 2010). These approaches rely on training data of annotated sentence-meaning pairs, however. Such data are costly to obtain and are quite different from the experience of language learners. Grounded Language Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supe"
Q13-1026,D11-1131,1,0.874321,"Missing"
Q13-1026,P09-1010,0,0.0363044,"Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Goldwasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the p"
Q13-1026,W10-2903,0,0.0208243,"-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Goldwasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to make it tractable for more complex problems. Later, Kim and Mooney (2013) adapted discriminative reranking to the grounded learning problem"
Q13-1026,P81-1022,0,0.796317,"becomes enormous: the time complexity of CYK is O(n3 ) for an input of length n. Fortunately, we can take advantage of a special structural property of our grammars. The shape of the parse tree is completely determined by the input string; the only variation is in the topic annotations in the nonterminal labels. So even though the number of possible parses grows exponentially with input length n, the number of possible constituents grows only linearly with input length, and the possible constituents can be identified from the left context.2 These constraints ensure that the Earley algorithm3 (Earley, 1970) will parse an input of length n with this grammar in time O(n). A second challenge in parsing very long strings is that the probability of a parse is the product of the probabilities of the rules involved in its derivation. As the length of a derivation grows linearly with the length of the input, the parse probabilities decrease exponentially as a function of sentence length, causing floating-point underflow on inputs of even moderate length. The standard method for handling this is to compute log probabilities (which decrease linearly as a function of input length, rather than exponentially"
Q13-1026,P12-1007,0,0.0394395,"ations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded Learning Task Our focus in this paper is to develop computational models that help us better understand children’s language acquisition. The goal is to learn both the long term lexicon of mappings between words and objects (language learning) as well as the intended 317 topic of individual utterances (language comprehension). We consider a corpus of child-directed speech annotated with social cues, described in (Frank et al., 2013). There are a total of 4,76"
Q13-1026,D08-1036,1,0.822986,"e (IO) algorithm, or equivalently the Expectation Maximization (EM) algorithm, for several reasons: (1) it has been shown to be less likely to cause over-fitting for PCFGs than EM (Kurihara and Sato, 2004) and (2) implementationwise, VB is a straightforward extension from EM as they both share the same process of computing the expected counts (the IO part) and only differ at how rule probabilities are reestimated. At the same time, VB has also been demonstrated to do well on large datasets and is competitive with Gibbs samplers while having the fastest convergence time among these estimators (Gao and Johnson, 2008). The rule reestimation in VB is carried as follows. Let αr be the prior hyperparameter of a rule r in the rule set R and cr be its expected count accumulated over the entire corpus after an IO iteration. The posterior hyperparameter for r is αr∗ = αr + cr . Let ψ be the digamma function, the rule parameter [ (∑update formula )] is: θr:X→λ = ∗ exp ψ (αr∗ ) − ψ α . ′ ′ ′ r :X→λ r Whereas IO minimizes the negative loglikelihood of the observed data (sentences), -log p(x), VB minimizes a quantity called free energy, which we will use later to monitor convergence. Here x denotes the observed data"
Q13-1026,P11-1149,0,0.0134915,"ative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Goldwasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to make it tractable for more complex problems. Later, Kim and Mooney (2013) adapted discriminative reranking to the grounded learning problem using a form of weak supervision. We employ this general grammatical inference approach in the current work. Children"
Q13-1026,P10-1110,0,0.0191109,"on the fly using three transition operations: predict (add states to charts), scan (shift dots across terminals), and complete (merge two states). Figure 2 shows an example of a completion step which also illustrates the implicit binarization automatically done in Earley algorithm.  ՜   Ǥ   ՜  Ǥ      ՜ Ǥ cause it enables probabilistic prediction of possible follow-words xi+1 as P (xi+1 |x0 . . . xi ) = P (x0 ...xi xi+1 ) (Jelinek and Lafferty, 1991). These P (x0 ...xi ) conditional probabilities allow estimation of the incremental costs of a stack decoder (Bahl et al., 1983). In (Huang and Sagae, 2010), a conceptually similar prefix cost is defined to order states in a beam search decoder. Moreover, the negative logarithm of such conditional probabilities are termed as surprisal values in the psycholinguistics literature (e.g., Hale, 2001; Levy, 2008), to describe how difficult a word is in a given context. Interestingly, we show that prefix probabilities lead us to construct a parser that could parse extremely long strings next. 4.3 Inside Outside Algorithm Figure 2: Completion step – merging two states [l, m]: X→α . Y β and [m, r]:Y →ν . to produce a new state [l, r]: X→αY . β. In order t"
Q13-1026,J91-3004,0,0.678123,"Missing"
Q13-1026,P12-1093,1,0.0554223,"e assumption discards cues that are important to the learner, e.g., the fact that consecutive utterances are likely to share the same referent (Frank et al., 2013). The current paper describes an approach to the problem of simultaneously modeling grounded language at the sentence and discourse levels. We combine ideas from parsing and grammar induction to produce a parser that can handle long input strings with thousands of tokens, creating parse trees that represent full discourses. By casting grounded language learning as a grammatical inference task, we use our parser to extend the work of Johnson et al. (2012), investigating the importance of discourse continuity in children’s language acquisition and its interaction with social cues. Our model boosts performance in a language acquisition task and yields good discourse segmentations compared with human annotators. 1 Introduction Learning mappings between natural language (NL) and meaning representations (MR) is an important goal for both computational linguistics and cognitive science. Accurately learning novel mappings is crucial in grounded language understanding tasks and such systems can suggest insights into the nature of children language lea"
Q13-1026,P10-1117,1,0.886114,"Missing"
Q13-1026,D12-1040,0,0.0145715,"natory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Goldwasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to make it tractable for more complex problems. Later, Kim and Mooney (2013) adapted discriminative reranking to the grounded learning problem using a form of weak supervision. We employ this general grammatical inference approach in the current work. Children Language Acquisition. In the context of language acquisition, Frank et al. (2008) proposed a system that learned words and jointly inferred speakers’ intended referent (utterance topic) using graphical models. Johnson et al. (2012) used grammatical inference to demonstrate the importance of social cues in children"
Q13-1026,P13-1022,0,0.0180306,"ive forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Goldwasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to make it tractable for more complex problems. Later, Kim and Mooney (2013) adapted discriminative reranking to the grounded learning problem using a form of weak supervision. We employ this general grammatical inference approach in the current work. Children Language Acquisition. In the context of language acquisition, Frank et al. (2008) proposed a system that learned words and jointly inferred speakers’ intended referent (utterance topic) using graphical models. Johnson et al. (2012) used grammatical inference to demonstrate the importance of social cues in children’s early word learning. We extend this body of work by capturing discourse-based dependencies among"
Q13-1026,D10-1119,0,0.0212502,"ollow in”: they name objects that the child is already looking at (Tomasello and Farrar, 1986). In addition, our discourse model produces a performance improvement in a language acquisition task and yields good discourse segmentations compared with human annotators. 2 Related Work Supervised semantic parsers. Previous work has 316 developed supervised semantic parsers to map sentences to meaning representations of various forms, including meaning hierarchies (Lu et al., 2008) and, most dominantly, λ-calculus expressions (Zettlemoyer and Collins, 2005; Zettlemoyer, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010). These approaches rely on training data of annotated sentence-meaning pairs, however. Such data are costly to obtain and are quite different from the experience of language learners. Grounded Language Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Sny"
Q13-1026,P09-1011,0,0.0216639,"d are quite different from the experience of language learners. Grounded Language Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Goldwasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by redu"
Q13-1026,P11-1060,0,0.018795,"tion (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Goldwasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to make it tractable for more complex problems. Later, Kim and Mooney (2013) adapted discriminative reranking to the grounded learning problem using a form of weak supervision. We employ this"
Q13-1026,D08-1082,0,0.027866,"sitive cue for word learning (Johnson et al., 2012). Our data provide support for the hypothesis (from previous work) that caregivers “follow in”: they name objects that the child is already looking at (Tomasello and Farrar, 1986). In addition, our discourse model produces a performance improvement in a language acquisition task and yields good discourse segmentations compared with human annotators. 2 Related Work Supervised semantic parsers. Previous work has 316 developed supervised semantic parsers to map sentences to meaning representations of various forms, including meaning hierarchies (Lu et al., 2008) and, most dominantly, λ-calculus expressions (Zettlemoyer and Collins, 2005; Zettlemoyer, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010). These approaches rely on training data of annotated sentence-meaning pairs, however. Such data are costly to obtain and are quite different from the experience of language learners. Grounded Language Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs dist"
Q13-1026,P97-1013,0,0.13753,"ing graphical models. Johnson et al. (2012) used grammatical inference to demonstrate the importance of social cues in children’s early word learning. We extend this body of work by capturing discourse-based dependencies among utterances rather than treating each utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded Learning Task Our focus in this paper is to develop computational models that help us bette"
Q13-1026,P99-1047,0,0.0676513,"y capturing discourse-based dependencies among utterances rather than treating each utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded Learning Task Our focus in this paper is to develop computational models that help us better understand children’s language acquisition. The goal is to learn both the long term lexicon of mappings between words and objects (language learning) as well as the intended 317"
Q13-1026,J02-1002,0,0.0145039,"learners. To differentiate these cases, Frank and Rohde (under review) added a new set of annotations (to the dataset used in Section 7) based on the discourse structure perceived by human, similar to column discourse, . We utilize these new annotations to judge topics predicted by our discourse model and adopt previous metrics for discourse segmentation evaluation: a=b, a simple proportion equivalence of discourse assignments; pk , a window method (Beeferman et al., 1999) to measure the probability of two random utterances correctly classified as being in the same discourse; and WindowDiff (Pevzner and Hearst, 2002), an improved version of pk which gives “partial credit” to boundaries close to the correct ones. Results in Table 7 demonstrate that our model is in better agreement with human annotation (modelhuman) than the raw annotation (raw-human) across all metrics. As is visible from the limited change in the a=b metric, relatively few topic assignments are altered; yet these alterations create much more coherent discourses that allow for far better segmentation performance under pk and WindowDiff. a=b pk WindowDiff raw-human 63.6 57.0 36.2 model-human 69.3 83.6 61.2 Table 7: Discourse evaluation. Sin"
Q13-1026,W04-2322,0,0.047148,"Missing"
Q13-1026,D09-1001,0,0.0301773,"and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Goldwasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semantic parser as a PCFG induction task, achieving state-of the art performance in the RoboCup domain. Kim and Mooney (2012) extended the technique to make it tractable for more complex problems. Later, Kim and Mooney (2013) adapted discriminative reranking to the grounded learning problem using a form of weak supervision. We employ this general grammatical inference approach in t"
Q13-1026,C88-2120,0,0.472026,"ge acquisition, Frank et al. (2008) proposed a system that learned words and jointly inferred speakers’ intended referent (utterance topic) using graphical models. Johnson et al. (2012) used grammatical inference to demonstrate the importance of social cues in children’s early word learning. We extend this body of work by capturing discourse-based dependencies among utterances rather than treating each utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in"
Q13-1026,N03-1030,0,0.136065,"iscourse-based dependencies among utterances rather than treating each utterance independently. Discourse Parsing. A substantial literature has examined formal representations of discourse across a wide variety of theoretical perspectives (Mann and Thompson, 1988; Scha and Polanyi, 1988; Hobbs, 1990; Lascarides and Asher, 1993; Knott and Sanders, 1997). Although much of this work was highly influential, Marcu (1997)’s work on discourse parsing brought this task to special prominence. Since then, more and more sophisticated models of discourse analysis have been developed:, e.g., (Marcu, 1999; Soricut and Marcu, 2003; Forbes et al., 2003; Polanyi et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Hernault et al., 2010; Lin et al., 2012; Feng and Hirst, 2012). Our contribution to work on this task is to examine latent discourse structure specifically in grounded language learning. 3 A Grounded Learning Task Our focus in this paper is to develop computational models that help us better understand children’s language acquisition. The goal is to learn both the long term lexicon of mappings between words and objects (language learning) as well as the intended 317 topic of individual utte"
Q13-1026,J95-2002,0,0.714884,"the probability of a parse is the product of the probabilities of the rules involved in its derivation. As the length of a derivation grows linearly with the length of the input, the parse probabilities decrease exponentially as a function of sentence length, causing floating-point underflow on inputs of even moderate length. The standard method for handling this is to compute log probabilities (which decrease linearly as a function of input length, rather than exponentially), but as we explain later (Section 5), we can use the ability of the Earley algorithm to compute prefix probabilities (Stolcke, 1995) to rescale the probability of the parse incrementally and avoid floating-point underflows. In the next section, we provide background information on the Earley algorithm for PCFGs, the prefix probability scheme we use, and the insideoutside algorithm in the Earley context. 4 Background 4.1 Earley Algorithm for PCFGs The Earley algorithm was developed by Earley (1970) and known to be efficient for certain kinds of CFGs (Aho and Ullman, 1972). An Earley parser 2 The prefix markers # and ## and the topic markers such as “.dog” enable a left-to- right parser to unambiguously identify its location"
Q13-1026,N09-1064,0,0.151505,"Missing"
Q13-1026,P10-1083,0,0.0276378,"to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Barzilay and Lapata, 2005; Snyder and Barzilay, 2007), iterative retraining (Chen et al., 2010), and generative models of segmentation and alignment (Liang et al., 2009) to text-to-interaction mapping using reinforcement learning (Branavan et al., 2009; Vogel and Jurafsky, 2010), graphical model semantics representation (Tellex et al., 2011a; Tellex et al., 2011b), and Combinatory Categorial Grammar (Artzi and Zettlemoyer, 2013). A number of systems have also used alternative forms of supervision, including sentences paired with responses (Clarke et al., 2010; Goldwasser and Roth, 2011; Liang et al., 2011) and no supervision (Poon and Domingos, 2009; Goldwasser et al., 2011). Recent work has also introduced an alternative approach to grounded learning by reducing it to a grammatical inference problem. B¨orschinger et al. (2011) casted the problem of learning a semant"
Q13-1026,P07-1121,0,0.0406764,"ork) that caregivers “follow in”: they name objects that the child is already looking at (Tomasello and Farrar, 1986). In addition, our discourse model produces a performance improvement in a language acquisition task and yields good discourse segmentations compared with human annotators. 2 Related Work Supervised semantic parsers. Previous work has 316 developed supervised semantic parsers to map sentences to meaning representations of various forms, including meaning hierarchies (Lu et al., 2008) and, most dominantly, λ-calculus expressions (Zettlemoyer and Collins, 2005; Zettlemoyer, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010). These approaches rely on training data of annotated sentence-meaning pairs, however. Such data are costly to obtain and are quite different from the experience of language learners. Grounded Language Learning. In contrast to semantic parsers, grounded language learning systems aim to learn the meanings of words and sentences given an observed world state (Yu and Ballard, 2004; Gorniak and Roy, 2007). A growing body of work in this field employs distinct techniques from a wide variety of perspectives from text-to-record alignment using structured classification (Bar"
Q13-1026,D07-1071,0,0.10669,"Missing"
Q13-1026,J00-3005,0,\N,Missing
Q14-1008,C12-1021,1,0.912087,"Missing"
Q14-1008,P13-1148,1,0.91389,"Missing"
Q14-1008,N13-1012,0,0.163845,"plicate our experiments are available from http://web.science.mq.edu.au/ ˜bborschi/ tion of its effectiveness in adult speech processing (Cutler et al., 1986). Several studies have investigated the role of stress in word segmentation using computational models, using both neural network and “algebraic” (as opposed to “statistical”) approaches (Christiansen et al., 1998; Yang, 2004; Lignos and Yang, 2010; Lignos, 2011; Lignos, 2012). Bayesian models of word segmentation (Brent, 1999; Goldwater, 2007), however, have until recently completely ignored stress. The sole exception in this respect is Doyle and Levy (2013) who added stress cues to the Bigram model (Goldwater et al., 2009), demonstrating that this leads to an improvement in segmentation performance. In this paper, we extend their work and show how to integrate stress cues into the flexible Adaptor Grammar framework (Johnson et al., 2007). This allows us to both start from a stronger baseline model and to investigate how the role of stress cues interacts with other aspects of the model. In particular, we find that phonotactic cues to word-boundaries interact with stress cues, indicating synergistic effects for small inputs and partial redundancy"
Q14-1008,C10-1060,1,0.921492,"Missing"
Q14-1008,N09-1036,1,0.578196,"r than that reported by Lignos (2011). Our own approach differs from theirs in assuming phonemic rather than pre-syllabified input (although our model could, trivially, be run on syllabified input as well) and makes use of Adaptor Grammars instead of the Goldwater et al. (2009) Bigram model, providing us with a flexible framework for exploring the usefulness of stress in different models. Adaptor Grammar (Johnson et al., 2007) is a grammar-based formalism for specifying nonparametric hierarchical models. Previous work explored the usefulness of, for example, syllablestructure (Johnson, 2008b; Johnson and Goldwater, 2009) or morphology (Johnson, 2008b; Johnson, 2008a) in word segmentation. The closest work to our own is Johnson and Demuth (2010) who investigate the usefulness of tones for Mandarin phonemic segmentation. Their way of adding tones to a model of word segmentation is very similar to our way of incorporating stress. 3 Models We give an intuitive description of the mathematical background of Adaptor Grammars in 3.1, referring the reader to Johnson et al. (2007) for technical details. The models we examine are derived from the collocational model of Johnson and Goldwater (2009) by varying three param"
Q14-1008,W08-0704,1,0.951771,"e is even smaller than that reported by Lignos (2011). Our own approach differs from theirs in assuming phonemic rather than pre-syllabified input (although our model could, trivially, be run on syllabified input as well) and makes use of Adaptor Grammars instead of the Goldwater et al. (2009) Bigram model, providing us with a flexible framework for exploring the usefulness of stress in different models. Adaptor Grammar (Johnson et al., 2007) is a grammar-based formalism for specifying nonparametric hierarchical models. Previous work explored the usefulness of, for example, syllablestructure (Johnson, 2008b; Johnson and Goldwater, 2009) or morphology (Johnson, 2008b; Johnson, 2008a) in word segmentation. The closest work to our own is Johnson and Demuth (2010) who investigate the usefulness of tones for Mandarin phonemic segmentation. Their way of adding tones to a model of word segmentation is very similar to our way of incorporating stress. 3 Models We give an intuitive description of the mathematical background of Adaptor Grammars in 3.1, referring the reader to Johnson et al. (2007) for technical details. The models we examine are derived from the collocational model of Johnson and Goldwate"
Q14-1008,P08-1046,1,0.887222,"e is even smaller than that reported by Lignos (2011). Our own approach differs from theirs in assuming phonemic rather than pre-syllabified input (although our model could, trivially, be run on syllabified input as well) and makes use of Adaptor Grammars instead of the Goldwater et al. (2009) Bigram model, providing us with a flexible framework for exploring the usefulness of stress in different models. Adaptor Grammar (Johnson et al., 2007) is a grammar-based formalism for specifying nonparametric hierarchical models. Previous work explored the usefulness of, for example, syllablestructure (Johnson, 2008b; Johnson and Goldwater, 2009) or morphology (Johnson, 2008b; Johnson, 2008a) in word segmentation. The closest work to our own is Johnson and Demuth (2010) who investigate the usefulness of tones for Mandarin phonemic segmentation. Their way of adding tones to a model of word segmentation is very similar to our way of incorporating stress. 3 Models We give an intuitive description of the mathematical background of Adaptor Grammars in 3.1, referring the reader to Johnson et al. (2007) for technical details. The models we examine are derived from the collocational model of Johnson and Goldwate"
Q14-1008,W10-2912,0,0.14521,"Missing"
Q14-1008,W11-0304,0,0.651549,"Missing"
Q14-1011,J13-1002,0,0.056191,"in the final configuration. We follow the suggestion 133 (σ, i|β, A, D) ` (σ|i, β, A, D) (σ|i, j|β, A, D) ` (σ, j|β, A ∪ {j → i}, D) Only if i does not have an incoming arc. (σ|i, j|β, A, D) ` (σ|i|j, β, A ∪ {i → j}, D) (σ|i, β, A, D) ` (σ, β, A, D) Only if i has an incoming arc. (σ|i, j|β, A, D) ` (σ|[x1 , xn ], j|β, A0 , D0 ) Where A0 = A  {x → y or y → x : ∀x ∈ [i, j), ∀y ∈ N} D0 = D ∪ [i, j) x1 ...xn are the former left children of i S L R D E Figure 2: Our parser’s transition system. The first four transitions are the standard arc-eager system; the fifth is our novel Edit transition. of Ballesteros and Nivre (2013) and add a dummy token that governs root dependencies to the end of the sentence. Parsing terminates when this token is at the start of the buffer, and the stack is empty. Disfluencies are added to D via the Edit transition, E, which we now define. 4 A Non-Monotonic Edit Transition One of the reasons disfluent sentences are hard to parse is that there often appear to be syntactic relationships between words in the reparandum and the fluent sentence. When these relations are considered in addition to the dependencies between fluent words, the resulting structure is not necessarily a projective"
Q14-1011,P01-1017,0,0.0148994,"racy into context. We therefore compare our joint model to two pipeline systems, which consist of a disfluency detector, followed by our dependency parser. We also evaluate parse accuracies after oracle pre-processing, to gauge the net effect of disfluencies on our parser’s accuracy. The dependency parser for the pipeline systems was trained on text with all disfluencies removed, following Charniak and Johnson (2001). The two disfluency detection systems we used were the Qian and Liu (2013) sequence-tagging model, and a version of the Johnson and Charniak (2004) noisy channel model, using the Charniak (2001) syntactic language model and the reranking features of Zwarts and Johnson (2011). They are the two best published disfluency detection systems. 8 Results Table 1 shows the development set accuracies for our joint parser. Both the disfluency features and the Edit transition make statistically significant improvements, in both disfluency F -measure, unlabelled attachment score (UAS), and labelled attachment score (LAS). The Oracle pipeline system, which uses the gold-standard to clean disfluencies prior to parsing, shows the total impact of speech-errors on the parser. The baseline parser, whic"
Q14-1011,N12-1015,0,0.0399042,"Missing"
Q14-1011,N01-1016,1,0.676199,"ovided: one for syntactic bracketing (MRG files), and one for disfluencies (DPS files). The disfluency layer marks elements with little or no syntactic function, such as filled pauses and discourse markers, and annotates speech repairs using the Shriberg (1994) system of reparandum/interregnum/repair. An example is shown in Figure 1. In the syntactic annotation, edited words are covered by a special node labelled EDITED. The idea is to mark text which, if excised, would result in a grammatical sentence. The MRG files do not mark other types of disfluencies. We follow the evaluation defined by Charniak and Johnson (2001), which evaluates the accuracy of identifying speech repairs and restarts. This definition of the task is the standard in recent work. The reason for this is that filled pauses can be detected using a simple rule-based approach, and parentheticals have less impact on readability and down-stream processing accuracy. The MRG and DPS layers have high but imperfect agreement over what tokens they mark as speech repairs: of the text annotated with both layers, 33,720 tokens are marked as disfluent in at least one layer, 32,310 are only marked as disfluent by the DPS files, and 32,742 are only marke"
Q14-1011,P05-1022,1,0.74136,"Missing"
Q14-1011,W02-1001,0,0.0909652,"se went is disfluent, but the Left-Arc and even the Right-Arc are also correct, in that there are continuations from them that lead to the gold-standard analysis. We regard all transition sequences that can result in the correct analysis as equally valid, and want to avoid stipulating one of them during training. We achieve this by following Goldberg and Nivre (2012) in using a dynamic oracle to create partially labelled training data.2 A dynamic oracle is a function that determines the cost of applying an action to a state, in terms of gold-standard arcs that are newly unreachable. We follow Collins (2002) in training an averaged perceptron model to predict transition sequences, rather than individual transitions. This type of model is often referred to as a structured perceptron, or sometimes a global perceptron. During training, if the model does not predict the correct sequence, an update is performed, based on the gold-standard sequence and part of the sequence predicted by the current weights. Only part of the sequence is used to calculate the weight update, in order to account for search errors. We use the maximum violation strategy described by Huang et al. (2012) to select the subsequen"
Q14-1011,de-marneffe-etal-2006-generating,0,0.060663,"Missing"
Q14-1011,C12-1059,0,0.0263552,"d by the Left-Arc in Line 11, and bankrupt being RightArced to went in Line 12. To conserve space, we have omitted the dummy ROOT token, which is placed at the end of the sentence, following the suggestion of Ballesteros and Nivre (2013). The final action will be a Left-Arc from the ROOT token to went. 4.2 Dynamic Oracle Training Algorithm Our non-monotonic transition system introduces substantial spurious ambiguity: the gold-standard parse can be derived via many different transition sequences. Recent work has shown that this can be advantageous (Sartorio et al., 2013; Honnibal et al., 2013; Goldberg and Nivre, 2012), because difficult decisions can sometimes be delayed until more information is available. Line 5 of Figure 4 shows a state that introduces spurious ambiguity. From this configuration, there are multiple actions that could be considered ‘correct’, in the sense that the gold-standard analysis can be derived from them. The Edit transition is correct because went is disfluent, but the Left-Arc and even the Right-Arc are also correct, in that there are continuations from them that lead to the gold-standard analysis. We regard all transition sequences that can result in the correct analysis as equ"
Q14-1011,W07-2435,0,0.581986,"Missing"
Q14-1011,P10-1001,0,0.0514019,"Missing"
Q14-1011,J93-2004,0,0.0493884,"Tuesday |{z } I |mean FP RM IM RP Figure 1: A sentence with disfluencies annotated in the style of Shriberg (1994) and the Switchboard corpus. FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair. We follow previous work in evaluating the system on the accuracy with which it identifies speech-repairs, marked reparandum above. • several novel feature classes, • direct comparison against the two best disfluency pre-processors, and • state-of-the-art accuracy for both speech parsing and disfluency detection. 2 Switchboard Disfluency Annotations The Switchboard portion of the Penn Treebank (Marcus et al., 1993) consists of telephone conversations between strangers about an assigned topic. Two annotation layers are provided: one for syntactic bracketing (MRG files), and one for disfluencies (DPS files). The disfluency layer marks elements with little or no syntactic function, such as filled pauses and discourse markers, and annotates speech repairs using the Shriberg (1994) system of reparandum/interregnum/repair. An example is shown in Figure 1. In the syntactic annotation, edited words are covered by a special node labelled EDITED. The idea is to mark text which, if excised, would result in a gramm"
Q14-1011,W03-3017,0,0.0794707,"Missing"
Q14-1011,J08-4003,0,0.108289,"Missing"
Q14-1011,N13-1102,0,0.32503,"rs, 33,720 tokens are marked as disfluent in at least one layer, 32,310 are only marked as disfluent by the DPS files, and 32,742 are only marked as disfluent by the MRG layer. The Switchboard annotation project was not fully completed. Because disfluency annotation is cheaper to produce, many of the DPS training files do not have matching MRG files. Only 619,236 of the 1,482,845 tokens in the DPS disfluency132 detection training data have gold-standard syntactic parses. Our system requires the more expensive syntactic annotation, but we find that it outperforms the previous state-of-the-art (Qian and Liu, 2013), despite training on less than half the data. 2.1 Dependency Conversion As is standard in statistical dependency parsing of English, we acquire our gold-standard dependencies from phrase-structure trees. We used the 2013-04-05 version of the Stanford dependency converter (de Marneffe et al., 2006). As is standard for English dependency parsing, we use the Basic Dependencies scheme, which produces strictly projective representations. At first we feared that the filled pauses, disfluencies and meta-data tokens in the Switchboard corpus might disrupt the conversion process, by making it more dif"
Q14-1011,D13-1013,0,0.483899,"Missing"
Q14-1011,roark-etal-2006-sparseval,1,0.917949,"Missing"
Q14-1011,P13-1014,0,0.0118875,"the correct governor of company being assigned by the Left-Arc in Line 11, and bankrupt being RightArced to went in Line 12. To conserve space, we have omitted the dummy ROOT token, which is placed at the end of the sentence, following the suggestion of Ballesteros and Nivre (2013). The final action will be a Left-Arc from the ROOT token to went. 4.2 Dynamic Oracle Training Algorithm Our non-monotonic transition system introduces substantial spurious ambiguity: the gold-standard parse can be derived via many different transition sequences. Recent work has shown that this can be advantageous (Sartorio et al., 2013; Honnibal et al., 2013; Goldberg and Nivre, 2012), because difficult decisions can sometimes be delayed until more information is available. Line 5 of Figure 4 shows a state that introduces spurious ambiguity. From this configuration, there are multiple actions that could be considered ‘correct’, in the sense that the gold-standard analysis can be derived from them. The Edit transition is correct because went is disfluent, but the Left-Arc and even the Right-Arc are also correct, in that there are continuations from them that lead to the gold-standard analysis. We regard all transition sequen"
Q14-1011,P10-1040,0,0.0343455,"Missing"
Q14-1011,P13-1074,0,0.0627626,"Missing"
Q14-1011,J11-1005,0,0.0320454,"Missing"
Q14-1011,P11-2033,0,0.362268,"new figure-of-merit is the arithmetic mean of the candidate’s transition scores, where previously the figure-of-merit was the sum of the candidate’s transition scores. Interestingly, Zhu et al. (2013) report that they tried exactly this, and that it was less effective than their solution. We found that the features associated with the IDLE transition were uninformative (the state is at termination, so the stack and buffer are empty), and had nothing to do with how many edit transitions were earlier applied. 5 Features for the Joint Parser Our baseline parser uses the feature set described by Zhang and Nivre (2011). The feature set contains 73 templates that mostly refer to the properties of 12 context tokens: the top of the stack (S0), its two leftmost and rightmost children (S0L , S0L2 , S0R , S0R2 ), its parent and grand-parent (S0h , S0h2 ), the first word of the buffer and its two leftmost children (N0, N0L , N0LL ), and the next two words of the buffer (N1, N2). Atomic features consist of the word, part-ofspeech tag, or dependency label for these tokens; and multiple feature atoms are often combined for feature templates. There are also features for the string-distance between S0 and N0, and the l"
Q14-1011,P13-1043,0,0.0234054,"with leftward children, those children are returned to the stack, and processed again. This has little to no impact on the algorithm’s empirical efficiency, although worst-case complexity is no longer linear, but it does pose a problem for decoding. The perceptron model tends to assign large positive scores to its top prediction. We thus observed a problem when comparing paths of different lengths, at the end of the sentence. Paths that included Edit transitions were longer, so the sum of their scores tended to be higher. The same problem has been observed during incremental PCFG parsing, by Zhu et al. (2013). They introduce an additional transition, IDLE, to ensure that paths are the same length. So long as one candidate in the beam is still being processed, all other candidates apply the IDLE transition. We adopt a simpler solution. We normalise the figure-of-merit for a candidate state, which is used to rank it in the beam, by the length of its transition history. The new figure-of-merit is the arithmetic mean of the candidate’s transition scores, where previously the figure-of-merit was the sum of the candidate’s transition scores. Interestingly, Zhu et al. (2013) report that they tried exactl"
Q14-1011,P11-1071,1,0.810028,"systems, which consist of a disfluency detector, followed by our dependency parser. We also evaluate parse accuracies after oracle pre-processing, to gauge the net effect of disfluencies on our parser’s accuracy. The dependency parser for the pipeline systems was trained on text with all disfluencies removed, following Charniak and Johnson (2001). The two disfluency detection systems we used were the Qian and Liu (2013) sequence-tagging model, and a version of the Johnson and Charniak (2004) noisy channel model, using the Charniak (2001) syntactic language model and the reranking features of Zwarts and Johnson (2011). They are the two best published disfluency detection systems. 8 Results Table 1 shows the development set accuracies for our joint parser. Both the disfluency features and the Edit transition make statistically significant improvements, in both disfluency F -measure, unlabelled attachment score (UAS), and labelled attachment score (LAS). The Oracle pipeline system, which uses the gold-standard to clean disfluencies prior to parsing, shows the total impact of speech-errors on the parser. The baseline parser, which uses the Zhang and Nivre (2011) feature set plus the Brown cluster features, sc"
Q14-1011,C10-1154,1,0.806672,"Missing"
Q14-1011,W13-3518,1,\N,Missing
Q14-1011,J92-4003,0,\N,Missing
Q14-1011,P04-1005,1,\N,Missing
Q15-1022,D12-1039,0,0.0161468,"7 13,428 6,347 1,390 Finally, we also experimented on the publicly available Sanders Twitter corpus.8 This corpus consists of 5,512 Tweets grouped into four different topics (Apple, Google, Microsoft, and Twitter). Due to restrictions in Twitter’s Terms of Service, the actual Tweets need to be downloaded using 5,512 Tweet IDs. There are 850 Tweets not available to download. After removing the non-English Tweets, 3,115 Tweets remain. In addition to converting into lowercase and removing non-alphabetic characters, words were normalized by using a lexical normalization dictionary for microblogs (Han et al., 2012). We then removed stop-words, words shorter than 3 characters or appearing less than 3 times in the corpus. The four words apple, google, microsoft and twitter were removed as these four words occur in every Tweet in the corresponding topic. Moreover, words not found in both Google and Stanford vector lists were also removed.9 In all our experiments, after removing words from documents, any document with a zero word count was also removed from the corpus. For the Twitter corpus, this resulted in just 2,520 remaining Tweets. 4.1.3 General settings The hyper-parameter β used in baseline LDA and"
Q15-1022,P10-1117,1,0.214613,"ow that by using information from the external corpora, our new models produce significant improvements on topic coherence, document clustering and document classification tasks, especially on datasets with few or short documents. 1 Introduction Topic modeling algorithms, such as Latent Dirichlet Allocation (Blei et al., 2003) and related methods (Blei, 2012), are often used to learn a set of latent topics for a corpus, and predict the probabilities of each word in each document belonging to each topic (Teh et al., 2006; Newman et al., 2006; Toutanova and Johnson, 2008; Porteous et al., 2008; Johnson, 2010; Xie and Xing, 2013; Hingmire et al., 2013). Conventional topic modeling algorithms such as these infer document-to-topic and topic-to-word distributions from the co-occurrence of words within documents. But when the training corpus of documents is small or when the documents are short, the resulting distributions might be based on little evidence. Sahami and Heilman (2006) and Phan et al. (2011) show that it helps to exploit external knowledge to improve the topic representations. Sahami and Heilman (2006) employed web search results to improve the information in short texts. Phan et al. (20"
Q15-1022,E14-1056,0,0.120704,"Missing"
Q15-1022,D11-1024,0,0.785259,"Missing"
Q15-1022,N10-1012,0,0.380251,"Missing"
Q15-1022,Q15-1004,0,0.0102457,"ent. This approximation is reasonably accurate for short documents. This distribution Q simplifies the coupling between zd and sd . This enables us to integrate out sd in Q. We first sample the document topic zd for document d using Q(zd ), marginalizing over sd : t,wd i i ¬d t ¬d i &gt; 3.5 Learning latent feature vectors for topics 4.1 To estimate the topic vectors after each Gibbs sampling iteration through the data, we apply regularized maximum likelihood estimation. Applying MAP estimation to learn log-linear models for topic models is also used in SAGE (Eisenstein et al., 2011) and SPRITE (Paul and Dredze, 2015). How4.1.1 Distributed word representations We experimented with two state-of-the-art sets of pre-trained word vectors here. 303 1 Experimental setup The L2 regularizer constant was set to µ = 0.01. We used the L-BFGS implementation from the Mallet toolkit (McCallum, 2002). 2 Google word vectors3 are pre-trained 300dimensional vectors for 3 million words and phrases. These vectors were trained on a 100 billion word subset of the Google News corpus by using the Google Word2Vec toolkit (Mikolov et al., 2013). Stanford vectors4 are pre-trained 300-dimensional vectors for 2 million words. These ve"
Q15-1022,D14-1162,0,0.125631,"pic representations in the small corpus. However, if the larger corpus has many irrelevant topics, this will “use up” the topic space of the model. In addition, Petterson et al. (2010) proposed an extension of LDA that uses external information about word similarity, such as thesauri and dictionaries, to smooth the topic-to-word distribution. Topic models have also been constructed using latent features (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013; Cao et al., 2015). Latent feature (LF) vectors have been used for a wide range of NLP tasks (Glorot et al., 2011; Socher et al., 2013; Pennington et al., 2014). The combination of values permitted by latent features forms a high dimensional space which makes it is well suited to model topics of very large corpora. Rather than relying solely on a multinomial or latent feature model, as in Salakhutdinov and Hinton (2009), Srivastava et al. (2013) and Cao et al. (2015), we explore how to take advantage of both latent feature and multinomial models by using a latent feature representation trained on a large external corpus to supplement a multinomial topic model estimated from a smaller corpus. Our main contribution is that we propose two new latent fea"
Q15-1022,P13-1045,0,0.0165863,"to help shape the topic representations in the small corpus. However, if the larger corpus has many irrelevant topics, this will “use up” the topic space of the model. In addition, Petterson et al. (2010) proposed an extension of LDA that uses external information about word similarity, such as thesauri and dictionaries, to smooth the topic-to-word distribution. Topic models have also been constructed using latent features (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013; Cao et al., 2015). Latent feature (LF) vectors have been used for a wide range of NLP tasks (Glorot et al., 2011; Socher et al., 2013; Pennington et al., 2014). The combination of values permitted by latent features forms a high dimensional space which makes it is well suited to model topics of very large corpora. Rather than relying solely on a multinomial or latent feature model, as in Salakhutdinov and Hinton (2009), Srivastava et al. (2013) and Cao et al. (2015), we explore how to take advantage of both latent feature and multinomial models by using a latent feature representation trained on a large external corpus to supplement a multinomial topic model estimated from a smaller corpus. Our main contribution is that we"
Q15-1022,D12-1087,0,0.169543,"Missing"
Q18-1048,P11-1062,0,0.135964,"t and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies that of i→k. Although they showed transitivity constraints to be effective in learning entailment graphs, the Integer Linear Programming (ILP) solution of Berant et al. is not scalable beyond a few hundred nodes. In fact, the problem of finding a maximally weighted transitive subgraph of a graph with arbitrary edge weights is NP-hard (Berant et al., 2011). This paper instead proposes a scalable solution that does not rely on transitivity closure, but 703 Transactions of the Association fo"
Q18-1048,D15-1075,0,0.125403,"Missing"
Q18-1048,D17-1070,0,0.0422865,"Missing"
Q18-1048,P14-1061,1,0.833691,"better by additionally assuming the entailment graphs are “forest reducible,” where a predicate cannot entail two (or more) predicates j and k such that neither j→k nor k→j (FRG assumption). However, the FRG assumption is not correct for many real-world domains. For example, a person visiting a place entails both arriving at that place and leaving that place, although the latter do not necessarily entail each other. Our work injects two other types of prior knowledge about the structure of the graph that are less expensive to incorporate and yield better results on entailment rule data sets. Abend et al. (2014) learn entailment relations over multi-word predicates with different levels of compositionality. Pavlick et al. (2015) add variety of relations, including entailment, to phrase pairs in PPDB. This includes a broader range of entailment relations such as lexical entailment. In contrast to our method, these works rely on supervised data and take a local learning approach. Another related strand of research is link prediction (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Yang et al., 2015; Trouillon et al., 2016; Dettmers et al., 2018), where the source data are extractions fro"
Q18-1048,P15-1034,0,0.0372871,"n C1 =3 unique predicates; (2) remove any predicate that is observed with fewer than C2 =3 unique argument-pairs. This leaves us with |P |=101K unique predicates in 346 entailment graphs. The maximum graph size is 53K nodes,8 and the total number of non-zero local scores in all graphs is 66M. In the future, we plan to test our method on an even larger corpus, but preliminary experiments suggest that data sparsity will persist regardless of the corpus size, because of the power law distribution of the terms. We compared our extractions qualitatively with Stanford Open IE (Etzioni et al., 2011; Angeli et al., 2015). Our CCG-based extraction generated noticeably 7 In our experiments, the total number of edges is ≈ .01|V |2 and most of predicate pairs are seen in less than 20 subgraphs, rather than |T |2 . 8 There are 4 graphs with more than 20K nodes, 3 graphs with 10K to 20K nodes, and 16 graphs with 1K to 10K nodes. 708 better relations for longer sentences with longrange dependencies such as those involving coordination. 5.2 ment graphs based on the predicates in their corpus. The data set contains 3,427 edges (positive), and 35,585 non-edges (negative). We evaluate our method on all the examples of B"
Q18-1048,J15-2003,0,0.30255,"ing the relation name with the object. For example, for the sentence China has a border with India, we extract a relation have border1,with between China and India. We perform a similar process for prepositional phrases attached to verb phrases. Most of the light verbs and multiword predicates will be extracted by the above post-processing (e.g., take care1,of ), which will recover many salient ternary relations. Although entailments and paraphrasing can benefit from n-ary relations—for example, person visits a location in a time—we currently follow previous work (Lewis and Steedman, 2013a); (Berant et al., 2015) in confining our attention to binary relations, leaving the construction of n-ary graphs to future work. wisdom is that entailment relations are a byproduct of these methods (Riedel et al., 2013). However, this assumption has not usually been explicitly evaluated. Explicit entailment rules provide explainable resources that can be used in downstream tasks. Our experiments show that our method significantly outperforms a state-of-the-art link prediction method. 3 Computing Local Similarity Scores We first extract binary relations as predicateargument pairs using a combinatory categorial gramma"
Q18-1048,D17-1091,1,0.841356,"only a few hours to apply to more than 100K predicates.1,2 Our experiments (§6) show that the global scores improve significantly over local scores and outperform state-of-the-art entailment graphs on two standard entailment rule data sets (Berant et al., 2011; Holt, 2018). We ultimately intend the typed entailment graphs to provide a resource for entailment and paraphrase rules for use in semantic parsing and open domain question answering, as has been done for similar resources such as the Paraphrase Database (PPDB; Ganitkevitch et al., 2013; Pavlick et al., 2015) in Wang et al. (2015) and Dong et al. (2017).3 With that end in view, we have included a comparison with PPDB in our evaluation on the entailment data sets. We also show that the learned entailment rules improve performance on a question-answering task (§7) with no tuning or prior knowledge of the task. 2 Figure 2: Learning entailments that are consistent (A) across different but related typed entailment graphs and (B) within each graph. 0 ≤ β ≤ 1 determines how much different graphs are related. The dotted edges are missing, but will be recovered by considering relationships shown by across-graph (red) and within-graph (light blue) con"
Q18-1048,P12-1013,0,0.0150022,"ith PPDB in our evaluation on the entailment data sets. We also show that the learned entailment rules improve performance on a question-answering task (§7) with no tuning or prior knowledge of the task. 2 Figure 2: Learning entailments that are consistent (A) across different but related typed entailment graphs and (B) within each graph. 0 ≤ β ≤ 1 determines how much different graphs are related. The dotted edges are missing, but will be recovered by considering relationships shown by across-graph (red) and within-graph (light blue) connections. on assumptions concerning the graph structure. Berant et al. (2012, 2015) propose Tree-Node-Fix (TNF), an approximation method that scales better by additionally assuming the entailment graphs are “forest reducible,” where a predicate cannot entail two (or more) predicates j and k such that neither j→k nor k→j (FRG assumption). However, the FRG assumption is not correct for many real-world domains. For example, a person visiting a place entails both arriving at that place and leaving that place, although the latter do not necessarily entail each other. Our work injects two other types of prior knowledge about the structure of the graph that are less expensiv"
Q18-1048,P16-2041,0,0.530878,"han 20K nodes, 3 graphs with 10K to 20K nodes, and 16 graphs with 1K to 10K nodes. 708 better relations for longer sentences with longrange dependencies such as those involving coordination. 5.2 ment graphs based on the predicates in their corpus. The data set contains 3,427 edges (positive), and 35,585 non-edges (negative). We evaluate our method on all the examples of Berant’s entailment data set. The types of this data set do not match with FIGER types, but we perform a simple handmapping between their types and FIGER types.10 Evaluation Entailment Data Sets Levy/Holt’s Entailment Data Set Levy and Dagan (2016) proposed a new annotation method (and a new data set) for collecting relational inference data in context. Their method removes a major bias in other inference data sets such as Zeichner’s (Zeichner et al., 2012), where candidate entailments were selected using a directional similarity measure. Levy and Dagan form questions of the type which city (qtype ), is located near (qrel ), mountains (qarg )? and provide possible answers of the form Kyoto (aanswer ), is surrounded by (arel ), mountains (aarg ). Annotators are shown a question with multiple possible answers, where aanswer is masked by q"
Q18-1048,S17-1026,0,0.22754,"Missing"
Q18-1048,N13-1092,0,0.0938356,"Missing"
Q18-1048,D13-1064,1,0.942149,"third argument by concatenating the relation name with the object. For example, for the sentence China has a border with India, we extract a relation have border1,with between China and India. We perform a similar process for prepositional phrases attached to verb phrases. Most of the light verbs and multiword predicates will be extracted by the above post-processing (e.g., take care1,of ), which will recover many salient ternary relations. Although entailments and paraphrasing can benefit from n-ary relations—for example, person visits a location in a time—we currently follow previous work (Lewis and Steedman, 2013a); (Berant et al., 2015) in confining our attention to binary relations, leaving the construction of n-ary graphs to future work. wisdom is that entailment relations are a byproduct of these methods (Riedel et al., 2013). However, this assumption has not usually been explicitly evaluated. Explicit entailment rules provide explainable resources that can be used in downstream tasks. Our experiments show that our method significantly outperforms a state-of-the-art link prediction method. 3 Computing Local Similarity Scores We first extract binary relations as predicateargument pairs using a comb"
Q18-1048,W14-2406,1,0.894326,"Missing"
Q18-1048,P05-1014,0,0.315327,"o arguments, where the type of each predicate is determined by the types of its arguments. We construct typed entailment graphs, with typed predicates as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions signif"
Q18-1048,P98-2127,0,0.422662,"es as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies"
Q18-1048,P13-2078,0,0.0219818,"ype of each predicate is determined by the types of its arguments. We construct typed entailment graphs, with typed predicates as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imp"
Q18-1048,C16-1268,0,0.106009,"ed by the types of its arguments. We construct typed entailment graphs, with typed predicates as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on"
Q18-1048,P18-1188,1,0.78834,"Missing"
Q18-1048,W04-3250,0,0.0561007,"Missing"
Q18-1048,W17-2623,0,0.121698,"ign workers . . . . . . Barnes & Noble CEO William Lynch said as he unveiled his company’s Nook Tablet on Monday. The report said opium has accounted for more than half of Afghanistan’s gross domestic product in 2007. Who praised Mitt Romney’s credentials? Which gene did the ALS association discover ? How many Americans suffer from food allergies? What law might the deal break? Who launched the Nook Tablet? What makes up half of Afghanistans GDP ? Table 3: Examples where explicit entailment relations improve the rankings. The related words are boldfaced. contains questions about CNN articles (Trischler et al., 2017). Machine reading comprehension is usually evaluated by posing questions about a text passage and then assessing the answers of a system (Trischler et al., 2017). The data sets that are used for this task are often in the form of (document,question,answer) triples, where answer is a short span of the document. Answer selection is an important task, where the goal is to select the sentence(s) that contain the answer. We show improvements by adding knowledge from our learned entailments without changing the graphs or tuning them to this task in any way. Inverse sentence frequency (ISF) is a stro"
Q18-1048,P15-2070,0,0.0778647,"Missing"
Q18-1048,D14-1162,0,0.0911904,"Missing"
Q18-1048,P15-1129,0,0.0166683,"arallelizable and takes only a few hours to apply to more than 100K predicates.1,2 Our experiments (§6) show that the global scores improve significantly over local scores and outperform state-of-the-art entailment graphs on two standard entailment rule data sets (Berant et al., 2011; Holt, 2018). We ultimately intend the typed entailment graphs to provide a resource for entailment and paraphrase rules for use in semantic parsing and open domain question answering, as has been done for similar resources such as the Paraphrase Database (PPDB; Ganitkevitch et al., 2013; Pavlick et al., 2015) in Wang et al. (2015) and Dong et al. (2017).3 With that end in view, we have included a comparison with PPDB in our evaluation on the entailment data sets. We also show that the learned entailment rules improve performance on a question-answering task (§7) with no tuning or prior knowledge of the task. 2 Figure 2: Learning entailments that are consistent (A) across different but related typed entailment graphs and (B) within each graph. 0 ≤ β ≤ 1 determines how much different graphs are related. The dotted edges are missing, but will be recovered by considering relationships shown by across-graph (red) and within"
Q18-1048,Q14-1030,1,0.832844,"(if any). We thus type all entities that can be grounded in Wikipedia. We first map the Wikipedia URL of the entities to Freebase (Bollacker et al., 2008). We select the most notable type of the entity from Freebase and map it to FIGER types (Ling and Weld, 2012) such as building, disease, person, and location, using only the first level of the FIGER type hierarchy.4 For example, instead of event/sports_event, we use event as type. If an entity cannot be grounded in Wikipedia or its Freebase type does not have a mapping to FIGER, we assign the default type thing to it. The semantic parser of Reddy et al. (2014), GraphParser, is run on the NewsSpike corpus (Zhang and Weld, 2013) to extract binary relations between a predicate and its arguments from sentences. GraphParser uses CCG syntactic derivations and λ-calculus to convert sentences to neo-Davisonian semantics, a first-order logic that uses event identifiers (Parsons, 1990). For example, for the sentence, Obama visited Hawaii in 2012, GraphParser produces the logical form ∃e.visit1 (e, Obama) ∧ visit2 (e, Hawaii)∧ visitin (e, 2012), where e denotes an event. We will consider a relation for each pair of arguments, hence, there will be three relati"
Q18-1048,W03-1011,0,0.834164,"and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies that of i→k. Although"
Q18-1048,N13-1008,0,0.245356,"dge about the structure of the graph that are less expensive to incorporate and yield better results on entailment rule data sets. Abend et al. (2014) learn entailment relations over multi-word predicates with different levels of compositionality. Pavlick et al. (2015) add variety of relations, including entailment, to phrase pairs in PPDB. This includes a broader range of entailment relations such as lexical entailment. In contrast to our method, these works rely on supervised data and take a local learning approach. Another related strand of research is link prediction (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Yang et al., 2015; Trouillon et al., 2016; Dettmers et al., 2018), where the source data are extractions from text, facts in knowledge bases, or both. Unlike our work, which directly learns entailment relations between predicates, these methods aim at predicting the source data—that is, whether two entities have a particular relationship. The common Related Work Our work is closely related to Berant et al. (2011), where entailment graphs are learned by imposing transitivity constraints on the entailment relations. However, the exact solution to the problem is not scalabl"
Q18-1048,D10-1106,0,0.090144,"Missing"
Q18-1048,P12-2031,0,0.170474,"Missing"
Q18-1048,D13-1183,0,0.540702,"edia. We first map the Wikipedia URL of the entities to Freebase (Bollacker et al., 2008). We select the most notable type of the entity from Freebase and map it to FIGER types (Ling and Weld, 2012) such as building, disease, person, and location, using only the first level of the FIGER type hierarchy.4 For example, instead of event/sports_event, we use event as type. If an entity cannot be grounded in Wikipedia or its Freebase type does not have a mapping to FIGER, we assign the default type thing to it. The semantic parser of Reddy et al. (2014), GraphParser, is run on the NewsSpike corpus (Zhang and Weld, 2013) to extract binary relations between a predicate and its arguments from sentences. GraphParser uses CCG syntactic derivations and λ-calculus to convert sentences to neo-Davisonian semantics, a first-order logic that uses event identifiers (Parsons, 1990). For example, for the sentence, Obama visited Hawaii in 2012, GraphParser produces the logical form ∃e.visit1 (e, Obama) ∧ visit2 (e, Hawaii)∧ visitin (e, 2012), where e denotes an event. We will consider a relation for each pair of arguments, hence, there will be three relations for the given sentence: visit1,2 with arguments (Obama, Hawaii),"
Q18-1048,C08-1107,0,0.828734,"as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies that of i→k. Although they showed transitivity"
roark-etal-2006-sparseval,A00-2018,1,\N,Missing
roark-etal-2006-sparseval,J93-2004,0,\N,Missing
roark-etal-2006-sparseval,P97-1003,0,\N,Missing
roark-etal-2006-sparseval,N01-1016,1,\N,Missing
roark-etal-2006-sparseval,N04-4032,1,\N,Missing
roark-etal-2006-sparseval,J01-2004,1,\N,Missing
U10-1015,H01-1052,0,0.0391686,"y detection in transcripts of spontaneous spoken language. Many models have been proposed for this task in the literature; the best performing models so far are statistical by nature and have large data needs. A statistical natural language processing algorithm typically has two important components: a model that describes the behaviour of interest, and the training data which is necessary to guide that model. It has been observed that simple algorithms can outperform more complex models when these simple algorithms have the advantage in terms of the amount of data available; so, for example, Brill and Banko (2001) argue that more data is more important than better algorithms for some natural language processing tasks. It is this insight that drives the work described in this paper. Our current approach to speech disfluency detection is trained on manually-constructed spoken language corpora which contain annotations of all disfluencies as part of the transcription process. Our model is based on the noisy channel model and consists of a language model and a channel model. As we have reported elsewhere (Zwarts et al., 2010), we are able to achieve reasonable results when using Switchboard data: we obtain"
U10-1015,P01-1017,0,0.0929352,"Missing"
U10-1015,P04-1005,1,0.818224,"our work typically have this characteristic: when a speaker edits her speech for meaningrelated reasons, rather than errors that arise from performance, the resulting disfluency can be by itself fluent. We can see this in Example (1): the repair and the reparandum are equally fluent. This makes it difficult to distinguish reparanda as being part of disfluencies when only lexical cues are available. Since the transcripts we work with do not have prosodic cues annotated, we need to look elsewhere for a solution to this problem. Noisy Channel models have done very well in this area; the work of Johnson and Charniak (2004) explores such an approach. This approach performs very well when compared with other approaches. Johnson et al. (2004) adds some handwritten rules to the noisy channel model, providing the current state of the art in disfluency detection. Lease and Johnson (2006) also use this approach, but they are particularly interested in finding fillers; they use early filler detection and deletion in this model. The following section describes the noisy channel approach in more detail. 3.2 The Noisy Channel Approach The approach we build on is that first introduced by Johnson and Charniak (Johnson and C"
U10-1015,N06-2019,1,0.850171,"re equally fluent. This makes it difficult to distinguish reparanda as being part of disfluencies when only lexical cues are available. Since the transcripts we work with do not have prosodic cues annotated, we need to look elsewhere for a solution to this problem. Noisy Channel models have done very well in this area; the work of Johnson and Charniak (2004) explores such an approach. This approach performs very well when compared with other approaches. Johnson et al. (2004) adds some handwritten rules to the noisy channel model, providing the current state of the art in disfluency detection. Lease and Johnson (2006) also use this approach, but they are particularly interested in finding fillers; they use early filler detection and deletion in this model. The following section describes the noisy channel approach in more detail. 3.2 The Noisy Channel Approach The approach we build on is that first introduced by Johnson and Charniak (Johnson and Charniak, 2004). This approach is modular by nature, making it possible to interchange different sub-components. The original paper explores the use of different types of language models, and demonstrates how some models provide better overall performance than othe"
U10-1015,J10-1001,0,0.01946,"nd end positions of each of these three components. We can think of each word in an utterance as belonging to one of four categories: fluent material, reparandum, filler, or repair. We can then assess the accuracy of techniques that attempt to detect disfluencies by computing precision and recall values for the assignment of the correct categories to each of the words in the utterance, as compared to the gold standard as indicated by annotations in the corpus. 3 3.1 Disfluency Detection Models Related Work A number of different techniques have been proposed for automatic disfluency detection. Schuler et al. (2010) propose a Hierarchical Hidden Markov Model approach; this is a statistical approach which builds up a syntactic analysis of the sentence and marks those subtrees which it considers to be made up of disfluent material. Although this is one of the few models that actually builds up a syntactic analysis of the utterance being analysed, its final F-score for fluency detection is lower than that of other models. Snover et al. (2004) investigate the use of purely lexical features combined with part-ofspeech tags to detect disfluencies. This approach is compared against approaches which use primaril"
U10-1015,C90-3045,0,0.0515901,"Missing"
U10-1015,N04-4040,0,0.0326869,"d by annotations in the corpus. 3 3.1 Disfluency Detection Models Related Work A number of different techniques have been proposed for automatic disfluency detection. Schuler et al. (2010) propose a Hierarchical Hidden Markov Model approach; this is a statistical approach which builds up a syntactic analysis of the sentence and marks those subtrees which it considers to be made up of disfluent material. Although this is one of the few models that actually builds up a syntactic analysis of the utterance being analysed, its final F-score for fluency detection is lower than that of other models. Snover et al. (2004) investigate the use of purely lexical features combined with part-ofspeech tags to detect disfluencies. This approach is compared against approaches which use primarily prosodic cues, and appears to perform equally well. However, the authors note that this model finds it difficult to identify disfluencies which by themselves are very fluent. The edit repairs which are the focus of our work typically have this characteristic: when a speaker edits her speech for meaningrelated reasons, rather than errors that arise from performance, the resulting disfluency can be by itself fluent. We can see t"
U10-1015,C10-1154,1,0.707346,"s have the advantage in terms of the amount of data available; so, for example, Brill and Banko (2001) argue that more data is more important than better algorithms for some natural language processing tasks. It is this insight that drives the work described in this paper. Our current approach to speech disfluency detection is trained on manually-constructed spoken language corpora which contain annotations of all disfluencies as part of the transcription process. Our model is based on the noisy channel model and consists of a language model and a channel model. As we have reported elsewhere (Zwarts et al., 2010), we are able to achieve reasonable results when using Switchboard data: we obtain an F-score of 0.757 in determining which constituents of an utterance belong to a disfluency. We would like to see if we can improve on our previously reported performance by adding more data. Our language model does not need any special annotation, and so our first set of experiments investigates whether we can improve results by vastly increasing the training data for the language model. The task of increasing the training data for the channel model is a more difficult one, since here we require the annotation"
U10-1015,W90-0102,0,\N,Missing
U11-1004,N09-1036,1,0.893203,"Missing"
U11-1004,N07-1018,1,0.845066,"holds, N and N2 . 6 (N ) create N empty models s0 to s0 (l) set initial weights w0 to N1 for example i = 1 → K do for particle l = 1 → N do (l) (l) sample σi ∼ Qσ (· |si−1 , oi , α) (l) (l) S (l) si = si−1 σi of the probability of the proposed segmentation under the current model. Also, the formula we use for calculating the particle weights, taken from Doucet et al. (2000), overcomes a fundamental problem in applying Particle Filters to this kind of model: we are usually not able to efficiently sample directly from P , because P does not decompose in the way required for Dynamic Programming (Johnson et al., 2007; Mochihashi et al., 2009). The SIR algorithm, however, allows us to use an arbitrary proposal distribution Q for the samples. All that is needed is that we can calculate the true probability of each sample according to P , which is easily done using eq. 6. Our proposal distribution Q ignores the dependencies between the words within an utterance, as in eq. 7. This can be thought of as ‘freezing’ the model in order to determine a segmentation, just as in the PCFG proposal distribution of Johnson et al. (2007). Thus, the proposal segmentations and other quantities required to calculate the weig"
U11-1004,N09-1069,0,0.0229435,"Missing"
U11-1004,P09-1012,0,0.0840452,") create N empty models s0 to s0 (l) set initial weights w0 to N1 for example i = 1 → K do for particle l = 1 → N do (l) (l) sample σi ∼ Qσ (· |si−1 , oi , α) (l) (l) S (l) si = si−1 σi of the probability of the proposed segmentation under the current model. Also, the formula we use for calculating the particle weights, taken from Doucet et al. (2000), overcomes a fundamental problem in applying Particle Filters to this kind of model: we are usually not able to efficiently sample directly from P , because P does not decompose in the way required for Dynamic Programming (Johnson et al., 2007; Mochihashi et al., 2009). The SIR algorithm, however, allows us to use an arbitrary proposal distribution Q for the samples. All that is needed is that we can calculate the true probability of each sample according to P , which is easily done using eq. 6. Our proposal distribution Q ignores the dependencies between the words within an utterance, as in eq. 7. This can be thought of as ‘freezing’ the model in order to determine a segmentation, just as in the PCFG proposal distribution of Johnson et al. (2007). Thus, the proposal segmentations and other quantities required to calculate the weights and propagate the part"
U11-1004,J01-3002,0,0.11032,"nline’ in the literature. For example, the Online EM algorithms in Liang and Klein (2009) make local updates but iterate over the whole data multiple times, thus violating (a). Pearl et al.’s (2011) DMCMC algorithm, discussed in the next section, is able to revisit earlier examples in the light of new evidence, violating thus both (a) and (b). 3 Previous work Online learning algorithms for Bayesian models are discussed within both Statistics and Computational Linguistics but have, to our knowledge, not yet been widely applied to the specific problem of word segmentation. Both Brent (1999) and Venkataraman (2001) propose heuristic online learning algorithms employing Dynamic Programming that have, however, been shown to not actually maximize the objective defined by the model (Goldwater, 2007). Brent’s algorithm has recently been reconsidered as an online learning algorithm in Pearl et al. (2011). It is an instance of the familiar Viterbi algorithm that efficiently finds the optimal solution to many problems; not, however, for the word segmentation models under discussion. The algorithm is “greedy” in that it determines the locally optimal segmentation for an utterance given its current knowledge usin"
U11-1005,D11-1131,1,0.836476,"example below, we want to to learn a map that generalizes to previously unseen sentences. 1. Sentence: what is the capital of texas ? Meaning: answer(capital 1(stateid(texas))) Researchers have formalized the learning problem in various ways, with approaches including Sharon Goldwater School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK sgwater@inf.ed.ac.uk string classifiers (Kate and Mooney, 2006), synchronous grammar (Wong and Mooney, 2006), combinatory categorial grammar (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010), and PCFG-based approaches (Lu et al., 2008; Borschinger et al., 2011). Each approach has required its own custom algorithms, which has made model development and innovation slow. Nevertheless, there are many similarities between the approaches, which all exploit parallels between the structure of the meaning representation and that of the natural language. The meaning representation, as a context-free formal language, has an obvious tree structure. Trees are also widely used to describe natural language structure. Consequently, the semantic parsing problem can be generally defined as learning a mapping between trees, one of which may be latent. This mapping can"
U11-1005,J93-2003,0,0.0166146,"tup We use Tiburon (May and Knight, 2006), a tree transducer toolkit, to train our transducer using 40 iterations of its inside-outside-like EM training procedure, and modify it slightly to include the mean field VB approximation for a symmetric Dirichlet prior over the multinomial parameters as just described. Decoding is handled the same by Tiburon for both training procedures, producing the MR input tree with the tree transducer derivation that maximizes the probability over derivations of equation 2. In keeping with the original hybrid tree, we run 100 iterations of IBM alignment model 1 (Brown et al., 1993) to initialize the word distribution parameters. Also in keeping with Lu et al. (2008), we use the standard noun phrase list from the given language to help initialize the word distributions for their counterparts in the meaning representation language. 9 Results To evaluate our models, we use the the GeoQuery corpus, a standard benchmark data set. The corpus contains English sentences (questions about U.S. geography) paired with an MR in a database query language, 250 of which were translated into Japanese (among other languages) yielding two training sets using the same MRs. For testing we r"
U11-1005,J08-3004,0,0.257175,"1970) are generalizations of finite state machines that take trees as inputs and either output a string or another tree. Mirroring the branching nature of its input, the tree transducer may simultaneously transition to any number of successor states, assigning a separate state to process each sub-tree. Although they were originally conceived of by Rounds (1970) as a way to formalize tree transformations in linguistic theory, they have since received far more interest in theoretical computer science. Recently, however, they have also been used for syntax-based statistical machine translation (Graehl et al., 2008; Knight and Greahl, 2005). Figure 1 presents an example of a tree-to-tree transducer. It is defined using tree transformation rules, where the left hand side identifies a state of the transducer and a fragment of the input tree, and the right hand side describes a fragment of the output tree. Variables xi stand for entire sub-trees. There are many classes of transducer, each with its own selection of algorithms (Knight and Greahl, 2005). In this paper we restrict consideration primarily to the extended left hand side, root-to-frontier, linear, nondeleting tree transducers (Maletti et al., 200"
U11-1005,P06-1115,0,0.0285966,"nguage sentences to formal representations of their meaning, a problem that arises in developing natural language interfaces, for example. Given a set of (sentence, meaning representation) pairs like the example below, we want to to learn a map that generalizes to previously unseen sentences. 1. Sentence: what is the capital of texas ? Meaning: answer(capital 1(stateid(texas))) Researchers have formalized the learning problem in various ways, with approaches including Sharon Goldwater School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK sgwater@inf.ed.ac.uk string classifiers (Kate and Mooney, 2006), synchronous grammar (Wong and Mooney, 2006), combinatory categorial grammar (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010), and PCFG-based approaches (Lu et al., 2008; Borschinger et al., 2011). Each approach has required its own custom algorithms, which has made model development and innovation slow. Nevertheless, there are many similarities between the approaches, which all exploit parallels between the structure of the meaning representation and that of the natural language. The meaning representation, as a context-free formal language, has an obvious tree structure. Trees are"
U11-1005,D10-1119,1,0.915678,"ample. Given a set of (sentence, meaning representation) pairs like the example below, we want to to learn a map that generalizes to previously unseen sentences. 1. Sentence: what is the capital of texas ? Meaning: answer(capital 1(stateid(texas))) Researchers have formalized the learning problem in various ways, with approaches including Sharon Goldwater School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK sgwater@inf.ed.ac.uk string classifiers (Kate and Mooney, 2006), synchronous grammar (Wong and Mooney, 2006), combinatory categorial grammar (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010), and PCFG-based approaches (Lu et al., 2008; Borschinger et al., 2011). Each approach has required its own custom algorithms, which has made model development and innovation slow. Nevertheless, there are many similarities between the approaches, which all exploit parallels between the structure of the meaning representation and that of the natural language. The meaning representation, as a context-free formal language, has an obvious tree structure. Trees are also widely used to describe natural language structure. Consequently, the semantic parsing problem can be generally defined as learnin"
U11-1005,P11-1060,0,0.0143339,"n a standard PCFG) distinguishes between functions and predicates with the same name but different semantics (Wong and Mooney, 2006). To formally define the probability of the MR, let paths be the set of paths from the root to every node in the MR where paths are represented using a variety of Gorn’s notation (Gorn, 1962)2 . Let argsi be the set of indices of the children of the node at path i; and Ri be the grammar rule that derives the symbol at i according to the MR parse. Then, the following 1 With a pre-parsing step, it may also be possible to represent lambda expressions with trees (see Liang et al. (2011)). 2 I.e., paths are represented by strings where the empty string ǫ is the path to the root, and if i is a path and j is the index of a child of the node at i, i · j is the path to that child. 21 equation defines P (MR). Y P (MR) =P (Rǫ ) Y P (Ri·j |j, Ri ) (1) i∈paths j∈argsi In other words, each node in the tree is generated according to the probability of the MR rule that derives it conditioned on (1) the MR rule Ri that derives its parent symbol and (2) its position j beneath that parent. The hybrid tree model then re-orders and extends this basic skeleton to include the NL. The probabili"
U11-1005,D08-1082,0,0.103566,"n) pairs like the example below, we want to to learn a map that generalizes to previously unseen sentences. 1. Sentence: what is the capital of texas ? Meaning: answer(capital 1(stateid(texas))) Researchers have formalized the learning problem in various ways, with approaches including Sharon Goldwater School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK sgwater@inf.ed.ac.uk string classifiers (Kate and Mooney, 2006), synchronous grammar (Wong and Mooney, 2006), combinatory categorial grammar (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010), and PCFG-based approaches (Lu et al., 2008; Borschinger et al., 2011). Each approach has required its own custom algorithms, which has made model development and innovation slow. Nevertheless, there are many similarities between the approaches, which all exploit parallels between the structure of the meaning representation and that of the natural language. The meaning representation, as a context-free formal language, has an obvious tree structure. Trees are also widely used to describe natural language structure. Consequently, the semantic parsing problem can be generally defined as learning a mapping between trees, one of which may"
U11-1005,C69-0101,0,0.637729,"frontier, linear, non-deleting, tree-to-tree transducer (a) and an example derivation (b). Numbered arrows in the derivation indicate which rules apply during that step. Rule [1] is the only rule with an extended left hand side. framework, and add a small extension, made easy by the framework. We also update a standard tree transducer training algorithm to incorporate a Variational Bayes approximation. The result is the first purely generative model to achieve state-of-the-art results on a standard data set. 2 Extended, root-to-frontier, linear, non-deleting tree transducers Tree transducers (Rounds, 1970; Thatcher, 1970) are generalizations of finite state machines that take trees as inputs and either output a string or another tree. Mirroring the branching nature of its input, the tree transducer may simultaneously transition to any number of successor states, assigning a separate state to process each sub-tree. Although they were originally conceived of by Rounds (1970) as a way to formalize tree transformations in linguistic theory, they have since received far more interest in theoretical computer science. Recently, however, they have also been used for syntax-based statistical machine tr"
U11-1005,N06-1056,0,\N,Missing
U11-1006,N07-1018,1,0.924549,"l on separate instances of the parser. This Mark Johnson. 2011. Parsing in Parallel on Multiple Cores and GPUs. In Proceedings of Australasian Language Technology Association Workshop, pages 29−37 is likely to be the best way to exploit parallelism with networked clusters and SMP multi-core machines when parsing a large corpus of sentences offline. However, there are situations where parsing must be on-line; e.g., when parsing is a component of a system that interacts with users, or with machine-learning algorithms such as MetropolisHastings Sampling that update after each sentence is parsed (Johnson et al., 2007). 2 Previous work for i in 0, . . ., n−1: for a in 0, . . ., m−1: C[i,i+1,a] = T[W[i],a] Parsing in parallel has been studied for several decades, and space constraints prevent anything but a cursory summary here. Hill and Wayne (1991) identified the basic data dependencies between the entries in chart cells, and discussed their implications for parallel parsing. Nijholt (1994) also studied the order in which chart cells can be filled, and discusses its implications for a variety of shiftreduce and chart-based parsing algorithms. Thompson (1994) pointed out that the close relationship between"
U11-1006,N10-4001,0,\N,Missing
U11-1007,N09-1075,0,0.0273569,"Missing"
U11-1007,J93-2003,0,0.026388,"language model based on the Gigaword corpus (Graff et al., 2005), and show that predictors based on it are also statistically significant predictors of the N400m. However, we go on to observe that many of the prime-target word pairs in our experimental sentences are separated by more than 3 words, so there is no way that a 4-gram language can capture the relationship between these words. This leads us to develop a “pairwise-priming” language model that captures longer-range dependencies between pairs of words. This pairwise priming model is based on the IBM Model 1 machine translation model (Brown et al., 1993), and trained using a similar EM-procedure. We train this model on Gigaword, and show that predictors based on this model are also statistically significant. Finally, we compare the predictors from the various language models with the original manual classification of the experimental sentences into “constraining” or “non-constraining” contexts given by Kalikow et al. (1977). We show that the predictor based on LSA is statistically significant even when the human “constraining” annotations are present as a factor. We also find out that the 4-gram model and the pairwise-priming model are highly"
U11-1007,N01-1021,0,0.366363,"neural state. Mitchell et al. (2008) have trained a classifier that identifies the word a subject is thinking about from input derived from fMRI images of the subject’s brain, and Murphy et al. (2009) have constructed a similar classifier that takes EEG signals as its input. Abstractly then, this work uses classifiers that take as input information about a subject’s brain state to predict the (linguistic or visual) stimulus the subject is exposed to. A more traditional line of research tries to identify factors that cause particular psycholinguistic or neurolinguistic responses. For example, Hale (2001), Bicknell and Levy (2009) and many others show that predictors derived from on-line parsing models can help explain eye-movements and word-byword reading times. Abstractly, this work involves building statistical models which take as input properties of the stimuli presented to the subject (i.e., the sentence they are hearing or reading) to predict their psychological or neural responses. The goal of this line of research is to establish which properties of the input sentence or the parsing model’s state determine the psychological or neural responses, rather than just predicting these respon"
U11-1007,P10-2012,0,0.187086,". We also show that the proposed predictors can be grouped into two clusters of significant predictors, suggesting that each cluster is capturing a different characteristic of the N400m response. 1 Introduction There is increasing interest in using computational models to help understand on-line sentence processing in humans. New experimental techniques in psycholinguistics and neurolinguistics are producing rich data sets that are difficult to interpret using standard techniques, and it is reasonable to ask if the statistical models developed in computational linguistics can be helpful here (Keller, 2010). The N400 is a human brain response to semantic incongruity or implausibility that has been widely studied in psycholinguistics and neurolinguistics. A large set of factors has been shown to influence the strength of the N400, including intra- and extra-sentential context (Kutas and Federmeier, 2000; Van Petten and Kutas, 1990). Here we study the strength of the N400 as measured by magnetoencephalography (MEG) (so the signal we study is sometimes called the N400m) on sentencefinal words in a variety of “constraining” and “nonconstraining” sentential contexts (Kalikow et al., 1977). For exampl"
U11-1007,P10-1021,0,0.252137,"the prize is a non-constraining context sentence (target words are underlined in this paper). This paper shows that language models of the kind developed in computational linguistics can be used to help identify the factors that determine the strength of the N400. We investigate a number of different kinds of predictors constructed from a variety of language models and Latent Semantic Analysis (LSA) to determine how well they describe the N400. The first set of predictors is derived from LSA, which is a method for analysing relationships between a set of documents and the terms they contain (Mitchell et al., 2010). LSA has been successfully applied in similar research areas such as eyemovements and word-by-word reading times. Our experiments show that these predictors are significant in modeling the N400m response. The second set of predictors is that proposed by Roark et al. (2009), which is derived from the Roark (2001) Mehdi Parviz, Mark Johnson, Blake Johnson and Jon Brock. 2011. Using Language Models and Latent Semantic Analysis to Characterise the N400m Neural Response. In Proceedings of Australasian Language Technology Association Workshop, pages 38−46 ject is seeing or thinking based on informa"
U11-1007,D09-1065,0,0.0301816,"esponse. The second set of predictors is that proposed by Roark et al. (2009), which is derived from the Roark (2001) Mehdi Parviz, Mark Johnson, Blake Johnson and Jon Brock. 2011. Using Language Models and Latent Semantic Analysis to Characterise the N400m Neural Response. In Proceedings of Australasian Language Technology Association Workshop, pages 38−46 ject is seeing or thinking based on information about their neural state. Mitchell et al. (2008) have trained a classifier that identifies the word a subject is thinking about from input derived from fMRI images of the subject’s brain, and Murphy et al. (2009) have constructed a similar classifier that takes EEG signals as its input. Abstractly then, this work uses classifiers that take as input information about a subject’s brain state to predict the (linguistic or visual) stimulus the subject is exposed to. A more traditional line of research tries to identify factors that cause particular psycholinguistic or neurolinguistic responses. For example, Hale (2001), Bicknell and Levy (2009) and many others show that predictors derived from on-line parsing models can help explain eye-movements and word-byword reading times. Abstractly, this work involv"
U11-1007,D09-1034,0,0.315171,"ate a number of different kinds of predictors constructed from a variety of language models and Latent Semantic Analysis (LSA) to determine how well they describe the N400. The first set of predictors is derived from LSA, which is a method for analysing relationships between a set of documents and the terms they contain (Mitchell et al., 2010). LSA has been successfully applied in similar research areas such as eyemovements and word-by-word reading times. Our experiments show that these predictors are significant in modeling the N400m response. The second set of predictors is that proposed by Roark et al. (2009), which is derived from the Roark (2001) Mehdi Parviz, Mark Johnson, Blake Johnson and Jon Brock. 2011. Using Language Models and Latent Semantic Analysis to Characterise the N400m Neural Response. In Proceedings of Australasian Language Technology Association Workshop, pages 38−46 ject is seeing or thinking based on information about their neural state. Mitchell et al. (2008) have trained a classifier that identifies the word a subject is thinking about from input derived from fMRI images of the subject’s brain, and Murphy et al. (2009) have constructed a similar classifier that takes EEG sig"
U11-1007,J01-2004,0,0.651539,"onstructed from a variety of language models and Latent Semantic Analysis (LSA) to determine how well they describe the N400. The first set of predictors is derived from LSA, which is a method for analysing relationships between a set of documents and the terms they contain (Mitchell et al., 2010). LSA has been successfully applied in similar research areas such as eyemovements and word-by-word reading times. Our experiments show that these predictors are significant in modeling the N400m response. The second set of predictors is that proposed by Roark et al. (2009), which is derived from the Roark (2001) Mehdi Parviz, Mark Johnson, Blake Johnson and Jon Brock. 2011. Using Language Models and Latent Semantic Analysis to Characterise the N400m Neural Response. In Proceedings of Australasian Language Technology Association Workshop, pages 38−46 ject is seeing or thinking based on information about their neural state. Mitchell et al. (2008) have trained a classifier that identifies the word a subject is thinking about from input derived from fMRI images of the subject’s brain, and Murphy et al. (2009) have constructed a similar classifier that takes EEG signals as its input. Abstractly then, this"
U11-1007,P04-1066,0,\N,Missing
U11-1015,D11-1131,1,0.830287,"Missing"
U11-1015,P05-1022,1,0.590474,"(Granger et al., 2009), across seven languages (those of Koppel et al. (2005) with the two Asian languages Chinese and Japanese). The later work of Wong and Dras (2011), on the same data, further explored the usefulness of syntactic features in a broader sense by characterising syntactic errors with cross sections of parse trees obtained from statistical parsing. More specifically, they utilised two types of parse tree substructure to use as classification features — horizontal slices of the trees as sets of CFG production rules and the feature schemas used in discriminative parse reranking (Charniak and Johnson, 2005). It was demonstrated that using these kinds of syntactic features performs significantly better than lexical features alone. One key phenomenon observed by Wong and Dras (2011) was that there were different proportions of parse production rules indicative of particular native languages. One example is the production rule NP → NN NN, which appears to be very common amongst Chinese speakers compared with other native language groups; they claim that this is likely to reflect determiner-noun agreement errors, as that rule is used at the expense of one headed by a plural noun (NP → NN NNS). Our i"
U11-1015,P10-1117,1,0.83616,"to estimate the posterior topic distributions θ j for each document Dj as well as the posterior word distributions φi for each topic i that maximise the log likelihood of the corpus. As exact inference of these posterior distributions is generally intractable, there is a wide variety of means of approximate inference for LDA models which include approximation algorithms such as Variational Bayes (Blei et al., 2003) and expectation propagation (Minka and Lafferty, 2002) as well as Markov Chain Monte Carlo inference algorithm with Gibbs sampling (Griffiths and Steyvers, 2004). 3.1.2 LDA as PCFG Johnson (2010) showed that LDA topic models can be regarded as a specific type of probabilistic context-free grammar (PCFG), and that Bayesian inference for PCFGs can be used to learn LDA models where the inferred distributions of PCFGs correspond to those distributions of LDA. A general schema used for generating PCFG rule instances for representing m documents with t topics is as follows:2 Sentence → Doc0j Doc0j → j Doc0j → Doc0j Docj Docj → T opici T opici → w of LDA. Similarly, inference on the posterior rule distributions can be approximated with Variational Bayes and Gibbs sampling. We use this PCFG f"
U11-1015,W11-0321,0,0.164211,"exactly these that are used in NLI, so the above work does guarantee that an LDAbased approach will be helpful here. Two particularly relevant pieces of work on using LDA in classification are for the related task of authorship attribution, determining which author wrote a particular document. Rajkumar et al. (2009) claim that models with stopwords (function words) alone are sufficient to achieve high accuracy in classification, which seems to peak at 25 topics, and outperform content word-based models; the results presented in Table 2 and the discussion are, however, somewhat contradictory. Seroussi et al. (2011) also include both function words and content words in their models; they find that filtering words by frequency is almost always harmful, suggesting that function words are helping in this task.1 In this paper we will explore both function words and PoS n-grams, the latter of which is quite novel to our knowledge in terms of classification using LDA, to investigate whether clustering shows any potential for our task. 3 Experimental Setup 3.1 3.1.1 Mechanics of LDA General Definition Formally, each document is formed from a fixed set of vocabulary V and fixed set of topics T (|T |= t). Followi"
U11-1015,P08-1036,0,0.114902,"c subfields. It has also been augmented in various ways: supervised LDA, where topic models are integrated with a response variable, was introduced by Blei and McAuliffe (2008) and applied to predicting sentiment scores from movie review data, treating it as a regression problem rather than a classification problem. Work by Wang et al. (2009) followed from that, extending it to classification problems, and applying it to the simultaneous classification and annotation of images. An alternative approach to joint models of text and response variables for sentiment classification of review texts (Titov and McDonald, 2008), with a particular focus on constructing topics related to aspects of reviews (e.g. food, decor, or service for restaurant reviews), found that LDA topics were predictively useful and seemed qualitatively intuitive. In all of this preceding work, a document to be classified is represented by an exchangeable set of (content) words: function words are generally removed, and are not typically found in topics useful for classification. It is exactly these that are used in NLI, so the above work does guarantee that an LDAbased approach will be helpful here. Two particularly relevant pieces of work"
U11-1015,W07-0602,0,0.2051,"racy decreases, there is some evidence of coherent clustering, which could help with much larger syntactic feature spaces. 1 Introduction Native language identification (NLI), the task of determining the native language of an author writing in a second language, typically English, has gained increased attention in recent years. The problem was first phrased as a text classification task by Koppel et al. (2005), using a machine learner with fundamentally lexical features — function words, character ngrams, and part-of-speech (PoS) n-grams. A number of subsequent pieces of work, such as that of Tsur and Rappoport (2007), Estival et al. (2007), Wong and Dras (2009) and Wong and Dras (2011), have taken that as a starting point, typically along with a wider range of features, such as document structure or syntactic structure. Wong and Dras (2011) looked particularly at syntactic structure, in the form of production rules and parse reranking templates. They noted that they did not find the expected instances of clearly ungrammatical elements of syntactic structure indicating non-native speaker errors; instead there were just different distributions over regular elements of grammatical structure for different nat"
U11-1015,U09-1008,1,0.916533,"clustering, which could help with much larger syntactic feature spaces. 1 Introduction Native language identification (NLI), the task of determining the native language of an author writing in a second language, typically English, has gained increased attention in recent years. The problem was first phrased as a text classification task by Koppel et al. (2005), using a machine learner with fundamentally lexical features — function words, character ngrams, and part-of-speech (PoS) n-grams. A number of subsequent pieces of work, such as that of Tsur and Rappoport (2007), Estival et al. (2007), Wong and Dras (2009) and Wong and Dras (2011), have taken that as a starting point, typically along with a wider range of features, such as document structure or syntactic structure. Wong and Dras (2011) looked particularly at syntactic structure, in the form of production rules and parse reranking templates. They noted that they did not find the expected instances of clearly ungrammatical elements of syntactic structure indicating non-native speaker errors; instead there were just different distributions over regular elements of grammatical structure for different native languages. Our intuition is that it is se"
U11-1015,D11-1148,1,0.587838,"help with much larger syntactic feature spaces. 1 Introduction Native language identification (NLI), the task of determining the native language of an author writing in a second language, typically English, has gained increased attention in recent years. The problem was first phrased as a text classification task by Koppel et al. (2005), using a machine learner with fundamentally lexical features — function words, character ngrams, and part-of-speech (PoS) n-grams. A number of subsequent pieces of work, such as that of Tsur and Rappoport (2007), Estival et al. (2007), Wong and Dras (2009) and Wong and Dras (2011), have taken that as a starting point, typically along with a wider range of features, such as document structure or syntactic structure. Wong and Dras (2011) looked particularly at syntactic structure, in the form of production rules and parse reranking templates. They noted that they did not find the expected instances of clearly ungrammatical elements of syntactic structure indicating non-native speaker errors; instead there were just different distributions over regular elements of grammatical structure for different native languages. Our intuition is that it is several elements together t"
U14-1002,D13-1143,0,0.0924991,"y and efficacy, they suffer from a major drawback: they cannot characterise the long-range relations between words. A syntactic language model, which exploits syntactic dependencies, can incorporate richer syntactic knowledge and information through syntactic parsing. In particular, syntactic structure leads to better performance of language model compared to traditional n-gram language models (Chelba and Jelinek, 2000). Most of the syntactic language models have used a probabilistic context-free grammar (PCFG) (Roark, 2001; Charniak, 2001) or a dependency grammar (DG) (Wang and Harper, 2002; Gubbins and Vlachos, 2013) in order to capture the surface syntactic structures of sentences. Researchers have shown an increased interest in dependency parsing and it has increasingly been recognised as an alternative to constituency parsing in the past years. Accordingly, various representations and associated parsers have been proposed with respect to DG. DG describes the syntactic structure of a sentence in terms of headdependent relations between words. Unlike constituency models of syntax, DG directly models relationships between pairs of words, which leads to a simple framework that is easy to lexicalise and par"
U14-1002,J96-1002,0,0.0250938,"arsing.∗ (3) t=1 where m denotes the total number of decision trees and P j (w|·) is the probability of the word w calculated by the j th decision tree. The word probability is calculated conditioned on the stack features of the current configuration. Table 1 shows the stack features that are used in our RF model. Our unlabelled parsing model estimates the conditional probabilities of the word, given 14 different features. The labelled parsing model uses four additional label-related features. Type Unigram Bigram 4.4 Maximum Entropy Model for Transition/POS Distribution Conditional ME models (Berger et al., 1996) are used as another classifier in our parsing-based language model together with RF. The probability of a transition or a POS tag in Eq (2) is calculated from the corresponding transition ME or POS ME model. For instance, the probability of Shift is calculated as shown in Eq (4). P log Pλ (yi |xi ) + i X λ2j j 2σj2 ) (5) P 2 2 where j λj /2σj is a Gaussian prior regulariser that reduces overfitting by penalising large weights. In practice, we use a single parameter σ instead of having a different parameter σi for each feature. In this work we use the limited memory BFGS (L-BFGS) algorithm (Li"
U14-1002,P01-1017,0,0.57776,"based language models are the most widely used due to their simplicity and efficacy, they suffer from a major drawback: they cannot characterise the long-range relations between words. A syntactic language model, which exploits syntactic dependencies, can incorporate richer syntactic knowledge and information through syntactic parsing. In particular, syntactic structure leads to better performance of language model compared to traditional n-gram language models (Chelba and Jelinek, 2000). Most of the syntactic language models have used a probabilistic context-free grammar (PCFG) (Roark, 2001; Charniak, 2001) or a dependency grammar (DG) (Wang and Harper, 2002; Gubbins and Vlachos, 2013) in order to capture the surface syntactic structures of sentences. Researchers have shown an increased interest in dependency parsing and it has increasingly been recognised as an alternative to constituency parsing in the past years. Accordingly, various representations and associated parsers have been proposed with respect to DG. DG describes the syntactic structure of a sentence in terms of headdependent relations between words. Unlike constituency models of syntax, DG directly models relationships between pair"
U14-1002,P13-3005,0,0.0790606,"hat is easy to lexicalise and parse with. Moreover, a DG-based, particularly transition-based, parser is fast and even achieves state-of-the-art performance compared to the other grammar-based parsers. It is therefore suitable for identifying syntactic structures in incremental processing, which is a useful feature for online processing tasks such as speech recognition or machine translation. The aim of this study is to explore different dependency representations and to investigate their effects on the language modelling task. There are publicly available converters to generate dependencies. Ivanova et al. (2013) investigated the effect of each dependency scheme in terms of parsing accuracy. Elming et al. (2013) evaluated four dependency schemes in five different natural language processing (NLP) applications. However, to our knowledge, no previous work has investigated the effect of dependency representation on a syntactic language model. The remainder of this paper is organised as follows. Section 2 gives a brief overview of related work on dependency schemes and language modelling. In Section 3 we discuss three dependency schemes and Section 4 describes our dependency parsing language model. Then,"
U14-1002,P96-1041,0,0.233411,"ving that a decision tree essentially provides a hierarchical clustering of the data, since each node splits the data. We linearly interpolate the distributions of the leaf nodes recursively with their parents, ultimately backing off to the root node. The interpolation parameters should be sensitive to the counts in each node so that we back off only when the more specific node does not have enough observations to be reliable. However, giving each node of each decision tree its own interpolation parameter would itself introduce data sparsity problems. We instead use Chen’s bucketing approach (Chen and Goodman, 1996) for each tree level, in which nodes on the same level are grouped into buckets and one interpolation parameter is estimated for each bucket. The first step is to divide up the nodes on the same level into bins based on Chen’s scores. We then use the EM algorithm to find the optimal interpolation parameter for all the nodes in each bucket using heldout data. In a RF model, the predictions of all decision trees are averaged to produce a more robust pre4.2 Beam Search Our parsing model maintains a beam containing multiple configurations. To allow the parser to consider configurations with expens"
U14-1002,W07-2416,0,0.0729888,"anguage Technology Association Workshop, pages 4−13. to use unlabelled and labelled dependency grammar language models to solve the Sentence Completion Challenge set (Zweig and Burges, 2012). Their models performed substantially better than n-gram models. settings and a series of results are presented in Section 5. Finally, conclusions and directions for future work are given in Section 6. 2 Related Work 3 A number of studies have been conducted on dependency representations in NLP tasks. Several constituent-to-dependency conversion schemes have been proposed as the outputs of the converters (Johansson and Nugues, 2007; de Marneffe and Manning, 2008; Choi and Palmer, 2010; Tratz and Hovy, 2011). Previous work has evaluated the effects of different dependency representations in various NLP applications (Miwa et al., 2010; Popel et al., 2013; Ivanova et al., 2013; Elming et al., 2013). A substantial literature has examined the impact of combining DG with another diverse grammar representation, particularly in the context of parsing (Sagae et al., 2007; Øvrelid et al., 2009; Farkas and Bohnet, 2012; Kim et al., 2012). Many works on syntactic language models have been carried out using phrase structures. Chelba"
U14-1002,E06-1008,0,0.0168714,"thing to note is that it is hard to tell what scheme is overall best in our experiments. An interesting observation is that the dependency labels are somewhat ineffective for the speech recognition task in contrast to perplexity evaluation. The results demonstrate that each evaluation measure tends to have its own preferred scheme. For instance, the LTH scheme is preferred for the perplexity evaluation, whereas STD is preferred under WER. Our finding is in line with previous work, which claims that a task-based evaluation does not correlate well with a theoretical evaluation (Rosenfeld, 2000; Jonson, 2006; Och et al., 2004; Miwa et al., 2010; Elming et al., 2013; Smith, 2012). They commonly claim that lower perplexity does not necessarily mean lower WER, and the relation between two measures is clearly not transparent. Miwa et al. (2010) found that STD performs better for event extraction, whereas LTH outperforms STD in terms of parsing accuracy. 6 Conclusion and Future Work We explored three different dependency schemes using a dependency parsing language model. Our study indicates that the choice of scheme has an impact on overall performance. However, no dependency scheme is uniformly bette"
U14-1002,C12-1088,1,0.859213,"to-dependency conversion schemes have been proposed as the outputs of the converters (Johansson and Nugues, 2007; de Marneffe and Manning, 2008; Choi and Palmer, 2010; Tratz and Hovy, 2011). Previous work has evaluated the effects of different dependency representations in various NLP applications (Miwa et al., 2010; Popel et al., 2013; Ivanova et al., 2013; Elming et al., 2013). A substantial literature has examined the impact of combining DG with another diverse grammar representation, particularly in the context of parsing (Sagae et al., 2007; Øvrelid et al., 2009; Farkas and Bohnet, 2012; Kim et al., 2012). Many works on syntactic language models have been carried out using phrase structures. Chelba and Jelinek (2000) experiment with the application of syntactic structure in a language model for speech recognition. Their model builds the syntactic trees incrementally in a bottom-up strategy while processing the sentence in a left-to-right fashion and assigns a probability to every word sequence and parse. The model is very close to the arc-standard model that we investigate in this paper. Roark (2001) implements an incremental top-down and left-corner parsing model, which is used as a syntactic"
U14-1002,D11-1114,0,0.0547948,"Missing"
U14-1002,W08-1301,0,0.147726,"Missing"
U14-1002,N13-1070,0,0.231835,"is fast and even achieves state-of-the-art performance compared to the other grammar-based parsers. It is therefore suitable for identifying syntactic structures in incremental processing, which is a useful feature for online processing tasks such as speech recognition or machine translation. The aim of this study is to explore different dependency representations and to investigate their effects on the language modelling task. There are publicly available converters to generate dependencies. Ivanova et al. (2013) investigated the effect of each dependency scheme in terms of parsing accuracy. Elming et al. (2013) evaluated four dependency schemes in five different natural language processing (NLP) applications. However, to our knowledge, no previous work has investigated the effect of dependency representation on a syntactic language model. The remainder of this paper is organised as follows. Section 2 gives a brief overview of related work on dependency schemes and language modelling. In Section 3 we discuss three dependency schemes and Section 4 describes our dependency parsing language model. Then, the experimental Sunghwan Kim, John Pate and Mark Johnson. 2014. The Effect of Dependency Representat"
U14-1002,D11-1116,0,0.0158429,"d dependency grammar language models to solve the Sentence Completion Challenge set (Zweig and Burges, 2012). Their models performed substantially better than n-gram models. settings and a series of results are presented in Section 5. Finally, conclusions and directions for future work are given in Section 6. 2 Related Work 3 A number of studies have been conducted on dependency representations in NLP tasks. Several constituent-to-dependency conversion schemes have been proposed as the outputs of the converters (Johansson and Nugues, 2007; de Marneffe and Manning, 2008; Choi and Palmer, 2010; Tratz and Hovy, 2011). Previous work has evaluated the effects of different dependency representations in various NLP applications (Miwa et al., 2010; Popel et al., 2013; Ivanova et al., 2013; Elming et al., 2013). A substantial literature has examined the impact of combining DG with another diverse grammar representation, particularly in the context of parsing (Sagae et al., 2007; Øvrelid et al., 2009; Farkas and Bohnet, 2012; Kim et al., 2012). Many works on syntactic language models have been carried out using phrase structures. Chelba and Jelinek (2000) experiment with the application of syntactic structure in"
U14-1002,C10-1088,0,0.141807,"substantially better than n-gram models. settings and a series of results are presented in Section 5. Finally, conclusions and directions for future work are given in Section 6. 2 Related Work 3 A number of studies have been conducted on dependency representations in NLP tasks. Several constituent-to-dependency conversion schemes have been proposed as the outputs of the converters (Johansson and Nugues, 2007; de Marneffe and Manning, 2008; Choi and Palmer, 2010; Tratz and Hovy, 2011). Previous work has evaluated the effects of different dependency representations in various NLP applications (Miwa et al., 2010; Popel et al., 2013; Ivanova et al., 2013; Elming et al., 2013). A substantial literature has examined the impact of combining DG with another diverse grammar representation, particularly in the context of parsing (Sagae et al., 2007; Øvrelid et al., 2009; Farkas and Bohnet, 2012; Kim et al., 2012). Many works on syntactic language models have been carried out using phrase structures. Chelba and Jelinek (2000) experiment with the application of syntactic structure in a language model for speech recognition. Their model builds the syntactic trees incrementally in a bottom-up strategy while pro"
U14-1002,W02-1031,0,0.438358,"due to their simplicity and efficacy, they suffer from a major drawback: they cannot characterise the long-range relations between words. A syntactic language model, which exploits syntactic dependencies, can incorporate richer syntactic knowledge and information through syntactic parsing. In particular, syntactic structure leads to better performance of language model compared to traditional n-gram language models (Chelba and Jelinek, 2000). Most of the syntactic language models have used a probabilistic context-free grammar (PCFG) (Roark, 2001; Charniak, 2001) or a dependency grammar (DG) (Wang and Harper, 2002; Gubbins and Vlachos, 2013) in order to capture the surface syntactic structures of sentences. Researchers have shown an increased interest in dependency parsing and it has increasingly been recognised as an alternative to constituency parsing in the past years. Accordingly, various representations and associated parsers have been proposed with respect to DG. DG describes the syntactic structure of a sentence in terms of headdependent relations between words. Unlike constituency models of syntax, DG directly models relationships between pairs of words, which leads to a simple framework that i"
U14-1002,J08-4003,0,0.0205893,"ative transition-based dependency parsing introduced by Cohen et al. (2011), we propose a generative version of the arc-standard model that defines probability distributions over transitions and finds the most probable ones given stack features. P (π) = 2n−1 Y P (ti |αi−1 , Ai−1 ) (1) i=1 where π is a parse tree, n is the number of words in a sentence and t is a transition such that t ∈ T . More specifically, transitions are conditioned on topmost stack items and in particular, the probability of Shift is defined as follows: 4.1 Generative Dependency Parsing Model P (Shif tpw |α, A) Following Nivre (2008), a transition-based dependency parsing model is defined as a tuple S = (C, T, I, Ct ), where C is a set of configurations, T is a set of permissible transitions, I is an initialisation function, and Ct is a set of terminal configurations. A transition sequence for a sentence is a sequence of configurations where each non-initial configuration is obtained by applying a transition to a previous configuration. A configuration is a triple (α, β, A), where α is stack, β is queue and A is a set of dependency arcs. The stack α stores partially processed words and the queue β records the remaining in"
U14-1002,W12-2704,0,0.0246014,"der of this paper is organised as follows. Section 2 gives a brief overview of related work on dependency schemes and language modelling. In Section 3 we discuss three dependency schemes and Section 4 describes our dependency parsing language model. Then, the experimental Sunghwan Kim, John Pate and Mark Johnson. 2014. The Effect of Dependency Representation Scheme on Syntactic Language Modelling. In Proceedings of Australasian Language Technology Association Workshop, pages 4−13. to use unlabelled and labelled dependency grammar language models to solve the Sentence Completion Challenge set (Zweig and Burges, 2012). Their models performed substantially better than n-gram models. settings and a series of results are presented in Section 5. Finally, conclusions and directions for future work are given in Section 6. 2 Related Work 3 A number of studies have been conducted on dependency representations in NLP tasks. Several constituent-to-dependency conversion schemes have been proposed as the outputs of the converters (Johansson and Nugues, 2007; de Marneffe and Manning, 2008; Choi and Palmer, 2010; Tratz and Hovy, 2011). Previous work has evaluated the effects of different dependency representations in va"
U14-1002,W12-2700,0,0.171411,"Missing"
U14-1002,P13-1051,0,0.0514727,"Missing"
U14-1002,J01-2004,0,0.486729,"hough n-gram based language models are the most widely used due to their simplicity and efficacy, they suffer from a major drawback: they cannot characterise the long-range relations between words. A syntactic language model, which exploits syntactic dependencies, can incorporate richer syntactic knowledge and information through syntactic parsing. In particular, syntactic structure leads to better performance of language model compared to traditional n-gram language models (Chelba and Jelinek, 2000). Most of the syntactic language models have used a probabilistic context-free grammar (PCFG) (Roark, 2001; Charniak, 2001) or a dependency grammar (DG) (Wang and Harper, 2002; Gubbins and Vlachos, 2013) in order to capture the surface syntactic structures of sentences. Researchers have shown an increased interest in dependency parsing and it has increasingly been recognised as an alternative to constituency parsing in the past years. Accordingly, various representations and associated parsers have been proposed with respect to DG. DG describes the syntactic structure of a sentence in terms of headdependent relations between words. Unlike constituency models of syntax, DG directly models relations"
U14-1002,P07-1079,0,0.0279625,"ed on dependency representations in NLP tasks. Several constituent-to-dependency conversion schemes have been proposed as the outputs of the converters (Johansson and Nugues, 2007; de Marneffe and Manning, 2008; Choi and Palmer, 2010; Tratz and Hovy, 2011). Previous work has evaluated the effects of different dependency representations in various NLP applications (Miwa et al., 2010; Popel et al., 2013; Ivanova et al., 2013; Elming et al., 2013). A substantial literature has examined the impact of combining DG with another diverse grammar representation, particularly in the context of parsing (Sagae et al., 2007; Øvrelid et al., 2009; Farkas and Bohnet, 2012; Kim et al., 2012). Many works on syntactic language models have been carried out using phrase structures. Chelba and Jelinek (2000) experiment with the application of syntactic structure in a language model for speech recognition. Their model builds the syntactic trees incrementally in a bottom-up strategy while processing the sentence in a left-to-right fashion and assigns a probability to every word sequence and parse. The model is very close to the arc-standard model that we investigate in this paper. Roark (2001) implements an incremental to"
U14-1002,J93-2004,0,\N,Missing
U14-1002,P04-1005,1,\N,Missing
U14-1002,C12-1052,0,\N,Missing
U14-1002,N04-1021,0,\N,Missing
U15-1004,P07-1073,0,0.0960549,"Missing"
U15-1004,D14-1165,0,0.0307584,"Missing"
U15-1004,P04-1054,0,0.0609795,"d to exploit the notable type information available in FreeBase. Section 4 specifies the inference procedures used to identify the values of the model parameters, while section 5 explains how we evaluate our models and presents a systematic experimental comparison of the models by ablating the notable type in different ways based on entities’ NE categories. Section 6 concludes the paper and discusses future work. 2 Related work Most approaches to relation extraction are either supervised or semi-supervised. Supervised approaches require a large set of manually annotated text as training data (Culotta and Sorensen, 2004), 3 Relation extraction as matrix completion Riedel et al. (2013) formulated the relation extrac32 lation r that will be learnt from the training data. The neighbourhood model can be regarded as predicting an entry θr,t by using entries along the same row. It functions as a logistic regression classifier predicting the log odds of a FreeBase relation r applying to the entity tuple t using as features the syntactic relations r0 that hold of t. Our notable type extension to the neighbourhood model enriches the syntactic patterns in the training data O with notable type information. For example,"
U15-1004,N13-1008,0,0.24479,"4 specifies the inference procedures used to identify the values of the model parameters, while section 5 explains how we evaluate our models and presents a systematic experimental comparison of the models by ablating the notable type in different ways based on entities’ NE categories. Section 6 concludes the paper and discusses future work. 2 Related work Most approaches to relation extraction are either supervised or semi-supervised. Supervised approaches require a large set of manually annotated text as training data (Culotta and Sorensen, 2004), 3 Relation extraction as matrix completion Riedel et al. (2013) formulated the relation extrac32 lation r that will be learnt from the training data. The neighbourhood model can be regarded as predicting an entry θr,t by using entries along the same row. It functions as a logistic regression classifier predicting the log odds of a FreeBase relation r applying to the entity tuple t using as features the syntactic relations r0 that hold of t. Our notable type extension to the neighbourhood model enriches the syntactic patterns in the training data O with notable type information. For example, if there is a syntactic pattern for X-director-of-Y in our traini"
U15-1004,D11-1142,0,0.0867436,"Missing"
U15-1004,D11-1072,0,0.130074,"Missing"
U15-1004,D14-1203,0,0.0378274,"Missing"
U15-1004,D13-1136,0,0.148429,"Missing"
U15-1004,N07-2025,0,0.0637805,"Missing"
U15-1004,P10-1013,0,0.0623302,"Missing"
U15-1004,D12-1048,0,0.0632683,"Missing"
U15-1004,D10-1099,0,0.505221,"Missing"
U15-1004,P09-1113,0,0.31526,"Missing"
U15-1011,N09-1036,1,0.922814,"for denoting the recursive rules and thus the first rule is a short-hand writing for: Word → Morphs Morphs → Morph Morphs → Morph Morphs The underline denotes the adapted non-terminals, i.e. the sub-trees rooted in those non-terminals are cached by the model and their probabilities are computed according to the PYP. In the given example the Morph non-terminal is adapted, which means that the model prefers to re-generate the same subtrees denoting the morphemes repeatedly. We use in our experiments an existing AG implementation1 , the technical details of this implementation are described in (Johnson and Goldwater, 2009). 3 POS-dependent Grammars The POS-dependent grammars are inspired by the grammars that have been used to learn topic models (Johnson, 2010). Whereas the topic modeling grammars have one rule for every latent topic, the POS-dependent grammars have one rule for each possible tag, which enables the model to cache the subtrees corresponding to morphemes in words with specific syntactic category. Adaptor Grammars Adaptor Grammars (AG) (Johnson et al., 2007) is a non-parametric Bayesian framework for learning latent structures over sequences of strings. In the current context, the sequence of strin"
U15-1011,N10-1083,0,0.0704145,"Missing"
U15-1011,W08-0704,1,0.681736,"Missing"
U15-1011,P10-1117,1,0.850158,"notes the adapted non-terminals, i.e. the sub-trees rooted in those non-terminals are cached by the model and their probabilities are computed according to the PYP. In the given example the Morph non-terminal is adapted, which means that the model prefers to re-generate the same subtrees denoting the morphemes repeatedly. We use in our experiments an existing AG implementation1 , the technical details of this implementation are described in (Johnson and Goldwater, 2009). 3 POS-dependent Grammars The POS-dependent grammars are inspired by the grammars that have been used to learn topic models (Johnson, 2010). Whereas the topic modeling grammars have one rule for every latent topic, the POS-dependent grammars have one rule for each possible tag, which enables the model to cache the subtrees corresponding to morphemes in words with specific syntactic category. Adaptor Grammars Adaptor Grammars (AG) (Johnson et al., 2007) is a non-parametric Bayesian framework for learning latent structures over sequences of strings. In the current context, the sequence of strings is a se1 available from http://web.science.mq.edu. au/˜mjohnson/Software.htm 92 Consider for instance a tagset that contains three tags:"
U15-1011,D11-1059,0,0.0142123,". We show that the gold-standard tags lead to the biggest improvement as expected. However, using automatically induced tags also brings some improvement over the tagindependent baseline. 1 Introduction Linguistially, part-of-speech (POS) tagging and morphology are closely related and this relation has been heavily exploited in both supervised and unsupervised POS tagging. For instance, the supervised Stanford tagger (Toutanova et al., 2003) as well as some unsupervised POS taggers (BergKirkpatrick et al., 2010; Lee et al., 2010) use character prefix and/or suffix features, while the model by Christodoulopoulos et al. (2011) makes use of suffixes learned with an unsupervised morphological segmentation model. There have been some attempts to exploit the relation in the opposite direction to learn the segmentations dependent on POS tags. For instance, the segmentation procedures described by Freitag (2005) and Can and Manandhar (2009) find the syntactic clusters of words and then perform morphology learning using those clusters. Few works have included a small number of syntactic classes Mark Johnson Department of Computing Macquarie University Australia mark.johnson@mq.edu.au directly into the segmentation model ("
U15-1011,W11-0301,0,0.0377032,"Missing"
U15-1011,erjavec-2004-multext,0,0.0178712,"ations learned with this scenario are better than the baseline without any tags and worse than using gold-standard tags. The experimental results presented later confirm that this is indeed the case. The final scenario is the second baseline using tags generated uniformly at random. By evaluating this scenario we hope to show that not just any tagging configuration improves the segmentation results but the tags must really correspond at least to some extent to real syntactic tags. 5 English 5.1 Data All experiments were conducted on English and Estonian parts of the Multext-East (MTE) corpus (Erjavec, 2004) that contains G. Orwell’s novel ”1984”. The MTE corpora are morpho-syntactically annotated and the label of each word also contains the POS tag, which we can use in the oracle experiments that make use of gold-standard tags. However, the annotations do not include morphological segmentations. For Estonian, this text is also part of the morphologically disambiguated corpus,2 which has been manually annotated and also contains segmentations. We use Celex (Baayen et al., 1995) as the source for English gold-standard segmentations, which have been extracted with the Hutmegs package (Creutz and Li"
U15-1011,N12-1045,1,0.895044,"Missing"
U15-1011,D13-1004,0,0.0335783,"Missing"
U15-1011,Q13-1021,1,0.90339,"ised model (Can, 2011; Sirts and Alum¨ae, 2012; Frank et al., 2013). However, the results presented in those papers fail to demonstrate clearly the utility of using the tag information in segmentation learning over the scenario where the tags are missing. The goal of this paper is to explore the relation between POS tags and morphological segmentations and in particular, to study if and how much the POS tags help to learn better segmentations. We start with experiments learning segmentations without POS tags as has been standard in previous literature (Goldsmith, 2001; Creutz and Lagus, 2007; Sirts and Goldwater, 2013) and then add the POS information. We first add the information about gold-standard tags, which provides a kind of upper bound of how much the segmentation accuracy can gain from POS information. Secondly, we also experiment with automatically induced tags. We expect to see that gold-standard POS tags improve the segmentation accuracy and that induced tags Kairit Sirts and Mark Johnson. 2015. Do POS Tags Help to Learn Better Morphological Segmentations? . In Proceedings of Australasian Language Technology Association Workshop, pages 91−100. are also helpful. The results of these experiments ca"
U15-1011,P12-2063,0,0.022393,"egmentation using gold-standard tags; 3. POS-dependent segmentation using syntactic clustering learned with an unsupervised model; 4. POS-dependent segmentation using randomly generated tags. Estonian 8438 7659 3831 2691 1629 15132 15132 8162 4004 3111 Table 1: The number of open class words (nouns, verbs and adjectives) used for training and evaluation. triple for 10 times with different random initialisations. We run the sampler for 1000 iterations, after which we collect a single sample and aggregate the samples from all runs by using maximum marginal decoding (Johnson and Goldwater, 2009; Stallard et al., 2012). We use batch initialisation, table label resampling is turned on and all hyperparameters are inferred. The first scenario does not use any tags at all and is thus the standard setting used in previous work for conducting unsupervised morphological segmentation. This is the baseline we expect the other, tag-dependent scenarios to exceed. The second scenario, which uses gold-standard POS tags, is an oracle setting that gives an upper bound of how much the tags can help to improve the segmentation accuracy when using a particular segmentation model. Hypothetically, there could exist tagging con"
U15-1011,W05-0617,0,0.0313933,"been heavily exploited in both supervised and unsupervised POS tagging. For instance, the supervised Stanford tagger (Toutanova et al., 2003) as well as some unsupervised POS taggers (BergKirkpatrick et al., 2010; Lee et al., 2010) use character prefix and/or suffix features, while the model by Christodoulopoulos et al. (2011) makes use of suffixes learned with an unsupervised morphological segmentation model. There have been some attempts to exploit the relation in the opposite direction to learn the segmentations dependent on POS tags. For instance, the segmentation procedures described by Freitag (2005) and Can and Manandhar (2009) find the syntactic clusters of words and then perform morphology learning using those clusters. Few works have included a small number of syntactic classes Mark Johnson Department of Computing Macquarie University Australia mark.johnson@mq.edu.au directly into the segmentation model (Goldwater et al., 2006; Lee et al., 2011). However, Goldwater et al. (2006) only trains the model on verbs, which means that the classes model different verb paradigms rather than POS tags. Secondly, the model is never evaluated in a single class configuration and thus it is not known"
U15-1011,J01-2001,0,0.343308,"Missing"
U15-1011,N03-1033,0,0.0435516,"three different scenarios: without POS tags, with gold-standard tags and with automatically induced tags, and show that the segmentation F1-score improves when the tags are used. We show that the gold-standard tags lead to the biggest improvement as expected. However, using automatically induced tags also brings some improvement over the tagindependent baseline. 1 Introduction Linguistially, part-of-speech (POS) tagging and morphology are closely related and this relation has been heavily exploited in both supervised and unsupervised POS tagging. For instance, the supervised Stanford tagger (Toutanova et al., 2003) as well as some unsupervised POS taggers (BergKirkpatrick et al., 2010; Lee et al., 2010) use character prefix and/or suffix features, while the model by Christodoulopoulos et al. (2011) makes use of suffixes learned with an unsupervised morphological segmentation model. There have been some attempts to exploit the relation in the opposite direction to learn the segmentations dependent on POS tags. For instance, the segmentation procedures described by Freitag (2005) and Can and Manandhar (2009) find the syntactic clusters of words and then perform morphology learning using those clusters. Fe"
U15-1011,D10-1083,0,\N,Missing
U15-1013,W12-0601,0,0.0183419,"per (for example, Sport, Business, Lifestyle, Drive and so on), it may be useful to organise articles ignoring their section of origin, and instead focus on the subjects of each article, that is, the people, places, organisations and events (e.g. earthquake or Election). Such information is typically represented in the articles’ nouns. This study builds on the work of Griffiths et al. Fiona Martin and Mark Johnson. 2015. More Efficient Topic Modelling Through a Noun Only Approach . In Proceedings of Australasian Language Technology Association Workshop, pages 111−115. (2005), Jiang (2009) and Darling et al. (2012), where topics were generated for specific parts of speech. The novelty in this current study is that it is concerned solely with noun topics, and reduces the corpus to nouns prior to topic modelling. As a news corpus tends have a broad and varied vocabulary, that can be time consuming to topic model, limiting articles to only the nouns also offers the advantage of reducing the size of the vocabulary to be modelled. The question of interest in this current study was whether reducing a news corpus to nouns only would efficiently produce topics that implied coherent themes, which, in turn, may o"
U15-1013,E14-1056,0,0.172125,"pus to nouns prior to topic modelling. As a news corpus tends have a broad and varied vocabulary, that can be time consuming to topic model, limiting articles to only the nouns also offers the advantage of reducing the size of the vocabulary to be modelled. The question of interest in this current study was whether reducing a news corpus to nouns only would efficiently produce topics that implied coherent themes, which, in turn, may offer more meaningful document clusters. The measures of interest were topic coherence and the time taken to generate the topic model. Previous work by Lau et al. (2014) suggests that lemmatising a corpus improves topic coherence. This study sought to replicate that finding, and then examine if further improvement occurs by limiting the corpus to nouns. The news corpus and the tools applied to that corpus are detailed in the next section. Section 3 provides the results of the topic coherence evaluations, and Section 4 discusses these results in relation to the goal of efficiently generating coherent topics. 2 Data and Methods 2.1 Data and Pre-Processing Topic models were generated based on a 1991 set of San Jose Mercury News (SJMN) articles, from the Tipster"
U15-1013,D11-1024,0,0.0361206,"ent words associated with the topic, to determine if these words together suggest a particular theme. For example, a topic with the most frequent words {water plant tree garden flower fruit valley drought} suggests a possible label of “garden”, whereas a topic of {art good house room style work fashion draw} seems to combine multiple themes, and is harder to label. Manually assigning a meaning to a topic (e.g. “gardening”) is easier for a reviewer if the most frequent words in the topic are semantically coherent. One issue identified with topic modelling is that it can generate ‘junk’ topics (Mimno et al., 2011), that is, topics lacking coherence (as in the second example above). Such topics are either ambiguous or have no interpretable theme. While in some instances there may be interest in examining adjectives (say for sentiment analysis), or verbs (if seeking to identify change, for example), often interest centres around entities such as people, places, organisations and events. For articles drawn from all sections of a newspaper (for example, Sport, Business, Lifestyle, Drive and so on), it may be useful to organise articles ignoring their section of origin, and instead focus on the subjects of"
U15-1013,N03-1033,0,0.0498539,"Missing"
U15-1014,N10-1083,0,0.0903286,"Missing"
U15-1014,P11-1026,0,0.020909,"ect to the j ele2013) to learn 25-dimensional word vectors on the ment of the vector for each topic z is: experimental dataset, using a local 10-word win  dow context.4 X X X ∂L = nd,w P(z |w, d) vw,j − vw0 ,j φz,w0 ∂µz,j The numbers of topics is set to 20. For varid∈D w∈W w0 ∈W ational inference LDA, we use Blei’s implemen− 2π2 µz,j − π1 sign(µz,j ) tation.5 For Gibbs sampling LDA, we use the (6) jLDADMM package6 (Nguyen, 2015) with comWe used OWL - QN1 (Andrew and Gao, 2007) to mon hyper-parameters β = 0.01 and α = 0.1 find the topic vector µz and the parameters ξd,z (Newman et al., 2009; Hu et al., 2011; Xie and and ψz,w that maximize L. Xing, 2013). We ran Gibbs sampling LDA for 2000 iterations and evaluated the topics assigned 4 Experiments to words in the last sample. We then used the To investigate the performance of our new apdocument-to-topic and topic-to-word distributions proach, we compared it with two baselines on from the last sample of Gibbs sampling LDA to topic coherence: 1) variational inference LDA initialize the parameters ξd,z and ψz,w while topic (Blei et al., 2003); and 2) Gibbs sampling LDA vectors µz are initialized as zero vectors in our (Griffiths and Steyvers, 2004)."
U15-1014,P10-1117,1,0.802126,"iminary results show that the word vectors induced from the experimental corpus can be used to improve the assignments of topics to words. Keywords: MAP estimation, LDA, Topic model, Word vectors, Topic coherence 1 Introduction Topic modeling algorithms, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and related methods (Blei, 2012), are often used to learn a set of latent topics for a corpus of documents and to infer document-to-topic and topicto-word distributions from the co-occurrence of words within the documents (Wallach, 2006; Blei and McAuliffe, 2008; Wang et al., 2007; Johnson, 2010; Yan et al., 2013; Xie et al., 2015; Yang et al., 2015). With enough training data there is sufficient information in the corpus to accurately estimate the distributions. However, most topic models consider each document as a bag-of-words, i.e. the word order or the window-based local context information is not taken into account. Topic models have also been constructed using latent features (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013; Cao et al., 2015). Latent feature vectors have been recently successfully exploited for a wide range of NLP tasks (Glorot et al., 2011; Socher et"
U15-1014,E14-1056,0,0.0461841,"d 2) Gibbs sampling LDA vectors µz are initialized as zero vectors in our (Griffiths and Steyvers, 2004). The topic coherMAP learner. For our MAP approach, we set7 ence evaluation measures the coherence of the λ2 = π2 = 0.01, λ1 = π1 = 1.0e−6, 2 = 0.1 topic-to-word associations, i.e. it directly evaluand 1 = 0.01. We report the mean and standard ates how the high-probability words in each topic deviation of the results of ten repetitions of each are semantically coherent (Chang et al., 2009; experiment. Newman et al., 2010; Mimno et al., 2011; Stevens 4.2 Quantitative analysis et al., 2012; Lau et al., 2014; R¨oder et al., 2015). − 22 ψz,w − 1 sign(ψz,w ) 4.1 Experimental setup We conducted experiments on the standard benchmark 20-Newsgroups dataset.2 In addition to converting into lowercase and removing non-alphabetic characters, we removed stop-words found in the stop-word list in the Mallet toolkit (McCallum, 2002). We then removed words shorter than 3 characters or words appearing less than 10 times. Table 1 presents details of the experimental dataset. As pointed out in Levy and Goldberg (2014) and Pennington et al. (2014), the prediction-based methods and count-based methods for learning"
U15-1014,D11-1024,0,0.100948,"ize the parameters ξd,z and ψz,w while topic (Blei et al., 2003); and 2) Gibbs sampling LDA vectors µz are initialized as zero vectors in our (Griffiths and Steyvers, 2004). The topic coherMAP learner. For our MAP approach, we set7 ence evaluation measures the coherence of the λ2 = π2 = 0.01, λ1 = π1 = 1.0e−6, 2 = 0.1 topic-to-word associations, i.e. it directly evaluand 1 = 0.01. We report the mean and standard ates how the high-probability words in each topic deviation of the results of ten repetitions of each are semantically coherent (Chang et al., 2009; experiment. Newman et al., 2010; Mimno et al., 2011; Stevens 4.2 Quantitative analysis et al., 2012; Lau et al., 2014; R¨oder et al., 2015). − 22 ψz,w − 1 sign(ψz,w ) 4.1 Experimental setup We conducted experiments on the standard benchmark 20-Newsgroups dataset.2 In addition to converting into lowercase and removing non-alphabetic characters, we removed stop-words found in the stop-word list in the Mallet toolkit (McCallum, 2002). We then removed words shorter than 3 characters or words appearing less than 10 times. Table 1 presents details of the experimental dataset. As pointed out in Levy and Goldberg (2014) and Pennington et al. (2014),"
U15-1014,N10-1012,0,0.0604883,"inference LDA initialize the parameters ξd,z and ψz,w while topic (Blei et al., 2003); and 2) Gibbs sampling LDA vectors µz are initialized as zero vectors in our (Griffiths and Steyvers, 2004). The topic coherMAP learner. For our MAP approach, we set7 ence evaluation measures the coherence of the λ2 = π2 = 0.01, λ1 = π1 = 1.0e−6, 2 = 0.1 topic-to-word associations, i.e. it directly evaluand 1 = 0.01. We report the mean and standard ates how the high-probability words in each topic deviation of the results of ten repetitions of each are semantically coherent (Chang et al., 2009; experiment. Newman et al., 2010; Mimno et al., 2011; Stevens 4.2 Quantitative analysis et al., 2012; Lau et al., 2014; R¨oder et al., 2015). − 22 ψz,w − 1 sign(ψz,w ) 4.1 Experimental setup We conducted experiments on the standard benchmark 20-Newsgroups dataset.2 In addition to converting into lowercase and removing non-alphabetic characters, we removed stop-words found in the stop-word list in the Mallet toolkit (McCallum, 2002). We then removed words shorter than 3 characters or words appearing less than 10 times. Table 1 presents details of the experimental dataset. As pointed out in Levy and Goldberg (2014) and Penni"
U15-1014,Q15-1004,0,0.0167403,"g the MAP estimation for the LDA model has been suggested before. Chien and Wu (2008), Asuncion et al. (2009) and Taddy (2012) proposed EM algorithms for estimating θd,z and φz,w , while we use direct gradient-based optimization methods. Sontag and Roy (2011) optimized the MAP estimates of φz,w and θd,z in turn by integrating out θd,z and φz,w respectively. We, on the other hand, estimate all parameters jointly in a single optimization step. In addition to Taddy (2012)’s approach, applying MAP estimation to learn log-linear models for topic models is also found in Eisenstein et al. (2011) and Paul and Dredze (2015). Our MAP model is also defined in log-linear representation. However, unlike our MAP approach, those approaches do not use latent feature word vectors to characterize the topic-to-word distributions. Furthermore, Berg-Kirkpatrick et al. (2010) proposed a direct optimization approach of the objective function for Hidden Markov Model-like generative models. However, they applied the approach to various unsupervised NLP tasks, such as part-of-speech induction, grammar induction, word alignment, and word segmentation, but not to topic models. 117 3 Direct MAP estimation approach In this section,"
U15-1014,D14-1162,0,0.0776879,"., 2013; Xie et al., 2015; Yang et al., 2015). With enough training data there is sufficient information in the corpus to accurately estimate the distributions. However, most topic models consider each document as a bag-of-words, i.e. the word order or the window-based local context information is not taken into account. Topic models have also been constructed using latent features (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013; Cao et al., 2015). Latent feature vectors have been recently successfully exploited for a wide range of NLP tasks (Glorot et al., 2011; Socher et al., 2013; Pennington et al., 2014). Rather than relying solely on word count information as the standard multinomial LDA does, or using only distributed feature representations, as in Salakhutdinov and Hinton (2009), Srivastava et al. (2013) and Cao et al. (2015), Nguyen et al. (2015) integrated pretrained latent feature word representations containing external information from very large corpora into existing topic models and obtained significant improvements on small document collections and short text datasets. However, their implementation is computationally quite expensive because they have to compute a MAP estimate in ea"
U15-1014,P13-1045,0,0.0251003,"nson, 2010; Yan et al., 2013; Xie et al., 2015; Yang et al., 2015). With enough training data there is sufficient information in the corpus to accurately estimate the distributions. However, most topic models consider each document as a bag-of-words, i.e. the word order or the window-based local context information is not taken into account. Topic models have also been constructed using latent features (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013; Cao et al., 2015). Latent feature vectors have been recently successfully exploited for a wide range of NLP tasks (Glorot et al., 2011; Socher et al., 2013; Pennington et al., 2014). Rather than relying solely on word count information as the standard multinomial LDA does, or using only distributed feature representations, as in Salakhutdinov and Hinton (2009), Srivastava et al. (2013) and Cao et al. (2015), Nguyen et al. (2015) integrated pretrained latent feature word representations containing external information from very large corpora into existing topic models and obtained significant improvements on small document collections and short text datasets. However, their implementation is computationally quite expensive because they have to co"
U15-1014,D12-1087,0,0.0477563,"Missing"
U15-1014,N15-1074,0,0.0190303,"d vectors induced from the experimental corpus can be used to improve the assignments of topics to words. Keywords: MAP estimation, LDA, Topic model, Word vectors, Topic coherence 1 Introduction Topic modeling algorithms, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and related methods (Blei, 2012), are often used to learn a set of latent topics for a corpus of documents and to infer document-to-topic and topicto-word distributions from the co-occurrence of words within the documents (Wallach, 2006; Blei and McAuliffe, 2008; Wang et al., 2007; Johnson, 2010; Yan et al., 2013; Xie et al., 2015; Yang et al., 2015). With enough training data there is sufficient information in the corpus to accurately estimate the distributions. However, most topic models consider each document as a bag-of-words, i.e. the word order or the window-based local context information is not taken into account. Topic models have also been constructed using latent features (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013; Cao et al., 2015). Latent feature vectors have been recently successfully exploited for a wide range of NLP tasks (Glorot et al., 2011; Socher et al., 2013; Pennington et al., 2014)."
U15-1014,D15-1037,0,0.0136596,"from the experimental corpus can be used to improve the assignments of topics to words. Keywords: MAP estimation, LDA, Topic model, Word vectors, Topic coherence 1 Introduction Topic modeling algorithms, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and related methods (Blei, 2012), are often used to learn a set of latent topics for a corpus of documents and to infer document-to-topic and topicto-word distributions from the co-occurrence of words within the documents (Wallach, 2006; Blei and McAuliffe, 2008; Wang et al., 2007; Johnson, 2010; Yan et al., 2013; Xie et al., 2015; Yang et al., 2015). With enough training data there is sufficient information in the corpus to accurately estimate the distributions. However, most topic models consider each document as a bag-of-words, i.e. the word order or the window-based local context information is not taken into account. Topic models have also been constructed using latent features (Salakhutdinov and Hinton, 2009; Srivastava et al., 2013; Cao et al., 2015). Latent feature vectors have been recently successfully exploited for a wide range of NLP tasks (Glorot et al., 2011; Socher et al., 2013; Pennington et al., 2014). Rather than relying"
U15-1014,Q15-1022,1,\N,Missing
U16-1006,N06-2007,0,0.0418168,"g that the deep models significantly outperform the conventional approaches (Jiang and Zhai, 2007; Chan and Roth, 2010; Chan and Roth, 2011) on the ACE relation extraction datasets. Except for the FCM model (Yu et al., 2015), at the core of almost all deep RE models are variants of convolutional neural networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016; Miwa and Bansal, 2016), recurrent neural networks (RNN) (Zhang et al., 2015; Socher et al., 2012; Ebrahimi and Dou, 2015; Lin et al., 2016), or both of them (Liu et al., 2015; Cai et al., 2016). Several RE systems (Chen et al., 2006a; GuoDong et al., 2009; Li et al., 2010; LongHua and Qiaoming, 2008; Chen et al., 2006b; Kim and Lee, 2012) are built upon the semi-supervised learning algorithm label propagation to exploit the use of unlabeled data. This family of algorithms start with building a similarity graph between each pair of relation mentions, and propagate relation labels from labeled ones to unlabeled ones. However, deep RE models require substantial change in order to use these algorithms, while our methods just need to replace the training criterion during pre-training, which is easy-to-implement by using a sta"
U16-1006,D14-1164,0,0.0961413,"E has been intensively studied for several years (Chan and Roth, 2011; Chan and Roth, 2010). Recently, RE models based on deep neural networks (DNN) have achieved better performance than conventional RE models that rely on handcrafted features (Xu et al., 2015). However, these DNN models require a large amount of annotated training data, which is difficult and expensive to obtain. The data problem is not completely solved by relying on methods such as large external knowledge bases and distant supervision because i) models employing only large knowledge bases often still perform poorly on RE (Angeli et al., 2014); ii) the external knowledge bases are incomplete; and iii) many important applications lack the relevant domain specific knowledge bases. This paper asks the question: can we use unlabeled data to help training DNN RE models? Although unsupervised pre-training is known to be effective for training deep neural networks, it remains unclear how to apply it to the recently proposed DNN RE models. The main advantage of deep models (compared to the shallow counterparts) is that they automatically learn distributed representations of the relevant components of the model (e.g., words, entities, relat"
U16-1006,P16-1072,0,0.0239145,"Missing"
U16-1006,P15-1061,0,0.321021,"py loss, as a result of applying multi-class logistic regression (LR) in the supervised setting. The deep RE models proposed recently are variants of Long Short Term Memory (LSTM) (Graves and Schmidhuber, 2005) and Convolutional Neural Networks (CNN) (Krizhevsky et al., 2012). As representative examples we consider three recent RE models: i) bidirectional LSTM that takes words around entity mentions as input (Zhang et al., 2015), coined BiLSTM; ii) LSTM taking shortest paths in dependency trees as input, coined DepTreeLSTM; iii) CNN taking words sequences and position embeddings as input (dos Santos et al., 2015), coined PCNN. ut , ct = LSTM(xt , ut−1 , ct−1 ) (1) where xt is the input to LSTM at time step t, and ut and ct are the hidden states and memory states of LSTM at time step t, respectively. BiLSTM reads an input word sequence in both directions with two separate LSTM layers. As illustrated in Figure 2c, one LSTM reads the word sequence between two entity mentions in forwards direction, while the other with shared parameters reads the same sequence in the reverse direction. As a result, they generate two hidden representa→ − ← − tions h and h , which are further concatenated to form the input"
U16-1006,P16-1200,0,0.0179885,"Work Recent advance of relation extraction demonstrates the power of deep learning by showing that the deep models significantly outperform the conventional approaches (Jiang and Zhai, 2007; Chan and Roth, 2010; Chan and Roth, 2011) on the ACE relation extraction datasets. Except for the FCM model (Yu et al., 2015), at the core of almost all deep RE models are variants of convolutional neural networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016; Miwa and Bansal, 2016), recurrent neural networks (RNN) (Zhang et al., 2015; Socher et al., 2012; Ebrahimi and Dou, 2015; Lin et al., 2016), or both of them (Liu et al., 2015; Cai et al., 2016). Several RE systems (Chen et al., 2006a; GuoDong et al., 2009; Li et al., 2010; LongHua and Qiaoming, 2008; Chen et al., 2006b; Kim and Lee, 2012) are built upon the semi-supervised learning algorithm label propagation to exploit the use of unlabeled data. This family of algorithms start with building a similarity graph between each pair of relation mentions, and propagate relation labels from labeled ones to unlabeled ones. However, deep RE models require substantial change in order to use these algorithms, while our methods just need to"
U16-1006,P15-2047,0,0.0192577,"raction demonstrates the power of deep learning by showing that the deep models significantly outperform the conventional approaches (Jiang and Zhai, 2007; Chan and Roth, 2010; Chan and Roth, 2011) on the ACE relation extraction datasets. Except for the FCM model (Yu et al., 2015), at the core of almost all deep RE models are variants of convolutional neural networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016; Miwa and Bansal, 2016), recurrent neural networks (RNN) (Zhang et al., 2015; Socher et al., 2012; Ebrahimi and Dou, 2015; Lin et al., 2016), or both of them (Liu et al., 2015; Cai et al., 2016). Several RE systems (Chen et al., 2006a; GuoDong et al., 2009; Li et al., 2010; LongHua and Qiaoming, 2008; Chen et al., 2006b; Kim and Lee, 2012) are built upon the semi-supervised learning algorithm label propagation to exploit the use of unlabeled data. This family of algorithms start with building a similarity graph between each pair of relation mentions, and propagate relation labels from labeled ones to unlabeled ones. However, deep RE models require substantial change in order to use these algorithms, while our methods just need to replace the training criterion duri"
U16-1006,N15-1133,0,0.0599241,"rom Wikipedia. 2 Related Work Recent advance of relation extraction demonstrates the power of deep learning by showing that the deep models significantly outperform the conventional approaches (Jiang and Zhai, 2007; Chan and Roth, 2010; Chan and Roth, 2011) on the ACE relation extraction datasets. Except for the FCM model (Yu et al., 2015), at the core of almost all deep RE models are variants of convolutional neural networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016; Miwa and Bansal, 2016), recurrent neural networks (RNN) (Zhang et al., 2015; Socher et al., 2012; Ebrahimi and Dou, 2015; Lin et al., 2016), or both of them (Liu et al., 2015; Cai et al., 2016). Several RE systems (Chen et al., 2006a; GuoDong et al., 2009; Li et al., 2010; LongHua and Qiaoming, 2008; Chen et al., 2006b; Kim and Lee, 2012) are built upon the semi-supervised learning algorithm label propagation to exploit the use of unlabeled data. This family of algorithms start with building a similarity graph between each pair of relation mentions, and propagate relation labels from labeled ones to unlabeled ones. However, deep RE models require substantial change in order to use these algorithms, while our me"
U16-1006,I13-1081,0,0.0169186,"a target relation exists in a knowledge base or not. There have also been other efforts towards minimizing the use of labeled data. In (Sun, 2009), they proposed a bootstrapping approach to extract textual patterns for training a SVM-based relation extraction system. In (Chan and Roth, 2011), they show that supervised models equipped with syntactico-semantic features are capable of classifying relation mentions with a few labeled data. However, both work are customized for supervised models with handcrafted features and relations between nominals. In other lines of research, active learning (Fu and Grishman, 2013; Sun and Grishman, 2012) and domain adaptation (Nguyen and Grishman, 2014) pursued to select high quality training examples for training relation extraction models. Jiang (2009) leverages the knowledge of known relations to predict new relations in a weakly supervised setting. These approaches have different problem settings than ours, which focus on the use of unlabeled data. Since 2006, various pre-training techniques are proposed to make the training of deep neural networks practical (Hinton and Salakhutdinov, 2006; Dahl et al., 2010; Bengio, 2009). They are not universally applicable for"
U16-1006,I08-1005,0,0.0350274,"ional approaches (Jiang and Zhai, 2007; Chan and Roth, 2010; Chan and Roth, 2011) on the ACE relation extraction datasets. Except for the FCM model (Yu et al., 2015), at the core of almost all deep RE models are variants of convolutional neural networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016; Miwa and Bansal, 2016), recurrent neural networks (RNN) (Zhang et al., 2015; Socher et al., 2012; Ebrahimi and Dou, 2015; Lin et al., 2016), or both of them (Liu et al., 2015; Cai et al., 2016). Several RE systems (Chen et al., 2006a; GuoDong et al., 2009; Li et al., 2010; LongHua and Qiaoming, 2008; Chen et al., 2006b; Kim and Lee, 2012) are built upon the semi-supervised learning algorithm label propagation to exploit the use of unlabeled data. This family of algorithms start with building a similarity graph between each pair of relation mentions, and propagate relation labels from labeled ones to unlabeled ones. However, deep RE models require substantial change in order to use these algorithms, while our methods just need to replace the training criterion during pre-training, which is easy-to-implement by using a standard deep learning toolkit. It is also too expensive to involve all"
U16-1006,P16-1105,0,0.0208505,"pecific corpora for pre-training; in fact, they work well with 13,000 sentences randomly sampled from Wikipedia. 2 Related Work Recent advance of relation extraction demonstrates the power of deep learning by showing that the deep models significantly outperform the conventional approaches (Jiang and Zhai, 2007; Chan and Roth, 2010; Chan and Roth, 2011) on the ACE relation extraction datasets. Except for the FCM model (Yu et al., 2015), at the core of almost all deep RE models are variants of convolutional neural networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016; Miwa and Bansal, 2016), recurrent neural networks (RNN) (Zhang et al., 2015; Socher et al., 2012; Ebrahimi and Dou, 2015; Lin et al., 2016), or both of them (Liu et al., 2015; Cai et al., 2016). Several RE systems (Chen et al., 2006a; GuoDong et al., 2009; Li et al., 2010; LongHua and Qiaoming, 2008; Chen et al., 2006b; Kim and Lee, 2012) are built upon the semi-supervised learning algorithm label propagation to exploit the use of unlabeled data. This family of algorithms start with building a similarity graph between each pair of relation mentions, and propagate relation labels from labeled ones to unlabeled ones."
U16-1006,N07-1015,0,0.0432027,"e size of the labeled training data is small, the deep relation extraction models pretrained with our unsupervised pre-training method using half or even a quarter of the labeled data are able to achieve similar performance as the models without pre-training. Our unsupervised approach does not need domain-specific corpora for pre-training; in fact, they work well with 13,000 sentences randomly sampled from Wikipedia. 2 Related Work Recent advance of relation extraction demonstrates the power of deep learning by showing that the deep models significantly outperform the conventional approaches (Jiang and Zhai, 2007; Chan and Roth, 2010; Chan and Roth, 2011) on the ACE relation extraction datasets. Except for the FCM model (Yu et al., 2015), at the core of almost all deep RE models are variants of convolutional neural networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016; Miwa and Bansal, 2016), recurrent neural networks (RNN) (Zhang et al., 2015; Socher et al., 2012; Ebrahimi and Dou, 2015; Lin et al., 2016), or both of them (Liu et al., 2015; Cai et al., 2016). Several RE systems (Chen et al., 2006a; GuoDong et al., 2009; Li et al., 2010; LongHua and Qiaoming, 2008; Chen et al"
U16-1006,P14-2012,0,0.024534,"been other efforts towards minimizing the use of labeled data. In (Sun, 2009), they proposed a bootstrapping approach to extract textual patterns for training a SVM-based relation extraction system. In (Chan and Roth, 2011), they show that supervised models equipped with syntactico-semantic features are capable of classifying relation mentions with a few labeled data. However, both work are customized for supervised models with handcrafted features and relations between nominals. In other lines of research, active learning (Fu and Grishman, 2013; Sun and Grishman, 2012) and domain adaptation (Nguyen and Grishman, 2014) pursued to select high quality training examples for training relation extraction models. Jiang (2009) leverages the knowledge of known relations to predict new relations in a weakly supervised setting. These approaches have different problem settings than ours, which focus on the use of unlabeled data. Since 2006, various pre-training techniques are proposed to make the training of deep neural networks practical (Hinton and Salakhutdinov, 2006; Dahl et al., 2010; Bengio, 2009). They are not universally applicable for all problems and most of them focus on computer vision problems. To the bes"
U16-1006,P09-1114,0,0.0233304,"to extract textual patterns for training a SVM-based relation extraction system. In (Chan and Roth, 2011), they show that supervised models equipped with syntactico-semantic features are capable of classifying relation mentions with a few labeled data. However, both work are customized for supervised models with handcrafted features and relations between nominals. In other lines of research, active learning (Fu and Grishman, 2013; Sun and Grishman, 2012) and domain adaptation (Nguyen and Grishman, 2014) pursued to select high quality training examples for training relation extraction models. Jiang (2009) leverages the knowledge of known relations to predict new relations in a weakly supervised setting. These approaches have different problem settings than ours, which focus on the use of unlabeled data. Since 2006, various pre-training techniques are proposed to make the training of deep neural networks practical (Hinton and Salakhutdinov, 2006; Dahl et al., 2010; Bengio, 2009). They are not universally applicable for all problems and most of them focus on computer vision problems. To the best of our knowledge, we are the first to explore the use of pre-training for deep RE models. sequence mo"
U16-1006,W15-1506,0,0.0189873,"r unsupervised approach does not need domain-specific corpora for pre-training; in fact, they work well with 13,000 sentences randomly sampled from Wikipedia. 2 Related Work Recent advance of relation extraction demonstrates the power of deep learning by showing that the deep models significantly outperform the conventional approaches (Jiang and Zhai, 2007; Chan and Roth, 2010; Chan and Roth, 2011) on the ACE relation extraction datasets. Except for the FCM model (Yu et al., 2015), at the core of almost all deep RE models are variants of convolutional neural networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016; Miwa and Bansal, 2016), recurrent neural networks (RNN) (Zhang et al., 2015; Socher et al., 2012; Ebrahimi and Dou, 2015; Lin et al., 2016), or both of them (Liu et al., 2015; Cai et al., 2016). Several RE systems (Chen et al., 2006a; GuoDong et al., 2009; Li et al., 2010; LongHua and Qiaoming, 2008; Chen et al., 2006b; Kim and Lee, 2012) are built upon the semi-supervised learning algorithm label propagation to exploit the use of unlabeled data. This family of algorithms start with building a similarity graph between each pair of relation mentions, and propagate relation"
U16-1006,P12-2010,0,0.0302335,"Roth, 2010; Chan and Roth, 2011) on the ACE relation extraction datasets. Except for the FCM model (Yu et al., 2015), at the core of almost all deep RE models are variants of convolutional neural networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016; Miwa and Bansal, 2016), recurrent neural networks (RNN) (Zhang et al., 2015; Socher et al., 2012; Ebrahimi and Dou, 2015; Lin et al., 2016), or both of them (Liu et al., 2015; Cai et al., 2016). Several RE systems (Chen et al., 2006a; GuoDong et al., 2009; Li et al., 2010; LongHua and Qiaoming, 2008; Chen et al., 2006b; Kim and Lee, 2012) are built upon the semi-supervised learning algorithm label propagation to exploit the use of unlabeled data. This family of algorithms start with building a similarity graph between each pair of relation mentions, and propagate relation labels from labeled ones to unlabeled ones. However, deep RE models require substantial change in order to use these algorithms, while our methods just need to replace the training criterion during pre-training, which is easy-to-implement by using a standard deep learning toolkit. It is also too expensive to involve all unlabeled data in both training and pre"
U16-1006,K15-1009,1,0.904929,", which is a pair of named entity mentions (mh , mt ) together with its relation expression in a sentence S. Each mention m is disambiguated into an entity e. Let x ∈ X denote a relation mention, where X is the space of all relation mentions, RE models assign a binary relation y ∈ Y to x, where Y is a finite set 55 Figure 1: General Architecture for deep RE models. word into its word embedding. Herein we denote the word embedding of a word i by ei ∈ RM , where M is the dimension of word embeddings. All word embeddings are initialized with the ones pre-trained on a large domain-general corpus (Qu et al., 2015). As suggested in (Qu et al., 2015), we do not update these word embeddings during training to avoid overfitting. In the next step, a feature learning component projects the embeddings into a hidden representation h. If it is in a supervised setting, both h and handcrafted features are taken as the input of a multi-class LR classifier for categorizing target relations. In case of unsupervised pre-training, h is fed into a classifier for a designated unsupervised predictive task. The RE models based on BiLSTM and TreeLSTM are extensions of LSTM. LSTM is a recurrent neural network capable of cap"
U16-1006,N13-1008,0,0.0131019,"ion composed of words and named entities to learn representations of expressions, such that semantically similar expressions tend to have similar representations. In this paper, we propose a pre-training method that generalizes well-known sequence-toZhuang Li, Lizhen Qu, Qiongkai Xu and Mark Johnson. 2016. Unsupervised Pre-training With Seq2Seq Reconstruction Loss for Deep Relation Extraction Models. In Proceedings of Australasian Language Technology Association Workshop, pages 54−64. corpora. There is also ample of work exploring the idea of distant supervision for knowledge base completion (Riedel et al., 2013; Weston et al., 2013; Yang et al., 2014; Bordes et al., 2013) in order to avoid the use of manually labeled data. Although some of these models include a relation extraction component (Surdeanu et al., 2012; Angeli et al., 2014; Toutanova et al., 2015), the outputs of their systems are whether a relation holds between entities rather than entity mentions. In contrast, we aim to classify relation mentions no matter if a target relation exists in a knowledge base or not. There have also been other efforts towards minimizing the use of labeled data. In (Sun, 2009), they proposed a bootstrapping"
U16-1006,D12-1110,0,0.0560569,"es randomly sampled from Wikipedia. 2 Related Work Recent advance of relation extraction demonstrates the power of deep learning by showing that the deep models significantly outperform the conventional approaches (Jiang and Zhai, 2007; Chan and Roth, 2010; Chan and Roth, 2011) on the ACE relation extraction datasets. Except for the FCM model (Yu et al., 2015), at the core of almost all deep RE models are variants of convolutional neural networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016; Miwa and Bansal, 2016), recurrent neural networks (RNN) (Zhang et al., 2015; Socher et al., 2012; Ebrahimi and Dou, 2015; Lin et al., 2016), or both of them (Liu et al., 2015; Cai et al., 2016). Several RE systems (Chen et al., 2006a; GuoDong et al., 2009; Li et al., 2010; LongHua and Qiaoming, 2008; Chen et al., 2006b; Kim and Lee, 2012) are built upon the semi-supervised learning algorithm label propagation to exploit the use of unlabeled data. This family of algorithms start with building a similarity graph between each pair of relation mentions, and propagate relation labels from labeled ones to unlabeled ones. However, deep RE models require substantial change in order to use these"
U16-1006,D13-1136,0,0.0161437,"and named entities to learn representations of expressions, such that semantically similar expressions tend to have similar representations. In this paper, we propose a pre-training method that generalizes well-known sequence-toZhuang Li, Lizhen Qu, Qiongkai Xu and Mark Johnson. 2016. Unsupervised Pre-training With Seq2Seq Reconstruction Loss for Deep Relation Extraction Models. In Proceedings of Australasian Language Technology Association Workshop, pages 54−64. corpora. There is also ample of work exploring the idea of distant supervision for knowledge base completion (Riedel et al., 2013; Weston et al., 2013; Yang et al., 2014; Bordes et al., 2013) in order to avoid the use of manually labeled data. Although some of these models include a relation extraction component (Surdeanu et al., 2012; Angeli et al., 2014; Toutanova et al., 2015), the outputs of their systems are whether a relation holds between entities rather than entity mentions. In contrast, we aim to classify relation mentions no matter if a target relation exists in a knowledge base or not. There have also been other efforts towards minimizing the use of labeled data. In (Sun, 2009), they proposed a bootstrapping approach to extract t"
U16-1006,D15-1206,0,0.115613,"task of detecting and categorizing semantic relations between named entities mentioned in a text corpus. This is important for a wide variety of practical applications. For example, tourism planning bodies are interested in mining social media such as tweets to identifying which restaurants tourists eat in and which hotels those same tourists stay in. RE has been intensively studied for several years (Chan and Roth, 2011; Chan and Roth, 2010). Recently, RE models based on deep neural networks (DNN) have achieved better performance than conventional RE models that rely on handcrafted features (Xu et al., 2015). However, these DNN models require a large amount of annotated training data, which is difficult and expensive to obtain. The data problem is not completely solved by relying on methods such as large external knowledge bases and distant supervision because i) models employing only large knowledge bases often still perform poorly on RE (Angeli et al., 2014); ii) the external knowledge bases are incomplete; and iii) many important applications lack the relevant domain specific knowledge bases. This paper asks the question: can we use unlabeled data to help training DNN RE models? Although unsup"
U16-1006,R09-2014,0,0.0199255,"ledge base completion (Riedel et al., 2013; Weston et al., 2013; Yang et al., 2014; Bordes et al., 2013) in order to avoid the use of manually labeled data. Although some of these models include a relation extraction component (Surdeanu et al., 2012; Angeli et al., 2014; Toutanova et al., 2015), the outputs of their systems are whether a relation holds between entities rather than entity mentions. In contrast, we aim to classify relation mentions no matter if a target relation exists in a knowledge base or not. There have also been other efforts towards minimizing the use of labeled data. In (Sun, 2009), they proposed a bootstrapping approach to extract textual patterns for training a SVM-based relation extraction system. In (Chan and Roth, 2011), they show that supervised models equipped with syntactico-semantic features are capable of classifying relation mentions with a few labeled data. However, both work are customized for supervised models with handcrafted features and relations between nominals. In other lines of research, active learning (Fu and Grishman, 2013; Sun and Grishman, 2012) and domain adaptation (Nguyen and Grishman, 2014) pursued to select high quality training examples f"
U16-1006,N15-1155,0,0.0486114,"Missing"
U16-1006,D12-1042,0,0.0184022,"ethod that generalizes well-known sequence-toZhuang Li, Lizhen Qu, Qiongkai Xu and Mark Johnson. 2016. Unsupervised Pre-training With Seq2Seq Reconstruction Loss for Deep Relation Extraction Models. In Proceedings of Australasian Language Technology Association Workshop, pages 54−64. corpora. There is also ample of work exploring the idea of distant supervision for knowledge base completion (Riedel et al., 2013; Weston et al., 2013; Yang et al., 2014; Bordes et al., 2013) in order to avoid the use of manually labeled data. Although some of these models include a relation extraction component (Surdeanu et al., 2012; Angeli et al., 2014; Toutanova et al., 2015), the outputs of their systems are whether a relation holds between entities rather than entity mentions. In contrast, we aim to classify relation mentions no matter if a target relation exists in a knowledge base or not. There have also been other efforts towards minimizing the use of labeled data. In (Sun, 2009), they proposed a bootstrapping approach to extract textual patterns for training a SVM-based relation extraction system. In (Chan and Roth, 2011), they show that supervised models equipped with syntactico-semantic features are capable of"
U16-1006,C14-1220,0,0.0400478,"ut pre-training. Our unsupervised approach does not need domain-specific corpora for pre-training; in fact, they work well with 13,000 sentences randomly sampled from Wikipedia. 2 Related Work Recent advance of relation extraction demonstrates the power of deep learning by showing that the deep models significantly outperform the conventional approaches (Jiang and Zhai, 2007; Chan and Roth, 2010; Chan and Roth, 2011) on the ACE relation extraction datasets. Except for the FCM model (Yu et al., 2015), at the core of almost all deep RE models are variants of convolutional neural networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016; Miwa and Bansal, 2016), recurrent neural networks (RNN) (Zhang et al., 2015; Socher et al., 2012; Ebrahimi and Dou, 2015; Lin et al., 2016), or both of them (Liu et al., 2015; Cai et al., 2016). Several RE systems (Chen et al., 2006a; GuoDong et al., 2009; Li et al., 2010; LongHua and Qiaoming, 2008; Chen et al., 2006b; Kim and Lee, 2012) are built upon the semi-supervised learning algorithm label propagation to exploit the use of unlabeled data. This family of algorithms start with building a similarity graph between each pair of relation mentio"
U16-1006,Y15-1009,0,0.0615796,"with 13,000 sentences randomly sampled from Wikipedia. 2 Related Work Recent advance of relation extraction demonstrates the power of deep learning by showing that the deep models significantly outperform the conventional approaches (Jiang and Zhai, 2007; Chan and Roth, 2010; Chan and Roth, 2011) on the ACE relation extraction datasets. Except for the FCM model (Yu et al., 2015), at the core of almost all deep RE models are variants of convolutional neural networks (CNN) (Zeng et al., 2014; Nguyen and Grishman, 2015; Wang et al., 2016; Miwa and Bansal, 2016), recurrent neural networks (RNN) (Zhang et al., 2015; Socher et al., 2012; Ebrahimi and Dou, 2015; Lin et al., 2016), or both of them (Liu et al., 2015; Cai et al., 2016). Several RE systems (Chen et al., 2006a; GuoDong et al., 2009; Li et al., 2010; LongHua and Qiaoming, 2008; Chen et al., 2006b; Kim and Lee, 2012) are built upon the semi-supervised learning algorithm label propagation to exploit the use of unlabeled data. This family of algorithms start with building a similarity graph between each pair of relation mentions, and propagate relation labels from labeled ones to unlabeled ones. However, deep RE models require substantial change i"
U16-1006,D15-1174,0,0.0212204,"oZhuang Li, Lizhen Qu, Qiongkai Xu and Mark Johnson. 2016. Unsupervised Pre-training With Seq2Seq Reconstruction Loss for Deep Relation Extraction Models. In Proceedings of Australasian Language Technology Association Workshop, pages 54−64. corpora. There is also ample of work exploring the idea of distant supervision for knowledge base completion (Riedel et al., 2013; Weston et al., 2013; Yang et al., 2014; Bordes et al., 2013) in order to avoid the use of manually labeled data. Although some of these models include a relation extraction component (Surdeanu et al., 2012; Angeli et al., 2014; Toutanova et al., 2015), the outputs of their systems are whether a relation holds between entities rather than entity mentions. In contrast, we aim to classify relation mentions no matter if a target relation exists in a knowledge base or not. There have also been other efforts towards minimizing the use of labeled data. In (Sun, 2009), they proposed a bootstrapping approach to extract textual patterns for training a SVM-based relation extraction system. In (Chan and Roth, 2011), they show that supervised models equipped with syntactico-semantic features are capable of classifying relation mentions with a few label"
U16-1006,P16-1123,0,0.0217204,"Missing"
U16-1017,P16-1231,0,0.0545678,"Missing"
U16-1017,W06-2920,0,0.344542,"Missing"
U16-1017,D14-1082,0,0.165994,"Missing"
U16-1017,P15-1038,0,0.113202,"Missing"
U16-1017,P15-1033,0,0.0502673,"Missing"
U16-1017,P11-2125,0,0.0214393,"uses#1 himself#2 , accuses#3 himself .” Oracle Tree Arc With punct. LAS UAS LS 79.20 85.22 88.38 85.98 90.50 92.67 Without punct. LAS UAS LS 79.33 86.24 86.66 85.96 91.14 91.57 Table 5: Upper bound of ensemble performance. One simple approach to improve parsing performance for Vietnamese is to separately use the graph-based parser BistG for short sentences and the transition-based parser BistT for longer sentences. Another approach is to use system combination (Nivre and McDonald, 2008; Zhang and Clark, 2008), e.g. building ensemble systems (Sagae and Tsujii, 2007; Surdeanu and Manning, 2010; Haffari et al., 2011). Table 5 presents an upper bound of oracle ensemble performance, using the DEPENDABLE toolkit (Choi et al., 2015). DEPENDABLE assumes that either the best tree or the best arc can be determined by an oracle. 4 Conclusions We have presented an empirical comparison for Vietnamese dependency parsing. Experimental results on the Vietnamese dependency treebank VnDT (Nguyen et al., 2014b) show that the neural network-based parsers (Kiperwasser and Goldberg, 2016b) obtain significantly higher scores than the traditional parsers (McDonald et al., 2005; Nivre et al., 2007b). More specifically, in each"
U16-1017,P82-1020,0,0.823,"Missing"
U16-1017,Q16-1032,0,0.0674042,"Missing"
U16-1017,Q16-1023,0,0.0898462,"Missing"
U16-1017,P13-2109,0,0.102004,"Missing"
U16-1017,J11-1007,0,0.169542,"Missing"
U16-1017,P05-1012,0,0.526091,"Missing"
U16-1017,E14-2005,1,0.927337,"results across many languages. Chen and Manning (2014), Weiss et al. (2015), Pei et al. (2015), and Andor et al. (2016) represent the core features with dense vector embeddings and then feed them as inputs to neural network-based classifiers, while Dyer et al. (2015), Kiperwasser and Goldberg (2016a), and Kiperwasser and Goldberg (2016b) propose novel neural network architectures to solve the feature-engineering problem. Dependency parsing for Vietnamese has not been actively explored. One main reason is because there is no manually labeled dependency treebank available. Thi et al. (2013) and Nguyen et al. (2014b) propose constituent-to-dependency conversion approaches to automatically translate the manually built constituent treebank for Vietnamese (Nguyen et al., 2009) to dependency treebanks. The converted dependency treebanks are then used in later works on Vietnamese dependency parsing, including Vu-Manh et al. (2015), Le-Hong et al. (2015) and Nguyen and Nguyen (2015). All of the previous research works use either the MSTparser (McDonald et al., 2005) or the Maltparser (Nivre et al., 2007b) for their parsing experiments. Among them, Nguyen et al. (2014b) report the highest results with LAS at 7"
U16-1017,P08-1108,0,0.0387999,"ttach the indexed3 word to be dependent to the indexed-2 word by the label nmod. This sentence is translated to English as “He who excuses#1 himself#2 , accuses#3 himself .” Oracle Tree Arc With punct. LAS UAS LS 79.20 85.22 88.38 85.98 90.50 92.67 Without punct. LAS UAS LS 79.33 86.24 86.66 85.96 91.14 91.57 Table 5: Upper bound of ensemble performance. One simple approach to improve parsing performance for Vietnamese is to separately use the graph-based parser BistG for short sentences and the transition-based parser BistT for longer sentences. Another approach is to use system combination (Nivre and McDonald, 2008; Zhang and Clark, 2008), e.g. building ensemble systems (Sagae and Tsujii, 2007; Surdeanu and Manning, 2010; Haffari et al., 2011). Table 5 presents an upper bound of oracle ensemble performance, using the DEPENDABLE toolkit (Choi et al., 2015). DEPENDABLE assumes that either the best tree or the best arc can be determined by an oracle. 4 Conclusions We have presented an empirical comparison for Vietnamese dependency parsing. Experimental results on the Vietnamese dependency treebank VnDT (Nguyen et al., 2014b) show that the neural network-based parsers (Kiperwasser and Goldberg, 2016b) obtai"
U16-1017,P15-1032,0,0.0567917,"Missing"
U16-1017,D08-1059,0,0.0257571,"be dependent to the indexed-2 word by the label nmod. This sentence is translated to English as “He who excuses#1 himself#2 , accuses#3 himself .” Oracle Tree Arc With punct. LAS UAS LS 79.20 85.22 88.38 85.98 90.50 92.67 Without punct. LAS UAS LS 79.33 86.24 86.66 85.96 91.14 91.57 Table 5: Upper bound of ensemble performance. One simple approach to improve parsing performance for Vietnamese is to separately use the graph-based parser BistG for short sentences and the transition-based parser BistT for longer sentences. Another approach is to use system combination (Nivre and McDonald, 2008; Zhang and Clark, 2008), e.g. building ensemble systems (Sagae and Tsujii, 2007; Surdeanu and Manning, 2010; Haffari et al., 2011). Table 5 presents an upper bound of oracle ensemble performance, using the DEPENDABLE toolkit (Choi et al., 2015). DEPENDABLE assumes that either the best tree or the best arc can be determined by an oracle. 4 Conclusions We have presented an empirical comparison for Vietnamese dependency parsing. Experimental results on the Vietnamese dependency treebank VnDT (Nguyen et al., 2014b) show that the neural network-based parsers (Kiperwasser and Goldberg, 2016b) obtain significantly higher s"
U16-1017,P11-2033,0,0.109133,"Missing"
U16-1017,P15-1031,0,0.0956167,"Missing"
U16-1017,D07-1111,0,0.0426341,"his sentence is translated to English as “He who excuses#1 himself#2 , accuses#3 himself .” Oracle Tree Arc With punct. LAS UAS LS 79.20 85.22 88.38 85.98 90.50 92.67 Without punct. LAS UAS LS 79.33 86.24 86.66 85.96 91.14 91.57 Table 5: Upper bound of ensemble performance. One simple approach to improve parsing performance for Vietnamese is to separately use the graph-based parser BistG for short sentences and the transition-based parser BistT for longer sentences. Another approach is to use system combination (Nivre and McDonald, 2008; Zhang and Clark, 2008), e.g. building ensemble systems (Sagae and Tsujii, 2007; Surdeanu and Manning, 2010; Haffari et al., 2011). Table 5 presents an upper bound of oracle ensemble performance, using the DEPENDABLE toolkit (Choi et al., 2015). DEPENDABLE assumes that either the best tree or the best arc can be determined by an oracle. 4 Conclusions We have presented an empirical comparison for Vietnamese dependency parsing. Experimental results on the Vietnamese dependency treebank VnDT (Nguyen et al., 2014b) show that the neural network-based parsers (Kiperwasser and Goldberg, 2016b) obtain significantly higher scores than the traditional parsers (McDonald et al., 200"
U16-1017,N10-1091,0,0.0484834,"Missing"
U16-1017,P81-1022,0,0.467235,"Missing"
U16-1017,P13-1104,0,\N,Missing
U17-1013,W03-0316,0,0.14222,"Missing"
U17-1013,P09-1058,0,0.0273785,"a written text “thu∏ thu nh™p cá nhân” (individualcá_nhân incomethu_nh™p taxthu∏ ) consisting of 5 syllables, the word segmenter returns a two-word phrase “thu∏_thu_nh™p cá_nhân.”1 Then given the input segmented text “thu∏_thu_nh™p cá_nhân”, the POS tagger returns “thu∏_thu_nh™p/N cá_nhân/N.” A class of approaches to POS tagging from unsegmented text that has been actively explored in other languages, such as in Chinese and Japanese, is joint word segmentation and POS tagging (Zhang and Clark, 2008). A possible joint strategy is to assign a combined segmentation and POS tag to each syllable (Kruengkrai et al., 2009). For example, given the input text “thu∏ thu nh™p cá nhân”, the joint strategy would produce “thu∏/BN thu/I-N nh™p/I-N cá/B-N nhân/I-N”, where B refers to the beginning of a word and I refers to the inside of a word. Shao et al. (2017) showed that this joint strategy gives SOTA results for Chinese POS tagging by utilizing a BiLSTM-CNNCRF model (Ma and Hovy, 2016). In this paper, we present the first empirical study comparing the joint and pipeline strategies for Vietnamese POS tagging from unsegmented text. In addition, we make a comparison between SOTA feature-based and neural networkbased m"
U17-1013,N16-1030,0,0.082684,"tion rule-based learning model which obtained the highest accuracy at the VLSP 2013 POS tagging shared task.4 • MarMoT (Mueller et al., 2013) is a generic CRF framework and a SOTA POS and morphological tagger.5 • BiLSTM-CRF (Huang et al., 2015) is a sequence labeling model which extends the BiLSTM model with a CRF layer. • BiLSTM-CRF + CNN-char, i.e. BiLSTMCNN-CRF, is an extension of the BiLSTMCRF, using CNN to derive character-based representations (Ma and Hovy, 2016). • BiLSTM-CRF + LSTM-char is another extension of the BiLSTM-CRF, using BiLSTM to derive the character-based representations (Lample et al., 2016). Here, for the pipeline strategy, we train these models to predict POS tags with respect to (w.r.t.) gold word segmentation. In addition, we also retrain the fast and accurate Vietnamese word segmenter RDRsegmenter (Nguyen et al., 2017b) using the training set of 27k sentences.6 3 The data was officially used for the Vietnamese POS tagging shared task at the second VLSP 2013 workshop. 4 http://rdrpostagger.sourceforge.net 5 http://cistern.cis.lmu.de/marmot 6 RDRsegmenter obtains a segmentation speed at 60k words per second, computed on a personal computer of Intel Core i7 2.2 GHz. RDRsegmente"
U17-1013,2010.jeptalnrecital-long.36,0,0.579301,"Missing"
U17-1013,D13-1032,0,0.167826,"Missing"
U17-1013,Y06-1028,0,0.805919,"Missing"
U17-1013,K17-3014,1,0.90322,"2008), Pham et al. (2009) and Tran et al. (2012) used the maximum matching method (NanYuan and YanBin, 1991) to generate all possible segmentations for each input sentence; then to select the best segmentation, Le et al. (2008) and Tran et al. (2012) applied ngram language model while Pham et al. (2009) employed POS information from an external POS tagger. Later, Liu and Lin (2014) and Nguyen and Le (2016) proposed approaches based on pointwise prediction, where a binary classifier is trained to identify whether or not there is a word boundary at each point between two syllables. Furthermore, Nguyen et al. (2017b) proposed a rule-based approach which gets the highest results to date in terms of both segmentation accuracy and speed. 2.2 POS tagging Regarding Vietnamese POS tagging, Dien and Kiem (2003) projected POS annotations from English to Vietnamese via a bilingual corpus of word alignments. As a standard sequence labeling task, previous research has applied the CRF, SVM or MaxEnt model to assign each word a POS tag (Nghiem et al., 2008; Tran et al., 2009; Le-Hong et al., 2010; Nguyen et al., 2010; Tran et al., 2010; Bach et al., 2013). In addition, Nguyen et al. (2011) proposed a rule-based appr"
U17-1013,E14-2005,1,0.665315,"labeling models on the syllable-based transformed corpus. 3.2 Dataset The Vietnamese treebank (Nguyen et al., 2009) is the largest annotated corpus for Vietnamese, providing a set of 27,870 manually POS-annotated sentences for training and development (about 23 words per sentence on average) and a test set of 2120 manually POS-annotated sentences (about 31 words per sentence).3 From the set of 27,870 sentences, we use the first 27k sentences for training and the last 870 sentences for development. 3.3 Models For both joint and pipeline strategies, we use the following models: • RDRPOSTagger (Nguyen et al., 2014a) is a transformation rule-based learning model which obtained the highest accuracy at the VLSP 2013 POS tagging shared task.4 • MarMoT (Mueller et al., 2013) is a generic CRF framework and a SOTA POS and morphological tagger.5 • BiLSTM-CRF (Huang et al., 2015) is a sequence labeling model which extends the BiLSTM model with a CRF layer. • BiLSTM-CRF + CNN-char, i.e. BiLSTMCNN-CRF, is an extension of the BiLSTMCRF, using CNN to derive character-based representations (Ma and Hovy, 2016). • BiLSTM-CRF + LSTM-char is another extension of the BiLSTM-CRF, using BiLSTM to derive the character-based"
U17-1013,I17-3010,0,0.256196,"Missing"
U17-1013,D17-1035,0,0.019357,"second, computed on a personal computer of Intel Core i7 2.2 GHz. RDRsegmenter is available at: https: //github.com/datquocnguyen/RDRsegmenter 110 Pipeline 100 100 150 Joint 200 250 250 Table 1: Optimal number of LSTM units. 3.4 Implementation details We use the original pure Java implementations of RDRPOSTagger and MarMoT with default hyperparameter settings in our experiments. Instead of using implementations independently provided by authors of BiLSTM-CRF, BiLSTM-CRF + CNNchar7 and BiLSTM-CRF + LSTM-char, we use a reimplementation which is optimized for performance of all these models from Reimers and Gurevych (2017).8 For three BiLSTM-CRF-based models, we use default hyper-parameters provided by Reimers and Gurevych (2017) with the following exceptions: we use a dropout rate at 0.5 (Ma and Hovy, 2016) with the frequency threshold of 5 for unknown word and syllable types. We initialize word and syllable embeddings with 100-dimensional pretrained embeddings,9 then learn them together with other model parameters during training by using Nadam (Dozat, 2016). For training, we run for 100 epochs. We perform a grid search of hyperparameters to select the number of BiLSTM layers from {1, 2, 3} and the number of"
U17-1013,I17-1018,0,0.0204126,"the POS tagger returns “thu∏_thu_nh™p/N cá_nhân/N.” A class of approaches to POS tagging from unsegmented text that has been actively explored in other languages, such as in Chinese and Japanese, is joint word segmentation and POS tagging (Zhang and Clark, 2008). A possible joint strategy is to assign a combined segmentation and POS tag to each syllable (Kruengkrai et al., 2009). For example, given the input text “thu∏ thu nh™p cá nhân”, the joint strategy would produce “thu∏/BN thu/I-N nh™p/I-N cá/B-N nhân/I-N”, where B refers to the beginning of a word and I refers to the inside of a word. Shao et al. (2017) showed that this joint strategy gives SOTA results for Chinese POS tagging by utilizing a BiLSTM-CNNCRF model (Ma and Hovy, 2016). In this paper, we present the first empirical study comparing the joint and pipeline strategies for Vietnamese POS tagging from unsegmented text. In addition, we make a comparison between SOTA feature-based and neural networkbased models, which, to the best of our knowledge, has not done in any prior work on Vietnamese. On the benchmark Vietnamese treebank (Nguyen et al., 2009), we show that the pipeline strategy produces better scores than the joint strategy. We"
U17-1013,N03-1033,0,0.183629,"Missing"
U17-1013,K17-3001,0,0.0615701,"Missing"
U17-1013,P08-1101,0,0.0409698,"ed the word-segmented text—which is the output of the word segmenter—as the input to a POS tagger. For example, given a written text “thu∏ thu nh™p cá nhân” (individualcá_nhân incomethu_nh™p taxthu∏ ) consisting of 5 syllables, the word segmenter returns a two-word phrase “thu∏_thu_nh™p cá_nhân.”1 Then given the input segmented text “thu∏_thu_nh™p cá_nhân”, the POS tagger returns “thu∏_thu_nh™p/N cá_nhân/N.” A class of approaches to POS tagging from unsegmented text that has been actively explored in other languages, such as in Chinese and Japanese, is joint word segmentation and POS tagging (Zhang and Clark, 2008). A possible joint strategy is to assign a combined segmentation and POS tag to each syllable (Kruengkrai et al., 2009). For example, given the input text “thu∏ thu nh™p cá nhân”, the joint strategy would produce “thu∏/BN thu/I-N nh™p/I-N cá/B-N nhân/I-N”, where B refers to the beginning of a word and I refers to the inside of a word. Shao et al. (2017) showed that this joint strategy gives SOTA results for Chinese POS tagging by utilizing a BiLSTM-CNNCRF model (Ma and Hovy, 2016). In this paper, we present the first empirical study comparing the joint and pipeline strategies for Vietnamese PO"
W02-1007,A00-2018,1,0.809791,"9 5 0.429 4 0.343 4 0.343 3 0.257 3 0.257 3 0.257 3 0.257 3 0.257 3 0.257 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 2 1 1 0.171 0.171 0.085 0.085 1 0.085 1 0.085 Table 2: The 40 Most Common Parentheticals erty accounts for their observation that removing these dis uencies does not help in language modeling perplexity results. This strongly suggests that INTJ/PRN location information in speech text might in fact, improve parsing performance by helping the parser locate constituent boundaries with high accuracy. That is, a statistic parser such as [1] or [3] when trained on parsed Switchboard text with these phenomena left in, might learn the statistical correlations between them and phrase boundaries just as they are obviously learning the correlations between punctuation and phrase boundaries in written text. In this paper then we wish to determine if the presence of INTJs and PRNs do help parsing, at least for one state-of-the-art statistical parser [1]. 2 Experimental Design The experimental design used was more complicated than we initially expected. We had anticipated that the experiments would be conducted analogously to the 
o pun"
W02-1007,N01-1016,1,0.909019,"Missing"
W02-1007,P97-1003,0,0.197375,"29 4 0.343 4 0.343 3 0.257 3 0.257 3 0.257 3 0.257 3 0.257 3 0.257 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 0.171 2 2 1 1 0.171 0.171 0.085 0.085 1 0.085 1 0.085 Table 2: The 40 Most Common Parentheticals erty accounts for their observation that removing these dis uencies does not help in language modeling perplexity results. This strongly suggests that INTJ/PRN location information in speech text might in fact, improve parsing performance by helping the parser locate constituent boundaries with high accuracy. That is, a statistic parser such as [1] or [3] when trained on parsed Switchboard text with these phenomena left in, might learn the statistical correlations between them and phrase boundaries just as they are obviously learning the correlations between punctuation and phrase boundaries in written text. In this paper then we wish to determine if the presence of INTJs and PRNs do help parsing, at least for one state-of-the-art statistical parser [1]. 2 Experimental Design The experimental design used was more complicated than we initially expected. We had anticipated that the experiments would be conducted analogously to the 
o punctuatio"
W03-1019,P99-1069,1,0.756674,"t this method is faster to converge than the conjugate gradient descent method. It is well known that optimizing log-loss functions may result in overfitting, especially with noisy data. For this reason, we used a regularization term in our cost functions. We experimented with different regularization terms. As expected, we observed that the regularization term increases the accuracy, especially when the training data is small; but we did not observe much difference when we used different regularization terms. The results we report are with the Gaussian prior regularization term described in (Johnson et al., 1999). Our goal in this paper is not to build the best tagger or recognizer, but to compare different loss functions and optimization methods. Since we did not spend much effort on designing the most useful features, our results are slightly worse than, but comparable to the best performing models. We extracted corpora of different sizes (ranging from 300 sentences to the complete corpus) and ran experiments optimizing the four loss functions using different feature sets. In Table 1 and Table 2, we report the accuracy of predicting every individual label. It can be seen that the test accuracy obtai"
W03-1019,W02-1002,0,0.29434,"ve as changing the features included in the model. 1 Introduction Until recent years, generative models were the most common approach for many NLP tasks. Recently, there is a growing interest on discriminative models in the NLP community, and these models were shown to be successful for different tasks(Lafferty et al., 2001; Ratnaparkhi, 1999; Collins, 2000). Discriminative models do not only have theoretical advantages over generative models, as we discuss in Section 2, but they are also shown to be empirically favorable over generative models when features and objective functions are fixed (Klein and Manning, 2002). In this paper, we use discriminative models to investigate the optimization of different objective functions by a variety of optimization methods. We Thomas Hofmann Computer Science Brown University Providence, RI 02912 th@cs.brown.edu focus on label sequence learning tasks. Part-ofSpeech (POS) tagging and Named Entity Recognition (NER) are the most studied applications among these tasks. However, there are many others, such as chunking, pitch accent prediction and speech edit detection. These tasks differ in many aspects, such as the nature of the label sequences (chunks or individual label"
W03-1019,W00-1308,0,0.0712259,"Entropy Markov Models (MEMMs) (McCallum et al., 2000), Projection based Markov Models (PMMs) (Punyakanok and Roth, 2000) and Conditional Random Fields (CRFs) (Lafferty et al., 2001), have been proposed to overcome these problems. The common property of these models is their discriminative approach. They model the probability distribution of the +label  sequences given the observation sequences: * )  . The best performing models of label sequence learning are MEMMs or PMMs (also known as Maximum Entropy models) whose features are carefully designed for the specific tasks (Ratnaparkhi, 1999; Toutanova and Manning, 2000). However, maximum entropy models suffer from the so called label bias problem, the problem of making local decisions (Lafferty et al., 2001). Lafferty et al. (2001) show that CRFs overcome the label-bias problem and outperform MEMMs in POS tagging. CRFs define a probability distribution over the whole sequence  , globally conditioning over the whole observation sequence  (Figure 1b). Because they condition on the observation (as opposed to generating it),  they - can use overlapping features.  used in this paper are of the The features ,  form: 1. Current label and information about"
W03-1019,P02-1062,0,\N,Missing
W03-1022,A00-2018,0,0.0145582,"ixes, suffixes, complex morphology: R!S$05 , R!S$0573 ... RQ-2$=@ , R-T$0AQ@ ... R/P$0573!A , R/P$=@1A?U!647DA ... 3 A simple option to deal with ambiguous words would be to distribute an ambiguous noun’s counts to all its senses. However, in preliminary experiments we found that a better accuracy is achieved using only non-ambiguous nouns. We will investigate this issue in future research. Open class words were morphologically simplified with the “morph” function included in WordNet. We parsed the WordNet definitions and example sentences with the same syntactic parser used for Bllip (Charniak, 2000). It is not always possible to identify the noun that represents the synset in the WordNet glosses. For example, in the gloss for the synset relegation the example sentence is “He has been relegated to a post in Siberia”, where a verb is used instead of the noun. When it was possible to identify the target noun the complete feature set was used; otherwise only the surrounding-word features (2) and the spelling features (7) of all synonyms were used. With the definitions it is much harder to individuate the target; consider the definition “a member of the genus Canis” for dog. For all definitio"
W03-1022,W02-0903,1,0.772998,"assigned to synsets, often ambiguity is preserved even at this level. For example, chair has three supersenses: “person”, “artifact”, and “act”. This set of labels has a number of attractive features for the purposes of lexical acquisition. It is fairly general and therefore small. The reasonable size of the label set makes it possible to apply stateof-the-art machine learning methods. Otherwise, classifying new words at the synset level defines a multiclass problem with a huge class space - more than 66,000 noun synsets in WordNet 1.6, more than 75,000 in the newest release, 1.71 (cf. also (Ciaramita, 2002) on this problem). At the same time the labels are not too abstract or vague. Most of the classes seem natural and easily recognizable. That is probably why they were chosen by the lexicographers to facilitate their task. But there are more important practical and methodological advantages. 3.2 Extra Training Data from WordNet WordNet contains a great deal of information about words and word senses.The information contained 1 There are also 15 lexicographer classes for verbs, 3 for adjectives and 1 for adverbs. 2 The label “Tops” refers to about 40 very general synsets, such as “phenomenon” “e"
W03-1022,W99-0613,0,0.252509,"actic and semantic properties of unknown words. In terms of the WordNet lexical database, one would like to automatically assign unknown words a position in the synset hierarchy, introducing new synsets and extending the synset hierarchy where appropriate. Doing this accurately is a difficult problem, and in this paper we address a simpler problem: automatically determining the broad semantic class, or supersense, to which unknown words belong. Systems for thesaurus extension (Hearst, 1992; Roark and Charniak, 1998), information extraction (Riloff and Jones, 1999) or named-entity recognition (Collins and Singer, 1999) each partially address this problem in different ways. The goal in these tasks is automatically tagging words with semantic labels such as “vehicle”, “organization”, “person”, etc. In this paper we extend the named-entity recognition approach to the classification of common nouns into 26 different supersenses. Rather than define these ourselves, we adopted the 26 “lexicographer class” labels used in WordNet, which include labels such as person, location, event, quantity, etc. We believe our general approach should generalize to other definitions of supersenses. Using the WordNet lexicographer"
W03-1022,W02-1001,0,0.0927723,"cy on Test that the update is balanced, which is crucial to guaranteeing the convergence of the learning procedure (cf. (Crammer and Singer, 2002)). We have focused on the simplest case of uniform update weights, ¢!} [  V     for a©h  . The algorithm is summarized in Algorithm 1. Notice that the multiclass perceptron algorithm learns all weight vectors in a coupled manner, in contrast to methods that perform multiclass classification by combining binary classifiers, for example, training a classifier for each class in a one-againstthe-rest manner. The averaged version of the perceptron (Collins, 2002), like the voted perceptron (Freund and Schapire, 1999), reduces the effect of over-training. In addition to the matrix of weight vectors s the model keeps track for each feature ª of each value it assumed during training, ª o , and the number of consecutive training instance presentations during which this weight was not changed, or “life span”,  ] ª ob . When training is done these weights are averaged and the final averaged weight ªr«1¬; of feature ª is computed as 38 34 33 32 31 30 29 Averaged perceptron Basic perceptron 28 27 0 100 Parameters Setting We used an implementation with full"
W03-1022,C92-2082,0,0.062142,"Science Foundation under Grant No. 0085940. Mark Johnson Brown University mark johnson@brown.edu identifying the syntactic and semantic properties of unknown words. In terms of the WordNet lexical database, one would like to automatically assign unknown words a position in the synset hierarchy, introducing new synsets and extending the synset hierarchy where appropriate. Doing this accurately is a difficult problem, and in this paper we address a simpler problem: automatically determining the broad semantic class, or supersense, to which unknown words belong. Systems for thesaurus extension (Hearst, 1992; Roark and Charniak, 1998), information extraction (Riloff and Jones, 1999) or named-entity recognition (Collins and Singer, 1999) each partially address this problem in different ways. The goal in these tasks is automatically tagging words with semantic labels such as “vehicle”, “organization”, “person”, etc. In this paper we extend the named-entity recognition approach to the classification of common nouns into 26 different supersenses. Rather than define these ourselves, we adopted the 26 “lexicographer class” labels used in WordNet, which include labels such as person, location, event, qu"
W03-1022,P98-2182,0,0.139555,"ation under Grant No. 0085940. Mark Johnson Brown University mark johnson@brown.edu identifying the syntactic and semantic properties of unknown words. In terms of the WordNet lexical database, one would like to automatically assign unknown words a position in the synset hierarchy, introducing new synsets and extending the synset hierarchy where appropriate. Doing this accurately is a difficult problem, and in this paper we address a simpler problem: automatically determining the broad semantic class, or supersense, to which unknown words belong. Systems for thesaurus extension (Hearst, 1992; Roark and Charniak, 1998), information extraction (Riloff and Jones, 1999) or named-entity recognition (Collins and Singer, 1999) each partially address this problem in different ways. The goal in these tasks is automatically tagging words with semantic labels such as “vehicle”, “organization”, “person”, etc. In this paper we extend the named-entity recognition approach to the classification of common nouns into 26 different supersenses. Rather than define these ourselves, we adopted the 26 “lexicographer class” labels used in WordNet, which include labels such as person, location, event, quantity, etc. We believe our"
W03-1022,C98-2177,0,\N,Missing
W04-0105,W02-0603,0,0.0527446,"ly applicable, and possibly more successful, with phonological transcriptions. However, since we wish to have an entirely unsupervised system and we require a morphological segmentation as input, we are currently limited by the capabilities of Linguistica, which requires standard textual input. For the remainder of this paper, we use “phonology” and “phonological rules” in a broad sense to include orthography as well.    lift            jump  roll ×            walk  ...  −s −ed −ing ...          Figure 1: An example signature segmentation (Goldsmith, 2001; Creutz and Lagus, 2002), discovery of syllabicity and sonority (Ellison, 1993), and learning constraints on vowel harmony and consonant clusters (Ellison, 1994). However, our work shows that a straightforward MDL approach, where the prior − log Pr(H) depends on the length of the phonological rules and the rest of the grammar in the obvious way, does not result in a successful system for learning phonological rules. We discuss why this is so, and then present several changes that can be made to the prior in order to learn phonological rules successfully. Our conclusion is that, although Bayesian techniques can be suc"
W04-0105,J01-2001,0,0.609483,"etermine the prior probabilities of various hypotheses. In this paper, we compare the results of using two different prior distributions for an unsupervised learning task in the domain of morpho-phonology. Our goal is to learn transformation rules of the form x → y / C, where x and y are individual characters (or the empty character ) and C is some representation of the context licensing the transformation. Our input is an existing segmentation of words from the Penn Treebank (Marcus et al., 93) into stems and suffixes. This segmentation is provided by the Linguistica morphological analyzer (Goldsmith, 2001; Goldsmith, 2004b), itself an unsupervised algorithm. Using the transformation rules we learn, we are able to output a new segmentation that more closely matches our linguistic intuitions. 1 We are not the first to apply Bayesian learning techniques for unsupervised learning of morphology and phonology. Several other researchers have also pursued these methods, usually within a Minimum Description Length (MDL) framework (Rissanen, 1989). In MDL approaches, − log Pr(H) is taken to be proportional to the length of H in some standard encoding, and − log Pr(D|H) is the length of D using the encod"
W04-0105,P84-1070,1,0.76251,"of similar signature pairs, the rules relating them, and the possible contexts for those rules, we need to determine which rules are actually phonologically legitimate and which are simply accidents of the data. We do this by simply considering each rule and context in turn, proceeding from the most attested to least attested rules and from most likely to least likely contexts. For each rule-context pair, we add the rule to the grammar 4 The reasoning we use to finding conditioning contexts for deletion rules was also described by Goldsmith (2004a), and is similar to the much earlier work of Johnson (1984). FIND P HONO RULES () 1 G ← grammar produced by Linguistica 2 R ← ordered set of possible rules 3 for each r ∈ R 4 do 5 Cr ← ordered set of possible contexts for r 6 C←∅ 7 while Cr 6= ∅ 8 do c ← next c ∈ Cr 9 Cr ← Cr  {c} 10 C ← C ∪ {c} 11 G0 ← collapseInContext(G, r, C) 12 G0 ← pruneRules(G0 ) 13 if score(G0 ) &lt; score(G) 14 then G ← G0 15 return G COLLAPSE I N C ONTEXT (G, r, C) 1 2 3 4 for each σi ∈ G do for each σj ∈ G do if (σi → σj ) ∧ (∀(t, f ) ∈ σi , ctx(t, f ) ∈ C) r then collapseSigs(σi , σj ) Figure 4: Pseudocode for our search algorithm with that context and collapse any pairs of"
W04-0105,J93-2004,0,\N,Missing
W04-0824,A00-2018,0,0.0152777,"r way up the hierarchy following the paths from the sense to the top until we found &lt; synsets. At The same features were extracted from the given each level we look for the closest &lt; descendants test and training data, and the additional dataset. of the current synset as follows - this is the “closPOS and other syntactic features were extracted est descendants()” function of Algorithm 1 above. from parse trees. Training and test data, and If there are &lt; or less descendants we collect them the Wordnet glosses, were parsed with Charniak’s all. Otherwise, we take the closest &lt; around the parser (Charniak, 2000). Open class words were synset exploiting the fact that when ordered, using morphologically simplified with the “morph” functhe synset IDs as keys, similar synsets tend to be tion from the Wordnet library “wn.h”. When it close to each other4 . For example, synsets around was not possible to identify the noun or verb in the “Rhode Islander” refer to other American states’ inglosses 2 we only extracted a limited set of features: habitants’ names: WS, WC, and morphological features. Each gloss provides one training instance per synset. Overall we found approximately 200,000 features. w 3 External"
W04-0824,W02-1001,0,0.0117524,"rithm the implementation is simpler especially if several components are included. We use the simplest case of uniform update weights, 4      for 6; * . The perceptron algorithm defines a sequence of  1*5 1=&lt;5 1 5 is the weight matrices ,B, 1 , where weight matrix after the first  training items have been processed. In the standard perceptron, the  1 5 &gt; weight matrix 1 is used to classify the unlabeled test examples. However, a variety of methods can be used for regularization or smoothing in order to reduce the effect of overtraining. Here we used the averaged perceptron (Collins, 2002), where the weight matrix used to classify the test data is the average of all of the matrices posited dur * 4 &gt; 1*3254 . ing training, i.e., &gt; 4.3 Multilabel cases Often, several senses of an ambiguous word are very close in the hierarchy. Thus it can happen that a synset belongs to the neighbor set of more than one sense of the ambiguous word. When this is the case the training instance for that synset is treated as a multilabeled instance; i.e., . |* is actually a set of la |&apos; & / . Several methods can bels for (D* , that is, . |*2R 8: be used to deal with multilabeled instances, here we"
W04-0824,W02-1006,0,0.0610581,"Missing"
W05-0615,W95-0101,0,0.0392109,"slation. However, most of the successful work to date has used supervised learning techniques. Unsupervised algorithms that can learn from raw linguistic data, as humans can, remain a challenge. In a statistical Despite the advantages of maximum likelihood estimation and its implementation via various instantiations of the EM algorithm, it is widely regarded as ineffective for unsupervised language learning. Merialdo (1994) showed that with only a tiny amount of tagged training data, supervised training of an HMM part-of-speech tagger outperformed unsupervised EM training. Later results (e.g. Brill (1995)) seemed to indicate that other methods of unsupervised learning could be more effective (although the work of Banko and Moore (2004) suggests that the difference may be far less than previ112 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 112–119, Ann Arbor, June 2005. 2005 Association for Computational Linguistics ously assumed). Klein and Manning (2001; 2002) recently achieved more encouraging results using an EM-like algorithm to induce syntactic constituent grammars, based on a deficient probability model. It has been suggested that EM often"
W05-0615,W01-0714,0,0.126039,"language learning. Merialdo (1994) showed that with only a tiny amount of tagged training data, supervised training of an HMM part-of-speech tagger outperformed unsupervised EM training. Later results (e.g. Brill (1995)) seemed to indicate that other methods of unsupervised learning could be more effective (although the work of Banko and Moore (2004) suggests that the difference may be far less than previ112 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 112–119, Ann Arbor, June 2005. 2005 Association for Computational Linguistics ously assumed). Klein and Manning (2001; 2002) recently achieved more encouraging results using an EM-like algorithm to induce syntactic constituent grammars, based on a deficient probability model. It has been suggested that EM often yield poor results because it is overly sensitive to initial parameter values and tends to converge on likelihood maxima that are local, but not global (Carroll and Charniak, 1992). In this paper, we present a series of experiments indicating that for the task of learning a syllable structure grammar, the initial parameter weights are not crucial. Rather, it is the choice of the model class, i.e., the"
W05-0615,P02-1017,0,0.0987533,"Missing"
W05-0615,J94-2001,0,0.592124,"text-Free Grammars (PCFGs). Introduction The use of statistical methods in computational linguistics has produced advances in tasks such as parsing, information retrieval, and machine translation. However, most of the successful work to date has used supervised learning techniques. Unsupervised algorithms that can learn from raw linguistic data, as humans can, remain a challenge. In a statistical Despite the advantages of maximum likelihood estimation and its implementation via various instantiations of the EM algorithm, it is widely regarded as ineffective for unsupervised language learning. Merialdo (1994) showed that with only a tiny amount of tagged training data, supervised training of an HMM part-of-speech tagger outperformed unsupervised EM training. Later results (e.g. Brill (1995)) seemed to indicate that other methods of unsupervised learning could be more effective (although the work of Banko and Moore (2004) suggests that the difference may be far less than previ112 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 112–119, Ann Arbor, June 2005. 2005 Association for Computational Linguistics ously assumed). Klein and Manning (2001; 2002) rec"
W05-0615,P01-1053,0,0.341835,"Missing"
W05-0615,W02-0608,0,0.292493,"Missing"
W05-0615,W98-1223,0,0.0543507,"Missing"
W05-0615,J01-3002,0,0.0321306,"cedures and representations that can lead to successful unsupervised language learning in both computers and humans. Our work has some similarity to that of M¨uller, 113 who trains a PCFG of syllable structure from a corpus of words with syllable boundaries marked. We, too, use a model defined by a grammar to describe syllable structure.1 However, our work differs from M¨uller’s in that it focuses on how to learn the model’s parameters in an unsupervised manner. Several researchers have worked on unsupervised learning of phonotactic constraints and word segmentation (Elman, 2003; Brent, 1999; Venkataraman, 2001), but to our knowledge there is no previously published work on unsupervised learning of syllable structure. In the work described here, we experimented with two different classes of models of syllable structure. Both of these model classes are presented as PCFGs. The first model class, described in M¨uller (2002), encodes information about the positions within a word or syllable in which each phoneme is likely to appear. In this positional model, each syllable is labeled as initial (I), medial (M), final (F), or as the one syllable in a monosyllabic word (O). Syllables are broken down into an"
W05-0615,C04-1080,0,\N,Missing
W06-1636,N03-1014,0,0.0661366,"on automatically. Our results are not quite as good as those carefully created by hand, but they are close (84.8 vs 85.7). 1 Introduction and Previous Research It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank (Marcus et al., 1993) labels (e.g., S, N P , etc.) (Charniak, 1996). Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (Magerman, 1995; Collins, 1996; Collins, 1997; Johnson, 1998; Charniak, 2000; Henderson, 2003; Klein and Manning, 2003; Matsuzaki et al., 2005) (and others). One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998; Klein and Manning, 2003). Rather than imagining the parser roaming around the tree for picking up the infor301 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 301–307, c Sydney, July 2006. 2006 Association for Computational Linguistics splitting them. We also consider a much wider range of contextual information than just parent phrase-markers."
W06-1636,J98-4004,1,0.908409,"to learn the “right” combination automatically. Our results are not quite as good as those carefully created by hand, but they are close (84.8 vs 85.7). 1 Introduction and Previous Research It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank (Marcus et al., 1993) labels (e.g., S, N P , etc.) (Charniak, 1996). Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (Magerman, 1995; Collins, 1996; Collins, 1997; Johnson, 1998; Charniak, 2000; Henderson, 2003; Klein and Manning, 2003; Matsuzaki et al., 2005) (and others). One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998; Klein and Manning, 2003). Rather than imagining the parser roaming around the tree for picking up the infor301 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 301–307, c Sydney, July 2006. 2006 Association for Computational Linguistics splitting them. We also consider a much wider range of contextual information"
W06-1636,P03-1054,0,0.168315,"r relabel the nodes to directly encode this information. Thus rather than have the parser “look” to find out that, say, the parent of some N P is an S, we simply relabel the N P as an N P [S]. This viewpoint is even more compelling if one does not intend to smooth the probabilities. For example, consider p(N P → P RN |N P [S]) If we have no intention of backing off this probability to p(N P → P RN |N P ) we can treat N P [S] as an uninterpreted phrasal category and run all of the standard PCFG algorithms without change. The result is a vastly simplified parser. This is exactly what is done by Klein and Manning (2003). Thus the “phrasal categories” of our title refer to these new, hybrid categories, such as N P [S]. We hope to learn which of these categories work best given that they cannot be made too specific because that would create sparse data problems. The Klein and Manning (2003) parser is an unlexicalized PCFG with various carefully selected context annotations. Their model uses some parent annotations, and marks nodes which initiate or in certain cases conclude unary productions. They also propose linguistically motivated annotations for several tags, including V P , IN , CC,N P and S. This result"
W06-1636,P95-1037,0,0.414689,"by hand. Here we try to automate the process — to learn the “right” combination automatically. Our results are not quite as good as those carefully created by hand, but they are close (84.8 vs 85.7). 1 Introduction and Previous Research It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank (Marcus et al., 1993) labels (e.g., S, N P , etc.) (Charniak, 1996). Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (Magerman, 1995; Collins, 1996; Collins, 1997; Johnson, 1998; Charniak, 2000; Henderson, 2003; Klein and Manning, 2003; Matsuzaki et al., 2005) (and others). One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998; Klein and Manning, 2003). Rather than imagining the parser roaming around the tree for picking up the infor301 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 301–307, c Sydney, July 2006. 2006 Association for Computational Linguistics splitting them. We also consider"
W06-1636,J93-2004,0,0.0264133,"create various grammars with different contextual annotations on the non-terminals. These grammars were then used in conjunction with a CKY parser. The authors explored the space of different annotation combinations by hand. Here we try to automate the process — to learn the “right” combination automatically. Our results are not quite as good as those carefully created by hand, but they are close (84.8 vs 85.7). 1 Introduction and Previous Research It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank (Marcus et al., 1993) labels (e.g., S, N P , etc.) (Charniak, 1996). Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (Magerman, 1995; Collins, 1996; Collins, 1997; Johnson, 1998; Charniak, 2000; Henderson, 2003; Klein and Manning, 2003; Matsuzaki et al., 2005) (and others). One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998; Klein and Manning, 2003). Rather than imagining the parser roaming around the tree for picking up the infor301 Proce"
W06-1636,C02-1068,0,0.018351,"n talking about clustering nodes, as we do, Magerman creates a decision tree, but the differences between clustering and decision trees are small. Perhaps a more substantial difference is that by not casting his problem as one of learning phrasal categories Magerman loses all of the free PCFG technology that we can leverage. For instance, Magerman must use heuristic search to find his parses and incurs search errors because of it. We use an efficient CKY algorithm to do exhaustive search in reasonable time. where φs ∈ Φ are clusters of annotated nonterminals and where: C(φi → φj φk . . .) = P Belz (2002) considers the problem in a manner more similar to our approach. Beginning with both a non-annotated grammar and a parent annotated grammar, using a beam search they search the space of grammars which can be attained via merging nonterminals. They guide the search using the performance on parsing (and several other tasks) of the grammar at each stage in the search. In contrast, our approach explores the space of grammars by starting with few nonterminals and (λi ,λj ,λk ...)∈φi ×φj ×φk ... C(λi → λj λk . . .) We refer to the PCFG of some clustering as the clustered grammar. 2.1 Features Most o"
W06-1636,P05-1010,0,0.160362,"e as good as those carefully created by hand, but they are close (84.8 vs 85.7). 1 Introduction and Previous Research It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank (Marcus et al., 1993) labels (e.g., S, N P , etc.) (Charniak, 1996). Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (Magerman, 1995; Collins, 1996; Collins, 1997; Johnson, 1998; Charniak, 2000; Henderson, 2003; Klein and Manning, 2003; Matsuzaki et al., 2005) (and others). One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998; Klein and Manning, 2003). Rather than imagining the parser roaming around the tree for picking up the infor301 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 301–307, c Sydney, July 2006. 2006 Association for Computational Linguistics splitting them. We also consider a much wider range of contextual information than just parent phrase-markers. ters of these annotations. Mohri and Roark (2006)"
W06-1636,N06-1020,1,0.792088,"atsuzaki et al., 2005) (and others). One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998; Klein and Manning, 2003). Rather than imagining the parser roaming around the tree for picking up the infor301 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 301–307, c Sydney, July 2006. 2006 Association for Computational Linguistics splitting them. We also consider a much wider range of contextual information than just parent phrase-markers. ters of these annotations. Mohri and Roark (2006) tackle this problem by searching for what they call “structural zeros”or sets of events which are individually very likely, but are unlikely to coincide. This is to be contrasted with sets of events that do not appear together simply because of sparse data. They consider a variety of statistical tests to decide whether a joint event is a structural zero. They mark the highest scoring nonterminals that are part of these joint events in the treebank, and use the resulting PCFG. 2 Background A PCFG is a tuple (V, M, µ0 , R, q : R → [0, 1]), where V is a set of terminal symbols; M = {µi } is a se"
W06-1636,A00-2018,1,\N,Missing
W06-1636,W04-3242,0,\N,Missing
W06-1636,P97-1003,0,\N,Missing
W06-1636,J03-4003,0,\N,Missing
W06-1636,P96-1025,0,\N,Missing
W08-0704,P06-1085,1,0.652939,"Missing"
W08-0704,W05-0504,0,0.0340492,"ine morphological learning with word segmentation. This paper continues that research by applying the same kinds of models to Sesotho, a Bantu language spoken in Southern Africa. Bantu languages are especially interesting for this kind of study, as they have rich productive agglutinative morphologies and relatively transparent phonologies, as compared to languages such as Finnish or Turkish which have complex harmony processes and other phonological complexities. The relative clarity of Bantu 21 has inspired previous computational work, such as the algorithm for learning Swahili morphology by Hu et al. (2005). The Hu et al. algorithm uses a Minimum Description Length procedure (Rissanen, 1989) that is conceptually related to the nonparametric Bayesian procedure used here. However, the work here is focused on determining whether the word segmentation methods that work well for English generalize to Sesotho and whether modeling morphological and/or syllable structure improves Sesotho word segmentation, rather than learning Sesotho morphological structure per se. The rest of this paper is structured as follows. Section 2 informally reviews adaptor grammars and describes how they are used to specify d"
W08-0704,P08-1046,1,0.920289,"seem simpler than learning syntax, where the non-linguistic context plausibly supplies important information to human learners. Virtually everyone agrees that the set of possible morphemes and words, if not infinite, is astronomically large, so it seems plausible that humans use some kind of nonparametric procedure to learn the lexicon. Johnson et al. (2007) introduced Adaptor Grammars as a framework in which a wide variety of linguistically-interesting nonparametric inference problems can be formulated and evaluated, including a number of variants of the models described by Goldwater (2007). Johnson (2008) presented a variety of different adaptor grammar word segmentation models and applied them to the problem of segmenting Brent’s phonemicized version of the BernsteinRatner corpus of child-directed English (BernsteinRatner, 1987; Brent, 1999). The main results of that paper were the following: 1. it confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al., 2006a), 2. it showed a small but significant improvement to segmentation accuracy by learning the possible syllable structures of the language together with the lexicon, and 3."
W08-0704,D07-1072,0,0.0384188,"e they are very simple and natural models of hierarchical structure. They are parametric models because each PCFG has a fixed number of rules, each of which has a numerical parameter associated with it. One way to construct nonparametric Bayesian models is to take a parametric model class and let one or more of their components grow unboundedly. There are two obvious ways to construct nonparametric models from PCFGs. First, we can let the number of nonterminals grow unboundedly, as in the Infinite PCFG, where the nonterminals of the grammar can be indefinitely refined versions of a base PCFG (Liang et al., 2007). Second, we can fix the set of nonterminals but permit the number of rules or productions to grow unboundedly, which leads to Adaptor Grammars (Johnson et al., 2007). At any point in learning, an Adaptor Grammar has a finite set of rules, but these can grow unboundedly (typically logarithmically) with the size of the training data. In a word-segmentation application these rules typically generate words or morphemes, so the learner is effectively learning the morphemes and words of its language. The new rules learnt by an Adaptor Grammar are compositions of old ones (that can themselves be com"
W09-0103,J97-4005,0,0.0605491,"“parsing as deduction” (one of the slogans touted by the grammar-based crowd) as unnecessarily restrictive; after all, psycholinguistic research shows that humans are exquisitely Moreover, it’s not necessary for a statistical model to exactly replicate a linguistic constraint in order for it to effectively capture the corresponding generalization: all that’s necessary is that the statistical features “cover” the relevant examples. For example, adding a subject-verb agreement fea5 sensitive to distributional information, so why shouldn’t we let our parsers use that information as well? And as Abney (1997) showed, it is mathematically straight-forward to define probability distributions over the representations used by virtually any theory of grammar (even those of Chomsky’s Minimalism), which means that theoretically the arsenal of statistical methods for parsing and learning can be applied to any grammar just as well. I was interested in; all I needed were the corresponding c-structure or phrase-structure trees. And of course there are many ways of obtaining phrase-structure trees. At the time my colleague Eugene Charniak was developing a statistical phrase-structure parser that was more robu"
W09-0103,P04-1005,1,0.86554,"rammatical then the channel would prefer the identity transformation, while if the input is ungrammatical the channel model would map it to close grammatical sentences. For example, if such a parser were given the input “man bites dog” it might decide that the most probable underlying sentence is “a man bites a dog” and return a parse for that sentence. Such an approach might be regarded as a way of formalizing the idea that ungrammatical sentences are interpreted by analogy with grammatical ones. (Charniak and I proposed a noisy channel model along these lines for parsing transcribed speech (Johnson and Charniak, 2004)). characterizing the set of grammatical analyses in some way, and then assuming that all other analyses are ungrammatical. Borrowing terminology from logic programming (Lloyd, 1987) we might call this a closed-world assumption: any analysis the grammar does not generate is assumed to be ungrammatical. Interestingly, I think that the probabilistic models used statistical parsing generally make an open-world assumption about linguistic analyses. These probabilistic models prefer certain linguistic structures over others, but the smoothing mechanisms that these methods use ensure that every poss"
W09-0103,P99-1069,1,0.746579,"ng a statistical phrase-structure parser that was more robust and had broader coverage than the LFG parser I was working with, and I found I generally got better performance if I used the trees his parser produced, so that’s what I did. This leads to the discriminative re-ranking approach developed by Collins and Koo (2005), in which a statistical parser trained on a treebank is used to produce a set of candidate parses which are then “re-ranked” by an Abney-style probabilistic model. In the late 1990s I explored these kinds of statistical models for Lexical-Functional Grammar (Bresnan, 1982; Johnson et al., 1999). The hope was that statistical features based on LFG’s richer representations (specifically, f -structures) might result in better parsing accuracy. However, this seems not to be the case. As mentioned above, Abney’s formulation of probabilistic models makes essentially no demands on what linguistic representations actually are; all that is required is that the statistical features are functions that map each representation to a real number. These are used to map a set of linguistic representations (say, the set of all grammatical analyses) to a set of vectors of real numbers. Then by definin"
W09-0103,P05-1022,1,0.596745,"-based approaches are also often described as more linguistically based, while statistical approaches are viewed as less linguistically informed. I think this view primarily reflects the origins of the two approaches: the grammar-based approach arose from the collaboration between linguists and computer scientists in the 1980s mentioned earlier, while the statistical approach has its origins in engineering work in speech recognition in which linguists did not play a major role. I also think this view is basically false. In the grammar-based approaches lin4 ture to the Charniak-Johnson parser (Charniak and Johnson, 2005) has no measurable effect on parsing accuracy. After doing this experiment I realized this shouldn’t be surprising: the Charniak parser already conditions each argument’s part-ofspeech (POS) on its governor’s POS, and since POS tags distinguish singular and plural nouns and verbs, these general head-argument POS features capture most cases of subject-verb agreement. guists write the grammars while in statistical approaches linguists annotate the corpora with syntactic parses, so linguists play a central role in both. (It’s an interesting question as to why corpus annotation plus statistical in"
W09-0103,A00-2018,0,0.16514,"lable to our machines may be so different to what is available to a child that the features that work best in our parsers need not bear much relationship to those used by humans. Still, I view the design of the features used in statistical parsers as a fundamentally linguistic issue (albeit one with computational consequences, since the search problem in parsing is largely determined by the features involved), and I expect there is still more to learn about which combinations of features are most useful for statistical parsing. My guess is that the features used in e.g., the Collins (2003) or Charniak (2000) parsers are probably close to optimal for English Penn Treebank parsing (Marcus et al., 1993), but that other features might improve parsing of other languages or even other English genres. Unfortunately changing the features used in these parsers typically involves significant reprogramming, which makes it difficult for linguists to experiment with new features. However, it might be possible to develop a kind of statistical parsing framework that makes it possible to define new features and integrate them into a statistical parser without any programming which would make it easy to explore n"
W09-0103,P04-1061,0,0.0505771,"in both fields applying ideas and results from computational linguistics in their work and using experimental results from their field to develop and improve the computational models. For example, in psycholinguistics researchers such as Hale (2006) and Levy (2008) are using probabilistic models of syntactic structure to make predictions about human sentence processing, and Bachrach (2008) is using predictions from the Roark (2001) parser to help explain the patterns of fMRI activation observed during sentence comprehension. In the field of language acquisition computational linguists such as Klein and Manning (2004) have studied the unsupervised acquisition of syntactic structure, while linguists such as Boersma and Hayes (2001), Goldsmith (2001), Pater (2008) and Albright and Hayes (2003) are developing probabilistic models of the acquisition of phonology and/or morphology, and Frank et al. (2007) experimentally tests the predictions of a Bayesian model of lexical acquisition. Since I have more experience with computational models of language acquisition, I’ll concentrate on this topic for the rest of this section. Much of this work can be viewed under the slogan “structured statistical learning”. That"
W09-0103,J05-1003,0,0.0175003,"methods for parsing and learning can be applied to any grammar just as well. I was interested in; all I needed were the corresponding c-structure or phrase-structure trees. And of course there are many ways of obtaining phrase-structure trees. At the time my colleague Eugene Charniak was developing a statistical phrase-structure parser that was more robust and had broader coverage than the LFG parser I was working with, and I found I generally got better performance if I used the trees his parser produced, so that’s what I did. This leads to the discriminative re-ranking approach developed by Collins and Koo (2005), in which a statistical parser trained on a treebank is used to produce a set of candidate parses which are then “re-ranked” by an Abney-style probabilistic model. In the late 1990s I explored these kinds of statistical models for Lexical-Functional Grammar (Bresnan, 1982; Johnson et al., 1999). The hope was that statistical features based on LFG’s richer representations (specifically, f -structures) might result in better parsing accuracy. However, this seems not to be the case. As mentioned above, Abney’s formulation of probabilistic models makes essentially no demands on what linguistic re"
W09-0103,J03-4003,0,0.123882,"onstructions) available to our machines may be so different to what is available to a child that the features that work best in our parsers need not bear much relationship to those used by humans. Still, I view the design of the features used in statistical parsers as a fundamentally linguistic issue (albeit one with computational consequences, since the search problem in parsing is largely determined by the features involved), and I expect there is still more to learn about which combinations of features are most useful for statistical parsing. My guess is that the features used in e.g., the Collins (2003) or Charniak (2000) parsers are probably close to optimal for English Penn Treebank parsing (Marcus et al., 1993), but that other features might improve parsing of other languages or even other English genres. Unfortunately changing the features used in these parsers typically involves significant reprogramming, which makes it difficult for linguists to experiment with new features. However, it might be possible to develop a kind of statistical parsing framework that makes it possible to define new features and integrate them into a statistical parser without any programming which would make i"
W09-0103,P95-1037,0,0.15083,"stic knowledge is contained in the grammar, which the computational linguist implementing the parsing framework doesn’t actually have to understand. All she has to do is correctly implement an inference engine for grammars written in the relevant grammar formalism. By contrast, statistical parsers define the probability of a parse in terms of its (statistical) features or properties, and a parser designer needs to choose which features their parser will use, and many of these features reflect at least an intuitive understanding of linguistic dependencies. For example, statistical parsers from Magerman (1995) on use features based on head-dependent relationships. (The parsers developed by the Berkeley group are a notable exception (Petrov and Klein, 2007)). While it’s true that only a small fraction of our knowledge about linguistic structure winds up expressed by features in modern statistical parsers, as discussed above there’s no reason to expect all of our scientific knowledge to be relevant to any engineering problem. And while many of the features used in statistical parsers don’t correspond to linguistic constraints, nobody seriously claims that humans understand language only using linguis"
W09-0103,J93-2004,0,0.0487769,"es that work best in our parsers need not bear much relationship to those used by humans. Still, I view the design of the features used in statistical parsers as a fundamentally linguistic issue (albeit one with computational consequences, since the search problem in parsing is largely determined by the features involved), and I expect there is still more to learn about which combinations of features are most useful for statistical parsing. My guess is that the features used in e.g., the Collins (2003) or Charniak (2000) parsers are probably close to optimal for English Penn Treebank parsing (Marcus et al., 1993), but that other features might improve parsing of other languages or even other English genres. Unfortunately changing the features used in these parsers typically involves significant reprogramming, which makes it difficult for linguists to experiment with new features. However, it might be possible to develop a kind of statistical parsing framework that makes it possible to define new features and integrate them into a statistical parser without any programming which would make it easy to explore novel combinations of statistical features; see Goodman (1998) for an interesting suggestion al"
W09-0103,J05-1004,0,0.0245607,"st strongly with those areas of linguistics that study linguistic processing, namely psycholinguistics and language acquisition. As I explain in section 3 below, I think we are starting to see this happen. tant. The grammar-based approaches are sometimes described as producing deeper representations that are closer to meaning. It certainly is true that grammar-based analyses typically represent predicate-argument structure and perhaps also quantifier scope. But one can recover predicateargument structure using statistical methods (see the work on semantic role labeling and “PropBank” parsing (Palmer et al., 2005)), and presumably similar methods could be used to resolve quantifier scope as well. I suspect the main reason why statistical parsing has concentrated on more superficial syntactic structure (such as phrase structure) is because there aren’t many actual applications for the syntactic analyses our parsers return. Given the current state-of-the-art in knowledge representation and artificial intelligence, even if we could produce completely accurate logical forms in some higher-order logic, it’s not clear whether we could do anything useful with them. It’s hard to find real applications that ben"
W09-0103,J01-2004,0,0.0362017,"don’t expect the average researcher in those fields to start doing computational linguistics any time soon. However, I do think there are an emerging cadre of young researchers in both fields applying ideas and results from computational linguistics in their work and using experimental results from their field to develop and improve the computational models. For example, in psycholinguistics researchers such as Hale (2006) and Levy (2008) are using probabilistic models of syntactic structure to make predictions about human sentence processing, and Bachrach (2008) is using predictions from the Roark (2001) parser to help explain the patterns of fMRI activation observed during sentence comprehension. In the field of language acquisition computational linguists such as Klein and Manning (2004) have studied the unsupervised acquisition of syntactic structure, while linguists such as Boersma and Hayes (2001), Goldsmith (2001), Pater (2008) and Albright and Hayes (2003) are developing probabilistic models of the acquisition of phonology and/or morphology, and Frank et al. (2007) experimentally tests the predictions of a Bayesian model of lexical acquisition. Since I have more experience with computa"
W09-0103,N07-1051,0,\N,Missing
W09-0103,J01-2001,0,\N,Missing
W13-1810,P13-1091,1,0.791979,"y edge and then attaching the new subgraph. Rule r3 has an external vertex sequence of 1 to 5, and these are fused to the incident vertices of the nonterminal edge Nsay 1 → 5. The edge to be replaced in each step has been highlighted in red to ease reading. 3 Tree Decompositions We now introduce one additional piece of theoretical machinery, the tree decomposition 56 (Bodlaender, 1993). Tree decompositions play an important role in graph theory, feature prominently in the junction tree algorithm from machine learning (Pearl, 1988), and have proven valuable for efficient parsing (Gildea, 2011; Chiang et al., 2013). Importantly, Lautemann (1988) proved that every HRG parse identifies a particular tree decomposition, and by restricting ourselves to a certain type of tree we will draw an even tighter relationship, allowing us to identify parses given tree decompositions. A tree decomposition of a graph g is a tree whose nodes identify subsets of the vertices of g which satisfy the following three properties:1 • Vertex Cover: Every vertex of g is contained by at least one tree node. • Edge Cover: For every edge e of the graph, there is a tree node η such that each vertex of α(e) is in η. • Running Intersec"
W13-1810,W08-1301,0,0.021476,"Missing"
W13-1810,P99-1069,1,0.725561,"Missing"
W13-1810,C12-1083,1,0.721826,"syntactic subject, so could a graph grammar represent lake as a constituent in a parse of the corresponding semantic graph. In fact, picking a formalism that is so similar to the PCFG makes it easy to adapt proven, familiar techniques for training and inference such as the inside-outside algorithm, and because HRG is context-free, parses can be represented by trees, facilitating the use of many more tools from tree automata (Knight and Graehl, 2005). Furthermore, the operational parallelism with PCFG makes it easy to integrate graph-based systems with syntactic models in synchronous grammars (Jones et al., 2012). Probabilistic versions of deep syntactic models such as Lexical Functional Grammar and HPSG (Johnson et al., 1999; Riezler et al., 2000) are one grammar-based approach to Proceedings of the 11th International Conference on Finite State Methods and Natural Language Processing, pages 54–62, c St Andrews–Sctotland, July 15–17, 2013. 2013 Association for Computational Linguistics modeling graphs represented in the form of feature structures. However, these models are tied to a particular linguistic paradigm, and they are complex, requiring a great deal of effort to engineer and annotate the nece"
W13-1810,C12-1094,0,0.031056,"ibes a natural generalization of the n-gram to graphs, making use of Hyperedge Replacement Grammars to define generative models of graph languages. 1 Introduction While most work in natural language processing (NLP), and especially within statistical NLP, has historically focused on strings and trees, there is increasing interest in deeper graph-based analyses which could facilitate natural language understanding and generation applications. Graphs have a long tradition within knowledge representation (Sowa, 1976), natural language semantics (Titov et al., 2009; Martin and White, 2011; Le and Zuidema, 2012), and in models of deep syntax (Oepen et al., 2004; de Marneffe and Manning, 2008). Graphs seem particularly appropriate for representing semantic structures, since a single concept could play multiple roles within a sentence. For instance, in the semantic representation at the bottom right of Figure 1 lake is an argument of both rich-in and own in the sentence, “The lake is said to be rich in fish but is privately owned.” However, work 54 Mark Johnson† mark.johnson@mq.edu.au † Department of Computing Macquarie University Sydney, Australia on graphs has been hampered, due, in part, to the abse"
W13-1810,W11-1609,0,0.0147533,"eneral graphs. This paper describes a natural generalization of the n-gram to graphs, making use of Hyperedge Replacement Grammars to define generative models of graph languages. 1 Introduction While most work in natural language processing (NLP), and especially within statistical NLP, has historically focused on strings and trees, there is increasing interest in deeper graph-based analyses which could facilitate natural language understanding and generation applications. Graphs have a long tradition within knowledge representation (Sowa, 1976), natural language semantics (Titov et al., 2009; Martin and White, 2011; Le and Zuidema, 2012), and in models of deep syntax (Oepen et al., 2004; de Marneffe and Manning, 2008). Graphs seem particularly appropriate for representing semantic structures, since a single concept could play multiple roles within a sentence. For instance, in the semantic representation at the bottom right of Figure 1 lake is an argument of both rich-in and own in the sentence, “The lake is said to be rich in fish but is privately owned.” However, work 54 Mark Johnson† mark.johnson@mq.edu.au † Department of Computing Macquarie University Sydney, Australia on graphs has been hampered, du"
W13-1810,P00-1061,1,0.697607,"king a formalism that is so similar to the PCFG makes it easy to adapt proven, familiar techniques for training and inference such as the inside-outside algorithm, and because HRG is context-free, parses can be represented by trees, facilitating the use of many more tools from tree automata (Knight and Graehl, 2005). Furthermore, the operational parallelism with PCFG makes it easy to integrate graph-based systems with syntactic models in synchronous grammars (Jones et al., 2012). Probabilistic versions of deep syntactic models such as Lexical Functional Grammar and HPSG (Johnson et al., 1999; Riezler et al., 2000) are one grammar-based approach to Proceedings of the 11th International Conference on Finite State Methods and Natural Language Processing, pages 54–62, c St Andrews–Sctotland, July 15–17, 2013. 2013 Association for Computational Linguistics modeling graphs represented in the form of feature structures. However, these models are tied to a particular linguistic paradigm, and they are complex, requiring a great deal of effort to engineer and annotate the necessary grammars and corpora. It is also not obvious how to define generative probabilistic models with such grammars, limiting their utilit"
W13-1810,J11-1008,0,\N,Missing
W13-2601,C10-1060,1,0.842537,"Missing"
W13-2601,N09-1036,1,0.810417,"which in turn results in more segmentation errors. In this section, we explore the possibility of mitigating some of these negative consequences. In section 3, we saw that when the Unigram model tries to learn Japanese words, it produces an output lexicon composed of both over- and undersegmented words in addition to words that result from a segmentation across word boundaries. One way to address this is by learning multiple kinds of units jointly, rather than just words; indeed, previous work has shown that richer models with multiple levels improve segmentation for English (Johnson, 2008a; Johnson and Goldwater, 2009). ADS — Eng. Jap. Eng. Jap. Syllable types 2.40 1.38 2.58 1.03 Token lengths 0.62 2.04 0.99 1.69 Table 4 : Entropies of syllable types and token lengths in terms of syllables (in bits) We suggest that this trade-off is responsible for the difference in the lexicon ambiguity across the two languages. Specifically, the combination of a small number of syllable types and, as a consequence, the tendency for multi-syllabic word types in Japanese makes it likely that a long word will be composed of smaller ones. This cannot happen very often in English, since most words are monosyllabic, and words s"
W13-2601,C12-1021,1,0.820566,"Missing"
W13-2601,W11-0601,1,0.859801,"Missing"
W13-2601,N07-1018,1,0.791,"ns with 40 speakers in American English. To make it comparable to the other corpora in this paper, we only used the idealized phonemic transcription. Finally, for Japanese ADS, we used the first 10,000 utterances of a phonemic transcription of the Corpus of Spontaneous Japanese (Maekawa et al., 2000). It consists of recorded spontaneous conversations, or public speeches in different fields ranging from engineering to humanities. For each corpus, we present elementary statistics in Table 2. Computational Framework and Corpora Adaptor Grammar In this study, we use the Adaptor Grammar framework (Johnson et al., 2007) to test different models of word segmentation on English and Japanese Corpora. This framework makes it possible to express a class of hierarchical non-parametric Bayesian models using an extension of probabilistic context-free grammars called Adaptor Grammar (AG). It allows one to easily define models that incorporate different assumptions about linguistic structure and is therefore a useful practical tool for exploring different hypotheses about word segmentation (Johnson, 2008b; Johnson, 2008a; Johnson et al., 2010; B¨orschinger et al., 2012). For mathematical details and a description of t"
W13-2601,W08-0704,1,0.942212,"min B¨orschinger2,3 Mark Johnson3 and Emmanuel Dupoux1 (1) Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris (2) Department of Computing, Macquarie University (3) Department of Computational Linguistics, Heidelberg University {abdellah.fourtassi, emmanuel.dupoux}@gmail.com , {benjamin.borschinger, mark.johnson}@mq.edu.au — Abstract novel utterances. State-of-the-art lexicon-building segmentation algorithms are typically reported to yield better performance than word boundary detection algorithms (Brent, 1999; Venkataraman, 2001; Batchelder, 2002; Goldwater, 2007; Johnson, 2008b; Fleck, 2008; Blanchard et al., 2010). As seen in Table 1, however, the performance varies considerably across languages with English winning by a high margin. This raises a generalizability issue for NLP applications, but also for the modeling of language acquisition since, obviously, it is not the case that in some languages, infants fail to acquire an adult lexicon. Are these performance differences only due to the fact that the algorithms might be optimized for English? Or do they also reflect some intrinsic linguistic differences between languages? Cross-linguistic studies on unsupervis"
W13-2601,P08-1046,1,0.900732,"min B¨orschinger2,3 Mark Johnson3 and Emmanuel Dupoux1 (1) Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris (2) Department of Computing, Macquarie University (3) Department of Computational Linguistics, Heidelberg University {abdellah.fourtassi, emmanuel.dupoux}@gmail.com , {benjamin.borschinger, mark.johnson}@mq.edu.au — Abstract novel utterances. State-of-the-art lexicon-building segmentation algorithms are typically reported to yield better performance than word boundary detection algorithms (Brent, 1999; Venkataraman, 2001; Batchelder, 2002; Goldwater, 2007; Johnson, 2008b; Fleck, 2008; Blanchard et al., 2010). As seen in Table 1, however, the performance varies considerably across languages with English winning by a high margin. This raises a generalizability issue for NLP applications, but also for the modeling of language acquisition since, obviously, it is not the case that in some languages, infants fail to acquire an adult lexicon. Are these performance differences only due to the fact that the algorithms might be optimized for English? Or do they also reflect some intrinsic linguistic differences between languages? Cross-linguistic studies on unsupervis"
W13-2601,P08-1016,0,0.0153653,"2,3 Mark Johnson3 and Emmanuel Dupoux1 (1) Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris (2) Department of Computing, Macquarie University (3) Department of Computational Linguistics, Heidelberg University {abdellah.fourtassi, emmanuel.dupoux}@gmail.com , {benjamin.borschinger, mark.johnson}@mq.edu.au — Abstract novel utterances. State-of-the-art lexicon-building segmentation algorithms are typically reported to yield better performance than word boundary detection algorithms (Brent, 1999; Venkataraman, 2001; Batchelder, 2002; Goldwater, 2007; Johnson, 2008b; Fleck, 2008; Blanchard et al., 2010). As seen in Table 1, however, the performance varies considerably across languages with English winning by a high margin. This raises a generalizability issue for NLP applications, but also for the modeling of language acquisition since, obviously, it is not the case that in some languages, infants fail to acquire an adult lexicon. Are these performance differences only due to the fact that the algorithms might be optimized for English? Or do they also reflect some intrinsic linguistic differences between languages? Cross-linguistic studies on unsupervised word segmen"
W13-2601,maekawa-etal-2000-spontaneous,0,0.0379472,"us speech to a single child collected from when the child was 2 up to when it was 3.5 years old. Both CDS corpora are available from the CHILDES database (MacWhinney, 2000). As for English ADS, we used the first 10,000 utterances of the Buckeye Speech Corpus (Pitt et al., 2007) which consists in spontaneous conversations with 40 speakers in American English. To make it comparable to the other corpora in this paper, we only used the idealized phonemic transcription. Finally, for Japanese ADS, we used the first 10,000 utterances of a phonemic transcription of the Corpus of Spontaneous Japanese (Maekawa et al., 2000). It consists of recorded spontaneous conversations, or public speeches in different fields ranging from engineering to humanities. For each corpus, we present elementary statistics in Table 2. Computational Framework and Corpora Adaptor Grammar In this study, we use the Adaptor Grammar framework (Johnson et al., 2007) to test different models of word segmentation on English and Japanese Corpora. This framework makes it possible to express a class of hierarchical non-parametric Bayesian models using an extension of probabilistic context-free grammars called Adaptor Grammar (AG). It allows one"
W13-2601,J01-3002,0,0.209255,"hyisenglishsoeasytosegment? Abdellah Fourtassi1 , Benjamin B¨orschinger2,3 Mark Johnson3 and Emmanuel Dupoux1 (1) Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris (2) Department of Computing, Macquarie University (3) Department of Computational Linguistics, Heidelberg University {abdellah.fourtassi, emmanuel.dupoux}@gmail.com , {benjamin.borschinger, mark.johnson}@mq.edu.au — Abstract novel utterances. State-of-the-art lexicon-building segmentation algorithms are typically reported to yield better performance than word boundary detection algorithms (Brent, 1999; Venkataraman, 2001; Batchelder, 2002; Goldwater, 2007; Johnson, 2008b; Fleck, 2008; Blanchard et al., 2010). As seen in Table 1, however, the performance varies considerably across languages with English winning by a high margin. This raises a generalizability issue for NLP applications, but also for the modeling of language acquisition since, obviously, it is not the case that in some languages, infants fail to acquire an adult lexicon. Are these performance differences only due to the fact that the algorithms might be optimized for English? Or do they also reflect some intrinsic linguistic differences between"
W13-2601,W04-1313,0,0.0257503,"word boundary cues, like prosody (Jusczyk et al., 1999; Mattys et al., 1999), transition probabilities (Saffran et al., 1996; Pelucchi et al., 2009), phonotactics (Mattys et al., 2001), coarticulation (Johnson and Jusczyk, 2001) and combine these cues with different weights (Weiss et al., 2010). Computational models of word segmentation have played a major role in assessing the relevance and reliability of different statistical cues present in the speech input. Some of these models focus mainly on boundary detection, and assess different strategies to identify them (Christiansen et al., 1998; Xanthos, 2004; Swingley, 2005; Daland and Pierrehumbert, 2011). Other models, sometimes called lexicon-building algorithms, learn the lexicon and the segmentation at the same time and use knowledge about the extracted lexicon to segment F-score Model Reference 0.89 AG Johnson (2009) 0.77 AG Johnson (2010) 0.58 DP Bigram Fleck (2008) 0.56 WordEnds Fleck (2008) 0.55 AG Johnson (2008) 0.55 BootLex Batchelder (2002) 0.54 NGS-u Boruta (2011) Table 1: State-of-the-art unsupervised segmentation scores for eight languages. The aim of the present work is to understand why English usually scores better than other la"
W13-3011,D10-1028,0,0.0360237,"Missing"
W13-3011,C10-1060,1,0.907494,"Missing"
W13-3011,P12-1093,1,0.841816,"Missing"
W13-3011,P08-1046,1,0.860602,"Missing"
W13-3518,C12-1078,0,0.0416419,"Missing"
W13-3518,N07-1049,0,0.0168156,"The accuracy improvement on Hungarian and Arabic did not meet our significance threshold. The nonmonotonic transitions did not decrease accuracy significantly on any of the languages. 9 Related Work One can view our non-monotonic parsing system as adding “repair” operations to a greedy, deterministic parser, allowing it to undo previous decisions and thus mitigating the effect of incorrect parsing decisions due to uncertain future, which is inherent in greedy left-to-right transition-based parsers. Several approaches have been taken to address this problem, including: Post-processing Repairs (Attardi and Ciaramita, 2007; Hall and Nov´ak, 2005; Inokuchi and Yamaoka, 2012) Closely related to stacking, this line of work attempts to train classifiers to repair attachment mistakes after a parse is proposed by a parser by changing head attachment decisions. The present work differs from these by incorporating the repair process into the transition system. Stacking (Nivre and McDonald, 2008; Martins et al., 2008), in which a second-stage parser runs over the sentence using the predictions of the first parser as features. In contrast our parser works in 169 System Baseline NM L+D AR 83.4 83.6 BASQ 76.2 76.1 C AT 91."
W13-3518,P10-1001,0,0.0210397,"532 FN 285 250 535 Table 2: True/False positive/negative rates for the prediction of the non-monotonic transitions. The non-monotonic transitions add correct dependencies 112 times, and produce worse parses 40 times. 535 opportunities for non-monotonic transitions were missed. System K&C 10 Z&N 11 G&N 12 Baseline(G&N-12) NM L+D O n3 nk n n n Stanford Penn2Malt LAS UAS LAS UAS — 91.9 88.72 88.7 88.9 — 93.5 90.96 90.9 91.1 — 91.8 — 88.7 88.9 93.00 92.9 — 90.6 91.0 Table 3: WSJ 23 test results, with comparison against the state-of-the-art systems from the literature of different runtimes. K&C 10=Koo and Collins (2010); Z&N 11=Zhang and Nivre (2011); G&N 12=Goldberg and Nivre (2012). achieved with a purely greedy system, with a statistically significant improvement over G&N 12. CoNLL 2007 evaluation. Table 4 shows the effect of the non-monotonic transitions across the ten languages in the CoNLL 2007 data sets. Statistically significant improvements in accuracy were observed for five of the ten languages. The accuracy improvement on Hungarian and Arabic did not meet our significance threshold. The nonmonotonic transitions did not decrease accuracy significantly on any of the languages. 9 Related Work One can"
W13-3518,P11-1068,0,0.178617,"Missing"
W13-3518,J13-1002,0,0.0405808,"nd the buffer hold the words of a sentence, and the set of arcs represent derived dependency relations. We use a notation in which the stack items are indicated by Si , with S0 being the top of the stack, S1 the item previous to it and so on. Similarly, buffer items are indicated as Bi , with B0 being the first item on the buffer. The arcs are of the form (h, l, m), indicating a dependency in which the word m modifies the word h with label l. In the initial configuration the stack is empty, and the buffer contains the words of the sentence followed by an artificial ROOT token, as suggested by Ballesteros and Nivre (2013). In the final configuration the buffer is empty and the stack contains the ROOT token. There are four parsing actions (Shift, Left-Arc, Right-Arc and Reduce, abbreviated as S,L,R,D respectively) that manipulate stack and buffer items. The Shift action pops the first item from the buffer and pushes it on the stack (the Shift action has a natural precondition that the buffer is not empty, as well as a precondition that ROOT can only be pushed to an empty stack). The Right-Arc action is similar to the Shift action, but it also adds a dependency arc (S0 , B0 ), with the current top of the stack a"
W13-3518,de-marneffe-etal-2006-generating,0,0.0136933,"Missing"
W13-3518,J93-2004,0,0.045362,"additive. and Nivre (2011). We follow Goldberg and Nivre (2012) in training all models for 15 iterations, and shuffling the sentences before each iteration. Because the sentence ordering affects the model’s accuracy, all results are averaged from scores produced using 20 different random seeds. The seed determines how the sentences are shuffled before each iteration, as well as when to follow an optimal action and when to follow a nonoptimal action during training. The Wilcoxon signed-rank test was used for significance testing. A train/dev/test split of 02-21/22/23 of the Penn Treebank WSJ (Marcus et al., 1993) was used for all models. The data was converted into Stanford dependencies (de Marneffe et al., 2006) with copula-as-head and the original PTB noun-phrase bracketing. We also evaluate our models on dependencies created by the P ENN 2MALT tool, to assist comparison with previous results. Automatically assigned POS tags were used during training, to match the test data more closely. 6 We also evaluate the non-monotonic transitions on the CoNLL 2007 multi-lingual data. 8 Results and analysis We base our experiments on the parser described by Goldberg and Nivre (2012). We began by implementing th"
W13-3518,D08-1017,0,0.0496482,"Missing"
W13-3518,N10-1115,1,0.466464,"e NM L+D AR 83.4 83.6 BASQ 76.2 76.1 C AT 91.5 91.5 C HI 82.3 82.7 CZ 78.8 80.1 E NG 87.9 88.4 GR 81.2 81.8 H UN 77.6 77.9 I TA 83.8 84.1 T UR 78.0 78.0 Table 4: Multi-lingual evaluation. Accuracy improved on Chinese, Czech, English, Greek and Italian (p < 0.001), trended upward on Arabic and Hungarian (p < 0.005), and was unchanged on Basque, Catalan and Turkish (p &gt; 0.4). As we explained in the paper, with the greedy decoding used here additional spurious ambiguity is not necessarily a draw-back. a single, left-to-right pass over the sentence. Non-directional Parsing The EasyFirst parser of Goldberg and Elhadad (2010) tackles similar forms of ambiguities by dropping the Shift action altogether, and processing the sentence in an easyto-hard bottom-up order instead of left-to-right, resulting in a greedy but non-directional parser. The indeterminate processing order increases the parser’s runtime from O(n) to O(n log n). In contrast, our parser processes the sentence incrementally, and runs in a linear time. Beam Search An obvious approach to tackling ambiguities is to forgo the greedy nature of the parser and instead to adopt a beam search (Zhang and Clark, 2008; Zhang and Nivre, 2011) or a dynamic programm"
W13-3518,W03-3017,0,0.644818,"Missing"
W13-3518,C12-1059,1,0.35868,"previously predicted label. This would allow the parser to con165 5 To summarize, our Non-Monotnonic ArcEager system differs from the monotonic Arc-Eager system by: An essential component when training a transition-based parser is an oracle which, given a gold-standard tree, dictates the sequence of moves a parser should make in order to derive it. Traditionally, these oracles are defined as functions from trees to sequences, mapping a gold tree to a single sequence of actions deriving it, even if more than one sequence of actions derives the gold tree. We call such oracles static. Recently, Goldberg and Nivre (2012) introduced the concept of a dynamic oracle, and presented a concrete oracle for the arc-eager system. Instead of mapping a gold tree to a sequence of actions, the dynamic oracle maps a hconfiguration, gold treei pair to a set of optimal transitions. More concretely, the dynamic oracle presented in Goldberg and Nivre (2012) maps haction, configuration, treei tuples to an integer, indicating the number of gold arcs in tree that can be derived from conf iguration by some sequence of actions, but could not be derived after applying action to the configuration. There are two advantages to this. Fi"
W13-3518,J08-4003,0,0.0955009,"ng the parser to recover from the incorrect head assignments which are forced by an incorrect resolution of a Shift/Right-Arc ambiguity. In transition-based parsing, a parser consists of a state (or a configuration) which is manipulated by a set of actions. An action is applied to a state and results in a new state. The parsing process concludes when the parser reaches a final state, at which the parse tree is read from the state. A particular set of states and actions yield a transitionsystem. Our starting point in this paper is the popular Arc-Eager transition system, described in detail by Nivre (2008). The state of the arc-eager system is composed of a stack, a buffer and a set of arcs. The stack and the buffer hold the words of a sentence, and the set of arcs represent derived dependency relations. We use a notation in which the stack items are indicated by Si , with S0 being the top of the stack, S1 the item previous to it and so on. Similarly, buffer items are indicated as Bi , with B0 being the first item on the buffer. The arcs are of the form (h, l, m), indicating a dependency in which the word m modifies the word h with label l. In the initial configuration the stack is empty, and t"
W13-3518,W04-2407,0,0.0194734,", with the current top of the stack as the head of the newly pushed item (the Right action has an additional precondition that the stack is not empty).1 The Left-Arc action adds a dependency arc (B0 , S0 ) with the first item in the buffer as the head of the top of the stack, and pops the stack (with a precondition that the stack and buffer are not empty, and that S0 is not assigned a head yet). Finally, the Reduce action pops the stack, with a precondition that the stack is not empty and that S0 is already assigned a head. 3 The Non-Monotonic Arc-Eager System The Arc-Eager transition system (Nivre et al., 2004) has four moves. Two of them create dependencies, two push a word from the buffer to the stack, and two remove an item from the stack: Push Pop Adds dependency Right-Arc Left-Arc No new dependency Shift Reduce Every word in the sentence is pushed once and popped once; and every word must have exactly one head. This creates two pairings, along the diagonals: (S, L) and (R, D). Either the push move adds the head or the pop move does, but not both and not neither. 1 For labelled dependency parsing, the Right-Arc and Left-Arc actions are parameterized by a label L such that the action RightL adds"
W13-3518,P08-1108,0,0.00787616,"ffect of incorrect parsing decisions due to uncertain future, which is inherent in greedy left-to-right transition-based parsers. Several approaches have been taken to address this problem, including: Post-processing Repairs (Attardi and Ciaramita, 2007; Hall and Nov´ak, 2005; Inokuchi and Yamaoka, 2012) Closely related to stacking, this line of work attempts to train classifiers to repair attachment mistakes after a parse is proposed by a parser by changing head attachment decisions. The present work differs from these by incorporating the repair process into the transition system. Stacking (Nivre and McDonald, 2008; Martins et al., 2008), in which a second-stage parser runs over the sentence using the predictions of the first parser as features. In contrast our parser works in 169 System Baseline NM L+D AR 83.4 83.6 BASQ 76.2 76.1 C AT 91.5 91.5 C HI 82.3 82.7 CZ 78.8 80.1 E NG 87.9 88.4 GR 81.2 81.8 H UN 77.6 77.9 I TA 83.8 84.1 T UR 78.0 78.0 Table 4: Multi-lingual evaluation. Accuracy improved on Chinese, Czech, English, Greek and Italian (p < 0.001), trended upward on Arabic and Hungarian (p < 0.005), and was unchanged on Basque, Catalan and Turkish (p &gt; 0.4). As we explained in the paper, with the"
W13-3518,D08-1059,0,0.0194823,"ctional Parsing The EasyFirst parser of Goldberg and Elhadad (2010) tackles similar forms of ambiguities by dropping the Shift action altogether, and processing the sentence in an easyto-hard bottom-up order instead of left-to-right, resulting in a greedy but non-directional parser. The indeterminate processing order increases the parser’s runtime from O(n) to O(n log n). In contrast, our parser processes the sentence incrementally, and runs in a linear time. Beam Search An obvious approach to tackling ambiguities is to forgo the greedy nature of the parser and instead to adopt a beam search (Zhang and Clark, 2008; Zhang and Nivre, 2011) or a dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011) approach. While these approaches are very successful in producing highaccuracy parsers, we here explore what can be achieved in a strictly deterministic system, which results in much faster and incremental parsing algorithms. The use of non-monotonic transitions in beam-search parser is an interesting topic for future work. 10 The conventional training procedure for transition-based parsers uses a “static” oracle based on “gold” parses that never predicts a non-monotonic transition, so it is clearl"
W13-3518,P11-2033,0,0.203041,"itions on labelled and unlabelled attachment score on the development data. All results are averages from 20 models trained with different random seeds, as the ordering of the sentences at each iteration of the Perceptron algorithm has an effect on the system’s accuracy. The two non-monotonic transitions each bring small but statistically significant improvements that are additive when combined in the NM L+D system. The result is stable 5 If using a labeled reduce transition, the label assignment costs should be handled here. 6 We thank Yue Zhang for supplying the POS-tagged files used in the Zhang and Nivre (2011) experiments. 7 Experiments 168 across both dependency encoding schemes. Frequency analysis. Recall that there are two pop moves available: Left-Arc and Reduce. The LeftArc is considered non-monotonic if the top of the stack has a head specified, and the Reduce move is considered non-monotonic if it does not. How often does the parser select monotonic and nonmonotonic pop moves, and how often is its decision correct? In Table 2, the True Positive column shows how often non-monotonic transitions were used to add gold standard dependencies. The False Positive column shows how often they were use"
W13-3518,W05-1505,0,\N,Missing
W13-3518,P10-1110,0,\N,Missing
W17-6810,D15-1198,0,0.216455,"ning Representations (AMRs), graphs which represent the predicate-argument structure of the sentences. Such work builds upon the AMRBank (Banarescu et al., 2013), a corpus in which each sentence has been manually annotated with an AMR. The training instances in the AMRBank are annotated only with the AMRs themselves, not with the structure of a compositional derivation of the AMR. This poses a challenge for semantic parsing, especially for approaches which induce a grammar from the data and thus must make this compositional structure explicit in order to learn rules (Jones et al., 2012, 2013; Artzi et al., 2015; Peng et al., 2015). In general, the number of ways in which an AMR graph can be built from its atomic parts, e.g. using the generic graph-combining operations of the HR algebra (Courcelle, 1993), is huge (Groschwitz et al., 2015). This makes grammar induction computationally expensive and undermines its ability to discover grammatical structures that can be shared across multiple instances. Existing approaches therefore resort to heuristics that constrain the space of possible analyses, often with limited regard to the linguistic reality of these heuristics. We propose a novel method to gene"
W17-6810,P13-1091,0,0.0319973,"3a. This set of terms is riddled with spurious ambiguity and linguistically bizarre analyses, such as the term shown in Fig. 3b. Two strange aspects of this example are: one, prince becomes an X-source by first switching X and rt and then switching them back; this step is unnecessary and inconsistent with the corresponding process for rose here. Two, prince and rose are combined with empty argument connectors before love finally is inserted as the predicate, despite these roles being originally defined in love’s semantic frame. Not only does this make graph parsing computationally expensive (Chiang et al., 2013; Groschwitz et al., 2015), it also makes grammar induction difficult. For example, Bayesian algorithms sample random terms from the AMRBank and attempt to discover grammatical structures that are shared across different training instances. When the number of possible terms is huge, the chance that no two rules share any grammatical structure increases, undermining the grammar induction process. Existing systems therefore apply heuristics to constrain the space of allowable HR terms. However, these heuristics are typically ad-hoc, and not motivated on linguistic grounds. Thus there is a risk t"
W17-6810,P01-1019,0,0.223306,"sly connected to research that models the compositional mapping from strings to AMRs with grammars – using either synchronous grammars (Jones et al., 2012; Peng et al., 2015) or CCG (Artzi et al., 2015; Misra and Artzi, 2016). Not all AMR parsers learn explicit grammars (Flanigan et al., 2014; Peng et al., 2017). However, we believe that these, too, may benefit from access to the compositional structure of the AMRs, which the algebra we present makes easier to compute. The operations our algebra uses to combine semantic representations are closely related to those of the “semantic algebra” of Copestake et al. (2001), which was intended to reflect universal semantic combination operations for large-scale handwritten HPSG grammars. More distantly, the ability of our semantic representations to select the type of its arguments echoes the use of types in Montague Grammar and in CCG (Steedman, 2001), applied to graphs. 3 Algebras for constructing graphs We start by reviewing the HR algebra and discussing some of its shortcomings in the context of grammar induction. Then we introduce the apply-modify graph algebra, which tackles these shortcomings in a linguistically adequate way. Notation: For a given (partia"
W17-6810,P14-1134,0,0.092468,"For example, Figure 1: AMR of The destruction of Rome and Rome was destroyed have the same AMR; among snake swallows its prey without chewing. others, tense and determiners are dropped. The availability of the AMRBank has spawned much research on semantic parsing into AMR representations. The work in this paper is most obviously connected to research that models the compositional mapping from strings to AMRs with grammars – using either synchronous grammars (Jones et al., 2012; Peng et al., 2015) or CCG (Artzi et al., 2015; Misra and Artzi, 2016). Not all AMR parsers learn explicit grammars (Flanigan et al., 2014; Peng et al., 2017). However, we believe that these, too, may benefit from access to the compositional structure of the AMRs, which the algebra we present makes easier to compute. The operations our algebra uses to combine semantic representations are closely related to those of the “semantic algebra” of Copestake et al. (2001), which was intended to reflect universal semantic combination operations for large-scale handwritten HPSG grammars. More distantly, the ability of our semantic representations to select the type of its arguments echoes the use of types in Montague Grammar and in CCG (S"
W17-6810,P16-1192,1,0.844109,"ise quantitative analysis, e.g. in the context of grammar induction, for future work. Runtime. We finish by measuring the mean runtimes to compute the decomposition automata (Fig. 13c). Once again, we find that the AM algebra solidly outperforms the HR algebra. The runtimes of HR-S3 are too slow to be useful in practice, whereas even the highest-coverage algebra AM-C2 decomposes even large graphs in seconds. Moreover, the runtimes for AM-C1 are faster than even for the very low-coverage HR-S2 algebra. The previously fastest parser for graphs using hyperedge replacement grammars was the one of Groschwitz et al. (2016), which used Interpreted Regular Tree Grammars (IRTGs) (Koller and Kuhlmann, 2011) together with the HR algebra. Because we have seen how to compute decomposition automata for the AM algebra in Section 5, we can do graph parsing with IRTGs over the AM algebra instead. The fact that decomposition automata for the AM algebra are smaller and faster to compute promises a further speed-up for graph parsing as well, making wide-coverage graph parsing for large graphs feasible. 7 Conclusion In this paper, we have introduced the apply-modify (AM) algebra for graphs. The AM algebra replaces the general"
W17-6810,P15-1143,1,0.718877,"th an AMR. The training instances in the AMRBank are annotated only with the AMRs themselves, not with the structure of a compositional derivation of the AMR. This poses a challenge for semantic parsing, especially for approaches which induce a grammar from the data and thus must make this compositional structure explicit in order to learn rules (Jones et al., 2012, 2013; Artzi et al., 2015; Peng et al., 2015). In general, the number of ways in which an AMR graph can be built from its atomic parts, e.g. using the generic graph-combining operations of the HR algebra (Courcelle, 1993), is huge (Groschwitz et al., 2015). This makes grammar induction computationally expensive and undermines its ability to discover grammatical structures that can be shared across multiple instances. Existing approaches therefore resort to heuristics that constrain the space of possible analyses, often with limited regard to the linguistic reality of these heuristics. We propose a novel method to generate a constrained set of derivations directly from an AMR, but without losing linguistically significant phenomena and parses. To this end we present an apply-modify (AM) graph algebra for combining graphs using operations that re"
W17-6810,C12-1083,0,0.208863,"sentences to Abstract Meaning Representations (AMRs), graphs which represent the predicate-argument structure of the sentences. Such work builds upon the AMRBank (Banarescu et al., 2013), a corpus in which each sentence has been manually annotated with an AMR. The training instances in the AMRBank are annotated only with the AMRs themselves, not with the structure of a compositional derivation of the AMR. This poses a challenge for semantic parsing, especially for approaches which induce a grammar from the data and thus must make this compositional structure explicit in order to learn rules (Jones et al., 2012, 2013; Artzi et al., 2015; Peng et al., 2015). In general, the number of ways in which an AMR graph can be built from its atomic parts, e.g. using the generic graph-combining operations of the HR algebra (Courcelle, 1993), is huge (Groschwitz et al., 2015). This makes grammar induction computationally expensive and undermines its ability to discover grammatical structures that can be shared across multiple instances. Existing approaches therefore resort to heuristics that constrain the space of possible analyses, often with limited regard to the linguistic reality of these heuristics. We prop"
W17-6810,W13-1810,1,0.90823,"Missing"
W17-6810,W15-0127,1,0.946888,"nction between F and f as in the definition above. But as a general principle in this paper, this common notation always refers to the function in text and definitions, and to the symbol in terms such as in Figures 2a and 2b. 3.1 S-graphs and the HR algebra love rt A standard algebra for the theoretical literature for describing graphs ARG0 ARG1 is the HR algebra of Courcelle (1993). It is very closely related to prince rose hyperedge replacement grammars (Drewes et al., 1997), which have been used extensively for grammars of AMR languages (Chiang et al., (a) AMR 2013; Peng et al., 2015), and Koller (2015) showed explicitly how to do compositional semantic construction using the HR algebra. fX The objects of the HR algebra are s-graphs G = (g, S), consisting of a graph g (here, directed and with node and edge labels) and a par|| tial function S : S V , which maps sources from a fixed finite set S of source names to nodes of g. Sources thus serve as external, inter|| Nlove pretable names for some of the nodes. If we have S(a) = v, then we call v an a-source of G. An example of an s-graph with a root-source || (rt) and a subject-source (S) is shown in Fig. 2f. Sources are marked in renrt7→X diagr"
W17-6810,W11-2902,1,0.922807,"never forgotten, and each can therefore be used only on one node in the derivation. Two COREF sources with the same index will be automatically merged together during the usual A PP and MOD operations, due to the semantics of the underlying merge operation of the HR algebra. COREF sources increase runtimes and the number of possible terms per graph significantly (see Section 6), and thus we limit the number of COREF sources to zero to two in practice. 5.3 Obtaining the set of terms We can compactly represent the set of all AM terms that evaluate to a given AMR G in a decomposition automaton (Koller and Kuhlmann, 2011), a chart-like data structure in which shared subterms are represented only once. We can enumerate the terms from this automaton. To enumerate all rules of the decomposition automaton, we explore it bottom-up, with Algorithm 1. We first find all constants for G in Line 2, as described in Section 5.1, and then repeatedly apply A PP and MOD operations (Lines 3 onward; the set O contains all relevant A PP and MOD operations). The constants and the successful operation applications are stored as rules in the automaton. To ensure that the resulting terms evaluate to the input graph G, we use subgra"
W17-6810,D16-1183,0,0.0305352,"because they gloss over certain details of the syntactic realization. For example, Figure 1: AMR of The destruction of Rome and Rome was destroyed have the same AMR; among snake swallows its prey without chewing. others, tense and determiners are dropped. The availability of the AMRBank has spawned much research on semantic parsing into AMR representations. The work in this paper is most obviously connected to research that models the compositional mapping from strings to AMRs with grammars – using either synchronous grammars (Jones et al., 2012; Peng et al., 2015) or CCG (Artzi et al., 2015; Misra and Artzi, 2016). Not all AMR parsers learn explicit grammars (Flanigan et al., 2014; Peng et al., 2017). However, we believe that these, too, may benefit from access to the compositional structure of the AMRs, which the algebra we present makes easier to compute. The operations our algebra uses to combine semantic representations are closely related to those of the “semantic algebra” of Copestake et al. (2001), which was intended to reflect universal semantic combination operations for large-scale handwritten HPSG grammars. More distantly, the ability of our semantic representations to select the type of its"
W17-6810,K15-1004,0,0.57415,"(AMRs), graphs which represent the predicate-argument structure of the sentences. Such work builds upon the AMRBank (Banarescu et al., 2013), a corpus in which each sentence has been manually annotated with an AMR. The training instances in the AMRBank are annotated only with the AMRs themselves, not with the structure of a compositional derivation of the AMR. This poses a challenge for semantic parsing, especially for approaches which induce a grammar from the data and thus must make this compositional structure explicit in order to learn rules (Jones et al., 2012, 2013; Artzi et al., 2015; Peng et al., 2015). In general, the number of ways in which an AMR graph can be built from its atomic parts, e.g. using the generic graph-combining operations of the HR algebra (Courcelle, 1993), is huge (Groschwitz et al., 2015). This makes grammar induction computationally expensive and undermines its ability to discover grammatical structures that can be shared across multiple instances. Existing approaches therefore resort to heuristics that constrain the space of possible analyses, often with limited regard to the linguistic reality of these heuristics. We propose a novel method to generate a constrained s"
W17-6810,E17-1035,0,0.0512184,"AMR of The destruction of Rome and Rome was destroyed have the same AMR; among snake swallows its prey without chewing. others, tense and determiners are dropped. The availability of the AMRBank has spawned much research on semantic parsing into AMR representations. The work in this paper is most obviously connected to research that models the compositional mapping from strings to AMRs with grammars – using either synchronous grammars (Jones et al., 2012; Peng et al., 2015) or CCG (Artzi et al., 2015; Misra and Artzi, 2016). Not all AMR parsers learn explicit grammars (Flanigan et al., 2014; Peng et al., 2017). However, we believe that these, too, may benefit from access to the compositional structure of the AMRs, which the algebra we present makes easier to compute. The operations our algebra uses to combine semantic representations are closely related to those of the “semantic algebra” of Copestake et al. (2001), which was intended to reflect universal semantic combination operations for large-scale handwritten HPSG grammars. More distantly, the ability of our semantic representations to select the type of its arguments echoes the use of types in Montague Grammar and in CCG (Steedman, 2001), appl"
W17-6810,W13-2322,0,\N,Missing
W98-1115,H90-1053,0,0.0422779,"have found that exhaustively parsing maximum-40-word sentences from the Penn II treebank requires an average of about 1.2 million edges per sentence. Numbers like this suggest that any approach that offers the possibility of reducing the work load is well worth pursuing, a fact that has been noted by several researchers. Early on, Kay (1980) suggested the use of the chart agenda for this purpose. More recently, the statistical approach to language processing and the use of probabilistic context-free grammars (PCFGs) has suggested using the PCFG probabilities to create a FOM. Bobrow (1990) and Chitrao and Grishman (1990) introduced best-first PCFG parsing, the approach taken here. Subsequent work has suggested different FOMs built from PCFG probabilities (Miller and Fox. 1994: Kochman and Kupin. 1991: Magerman and Marcus, 1991). Probably the most extensive comparison of possible metrics for best-first PCFG parsing is that of Caraballo and Charniak (henceforth C&C) (Forthcoming). They consider a large number of FOMs, and view them as approximations of some ""ideal"" (but only computable after the fact) FOM. Of these they recommend one as the best of the lot. In this paper we basically adopt both their framework"
W98-1115,P97-1003,0,0.104889,"y more plausible, smaller ones. 6 Conclusion It is worth noting that while we have presented the use of edge-based best-first chart parsing in the service of a rather pure form of P C F G parsing, there is no particular reason to assume that the technique is so limited in its domain of applicability. One can imagine the same techniques coupled with more informative probability distributions, such as lexicalized P C F G s (Charniak, 1997), or even grammars not based upon literal rules, b u t probability distributions that describe how rules are built up from smaller components (Magerman, 1995; Collins, 1997). Clearly further research is warranted. Be this as it may, the take-home lesson from this paper is simple: combining an edge-based agenda with the figure of merit from C & C • is easy to do by simply binarizing the grammar • provides a factor of 20 or so reduction in the number of edges required to find a first parse, and • improves parsing precision and recall over exhaustive parsing. To the best of our knowledge this is currently the most effecient parsing technique for P C F G grammars induced from large tree-banks. As such we strongly recommend this technique to others interested in PCFG"
W98-1115,P96-1024,0,0.142388,"Missing"
W98-1115,W97-0302,0,0.0885422,"1994: Kochman and Kupin. 1991: Magerman and Marcus, 1991). Probably the most extensive comparison of possible metrics for best-first PCFG parsing is that of Caraballo and Charniak (henceforth C&C) (Forthcoming). They consider a large number of FOMs, and view them as approximations of some ""ideal"" (but only computable after the fact) FOM. Of these they recommend one as the best of the lot. In this paper we basically adopt both their framework and their recommended FOM. The next section describes their work in more detail, Besides C&C the work that is most directly comparable to ours is that of Goodman (1997) and Ratnaparki (1997). G o o d m a n uses an FOM that is similar to that of C&C but one that should, in general, be somewhat more accurate. However, both Goodman's and Ratnaparki's work assumes that one is doing a beam search of some sort, rather than a best-first search, and their FOM are unfortunately tied to their frameworks and thus cannot be adopted here. We briefly compare our results to theirs in Section 5. As noted, our paper takes off from that of C&C and uses the same FOM. The major difference is simply that our parser uses the FOM to rank edges (including incomplete edges), rather"
W98-1115,H91-1045,0,0.196176,"Missing"
W98-1115,H91-1044,0,0.0428787,"ssibility of reducing the work load is well worth pursuing, a fact that has been noted by several researchers. Early on, Kay (1980) suggested the use of the chart agenda for this purpose. More recently, the statistical approach to language processing and the use of probabilistic context-free grammars (PCFGs) has suggested using the PCFG probabilities to create a FOM. Bobrow (1990) and Chitrao and Grishman (1990) introduced best-first PCFG parsing, the approach taken here. Subsequent work has suggested different FOMs built from PCFG probabilities (Miller and Fox. 1994: Kochman and Kupin. 1991: Magerman and Marcus, 1991). Probably the most extensive comparison of possible metrics for best-first PCFG parsing is that of Caraballo and Charniak (henceforth C&C) (Forthcoming). They consider a large number of FOMs, and view them as approximations of some ""ideal"" (but only computable after the fact) FOM. Of these they recommend one as the best of the lot. In this paper we basically adopt both their framework and their recommended FOM. The next section describes their work in more detail, Besides C&C the work that is most directly comparable to ours is that of Goodman (1997) and Ratnaparki (1997). G o o d m a n uses"
W98-1115,P95-1037,0,0.0839334,"g other, strictly more plausible, smaller ones. 6 Conclusion It is worth noting that while we have presented the use of edge-based best-first chart parsing in the service of a rather pure form of P C F G parsing, there is no particular reason to assume that the technique is so limited in its domain of applicability. One can imagine the same techniques coupled with more informative probability distributions, such as lexicalized P C F G s (Charniak, 1997), or even grammars not based upon literal rules, b u t probability distributions that describe how rules are built up from smaller components (Magerman, 1995; Collins, 1997). Clearly further research is warranted. Be this as it may, the take-home lesson from this paper is simple: combining an edge-based agenda with the figure of merit from C & C • is easy to do by simply binarizing the grammar • provides a factor of 20 or so reduction in the number of edges required to find a first parse, and • improves parsing precision and recall over exhaustive parsing. To the best of our knowledge this is currently the most effecient parsing technique for P C F G grammars induced from large tree-banks. As such we strongly recommend this technique to others int"
W98-1115,J93-2004,0,0.0255481,"edge B ~ '7- to yield the edge A ~ a B . fl, corresponds to the left-factored productions ' a B ' ~ a B if fl is non-empty or A ~ ' a ' B if fl is empty. Thus in general a single 'new' non-terminal in a C K Y parse using the left-factored grammar abbreviates several incomplete edges in the Earley algorithm. 4 Figure h r/vs. P o p p e d Edges 4o0 3oo 2oo 1.0 1.5 ! 2.0 N o r m a l i z a t i o n constant Figure 2: r] vs. Precision and Recall 76 ~ 74 130 , ! . . . . lO0 The Experiment For our experiment, we used a tree-bank grammar induced from sections 2-21 of the Penn Wall Street Journal text (Marcus et al., 1993), with section 22 reserved for testing. All sentences of length greater than 40 were ignored for testing purposes as done in b o t h C&C and Goodman (1997). We applied the binarization technique described above to the grammar. We chose to measure the amount of work done by the parser in terms of the average number of edges popped off the agenda before finding a parse. This method has the advantage of being platform independent, as well as providing a measure of ""perfection"". Here, perfection is the minimum number of edges we would need to pop off the agenda in order to create the correct parse"
W98-1115,H94-1051,0,0.135544,"Missing"
W98-1115,W97-0301,0,0.025136,"upin. 1991: Magerman and Marcus, 1991). Probably the most extensive comparison of possible metrics for best-first PCFG parsing is that of Caraballo and Charniak (henceforth C&C) (Forthcoming). They consider a large number of FOMs, and view them as approximations of some ""ideal"" (but only computable after the fact) FOM. Of these they recommend one as the best of the lot. In this paper we basically adopt both their framework and their recommended FOM. The next section describes their work in more detail, Besides C&C the work that is most directly comparable to ours is that of Goodman (1997) and Ratnaparki (1997). G o o d m a n uses an FOM that is similar to that of C&C but one that should, in general, be somewhat more accurate. However, both Goodman's and Ratnaparki's work assumes that one is doing a beam search of some sort, rather than a best-first search, and their FOM are unfortunately tied to their frameworks and thus cannot be adopted here. We briefly compare our results to theirs in Section 5. As noted, our paper takes off from that of C&C and uses the same FOM. The major difference is simply that our parser uses the FOM to rank edges (including incomplete edges), rather than simply completed"
W98-1115,H91-1042,0,\N,Missing
W98-1206,P97-1003,0,0.0472718,"t Geman and our students at Brown. Speech (POS) tags. Such a P C F G induced from a sufficiently large corpus typically generates all possible P O S tag strings. A parsing system can b e obtained by using a parser to find the maximum likelihood parse tree for an input string. Such parsing systems often perform as well as other broad coverage parsing systems for predicting tree structure from POS tags (Charniak, 1996). In addition, many more sophisticated parsing models are elaborations of such P C F G models, so understanding the properties of P C F G s is likely to be useful (Charniak, 1997; Collins, 1997). It is well-known that natural language exhibits dependencies that Context Free Grammars (CFGs), and hence PCFGs, cannot describe (Shieber, 1985). But as explained below, the independence assumptions implicit in P C F G s introduce biases in the statistical model induced from a tree bank even in constructions which are adequately described by a CFG. T h e direction and size of these biases depend o n factors such as the following: • the precise tree structures used in the tree bank, and • whether the set of well-formed trees according to the linguistic model used to assign trees to strings ca"
