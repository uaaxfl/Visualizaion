2012.amta-papers.26,C00-1006,0,0.347165,"n-fr fWER (q, s) = 1 − min(1, WER(s, q)) NIST also produces values that are unbounded in the positive. The solution we propose is to divide the outcome by the largest possible obtainable value for the current query, i.e. the value that would be produced by an exact match: fNIST (q, s) = 5 5.1 EMEA JRC-Acquis Experiments Data Evaluation Methodology Little attention has so far been devoted to the evaluation of TM systems. Gow (2003) proposes a general methodology, but which is very much usercentered, and that focuses on complete CAT environments rather than specifically on the TM functionality; Baldwin and Tanaka (2000) and Whyman and Somers (1999) both propose methods that are based on recall and precision, but these have not been widely used or evaluated. Test words 28 817 28 365 26 715 30 471 28 054 27 426 16 514 19 260 Table 1: Experimental Data NIST(s, q) NIST(q, q) We performed experiments to assess the performance of each MT evaluation metric as TM similarity function. Experiments were done under different conditions, corresponding to different corpora and language pairs. While most of the metrics we use are language-agnostic, Meteor relies on languagespecific resources which are not available in all"
2012.amta-papers.26,P01-1004,0,0.184721,"obody really knows how they work. In particular, the details of the similarity function at the heart of TMs are well-kept commercial secrets. One possibility when experimenting with TM functionalities is to rely on one of the commercial implementations, and treat the TM as a “black box” component. But this is unwieldy because in most commercial systems, the TM functionality is only accessible through a complex GUI; furthermore, this raises issues with regard to reproducibility of experiments. The alternative is to build a TM system from scratch. Based on hints about the inner workings of TMs (Baldwin, 2001; Somers, 2003), some researchers have developed their own in-house implementations (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; He et al., 2010). In what follows, we propose a straightforward approach to building and evaluating baseline research TM systems, based on well-known methods and widely available tools. In particular, our proposal hinges on MT evaluation metrics and software, which we use not only to assess the effectiveness of our TM systems, but also as active components in the implementation: our plan is to use MT evaluation metrics as TM similarity functions.4 It shoul"
2012.amta-papers.26,W05-0909,0,0.060822,"nts the number of chunks t and r would need to be broken into to allow them to be rearranged with no crossing alignments, Pβ,γ = 1 − γ(chunks/matches)β . Meteorα,β,γ = Fα × Pβ,γ Word-alignments are not restricted to surfacesimilar forms, as Meteor can rely on a lemmatiser and other linguistic resources (Wordnet and paraphrase tables) to account for semantic equivalence. In this study, we experiment with this metric used in two different modes6 : • without any linguistic resources; we refer to this as Vanilla-Meteor (or VMeteor). In this mode, the metric behaves more like its earlier versions (Banerjee and Lavie, 2005). • with all linguistic resources; we refer to this simply as Meteor. 4 MT Evaluation Metrics as TM Similarity Functions Since automatic MT evaluation methods are based on text-similarity metrics, it seems natural to use them in TMs as well. The idea is to replace the reference r and the system output t in the evaluation metrics (Section 3) with the TM’s query q and source-language segment s (Section 2), respectively. The four evaluation metrics described above are well-known to the MT community, and represent different perspectives on textual similarity. Given the large number of existing met"
2012.amta-papers.26,N10-1080,0,0.017332,"st similarity. Much more efficient implementations are of course possible, based on a two-pass strategy, where an efficient search method – for example (Koehn and Senellart, 2010b) – produces a reduced set of candidates, which are then reranked using one of the similarity functions proposed here. 7 TER would arguably have been a better choice, being a well-established metric in the MT community. In practice, however, when attempting to use existing TER implementations for our purposes, we ran into a number of technical difficulties; furthermore, WER and TER are known to behave very similarly (Cer et al., 2010). Open-source implementations exist in the public domain for all our MT evaluation metrics. Using these directly within an exhaustive TM search is certainly not optimal, but it is usually feasible. For instance, the publicly available Meteor software has options -nBest and -oracle, which allow to compare the reference (in our case, the query) to multiple system translations (in our case: candidate TM source-language matches) and output the one that produces the highest score. We used that implementation of Meteor for our experiments, but produced our own implementations of BLEU, NIST and WER."
2012.amta-papers.26,W11-2107,0,0.0192533,"he candidate translations are |r| 1− |t| shorter than the references, BP = min(1, e # "" N 1 X log pn BLEUN = BP · exp N ). (1) n=1 NIST: (Doddington, 2002) A variant of BLEU, in which n-gram precisions are averaged with harmonic rather than geometric mean; it also uses a slightly different brevity penalty   |t| (2) BP = exp β log min( , 1) |r| and, more importantly, weights n-gram matches by how informative they are; the informativeness of an 1 ...wn−1 ) n-gram w1 ...wn is estimated as log count(w count(w1 ...wn ) where counts are typically obtained from the reference translations. Meteor: (Denkowski and Lavie, 2011) Based on a one-to-one alignment between words of t and r, giving preference to alignments with less crossing alignments, Meteor computes unigram precision P and recall R, which are then combined in a weighted harmonic mean, Fα = P R/(αP + (1 − α)R), and scaled by a reordering penalty, which counts the number of chunks t and r would need to be broken into to allow them to be rearranged with no crossing alignments, Pβ,γ = 1 − γ(chunks/matches)β . Meteorα,β,γ = Fα × Pβ,γ Word-alignments are not restricted to surfacesimilar forms, as Meteor can rely on a lemmatiser and other linguistic resources"
2012.amta-papers.26,D12-1058,1,0.83046,"best lists of matches, which means that a general optimization scheme such as Minimum-Error Rate Training (MERT) could be applied in a relatively straightforward manner to optimize numerical parameters with regard to a given evaluation metric9 . Meteor also relies on a paraphrase table to discover semantic similarities. One possible way of optimizing the performance of that metric as a TM similarity function is to provide it with domain-specific paraphrases. In a preliminary experiment along this line, we created domain-specific paraphrase tables from each TM using the technique described in (Fujita et al., 2012), and used these with Meteor instead of the provided table. In practice, in-domain paraphrases do not lead to measurable gains or losses 9 Here, we overlook the integral nature of some parameters, such as N for BLEU and NIST, which raises problems for standard optimization techniques. Then again, using MERT to optimize N for BLEU is clearly overkill. Example 1 This is the process we are commencing. I suggest that we perhaps continue the work we have started. CMeteor This is the point at which we must start. WER This is the stage we are at. BLEU This is the stage we are at. NIST This is the sta"
2012.amta-papers.26,C10-2043,0,0.0897039,"Missing"
2012.amta-papers.26,2010.jec-1.4,0,0.392243,"mercial secrets. One possibility when experimenting with TM functionalities is to rely on one of the commercial implementations, and treat the TM as a “black box” component. But this is unwieldy because in most commercial systems, the TM functionality is only accessible through a complex GUI; furthermore, this raises issues with regard to reproducibility of experiments. The alternative is to build a TM system from scratch. Based on hints about the inner workings of TMs (Baldwin, 2001; Somers, 2003), some researchers have developed their own in-house implementations (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; He et al., 2010). In what follows, we propose a straightforward approach to building and evaluating baseline research TM systems, based on well-known methods and widely available tools. In particular, our proposal hinges on MT evaluation metrics and software, which we use not only to assess the effectiveness of our TM systems, but also as active components in the implementation: our plan is to use MT evaluation metrics as TM similarity functions.4 It should be pointed out that we do not aim at building a full-blown, operational TM environment. In particular, at this stage, we are not concer"
2012.amta-papers.26,2010.amta-papers.2,0,0.387511,"mercial secrets. One possibility when experimenting with TM functionalities is to rely on one of the commercial implementations, and treat the TM as a “black box” component. But this is unwieldy because in most commercial systems, the TM functionality is only accessible through a complex GUI; furthermore, this raises issues with regard to reproducibility of experiments. The alternative is to build a TM system from scratch. Based on hints about the inner workings of TMs (Baldwin, 2001; Somers, 2003), some researchers have developed their own in-house implementations (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; He et al., 2010). In what follows, we propose a straightforward approach to building and evaluating baseline research TM systems, based on well-known methods and widely available tools. In particular, our proposal hinges on MT evaluation metrics and software, which we use not only to assess the effectiveness of our TM systems, but also as active components in the implementation: our plan is to use MT evaluation metrics as TM similarity functions.4 It should be pointed out that we do not aim at building a full-blown, operational TM environment. In particular, at this stage, we are not concer"
2012.amta-papers.26,P07-2045,0,0.00481744,"produced it, and from there establish whether or not it is reliable. In other words, the TM technology is one that users can understand and trust. As a result, TMs have succeeded where most of MT has failed so far: in establishing themselves as a must-have item in translators’ toolboxes. Recently, however, this situation appears to be changing, as we see a surge of interest for MT among the translation community. This renewed enthusiasm is fueled in part by the increased quality of the output of MT systems, but also by the availability of reliable free software, most notably the Moses system (Koehn et al., 2007). Yet, because of the inherent qualities of TM outlined above, and their solid entrenchment in the translation community, it is unlikely that MT will completely replace TM, at least not in the near future. Instead, we will probably see them co-exist for some time. This phenomenon is already visible, as many work environments for translators now incorporate both TM and MT technology. See, for example, the products of SDL Trados1 , Multicorpora2 or the Google Transla1 2 http://www.trados.com http://www.multicorpora.com tor Toolkit3 . As researchers tackle the question of properly integrating the"
2012.amta-papers.26,2005.mtsummit-papers.11,0,0.0185458,"ds 28 817 28 365 26 715 30 471 28 054 27 426 16 514 19 260 Table 1: Experimental Data NIST(s, q) NIST(q, q) We performed experiments to assess the performance of each MT evaluation metric as TM similarity function. Experiments were done under different conditions, corresponding to different corpora and language pairs. While most of the metrics we use are language-agnostic, Meteor relies on languagespecific resources which are not available in all languages. For this reason, we restricted our experiments to English, French, German and Spanish. Our four datasets are drawn from the Europarl v.6 (Koehn, 2005), OPUS corpus (Tiedemann, 2009) (corpora ECB, featuring content from the European Central Bank and EMEA, from the European Medicines Agency) and the JRC-Acquis v.2.2 (Steinberger et al., 2006). All bilingual corpora are available aligned at the sentence level. From each corpus, we randomly sampled 1000 pairs of segments, to be used as test data; the rest was used to build translation memories. All experiments were performed “in-domain”, i.e. for any given experiment, test and TM data always come from the same corpus. Table 1 provides additional details. 5.2 ECB TM (“Train”) segments words 1.8M"
2012.amta-papers.26,C04-1072,0,0.0231565,"cased texts. The differences with true-cased texts are minimal, however; in the end, it’s probably a matter of user-preference. In all that follows, we assume lower-cased source-language texts8 . Both BLEU and NIST are designed to evaluate document-level MT quality, and are not well adapted to finer-grained evaluation; for example, if t and r do not have at least one 4-gram in common, then the product in Equation (1) goes to zero, and therefore the whole BLEU score. To compensate for this problem, it is common to use a “smoothed” version of the score, in which 1 is added to all n-gram counts (Lin and Och, 2004). BLEU and Meteor naturally produce scores comprised between 0 and 1, and can be used directly as similarity functions in a TM. NIST and WER are not as well-behaved: WER can produce scores larger than 1, when the number of edits required to convert t into r (or s into q) is larger than the number of words in r. In a TM setting, where such matches are unlikely to be useful, this problem is easily remedied by capping the value at 1. Also, because WER 8 Note, however, that all target-language evaluations were performed on true-cased texts. is a distance metric rather than a similarity metric, we"
2012.amta-papers.26,N07-1006,0,0.0184426,"that rely on linguistic resources, such as Meteor. In practice, they are easy to implement and produce results comparable to WER (on which existing commercial systems are believed to be based), especially in high-similarity situations, where it counts for real-life TM usage. Most metrics can be tuned, to optimize performance of TM systems for specific text domains. Optimization methods commonly used in statistical machine translation could easily be adapted to this task. One related aspect that we have not yet examined is the combination of different metrics into a single similarity function (Liu and Gildea, 2007). In a similar vein, preliminary experiments suggest that customizing linguistic resources such as paraphrase tables could help in better leveraging the contents of the TM when appropriate metrics are used, such as Meteor or TERp. Extracting domain-specific paraphrases is one possible avenue, but in a TM perspective, it would be interesting to extend similarity to other semantic relations besides synonymy, e.g. antonymy, hyponymy, etc. These are areas we hope to explore further in the near future. When evaluating the performance of TM systems using MT evaluation metrics, in general, we find th"
2012.amta-papers.26,N12-1019,0,0.0458782,"Missing"
2012.amta-papers.26,P03-1021,0,0.0114132,"f the query (average length ratios are given in Table 3). This contrasts with WER (red dots), which naturally favors segments that are much shorter than the query, and with the Meteor metrics (purple and black), which tend to produce source segments that are much longer than the query. On the target side, shorter TM matches such as those produced by the WER similarity function will be penalized at evaluation time by the BLEU and NIST 150 Lengh of best match To those with a background in statistical machine translation, who are familiar with the general approach of Minimum Error-Rate Training (Och, 2003), this may seem like a very natural outcome at first sight. After all, using any given metric as a similarity function is somewhat like optimizing the behavior of the system for that evaluation metric. The subtle difference here is that in a TM, similarity is measured in the source language. If we take the example of BLEU, this means that maximizing ngram precision relative to the source-language query somehow results in the n-gram precision being maximized in the target-language as well. 120 90 60 30 0 0 30 60 90 120 150 Length of query Figure 1: Length (in words) of TM best match source segm"
2012.amta-papers.26,P02-1040,0,0.105449,"en used extensively in speech recognition. It is computed as the word-based Levenshtein distance between t and r, divided by the number of words in the reference: |r|. Several variants exist, most notably TER (Translation Edit – or Error – Rate), in which local swaps of sequences of words are allowed. TER itself has two variants: HTER, in which the reference translation is manually produced by minimally post-editing the MT output under evaluation (Snover et al., 2006); and TERp, which also relies on a table of paraphrases to detect semantically equivalent matches (Snover et al., 2009). BLEU: (Papineni et al., 2002) The mother of all MT evaluation metrics: BLEU measures n-gram precision, i.e. the proportion of word n-grams of t that are also found in r. These n-gram precisions pn are calculated separately for values of n ranging from 1 to N (typically N = 4), and then combined using a geometric mean. The score is scaled by a brevity penalty if the candidate translations are |r| 1− |t| shorter than the references, BP = min(1, e # "" N 1 X log pn BLEUN = BP · exp N ). (1) n=1 NIST: (Doddington, 2002) A variant of BLEU, in which n-gram precisions are averaged with harmonic rather than geometric mean; it also"
2012.amta-papers.26,2009.mtsummit-papers.14,1,0.947549,"rt of TMs are well-kept commercial secrets. One possibility when experimenting with TM functionalities is to rely on one of the commercial implementations, and treat the TM as a “black box” component. But this is unwieldy because in most commercial systems, the TM functionality is only accessible through a complex GUI; furthermore, this raises issues with regard to reproducibility of experiments. The alternative is to build a TM system from scratch. Based on hints about the inner workings of TMs (Baldwin, 2001; Somers, 2003), some researchers have developed their own in-house implementations (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; He et al., 2010). In what follows, we propose a straightforward approach to building and evaluating baseline research TM systems, based on well-known methods and widely available tools. In particular, our proposal hinges on MT evaluation metrics and software, which we use not only to assess the effectiveness of our TM systems, but also as active components in the implementation: our plan is to use MT evaluation metrics as TM similarity functions.4 It should be pointed out that we do not aim at building a full-blown, operational TM environment. In particular, at th"
2012.amta-papers.26,2006.amta-papers.25,0,0.11355,"focus on four well-known MT evaluation metrics: WER, BLEU, NIST and Meteor. We briefly review these here: WER (Word Error Rate): This metric has been used extensively in speech recognition. It is computed as the word-based Levenshtein distance between t and r, divided by the number of words in the reference: |r|. Several variants exist, most notably TER (Translation Edit – or Error – Rate), in which local swaps of sequences of words are allowed. TER itself has two variants: HTER, in which the reference translation is manually produced by minimally post-editing the MT output under evaluation (Snover et al., 2006); and TERp, which also relies on a table of paraphrases to detect semantically equivalent matches (Snover et al., 2009). BLEU: (Papineni et al., 2002) The mother of all MT evaluation metrics: BLEU measures n-gram precision, i.e. the proportion of word n-grams of t that are also found in r. These n-gram precisions pn are calculated separately for values of n ranging from 1 to N (typically N = 4), and then combined using a geometric mean. The score is scaled by a brevity penalty if the candidate translations are |r| 1− |t| shorter than the references, BP = min(1, e # "" N 1 X log pn BLEUN = BP ·"
2012.amta-papers.26,steinberger-etal-2006-jrc,0,0.0607128,"Missing"
2012.amta-papers.26,H93-1040,0,0.355121,"Missing"
2012.amta-papers.26,1993.mtsummit-1.24,0,\N,Missing
2015.mtsummit-papers.1,P84-1068,0,0.64869,"Missing"
2015.mtsummit-papers.1,N12-1047,0,0.062571,"t (Koehn et al., 2007) with distortion limit of six as the baseline. We examined each of our SSSS transfer, and pre-ordering modules and their combination over the baseline. For reference, we investigated the performance of phrase-based SMT with a larger distortion limit 20, as well as hierarchical phrase-based SMT. Throughout the experiments, we used KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szał, 2010) for word alignment. We used the grow-diag-final method for obtaining phrase pairs. Weights of the models were tuned with n-best batch MIRA (Cherry and Foster, 2012) regarding BLEU (Papineni et al., 2002) as the objective. For each system, we performed weight tuning three times and selected for the test the setting that achieved the best BLEU on the development data. 4.3. Evaluation Metrics Each system is evaluated using two metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). Although our primary concern in this experiment is the effect of long distance relationship, in general, n-gram based metrics such as BLEU alone do not fully illustrate it. RIBES is therefore used alongside BLEU. RIBES is an automatic evaluation method based on r"
2015.mtsummit-papers.1,P05-1033,0,0.267749,"uilt a patent-adapted (not claim-adapted) parsing model by applying a self-learning procedure (Huang et al., 2009) to the above automatic parses. (ROOT (S (NP (PRP He)) (VP (VBZ likes) (NP (NNS apples))) (. .))) Figure 7. Parsing result of “He likes apples.” 4. Experiments We evaluated to what extent our SSSS transfer and pre-ordering improved the translation quality. As mentioned in Section 3, these methods are implemented as an add-on to off-the-shelf SMT systems. In particular, we used phrase-based SMT (Koehn et al., 2003) as the base system. We also regard it and its hierarchical version (Chiang, 2005) as baseline SMT systems. 4.1. Data The training data for SMT consists of two subcorpora. The first is the Japanese-English Patent Translation data comprising 3.2 million sentence pairs provided by the organizer of the Patent Machine Translation Task (PatentMT) at the NTCIR-9 Workshop (Goto et al., 2011). We randomly selected 3.0 million sentence pairs. Henceforth, we call this Corpus A. SMT systems trained on the corpus are reasonably good at lexical selection in translating claim sentences, because the vocabulary and phrases are commonly used in entire patent documents, and Corpus A is of a"
2015.mtsummit-papers.1,P05-1066,0,0.0440006,"most the entire gain in the Japanese-to-English setting. Conversely, English sentences are much more difficult to parse than Japanese. As a result, the pre-ordering module can sometimes fail to bring the English word order close to that in Japanese. Nevertheless, as a result of SSSS transfer, which divides an input English sentence into shorter pieces, pre-ordering became more accurate, and the RIBES score was further improved. 5. Related Work The quality of machine translation across distant languages has been improved as a result of the recent introduction of syntactic information into SMT (Collins et al., 2005; Quirk et al., 2005; Katz-Brown and Collins, 2008; Sudo et al., 2013; Hoshino et al., 2013; Cai et al., 2014; Goto Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 11 et al., 2015). One of the promising avenues for further improvement appears to be the incorporation of sublanguage-specific information (Buchmann et al., 1984; Luckhardt, 1991). This is particularly important for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an ea"
2015.mtsummit-papers.1,N15-1105,0,0.107415,"Missing"
2015.mtsummit-papers.1,2012.amta-caas14.1,0,0.0123933,"tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source and target sentences using minimal depth structures for facilitating the transfer between the source and target structures (Buchmann et al., 1984). Much of the recent work relating to document and sentence structures between close languages focuses on structures centered on discourse connectives (Miltsakaki et al., 2005; Pitler and Nenkova, 2009; Meyer et al., 2011; Hajlaoui and Popescu-Belis, 2012; Meyer et al., 2012) and on resolving the ambiguity of discourse connectives connecting structural components. Conversely, when dealing with structures across distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across dist"
2015.mtsummit-papers.1,P13-2121,0,0.0327298,"entence pairs was randomly divided into development and test data respectively consisting of 1,000 English-Japanese claim sentence pairs. 4.2. Systems In this experiment, we regard the implementation of phrase-based SMT in the Moses toolkit (Koehn et al., 2007) with distortion limit of six as the baseline. We examined each of our SSSS transfer, and pre-ordering modules and their combination over the baseline. For reference, we investigated the performance of phrase-based SMT with a larger distortion limit 20, as well as hierarchical phrase-based SMT. Throughout the experiments, we used KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szał, 2010) for word alignment. We used the grow-diag-final method for obtaining phrase pairs. Weights of the models were tuned with n-best batch MIRA (Cherry and Foster, 2012) regarding BLEU (Papineni et al., 2002) as the objective. For each system, we performed weight tuning three times and selected for the test the setting that achieved the best BLEU on the development data. 4.3. Evaluation Metrics Each system is evaluated using two metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). Although our primary c"
2015.mtsummit-papers.1,I13-1147,0,0.0147867,"much more difficult to parse than Japanese. As a result, the pre-ordering module can sometimes fail to bring the English word order close to that in Japanese. Nevertheless, as a result of SSSS transfer, which divides an input English sentence into shorter pieces, pre-ordering became more accurate, and the RIBES score was further improved. 5. Related Work The quality of machine translation across distant languages has been improved as a result of the recent introduction of syntactic information into SMT (Collins et al., 2005; Quirk et al., 2005; Katz-Brown and Collins, 2008; Sudo et al., 2013; Hoshino et al., 2013; Cai et al., 2014; Goto Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 11 et al., 2015). One of the promising avenues for further improvement appears to be the incorporation of sublanguage-specific information (Buchmann et al., 1984; Luckhardt, 1991). This is particularly important for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source a"
2015.mtsummit-papers.1,D10-1092,0,0.147645,"Missing"
2015.mtsummit-papers.1,P13-1048,0,0.0299936,"sive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies on pattern translat"
2015.mtsummit-papers.1,N03-1017,0,0.0190389,"ble. We first parsed 200,000 patent sentences using the initial parsing model. We then built a patent-adapted (not claim-adapted) parsing model by applying a self-learning procedure (Huang et al., 2009) to the above automatic parses. (ROOT (S (NP (PRP He)) (VP (VBZ likes) (NP (NNS apples))) (. .))) Figure 7. Parsing result of “He likes apples.” 4. Experiments We evaluated to what extent our SSSS transfer and pre-ordering improved the translation quality. As mentioned in Section 3, these methods are implemented as an add-on to off-the-shelf SMT systems. In particular, we used phrase-based SMT (Koehn et al., 2003) as the base system. We also regard it and its hierarchical version (Chiang, 2005) as baseline SMT systems. 4.1. Data The training data for SMT consists of two subcorpora. The first is the Japanese-English Patent Translation data comprising 3.2 million sentence pairs provided by the organizer of the Patent Machine Translation Task (PatentMT) at the NTCIR-9 Workshop (Goto et al., 2011). We randomly selected 3.0 million sentence pairs. Henceforth, we call this Corpus A. SMT systems trained on the corpus are reasonably good at lexical selection in translating claim sentences, because the vocabula"
2015.mtsummit-papers.1,C94-2183,0,0.0489313,"cross distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. Ther"
2015.mtsummit-papers.1,E91-1054,0,0.456166,"tactic parsing (Isozaki et al., 2010b; de Gispert et al., 2015), with growing volumes of parallel patent corpora available, have brought significant improvements in the performance of statistical machine translation (SMT) for translating patent documents across distant language pairs (Goto et al., 2012; Goto et al., 2015). However, among various sentences within a patent document, patent claim sentences still pose difficulties for SMT resulting in low translation quality, despite their utmost legal importance. A patent claim sentence is written in a kind of sublanguage (Buchmann et al., 1984; Luckhardt, 1991) in the sense that it has the following two characteristics: (i) comprising a patent claim by itself with an extreme length and (ii) having a typical sentence structure composed of a fixed set of components irrespective of language, such as those illustrated in Figures 1 and 2. The difficulties in patent claim translation lie in these two characteristics. Regarding the first characteristic, the extreme lengths cause syntactic parsers to fail with consequent low Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 1 reordering accuracy. Regarding the second"
2015.mtsummit-papers.1,2006.eamt-1.24,0,0.551983,"Missing"
2015.mtsummit-papers.1,2012.amta-papers.20,0,0.0173078,"document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source and target sentences using minimal depth structures for facilitating the transfer between the source and target structures (Buchmann et al., 1984). Much of the recent work relating to document and sentence structures between close languages focuses on structures centered on discourse connectives (Miltsakaki et al., 2005; Pitler and Nenkova, 2009; Meyer et al., 2011; Hajlaoui and Popescu-Belis, 2012; Meyer et al., 2012) and on resolving the ambiguity of discourse connectives connecting structural components. Conversely, when dealing with structures across distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in whi"
2015.mtsummit-papers.1,P02-1040,0,0.0931704,"(2) Claim sentences are translated according to the sentence structure, producing structurally natural translation outputs. We manually extracted a set of language independent claim components. Moreover, using these components, we constructed a set of synchronous rules for English and Japanese to transfer the SSSS in the source language to the target language. The results of an experiment demonstrate these two major effects of our SSSS transfer method. Regarding the first effect, when used in conjunction with pre-ordering, our method improves translation quality by five points in BLEU score (Papineni et al., 2002), in both English-to-Japanese and Japanese-to-English translations. Regarding the second effect, gains in RIBES score (Isozaki et al., 2010a) of over 30 points are obtained, indicating that our SSSS transfer is effective in transferring an input sentence structure to the output sentence. Components Preamble Transitional phrase Body Element Element Element Example strings An apparatus, comprising: a pencil; an eraser attached to the pencil; and a light attached to the pencil. Figure 1. Example of an English patent claim (WIPO, 2014) Components Element Body Element Element Transitional phrase Pr"
2015.mtsummit-papers.1,P09-2004,0,0.0322502,"tant for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source and target sentences using minimal depth structures for facilitating the transfer between the source and target structures (Buchmann et al., 1984). Much of the recent work relating to document and sentence structures between close languages focuses on structures centered on discourse connectives (Miltsakaki et al., 2005; Pitler and Nenkova, 2009; Meyer et al., 2011; Hajlaoui and Popescu-Belis, 2012; Meyer et al., 2012) and on resolving the ambiguity of discourse connectives connecting structural components. Conversely, when dealing with structures across distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this di"
2015.mtsummit-papers.1,P06-1055,0,0.0259631,"“having:”, ”備える”〉 〈“wherein:”, ”ことを特徴とする”〉 〈“wherein:”, ”する”〉 3.2. Pre-ordering Another major issue in patent claim translation is that the extreme lengths cause syntactic parsers to fail with consequent low reordering accuracy. To evaluate the effect of introducing our SSSS transfer on the translation quality, we also implemented a pre-ordering tool using state-of-the-art techniques (Isozaki et al., 2010b; Goto et al., 2012; Goto et al., 2015). Our pre-ordering method is based on syntactic parsing. First, the input sentence is parsed into a binary tree structure by using the Berkeley Parser (Petrov et al., 2006). For example, when “He likes apples.” is inputted into our English-to-Japanese translation system, it is parsed as shown in Figure 7. Second, the nodes in the parse tree are reordered using a classifier. For example, according to the classifier's decision, the two children of the “VP” node, i.e., “VBZ” and “NP”, are swapped, whereas the order of the two children of the “S” node, i.e., “NP” and “VP”, is retained. Once such a decision is made for every node with two children (henceforth, binary mode), the word order of the entire sentence becomes very similar to that in Japanese, i.e., “He (kar"
2015.mtsummit-papers.1,P05-1034,0,0.049209,"n the Japanese-to-English setting. Conversely, English sentences are much more difficult to parse than Japanese. As a result, the pre-ordering module can sometimes fail to bring the English word order close to that in Japanese. Nevertheless, as a result of SSSS transfer, which divides an input English sentence into shorter pieces, pre-ordering became more accurate, and the RIBES score was further improved. 5. Related Work The quality of machine translation across distant languages has been improved as a result of the recent introduction of syntactic information into SMT (Collins et al., 2005; Quirk et al., 2005; Katz-Brown and Collins, 2008; Sudo et al., 2013; Hoshino et al., 2013; Cai et al., 2014; Goto Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 11 et al., 2015). One of the promising avenues for further improvement appears to be the incorporation of sublanguage-specific information (Buchmann et al., 1984; Luckhardt, 1991). This is particularly important for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublang"
2015.mtsummit-papers.1,P13-2066,0,0.0184134,"re appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies on pattern translation (Xia et al., 2"
2015.mtsummit-papers.1,2007.mtsummit-papers.63,1,0.700657,"abulary and phrases are commonly used in entire patent documents, and Corpus A is of a substantial size to cover a large portion of them. However, the claim-specific sentence structure would never be taken into account, as Corpus A does not contain any claim sentences. To bring claim-specific characteristics into the SMT training, even for the baseline systems, we also used Corpus B comprising 1.0 million parallel sentences of patent claims. These were automatically extracted from pairs of English and Japanese patent documents published between 1999 and 2012 using a sentence alignment method (Utiyama and Isahara, 2007). The concatenation of Corpora A and B was used to train baseline SMT systems, as well as those for our extensions. 2 Note that Goto et al. (2015) learned the SWAP/STRAIGHT classification problem jointly with the parsing source sentences. 3 https://www.cis.upenn.edu/~treebank/ 4 https://www2.nict.go.jp/out-promotion/techtransfer/EDR/index.html Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 7 Development and test data were constructed separately from the training data in the following manner. First, we randomly extracted English patent documents from p"
2015.mtsummit-papers.1,N09-2004,0,0.0270482,"s, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies"
2015.mtsummit-papers.1,C04-1073,0,0.141971,"Missing"
2015.mtsummit-papers.1,P14-2092,0,0.218298,"Missing"
2015.mtsummit-papers.1,P09-2035,0,0.0194868,"rall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies on pattern translation (Xia et al., 2004; Murakami et al., 2009; Murakami et al., 2013) and sentence segmentation (Xiong et al., 2009; Jin and Liu, 2010) for dealing with long input sentences with complex structures. Our approach is similar to the above models in the sense that it incorporates structural information into SMT, but differs in that it uses sublanguage-specific sentence structures, rather than syntactically motivated structures. This results in significant improvement in translation quality for the claim sublanguage using only a handful of rules. 6. Conclusion In this paper, we described a method for transferring sublanguage-specific sentence structure for English-to-Japanese and Japanese-to-English patent clai"
2020.acl-main.532,D18-1045,0,0.0443142,"f monolingual data (Sennrich et al., 2016a) is undoubtedly the most prevalent one, as it remains widely used in state-of-the-art NMT systems (Barrault et al., 2019). NMT systems trained on back-translated data can generate more fluent translations (Sennrich et al., 2016a) thanks to the use of much larger data in the target language to better train the decoder, especially for low-resource conditions where only a small quantity of parallel training data is available. However, the impact of the noisiness of the synthetic source sentences generated by NMT largely remains unclear and understudied. Edunov et al. (2018) even showed that introducing synthetic noise in back-translations actually improves translation quality and enables the Q1. Do NMT systems trained on large backtranslated data capture some of the characteristics of human-produced translations, i.e., translationese? Q2. Does a tag for back-translations really help differentiate translationese from original texts? Q3. Are NMT systems trained on back-translation for low-resource conditions as sensitive to translationese as in high-resource conditions? 2 Motivation During the training with back-translated data (Sennrich et al., 2016a), we can exp"
2020.acl-main.532,P18-4020,0,0.0334377,"Missing"
2020.acl-main.532,P07-2045,0,0.0145408,"rather balanced on their source side between translationese and original texts. For 1 http://www.statmt.org/wmt19/ translation-task.html 2 http://www.statmt.org/wmt15/ translation-task.html 3 After pre-processing and cleaning, we obtained 5.2M and 32.8M sentence pairs for en-de and en-fr, respectively. evaluation, since most of the WMT test sets are made of both original and translationese texts, we used all the newstest sets, from WMT10 to WMT19 for en-de, and from WMT08 to WMT15 for en-fr.4 All our data were pre-processed in the same way: we performed tokenization and truecasing with Moses (Koehn et al., 2007). 3.2 NMT Systems For NMT, we used the Transformer (Vaswani et al., 2017) implemented in Marian (Junczys-Dowmunt et al., 2018) with standard hyper-parameters for training a Transformer base model.5 To compress the vocabulary, we learned 32k byte-pair encoding (BPE) operations (Sennrich et al., 2016b) for each side of the parallel training data. The back-translations were generated through decoding with Marian the sampled monolingual sentences using beam search with a beam size of 12 and a length normalization of 1.0. The back-translated data were then concatenated to the original parallel data"
2020.acl-main.532,P02-1040,0,0.10906,"ansformer base model.5 To compress the vocabulary, we learned 32k byte-pair encoding (BPE) operations (Sennrich et al., 2016b) for each side of the parallel training data. The back-translations were generated through decoding with Marian the sampled monolingual sentences using beam search with a beam size of 12 and a length normalization of 1.0. The back-translated data were then concatenated to the original parallel data and a new NMT model was trained from scratch using the same hyperparameters used to train the model that generated the back-translations. We evaluated all systems with BLEU (Papineni et al., 2002) computed by sacreBLEU (Post, 2018). To evaluate only on the part of the test set that have original text or translationese on the source side, we used the --origlang option of sacreBLEU with the value “non-L1” for translationese texts and “L1” for original texts, where L1 is the source language, and report on their respective BLEU scores.6 3.3 Results in Resource-Rich Conditions Our results with back-translations (BT) and tagged back-translations (T-BT) are presented in Table 1. When using BT, we consistently observed a drop of BLEU scores for original texts for all the translations tasks, wi"
2020.acl-main.532,P16-1009,0,0.648674,"ginal parallel data during training. We also show that, in contrast to high-resource configurations, NMT systems trained in lowresource settings are much less vulnerable to overfit back-translations. We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown. 1 Introduction During training, neural machine translation (NMT) can leverage a large amount of monolingual data in the target language. Among existing ways of exploiting monolingual data in NMT, the so-called back-translation of monolingual data (Sennrich et al., 2016a) is undoubtedly the most prevalent one, as it remains widely used in state-of-the-art NMT systems (Barrault et al., 2019). NMT systems trained on back-translated data can generate more fluent translations (Sennrich et al., 2016a) thanks to the use of much larger data in the target language to better train the decoder, especially for low-resource conditions where only a small quantity of parallel training data is available. However, the impact of the noisiness of the synthetic source sentences generated by NMT largely remains unclear and understudied. Edunov et al. (2018) even showed that int"
2020.acl-main.532,P16-1162,0,0.611239,"ginal parallel data during training. We also show that, in contrast to high-resource configurations, NMT systems trained in lowresource settings are much less vulnerable to overfit back-translations. We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown. 1 Introduction During training, neural machine translation (NMT) can leverage a large amount of monolingual data in the target language. Among existing ways of exploiting monolingual data in NMT, the so-called back-translation of monolingual data (Sennrich et al., 2016a) is undoubtedly the most prevalent one, as it remains widely used in state-of-the-art NMT systems (Barrault et al., 2019). NMT systems trained on back-translated data can generate more fluent translations (Sennrich et al., 2016a) thanks to the use of much larger data in the target language to better train the decoder, especially for low-resource conditions where only a small quantity of parallel training data is available. However, the impact of the noisiness of the synthetic source sentences generated by NMT largely remains unclear and understudied. Edunov et al. (2018) even showed that int"
2020.lrec-1.449,abdelali-etal-2014-amara,0,0.066042,", 2014) focused on collecting data from AMARA platform (Jansen et al., 2014). They usually aim at European and BRIC languages, such as German, Polish, and Russian. Using automatic alignment methods are more desirable, because they can help extract parallel sentences that are orders of magnitude larger than those that can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel"
2020.lrec-1.449,P17-1042,0,0.0121265,"necessarily hold for every language pair. We thus encourage researchers to try iterative refinement of training data in their own experimental settings. 5.6. Indirect Assessment of the Created In-Domain Data In this section, we evaluate the superiority of our sentence alignment method, presented in Section 3.2. (henceforth, MT+CS), over other methods, extrinsically, through MT performance. The following two similarity measures were additionally implemented and tested. Unsupervised: Cosine similarity over the cross-lingual sentence embeddings, learned by an unsupervised method, called VecMap (Artetxe et al., 2017).19 18 Compare the pairs (A1, A2), (A2, A3), (A1, A10), (B2, B3), (C3, C4), (C10, C11), (D3, D4), (D10, D11), and (A1, E14). 19 3646 https://github.com/artetxem/vecmap # of aligned lines Unsupervised MT+BLEU MT+CS (B16 in Table 6) 40,452 42,672 40,770 BLEU Ja→En En→Ja 4.0 2.8 6.2 4.3 3.4 6.4 Table 9: BLEU scores achieved with only Coursera parallel data extracted by different similarity measures. Acknowledgments This work was carried out when Haiyue Song was taking up an internship at NICT, Japan. A part of this work was conducted under the program “Research and Development of Enhanced Multili"
2020.lrec-1.449,L18-1528,0,0.0299661,"Missing"
2020.lrec-1.449,2012.eamt-1.60,0,0.254145,"lecture subtitles into other languages, including English, is also an important challenge. The TraMOOC project (Kordoni et al., 2015) aims at improving the accessibility of European languages through MT. They focus on collecting translations of lecture subtitles and constructing MT systems for eleven European and BRIC languages. However, the amount of parallel resources involving other languages, such as Chinese and Japanese, are still quite low. Subtitle translation falls under spoken language translation. Past studies in spoken language translation mainly focused on subtitles for TED talks (Cettolo et al., 2012). Even though the parallel data in this domain should be exploitable for lectures translation to some degree, university lectures are devoted mainly for educational purposes, and the subtle differences in domains may hinder translation quality. To obtain high-quality parallel data, professional translators are typically employed to translate. However, the cost is often very high and thus using this way to produce large quantities of parallel data is economically infeasible, especially for universities and non-profit organizations. In the case of online lectures and talks, subtitles are often t"
2020.lrec-1.449,W19-5435,0,0.0614599,"Missing"
2020.lrec-1.449,P17-2061,1,0.798899,"l., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006; Miceli Barone et al., 2017; Thompson et al., 2019). Furthermore, the domain divergence between the out-of- and in-domain corpora is another issue. 3. Parallel Corpus Alignment Extracting parallel data usable for MT involves crawling documents and aligning translation pairs in the corpora. To align translations, one can use crowdsourcing services ("
2020.lrec-1.449,D19-1146,1,0.865738,"TC→C flow. Bold indicates the initial training, and red-, blue-, and grey-colored cells mean inflation , deflation , and replacement of training data, respectively. into multiple stages where each stage uses data from different domains to maximize the impact of the domain-specific training data. As we have larger parallel corpora from other domains, such as TED (0.2M pairs; non-educational spoken domain) and ASPEC (3.0M pairs; scientific domain), we can leverage domain adaptation techniques, such as finetuning and mixed fine-tuning (Chu et al., 2017). Furthermore, Imankulova et al. (2019) and Dabre et al. (2019) showed that training in multiple stages where each stage contains different proportions of various types of training data leads to the best results. Following them, we decided to conduct an extensive experiment with multistage training with different proportions of training data from different domains at each stage. 5.2. Datasets As in the previous section, we performed Juman++ and NLTK tokenization for Japanese and English, respectively. Henceforth, we refer to the ASPEC training data of 1.0 million lines as “A,” the TED training data of 0.2 million lines as “T,” and the Coursera training da"
2020.lrec-1.449,L18-1545,0,0.0167994,"he best possible sentence alignments for Coursera data were already found, owing to our algorithm, similarity measure, and/or the initial MT system trained only on ASPEC and TED. • Leveraging out-of-domain data through multistage training is invaluable. • Gradually inflating the data starting from out-ofdomain corpus and adding the in-domain corpus at the end should give the best possible translation quality. One peculiarity of our results is that the BLEU scores for the Ja→En task were significantly higher than the En→Ja task, which is a reversal of a general tendency for this language pair (Imamura and Sumita, 2018; Nakazawa et al., 2019), even though the BLEU scores in different languages are not directly comparable. Table 7 gives a comparison of three different translation tasks. Upon manual investigation, we identified that the En→Ja translations in TED and Coursera tasks tend to be much shorter than the reference translation, receiving around 0.7 brevity penalty. When we tuned the length penalty for decoding on the development set, we observed 0.5 to 1.0 point BLEU gains on the test set Iterative Refinement of Aligned Data Having obtained a better MT system than the initial one, we can iterate the w"
2020.lrec-1.449,W19-6613,1,0.842963,"1) sub-paths of the A→AT→ATC→TC→C flow. Bold indicates the initial training, and red-, blue-, and grey-colored cells mean inflation , deflation , and replacement of training data, respectively. into multiple stages where each stage uses data from different domains to maximize the impact of the domain-specific training data. As we have larger parallel corpora from other domains, such as TED (0.2M pairs; non-educational spoken domain) and ASPEC (3.0M pairs; scientific domain), we can leverage domain adaptation techniques, such as finetuning and mixed fine-tuning (Chu et al., 2017). Furthermore, Imankulova et al. (2019) and Dabre et al. (2019) showed that training in multiple stages where each stage contains different proportions of various types of training data leads to the best results. Following them, we decided to conduct an extensive experiment with multistage training with different proportions of training data from different domains at each stage. 5.2. Datasets As in the previous section, we performed Juman++ and NLTK tokenization for Japanese and English, respectively. Henceforth, we refer to the ASPEC training data of 1.0 million lines as “A,” the TED training data of 0.2 million lines as “T,” and"
2020.lrec-1.449,W17-3204,0,0.0132128,"https://github.com/shyyhs/CourseraParallelCorpusMining http://mooc.org 4 https://www.coursera.org 5 https://iversity.org 6 https://wit3.fbk.eu/mt.php?release=2017-01-ted-test 7 https://www.opensubtitles.org 3 2.3. Domain Adaptation for Neural Machine Translation At present, neural machine translation (NMT) is known to give higher quality of translation. To train a sequence-tosequence model (Sutskever et al., 2014), attention-based model (Bahdanau et al., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addr"
2020.lrec-1.449,W15-4935,0,0.065899,"Missing"
2020.lrec-1.449,L16-1003,0,0.0411224,"Missing"
2020.lrec-1.449,L18-1236,0,0.0116466,"ic alignment methods are more desirable, because they can help extract parallel sentences that are orders of magnitude larger than those that can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel corpus in the educational lectures domain, relying on Coursera. Figure 1 gives an overview of our framework, where we assume the availability of in-domain parallel document"
2020.lrec-1.449,2015.iwslt-evaluation.11,0,0.0224968,"model (Sutskever et al., 2014), attention-based model (Bahdanau et al., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006; Miceli Barone et al., 2017; Thompson et al., 2019). Furthermore, the domain divergence between the out-of- and in-domain corpora is another issue. 3. Parallel Corpus Alignment Extracting parallel data usable for MT involves crawling documents and aligning translation pairs in the corpor"
2020.lrec-1.449,D17-1156,0,0.0170107,"ing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006; Miceli Barone et al., 2017; Thompson et al., 2019). Furthermore, the domain divergence between the out-of- and in-domain corpora is another issue. 3. Parallel Corpus Alignment Extracting parallel data usable for MT involves crawling documents and aligning translation pairs in the corpora. To align translations, one can use crowdsourcing services (Behnke et al., 2018). However, this can be extremely timeconsuming if not expensive. Previous research (Abdelali et 2 al., 2014) focused on collecting data from AMARA platform (Jansen et al., 2014). They usually aim at European and BRIC languages, such as German, Polish, and R"
2020.lrec-1.449,L16-1350,1,0.819808,"data. Dataset English Mean / Median / s.d. Japanese Mean / Median / s.d. ASPEC TED Coursera 25.4 / 23 / 11.4 20.4 / 17 / 13.9 21.1 / 19 / 11.1 27.5 / 20 / 12.0 19.8 / 16 / 14.1 22.2 / 20 / 11.8 Table 2: BLEU score for Ja→En on TED test set. A, T, and AT respectively stand for ASPEC training set, TED training set, and their balanced mixture. “→” indicates that the model trained on the left-hand side data is fine-tuned on the right-hand side data. domain. However, given its small size, it can lead to only an unreliable MT system. Therefore, we decided to use a larger out-of-domain ASPEC corpus (Nakazawa et al., 2016)13 to build a better MT system. Table 1 gives the statistics of the ASPEC and TED corpora that we used to train our initial MT system. We compared fine-tuning and mixed fine-tuning approaches proposed by Chu et al. (2017). When performing mixed fine-tuning on the concatenation of both two corpora, the TED corpus was oversampled to match the size of the ASPEC corpus. We trained our NMT models using tensor2tensor with its default hyper-parameters. Refer to Section 5.3. for further details on training configurations. So far, we do not have a test set for the target domain, i.e., Coursera. We ther"
2020.lrec-1.449,P02-1040,0,0.109811,"o translate one side into the other language using an MT system (Sennrich and Volk, 2010). To train such a system, we can leverage any existing parallel data in related or even distant domains. The MT system should generate translations as accurately as possible. In practice, domain adaptation techniques (Chu et al., 2017) are most useful in training an accurate MT system. 3.2.2. Similarity Measure The key component in the DP algorithm is the matching function, i.e., similarity measure in our context. Existing methods, such as that in Sennrich and Volk (2010), used sentence-level BLEU scores (Papineni et al., 2002) of machine-translated source sentence against the actual target language sentence as their similarity score: formally, Sim BLEU (fi , ej ) = BLEU (MT (fi ), ej ), (1) where fi and ej are the i-th sentence in the source document and the j-th sentence in the target document, respectively. However, due to the lack of in-domain data, MT system can give only translations of low quality and thus the BLEU scores can be misleading, especially for distant language pairs, such as Japanese and English. An alternative way is to directly compute cosine similarity of a given sentence pair (Bouamor and Sajj"
2020.lrec-1.449,N16-1110,0,0.0201253,"Missing"
2020.lrec-1.449,2010.amta-papers.14,0,0.10192,"such as German, Polish, and Russian. Using automatic alignment methods are more desirable, because they can help extract parallel sentences that are orders of magnitude larger than those that can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel corpus in the educational lectures domain, relying on Coursera. Figure 1 gives an overview of our framework, where we assume the"
2020.lrec-1.449,W11-4624,0,0.0319982,"nd Russian. Using automatic alignment methods are more desirable, because they can help extract parallel sentences that are orders of magnitude larger than those that can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel corpus in the educational lectures domain, relying on Coursera. Figure 1 gives an overview of our framework, where we assume the availability of in-domai"
2020.lrec-1.449,P16-1009,0,0.220059,"2014), attention-based model (Bahdanau et al., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006; Miceli Barone et al., 2017; Thompson et al., 2019). Furthermore, the domain divergence between the out-of- and in-domain corpora is another issue. 3. Parallel Corpus Alignment Extracting parallel data usable for MT involves crawling documents and aligning translation pairs in the corpora. To align translation"
2020.lrec-1.449,D19-1168,0,0.0189494,"erage similarity of all aligned sentence pairs within each document pair. We then subject these sorted and sentence aligned pairs to human evaluation using Algorithm 1 in order to obtain high-quality test and development sets, where the target volume of each set (volume) and document-level comparability (ratio) are the two parameters. We use the remaining sentence aligned document pairs for training. Our test, development, and training sets are all constructed at the document level and thus our corpora can be used to evaluate document-level translation (Voita et al., 2019; Wang et al., 2019b; Tan et al., 2019). 4. 20 Cleaning Documents Our framework is mostly language independent. The only language specific processes are tokenization and language mismatch detection. We first segmented the both English and Japanese paragraphs with full-stop (“.”), exclamation (“!”), and question marks (“?”) in Latin encoding and their full-width counterparts in UTF-8 followed by a space or the end of line. Then, we tokenized Japanese and English sentences, using Juman++ (Tolmachev et al., 2018)11 and NLTK,12 respectively. Algorithm 2 shows our rule-based language detection procedure for the Japanese–English setting."
2020.lrec-1.449,N19-1209,0,0.0109303,"arallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006; Miceli Barone et al., 2017; Thompson et al., 2019). Furthermore, the domain divergence between the out-of- and in-domain corpora is another issue. 3. Parallel Corpus Alignment Extracting parallel data usable for MT involves crawling documents and aligning translation pairs in the corpora. To align translations, one can use crowdsourcing services (Behnke et al., 2018). However, this can be extremely timeconsuming if not expensive. Previous research (Abdelali et 2 al., 2014) focused on collecting data from AMARA platform (Jansen et al., 2014). They usually aim at European and BRIC languages, such as German, Polish, and Russian. Using automatic"
2020.lrec-1.449,tiedemann-2012-parallel,0,0.0507603,"://iversity.org 6 https://wit3.fbk.eu/mt.php?release=2017-01-ted-test 7 https://www.opensubtitles.org 3 2.3. Domain Adaptation for Neural Machine Translation At present, neural machine translation (NMT) is known to give higher quality of translation. To train a sequence-tosequence model (Sutskever et al., 2014), attention-based model (Bahdanau et al., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006"
2020.lrec-1.449,L16-1559,0,0.333293,"data, professional translators are typically employed to translate. However, the cost is often very high and thus using this way to produce large quantities of parallel data is economically infeasible, especially for universities and non-profit organizations. In the case of online lectures and talks, subtitles are often translated by crowdsourcing (Behnke et al., 2018) which involves non-professional translators. The resulting translation can thus be often inaccurate and quality control is indispensable. There are many automatic ways to find parallel sentences from roughly parallel documents (Tiedemann, 2016).1 In particular, MT-based approaches are quite desirable because of their simplicity and it is possible to use existing translation models to extract additional parallel data. However, using an MT system trained on data from another domain can give unreliable translations which can lead to parallel data of low quality. In this paper, we propose a new method which combines machine translation and similarities of sentence vector representations to automatically align sentences between roughly aligned document pairs. As we are interested in educational lectures translation, we focus on extractin"
2020.lrec-1.449,C73-2019,0,0.51231,"Missing"
2020.lrec-1.449,C94-2175,0,0.762098,"at can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel corpus in the educational lectures domain, relying on Coursera. Figure 1 gives an overview of our framework, where we assume the availability of in-domain parallel documents (top-left), such as those available from Coursera, and out-of-domain parallel sentences (bottom-right). We give details about the way we pre"
2020.lrec-1.449,W18-1819,0,0.0295138,"ection 4.4., we decided to focus on the training schedule A→AT→ATC→TC→C, where in each stage we use datasets more similar with indomain dataset (Wang et al., 2019a), and thoroughly evaluated all of its sub-paths. We also used T and AC for some contrastive experiments. 5.3. Settings for MT We created a shared sub-word vocabulary for Japanese and English from ASPEC and TED training set using BPE (Sennrich et al., 2016b) with roughly 32k merge operations. This vocabulary was used for all experiments, even when a model is trained only on C. We trained NMT models using the tensor2tensor framework (Vaswani et al., 2018)16 with its default “transformer big” setting, such as dropout=0.2, attention dropout=0.1, optimizer=adam with beta1=0.9, beta2=0.997. We used eight Tesla V100 32GB GPUs with batch size of 4,096 sub-word tokens, and early-stopping on approximate BLEU score computed on the development set: the training process stops when the score shows no gain larger than 0.1 for 10,000 steps. When fine-tuning the model on a different dataset, we resumed the training process from the last checkpoint in the previous stage. In the decoding step, we used the average of the last 10 checkpoints, and decoded the tes"
2020.lrec-1.449,P19-1116,0,0.0265469,"pairs in the descending order of the average similarity of all aligned sentence pairs within each document pair. We then subject these sorted and sentence aligned pairs to human evaluation using Algorithm 1 in order to obtain high-quality test and development sets, where the target volume of each set (volume) and document-level comparability (ratio) are the two parameters. We use the remaining sentence aligned document pairs for training. Our test, development, and training sets are all constructed at the document level and thus our corpora can be used to evaluate document-level translation (Voita et al., 2019; Wang et al., 2019b; Tan et al., 2019). 4. 20 Cleaning Documents Our framework is mostly language independent. The only language specific processes are tokenization and language mismatch detection. We first segmented the both English and Japanese paragraphs with full-stop (“.”), exclamation (“!”), and question marks (“?”) in Latin encoding and their full-width counterparts in UTF-8 followed by a space or the end of line. Then, we tokenized Japanese and English sentences, using Juman++ (Tolmachev et al., 2018)11 and NLTK,12 respectively. Algorithm 2 shows our rule-based language detection proc"
2020.lrec-1.449,P19-1583,0,0.0198191,"allel sentences that are orders of magnitude larger than those that can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel corpus in the educational lectures domain, relying on Coursera. Figure 1 gives an overview of our framework, where we assume the availability of in-domain parallel documents (top-left), such as those available from Coursera, and out-of-domain parallel"
2020.lrec-1.449,P19-1123,0,0.0830708,"ding order of the average similarity of all aligned sentence pairs within each document pair. We then subject these sorted and sentence aligned pairs to human evaluation using Algorithm 1 in order to obtain high-quality test and development sets, where the target volume of each set (volume) and document-level comparability (ratio) are the two parameters. We use the remaining sentence aligned document pairs for training. Our test, development, and training sets are all constructed at the document level and thus our corpora can be used to evaluate document-level translation (Voita et al., 2019; Wang et al., 2019b; Tan et al., 2019). 4. 20 Cleaning Documents Our framework is mostly language independent. The only language specific processes are tokenization and language mismatch detection. We first segmented the both English and Japanese paragraphs with full-stop (“.”), exclamation (“!”), and question marks (“?”) in Latin encoding and their full-width counterparts in UTF-8 followed by a space or the end of line. Then, we tokenized Japanese and English sentences, using Juman++ (Tolmachev et al., 2018)11 and NLTK,12 respectively. Algorithm 2 shows our rule-based language detection procedure for the Japan"
2020.lrec-1.449,P19-1624,0,0.094315,"ding order of the average similarity of all aligned sentence pairs within each document pair. We then subject these sorted and sentence aligned pairs to human evaluation using Algorithm 1 in order to obtain high-quality test and development sets, where the target volume of each set (volume) and document-level comparability (ratio) are the two parameters. We use the remaining sentence aligned document pairs for training. Our test, development, and training sets are all constructed at the document level and thus our corpora can be used to evaluate document-level translation (Voita et al., 2019; Wang et al., 2019b; Tan et al., 2019). 4. 20 Cleaning Documents Our framework is mostly language independent. The only language specific processes are tokenization and language mismatch detection. We first segmented the both English and Japanese paragraphs with full-stop (“.”), exclamation (“!”), and question marks (“?”) in Latin encoding and their full-width counterparts in UTF-8 followed by a space or the end of line. Then, we tokenized Japanese and English sentences, using Juman++ (Tolmachev et al., 2018)11 and NLTK,12 respectively. Algorithm 2 shows our rule-based language detection procedure for the Japan"
2020.lrec-1.449,D16-1163,0,0.0201952,"eral domains. 2.2. https://github.com/shyyhs/CourseraParallelCorpusMining http://mooc.org 4 https://www.coursera.org 5 https://iversity.org 6 https://wit3.fbk.eu/mt.php?release=2017-01-ted-test 7 https://www.opensubtitles.org 3 2.3. Domain Adaptation for Neural Machine Translation At present, neural machine translation (NMT) is known to give higher quality of translation. To train a sequence-tosequence model (Sutskever et al., 2014), attention-based model (Bahdanau et al., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over"
2020.ngt-1.3,I17-1001,0,0.0669869,"Missing"
2020.ngt-1.3,2000.eamt-1.5,0,0.626159,"Missing"
2020.ngt-1.3,W18-6301,0,0.0510352,"Missing"
2020.ngt-1.3,P02-1040,0,0.106902,"he fact that vanilla and our tied-multi models have identical shapes when n and m for encoder and decoder layers are specified. the same number of examples during training,6 we trained the models for 300k iterations, with 1 GPU for the vanilla models and 2 GPUs with batch size halved for our tied-multi model. We averaged the last 10 checkpoints saved every after 1k updates, decoded the test sentences, fixing a beam size7 of 4 and length penalty of 0.6, and post-processed the decoded results using the detokenizer.perl and detruecase.perl in Moses. We evaluated our models using the BLEU metric (Papineni et al., 2002) implemented in sacreBLEU (Post, 2018).8 We also present the time consumed to translate the test data, which includes times for the model instantiation, loading the checkpoints, sub-word splitting and indexing, decoding, and subword de-indexing and merging, whereas times for detokenization are not taken into account. Note that we did not use any development data for two reasons. First, we train all models for the same number of iterations. Second, we use checkpoint averaging before decoding, which does not require development data unlike early stopping. the one for the vanilla model, when perf"
2020.ngt-1.3,N19-1423,0,0.0388521,"he independence of target labels (layer combinations) for a given input sentence allows for ties, the model is able to predict multiple layer combinations for the same input sentence. We implemented the model f with a multi-head self-attention neural network inspired by Vaswani et al. (2017). The number of layers and attention heads are optimized during a hyper-parameter search, while the feed-forward layer dimensionality is fixed to 2,048. Input sequences of tokens are mapped to their corresponding embeddings, initialized by the embedding table of the tied-multi NMT model. Similarly to BERT (Devlin et al., 2019), a specific token is prepended to input sequence before being fed to the classifier. This token is finally fed during the forward pass to the output linear layer for sentence classification. The output linear layer has K dimensions, allowing to output as many logits as the number of layer combinations in the tied-multi NMT model. Finally, a sigmoid function outputs probabilities for each layer combination among the K possible combinations. The parameters θ of the model f are learned using mini-batch stochastic gradient descent with Nesterov momentum (Sutskever et al., 2013) and the loss funct"
2020.ngt-1.3,W16-2341,0,0.115507,"Missing"
2020.ngt-1.3,W18-6319,0,0.0137795,"ve identical shapes when n and m for encoder and decoder layers are specified. the same number of examples during training,6 we trained the models for 300k iterations, with 1 GPU for the vanilla models and 2 GPUs with batch size halved for our tied-multi model. We averaged the last 10 checkpoints saved every after 1k updates, decoded the test sentences, fixing a beam size7 of 4 and length penalty of 0.6, and post-processed the decoded results using the detokenizer.perl and detruecase.perl in Moses. We evaluated our models using the BLEU metric (Papineni et al., 2002) implemented in sacreBLEU (Post, 2018).8 We also present the time consumed to translate the test data, which includes times for the model instantiation, loading the checkpoints, sub-word splitting and indexing, decoding, and subword de-indexing and merging, whereas times for detokenization are not taken into account. Note that we did not use any development data for two reasons. First, we train all models for the same number of iterations. Second, we use checkpoint averaging before decoding, which does not require development data unlike early stopping. the one for the vanilla model, when performing decoding with the 6 encoder and"
2020.ngt-1.3,D18-1457,0,0.0231562,"coder-decoder layer combinations. Section 4 describes our efforts towards designing and evaluating a mechanism for dynamically selecting encoderdecoder layer combinations prior to decoding. Section 5 describes two orthogonal extensions to our model aiming at further model compression and speeding-up of decoding. The paper ends with Section 6 containing conclusion and future work. 2 Related Work There are studies that exploit multiple layers simultaneously. Wang et al. (2018) fused hidden representations of multiple layers in order to improve the translation quality. Belinkov et al. (2017) and Dou et al. (2018) attempted to identify which layer can generate useful representations for different natural language processing tasks. Unlike them, we make all layers of the encoder and decoder usable for decoding with any encoder-decoder layer combination. In practical scenarios, we can save significant amounts of time by choosing shallower encoder and decoder layers for inference. Our method ties the parameters of multiple models, which is orthogonal to the work that ties parameters between layers (Dabre and Fujita, 2019) and/or between the encoder and decoder within a single model (Xia et al., 2019; Dabre"
2020.ngt-1.3,K16-1029,0,0.0613098,"Missing"
2020.ngt-1.3,D16-1139,0,0.308824,"ny encoder-decoder layer combination. In practical scenarios, we can save significant amounts of time by choosing shallower encoder and decoder layers for inference. Our method ties the parameters of multiple models, which is orthogonal to the work that ties parameters between layers (Dabre and Fujita, 2019) and/or between the encoder and decoder within a single model (Xia et al., 2019; Dabre and Fujita, 2019). Parameter tying leads to compact models, but they usually suffer from drops in inference quality. In this paper, we counter such drops with knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016; Freitag et al., 2017). This approach utilizes smoothed data or smoothed training signals instead of the actual training data. A model with a large number of parameters and high per1 Rather than casting the encoder-decoder model into a single column model with (N + M ) layers. 25 formance provides smoothed distributions that are then used as labels for training small models instead of one-hot vectors. As one of the aims in this work is model size reduction, it is related to a growing body of work that addresses the computational requirement reduction. Pruning of pre-trained models (See et al."
2020.ngt-1.3,N04-1022,0,0.101292,"wn in Figure 2, we tackled an advanced problem: dynamic selection of one layer combination prior to decoding.11 4.1 Method We formalize the encoder-decoder layer combination selection with a supervised learning approach 10 We measured the collapsed time for a fair comparison, assuming that all vanilla models were trained on a single GPU one after another, even though one may be able to use multiple GPUs to train the 36 vanilla models in parallel. 11 This is the crucial difference from two post-decoding processes: translation quality estimation (Specia et al., 2010) and n-best-list re-ranking (Kumar and Byrne, 2004). 28 the macro Fβ implemented following (4). where the objective is to minimize the following loss function (2). arg min θ 1 X L(f (si ; θ), tik ), |S |i LiFβ (2) s ∈S  = 1 − (1 + β 2 ) ·  P ·R , (β 2 · P ) + R (4) P P where µ/ k yˆki , R = µ/ k yki , and µ = P iP = yk · yki ). k (ˆ The final loss function is the linear interpolation of LBCE averaged over the K classes and LFβ with parameter λ, both averaged over the batch: λ × LBCE + (1 − λ) × LFβ . We tune α, β, and λ during the classifier hyper-parameter search based on the validation loss. where si is the i-th input sentence (1 ≤ i ≤ |S|"
2020.ngt-1.3,C18-1255,0,0.0209797,"on 2 briefly reviews related work for compressing neural models. Section 3 covers our method that ties multiple models by softmaxing all encoder-decoder layer combinations. Section 4 describes our efforts towards designing and evaluating a mechanism for dynamically selecting encoderdecoder layer combinations prior to decoding. Section 5 describes two orthogonal extensions to our model aiming at further model compression and speeding-up of decoding. The paper ends with Section 6 containing conclusion and future work. 2 Related Work There are studies that exploit multiple layers simultaneously. Wang et al. (2018) fused hidden representations of multiple layers in order to improve the translation quality. Belinkov et al. (2017) and Dou et al. (2018) attempted to identify which layer can generate useful representations for different natural language processing tasks. Unlike them, we make all layers of the encoder and decoder usable for decoding with any encoder-decoder layer combination. In practical scenarios, we can save significant amounts of time by choosing shallower encoder and decoder layers for inference. Our method ties the parameters of multiple models, which is orthogonal to the work that tie"
2020.ngt-1.3,P18-1166,0,0.0400003,"Missing"
2020.tacl-1.46,D19-5617,0,0.283662,"enote R2, of a very poor quality (see Section 2). Consequently, back-translations or forward translations generated this way would be too noisy to train R1↔R2 NMT systems. We verify this assumption in Section 5.2.1. Instead, as illustrated in Figure 4, we use R1→R2 and R2→R1 zero-shot NMT to synthesize parallel data from MR1 and MR2 monolingual data, respectively. Because our NMT system uses a pre-trained language model for R1 and R2, we can expect it to generate better translation than 5 Experiments In this section, we empirically evaluate the usefulness of the parallel data synthesized by 2 Berard et al. (2019a) showed that a large monolingual corpus of UGT can be successfully back-translated with a system trained on PR1-R2 parallel data. 3 Li and Specia (2019) observed improvements using forward translations but only in combination with manually produced PR1-R2 parallel data. 714 our proposed approaches in training better NMT systems for translating UGT. 5.1 Data We conducted experiments for two language pairs, English–French (en-fr) and English–Japanese (en-ja), with the MTNT translation tasks (Michel and Neubig, 2018). The test sets were made from posts extracted from an online discussion Web si"
2020.tacl-1.46,N19-1423,0,0.00919971,"Zero-Shot NMT For a given language pair L1-L2, we require only one multilingual and multidirectional NMT system to synthesize parallel data. The compo1 We do not consider L1→R1 and L2→R2 (see Section 4.1). 712 nents of this system are presented in Figure 2. Inspired by previous work in unsupervised NMT (Conneau and Lample, 2019), we first pretrain a cross-lingual language model to initialize the NMT system. We use the XLM approach (Conneau and Lample, 2019) trained with the combination of the following two different objectives: Masked Language Model (MLM): MLM has a similar objective to BERT (Devlin et al., 2019) but uses text streams for training instead of pairs of sentences. We optimize the MLM objective on the ML1 , ML2 , MR1 , and MR2 monolingual data. Translation Language Model (TLM): TLM is an extension of MLM where parallel data are leveraged so that we can rely on context in two different languages to predict masked words. We optimize the TLM objective on PL1-L2 parallel data, alternatively exploiting both translation directions. The XLM approach alternates between MLM and TLM objectives to train a single model. By sharing a single vocabulary for all of L1, L2, R1, and R2, we expect XLM to im"
2020.tacl-1.46,P13-2121,0,0.0420545,"Missing"
2020.tacl-1.46,J86-2003,0,0.0721487,"Missing"
2020.tacl-1.46,C04-1072,0,0.0154502,"ual data. 4.1 Parallel Data Alteration There exist several methods to synthesize parallel data of UGT from existing parallel data in various style or domains, but mostly requiring the use of UGT parallel data. Vaibhav et al. (2019) 713 translation for each of L1 and L2 sentences, respectively, to obtain a synthetic R1-R2 version, that is, PSR1-R2 , of the original PL1-L2 . The resulting PSR1-R2 can be too noisy to be used to train NMT. To filter PSR1-R2 , we evaluate the similarity between original L1 and L2 sentences with their respective R1 and R2 versions using sentence-level BLEU (Lin and Och, 2004) (sBLEU). Given a sentence pair in PSR1-R2 , if either sBLEU of L1 with respect to R1 or sBLEU of L2 with respect to R2 is below a predetermined threshold T , we filter out the sentence pair, consider that it has been too much altered. T can be set empirically: Create several version of PSR1-R2 using different T values, train an NMT system for each version, and choose the value that leads to the NMT system achieving the best BLEU score on some PL1-L2 validation data. Finally, after filtering, we exploit the resulting S PR1-R2 by concatenating it to the original PL1-L2 parallel data and train a"
2020.tacl-1.46,P07-2045,0,0.00869252,"titles) corpora, resulting in a total of 3.9 M sentence pairs. All PL1-L2 parallel data can be considered rather clean and/or formal in contrast to Reddit data. As monolingual data, ML1 and ML2 , we used the entire News Crawl provided for WMT204 for Japanese, 3.4M lines, and a sample of 25M lines for English and French. As MR1 and MR2 , we crawled data using the Reddit API and applied fastText5 for language identification.6 As preprocessing steps for English and French, we first normalized the punctuation of all the data, except for the reference translations in the test sets, with the Moses (Koehn et al., 2007)7 punctuation normalizer, and then tokenized all the data with the Moses tokenizer. Finally, we truecased the data with the Moses truecaser trained on the Reddit monolingual data. As for Japanese, we only tokenized the data with MeCab.8 We removed all empty lines and lines longer than 120 tokens from the monolingual and parallel data. Because we could crawled plenty of English data (595M lines) on Reddit, we only selected its noisiest part, similarly to Michel and Neubig (2018) when they built the MTNT dataset. We trained a language model on the English News Crawl monolingual data using LMPLZ"
2020.tacl-1.46,P16-1162,0,0.136765,"Missing"
2020.tacl-1.46,P12-1000,0,0.237626,"Missing"
2020.tacl-1.46,P18-1080,0,0.0246722,"systems trained on parallel data of UGT. To the best of our knowledge, Vaibhav et al. (2019) proposed the only approach that synthesizes parallel data of UGT without relying on existing parallel data of UGT. Having obtained texts in the target style of UGT, they designed editing operations to make existing parallel data in other styles more similar to the targeted style. Another line of work exploits NMT to perform style transfer across texts, that is, applying some characteristics of one text to another, without exploiting any parallel data of UGT, but has never been applied to NMT for UGT. Prabhumoye et al. (2018) performed style transfer through back-translation to preserve the meaning of the text while reducing its stylistic properties and then exploit adversarial generation algorithms to apply the desired style to the back-translated texts, assuming that meaning and style can be disentangled. Their approach also requires a classifier that can accurately predict the style of a given text. Zhang et al. (2018) proposed a threestep pipeline combining unsupervised statistical and neural MT to generate instances of texts in the targeted style that is then evaluated by a given style classifier as in Prabhu"
2020.tacl-1.46,2020.wmt-1.63,0,0.0132535,"racteristics of UGT of the targeted style. Our second method (#2) uses the same zero-shot NMT system to translate monolingual corpora of UGT for synthesizing parallel data useful to train NMT. We showed that both methods, separately or combined, improve translation quality for UGT. For future work, we will study the use of manually produced UGT parallel data to better train our NMT system that synthesizes the parallel data. We will also explore other applications for this framework, such as paraphrase generation. We will also investigate the use of the recently proposed mirror-generative NMT (Zheng et al., 2020), a semi-supervised architecture that exploits jointly large source and target monolingual corpora, such as those of UGT, during training using source and target language models in the same latent space. Acknowledgments We would like to thank the action editor, Philipp Koehn, and reviewers for their useful comments and suggestions. A part of this work was conducted under the program ‘‘Research and Development of Enhanced Multilingual and Multipurpose Speech Translation System’’ of the Ministry of Internal Affairs and Communications (MIC), Japan. Benjamin Marie was partly supported by JSPS KAKE"
2020.tacl-1.46,P16-1009,0,0.121294,"tly observed better results than when PSR1-R2 is used as forward translations.3 Note also that we do not filter the synthesized data and use all the data generated from the monolingual data, in contrast to another approach presented in Section 4.1. We could potentially obtain better results by filtering synthetic parallel data with some existing methods proposed, for instance, for filtering backtranslations (Imankulova et al., 2019). We leave the investigation of such filtering techniques for future work. Previous work also proposed to synthesize parallel data from monolingual data using NMT (Sennrich et al., 2016a): An L1→L2 NMT system is used to translate ML1 monolingual data into L2, and then the synthesized PSL1-L2 parallel data are concatenated to original parallel data and used to train new L2→L1 (back-translation) or L1→L2 (forward translation) NMT systems. However, to the best of our knowledge, nobody has studied the use of large UGT monolingual data, without any manually produced PR1-R2 parallel data, and its impact on translation quality.2 In our scenario, translating R1 texts with an L1→L2 would lead to translations of R1, that we can denote R2, of a very poor quality (see Section 2). Conseq"
2020.wmt-1.23,D18-1338,0,0.0178053,"t BLEU scores on this data allow for checkpoint saving. The parameters for decoding were fixed: a beam size of 4 and a length penalty of 0.6. • Big: 1024 embedding dimensions, 4,096 dimensions for the feed-forward, and 16 heads Highway Transformer Residual connections (RCs) (Srivastava et al., 2015a; He et al., 2016) have been shown to increase forward and backward information flow in deep neural networks (Hardt and Ma, 2017) and thus are a crucial component of the Transformer architecture. Removing them has a negative impact on training and on the overall performances of the resulting model (Bapna et al., 2018). However, incorporating RCs through the addition operation as it is commonly done in the Transformer network does not allow for a distribution of weights between carrying or transforming the input. An alternative, inspired by the Highway Network (Srivastava et al., 2015b) and implemented within the Transformer by Chai et al. (2020), includes a trainable gating mechanism that regulates the information flow. We applied a few modifications to the implementation proposed in Chai et al. (2020): removing all layer normalization operations, adding depth-aware parameter initialization (Junczys-Dowmun"
2020.wmt-1.23,P19-1309,0,0.0132665,"for Pl→En and Ja→En. Our results for En→Pl across all configurations remained similar, which defies the findings of previous work on back-translation and forward translation. We give the provided parallel data using Bivec (Luong et al., 2015). 2. Make all possible bilingual sentence pairs from News Crawl corpora in the source and target languages. 3. For each sentence pair, compute the similarity between the source and target sentences using the bilingual word embeddings trained with Bivec simply by measuring cosine similarity over the averaged word embeddings in each sentence as proposed by Artetxe and Schwenk (2019).6 4. Finally, keep only the sentence pairs with a score higher than a threshold among {1.0, 1.0025, 1.05} and select the value that results in the sentence pairs leading to the highest BLEU score on the validation data when mixing the selected sentence pairs with the original parallel data for training NMT. Table 3 gives an overview of the results obtained with the additional sentence pairs extracted from News Crawl. We did not observe significant improvements as we could only extract a very small amount of useful sentence pairs. Nevertheless, we decided to keep these additional data to train"
2020.wmt-1.23,W09-0432,0,0.0381508,"We did not observe significant improvements as we could only extract a very small amount of useful sentence pairs. Nevertheless, we decided to keep these additional data to train our other NMT systems, since it did not appear harmful according to BLEU. However, as we report in Section 8, it was not the optimal choice to obtain the best results on the test data. 5.2 Backward and Forward Translation of Monolingual Data Parallel data for training NMT can be augmented with synthetic parallel data, generated through a so-called back(ward)-translation, to significantly improve translation quality (Bertoldi and Federico, 2009; Bojar and Tamchyna, 2011; Sennrich et al., 2016a). We used the Fairseq Big system, trained on the provided parallel data and 6 We used the “Ratio” version of the scoring function. 233 System #sent. pairs En→Pl Pl→En Ja→En 0 24.4 29.1 18.1 BT 12.5M 25M 50M 26.1 26.2 26.3 32.1 32.1 31.7 21.1 21.2 21.1 TBT 12.5M 25M 50M 26.1 26.1 26.3 30.3 32.3 32.2 21.1 21.2 21.4 FWD 12.5M 25M 50M 26.3 26.4 26.3 29.7 29.5 29.6 18.3 17.4 16.4 Marian Base Table 4: Results of Marian Base on the validation data obtained using synthetic parallel data as back-translations (BT), tagged back-translations (TBT) or forw"
2020.wmt-1.23,Q17-1010,0,0.00778064,"r En–Pl En–Ja 8.7M 15.2M #tokens 239.5M (En) 394.5M (En) 310.0M (Pl) 380.6M (Ja) Table 1: Statistics of our pre-processed parallel data. corpora but also sampled only 200M lines from the “Common Crawl” corpora. To tune/validate and evaluate our systems, we used the official validation and test data provided by the organizers. 3.2 Pre-processing and Cleaning Since some corpora were crawled from the Web and therefore potentially very noisy, we first performed language identification on all the data to keep only lines that have a high probability of being in the right language. We used fastText (Bojanowski et al., 2017) and its large model for language identification.5 We only retained sentences that have a probability higher than 0.75 to be in the right language. For the parallel data, if at least one side of each sentence pair did not match this criteria, we removed the pair from the corpus. We used Moses (Koehn et al., 2007) punctuation normalizer, tokenizer, and truecaser for English and Polish. The truecaser was trained on the News Crawl 2019 corpora. Truecasing was then performed on all the tokenized data. Then, for the Pl–En language pair, we jointly learned 32k BPE operations (Sennrich et al., 2016b)"
2020.wmt-1.23,P07-2045,0,0.0223191,"Missing"
2020.wmt-1.23,W19-5206,0,0.0208394,"llel data extracted from the News Crawl monolingual corpora, denoted “NC.” The columns “#sent. pairs” indicate how many sentence pairs were extracted from the News Crawl monolingual corpora. the aligned News Crawl sentence pairs, to translate target monolingual sentences into the source language. Then, these back-translated sentences were simply mixed with the original parallel data, putting the synthetic side on the source side, to train from scratch a new NMT system. We also experimented with forward translation, i.e., with the synthetic part on the target side, and tagged back-translation (Caswell et al., 2019), which simply adds a tag at the beginning of each back-translation, as it has shown to lead to better results, especially when translating texts in their original language (Marie et al., 2020). For English, we translated 50M sentences made of the entire News Crawl 2019 corpus and randomly added sentences from News Crawl 2018 corpus until we have 50M sentences. For Polish and Japanese, we translated the entire News Crawl corpora and added sentences from the Common Crawl corpus until we have 50M sentences. For each configuration, i.e., back-translation, tagged back-translation, and forward tran"
2020.wmt-1.23,2020.acl-main.616,0,0.015196,"to increase forward and backward information flow in deep neural networks (Hardt and Ma, 2017) and thus are a crucial component of the Transformer architecture. Removing them has a negative impact on training and on the overall performances of the resulting model (Bapna et al., 2018). However, incorporating RCs through the addition operation as it is commonly done in the Transformer network does not allow for a distribution of weights between carrying or transforming the input. An alternative, inspired by the Highway Network (Srivastava et al., 2015b) and implemented within the Transformer by Chai et al. (2020), includes a trainable gating mechanism that regulates the information flow. We applied a few modifications to the implementation proposed in Chai et al. (2020): removing all layer normalization operations, adding depth-aware parameter initialization (Junczys-Dowmunt, 2019; Zhang et al., 2019), and initializing biases so that the residual blocks are initially forced to carry information rather than transforming it (Srivastava et al., 2015b). 4.2 5 Training Data Augmentation 5.1 Parallel Data Alignment We extracted additional training parallel data from the News Crawl monolingual corpora with t"
2020.wmt-1.23,D18-2012,0,0.017899,"ormalizer, tokenizer, and truecaser for English and Polish. The truecaser was trained on the News Crawl 2019 corpora. Truecasing was then performed on all the tokenized data. Then, for the Pl–En language pair, we jointly learned 32k BPE operations (Sennrich et al., 2016b) on the concatenation of English and Polish News Crawl 2019 corpora. We performed sub-word segmentation using this vocabulary on the Polish and English parallel and monolingual data. For the Ja– En language pair, we independently learned 32k BPE operations on the English News Crawl 2019 corpus for English, 32k sentence piece (Kudo and Richardson, 2018) operations on the Japanese News Crawl 2019 corpus for Japanese, and then applied the operations to perform sub-word segmentation on the data in their respective language. For further cleaning of the data, we applied the script “clean-corpus-n.perl” from Moses to remove empty lines and sentences longer than 120 sub-word tokens. Tables 1 and 2 present the statistics of the parallel and monolingual data, respectively, after pre-processing. Data Pre-processing and Cleaning 3.1 #sent. pairs Data As parallel data to train our systems, we used all the provided data for all our targeted translation d"
2020.wmt-1.23,N12-1047,0,0.0207236,"translation probabilities, for both translation directions Scores given by a 4-gram language model trained on all the monolingual corpora in the target language Difference between the length of the source sentence and the length of the translation hypothesis, and its absolute value Table 5: Set of features used by our reranking systems. The “Feature” column refers to the same feature used in Marie and Fujita (2018). The numbers between parentheses indicate the number of scores in each feature set. some plausible explanations for this peculiarity in our analysis in Section 8. 6 chose KB-MIRA (Cherry and Foster, 2012) as a rescoring framework and used a subset of the features proposed in Marie and Fujita (2018). All the following features we used are described in detail by Marie and Fujita (2018). As listed in Table 5, it includes all scores given by all our NMT models. We computed sentence-level translation probabilities using the lexical translation probabilities learned by mgiza on all the parallel training data of our NMT systems. One 4-gram language model trained on the target language model was also used. To account for hypotheses length, we added the difference, and its absolute value, between the n"
2020.wmt-1.23,W15-1521,0,0.0107916,"erences in BLEU when increasing the size of the back-translated data. TBT improves over BT as expected (Caswell et al., 2019), but only for Pl→En and Ja→En. On the other hand, using forward translations significantly decreased BLEU scores, as expected, since it introduces NMT translations to the target side of the training data (Bogoychev and Sennrich, 2019), but again only for Pl→En and Ja→En. Our results for En→Pl across all configurations remained similar, which defies the findings of previous work on back-translation and forward translation. We give the provided parallel data using Bivec (Luong et al., 2015). 2. Make all possible bilingual sentence pairs from News Crawl corpora in the source and target languages. 3. For each sentence pair, compute the similarity between the source and target sentences using the bilingual word embeddings trained with Bivec simply by measuring cosine similarity over the averaged word embeddings in each sentence as proposed by Artetxe and Schwenk (2019).6 4. Finally, keep only the sentence pairs with a score higher than a threshold among {1.0, 1.0025, 1.05} and select the value that results in the sentence pairs leading to the highest BLEU score on the validation da"
2020.wmt-1.23,2020.acl-main.253,0,0.032647,"ven the BLEU score on the validation data in the original language. It means that the translationese texts in the validation data had a negative impact on all our decisions for selecting the best framework, architecture, additional parallel sentences, and so forth, and that we could potentially had better results by taking our decisions by using only the original texts in the validation data. Translationese texts are particularly harmful for training a Reranker, as we can observe for Pl→En. Using them as training data for the • Back-translations should decrease BLEU scores for original texts (Edunov et al., 2020). • Tagged back-translations should improve BLEU scores for original texts (Marie et al., 2020). • Forward translation should lead to lower BLEU scores for translationese texts (Bogoychev and Sennrich, 2019). A possible explanation is that the texts denoted as “original” in the validation data and the test data, 235 # System Arch. NC BT TBT Orig. 1 2 3 4 5 6 Marian Marian Fairseq Fairseq Fairseq Fairseq Base Base Base Highway Big Big 7 8 9 Marian Fairseq Fairseq Big Base Big X X X 10 11 12 Marian Fairseq Fairseq Big Base Big X X X 13 Reranker∗ 14 15 Reranker Non-orig. Reranker Orig. En→Pl Vali"
2020.wmt-1.23,W18-1811,1,0.787407,"ws Translation Task.1 We participated in three translation directions: Japanese→English (Ja→En), English→Polish (En→Pl), and Polish→English (Pl→En). All our systems are constrained, i.e., we used only the parallel and monolingual data provided by the organizers to train and tune them, and validated/selected our best systems exclusively using the official validation data provided by the organizers. We trained NMT systems with several different frameworks and architectures, and combined them, for each translation direction, through n-best list reranking using informative features as proposed by Marie and Fujita (2018). This simple combination method, associated with the exploitation of large tagged back-translated monolingual data, improved BLEU scores on the official 1 2 Description of the Task The task is to translate texts in the news domain. For this purpose, news articles were sampled from online newspapers from September– November 2019. The sources of the test data are original texts whereas the targets are humanproduced translations, i.e., participants are not asked to translate translationese texts unlike past WMT translation tasks. Although organizers also mentioned that the provided validation da"
2020.wmt-1.23,2020.acl-main.532,1,0.903164,"aligned News Crawl sentence pairs, to translate target monolingual sentences into the source language. Then, these back-translated sentences were simply mixed with the original parallel data, putting the synthetic side on the source side, to train from scratch a new NMT system. We also experimented with forward translation, i.e., with the synthetic part on the target side, and tagged back-translation (Caswell et al., 2019), which simply adds a tag at the beginning of each back-translation, as it has shown to lead to better results, especially when translating texts in their original language (Marie et al., 2020). For English, we translated 50M sentences made of the entire News Crawl 2019 corpus and randomly added sentences from News Crawl 2018 corpus until we have 50M sentences. For Polish and Japanese, we translated the entire News Crawl corpora and added sentences from the Common Crawl corpus until we have 50M sentences. For each configuration, i.e., back-translation, tagged back-translation, and forward translation, we also experimented with sub-samples of 12.5M (only with Marian), and 25M synthetic sentence pairs, in addition to using the entire 50M sentence pairs, for retraining the NMT systems."
2020.wmt-1.23,P16-1009,0,0.0937486,"ojanowski et al., 2017) and its large model for language identification.5 We only retained sentences that have a probability higher than 0.75 to be in the right language. For the parallel data, if at least one side of each sentence pair did not match this criteria, we removed the pair from the corpus. We used Moses (Koehn et al., 2007) punctuation normalizer, tokenizer, and truecaser for English and Polish. The truecaser was trained on the News Crawl 2019 corpora. Truecasing was then performed on all the tokenized data. Then, for the Pl–En language pair, we jointly learned 32k BPE operations (Sennrich et al., 2016b) on the concatenation of English and Polish News Crawl 2019 corpora. We performed sub-word segmentation using this vocabulary on the Polish and English parallel and monolingual data. For the Ja– En language pair, we independently learned 32k BPE operations on the English News Crawl 2019 corpus for English, 32k sentence piece (Kudo and Richardson, 2018) operations on the Japanese News Crawl 2019 corpus for Japanese, and then applied the operations to perform sub-word segmentation on the data in their respective language. For further cleaning of the data, we applied the script “clean-corpus-n."
2020.wmt-1.23,W19-5321,0,0.0190502,"et al., 2018). However, incorporating RCs through the addition operation as it is commonly done in the Transformer network does not allow for a distribution of weights between carrying or transforming the input. An alternative, inspired by the Highway Network (Srivastava et al., 2015b) and implemented within the Transformer by Chai et al. (2020), includes a trainable gating mechanism that regulates the information flow. We applied a few modifications to the implementation proposed in Chai et al. (2020): removing all layer normalization operations, adding depth-aware parameter initialization (Junczys-Dowmunt, 2019; Zhang et al., 2019), and initializing biases so that the residual blocks are initially forced to carry information rather than transforming it (Srivastava et al., 2015b). 4.2 5 Training Data Augmentation 5.1 Parallel Data Alignment We extracted additional training parallel data from the News Crawl monolingual corpora with the following procedure: Frameworks and Settings Marian Our Models trained with the Marian toolkit (Junczys-Dowmunt et al., 2018) were only 1. Jointly train bilingual word embeddings on 232 Configuration w/o NC w/ NC En→Pl 26.3 26.4 En–Pl Pl→En #sent. pairs 30.4 30.6 0 257."
2020.wmt-1.23,P16-1162,0,0.261499,"ojanowski et al., 2017) and its large model for language identification.5 We only retained sentences that have a probability higher than 0.75 to be in the right language. For the parallel data, if at least one side of each sentence pair did not match this criteria, we removed the pair from the corpus. We used Moses (Koehn et al., 2007) punctuation normalizer, tokenizer, and truecaser for English and Polish. The truecaser was trained on the News Crawl 2019 corpora. Truecasing was then performed on all the tokenized data. Then, for the Pl–En language pair, we jointly learned 32k BPE operations (Sennrich et al., 2016b) on the concatenation of English and Polish News Crawl 2019 corpora. We performed sub-word segmentation using this vocabulary on the Polish and English parallel and monolingual data. For the Ja– En language pair, we independently learned 32k BPE operations on the English News Crawl 2019 corpus for English, 32k sentence piece (Kudo and Richardson, 2018) operations on the Japanese News Crawl 2019 corpus for Japanese, and then applied the operations to perform sub-word segmentation on the data in their respective language. For further cleaning of the data, we applied the script “clean-corpus-n."
2020.wmt-1.23,P18-4020,0,0.0416571,"Missing"
2020.wmt-1.23,D19-1083,0,0.0437447,"Missing"
2020.wmt-1.61,P17-2061,1,0.934704,"nly on SD. However, its impact on ELR settings is uncertain. Given that only few thousands of domain-specific sentences are available, training large NMT models tends to over-fit on the small corpora while compact NMT models will only lead to pseudo-targets of poor quality, both preventing the generation of useful distilled corpora. It is certainly possible to search for an optimal model size. However, it will involve a time-consuming hyper-parameter search, while the result may be specific to given corpora. 2.2 Transfer Learning Transfer learning (TL) can be in the form of domain adaptation (Chu et al., 2017) or cross-lingual or multilingual transfer (Firat et al., 2016; Zoph et al., 2016; Dabre et al., 2019; Johnson et al., 2017; Dabre et al., 2020) using helping bilingual corpora. Assume that L1 –L2 is an ELR language pair and L3 –L4 is a helping pair. The given parallel corpora 493 for the two pairs may belong to different domains. Typically, pre-training a model on the larger L3 –L4 corpus and then fine-tuning (“ft”) it on the smaller L1 –L2 corpus is known to give the best translation quality for the L1 –L2 pair (Zoph et al., 2016; Chu et al., 2017; Dabre et al., 2019), regardless of the numb"
2020.wmt-1.61,D19-1146,1,0.843886,"-specific sentences are available, training large NMT models tends to over-fit on the small corpora while compact NMT models will only lead to pseudo-targets of poor quality, both preventing the generation of useful distilled corpora. It is certainly possible to search for an optimal model size. However, it will involve a time-consuming hyper-parameter search, while the result may be specific to given corpora. 2.2 Transfer Learning Transfer learning (TL) can be in the form of domain adaptation (Chu et al., 2017) or cross-lingual or multilingual transfer (Firat et al., 2016; Zoph et al., 2016; Dabre et al., 2019; Johnson et al., 2017; Dabre et al., 2020) using helping bilingual corpora. Assume that L1 –L2 is an ELR language pair and L3 –L4 is a helping pair. The given parallel corpora 493 for the two pairs may belong to different domains. Typically, pre-training a model on the larger L3 –L4 corpus and then fine-tuning (“ft”) it on the smaller L1 –L2 corpus is known to give the best translation quality for the L1 –L2 pair (Zoph et al., 2016; Chu et al., 2017; Dabre et al., 2019), regardless of the number of model parameters. However, without careful regularization, this will definitely lead to the L1"
2020.wmt-1.61,N19-1423,0,0.017733,"ts size matches to the L3 –L4 corpus. Also, we can prepend the source sentences with two artificial tokens, one indicating the domain of the corpus (Chu et al., 2017), and another indicating the target language into which we want to translate (Johnson et al., 2017). Note that when L2 and L4 are the same, the target language tokens are unnecessary. If L1 and L3 are also the same, then we are essentially performing domain adaptation. translation (Sennrich et al., 2016), where target language monolingual data are translated into pseudosource sentences. Recently, pre-training on monolingual data (Devlin et al., 2019; Song et al., 2019; Mao et al., 2020) has been proven to significantly improve the translation quality of ELR settings. Approaches involving helping monolingual data are usually more time-consuming than those that use helping bilingual corpora. Furthermore, given that our approach already needs a reasonable amount of time due to the application of TL and forwardtranslation of the source sentences of the parallel corpora for distilling them, we consider that such approaches should be used when no more gains can be obtained from helping bilingual corpora. We refer interested readers to work on"
2020.wmt-1.61,2020.ngt-1.12,0,0.0224982,"ation (Courbariaux et al., 2017), and binary code prediction softmax (Oda et al., 2017) where the softmax is sped up by making it predict a binary code representing words instead of one-hot vectors. We expect these methods to further speed up the models obtained using our proposed method. 2.3 3 Mixed Training (“mxt”): Directly train on the concatenated corpus. Mixed Fine-Tuning (“mxft”): First train on the L3 –L4 corpus as in “ft,” but perform finetuning on the concatenated corpus. Other Related Work Some recent work tackled efficient NMT modeling in low-resource settings (Goyal et al., 2020; Gordon and Duh, 2020). Whereas they focus on applications of TL for compact models as this paper, there are some key differences between them and ours. Gordon and Duh (2020) focus on lowresource settings, but our low-resource data are significantly smaller than theirs. Second, whereas they use distillation twice and TL once, we recommend distillation once and TL twice. Finally, they do not examine cross-lingual TL for model compression. Goyal et al. (2020) focus on cross-lingual learning, but their approaches are centered more on leveraging orthographic or linguistic similarity, whereas we make no efforts towards"
2020.wmt-1.61,2020.acl-srw.22,0,0.127717,"essive model binarization (Courbariaux et al., 2017), and binary code prediction softmax (Oda et al., 2017) where the softmax is sped up by making it predict a binary code representing words instead of one-hot vectors. We expect these methods to further speed up the models obtained using our proposed method. 2.3 3 Mixed Training (“mxt”): Directly train on the concatenated corpus. Mixed Fine-Tuning (“mxft”): First train on the L3 –L4 corpus as in “ft,” but perform finetuning on the concatenated corpus. Other Related Work Some recent work tackled efficient NMT modeling in low-resource settings (Goyal et al., 2020; Gordon and Duh, 2020). Whereas they focus on applications of TL for compact models as this paper, there are some key differences between them and ours. Gordon and Duh (2020) focus on lowresource settings, but our low-resource data are significantly smaller than theirs. Second, whereas they use distillation twice and TL once, we recommend distillation once and TL twice. Finally, they do not examine cross-lingual TL for model compression. Goyal et al. (2020) focus on cross-lingual learning, but their approaches are centered more on leveraging orthographic or linguistic similarity, whereas we m"
2020.wmt-1.61,D16-1139,0,0.342272,"y but needs more computation and is unacceptable in a lowlatency real-time application where faster decoding is as valuable as if not more valuable than translation quality. Consequently, neural models that are compact and fast are extremely important and a growing body of research known as neural model efficiency focuses on this issue. One of the most popular techniques to train efficient models is knowledge distillation (Hinton et al., 2015) which relies on transferring the knowledge learned by a large model (called teacher) into a smaller model (called student). Sequence distillation (SD) (Kim and Rush, 2016) is a special case of knowledge distillation for sequence-to-sequence models, such as those used for NMT. Not only does it help in the training of compact and fast models with high translation quality, it sometimes helps in eliminating the need for beam search which further increases decoding speed. SD relies on the creation of distilled parallel corpora by translating the training source sentences into the target language by using a large model. The distilled corpora are simplified representations of how the large model sees the original corpora and their quality will have a direct impact on"
2020.wmt-1.61,L18-1548,0,0.0295442,"of different TL settings on the quality of distilled ELR corpora and hence the compact models trained. #2. With ELR and helping corpora: To determine the settings using both ELR and helping corpora that give compact models with highest possible translation quality. 4.1 Datasets We experimented with the ELR Vietnamese– English (Vi–En) and Hindi–English (Hi–E) pairs from the Asian Languages Treebank (ALT) with 18,088 training, 1,000 development, and 1,018 test sentence pairs. As for the helping corpora, we used the training part of the IWSLT 2015 Vietnamese– English3 and the IITB Hindi–English (Kunchukuttan et al., 2018),4 consisting of 133k and 1.5M lines, respectively. We chose large as well as small helping corpora in order to determine the impact of helping corpora sizes on the model training. 4.2 Implementation Details We used the Transformer model for our experiments (Vaswani et al., 2017) because it gives the state-of-the-art results for NMT. We made necessary changes to the code in the tensor2tensor v1.14 implementation of the Transformer in order to construct joint sub-word vocabularies as well as to handle oversampling. Tensor2tensor has its own default sub-word vocabulary learning method which we u"
2020.wmt-1.61,2020.lrec-1.454,1,0.826073,"so, we can prepend the source sentences with two artificial tokens, one indicating the domain of the corpus (Chu et al., 2017), and another indicating the target language into which we want to translate (Johnson et al., 2017). Note that when L2 and L4 are the same, the target language tokens are unnecessary. If L1 and L3 are also the same, then we are essentially performing domain adaptation. translation (Sennrich et al., 2016), where target language monolingual data are translated into pseudosource sentences. Recently, pre-training on monolingual data (Devlin et al., 2019; Song et al., 2019; Mao et al., 2020) has been proven to significantly improve the translation quality of ELR settings. Approaches involving helping monolingual data are usually more time-consuming than those that use helping bilingual corpora. Furthermore, given that our approach already needs a reasonable amount of time due to the application of TL and forwardtranslation of the source sentences of the parallel corpora for distilling them, we consider that such approaches should be used when no more gains can be obtained from helping bilingual corpora. We refer interested readers to work on distillation using unsupervised method"
2020.wmt-1.61,P17-1079,0,0.0217168,"sider that such approaches should be used when no more gains can be obtained from helping bilingual corpora. We refer interested readers to work on distillation using unsupervised methods (Sun et al., 2020). Independent of the application of TL, there exist methods for speeding up NMT, such as weight pruning (See et al., 2016) where model weights close to zero are pruned out, quantization (Lin et al., 2016) where weights are represented by faster to process integers instead of floating point numbers, aggressive model binarization (Courbariaux et al., 2017), and binary code prediction softmax (Oda et al., 2017) where the softmax is sped up by making it predict a binary code representing words instead of one-hot vectors. We expect these methods to further speed up the models obtained using our proposed method. 2.3 3 Mixed Training (“mxt”): Directly train on the concatenated corpus. Mixed Fine-Tuning (“mxft”): First train on the L3 –L4 corpus as in “ft,” but perform finetuning on the concatenated corpus. Other Related Work Some recent work tackled efficient NMT modeling in low-resource settings (Goyal et al., 2020; Gordon and Duh, 2020). Whereas they focus on applications of TL for compact models as t"
2020.wmt-1.61,P02-1040,0,0.10643,"ich further increases decoding speed. SD relies on the creation of distilled parallel corpora by translating the training source sentences into the target language by using a large model. The distilled corpora are simplified representations of how the large model sees the original corpora and their quality will have a direct impact on the translation quality of compact models trained with them. While SD is known to perform extremely well for high-resource settings, its direct application to extremely low-resource (ELR) settings will not work due to over-fitting. Table 1 gives the BLEU scores (Papineni et al., 2002) for Vietnamese– English (Vi–En) and Hindi–English (Hi–En) translation tasks in the Asian Languages Treebank (ALT) (Riza et al., 2016),1 where Transformer Base models (Vaswani et al., 2017) with 1, 2, 3, and 6 encoder and decoder layers were trained on the ALT training data of 18k sentence pairs. It is clear that there is a huge performance gap between the 1 http://www2.nict.go.jp/astrec-att/member/mutiyama/ ALT/ALT-Parallel-Corpus-20191206.zip 492 Proceedings of the 5th Conference on Machine Translation (WMT), pages 492–502 c Online, November 19–20, 2020. 2020 Association for Computational Li"
2020.wmt-1.61,K16-1029,0,0.0536086,"Missing"
2020.wmt-1.61,P16-1009,0,0.0423866,"ng the following two methods on both corpora has been studied: Prior to concatenating two corpora, the L1 –L2 corpus is typically oversampled so that its size matches to the L3 –L4 corpus. Also, we can prepend the source sentences with two artificial tokens, one indicating the domain of the corpus (Chu et al., 2017), and another indicating the target language into which we want to translate (Johnson et al., 2017). Note that when L2 and L4 are the same, the target language tokens are unnecessary. If L1 and L3 are also the same, then we are essentially performing domain adaptation. translation (Sennrich et al., 2016), where target language monolingual data are translated into pseudosource sentences. Recently, pre-training on monolingual data (Devlin et al., 2019; Song et al., 2019; Mao et al., 2020) has been proven to significantly improve the translation quality of ELR settings. Approaches involving helping monolingual data are usually more time-consuming than those that use helping bilingual corpora. Furthermore, given that our approach already needs a reasonable amount of time due to the application of TL and forwardtranslation of the source sentences of the parallel corpora for distilling them, we con"
2020.wmt-1.61,2020.acl-srw.37,1,0.833832,"our distilled corpora can be used in situations where quick deployment of compact and fast NMT models is important. 5.1.2 Domain Adaptation vs. Cross-Lingual Transfer Our experiment revealed that cross-lingual training is definitely a viable alternative. For instance, in Vi→En translation, the best BLEU score was achieved when the helping direction was also Vi→En. When the helping direction was Hi→En, these improvements were much smaller. Nevertheless, it is clear that cross-lingual training is useful when domain adaptation is not possible. Work on script mapping to improve the quality of TL (Song et al., 2020; Goyal et al., 2020) indicates that our cross-lingual distillation procedure might give better results if we mapped Hi to Vi or vice-versa. We leave this for future work. Consider two hypothetical settings for Vi→En translation, where we used the reversed, En→Vi and En→Hi, helping directions to generate distilled corpora for Vi→En translation. When using En→Vi as the helping direction, the BLEU scores of greedy search with 1-, 2-, and 3-layer models improved by 4.3, 4.4, and 5.6 BLEU points, respectively. These improvements are approximately 1.0 BLEU points lower than those obtained in the do"
2020.wmt-1.61,2020.acl-main.324,0,0.0244345,"s been proven to significantly improve the translation quality of ELR settings. Approaches involving helping monolingual data are usually more time-consuming than those that use helping bilingual corpora. Furthermore, given that our approach already needs a reasonable amount of time due to the application of TL and forwardtranslation of the source sentences of the parallel corpora for distilling them, we consider that such approaches should be used when no more gains can be obtained from helping bilingual corpora. We refer interested readers to work on distillation using unsupervised methods (Sun et al., 2020). Independent of the application of TL, there exist methods for speeding up NMT, such as weight pruning (See et al., 2016) where model weights close to zero are pruned out, quantization (Lin et al., 2016) where weights are represented by faster to process integers instead of floating point numbers, aggressive model binarization (Courbariaux et al., 2017), and binary code prediction softmax (Oda et al., 2017) where the softmax is sped up by making it predict a binary code representing words instead of one-hot vectors. We expect these methods to further speed up the models obtained using our pro"
2020.wmt-1.61,D16-1163,0,0.0166132,"thousands of domain-specific sentences are available, training large NMT models tends to over-fit on the small corpora while compact NMT models will only lead to pseudo-targets of poor quality, both preventing the generation of useful distilled corpora. It is certainly possible to search for an optimal model size. However, it will involve a time-consuming hyper-parameter search, while the result may be specific to given corpora. 2.2 Transfer Learning Transfer learning (TL) can be in the form of domain adaptation (Chu et al., 2017) or cross-lingual or multilingual transfer (Firat et al., 2016; Zoph et al., 2016; Dabre et al., 2019; Johnson et al., 2017; Dabre et al., 2020) using helping bilingual corpora. Assume that L1 –L2 is an ELR language pair and L3 –L4 is a helping pair. The given parallel corpora 493 for the two pairs may belong to different domains. Typically, pre-training a model on the larger L3 –L4 corpus and then fine-tuning (“ft”) it on the smaller L1 –L2 corpus is known to give the best translation quality for the L1 –L2 pair (Zoph et al., 2016; Chu et al., 2017; Dabre et al., 2019), regardless of the number of model parameters. However, without careful regularization, this will defini"
2021.acl-long.566,W05-0909,0,0.594351,"Missing"
2021.acl-long.566,E06-1032,0,0.562705,"Missing"
2021.acl-long.566,W17-3203,0,0.0208976,"not be considered good today, and vice versa. This evolution has not been studied and whether MT evaluation has become better, or worse, is debatable. On the other hand, several requirements for MT evaluation have been well-identified. For instance, the limitations of BLEU are well-known (CallisonBurch et al., 2006; Reiter, 2018; Mathur et al., 2020) and the necessity to report automatic metric scores through standardized tools, such as SacreBLEU, has been recognized (Post, 2018). Moreover, a trustworthy evaluation may adopt statistical significance testing (Koehn, 2004) and strong baselines (Denkowski and Neubig, 2017). However, to what extent these requirements have been met in MT publications is unclear. In this paper, we propose the first large-scale meta-evaluation of MT in which we manually annotated 769 research papers published from 2010 to 2020. Our study shows that evaluation in MT has dramatically changed since 2010. An increasing number of publications exclusively rely on BLEU scores to draw their conclusions. The large majority of publications do not perform statistical significance testing, especially since 2016. Moreover, an increasing number of papers copy and compare BLEU scores published by"
2021.acl-long.566,P18-1128,0,0.0442926,"Missing"
2021.acl-long.566,D10-1092,0,0.0786641,"Missing"
2021.acl-long.566,P18-4020,0,0.0457711,"Missing"
2021.acl-long.566,2020.emnlp-main.175,0,0.0241777,"Missing"
2021.acl-long.566,W04-3250,0,0.439686,"tion methodology ten years ago may not be considered good today, and vice versa. This evolution has not been studied and whether MT evaluation has become better, or worse, is debatable. On the other hand, several requirements for MT evaluation have been well-identified. For instance, the limitations of BLEU are well-known (CallisonBurch et al., 2006; Reiter, 2018; Mathur et al., 2020) and the necessity to report automatic metric scores through standardized tools, such as SacreBLEU, has been recognized (Post, 2018). Moreover, a trustworthy evaluation may adopt statistical significance testing (Koehn, 2004) and strong baselines (Denkowski and Neubig, 2017). However, to what extent these requirements have been met in MT publications is unclear. In this paper, we propose the first large-scale meta-evaluation of MT in which we manually annotated 769 research papers published from 2010 to 2020. Our study shows that evaluation in MT has dramatically changed since 2010. An increasing number of publications exclusively rely on BLEU scores to draw their conclusions. The large majority of publications do not perform statistical significance testing, especially since 2016. Moreover, an increasing number o"
2021.acl-long.566,W15-3049,0,0.0476228,"Missing"
2021.acl-long.566,W18-6319,0,0.0925719,"volved over the years. What could be considered, by the research community, as a good evaluation methodology ten years ago may not be considered good today, and vice versa. This evolution has not been studied and whether MT evaluation has become better, or worse, is debatable. On the other hand, several requirements for MT evaluation have been well-identified. For instance, the limitations of BLEU are well-known (CallisonBurch et al., 2006; Reiter, 2018; Mathur et al., 2020) and the necessity to report automatic metric scores through standardized tools, such as SacreBLEU, has been recognized (Post, 2018). Moreover, a trustworthy evaluation may adopt statistical significance testing (Koehn, 2004) and strong baselines (Denkowski and Neubig, 2017). However, to what extent these requirements have been met in MT publications is unclear. In this paper, we propose the first large-scale meta-evaluation of MT in which we manually annotated 769 research papers published from 2010 to 2020. Our study shows that evaluation in MT has dramatically changed since 2010. An increasing number of publications exclusively rely on BLEU scores to draw their conclusions. The large majority of publications do not perf"
2021.acl-long.566,J18-3002,0,0.0175093,"valid. Consequently, we assume that evaluation in MT is conducted with different degrees of thoroughness across papers and that evaluation practices have evolved over the years. What could be considered, by the research community, as a good evaluation methodology ten years ago may not be considered good today, and vice versa. This evolution has not been studied and whether MT evaluation has become better, or worse, is debatable. On the other hand, several requirements for MT evaluation have been well-identified. For instance, the limitations of BLEU are well-known (CallisonBurch et al., 2006; Reiter, 2018; Mathur et al., 2020) and the necessity to report automatic metric scores through standardized tools, such as SacreBLEU, has been recognized (Post, 2018). Moreover, a trustworthy evaluation may adopt statistical significance testing (Koehn, 2004) and strong baselines (Denkowski and Neubig, 2017). However, to what extent these requirements have been met in MT publications is unclear. In this paper, we propose the first large-scale meta-evaluation of MT in which we manually annotated 769 research papers published from 2010 to 2020. Our study shows that evaluation in MT has dramatically changed"
2021.acl-long.566,W05-0908,0,0.259631,"Missing"
2021.acl-long.566,2006.amta-papers.25,0,0.361931,"Missing"
2021.acl-long.566,2020.acl-main.321,0,0.0216192,"Missing"
2021.acl-long.566,N19-1313,0,0.0279495,"Missing"
2021.acl-long.566,2020.acl-main.448,0,0.129992,"ently, we assume that evaluation in MT is conducted with different degrees of thoroughness across papers and that evaluation practices have evolved over the years. What could be considered, by the research community, as a good evaluation methodology ten years ago may not be considered good today, and vice versa. This evolution has not been studied and whether MT evaluation has become better, or worse, is debatable. On the other hand, several requirements for MT evaluation have been well-identified. For instance, the limitations of BLEU are well-known (CallisonBurch et al., 2006; Reiter, 2018; Mathur et al., 2020) and the necessity to report automatic metric scores through standardized tools, such as SacreBLEU, has been recognized (Post, 2018). Moreover, a trustworthy evaluation may adopt statistical significance testing (Koehn, 2004) and strong baselines (Denkowski and Neubig, 2017). However, to what extent these requirements have been met in MT publications is unclear. In this paper, we propose the first large-scale meta-evaluation of MT in which we manually annotated 769 research papers published from 2010 to 2020. Our study shows that evaluation in MT has dramatically changed since 2010. An increas"
2021.acl-long.566,P02-1040,0,0.128184,", tools for reporting standardized metric scores are still far from being widely adopted by the MT community. After showing how the accumulation of these pitfalls leads to dubious evaluation, we propose a guideline to encourage better automatic MT evaluation along with a simple meta-evaluation scoring method to assess its credibility. 1 Introduction New research publications in machine translation (MT) regularly introduce new methods and algorithms to improve the translation quality of MT systems. In the literature, translation quality is usually evaluated with automatic metrics such as BLEU (Papineni et al., 2002) and, more rarely, by humans. To assess whether an MT system performs better than another MT system, their scores given by an automatic metric are directly compared. While such comparisons between MT systems are exhibited in the large majority of MT papers, there are no well-defined guideline nor clear prerequisites under which a comparison between MT systems is considered valid. Consequently, we assume that evaluation in MT is conducted with different degrees of thoroughness across papers and that evaluation practices have evolved over the years. What could be considered, by the research comm"
2021.eacl-main.132,2007.mtsummit-papers.1,0,0.0647565,"itectures, and text domains. Manual pre-editing has long been implemented in combination with controlled languages (Pym, 1990; Reuther, 2003; Nyberg et al., 2003; Kuhn, 2014). In the period of rule-based MT (RBMT), pre-editing was considered as a promising approach since the behaviour of RBMT is more predictable and controllable. For example, O’Brien and Roturier (2007) examined the impact of English controlled language rules on two different MT engines, revealing the rules of high effectiveness. The pre-editing approach with controlled languages has also been tested for statistical MT (SMT) (Aikawa et al., 2007; Hartley et al., 2012; Seretan et al., 2014). These studies developed or utilised a set of controlled language rules for rewriting ST. While these rule sets are optimised for particular MT systems and differ from each other, we can observe some shared characteristics among them. In particular, rules that prohibit long sentences (e.g., of more than 25 words) are widely adopted in the existing rule sets (O’Brien, 2003). Automation of pre-editing is also an important research field in natural language processing. Semi-automatic tools such as controlled language checkers (Bernth and Gdaniec, 2001"
2021.eacl-main.132,P19-1425,0,0.107623,"ts depending on the editing operation types. 1 However, the feasibility and possibility of preediting for neural MT (NMT) has not been examined extensively. While efforts have recently been invested in the implementation of pre-editing strategies for black-box NMT settings, achieving improved MT quality (e.g., Hiraoka and Yamada, 2019; Mehta et al., 2020), the potential gains of preediting remain unexplored. Notably, the impact of pre-editing on black-box MT is unpredictable in nature. In particular, NMT models trained in an end-to-end manner can be sensitive to minor modifications of the ST (Cheng et al., 2019), which may affect the feasibility of pre-editing. Introduction Recent advances in machine translation (MT) have greatly facilitated its practical use in various settings from business documentation to personal communication. In many practical cases, MT systems are used as black-box and one well-tested approach to make use of a black-box MT is preediting, i.e., modifying the source text (ST) to make it suitable for the intended MT system. The effectiveness of pre-editing has so far been demonstrated in many studies (Pym, 1990; O’Brien In short, while pre-editing has been implemented in practic"
2021.eacl-main.132,C18-1055,0,0.0272085,"nalysing them in detail. They demonstrated the maximum gain of pre-editing for an SMT and provided a comprehensive typology of editing operations. Nevertheless, their study has two major limitations: (1) recent NMT was not examined, and (2) practical insights for better practices of pre-editing were not sufficiently presented. NMT models trained in an end-to-end manner behave very differently from SMT and RBMT, which, in turn, affects pre-editing practices. As reported in several studies, despite their rapid improvement, NMT models are still vulnerable to input noise (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Cheng et al., 2019; Niu et al., 2020). The pre-editing operations identified in previous studies are not necessarily effective for current blackbox NMT systems.1 For example, Marzouk and Hansen-Schirra (2019) adopted nine controlled language rules2 and evaluated their impact on the MT output for German-to-English translation in the technical domain. The human evaluation results revealed that these rules improved the performance of the RBMT, SMT, and hybrid systems, but did not have positive effects on the NMT system. Hiraoka and Yamada (2019) demonstrated the effectiveness of the following t"
2021.eacl-main.132,2012.eamt-1.57,1,0.761244,"omains. Manual pre-editing has long been implemented in combination with controlled languages (Pym, 1990; Reuther, 2003; Nyberg et al., 2003; Kuhn, 2014). In the period of rule-based MT (RBMT), pre-editing was considered as a promising approach since the behaviour of RBMT is more predictable and controllable. For example, O’Brien and Roturier (2007) examined the impact of English controlled language rules on two different MT engines, revealing the rules of high effectiveness. The pre-editing approach with controlled languages has also been tested for statistical MT (SMT) (Aikawa et al., 2007; Hartley et al., 2012; Seretan et al., 2014). These studies developed or utilised a set of controlled language rules for rewriting ST. While these rule sets are optimised for particular MT systems and differ from each other, we can observe some shared characteristics among them. In particular, rules that prohibit long sentences (e.g., of more than 25 words) are widely adopted in the existing rule sets (O’Brien, 2003). Automation of pre-editing is also an important research field in natural language processing. Semi-automatic tools such as controlled language checkers (Bernth and Gdaniec, 2001; Mitamura et al., 200"
2021.eacl-main.132,W19-6710,0,0.0891784,"explicitness of the meaning of an ST and its syntactic structure is more important for obtaining better translations than making the ST shorter and simpler, and (2) although the impact of pre-editing on NMT is generally unpredictable, there are some tendencies of changes in the NMT outputs depending on the editing operation types. 1 However, the feasibility and possibility of preediting for neural MT (NMT) has not been examined extensively. While efforts have recently been invested in the implementation of pre-editing strategies for black-box NMT settings, achieving improved MT quality (e.g., Hiraoka and Yamada, 2019; Mehta et al., 2020), the potential gains of preediting remain unexplored. Notably, the impact of pre-editing on black-box MT is unpredictable in nature. In particular, NMT models trained in an end-to-end manner can be sensitive to minor modifications of the ST (Cheng et al., 2019), which may affect the feasibility of pre-editing. Introduction Recent advances in machine translation (MT) have greatly facilitated its practical use in various settings from business documentation to personal communication. In many practical cases, MT systems are used as black-box and one well-tested approach to m"
2021.eacl-main.132,P15-2023,0,0.0119092,"esearch field in natural language processing. Semi-automatic tools such as controlled language checkers (Bernth and Gdaniec, 2001; Mitamura et al., 2003) and interactive rewriting assistants (Mirkin et al., 2013; Gulati et al., 2015) were developed to facilitate manual pre-editing activities. Fully automatic pre-editing has long been explored (e.g., Shirai et al., 1998; Mitamura and Nyberg, 2001; Yoshimi, 2001; Sun et al., 2010). In particular, many researchers have examined methods of reordering the source-side word order as a pretranslation processing (Xia and McCord, 2004; Li et al., 2007; Hoshino et al., 2015). While the reordering approach has generally proven effective for SMT, its effectiveness for NMT is not obvious; negative effects have even be reported (Zhu, 2015; Du and Way, 2017). In recent years, techniques of automatic text simplification have been applied to ˇ improve NMT outputs (Stajner and Popovi´c, 2018; Mehta et al., 2020). The underlying assumption of these studies is that simpler sentences are more machine translatable. Previous studies have investigated various preediting methods from different perspectives, focusing on different linguistic phenomena. Indeed, individual research"
2021.eacl-main.132,W17-3204,0,0.0384553,"er word frequencies in Wikipedia. The 50th and 75th percentile values in the datasets imply that pre-editing induced the avoidance of low-frequency words. To further inspect the differences between the Org-ST and the Best-ST, we extracted the word types (a) that appeared only in the Org-ST and (b) that appeared only in the Best-ST. Figure 2 illustrates the rank distributions of (a) and (b) for each condition. It is clear that low-frequency words with a frequency rank of around 10,000 decreased in the Best-ST, while words with a frequency rank of around 2,000–4,000 increased in the Best-ST. As Koehn and Knowles (2017) demonstrated, lowfrequency words still pose major obstacles for NMT systems. Our results endorse this claim from a different perspective and can provide general strategies for word choice in the pre-editing task. 9 We used the whole text data of Japanese Wikipedia obtained in October 2019 (https://dumps.wikimedia.org/). 1543 105 104 103 102 JaOrg E( n-G 74) Ja Best-En-G (134) Ja-En Org ( -T 69) Ja-En Best -T (116) Ja-Zh Org ( -G 82) Ja-Zh Best -G (176) JaOrg Z( h-T 75) Ja-Zh Best -T (129) JaOrgK( o-G 70) Ja Best-Ko-G (133) Ja-Ko Org ( -T 87) Ja-Ko Best -T (135) Word frequency rank 106 Figure"
2021.eacl-main.132,J14-1005,0,0.011072,"Computational Linguistics 2 Related Work Pre-editing is the process of rewriting the source text (ST) to be translated in order to obtain better translations by MT. Though the scope of effective pre-editing operations depends on the downstream MT system and there is no deterministic relation between pre-editing operations and the quality of MT output, its effectiveness has been demonstrated for various translation directions, MT architectures, and text domains. Manual pre-editing has long been implemented in combination with controlled languages (Pym, 1990; Reuther, 2003; Nyberg et al., 2003; Kuhn, 2014). In the period of rule-based MT (RBMT), pre-editing was considered as a promising approach since the behaviour of RBMT is more predictable and controllable. For example, O’Brien and Roturier (2007) examined the impact of English controlled language rules on two different MT engines, revealing the rules of high effectiveness. The pre-editing approach with controlled languages has also been tested for statistical MT (SMT) (Aikawa et al., 2007; Hartley et al., 2012; Seretan et al., 2014). These studies developed or utilised a set of controlled language rules for rewriting ST. While these rule se"
2021.eacl-main.132,P07-1091,0,0.0378022,"so an important research field in natural language processing. Semi-automatic tools such as controlled language checkers (Bernth and Gdaniec, 2001; Mitamura et al., 2003) and interactive rewriting assistants (Mirkin et al., 2013; Gulati et al., 2015) were developed to facilitate manual pre-editing activities. Fully automatic pre-editing has long been explored (e.g., Shirai et al., 1998; Mitamura and Nyberg, 2001; Yoshimi, 2001; Sun et al., 2010). In particular, many researchers have examined methods of reordering the source-side word order as a pretranslation processing (Xia and McCord, 2004; Li et al., 2007; Hoshino et al., 2015). While the reordering approach has generally proven effective for SMT, its effectiveness for NMT is not obvious; negative effects have even be reported (Zhu, 2015; Du and Way, 2017). In recent years, techniques of automatic text simplification have been applied to ˇ improve NMT outputs (Stajner and Popovi´c, 2018; Mehta et al., 2020). The underlying assumption of these studies is that simpler sentences are more machine translatable. Previous studies have investigated various preediting methods from different perspectives, focusing on different linguistic phenomena. Inde"
2021.eacl-main.132,P13-4015,0,0.0152306,"veloped or utilised a set of controlled language rules for rewriting ST. While these rule sets are optimised for particular MT systems and differ from each other, we can observe some shared characteristics among them. In particular, rules that prohibit long sentences (e.g., of more than 25 words) are widely adopted in the existing rule sets (O’Brien, 2003). Automation of pre-editing is also an important research field in natural language processing. Semi-automatic tools such as controlled language checkers (Bernth and Gdaniec, 2001; Mitamura et al., 2003) and interactive rewriting assistants (Mirkin et al., 2013; Gulati et al., 2015) were developed to facilitate manual pre-editing activities. Fully automatic pre-editing has long been explored (e.g., Shirai et al., 1998; Mitamura and Nyberg, 2001; Yoshimi, 2001; Sun et al., 2010). In particular, many researchers have examined methods of reordering the source-side word order as a pretranslation processing (Xia and McCord, 2004; Li et al., 2007; Hoshino et al., 2015). While the reordering approach has generally proven effective for SMT, its effectiveness for NMT is not obvious; negative effects have even be reported (Zhu, 2015; Du and Way, 2017). In rec"
2021.eacl-main.132,2003.eamt-1.10,0,0.120941,"Hartley et al., 2012; Seretan et al., 2014). These studies developed or utilised a set of controlled language rules for rewriting ST. While these rule sets are optimised for particular MT systems and differ from each other, we can observe some shared characteristics among them. In particular, rules that prohibit long sentences (e.g., of more than 25 words) are widely adopted in the existing rule sets (O’Brien, 2003). Automation of pre-editing is also an important research field in natural language processing. Semi-automatic tools such as controlled language checkers (Bernth and Gdaniec, 2001; Mitamura et al., 2003) and interactive rewriting assistants (Mirkin et al., 2013; Gulati et al., 2015) were developed to facilitate manual pre-editing activities. Fully automatic pre-editing has long been explored (e.g., Shirai et al., 1998; Mitamura and Nyberg, 2001; Yoshimi, 2001; Sun et al., 2010). In particular, many researchers have examined methods of reordering the source-side word order as a pretranslation processing (Xia and McCord, 2004; Li et al., 2007; Hoshino et al., 2015). While the reordering approach has generally proven effective for SMT, its effectiveness for NMT is not obvious; negative effects h"
2021.eacl-main.132,2020.acl-main.755,0,0.0183037,"the maximum gain of pre-editing for an SMT and provided a comprehensive typology of editing operations. Nevertheless, their study has two major limitations: (1) recent NMT was not examined, and (2) practical insights for better practices of pre-editing were not sufficiently presented. NMT models trained in an end-to-end manner behave very differently from SMT and RBMT, which, in turn, affects pre-editing practices. As reported in several studies, despite their rapid improvement, NMT models are still vulnerable to input noise (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Cheng et al., 2019; Niu et al., 2020). The pre-editing operations identified in previous studies are not necessarily effective for current blackbox NMT systems.1 For example, Marzouk and Hansen-Schirra (2019) adopted nine controlled language rules2 and evaluated their impact on the MT output for German-to-English translation in the technical domain. The human evaluation results revealed that these rules improved the performance of the RBMT, SMT, and hybrid systems, but did not have positive effects on the NMT system. Hiraoka and Yamada (2019) demonstrated the effectiveness of the following three pre-editing rules in improving Jap"
2021.eacl-main.132,2003.eamt-1.14,0,0.384565,"9 - 23, 2021. ©2021 Association for Computational Linguistics 2 Related Work Pre-editing is the process of rewriting the source text (ST) to be translated in order to obtain better translations by MT. Though the scope of effective pre-editing operations depends on the downstream MT system and there is no deterministic relation between pre-editing operations and the quality of MT output, its effectiveness has been demonstrated for various translation directions, MT architectures, and text domains. Manual pre-editing has long been implemented in combination with controlled languages (Pym, 1990; Reuther, 2003; Nyberg et al., 2003; Kuhn, 2014). In the period of rule-based MT (RBMT), pre-editing was considered as a promising approach since the behaviour of RBMT is more predictable and controllable. For example, O’Brien and Roturier (2007) examined the impact of English controlled language rules on two different MT engines, revealing the rules of high effectiveness. The pre-editing approach with controlled languages has also been tested for statistical MT (SMT) (Aikawa et al., 2007; Hartley et al., 2012; Seretan et al., 2014). These studies developed or utilised a set of controlled language rules for"
2021.eacl-main.132,W18-7006,0,0.0337485,"Missing"
2021.eacl-main.132,C04-1073,0,0.106253,"n of pre-editing is also an important research field in natural language processing. Semi-automatic tools such as controlled language checkers (Bernth and Gdaniec, 2001; Mitamura et al., 2003) and interactive rewriting assistants (Mirkin et al., 2013; Gulati et al., 2015) were developed to facilitate manual pre-editing activities. Fully automatic pre-editing has long been explored (e.g., Shirai et al., 1998; Mitamura and Nyberg, 2001; Yoshimi, 2001; Sun et al., 2010). In particular, many researchers have examined methods of reordering the source-side word order as a pretranslation processing (Xia and McCord, 2004; Li et al., 2007; Hoshino et al., 2015). While the reordering approach has generally proven effective for SMT, its effectiveness for NMT is not obvious; negative effects have even be reported (Zhu, 2015; Du and Way, 2017). In recent years, techniques of automatic text simplification have been applied to ˇ improve NMT outputs (Stajner and Popovi´c, 2018; Mehta et al., 2020). The underlying assumption of these studies is that simpler sentences are more machine translatable. Previous studies have investigated various preediting methods from different perspectives, focusing on different linguisti"
2021.eacl-main.132,W15-5007,0,0.0231293,"ting assistants (Mirkin et al., 2013; Gulati et al., 2015) were developed to facilitate manual pre-editing activities. Fully automatic pre-editing has long been explored (e.g., Shirai et al., 1998; Mitamura and Nyberg, 2001; Yoshimi, 2001; Sun et al., 2010). In particular, many researchers have examined methods of reordering the source-side word order as a pretranslation processing (Xia and McCord, 2004; Li et al., 2007; Hoshino et al., 2015). While the reordering approach has generally proven effective for SMT, its effectiveness for NMT is not obvious; negative effects have even be reported (Zhu, 2015; Du and Way, 2017). In recent years, techniques of automatic text simplification have been applied to ˇ improve NMT outputs (Stajner and Popovi´c, 2018; Mehta et al., 2020). The underlying assumption of these studies is that simpler sentences are more machine translatable. Previous studies have investigated various preediting methods from different perspectives, focusing on different linguistic phenomena. Indeed, individual research has led to improved MT results. However, what is crucially needed is a broad understanding of what pre-editing is and how it works. For example, Miyata and Fujita"
2021.eacl-main.132,seretan-etal-2014-large,0,0.0481238,"Missing"
2021.eacl-main.132,2006.amta-papers.25,0,0.247602,"utput. As indicated in §2, NMT systems still lack robustness, and minor modifications of the input would drastically change the output. From the practical viewpoint of deploying pre-editing, predictability is an important object to pursue. Here, we examine the impacts of minimum edits of the ST on the NMT output. To measure the amount of text editing, hereafter, we use the Translation Edit Rate (TER), which is calculated by dividing the number of edits (insertion, deletion, substitution, and shift) required to change a string into the reference string by the average number of reference words (Snover et al., 2006). For any consecutive pair of STs or their corresponding MT outputs, we used the chronologically later version as the reference. For word-level tokenisation, we used MeCab for Japanese, NLTK12 for English, jieba13 for Chinese, and KoNLPy14 for Korean. 6.1 Correlation of the Amount of Edits between the ST and MT To grasp the general tendency, using all the collected pre-editing instances (see Table 3), we first calculated the correlation coefficients (Pearson’s r and Spearman’s ρ) between the amount of edits (the TER and the number of edits) in the ST and in the MT. More formally, let ST0 be th"
2021.eacl-main.132,2010.eamt-1.34,0,0.194536,"Missing"
2021.eval4nlp-1.15,C04-1046,0,0.695139,"s a metric-to-input attention mechanism allowing for several extensions of the habitual QE approach. First, since sentence-level QE scores are usually obtained with surface-level MT metrics computed between translation outputs and human produced references or post-edits such as HTER (Snover et al., 2006), we propose to make use of several metrics simultaneously in order to model translation errors at various granularities, i.e. 1 Introduction at the character, token, and phrase levels. Second, Quality Estimation (QE) for Machine Translation we design a metric embeddings model which repre(MT) (Blatz et al., 2004; Quirk, 2004; Specia sents metrics in their own space through a dedicated et al., 2009) aims at providing quality scores or set of learnable parameters, allowing for straightforlabels to MT output when translation references ward extensions of the number and type of metrics. are not available. Sentence-level QE is usually con- Third, by employing an attention mechanism beducted using human produced direct assessments tween metric embeddings and bilingual input rep(DA) (Graham et al., 2013) or post-edits. The lat- resentations, the metric-to-input attention weights ter allows to derive token-l"
2021.eval4nlp-1.15,2020.acl-main.747,0,0.0191082,"cluding the pretrained models, the datasets and functions for sentence-level QE are mean-squared the training procedure. All pretrained models and 149 Pretrained Models Test 4.1 Two types of pretrained models were necessary to conduct our experiments: contextual embedding LMs to encode bilingual input sequences and MT models to produce synthetic data required for QE pretraining. Contextual embedding LMs used in our experiments are based on a pretrained XLM-R checkpoint, namely xlm-roberta-large from the HuggingFace Transformers library (Wolf et al., 2020). This model, initially introduced in (Conneau et al., 2020), was pretrained on 2.5TB of filtered CommonCrawl data, covering 100 languages with a vocabulary of 250k BPE tokens (Sennrich et al., 2016), 1, 024 embedding and hidden-state dimensions, 4, 096-dimensional feed-forward layers and 16 attention heads. MT models used in our experiments are transformer-based neural MT (NMT) models. For two language pairs and translation directions of the Eval4NLP 2021 shared task, namely Estonian→English (ET–EN) and Romanian→English (RO–EN), we used pretrained NMT models made available by the WMT’20 QE shared task organizers (Specia et al., 2020).1 For German→Chin"
2021.eval4nlp-1.15,N19-1423,0,0.00495012,"d our submissions reach the first position in all language pairs. The extraction of metricto-input attention weights show that different metrics focus on different parts of the source and target text, providing strong rationales in the decision-making process of the QE model. indicators can be seen as explanations for sentencelevel scores, whether given by humans or automatically produced. However, explainability of QE models decisions is obscured by contemporary approaches relying on large data-driven neural-based models, making use of pretrained contextual language models (LM) such as BERT (Devlin et al., 2019) and XLM (Conneau and Lample, 2019), albeit showing steady performance increase as reported in the QE shared tasks (Fonseca et al., 2019; Specia et al., 2020). Yet, the QE layers and architectures are rarely investigated, neither for performance nor for interpretability purposes, and the center of attention is mainly on large pretrained models and generating additional (synthetic) training corpora. In this paper, we present a novel QE architecture which encompasses a metric-to-input attention mechanism allowing for several extensions of the habitual QE approach. First, since sentence-level QE"
2021.eval4nlp-1.15,N13-1073,0,0.0241408,"o the target side of the parallel corpora to produce sentence-level scores based on chrF (Popovi´c, 2016), TER (Snover et al., 2006) and BLEU (Papineni et al., 2002) metrics. Additionally, only for the synthetic data, we produced token-level scores following the usual procedure to determine post-editing effort (Specia et al., 2020).3 For this step, word alignments were required to obtain source-side token-level quality indicators. We used the same parallel corpora to produce synthetic data and to train word alignments based on the IBM 2 model (Brown et al., 1993) and trained using fast_align (Dyer et al., 2013). Details about the synthetic data are presented in Table 2.4 The special case of DE–ZH resulting from preliminary experiments, we noticed for this language pair that the translation quality of the synthetic data was low compared to the three other language pairs. We assumed that it was due to two issues: 3 Scripts and procedure available at https://github. com/deep-spin/qe-corpus-builder 4 Parallel corpora were collected from the WMT news translation task (Tiedemann, 2016) and OPUS (Tiedemann, 2016). 150 the quality of the DE–ZH parallel corpora and the performance of the NMT model. To tackle"
2021.eval4nlp-1.15,2020.tacl-1.35,0,0.0205835,"Missing"
2021.eval4nlp-1.15,W13-2305,0,0.0101631,"cond, Quality Estimation (QE) for Machine Translation we design a metric embeddings model which repre(MT) (Blatz et al., 2004; Quirk, 2004; Specia sents metrics in their own space through a dedicated et al., 2009) aims at providing quality scores or set of learnable parameters, allowing for straightforlabels to MT output when translation references ward extensions of the number and type of metrics. are not available. Sentence-level QE is usually con- Third, by employing an attention mechanism beducted using human produced direct assessments tween metric embeddings and bilingual input rep(DA) (Graham et al., 2013) or post-edits. The lat- resentations, the metric-to-input attention weights ter allows to derive token-level quality indicators indicate where each metric focuses given an input such as good and bad tags (Fonseca et al., 2019; sequence, increasing the interpretability of the QE Specia et al., 2020). Token-level QE is particu- components. We conduct a set of experiments on larly useful for applications such as source pre- the Eval4NLP 2021 shared task dataset (Fomicheva editing or focused MT post-editing, but requires et al., 2021) using only the training data along high-quality fine-grained a"
2021.eval4nlp-1.15,W19-5406,0,0.0153987,"le the token-level QE implementation makes use of each input token representation in context thanks to the pretrained LM, the sentence-level QE components relies only on the pooled representation of the input sequence. This approach drastically limits the amount of information flowing through the sentence-level specific set of layers and may force the network to focus more on cues and data artifacts which correlate with QE scores, instead of encoding translation-related features from source and target inputs (Sun et al., 2020). These findings corroborate with the empirical observation made by Kepler et al. (2019), where the authors obtained the best word-level QE results using BERT and ignoring target language features when predicting source quality labels and vice-versa. Additionally, most recent QE approaches do not allow for the interpretability of sentence-level QE predictions at test time and leads to the current state of QE as a set of black-box components. Furthermore, token-level error annotations is costly to produce. 3 Metric Embedding and Attention Motivated by the limitations to contextual LM based QE, we propose a novel architecture employing metric embeddings and attention, which is comp"
2021.eval4nlp-1.15,W17-4763,0,0.0370106,"according to the accuracy of their translation, while annotations of target tokens also take into account their position in the target sequence. metric is initially represented as a one-hot vector, noted mj ∈ Rg with j ∈ [1, g] ⊂ N. Its corresponding embedding is retrieved with mj · E, forming the query used in the attention mechanism (eqn. 1): Multiple losses are then computed, one for the sentence-level and one for each word-level outputs (source and target tokens), based on gold labels to train (or finetune) the contextual LM and the QE model in an end-to-end fashion using backpropagation (Kim et al., 2017; Lee, 2020; Rubino and Sumita, 2020). Commonly used losses are cross-entropy and mean-squared error for classification and regression respectively. Qi,j = (mj · E) · WiQ However, this approach has limitations While the token-level QE implementation makes use of each input token representation in context thanks to the pretrained LM, the sentence-level QE components relies only on the pooled representation of the input sequence. This approach drastically limits the amount of information flowing through the sentence-level specific set of layers and may force the network to focus more on cues and"
2021.eval4nlp-1.15,2021.eval4nlp-1.17,0,0.0373775,"Missing"
2021.eval4nlp-1.15,2020.wmt-1.118,0,0.338845,"ccuracy of their translation, while annotations of target tokens also take into account their position in the target sequence. metric is initially represented as a one-hot vector, noted mj ∈ Rg with j ∈ [1, g] ⊂ N. Its corresponding embedding is retrieved with mj · E, forming the query used in the attention mechanism (eqn. 1): Multiple losses are then computed, one for the sentence-level and one for each word-level outputs (source and target tokens), based on gold labels to train (or finetune) the contextual LM and the QE model in an end-to-end fashion using backpropagation (Kim et al., 2017; Lee, 2020; Rubino and Sumita, 2020). Commonly used losses are cross-entropy and mean-squared error for classification and regression respectively. Qi,j = (mj · E) · WiQ However, this approach has limitations While the token-level QE implementation makes use of each input token representation in context thanks to the pretrained LM, the sentence-level QE components relies only on the pooled representation of the input sequence. This approach drastically limits the amount of information flowing through the sentence-level specific set of layers and may force the network to focus more on cues and data artif"
2021.eval4nlp-1.15,2020.wmt-1.116,0,0.0290512,"Missing"
2021.eval4nlp-1.15,2020.wmt-1.119,0,0.0365083,"Missing"
2021.eval4nlp-1.15,N16-3020,0,0.0432879,"s section the training procedures employed for QE pretraining on the synthetic data and finetuning on the officially released training data. 5 Results and Analysis We present in Table 3 the results obtained on the Eval4NLP 2021 shared task as reported by the organizers, including our baselines and final submissions along with the three baselines proposed QE pretraining was conducted per language by the organizers, namely random scores, Tranpair starting from the XLM-R checkpoint presented sQuest (Ranasinghe et al., 2020b) combined with in Section 4.1 using two different random seeds and LIME (Ribeiro et al., 2016) (noted Official baseline learning rates. Additionally, four QE pretraining were conducted on the concatenation of all syn- 1), and XMoverScore (Zhao et al., 2020) combined with SHAP (Lundberg and Lee, 2017) (noted Ofthetic data using four different random seeds and ficial baseline 2). Our baselines are composed of two learning rates. Training was ran for two epochs ensembles of two finetuned language-specific QE for the language specific models and for a single pretrained models while our final submissions are epoch for the remaining ones. We restricted the composed of ensembles of eight fine"
2021.eval4nlp-1.15,N19-4009,0,0.0179507,"sional feed-forward layers and 16 attention heads. MT models used in our experiments are transformer-based neural MT (NMT) models. For two language pairs and translation directions of the Eval4NLP 2021 shared task, namely Estonian→English (ET–EN) and Romanian→English (RO–EN), we used pretrained NMT models made available by the WMT’20 QE shared task organizers (Specia et al., 2020).1 For German→Chinese (DE–ZH) and Russian→German (RU–DE), the two zero-shot pairs of the shared task, we used the mBART50 model (Liu et al., 2020; Tang et al., 2020).2 All NMT models are based on the fairseq library (Ott et al., 2019). 4.2 Valid Train scripts used in our experiments are based on PyTorch (Paszke et al., 2019) and all computations are conducted on NVIDIA V100 GPUs with CUDA v10.2. Datasets Two datasets were used in our experiments: a synthetic dataset for QE pretraining, and the shared task dataset consisting of training, validation and test sets. Details of the latter dataset are presented in Table 1 while we give more information about the synthetic data in this section. 1 Models available at https://github.com/ facebookresearch/mlqe/blob/master/nmt_ models/README-models.md 2 Model available at https://git"
2021.eval4nlp-1.15,2020.wmt-1.121,1,0.724687,"cific component is a regression head formalized by y s = σ(φ(hL 1:n )· s s s s s W + b ), where y ∈ [0, 1] ⊂ R. W and b are trainable parameters of the linear output layer, φ is a pooling function, and σ is the sigmoid function. The output y s of this QE component is a score indicating the sentence-level translation quality. Current state-of-the-art QE approaches are commonly based on sentence encoders taking as input For token-level QE a classification head is imt t t source–translation pairs (Ranasinghe et al., 2020a; plemented as y1:n = softmax (hL 1:n · W + b ), t n×|C| Wang et al., 2020; Rubino, 2020). Encoders are where y1:n ∈ R with C the set of word-level usually contextual LMs pretrained on large amount QE classes, W t and bt are trainable parameters t of this of multilingual data. Existing QE implementations of the linear output layer. The output y1:n commonly rely on additional layers added on top of QE component is a vector of labels indicating the 147 translation quality of corresponding input tokens. Source tokens are annotated according to the accuracy of their translation, while annotations of target tokens also take into account their position in the target sequence. metric is"
2021.eval4nlp-1.15,P02-1040,0,0.109336,"/ 411.0M 600.5M / 601.2M 422.8M / 708.1M 256.9M / 262.7M 4.8M / 2.8M 4.0M / 3.6M 4.5M / 3.3k 4.4M / 4.4M Table 2: Synthetic data produced for QE pretraining. Tokens and types columns contain source / MT counts, M stands for millions and k for thousands, Chinese tokens and types are characters. Synthetic data generation was based on gathered parallel corpora translated by the NMT systems presented in Section 4.1. The translated sentences were compared to the target side of the parallel corpora to produce sentence-level scores based on chrF (Popovi´c, 2016), TER (Snover et al., 2006) and BLEU (Papineni et al., 2002) metrics. Additionally, only for the synthetic data, we produced token-level scores following the usual procedure to determine post-editing effort (Specia et al., 2020).3 For this step, word alignments were required to obtain source-side token-level quality indicators. We used the same parallel corpora to produce synthetic data and to train word alignments based on the IBM 2 model (Brown et al., 1993) and trained using fast_align (Dyer et al., 2013). Details about the synthetic data are presented in Table 2.4 The special case of DE–ZH resulting from preliminary experiments, we noticed for this"
2021.eval4nlp-1.15,W16-2341,0,0.096671,"Missing"
2021.eval4nlp-1.15,quirk-2004-training,0,0.357709,"attention mechanism allowing for several extensions of the habitual QE approach. First, since sentence-level QE scores are usually obtained with surface-level MT metrics computed between translation outputs and human produced references or post-edits such as HTER (Snover et al., 2006), we propose to make use of several metrics simultaneously in order to model translation errors at various granularities, i.e. 1 Introduction at the character, token, and phrase levels. Second, Quality Estimation (QE) for Machine Translation we design a metric embeddings model which repre(MT) (Blatz et al., 2004; Quirk, 2004; Specia sents metrics in their own space through a dedicated et al., 2009) aims at providing quality scores or set of learnable parameters, allowing for straightforlabels to MT output when translation references ward extensions of the number and type of metrics. are not available. Sentence-level QE is usually con- Third, by employing an attention mechanism beducted using human produced direct assessments tween metric embeddings and bilingual input rep(DA) (Graham et al., 2013) or post-edits. The lat- resentations, the metric-to-input attention weights ter allows to derive token-level quality"
2021.eval4nlp-1.15,2020.wmt-1.122,0,0.639903,"extual token embeddings from the topmost (i.e., L-th) layer of the LM. For sentence-level QE the specific component is a regression head formalized by y s = σ(φ(hL 1:n )· s s s s s W + b ), where y ∈ [0, 1] ⊂ R. W and b are trainable parameters of the linear output layer, φ is a pooling function, and σ is the sigmoid function. The output y s of this QE component is a score indicating the sentence-level translation quality. Current state-of-the-art QE approaches are commonly based on sentence encoders taking as input For token-level QE a classification head is imt t t source–translation pairs (Ranasinghe et al., 2020a; plemented as y1:n = softmax (hL 1:n · W + b ), t n×|C| Wang et al., 2020; Rubino, 2020). Encoders are where y1:n ∈ R with C the set of word-level usually contextual LMs pretrained on large amount QE classes, W t and bt are trainable parameters t of this of multilingual data. Existing QE implementations of the linear output layer. The output y1:n commonly rely on additional layers added on top of QE component is a vector of labels indicating the 147 translation quality of corresponding input tokens. Source tokens are annotated according to the accuracy of their translation, while annotations"
2021.eval4nlp-1.15,2020.coling-main.445,0,0.0858503,"extual token embeddings from the topmost (i.e., L-th) layer of the LM. For sentence-level QE the specific component is a regression head formalized by y s = σ(φ(hL 1:n )· s s s s s W + b ), where y ∈ [0, 1] ⊂ R. W and b are trainable parameters of the linear output layer, φ is a pooling function, and σ is the sigmoid function. The output y s of this QE component is a score indicating the sentence-level translation quality. Current state-of-the-art QE approaches are commonly based on sentence encoders taking as input For token-level QE a classification head is imt t t source–translation pairs (Ranasinghe et al., 2020a; plemented as y1:n = softmax (hL 1:n · W + b ), t n×|C| Wang et al., 2020; Rubino, 2020). Encoders are where y1:n ∈ R with C the set of word-level usually contextual LMs pretrained on large amount QE classes, W t and bt are trainable parameters t of this of multilingual data. Existing QE implementations of the linear output layer. The output y1:n commonly rely on additional layers added on top of QE component is a vector of labels indicating the 147 translation quality of corresponding input tokens. Source tokens are annotated according to the accuracy of their translation, while annotations"
2021.eval4nlp-1.15,N18-1027,0,0.0134343,"ction, welladded on top of the pretrained LM to model met- suited for tasks such as machine translation as it rics in their own space, with a predefined set of results in few alignments between tokens involved sentence-level metrics M = {m1 , . . . , mg }. Each in the attention mechanism. However, in the case of 148 unsupervised sequence labeling such as token-level QE without annotated data, zero to many tokens may influence sentence-level scores given a metric. Thus, to allow more flexibility in the distribution of attention weights over input tokens and following the approach presented in (Rei and Søgaard, 2018), we replaced softmax by sigmoid . Sentence-level scores are obtained for each metric with the weighted sum of value vectors for each attention head (eqn. 4): X attn i,j = αi,j,1:n Vi , (4) where attn i,j ∈ R(d/u) , before concatenating the output of each head and projecting the result back in the dimensionality of the model (eqn. 5): 0 yjs = (attn 1,j ⊕ . . . ⊕ attn u,j ) · W O (5) 0 with W O ∈ Rd×d . Finally, we project yjs from the model dimensionality to a single score through a 0 metric specific linear layer: yjs = yjs · Wjs with Wjs ∈ Rd×1 and yjs ∈ [0, 1] ⊂ R. Token-level QE scores are"
2021.eval4nlp-1.15,P16-1162,0,0.0221314,"dels and 149 Pretrained Models Test 4.1 Two types of pretrained models were necessary to conduct our experiments: contextual embedding LMs to encode bilingual input sequences and MT models to produce synthetic data required for QE pretraining. Contextual embedding LMs used in our experiments are based on a pretrained XLM-R checkpoint, namely xlm-roberta-large from the HuggingFace Transformers library (Wolf et al., 2020). This model, initially introduced in (Conneau et al., 2020), was pretrained on 2.5TB of filtered CommonCrawl data, covering 100 languages with a vocabulary of 250k BPE tokens (Sennrich et al., 2016), 1, 024 embedding and hidden-state dimensions, 4, 096-dimensional feed-forward layers and 16 attention heads. MT models used in our experiments are transformer-based neural MT (NMT) models. For two language pairs and translation directions of the Eval4NLP 2021 shared task, namely Estonian→English (ET–EN) and Romanian→English (RO–EN), we used pretrained NMT models made available by the WMT’20 QE shared task organizers (Specia et al., 2020).1 For German→Chinese (DE–ZH) and Russian→German (RU–DE), the two zero-shot pairs of the shared task, we used the mBART50 model (Liu et al., 2020; Tang et al"
2021.eval4nlp-1.15,2006.amta-papers.25,0,0.653831,"2020). Yet, the QE layers and architectures are rarely investigated, neither for performance nor for interpretability purposes, and the center of attention is mainly on large pretrained models and generating additional (synthetic) training corpora. In this paper, we present a novel QE architecture which encompasses a metric-to-input attention mechanism allowing for several extensions of the habitual QE approach. First, since sentence-level QE scores are usually obtained with surface-level MT metrics computed between translation outputs and human produced references or post-edits such as HTER (Snover et al., 2006), we propose to make use of several metrics simultaneously in order to model translation errors at various granularities, i.e. 1 Introduction at the character, token, and phrase levels. Second, Quality Estimation (QE) for Machine Translation we design a metric embeddings model which repre(MT) (Blatz et al., 2004; Quirk, 2004; Specia sents metrics in their own space through a dedicated et al., 2009) aims at providing quality scores or set of learnable parameters, allowing for straightforlabels to MT output when translation references ward extensions of the number and type of metrics. are not av"
2021.eval4nlp-1.15,2009.eamt-1.5,0,0.0109312,"U multi gold Figure 3: Attention weights computed between individual metric embeddings, namely DA, TER, chrF and BLEU, along with the multimetric approach (see eqn. 6) and the human annotations (noted gold). Samples extracted from the ET–EN and RO–EN validation sets (top and bottom respectively). 6 Previous Work Since the shift of most NLP tasks towards using large pretrained contextual LMs as basis for taskspecific finetuning, the research community working on QE for MT moved from a classic two-step process of feature engineering followed by machine learning (Blatz et al., 2004; Quirk, 2004; Specia et al., 2009) to an end-to-end training neural-based paradigm. First attempts in this direction were conducted by Kim et al. (2017) with the predictorestimator, which inspired further work in using various types of encoders (Wang et al., 2020), enriching the model with features extracted from NMT models (Moura et al., 2020; Fomicheva et al., 2020a) or modifying the pretraining objective of contextual LMs for QE adaptation (Rubino and Sumita, 2020). 7 Conclusion This paper presented a novel QE architecture for unsupervised token-level quality prediction providing sentence-level explainable decisions from th"
2021.eval4nlp-1.15,2020.acl-main.558,0,0.0121273,"ession respectively. Qi,j = (mj · E) · WiQ However, this approach has limitations While the token-level QE implementation makes use of each input token representation in context thanks to the pretrained LM, the sentence-level QE components relies only on the pooled representation of the input sequence. This approach drastically limits the amount of information flowing through the sentence-level specific set of layers and may force the network to focus more on cues and data artifacts which correlate with QE scores, instead of encoding translation-related features from source and target inputs (Sun et al., 2020). These findings corroborate with the empirical observation made by Kepler et al. (2019), where the authors obtained the best word-level QE results using BERT and ignoring target language features when predicting source quality labels and vice-versa. Additionally, most recent QE approaches do not allow for the interpretability of sentence-level QE predictions at test time and leads to the current state of QE as a set of black-box components. Furthermore, token-level error annotations is costly to produce. 3 Metric Embedding and Attention Motivated by the limitations to contextual LM based QE,"
2021.eval4nlp-1.15,2016.eamt-2.8,0,0.013544,"synthetic data and to train word alignments based on the IBM 2 model (Brown et al., 1993) and trained using fast_align (Dyer et al., 2013). Details about the synthetic data are presented in Table 2.4 The special case of DE–ZH resulting from preliminary experiments, we noticed for this language pair that the translation quality of the synthetic data was low compared to the three other language pairs. We assumed that it was due to two issues: 3 Scripts and procedure available at https://github. com/deep-spin/qe-corpus-builder 4 Parallel corpora were collected from the WMT news translation task (Tiedemann, 2016) and OPUS (Tiedemann, 2016). 150 the quality of the DE–ZH parallel corpora and the performance of the NMT model. To tackle the first issue, we generated our own DE–ZH parallel corpora by pivot-based (back-) translation, starting from a monolingual Chinese corpus composed of CommonCrawl and NewsCrawl 2018 to 2020, translating it into English using an in-house NMT model trained with Marian (Junczys-Dowmunt et al., 2018) on the WMT’21 QE ZH–EN parallel corpus, then translating the English output into German using the EN–DE NMT model released by the WMT’20 QE shared task organizers, resulting in a"
2021.eval4nlp-1.15,2021.eacl-main.50,0,0.0296166,"Missing"
2021.eval4nlp-1.15,2020.acl-main.151,0,0.166826,"present in Table 3 the results obtained on the Eval4NLP 2021 shared task as reported by the organizers, including our baselines and final submissions along with the three baselines proposed QE pretraining was conducted per language by the organizers, namely random scores, Tranpair starting from the XLM-R checkpoint presented sQuest (Ranasinghe et al., 2020b) combined with in Section 4.1 using two different random seeds and LIME (Ribeiro et al., 2016) (noted Official baseline learning rates. Additionally, four QE pretraining were conducted on the concatenation of all syn- 1), and XMoverScore (Zhao et al., 2020) combined with SHAP (Lundberg and Lee, 2017) (noted Ofthetic data using four different random seeds and ficial baseline 2). Our baselines are composed of two learning rates. Training was ran for two epochs ensembles of two finetuned language-specific QE for the language specific models and for a single pretrained models while our final submissions are epoch for the remaining ones. We restricted the composed of ensembles of eight finetuned models length of training samples to a minimum of 5 and a maximum of 128 subword tokens for the bilin- for each of token-level tasks (source and target) and"
2021.mtsummit-research.10,D19-1146,1,0.842567,"., 2017) seeks to mitigate overconfident predictions but is not known to work well Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 115 for NMT. Our method is intended to complement these techniques, i.e., label smoothing and softmax entropy maximization, and not necessarily replace them. Existing methods effective for low-resource language pairs include data augmentation via back-translating additional monolingual data (Sennrich et al., 2016), exploitation of multilingualism (Firat et al., 2016; Zoph et al., 2016; Dabre et al., 2019), and pre-training on monolingual data (Devlin et al., 2019; Song et al., 2019; Mao et al., 2020). These require more training time and resources, while ours does not. 3 Softmax Tempering Softmax tempering (Hinton et al., 2015) consists of two tiny changes in the implementation of the training phase of any neural model used for classification. Assume that Di ∈ RV is the logit output of the decoder for the i-th word prediction in the target language sentence, Yi , where V stands for the target vocabulary size, and that Pi = P (Yi |Y&lt;i , X) = softmax (Di ) represents the softmax function produci"
2021.mtsummit-research.10,N19-1423,0,0.0320029,"not known to work well Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 115 for NMT. Our method is intended to complement these techniques, i.e., label smoothing and softmax entropy maximization, and not necessarily replace them. Existing methods effective for low-resource language pairs include data augmentation via back-translating additional monolingual data (Sennrich et al., 2016), exploitation of multilingualism (Firat et al., 2016; Zoph et al., 2016; Dabre et al., 2019), and pre-training on monolingual data (Devlin et al., 2019; Song et al., 2019; Mao et al., 2020). These require more training time and resources, while ours does not. 3 Softmax Tempering Softmax tempering (Hinton et al., 2015) consists of two tiny changes in the implementation of the training phase of any neural model used for classification. Assume that Di ∈ RV is the logit output of the decoder for the i-th word prediction in the target language sentence, Yi , where V stands for the target vocabulary size, and that Pi = P (Yi |Y&lt;i , X) = softmax (Di ) represents the softmax function producing the probability distribution, where X and Y&lt;i indicate t"
2021.mtsummit-research.10,P17-2012,0,0.0209806,"r this is over-fitting (Zoph et al., 2016; Koehn and Knowles, 2017), where the softmax distribution (sparse vector) ends up resembling the label distribution (one-hot vector). There are several solutions that address this issue, of which the two most effective ones are transfer learning and model regularization. Transfer learning can sometimes be considered as data regularization and comes in the form of monolingual or cross-lingual (multilingual) fashion (Zoph et al., 2016; Song et al., 2019), pseudo-parallel data generation (back-translation) (Sennrich et al., 2016), or multi-task learning (Eriguchi et al., 2017). On the other hand, model regularization techniques place constraints on the learning of model parameters in order to aid the model to learn robust representations that positively impact model performance. Among existing model regularization methods, dropout (Srivastava et al., 2014) is most commonly used and is known to be effective regardless of the size of data. Label smoothing (Szegedy Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 114 et al., 2016) is another effective approach that uses smoothed label vecto"
2021.mtsummit-research.10,N16-1101,0,0.0323529,"max entropy maximization (Pereyra et al., 2017) seeks to mitigate overconfident predictions but is not known to work well Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 115 for NMT. Our method is intended to complement these techniques, i.e., label smoothing and softmax entropy maximization, and not necessarily replace them. Existing methods effective for low-resource language pairs include data augmentation via back-translating additional monolingual data (Sennrich et al., 2016), exploitation of multilingualism (Firat et al., 2016; Zoph et al., 2016; Dabre et al., 2019), and pre-training on monolingual data (Devlin et al., 2019; Song et al., 2019; Mao et al., 2020). These require more training time and resources, while ours does not. 3 Softmax Tempering Softmax tempering (Hinton et al., 2015) consists of two tiny changes in the implementation of the training phase of any neural model used for classification. Assume that Di ∈ RV is the logit output of the decoder for the i-th word prediction in the target language sentence, Yi , where V stands for the target vocabulary size, and that Pi = P (Yi |Y&lt;i , X) = softmax (Di )"
2021.mtsummit-research.10,W17-3204,0,0.0278434,"known to give state-of-the-art results for a large variety of language pairs. NMT for high-resource language pairs is straightforward: choose an NMT architecture and implementation, and train a model on all existing data by minimizing the softmax cross-entropy loss, i.e., cross-entropy between the softmax distribution and the label distribution typically represented with a one-hot vector. In contrast, for low-resource language pairs, this does not work well due to the inability of neural networks to generalize from small amounts of data. One reason for this is over-fitting (Zoph et al., 2016; Koehn and Knowles, 2017), where the softmax distribution (sparse vector) ends up resembling the label distribution (one-hot vector). There are several solutions that address this issue, of which the two most effective ones are transfer learning and model regularization. Transfer learning can sometimes be considered as data regularization and comes in the form of monolingual or cross-lingual (multilingual) fashion (Zoph et al., 2016; Song et al., 2019), pseudo-parallel data generation (back-translation) (Sennrich et al., 2016), or multi-task learning (Eriguchi et al., 2017). On the other hand, model regularization tec"
2021.mtsummit-research.10,2020.lrec-1.454,1,0.83534,"he 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 115 for NMT. Our method is intended to complement these techniques, i.e., label smoothing and softmax entropy maximization, and not necessarily replace them. Existing methods effective for low-resource language pairs include data augmentation via back-translating additional monolingual data (Sennrich et al., 2016), exploitation of multilingualism (Firat et al., 2016; Zoph et al., 2016; Dabre et al., 2019), and pre-training on monolingual data (Devlin et al., 2019; Song et al., 2019; Mao et al., 2020). These require more training time and resources, while ours does not. 3 Softmax Tempering Softmax tempering (Hinton et al., 2015) consists of two tiny changes in the implementation of the training phase of any neural model used for classification. Assume that Di ∈ RV is the logit output of the decoder for the i-th word prediction in the target language sentence, Yi , where V stands for the target vocabulary size, and that Pi = P (Yi |Y&lt;i , X) = softmax (Di ) represents the softmax function producing the probability distribution, where X and Y&lt;i indicate the given source sentence and the past"
2021.mtsummit-research.10,D17-1156,0,0.0145084,"max tempering is also used in model calibration (Guo et al., 2017; Kumar and Sarawagi, 2019), where the temperature coefficient is optimized on the development set in order to penalize overconfident predictions, which is a common practice in low-resource settings. While model calibration is performed after a model is trained, we use softmax tempering during training. We regard softmax tempering as a regularization technique, since it adds noise to NMT model training. Thus, it is related to techniques, such as LN regularization (Ng, 2004), dropout (Srivastava et al., 2014), and tuneout (Miceli Barone et al., 2017). The most important aspect of our method is that it is only applied at the softmax layer whereas other regularization techniques add noise to several parts of the entire model. Label smoothing (Szegedy et al., 2016), which is known to help low-resource NMT (Sennrich and Zhang, 2019), is highly related to our idea, where the key difference is that label smoothing affects the label distributions whereas softmax tempering affects the softmax distributions. On a related note, softmax entropy maximization (Pereyra et al., 2017) seeks to mitigate overconfident predictions but is not known to work w"
2021.mtsummit-research.10,P02-1040,0,0.10997,"or v1.14.4 For “Transformer Base” and “Transformer Big” models, we used the hyper-parameter settings in transformer base single gpu and transformer big single gpu, respectively. Label smoothing of 0.1 was used. We used the internal sub-word tokenization mechanism of tensor2tensor with separate source and target language vocabularies of size 8,192 and 32,768 for low-resource and high-resource settings, respectively. We trained our models for each of the softmax temperature values, 1.0 (default softmax), 1.2, 1.4, 1.6, 1.8, 2.0, 3.0, 4.0, 5.0, and 10.0. We used early-stopping on the BLEU score (Papineni et al., 2002) for the development set which was evaluated every 1k iterations. Our early-stopping mechanism halts training when the BLEU score does not improve over 10 consecutive evaluation steps. For decoding, we averaged the final 10 checkpoints, and evaluated beam search and greedy search. Note that the training time temperature coefficient was used during decoding as well. If this is not done then the softmax distributions will be extremely sharp and beam search will collapse to greedy search. 4.3 Evaluation Criteria We evaluated translation quality of each model using BLEU (Papineni et al., 2002) pro"
2021.mtsummit-research.10,W18-6319,0,0.0218523,"set which was evaluated every 1k iterations. Our early-stopping mechanism halts training when the BLEU score does not improve over 10 consecutive evaluation steps. For decoding, we averaged the final 10 checkpoints, and evaluated beam search and greedy search. Note that the training time temperature coefficient was used during decoding as well. If this is not done then the softmax distributions will be extremely sharp and beam search will collapse to greedy search. 4.3 Evaluation Criteria We evaluated translation quality of each model using BLEU (Papineni et al., 2002) provided by SacreBLEU (Post, 2018).5 The optimal temperature (Topt ) for the tempered model was determined based on greedy-search BLEU score on the development set, given that beam- and greedy-search score improvements are almost always correlated. We therefore used these optimal temperature models to perform beam search, where the beam width (among 2, 4, 6, 8, 10, and 12) and length penalty (among 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, and 1.4) were tuned on the development set. We performed statistical significance testing6 to determine if differences in BLEU are significant. 4.4 Results in Low-Resource Settings Table 1 sho"
2021.mtsummit-research.10,P16-1009,0,0.11837,"ralize from small amounts of data. One reason for this is over-fitting (Zoph et al., 2016; Koehn and Knowles, 2017), where the softmax distribution (sparse vector) ends up resembling the label distribution (one-hot vector). There are several solutions that address this issue, of which the two most effective ones are transfer learning and model regularization. Transfer learning can sometimes be considered as data regularization and comes in the form of monolingual or cross-lingual (multilingual) fashion (Zoph et al., 2016; Song et al., 2019), pseudo-parallel data generation (back-translation) (Sennrich et al., 2016), or multi-task learning (Eriguchi et al., 2017). On the other hand, model regularization techniques place constraints on the learning of model parameters in order to aid the model to learn robust representations that positively impact model performance. Among existing model regularization methods, dropout (Srivastava et al., 2014) is most commonly used and is known to be effective regardless of the size of data. Label smoothing (Szegedy Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 114 et al., 2016) is another e"
2021.mtsummit-research.10,P19-1021,0,0.215095,"learn robust representations that positively impact model performance. Among existing model regularization methods, dropout (Srivastava et al., 2014) is most commonly used and is known to be effective regardless of the size of data. Label smoothing (Szegedy Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 114 et al., 2016) is another effective approach that uses smoothed label vectors as opposed to onehot label vectors. Previous work on NMT has shown that label smoothing is very effective in low-resource settings (Sennrich and Zhang, 2019) and we believe that this deserves further study. We thus focus on a technique that does not need additional data and can complement dropout and label smoothing in an extremely low-resource situation. In this paper, we propose to apply softmax tempering (Hinton et al., 2015) to the training of NMT models. Softmax tempering is realized by dividing the pre-softmax logits with a positive real number greater than 1.0. This leads to a smoother softmax probability distribution, which is then used to compute the cross-entropy loss. Softmax tempering has been devised and used regularly in knowledge di"
2021.mtsummit-research.10,D16-1163,0,0.0605124,"tion models and is known to give state-of-the-art results for a large variety of language pairs. NMT for high-resource language pairs is straightforward: choose an NMT architecture and implementation, and train a model on all existing data by minimizing the softmax cross-entropy loss, i.e., cross-entropy between the softmax distribution and the label distribution typically represented with a one-hot vector. In contrast, for low-resource language pairs, this does not work well due to the inability of neural networks to generalize from small amounts of data. One reason for this is over-fitting (Zoph et al., 2016; Koehn and Knowles, 2017), where the softmax distribution (sparse vector) ends up resembling the label distribution (one-hot vector). There are several solutions that address this issue, of which the two most effective ones are transfer learning and model regularization. Transfer learning can sometimes be considered as data regularization and comes in the form of monolingual or cross-lingual (multilingual) fashion (Zoph et al., 2016; Song et al., 2019), pseudo-parallel data generation (back-translation) (Sennrich et al., 2016), or multi-task learning (Eriguchi et al., 2017). On the other hand"
2021.mtsummit-research.18,D16-1162,0,0.0113992,"necessity and availability of each reference depending on the given skopos. Recent advances in MT go beyond segment-level and/or text-to-text processing. For instance, Voita et al. (2019) focused on several discourse-level issues, i.e., deixis, lexical cohesion, and ellipsis, occurring in segment-level text-to-text MT. Following studies proved that context-aware decoding that refers to several preceding segments better handles these linguistic phenomena (Lopes et al., 2020). There are several focused studies on exploiting extra-document and non-linguistic information, including terminologies (Arthur et al., 2016; Hasler et al., 2018), politeness (Sennrich et al., 2016a), domain (Chu et al., 2017; Kobus et al., 2017; Bapna and Firat, 2019), style (Niu et al., 2017; Michel and Neubig, 2018b), markups (Chatterjee et al., 2017; Hashimoto et al., 2019), and external lexical knowledge (Moussallem et al., 2019). However, the information indispensable for producing a proper translation have not been thoroughly studied. More importantly, no work guarantees to perfectly reflect such information. The MT community has benefited from manual analyses of translation issues4 caused by MT systems. Existing methodolog"
2021.mtsummit-research.18,D19-1165,0,0.0165961,"or text-to-text processing. For instance, Voita et al. (2019) focused on several discourse-level issues, i.e., deixis, lexical cohesion, and ellipsis, occurring in segment-level text-to-text MT. Following studies proved that context-aware decoding that refers to several preceding segments better handles these linguistic phenomena (Lopes et al., 2020). There are several focused studies on exploiting extra-document and non-linguistic information, including terminologies (Arthur et al., 2016; Hasler et al., 2018), politeness (Sennrich et al., 2016a), domain (Chu et al., 2017; Kobus et al., 2017; Bapna and Firat, 2019), style (Niu et al., 2017; Michel and Neubig, 2018b), markups (Chatterjee et al., 2017; Hashimoto et al., 2019), and external lexical knowledge (Moussallem et al., 2019). However, the information indispensable for producing a proper translation have not been thoroughly studied. More importantly, no work guarantees to perfectly reflect such information. The MT community has benefited from manual analyses of translation issues4 caused by MT systems. Existing methodologies for analyzing translation issues in MT outputs can be two-fold: (a) comparisons of independent products, i.e., MT outputs and"
2021.mtsummit-research.18,W18-6402,0,0.0191888,"anslation. Recent studies on neural MT (NMT) have addressed issues beyond 1 In this paper, we use “segment” for the unit of inputs for MT systems rather than “sentence,” because a segment is not necessarily composed of a single sentence, but can often be multiple sentences or non-sentential textual fragments. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 215 this formulation, exploiting further information such as document-level textual context (Voita et al., 2018, 2019; Lopes et al., 2020) and other modalities (Barrault et al., 2018). There are also several focused studies on exploiting extra-document and non-linguistic information. However, such information has not been extensively discussed. As a result, in translation production workflows at translation service providers (TSPs), where MT outputs are treated as draft translations, heavy human labor is necessary to fill the gap between MT outputs and translations in addition to resolving issues at the text-to-text level, for instance, by manual post-editing (PE). To design and establish more practical ways of exploiting MT systems in translation production workflows as w"
2021.mtsummit-research.18,J08-1001,0,0.0133749,"ead by some artificial examples, such as combinations of preferential edits in both segmentlevel PE and document-level PE. Class (b) examples exhibit revisions made by referring to the textual information in the document, but no more than that. They appeared at all issue levels in the typology except level 3, grammaticality, and the majority were either X16 (incohesive) or X3 (content distortion). To translate the mentions of each entity coherently and cohesively (Voita et al., 2019), we need to identify the correct referent of each mention. In the literature, a matrix called the entity grid (Barzilay and Lapata, 2008) is used to represent the appearance of entities and segments in the given source document. Actively studied document-level text-to-text MT might be able to capture such information, for instance, by enhancing the self-attention mechanisms (Vaswani et al., 2017; Maruf et al., 2019; Beltagy et al., 2020). However, as we confirmed in our analysis (Section 5.1), referents are not necessarily given in the source document, and we hence must seek reliable extra-document information. 6 Discussion and Future Directions Techniques for MT have been advanced thanks to the simplified problem setting, i.e."
2021.mtsummit-research.18,2020.lrec-1.461,0,0.0225809,"Missing"
2021.mtsummit-research.18,W17-4716,0,0.0147903,"ourse-level issues, i.e., deixis, lexical cohesion, and ellipsis, occurring in segment-level text-to-text MT. Following studies proved that context-aware decoding that refers to several preceding segments better handles these linguistic phenomena (Lopes et al., 2020). There are several focused studies on exploiting extra-document and non-linguistic information, including terminologies (Arthur et al., 2016; Hasler et al., 2018), politeness (Sennrich et al., 2016a), domain (Chu et al., 2017; Kobus et al., 2017; Bapna and Firat, 2019), style (Niu et al., 2017; Michel and Neubig, 2018b), markups (Chatterjee et al., 2017; Hashimoto et al., 2019), and external lexical knowledge (Moussallem et al., 2019). However, the information indispensable for producing a proper translation have not been thoroughly studied. More importantly, no work guarantees to perfectly reflect such information. The MT community has benefited from manual analyses of translation issues4 caused by MT systems. Existing methodologies for analyzing translation issues in MT outputs can be two-fold: (a) comparisons of independent products, i.e., MT outputs and human translations (Popovi´c and Ney, 2011; Irvine et al., 2013; Toral, 2020), and (b"
2021.mtsummit-research.18,2016.amta-researchers.8,0,0.0149463,"nformation indispensable for translation, such as those we described in Section 5.1, rather than indirectly representing them with text data. For instance, to enforce the use of particular expressions specified by pre-compiled terminologies and style specifications, we need to improve the decoding mechanism, such as constrained decoding (Hasler et al., 2018; Post and Vilar, 2018; Zhang et al., 2018). Style specifications and domain-specific knowledge might be learned from text data in a given fine-grained domain, such as the one in Figure 2. We can see related work in adaptive data selection (Chen et al., 2016) and extreme adaptation (Michel and Neubig, 2018a). Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 224 In addition to the enhancement of MT systems, we should also establish reliable and effective ways for identifying critical issues in MT outputs as well as determining translation scenarios where MT is promising or hopeless. For instance, word frequency and sentence length affect the segment-level MT quality (Koehn and Knowles, 2017). Such findings motivate the pre-editing of segments prior to decoding (Pym, 1990"
2021.mtsummit-research.18,P17-2061,0,0.054315,"ces in MT go beyond segment-level and/or text-to-text processing. For instance, Voita et al. (2019) focused on several discourse-level issues, i.e., deixis, lexical cohesion, and ellipsis, occurring in segment-level text-to-text MT. Following studies proved that context-aware decoding that refers to several preceding segments better handles these linguistic phenomena (Lopes et al., 2020). There are several focused studies on exploiting extra-document and non-linguistic information, including terminologies (Arthur et al., 2016; Hasler et al., 2018), politeness (Sennrich et al., 2016a), domain (Chu et al., 2017; Kobus et al., 2017; Bapna and Firat, 2019), style (Niu et al., 2017; Michel and Neubig, 2018b), markups (Chatterjee et al., 2017; Hashimoto et al., 2019), and external lexical knowledge (Moussallem et al., 2019). However, the information indispensable for producing a proper translation have not been thoroughly studied. More importantly, no work guarantees to perfectly reflect such information. The MT community has benefited from manual analyses of translation issues4 caused by MT systems. Existing methodologies for analyzing translation issues in MT outputs can be two-fold: (a) comparisons o"
2021.mtsummit-research.18,W17-0807,1,0.804569,"vision examples. Finally, we annotated each revision example with the following three types of labels. Need for document-level textual information: whether the textual information outside the segment but within the document was necessary to solve the issue. Need for extra information: whether any extra-document and/or non-linguistic information was necessary to solve the issue. If it was needed, we also noted the information types (more than one if applicable). Issue type: one of the 16 types in a translation issues typology designed for assessing and learning English-to-Japanese translation (Fujita et al., 2017). We chose this typology because its usefulness for this translation direction had been verified, whereas a widely used MQM had not. 18 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/bootstrap-hypothesis-difference-significance.pl Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 221 Document-level textual info. No need Necessary Extra info. No need Necessary (a) 196 (c) 168 (b) 116 (d) 49 Table 3: Revision examples classified according to the types of necessary information. #Examples (a) (b)"
2021.mtsummit-research.18,P14-6007,0,0.0207056,"ld: (a) comparisons of independent products, i.e., MT outputs and human translations (Popovi´c and Ney, 2011; Irvine et al., 2013; Toral, 2020), and (b) annotations of the issues in MT outputs according to pre-determined issue typologies, such as MQM and DQF (Lommel et al., 2015; Ye and Toral, 2020; Freitag et al., 2021). The issues identified in the former approach contain both true errors and preferential differences, i.e., alternative acceptable translations independently selected by MT systems and humans. The latter approach enables us to clearly separate them. For instance, past studies (Hardmeier, 2014; Scarton et al., 2015; Voita et al., 2019) analyzed outputs of segment-level text-to-text MT, showed the limitation of that approach, and encouraged the research on document-level MT. However, they discussed only the differences between two text-to-text approaches. Issues beyond the text-to-text processing, such as those related to extra-document and/or non-linguistic information, have seldom been mentioned (Castilho et al., 2020), and no focused and empirical analysis has been conducted. 3 Subject of Our Case Study Our focus in this paper is to clarify the types of extra-document and/or non-"
2021.mtsummit-research.18,W19-5212,0,0.0143996,"deixis, lexical cohesion, and ellipsis, occurring in segment-level text-to-text MT. Following studies proved that context-aware decoding that refers to several preceding segments better handles these linguistic phenomena (Lopes et al., 2020). There are several focused studies on exploiting extra-document and non-linguistic information, including terminologies (Arthur et al., 2016; Hasler et al., 2018), politeness (Sennrich et al., 2016a), domain (Chu et al., 2017; Kobus et al., 2017; Bapna and Firat, 2019), style (Niu et al., 2017; Michel and Neubig, 2018b), markups (Chatterjee et al., 2017; Hashimoto et al., 2019), and external lexical knowledge (Moussallem et al., 2019). However, the information indispensable for producing a proper translation have not been thoroughly studied. More importantly, no work guarantees to perfectly reflect such information. The MT community has benefited from manual analyses of translation issues4 caused by MT systems. Existing methodologies for analyzing translation issues in MT outputs can be two-fold: (a) comparisons of independent products, i.e., MT outputs and human translations (Popovi´c and Ney, 2011; Irvine et al., 2013; Toral, 2020), and (b) annotations of the issu"
2021.mtsummit-research.18,N18-2081,0,0.0428904,"Missing"
2021.mtsummit-research.18,P18-2070,0,0.0140743,"in the translation production workflow at TSPs for a decade, lies in that direction. In this way, to reduce the cognitive load of PE, we must continue to enhance both wheels, i.e., improving MT systems and determining the best practices in using them. As confirmed in Section 4.2, segment-level text-to-text MT still has much room for improvement. Yet, as shown in recent studies, textual information within the entire source document is useful. To generate cohesive texts, we should incorporate the latest outcomes in discourse processing and natural language generation, such as discourse parsing (Jia et al., 2018) and generating referential expressions (Paraboni et al., 2007). To assess MT outputs for further improvement while reducing the human labor in PE, we also need to invent documentlevel automatic evaluation methods, preferably analytic ones rather than holistic ones. Ultimately and ideally, we should also consider going beyond text-to-text processing, seeking better ways for incorporating information indispensable for translation, such as those we described in Section 5.1, rather than indirectly representing them with text data. For instance, to enforce the use of particular expressions specifi"
2021.mtsummit-research.18,P18-4020,0,0.0119832,"the ALT. For each source and target language, a sub-word vocabulary was also created from the corresponding side of this corpus: we determined 32k sub-words with byte-pair encoding (Sennrich et al., 2016b) after tokenization. Then, we fine-tuned the model parameters on a mixture of TexTra and the ALT training data. Following Chu et al. (2017), we used a balanced mixture of the two corpora by inflating the ALT training data K times and randomly sampling the same number of segment pairs from TexTra. Finally, we further fine-tuned the NMT model on the ALT training data only. We used Marian NMT (Junczys-Dowmunt et al., 2018)11 for all the NMT training and decoding processes, using the Transformer Base model and the hyper-parameters for training as 9 We are aware that our system would not be state of the art because we do not use synthetic parallel data, a model ensemble, nor re-ranking. However, because these are all the methods for improving segment-level text-to-text MT, we assume that omitting them does not affect the main issues that we identify during the document-level full PE stage. 10 The size is confidential. The generic model can be used via https://mt-auto-minhon-mlt.ucri.jgn-x.jp. 11 https://github.co"
2021.mtsummit-research.18,kobus-etal-2017-domain,0,0.0217786,"d segment-level and/or text-to-text processing. For instance, Voita et al. (2019) focused on several discourse-level issues, i.e., deixis, lexical cohesion, and ellipsis, occurring in segment-level text-to-text MT. Following studies proved that context-aware decoding that refers to several preceding segments better handles these linguistic phenomena (Lopes et al., 2020). There are several focused studies on exploiting extra-document and non-linguistic information, including terminologies (Arthur et al., 2016; Hasler et al., 2018), politeness (Sennrich et al., 2016a), domain (Chu et al., 2017; Kobus et al., 2017; Bapna and Firat, 2019), style (Niu et al., 2017; Michel and Neubig, 2018b), markups (Chatterjee et al., 2017; Hashimoto et al., 2019), and external lexical knowledge (Moussallem et al., 2019). However, the information indispensable for producing a proper translation have not been thoroughly studied. More importantly, no work guarantees to perfectly reflect such information. The MT community has benefited from manual analyses of translation issues4 caused by MT systems. Existing methodologies for analyzing translation issues in MT outputs can be two-fold: (a) comparisons of independent produc"
2021.mtsummit-research.18,W17-3204,0,0.0114335,"main, such as the one in Figure 2. We can see related work in adaptive data selection (Chen et al., 2016) and extreme adaptation (Michel and Neubig, 2018a). Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 224 In addition to the enhancement of MT systems, we should also establish reliable and effective ways for identifying critical issues in MT outputs as well as determining translation scenarios where MT is promising or hopeless. For instance, word frequency and sentence length affect the segment-level MT quality (Koehn and Knowles, 2017). Such findings motivate the pre-editing of segments prior to decoding (Pym, 1990; Miyata and Fujita, 2021). From a general perspective, we should consider educating people (all people) so that they acquire two types of literacy: translation literacy for understanding the norms, skopos, and other specifications in their translation task (Klitg˚ard, 2018), and MT literacy for understanding the characteristics of the intended MT service, which helps minimize potential risks (Bowker and Ciro, 2019). We believe that our method for clearly delineating between translation and the translation that te"
2021.mtsummit-research.18,2020.eamt-1.24,0,0.378148,"oes not necessarily qualify as a proper translation. Recent studies on neural MT (NMT) have addressed issues beyond 1 In this paper, we use “segment” for the unit of inputs for MT systems rather than “sentence,” because a segment is not necessarily composed of a single sentence, but can often be multiple sentences or non-sentential textual fragments. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 215 this formulation, exploiting further information such as document-level textual context (Voita et al., 2018, 2019; Lopes et al., 2020) and other modalities (Barrault et al., 2018). There are also several focused studies on exploiting extra-document and non-linguistic information. However, such information has not been extensively discussed. As a result, in translation production workflows at translation service providers (TSPs), where MT outputs are treated as draft translations, heavy human labor is necessary to fill the gap between MT outputs and translations in addition to resolving issues at the text-to-text level, for instance, by manual post-editing (PE). To design and establish more practical ways of exploiting MT sys"
2021.mtsummit-research.18,N19-1313,0,0.0174545,"ology except level 3, grammaticality, and the majority were either X16 (incohesive) or X3 (content distortion). To translate the mentions of each entity coherently and cohesively (Voita et al., 2019), we need to identify the correct referent of each mention. In the literature, a matrix called the entity grid (Barzilay and Lapata, 2008) is used to represent the appearance of entities and segments in the given source document. Actively studied document-level text-to-text MT might be able to capture such information, for instance, by enhancing the self-attention mechanisms (Vaswani et al., 2017; Maruf et al., 2019; Beltagy et al., 2020). However, as we confirmed in our analysis (Section 5.1), referents are not necessarily given in the source document, and we hence must seek reliable extra-document information. 6 Discussion and Future Directions Techniques for MT have been advanced thanks to the simplified problem setting, i.e., textto-text processing, and the advent of automatic evaluation metrics, such as BLEU (Papineni et al., 2002), which are based on comparison with reference translations. However, considering the large gap between what text-to-text MT can ultimately attain and the needs that trans"
2021.mtsummit-research.18,P18-2050,0,0.0701747,"t al. (2019) focused on several discourse-level issues, i.e., deixis, lexical cohesion, and ellipsis, occurring in segment-level text-to-text MT. Following studies proved that context-aware decoding that refers to several preceding segments better handles these linguistic phenomena (Lopes et al., 2020). There are several focused studies on exploiting extra-document and non-linguistic information, including terminologies (Arthur et al., 2016; Hasler et al., 2018), politeness (Sennrich et al., 2016a), domain (Chu et al., 2017; Kobus et al., 2017; Bapna and Firat, 2019), style (Niu et al., 2017; Michel and Neubig, 2018b), markups (Chatterjee et al., 2017; Hashimoto et al., 2019), and external lexical knowledge (Moussallem et al., 2019). However, the information indispensable for producing a proper translation have not been thoroughly studied. More importantly, no work guarantees to perfectly reflect such information. The MT community has benefited from manual analyses of translation issues4 caused by MT systems. Existing methodologies for analyzing translation issues in MT outputs can be two-fold: (a) comparisons of independent products, i.e., MT outputs and human translations (Popovi´c and Ney, 2011; Irvin"
2021.mtsummit-research.18,D18-1050,0,0.103286,"t al. (2019) focused on several discourse-level issues, i.e., deixis, lexical cohesion, and ellipsis, occurring in segment-level text-to-text MT. Following studies proved that context-aware decoding that refers to several preceding segments better handles these linguistic phenomena (Lopes et al., 2020). There are several focused studies on exploiting extra-document and non-linguistic information, including terminologies (Arthur et al., 2016; Hasler et al., 2018), politeness (Sennrich et al., 2016a), domain (Chu et al., 2017; Kobus et al., 2017; Bapna and Firat, 2019), style (Niu et al., 2017; Michel and Neubig, 2018b), markups (Chatterjee et al., 2017; Hashimoto et al., 2019), and external lexical knowledge (Moussallem et al., 2019). However, the information indispensable for producing a proper translation have not been thoroughly studied. More importantly, no work guarantees to perfectly reflect such information. The MT community has benefited from manual analyses of translation issues4 caused by MT systems. Existing methodologies for analyzing translation issues in MT outputs can be two-fold: (a) comparisons of independent products, i.e., MT outputs and human translations (Popovi´c and Ney, 2011; Irvin"
2021.mtsummit-research.18,2021.eacl-main.132,1,0.692233,"nd extreme adaptation (Michel and Neubig, 2018a). Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 224 In addition to the enhancement of MT systems, we should also establish reliable and effective ways for identifying critical issues in MT outputs as well as determining translation scenarios where MT is promising or hopeless. For instance, word frequency and sentence length affect the segment-level MT quality (Koehn and Knowles, 2017). Such findings motivate the pre-editing of segments prior to decoding (Pym, 1990; Miyata and Fujita, 2021). From a general perspective, we should consider educating people (all people) so that they acquire two types of literacy: translation literacy for understanding the norms, skopos, and other specifications in their translation task (Klitg˚ard, 2018), and MT literacy for understanding the characteristics of the intended MT service, which helps minimize potential risks (Bowker and Ciro, 2019). We believe that our method for clearly delineating between translation and the translation that text-to-text MT can ultimately attain as well as our case-study findings can be useful resources for such edu"
2021.mtsummit-research.18,D19-5201,0,0.0184227,"a translation. Among several translation tasks, this paper takes an English-to-Japanese news translation task as a case study and presents our indepth analysis. We chose it for two reasons. First, despite the high demand for it, the task is still very difficult, since the two languages are linguistically distant and used in substantially different cultures (cf. English-to-German studied by Scarton et al. (2015)). The norms for news texts are also substantially different in these languages, making them more difficult to translate than texts in other domains, such as scientific paper abstracts (Nakazawa et al., 2019) and patent documents (Goto et al., 2013). The second reason is that we wished to conduct an indepth analytic assessment of translation (see Section 5) by ourselves. We have a linguist who is highly competent in both linguistics and translation and has ample experiences in the analytic assessment of both MT outputs and human translations. As material for this case study, we used the documents in the Asian Language Treebank (ALT) (Riza et al., 2016).5 Table 1 gives statistics for the English source documents and Japanese target documents produced by professional human translators, where the num"
2021.mtsummit-research.18,D17-1299,0,0.0285172,"instance, Voita et al. (2019) focused on several discourse-level issues, i.e., deixis, lexical cohesion, and ellipsis, occurring in segment-level text-to-text MT. Following studies proved that context-aware decoding that refers to several preceding segments better handles these linguistic phenomena (Lopes et al., 2020). There are several focused studies on exploiting extra-document and non-linguistic information, including terminologies (Arthur et al., 2016; Hasler et al., 2018), politeness (Sennrich et al., 2016a), domain (Chu et al., 2017; Kobus et al., 2017; Bapna and Firat, 2019), style (Niu et al., 2017; Michel and Neubig, 2018b), markups (Chatterjee et al., 2017; Hashimoto et al., 2019), and external lexical knowledge (Moussallem et al., 2019). However, the information indispensable for producing a proper translation have not been thoroughly studied. More importantly, no work guarantees to perfectly reflect such information. The MT community has benefited from manual analyses of translation issues4 caused by MT systems. Existing methodologies for analyzing translation issues in MT outputs can be two-fold: (a) comparisons of independent products, i.e., MT outputs and human translations (Popo"
2021.mtsummit-research.18,J07-2004,0,0.0790838,"Missing"
2021.mtsummit-research.18,J11-4002,0,0.0961144,"Missing"
2021.mtsummit-research.18,W18-6319,0,0.0145353,"ummit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 219 used in Vaswani et al. (2017). We terminated the training at each phase by early-stopping with a patience of 5, regarding the model perplexity on the ALT development data, computed after every T iterations, as the evaluation criterion. The value of T was set to 5,000 for the phase 1, and 10 for the phases 2 and 3. For the value of sample size K in phase 2, we selected 32 from the options 1, 2, 4, 8, 16, 32, and 64 according to the BLEU score (Papineni et al., 2002) on the ALT development data, computed by SacreBLEU (Post, 2018).12 When decoding the ALT test data, the beam size was fixed 10, and the value for the length penalty was tuned on the ALT development data and set to 0.8. 4.2 Stage (2) Segment-level Minimal PE To perform a segment-level PE, we isolated each segment from the others in the same document by shuffling the pairs of source segment and corresponding segment-level MT output across all the test documents. We then asked13 an experienced, ISO-certified TSP with well-designed workflows for translation (ISO/TC37, 2015) and PE (ISO/TC37, 2017) to revise the MT output of each segment independently without"
2021.mtsummit-research.18,N18-1119,0,0.0131005,"so need to invent documentlevel automatic evaluation methods, preferably analytic ones rather than holistic ones. Ultimately and ideally, we should also consider going beyond text-to-text processing, seeking better ways for incorporating information indispensable for translation, such as those we described in Section 5.1, rather than indirectly representing them with text data. For instance, to enforce the use of particular expressions specified by pre-compiled terminologies and style specifications, we need to improve the decoding mechanism, such as constrained decoding (Hasler et al., 2018; Post and Vilar, 2018; Zhang et al., 2018). Style specifications and domain-specific knowledge might be learned from text data in a given fine-grained domain, such as the one in Figure 2. We can see related work in adaptive data selection (Chen et al., 2016) and extreme adaptation (Michel and Neubig, 2018a). Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 224 In addition to the enhancement of MT systems, we should also establish reliable and effective ways for identifying critical issues in MT outputs as well as determining translation"
2021.mtsummit-research.18,W15-4916,0,0.0608791,"Missing"
2021.mtsummit-research.18,N16-1005,0,0.139353,"on the given skopos. Recent advances in MT go beyond segment-level and/or text-to-text processing. For instance, Voita et al. (2019) focused on several discourse-level issues, i.e., deixis, lexical cohesion, and ellipsis, occurring in segment-level text-to-text MT. Following studies proved that context-aware decoding that refers to several preceding segments better handles these linguistic phenomena (Lopes et al., 2020). There are several focused studies on exploiting extra-document and non-linguistic information, including terminologies (Arthur et al., 2016; Hasler et al., 2018), politeness (Sennrich et al., 2016a), domain (Chu et al., 2017; Kobus et al., 2017; Bapna and Firat, 2019), style (Niu et al., 2017; Michel and Neubig, 2018b), markups (Chatterjee et al., 2017; Hashimoto et al., 2019), and external lexical knowledge (Moussallem et al., 2019). However, the information indispensable for producing a proper translation have not been thoroughly studied. More importantly, no work guarantees to perfectly reflect such information. The MT community has benefited from manual analyses of translation issues4 caused by MT systems. Existing methodologies for analyzing translation issues in MT outputs can be"
2021.mtsummit-research.18,P16-1162,0,0.0386281,"on the given skopos. Recent advances in MT go beyond segment-level and/or text-to-text processing. For instance, Voita et al. (2019) focused on several discourse-level issues, i.e., deixis, lexical cohesion, and ellipsis, occurring in segment-level text-to-text MT. Following studies proved that context-aware decoding that refers to several preceding segments better handles these linguistic phenomena (Lopes et al., 2020). There are several focused studies on exploiting extra-document and non-linguistic information, including terminologies (Arthur et al., 2016; Hasler et al., 2018), politeness (Sennrich et al., 2016a), domain (Chu et al., 2017; Kobus et al., 2017; Bapna and Firat, 2019), style (Niu et al., 2017; Michel and Neubig, 2018b), markups (Chatterjee et al., 2017; Hashimoto et al., 2019), and external lexical knowledge (Moussallem et al., 2019). However, the information indispensable for producing a proper translation have not been thoroughly studied. More importantly, no work guarantees to perfectly reflect such information. The MT community has benefited from manual analyses of translation issues4 caused by MT systems. Existing methodologies for analyzing translation issues in MT outputs can be"
2021.mtsummit-research.18,2006.amta-papers.25,0,0.142737,"Missing"
2021.mtsummit-research.18,W19-6627,0,0.0173324,"at can be resolved by referring only to the given textual information as much as possible, we decided to obtain translations that are attainable but closest to the outputs of text-to-text MT through minimal PE; we explicitly constrain the human workers by (i) prohibiting them from referring to any information other than the textual information and (ii) allowing only minimal edits,7 while also avoiding subjective stylistic changes.8 Even though document-level text-to-text MT 6 Translation obtainable through this method is not necessarily of high quality because it is, in the end, post-editese (Toral, 2019). We plan to analyze the gap between PE-based translation and high-quality human translation, i.e., the art of translation, in our future work. 7 This might be comparable with the goal of light PE (ISO/TC37, 2017): “obtain a merely comprehensible text without any attempt to produce a product comparable to a product obtained by human translation.” 8 Scarton et al. (2015) regarded style changes as the translator’s choice. However, according to ISO/TC37 (2015), the appropriate style is not determined by the translators, but by the extra-document specifications for translation, for instance in the"
2021.mtsummit-research.18,2020.eamt-1.20,0,0.0237922,"tterjee et al., 2017; Hashimoto et al., 2019), and external lexical knowledge (Moussallem et al., 2019). However, the information indispensable for producing a proper translation have not been thoroughly studied. More importantly, no work guarantees to perfectly reflect such information. The MT community has benefited from manual analyses of translation issues4 caused by MT systems. Existing methodologies for analyzing translation issues in MT outputs can be two-fold: (a) comparisons of independent products, i.e., MT outputs and human translations (Popovi´c and Ney, 2011; Irvine et al., 2013; Toral, 2020), and (b) annotations of the issues in MT outputs according to pre-determined issue typologies, such as MQM and DQF (Lommel et al., 2015; Ye and Toral, 2020; Freitag et al., 2021). The issues identified in the former approach contain both true errors and preferential differences, i.e., alternative acceptable translations independently selected by MT systems and humans. The latter approach enables us to clearly separate them. For instance, past studies (Hardmeier, 2014; Scarton et al., 2015; Voita et al., 2019) analyzed outputs of segment-level text-to-text MT, showed the limitation of that app"
2021.mtsummit-research.18,P19-1116,0,0.327343,"uable assets for improving efficiency in personal practices and workflows in TSPs. However, there is neither a comprehensive inven2 http://www.qt21.eu/launchpad/content/multidimensional-quality-metrics 3 https://www.taus.net/data-for-ai/dqf Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 216 tory of references, nor a common view of the extent of the necessity and availability of each reference depending on the given skopos. Recent advances in MT go beyond segment-level and/or text-to-text processing. For instance, Voita et al. (2019) focused on several discourse-level issues, i.e., deixis, lexical cohesion, and ellipsis, occurring in segment-level text-to-text MT. Following studies proved that context-aware decoding that refers to several preceding segments better handles these linguistic phenomena (Lopes et al., 2020). There are several focused studies on exploiting extra-document and non-linguistic information, including terminologies (Arthur et al., 2016; Hasler et al., 2018), politeness (Sennrich et al., 2016a), domain (Chu et al., 2017; Kobus et al., 2017; Bapna and Firat, 2019), style (Niu et al., 2017; Michel and N"
2021.mtsummit-research.18,P18-1117,0,0.102306,"ext-to-text translations does not necessarily qualify as a proper translation. Recent studies on neural MT (NMT) have addressed issues beyond 1 In this paper, we use “segment” for the unit of inputs for MT systems rather than “sentence,” because a segment is not necessarily composed of a single sentence, but can often be multiple sentences or non-sentential textual fragments. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 215 this formulation, exploiting further information such as document-level textual context (Voita et al., 2018, 2019; Lopes et al., 2020) and other modalities (Barrault et al., 2018). There are also several focused studies on exploiting extra-document and non-linguistic information. However, such information has not been extensively discussed. As a result, in translation production workflows at translation service providers (TSPs), where MT outputs are treated as draft translations, heavy human labor is necessary to fill the gap between MT outputs and translations in addition to resolving issues at the text-to-text level, for instance, by manual post-editing (PE). To design and establish more practica"
2021.mtsummit-research.18,2020.eamt-1.14,0,0.0107222,"oducing a proper translation have not been thoroughly studied. More importantly, no work guarantees to perfectly reflect such information. The MT community has benefited from manual analyses of translation issues4 caused by MT systems. Existing methodologies for analyzing translation issues in MT outputs can be two-fold: (a) comparisons of independent products, i.e., MT outputs and human translations (Popovi´c and Ney, 2011; Irvine et al., 2013; Toral, 2020), and (b) annotations of the issues in MT outputs according to pre-determined issue typologies, such as MQM and DQF (Lommel et al., 2015; Ye and Toral, 2020; Freitag et al., 2021). The issues identified in the former approach contain both true errors and preferential differences, i.e., alternative acceptable translations independently selected by MT systems and humans. The latter approach enables us to clearly separate them. For instance, past studies (Hardmeier, 2014; Scarton et al., 2015; Voita et al., 2019) analyzed outputs of segment-level text-to-text MT, showed the limitation of that approach, and encouraged the research on document-level MT. However, they discussed only the differences between two text-to-text approaches. Issues beyond the"
2021.mtsummit-research.18,N18-1120,0,0.012659,"mentlevel automatic evaluation methods, preferably analytic ones rather than holistic ones. Ultimately and ideally, we should also consider going beyond text-to-text processing, seeking better ways for incorporating information indispensable for translation, such as those we described in Section 5.1, rather than indirectly representing them with text data. For instance, to enforce the use of particular expressions specified by pre-compiled terminologies and style specifications, we need to improve the decoding mechanism, such as constrained decoding (Hasler et al., 2018; Post and Vilar, 2018; Zhang et al., 2018). Style specifications and domain-specific knowledge might be learned from text data in a given fine-grained domain, such as the one in Figure 2. We can see related work in adaptive data selection (Chen et al., 2016) and extreme adaptation (Michel and Neubig, 2018a). Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 224 In addition to the enhancement of MT systems, we should also establish reliable and effective ways for identifying critical issues in MT outputs as well as determining translation scenarios where MT i"
C08-1029,P05-1074,0,0.485939,"e various paraphrase phenomena from existing text corpora alone. To date, two streams of research have evolved: one acquires paraphrase knowledge from parallel/comparable corpora, while the other uses the regular corpus. Several alignment techniques have been proposed to acquire paraphrase knowledge from parallel/comparable corpora, imitating the techniques devised for machine translation. Multiple translations of the same text (Barzilay and McKeown, 2001), corresponding articles from multiple news sources (Barzilay and Lee, 2003; Quirk et al., 2004; Dolan et al., 2004), and bilingual corpus (Bannard and Callison-Burch, 2005) have been utilized. Unfortunately, this approach produces only a low coverage because the size of the parallel/comparable corpora is limited. In the second stream, i.e., paraphrase acquisition from the regular corpus, the distributional hypothesis (Harris, 1968) has been adopted. The similarity of two expressions, computed from this hypothesis, is called distributional similarity. The essence of this measure is summarized as follows: Feature representation: to compute the similarity, given expressions are first mapped to certain feature representations. Expressions that co-occur with the give"
C08-1029,P01-1008,0,0.48627,"s has been dedicated to developing context-free paraphrase knowledge. It is typically represented with pairs of fragmentary expressions that satisfy the following conditions: Condition 1. Semantically equivalent Condition 2. Substitutable in some context The most critical issue in developing such knowledge is ensuring the coverage of the paraphrase phenomena. To attain this coverage, we have proposed a strategy for dividing paraphrase phenomena into the following two classes (Fujita et al., 2007): (1) Non-productive (idiosyncratic) paraphrases a. burst into tears ⇔ cried b. comfort ⇔ console (Barzilay and McKeown, 2001) The most critical issue in generating and recognizing paraphrases is development of wide-coverage paraphrase knowledge. Previous work on paraphrase acquisition has collected lexicalized pairs of expressions; however, the results do not ensure full coverage of the various paraphrase phenomena. This paper focuses on productive paraphrases realized by general transformation patterns, and addresses the issues in generating instances of phrasal paraphrases with those patterns. Our probabilistic model computes how two phrases are likely to be correct paraphrases. The model consists of two component"
C08-1029,N03-1003,0,0.151452,"notice that it is hard to acquire paraphrase knowledge that ensures full coverage of the various paraphrase phenomena from existing text corpora alone. To date, two streams of research have evolved: one acquires paraphrase knowledge from parallel/comparable corpora, while the other uses the regular corpus. Several alignment techniques have been proposed to acquire paraphrase knowledge from parallel/comparable corpora, imitating the techniques devised for machine translation. Multiple translations of the same text (Barzilay and McKeown, 2001), corresponding articles from multiple news sources (Barzilay and Lee, 2003; Quirk et al., 2004; Dolan et al., 2004), and bilingual corpus (Bannard and Callison-Burch, 2005) have been utilized. Unfortunately, this approach produces only a low coverage because the size of the parallel/comparable corpora is limited. In the second stream, i.e., paraphrase acquisition from the regular corpus, the distributional hypothesis (Harris, 1968) has been adopted. The similarity of two expressions, computed from this hypothesis, is called distributional similarity. The essence of this measure is summarized as follows: Feature representation: to compute the similarity, given expres"
C08-1029,C04-1051,0,0.0689274,"knowledge that ensures full coverage of the various paraphrase phenomena from existing text corpora alone. To date, two streams of research have evolved: one acquires paraphrase knowledge from parallel/comparable corpora, while the other uses the regular corpus. Several alignment techniques have been proposed to acquire paraphrase knowledge from parallel/comparable corpora, imitating the techniques devised for machine translation. Multiple translations of the same text (Barzilay and McKeown, 2001), corresponding articles from multiple news sources (Barzilay and Lee, 2003; Quirk et al., 2004; Dolan et al., 2004), and bilingual corpus (Bannard and Callison-Burch, 2005) have been utilized. Unfortunately, this approach produces only a low coverage because the size of the parallel/comparable corpora is limited. In the second stream, i.e., paraphrase acquisition from the regular corpus, the distributional hypothesis (Harris, 1968) has been adopted. The similarity of two expressions, computed from this hypothesis, is called distributional similarity. The essence of this measure is summarized as follows: Feature representation: to compute the similarity, given expressions are first mapped to certain feature"
C08-1029,W07-1425,1,0.660659,"a University {fujita,ssato}@nuee.nagoya-u.ac.jp Abstract Most of previous work on generating and recognizing paraphrases has been dedicated to developing context-free paraphrase knowledge. It is typically represented with pairs of fragmentary expressions that satisfy the following conditions: Condition 1. Semantically equivalent Condition 2. Substitutable in some context The most critical issue in developing such knowledge is ensuring the coverage of the paraphrase phenomena. To attain this coverage, we have proposed a strategy for dividing paraphrase phenomena into the following two classes (Fujita et al., 2007): (1) Non-productive (idiosyncratic) paraphrases a. burst into tears ⇔ cried b. comfort ⇔ console (Barzilay and McKeown, 2001) The most critical issue in generating and recognizing paraphrases is development of wide-coverage paraphrase knowledge. Previous work on paraphrase acquisition has collected lexicalized pairs of expressions; however, the results do not ensure full coverage of the various paraphrase phenomena. This paper focuses on productive paraphrases realized by general transformation patterns, and addresses the issues in generating instances of phrasal paraphrases with those patter"
C08-1029,I08-1070,1,0.872049,"the paraphrase phenomena. From the transformation grammar (Har226 ris, 1957), this approach has been adopted by many researchers (Mel’ˇcuk and Polgu`ere, 1987; Jacquemin, 1999; Fujita et al., 2007). An important issue arises when such a pattern is used to generate instances of paraphrases by replacing its variables with specific words. This involves assessing the grammaticality of two expressions in addition to their semantic equivalence and substitutability. As a post-generation assessment of automatically generated productive paraphrases, we have applied distributional similarity measures (Fujita and Sato, 2008). Our findings from a series of empirical experiments are summarized as follows: • Search engines are useful for retrieving the contextual features of predicate phrases despite some limitations (Kilgarriff, 2007). • Distributional similarity measures produce a tolerable level of performance. The grammaticality of a phrase, however, is merely assessed by issuing the phrase as a query to a commercial search engine. Although a more frequent expression is more grammatical, the length bias should also be considered in the assessment. Quirk et al. (2004) built a paraphrase generation model from a mo"
C08-1029,C04-1036,0,0.0143628,"s, is called distributional similarity. The essence of this measure is summarized as follows: Feature representation: to compute the similarity, given expressions are first mapped to certain feature representations. Expressions that co-occur with the given expression, such as adjacent words (Barzilay and McKeown, 2001; Lin and Pantel, 2001), and modifiers/modifiees (Yamamoto, 2002; Weeds et al., 2005), have so far been examined. Feature weighting: to precisely compute the similarity, the weight for each feature is adjusted. Point-wise mutual information (Lin, 1998) and Relative Feature Focus (Geffet and Dagan, 2004) are well-known examples. Feature comparison measures: to convert two feature sets into a scalar value, several measures have been proposed, such as cosine, Lin’s measure (Lin, 1998), Kullback-Leibler (KL) divergence and its variants. While most researchers extract fully-lexicalized pairs of words or word sequences only, two algorithms collect template-like knowledge using dependency parsers. DIRT (Lin and Pantel, 2001) collects pairs of paths in dependency parses that connect two nominal entities. TEASE (Szpektor et al., 2004) discovers dependency sub-parses from the Web, based on sets of rep"
C08-1029,P99-1044,0,0.100126,"the notion of inferential selectional preference and collected expressions that would fill those slots. As mentioned in Section 1, the aim of the studies reviewed here is to collect paraphrase knowledge. Thus, they need not to take the grammaticality of expressions into account. 2.2 Generating paraphrase instances Representing productive paraphrases with a set of general patterns makes them maintainable and attains a higher coverage of the paraphrase phenomena. From the transformation grammar (Har226 ris, 1957), this approach has been adopted by many researchers (Mel’ˇcuk and Polgu`ere, 1987; Jacquemin, 1999; Fujita et al., 2007). An important issue arises when such a pattern is used to generate instances of paraphrases by replacing its variables with specific words. This involves assessing the grammaticality of two expressions in addition to their semantic equivalence and substitutability. As a post-generation assessment of automatically generated productive paraphrases, we have applied distributional similarity measures (Fujita and Sato, 2008). Our findings from a series of empirical experiments are summarized as follows: • Search engines are useful for retrieving the contextual features of pre"
C08-1029,kawahara-kurohashi-2006-case,0,0.0240668,"inichi 1991-2005 (1.5GB, 21M sentences). Yomiuri 2005 (350MB, 4.7M sentences) and Asahi 2005 (180MB, 2.7M sentences). 5 229 6 7 http://developer.yahoo.co.jp/search/ http://chasen.naist.jp/hiki/ChaSen/ where freqcp (r, e) indicates the frequency of the expression e appearing with something in relation r within the given corpus. Two different sorts of corpora are separately used to build two variations of P (f ). The one is Mainichi, which is used for building structured N -gram language models in Section 3.2, while the other is a huge corpus consisting of 470M sentences collected from the Web (Kawahara and Kurohashi, 2006). 4 Experiments 4.1 Data We conducted an empirical experiment to evaluate the proposed model using the test suite developed in (Fujita and Sato, 2008). The test suite consists of 176,541 pairs of paraphrase candidates that are automatically generated using a pattern-based paraphrase generation system (Fujita et al., 2007) for 4,002 relatively high-frequency phrases sampled from a newspaper corpus8 . To evaluate the system from a generation viewpoint, i.e., how well a system can rank a correct candidate first, we extracted paraphrase candidates for 200 randomly sampled source phrases from the t"
C08-1029,P99-1004,0,0.041238,"the four existing measures. where Fs and Ft denote sets of features with positive weights for words s and t, respectively. Although this measure has been widely cited and has so far exhibited good performance, its symmetry seems unnatural. Moreover, it may not work well for dealing with general predicate phrases because it is hard to enumerate all phrases to determine the weights of features w(·, f ). We thus simply adopted the co-occurrence frequency of the phrase and the feature as in (Fujita and Sato, 2008). Skew divergence The skew divergence, a variant of KL divergence, was proposed in (Lee, 1999) based on an insight: the substitutability of one word for another need not be symmetrical. The divergence is given by the following formula: dskew (t, s) = D (Ps αPt + (1 − α)Ps ) , where Ps and Pt are the probability distributions of features for the given original and substituted words s and t, respectively. 0 ≤ α ≤ 1 is a parameter for approximating KL divergence D. The score can be recast into a similarity score via, for example, the following function (Fujita and Sato, 2008): Parskew (s⇒t) = exp (−dskew (t, s)) . This measure offers an advantage: the weight for each feature is determine"
C08-1029,P98-2127,0,0.209155,"pressions, computed from this hypothesis, is called distributional similarity. The essence of this measure is summarized as follows: Feature representation: to compute the similarity, given expressions are first mapped to certain feature representations. Expressions that co-occur with the given expression, such as adjacent words (Barzilay and McKeown, 2001; Lin and Pantel, 2001), and modifiers/modifiees (Yamamoto, 2002; Weeds et al., 2005), have so far been examined. Feature weighting: to precisely compute the similarity, the weight for each feature is adjusted. Point-wise mutual information (Lin, 1998) and Relative Feature Focus (Geffet and Dagan, 2004) are well-known examples. Feature comparison measures: to convert two feature sets into a scalar value, several measures have been proposed, such as cosine, Lin’s measure (Lin, 1998), Kullback-Leibler (KL) divergence and its variants. While most researchers extract fully-lexicalized pairs of words or word sequences only, two algorithms collect template-like knowledge using dependency parsers. DIRT (Lin and Pantel, 2001) collects pairs of paths in dependency parses that connect two nominal entities. TEASE (Szpektor et al., 2004) discovers depe"
C08-1029,J87-3006,0,0.0371765,"Missing"
C08-1029,P07-1010,0,0.0359895,"Missing"
C08-1029,N07-1071,0,0.0441544,"DIRT (Lin and Pantel, 2001) collects pairs of paths in dependency parses that connect two nominal entities. TEASE (Szpektor et al., 2004) discovers dependency sub-parses from the Web, based on sets of representative entities for a given lexical item. The output of these systems contains the variable slots as shown in (4). (4) a. X wrote Y ⇔ X is the author of Y b. X solves Y ⇔ X deals with Y (Lin and Pantel, 2001) The knowledge in (4) falls between that in (1), which is fully lexicalized, and that in (3), which is almost fully abstracted. As a way of enriching such a template-like knowledge, Pantel et al. (2007) proposed the notion of inferential selectional preference and collected expressions that would fill those slots. As mentioned in Section 1, the aim of the studies reviewed here is to collect paraphrase knowledge. Thus, they need not to take the grammaticality of expressions into account. 2.2 Generating paraphrase instances Representing productive paraphrases with a set of general patterns makes them maintainable and attains a higher coverage of the paraphrase phenomena. From the transformation grammar (Har226 ris, 1957), this approach has been adopted by many researchers (Mel’ˇcuk and Polgu`e"
C08-1029,W04-3219,0,0.125077,"Missing"
C08-1029,W04-3206,0,0.0412407,"t-wise mutual information (Lin, 1998) and Relative Feature Focus (Geffet and Dagan, 2004) are well-known examples. Feature comparison measures: to convert two feature sets into a scalar value, several measures have been proposed, such as cosine, Lin’s measure (Lin, 1998), Kullback-Leibler (KL) divergence and its variants. While most researchers extract fully-lexicalized pairs of words or word sequences only, two algorithms collect template-like knowledge using dependency parsers. DIRT (Lin and Pantel, 2001) collects pairs of paths in dependency parses that connect two nominal entities. TEASE (Szpektor et al., 2004) discovers dependency sub-parses from the Web, based on sets of representative entities for a given lexical item. The output of these systems contains the variable slots as shown in (4). (4) a. X wrote Y ⇔ X is the author of Y b. X solves Y ⇔ X deals with Y (Lin and Pantel, 2001) The knowledge in (4) falls between that in (1), which is fully lexicalized, and that in (3), which is almost fully abstracted. As a way of enriching such a template-like knowledge, Pantel et al. (2007) proposed the notion of inferential selectional preference and collected expressions that would fill those slots. As m"
C08-1029,W05-1202,0,0.0139605,"is limited. In the second stream, i.e., paraphrase acquisition from the regular corpus, the distributional hypothesis (Harris, 1968) has been adopted. The similarity of two expressions, computed from this hypothesis, is called distributional similarity. The essence of this measure is summarized as follows: Feature representation: to compute the similarity, given expressions are first mapped to certain feature representations. Expressions that co-occur with the given expression, such as adjacent words (Barzilay and McKeown, 2001; Lin and Pantel, 2001), and modifiers/modifiees (Yamamoto, 2002; Weeds et al., 2005), have so far been examined. Feature weighting: to precisely compute the similarity, the weight for each feature is adjusted. Point-wise mutual information (Lin, 1998) and Relative Feature Focus (Geffet and Dagan, 2004) are well-known examples. Feature comparison measures: to convert two feature sets into a scalar value, several measures have been proposed, such as cosine, Lin’s measure (Lin, 1998), Kullback-Leibler (KL) divergence and its variants. While most researchers extract fully-lexicalized pairs of words or word sequences only, two algorithms collect template-like knowledge using depen"
C08-1029,W02-1411,0,0.0232719,"mparable corpora is limited. In the second stream, i.e., paraphrase acquisition from the regular corpus, the distributional hypothesis (Harris, 1968) has been adopted. The similarity of two expressions, computed from this hypothesis, is called distributional similarity. The essence of this measure is summarized as follows: Feature representation: to compute the similarity, given expressions are first mapped to certain feature representations. Expressions that co-occur with the given expression, such as adjacent words (Barzilay and McKeown, 2001; Lin and Pantel, 2001), and modifiers/modifiees (Yamamoto, 2002; Weeds et al., 2005), have so far been examined. Feature weighting: to precisely compute the similarity, the weight for each feature is adjusted. Point-wise mutual information (Lin, 1998) and Relative Feature Focus (Geffet and Dagan, 2004) are well-known examples. Feature comparison measures: to convert two feature sets into a scalar value, several measures have been proposed, such as cosine, Lin’s measure (Lin, 1998), Kullback-Leibler (KL) divergence and its variants. While most researchers extract fully-lexicalized pairs of words or word sequences only, two algorithms collect template-like"
C08-1029,C98-2122,0,\N,Missing
D12-1058,P05-1074,0,0.428349,"g various types of corpora, monolingual corpora can be considered the best source for highcoverage paraphrase acquisition, because there is far more monolingual than bilingual text available. Most methods that exploit monolingual corpora rely on the Distributional Hypothesis (Harris, 1968): expressions that appear in similar contexts are expected to have similar meaning. However, if one uses purely distributional criteria, it is difficult to distinguish real paraphrases from pairs of expressions that are related in other ways, such as antonyms and cousin words. In contrast, since the work in (Bannard and Callison-Burch, 2005), bilingual parallel corpora have been acknowledged as a good source of highquality paraphrases: paraphrases are obtained by putting together expressions that receive the same translation in the other language (pivot language). Because translation expresses a specific meaning more directly than context in the aforementioned approach, pairs of expressions acquired in this manner tend to be correct paraphrases. However, the coverage problem remains: there is much less bilingual parallel than monolingual text available. Our objective in this paper is to obtain paraphrases that have high quality ("
D12-1058,P01-1008,0,0.253566,"nting context of target expression (contextual features), criteria for weighting and filtering features, and aggregation functions. A drawback of relying only on contextual similarity is that it tends to give high scores to semantically related but non-equivalent expressions, such as antonyms and cousin words. To enhance the precision of the results, filtering mechanisms need to be introduced (Marton et al., 2011). 2.2 Alignment-based Methods Pairs of expressions that get translated to the same expression in a different language can be regarded as paraphrases. On the basis of this hypothesis, Barzilay and McKeown (2001) and Pang et al. (2003) 632 where Tr (e1 , e2 ) stands for the set of shared translations of e1 and e2 . Each factor p(e|f ) and p(f |e) is estimated from the number of times e and f are aligned and the number of occurrences of each expression in each language. Kok and Brockett (2010) showed how one can discover paraphrases that do not share any translation in one language by traversing a graph created from multiple translation tables, each corresponding to a bilingual parallel corpus. This approach, however, suffers from a coverage problem, because both monolingual parallel and bilingual para"
D12-1058,N03-1003,0,0.22822,"bilingual parallel corpus. This approach, however, suffers from a coverage problem, because both monolingual parallel and bilingual parallel corpora tend to be significantly smaller than monolingual non-parallel corpora. The acquired pairs of expressions include some nonparaphrases as well. Many of these come from erroneous alignments, which are particularly frequent when the given corpus is small. Monolingual comparable corpora have also been exploited as sources of paraphrases using alignmentbased methods. For instance, multiple news articles covering the same event (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Wubben et al., 2009) have been used. Such corpora have also been created manually through crowdsourcing (Chen and Dolan, 2011). However, the availability of monolingual comparable corpora is very limited for most languages; thus, approaches relying on these corpora have typically produced only very small collections of paraphrases. Hashimoto et al. (2011) found a way around this limitation by collecting sentences that constitute explicit definitions of particular words or phrases from monolingual nonparallel Web documents, pairing sentences that define the same noun phras"
D12-1058,P08-1077,0,0.0522461,"|e1 ) = Literature on Paraphrase Acquisition ∑ p(e2 |f )p(f |e1 ), (1) f ∈Tr (e1 ,e2 ) This section summarizes existing corpus-based methods for paraphrase acquisition, following the classification in (Hashimoto et al., 2011): similaritybased and alignment-based methods. 2.1 Similarity-based Methods Techniques that use monolingual (non-parallel) corpora mostly rely on the Distributional Hypothesis (Harris, 1968). Because a large quantity of monolingual data is available for many languages, a large number of paraphrase candidates can be acquired (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008, etc.). The recipes proposed so far are based on three main ingredients, i.e., features used for representing context of target expression (contextual features), criteria for weighting and filtering features, and aggregation functions. A drawback of relying only on contextual similarity is that it tends to give high scores to semantically related but non-equivalent expressions, such as antonyms and cousin words. To enhance the precision of the results, filtering mechanisms need to be introduced (Marton et al., 2011). 2.2 Alignment-based Methods Pairs of expressions that get translated to the"
D12-1058,N06-1003,0,0.0708393,"Missing"
D12-1058,D08-1021,0,0.671303,"” and “east” in (3c) and “system” and “apparatus” in (3a), could be captured, this would require the use of rich language resources, thereby making the method less portable to resource-poor languages. Note that our aim is to automatically capture general paraphrase patterns of the kind that have sometimes been manually described (Jacquemin, 1999; Fujita et al., 2007). This is different from approaches that attach variable slots to paraphrases for calculating their similarity (Lin and Pantel, 2001; Szpektor and Dagan, 2008) or for constraining the context in which they are regarded legitimate (Callison-Burch, 2008; Zhao et al., 2009). 3.3 Step 3. Paraphrase Instance Acquisition Given a set of paraphrase patterns, such as those shown in (4), a set of novel instances, i.e., novel paraphrases, PHvst , will now be harvested from monolingual non-parallel corpora. In other words, a set of appropriate slot-fillers will be extracted. First, expressions that match both elements of the pattern, except stop words, are collected from a given monolingual corpus. Pattern matching alone may generate inappropriate pairs, so we then assess the legitimacy of each collected slot-filler. Let LHS (w) and RHS (w) be the exp"
D12-1058,W11-2504,0,0.145596,"or acquiring paraphrases. The process is illustrated in Figure 1. First, a set of high-quality seed paraphrases, PSeed , is acquired from bilingual parallel corpora by using an alignment-based method. Then, our method collects further paraphrases through the following two steps. Generalization (Step 2): Paraphrase patterns are learned from the seed paraphrases, PSeed . Instantiation (Step 3): A novel set of paraphrase pairs, PHvst , is finally harvested from monolingual non-parallel corpora using the learned patterns; each newly acquired paraphrase pair is assessed by contextual similarity. 1 Chan et al. (2011) used monolingual corpora only for reranking paraphrases obtained from bilingual parallel corpora. To the best of our knowledge, bilingual comparable corpora have never been used as sources for acquiring paraphrases. 633 “health issue” ⇒ “problème de santé” “health problem” ⇒ “problème de santé” “look like” ⇒ “ressemble” “regional issue” ⇒ “problème régional” “regional problem” ⇒ “problème régional” “resemble” ⇒ “ressemble” Step 1. Seed Paraphrase Acquisition PSeed: Seed Paraphrases Monolingual Non-parallel Corpus “health issue” ⇒ “health problem” “look like” ⇒ “resemble” “regional issue” ⇒"
D12-1058,P11-1020,0,0.0599932,"tend to be significantly smaller than monolingual non-parallel corpora. The acquired pairs of expressions include some nonparaphrases as well. Many of these come from erroneous alignments, which are particularly frequent when the given corpus is small. Monolingual comparable corpora have also been exploited as sources of paraphrases using alignmentbased methods. For instance, multiple news articles covering the same event (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Wubben et al., 2009) have been used. Such corpora have also been created manually through crowdsourcing (Chen and Dolan, 2011). However, the availability of monolingual comparable corpora is very limited for most languages; thus, approaches relying on these corpora have typically produced only very small collections of paraphrases. Hashimoto et al. (2011) found a way around this limitation by collecting sentences that constitute explicit definitions of particular words or phrases from monolingual nonparallel Web documents, pairing sentences that define the same noun phrase, and then finding corresponding phrases in each sentence pair. One limitation of this approach is that it requires a considerable amount of labele"
D12-1058,C08-1018,0,0.0702596,"Missing"
D12-1058,W11-2107,0,0.0233728,"a similar manner, i.e., a LHS phrase lp is not qualified as a legitimate source of rp iff rp has another LHS phrase lp ′ (= lp) which satisfies the following conditions (see also Figure 3). • lp′ is a word sub-sequence of lp • lp ′ is a more likely source than lp, i.e., p(lp ′ |rp) > p(lp|rp) The two directions of filtering are separately applied and the intersection of their results is retained. Candidate pairs are finally filtered on the basis of their reliability score. Traditionally, a threshold (th p ) on the conditional probability given by Eq. (1) is used (Du et al., 2010; Max, 2010; Denkowski and Lavie, 2011, etc.). Furthermore, we also require that LHS and RHS phrases exceed a threshold (th s ) on their contextual similarity in a monolingual corpus. This paper neither proposes a specific recipe nor makes a comprehensive comparison of existing recipes for computing contextual similarity, although one particular recipe is used in our experiments (see Section 4.1). 2 cf. Denkowski and Lavie (2011); they only compared each RHS phrase to its corresponding LHS phrase. 634 3.2 Step 2. Paraphrase Pattern Induction From a set of seed paraphrases, PSeed , paraphrase patterns are induced. For instance, fro"
D12-1058,C04-1051,0,0.178329,"us. This approach, however, suffers from a coverage problem, because both monolingual parallel and bilingual parallel corpora tend to be significantly smaller than monolingual non-parallel corpora. The acquired pairs of expressions include some nonparaphrases as well. Many of these come from erroneous alignments, which are particularly frequent when the given corpus is small. Monolingual comparable corpora have also been exploited as sources of paraphrases using alignmentbased methods. For instance, multiple news articles covering the same event (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Wubben et al., 2009) have been used. Such corpora have also been created manually through crowdsourcing (Chen and Dolan, 2011). However, the availability of monolingual comparable corpora is very limited for most languages; thus, approaches relying on these corpora have typically produced only very small collections of paraphrases. Hashimoto et al. (2011) found a way around this limitation by collecting sentences that constitute explicit definitions of particular words or phrases from monolingual nonparallel Web documents, pairing sentences that define the same noun phrase, and then finding"
D12-1058,D10-1041,0,0.0903118,"rase rp are also compared in a similar manner, i.e., a LHS phrase lp is not qualified as a legitimate source of rp iff rp has another LHS phrase lp ′ (= lp) which satisfies the following conditions (see also Figure 3). • lp′ is a word sub-sequence of lp • lp ′ is a more likely source than lp, i.e., p(lp ′ |rp) > p(lp|rp) The two directions of filtering are separately applied and the intersection of their results is retained. Candidate pairs are finally filtered on the basis of their reliability score. Traditionally, a threshold (th p ) on the conditional probability given by Eq. (1) is used (Du et al., 2010; Max, 2010; Denkowski and Lavie, 2011, etc.). Furthermore, we also require that LHS and RHS phrases exceed a threshold (th s ) on their contextual similarity in a monolingual corpus. This paper neither proposes a specific recipe nor makes a comprehensive comparison of existing recipes for computing contextual similarity, although one particular recipe is used in our experiments (see Section 4.1). 2 cf. Denkowski and Lavie (2011); they only compared each RHS phrase to its corresponding LHS phrase. 634 3.2 Step 2. Paraphrase Pattern Induction From a set of seed paraphrases, PSeed , paraphrase p"
D12-1058,W07-1425,1,0.798663,"” ⇒ “Y in the X east” Word pairs of LHS and RHS phrases will be replaced with variable slots iff they are fully identical or singular-plural variants. Note that stop words are retained. While a deeper level of lexical correspondences, such as “eastern” and “east” in (3c) and “system” and “apparatus” in (3a), could be captured, this would require the use of rich language resources, thereby making the method less portable to resource-poor languages. Note that our aim is to automatically capture general paraphrase patterns of the kind that have sometimes been manually described (Jacquemin, 1999; Fujita et al., 2007). This is different from approaches that attach variable slots to paraphrases for calculating their similarity (Lin and Pantel, 2001; Szpektor and Dagan, 2008) or for constraining the context in which they are regarded legitimate (Callison-Burch, 2008; Zhao et al., 2009). 3.3 Step 3. Paraphrase Instance Acquisition Given a set of paraphrase patterns, such as those shown in (4), a set of novel instances, i.e., novel paraphrases, PHvst , will now be harvested from monolingual non-parallel corpora. In other words, a set of appropriate slot-fillers will be extracted. First, expressions that match"
D12-1058,D11-1108,0,0.113302,"Missing"
D12-1058,P11-1109,0,0.285732,"Missing"
D12-1058,P99-1044,0,0.0736733,"c. “X eastern Y ” ⇒ “Y in the X east” Word pairs of LHS and RHS phrases will be replaced with variable slots iff they are fully identical or singular-plural variants. Note that stop words are retained. While a deeper level of lexical correspondences, such as “eastern” and “east” in (3c) and “system” and “apparatus” in (3a), could be captured, this would require the use of rich language resources, thereby making the method less portable to resource-poor languages. Note that our aim is to automatically capture general paraphrase patterns of the kind that have sometimes been manually described (Jacquemin, 1999; Fujita et al., 2007). This is different from approaches that attach variable slots to paraphrases for calculating their similarity (Lin and Pantel, 2001; Szpektor and Dagan, 2008) or for constraining the context in which they are regarded legitimate (Callison-Burch, 2008; Zhao et al., 2009). 3.3 Step 3. Paraphrase Instance Acquisition Given a set of paraphrase patterns, such as those shown in (4), a set of novel instances, i.e., novel paraphrases, PHvst , will now be harvested from monolingual non-parallel corpora. In other words, a set of appropriate slot-fillers will be extracted. First, e"
D12-1058,D07-1103,1,0.929456,"lts with several filtering methods. The phrase pair extraction process of phrasebased SMT systems aims at high recall for increased robustness of the translation process. As a result, a naive application of the paraphrase acquisition method produces pairs of expressions that are not exact paraphrases. For instance, the algorithm explained in Koehn (2009, p.134) extracts both “dass” and “, dass” as counterparts of “that” from the sentence pair. To reduce that kind of noise, we apply some filtering techniques to the candidate translation pairs. First, statistically unreliable translation pairs (Johnson et al., 2007) are filtered out. Then, we also filter out phrases made up entirely of stop words (including punctuation marks), both in the language of interest and in the pivot language. Let PRaw be the initial set of paraphrase pairs extracted from the sanitized translation table. We first p(rp|lp) .172 lp: controller rp: control system lp: control apparatus rp: the control device lp: the control apparatus .005 .004 .003 rp: control device of the lp: control apparatus of rp: controlling device lp: controlling unit .001 rp: control system of lp: control equipment .001 .001 rp: a control system for an lp: c"
D12-1058,N03-1017,0,0.00867146,"n be pooled with the set PHvst harvested in the last stage of the process. 3.1 Step 1. Seed Paraphrase Acquisition The goal of the first step is to obtain a set of highquality paraphrase pairs, PSeed . For this purpose, alignment-based methods with bilingual or monolingual parallel corpora are preferable to similarity-based methods applied to nonparallel corpora. Among various options, in this paper, we start from the standard technique proposed by Bannard and Callison-Burch (2005) with bilingual parallel corpora (see also Section 2.2). In particular, we assume the phrase-based SMT framework (Koehn et al., 2003). Then, we purify the results with several filtering methods. The phrase pair extraction process of phrasebased SMT systems aims at high recall for increased robustness of the translation process. As a result, a naive application of the paraphrase acquisition method produces pairs of expressions that are not exact paraphrases. For instance, the algorithm explained in Koehn (2009, p.134) extracts both “dass” and “, dass” as counterparts of “that” from the sentence pair. To reduce that kind of noise, we apply some filtering techniques to the candidate translation pairs. First, statistically unre"
D12-1058,P09-5002,0,0.0145231,"in this paper, we start from the standard technique proposed by Bannard and Callison-Burch (2005) with bilingual parallel corpora (see also Section 2.2). In particular, we assume the phrase-based SMT framework (Koehn et al., 2003). Then, we purify the results with several filtering methods. The phrase pair extraction process of phrasebased SMT systems aims at high recall for increased robustness of the translation process. As a result, a naive application of the paraphrase acquisition method produces pairs of expressions that are not exact paraphrases. For instance, the algorithm explained in Koehn (2009, p.134) extracts both “dass” and “, dass” as counterparts of “that” from the sentence pair. To reduce that kind of noise, we apply some filtering techniques to the candidate translation pairs. First, statistically unreliable translation pairs (Johnson et al., 2007) are filtered out. Then, we also filter out phrases made up entirely of stop words (including punctuation marks), both in the language of interest and in the pivot language. Let PRaw be the initial set of paraphrase pairs extracted from the sanitized translation table. We first p(rp|lp) .172 lp: controller rp: control system lp: con"
D12-1058,N10-1017,0,0.0311391,"nyms and cousin words. To enhance the precision of the results, filtering mechanisms need to be introduced (Marton et al., 2011). 2.2 Alignment-based Methods Pairs of expressions that get translated to the same expression in a different language can be regarded as paraphrases. On the basis of this hypothesis, Barzilay and McKeown (2001) and Pang et al. (2003) 632 where Tr (e1 , e2 ) stands for the set of shared translations of e1 and e2 . Each factor p(e|f ) and p(f |e) is estimated from the number of times e and f are aligned and the number of occurrences of each expression in each language. Kok and Brockett (2010) showed how one can discover paraphrases that do not share any translation in one language by traversing a graph created from multiple translation tables, each corresponding to a bilingual parallel corpus. This approach, however, suffers from a coverage problem, because both monolingual parallel and bilingual parallel corpora tend to be significantly smaller than monolingual non-parallel corpora. The acquired pairs of expressions include some nonparaphrases as well. Many of these come from erroneous alignments, which are particularly frequent when the given corpus is small. Monolingual compara"
D12-1058,J10-3003,0,0.0259774,"r, the coverage problem remains: there is much less bilingual parallel than monolingual text available. Our objective in this paper is to obtain paraphrases that have high quality (like those extracted from bilingual parallel corpora via pivoting) but can be generated in large quantity (like those extracted (1) Introduction Paraphrases are semantically equivalent expressions in the same language. Because “equivalence” is the most fundamental semantic relationship, techniques for generating and recognizing paraphrases play an important role in a wide range of natural language processing tasks (Madnani and Dorr, 2010). In the last decade, automatic acquisition of knowledge about paraphrases from corpora has been drawing the attention of many researchers. Typically, the acquired knowledge is simply represented as pairs of semantically equivalent sub-sentential expressions as in (1). 631 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 631–642, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics from monolingual corpora via contextual similarity). To achieve this, we propose a meth"
D12-1058,D09-1040,0,0.0801327,"Missing"
D12-1058,W11-2128,0,0.0339243,"Missing"
D12-1058,D10-1064,0,0.047879,"compared in a similar manner, i.e., a LHS phrase lp is not qualified as a legitimate source of rp iff rp has another LHS phrase lp ′ (= lp) which satisfies the following conditions (see also Figure 3). • lp′ is a word sub-sequence of lp • lp ′ is a more likely source than lp, i.e., p(lp ′ |rp) > p(lp|rp) The two directions of filtering are separately applied and the intersection of their results is retained. Candidate pairs are finally filtered on the basis of their reliability score. Traditionally, a threshold (th p ) on the conditional probability given by Eq. (1) is used (Du et al., 2010; Max, 2010; Denkowski and Lavie, 2011, etc.). Furthermore, we also require that LHS and RHS phrases exceed a threshold (th s ) on their contextual similarity in a monolingual corpus. This paper neither proposes a specific recipe nor makes a comprehensive comparison of existing recipes for computing contextual similarity, although one particular recipe is used in our experiments (see Section 4.1). 2 cf. Denkowski and Lavie (2011); they only compared each RHS phrase to its corresponding LHS phrase. 634 3.2 Step 2. Paraphrase Pattern Induction From a set of seed paraphrases, PSeed , paraphrase patterns are"
D12-1058,I05-1011,0,0.489306,"Missing"
D12-1058,N03-1024,0,0.508795,"Missing"
D12-1058,W05-0822,1,0.80561,"05 104 103 106 107 108 # of words in the English side of bilingual corpus 107 106 105 PRaw PRaw (thp=0.01) PSeed (thp=ε, ths=ε) PSeed (thp=0.01, ths=ε) 104 103 106 107 108 # of words in the English side of bilingual corpus Figure 4: # of paraphrase pairs in PSeed (left: Europarl, right: Patent). Stop word lists for sanitizing translation pairs and paraphrase pairs were manually compiled: we enumerated 442 English words, 193 French words, and 149 Japanese morphemes, respectively. From a bilingual parallel corpus, a translation table was created by our in-house phrase-based SMT system, PORTAGE (Sadat et al., 2005). Phrase alignments of each sentence pair were identified by the heuristic “grow-diag-final”6 with a maximum phrase length 8. The resulting translation pairs were then filtered with the significance pruning technique of (Johnson et al., 2007), using α + ϵ as threshold. As contextual features for computing similarity of each paraphrase pair, all of the 1- to 4-grams of words adjacent to each occurrence of a phrase were counted. This is a compromise between less expensive but noisier approaches, such as bag-of-words, and more accurate but more expensive approaches that incorporate syntactic feat"
D12-1058,C08-1107,0,0.0412699,"e that stop words are retained. While a deeper level of lexical correspondences, such as “eastern” and “east” in (3c) and “system” and “apparatus” in (3a), could be captured, this would require the use of rich language resources, thereby making the method less portable to resource-poor languages. Note that our aim is to automatically capture general paraphrase patterns of the kind that have sometimes been manually described (Jacquemin, 1999; Fujita et al., 2007). This is different from approaches that attach variable slots to paraphrases for calculating their similarity (Lin and Pantel, 2001; Szpektor and Dagan, 2008) or for constraining the context in which they are regarded legitimate (Callison-Burch, 2008; Zhao et al., 2009). 3.3 Step 3. Paraphrase Instance Acquisition Given a set of paraphrase patterns, such as those shown in (4), a set of novel instances, i.e., novel paraphrases, PHvst , will now be harvested from monolingual non-parallel corpora. In other words, a set of appropriate slot-fillers will be extracted. First, expressions that match both elements of the pattern, except stop words, are collected from a given monolingual corpus. Pattern matching alone may generate inappropriate pairs, so we"
D12-1058,W09-0621,0,0.108629,"Missing"
D19-1146,2012.eamt-1.60,0,0.0681834,"and Vietnamese) in the Asian Language Treebank (ALT) corpus (Riza et al., 2016),3 as a result of soft division of labor: training of a strong English encoder, domain adaptation, and tuning of the decoder for each target language. 2 Adding a new language does not add any new translation knowledge. Such corpora deserve more attention, since adding a new language to it leads to parallel corpora between the new language and all the other languages in the corpora. 3 There exist other multi-parallel corpora, such as those for United Nations (Ziemski et al., 2016), Europarl (Koehn, 2005), Ted Talks (Cettolo et al., 2012), ILCI (Jha, 2010), and the Bible (Christodouloupoulos and Steedman, 2015). 1410 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1410–1416, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, whereas Firat et al. (2016) and Johnson"
D19-1146,P17-2061,1,0.921873,"whereas Firat et al. (2016) and Johnson et al. (2017) have worked on multilingual and multi-way NMT. These studies have focused on training a multilingual model from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these approaches do not try to combine multilingualism with multistage fine-tuning using only parallel corpora involving a large number of languages. 3 Multistage Fine-Tuning for NMT This paper focuses on translation from English (En) to N different languages. In particular,"
D19-1146,P15-1166,0,0.17506,"al., 2016). Translating from a resource-poor language into English is typically easier than the other direction, because large parallel corpora between English and other languages can be used for transfer learning (Zoph et al., 2016) and many-to-one modeling.1 Additionally, large English monolingual data can be back-translated to obtain pseudoparallel corpora for augmenting the performance of such NMT models. 1 Note that “many-to-one” does not mean multi-source machine translation (Zoph and Knight, 2016). Translating from English into a resource-poor language is substantially more difficult. Dong et al. (2015) have shown that a one-to-many model trained on middle-sized parallel data (200k sentence pairs) can improve the translation quality over a one-to-one model. However, it is unclear whether this works for much resource-poorer, more distant, and more diverse language pairs. Using pseudo-parallel data is a potential solution, but for most resource-poor languages, the amount of available clean and in-domain monolingual data are limited. It is also unclear what the real reason behind improvements in translation is: increase in the training data or multilingualism. This paper focuses on (a) improvin"
D19-1146,N16-1101,0,0.0294968,"Ted Talks (Cettolo et al., 2012), ILCI (Jha, 2010), and the Bible (Christodouloupoulos and Steedman, 2015). 1410 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1410–1416, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, whereas Firat et al. (2016) and Johnson et al. (2017) have worked on multilingual and multi-way NMT. These studies have focused on training a multilingual model from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed"
D19-1146,N18-1032,0,0.0267464,"9. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, whereas Firat et al. (2016) and Johnson et al. (2017) have worked on multilingual and multi-way NMT. These studies have focused on training a multilingual model from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these appr"
D19-1146,D18-1398,0,0.0280459,"9. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, whereas Firat et al. (2016) and Johnson et al. (2017) have worked on multilingual and multi-way NMT. These studies have focused on training a multilingual model from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these appr"
D19-1146,L18-1545,0,0.0789616,"from the overlapping vocabularies of Chinese and Japanese, the performance of other languages also improves despite no vocabulary overlap. Consequently, we believe that translation into a low-resource target language might benefit when the language is also target side of the helping corpus but multistage fine-tuning does not require such overlap and instead shows optimal performance when it leverages multilingualism during stage-wise tuning. In the future, we will test the above hypotheses thoroughly via some controlled experiments by using, for instance, larger scale multi-parallel corpora (Imamura and Sumita, 2018) and varieties of helping corpora. 5.2 How Does Multilingualism Help? The ALT corpus is multi-parallel and merely comprises of the same sentences in multiple languages. Although we used seven target languages, we did not introduce new English sentences to the source side. Table 1 shows that some languages are better translated (En-Vi) than others (En-Bn). We speculate that the representations for En-Vi might be better learned than those for En-Bn. When learning all language pairs jointly, the representations of sentences with the same meaning tend to be similar. As such, the representations fo"
D19-1146,W19-6613,1,0.78878,"from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these approaches do not try to combine multilingualism with multistage fine-tuning using only parallel corpora involving a large number of languages. 3 Multistage Fine-Tuning for NMT This paper focuses on translation from English (En) to N different languages. In particular, we consider exploiting two types of corpora. One is a small-scale multi-parallel corpus, En-YY1 · · ·-YYN , consisting of English and N target languages of interest. T"
D19-1146,Q17-1024,0,0.0864411,"Missing"
D19-1146,2005.mtsummit-papers.11,0,0.188451,"Japanese, Khmer, Malay, and Vietnamese) in the Asian Language Treebank (ALT) corpus (Riza et al., 2016),3 as a result of soft division of labor: training of a strong English encoder, domain adaptation, and tuning of the decoder for each target language. 2 Adding a new language does not add any new translation knowledge. Such corpora deserve more attention, since adding a new language to it leads to parallel corpora between the new language and all the other languages in the corpora. 3 There exist other multi-parallel corpora, such as those for United Nations (Ziemski et al., 2016), Europarl (Koehn, 2005), Ted Talks (Cettolo et al., 2012), ILCI (Jha, 2010), and the Bible (Christodouloupoulos and Steedman, 2015). 1410 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1410–1416, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, wherea"
D19-1146,I17-2050,0,0.0225962,"cs 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, whereas Firat et al. (2016) and Johnson et al. (2017) have worked on multilingual and multi-way NMT. These studies have focused on training a multilingual model from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these approaches do not try to combine multilingualism with multi"
D19-1146,P02-1040,0,0.104338,"tter than the 1-to-1 “En-YY” model (#1), the best model without the multi-parallel corpus (#2–#4), and the strongest baselines (#5–#6). the BLEU score on the development set (simply concatenated unlike training data) did not vary by more than 0.1 BLEU over 10 checkpoints. Instead of choosing the model with the best development set BLEU, we averaged the last 10 checkpoints saved every after 1,000 updates, following Vaswani et al. (2017), and decoded the test sets with a beam size of 4 and a length penalty, α, of 0.6 consistently across all the models. 4.3 Results Table 1 gives the BLEU scores (Papineni et al., 2002) for all the configurations. Among the seven configurations, irrespective of the external parallel corpus for En-XX, the three-stage fine-tuned model (#7) achieved the highest BLEU scores for all the seven target languages. Results for #1 demonstrate that NMT systems trained on 18k parallel sentences can achieve only poor results for Bn and Ja, whereas reasonably high BLEU scores (> 20) are achieved for the other target languages. Introducing a large external En-XX parallel corpus improved the translation quality consistently and significantly for all the seven target languages,14 irrespective"
D19-1146,P16-1009,0,0.0870722,"arameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these approaches do not try to combine multilingualism with multistage fine-tuning using only parallel corpora involving a large number of languages. 3 Multistage Fine-Tuning for NMT This paper focuses on translation from English (En) to N different languages. In particular, we consider exploiting two types of corpora. One is a small-scale multi-parallel corpus, En-YY1 · · ·-YYN , consisting of English and N target languages of interest. The other is a relatively larger helping parallel corpus, En-XX, where XX indicates the helping target language which needs not be one of the tar"
D19-1146,L16-1561,0,0.0220411,"s (Bengali, Filipino, Indonesian, Japanese, Khmer, Malay, and Vietnamese) in the Asian Language Treebank (ALT) corpus (Riza et al., 2016),3 as a result of soft division of labor: training of a strong English encoder, domain adaptation, and tuning of the decoder for each target language. 2 Adding a new language does not add any new translation knowledge. Such corpora deserve more attention, since adding a new language to it leads to parallel corpora between the new language and all the other languages in the corpora. 3 There exist other multi-parallel corpora, such as those for United Nations (Ziemski et al., 2016), Europarl (Koehn, 2005), Ted Talks (Cettolo et al., 2012), ILCI (Jha, 2010), and the Bible (Christodouloupoulos and Steedman, 2015). 1410 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1410–1416, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked o"
D19-1146,N16-1004,0,0.0202007,"and German–English (Bojar et al., 2018), it performs poorly for resource-poor pairs (Zoph et al., 2016; Riza et al., 2016). Translating from a resource-poor language into English is typically easier than the other direction, because large parallel corpora between English and other languages can be used for transfer learning (Zoph et al., 2016) and many-to-one modeling.1 Additionally, large English monolingual data can be back-translated to obtain pseudoparallel corpora for augmenting the performance of such NMT models. 1 Note that “many-to-one” does not mean multi-source machine translation (Zoph and Knight, 2016). Translating from English into a resource-poor language is substantially more difficult. Dong et al. (2015) have shown that a one-to-many model trained on middle-sized parallel data (200k sentence pairs) can improve the translation quality over a one-to-one model. However, it is unclear whether this works for much resource-poorer, more distant, and more diverse language pairs. Using pseudo-parallel data is a potential solution, but for most resource-poor languages, the amount of available clean and in-domain monolingual data are limited. It is also unclear what the real reason behind improvem"
D19-1146,D16-1163,0,0.450445,"tent-wise redundancy thus exhibiting the true power of multilingualism. Even when the helping target language is not one of the target languages of our concern, our multistage finetuning can give 3–9 BLEU score gains over a simple one-to-one model. 1 Introduction Encoder-decoder based neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) allows for end-to-end training of a translation model. While NMT is known to perform well for resource-rich language pairs, such as French– English and German–English (Bojar et al., 2018), it performs poorly for resource-poor pairs (Zoph et al., 2016; Riza et al., 2016). Translating from a resource-poor language into English is typically easier than the other direction, because large parallel corpora between English and other languages can be used for transfer learning (Zoph et al., 2016) and many-to-one modeling.1 Additionally, large English monolingual data can be back-translated to obtain pseudoparallel corpora for augmenting the performance of such NMT models. 1 Note that “many-to-one” does not mean multi-source machine translation (Zoph and Knight, 2016). Translating from English into a resource-poor language is substantially more di"
D19-5206,P07-2045,0,0.00846795,"Missing"
D19-5206,P18-1073,0,0.151243,"orpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual word embeddings, and the 300 target phrases with the highest scores were kept in the phrase table for each source phrase. As a result, the induced phrase table contains a total of 90M (300k×300) phrase pairs. For this induction, bilingual word embeddings of 300 dimensions were obtained using word embeddings trained with fastText13 and aligned in the same space using unsupervised Vecmap (Artetxe et al., 2018a). This alignment is the most critical step for unsupervised MT since it is used for initializing the training. It is expected to be extremely difficult for distant languages such as Myanmar, Khmer, and English, as reported by previous work (Søgaard et al., 2018). For each phrase pair, a total of four scores, to be used as features in the phrase tables were computed to mimic phraseReranking Framework and Features We rescored all the hypotheses in the list with a reranking framework using features to better model the fluency and the adequacy of each hypothesis. This method can find a better hy"
D19-5206,J82-2005,0,0.725497,"Missing"
D19-5206,D18-1399,0,0.253018,"orpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual word embeddings, and the 300 target phrases with the highest scores were kept in the phrase table for each source phrase. As a result, the induced phrase table contains a total of 90M (300k×300) phrase pairs. For this induction, bilingual word embeddings of 300 dimensions were obtained using word embeddings trained with fastText13 and aligned in the same space using unsupervised Vecmap (Artetxe et al., 2018a). This alignment is the most critical step for unsupervised MT since it is used for initializing the training. It is expected to be extremely difficult for distant languages such as Myanmar, Khmer, and English, as reported by previous work (Søgaard et al., 2018). For each phrase pair, a total of four scores, to be used as features in the phrase tables were computed to mimic phraseReranking Framework and Features We rescored all the hypotheses in the list with a reranking framework using features to better model the fluency and the adequacy of each hypothesis. This method can find a better hy"
D19-5206,W18-1811,1,0.810257,"oduction This paper describes neural (NMT) and statistical machine translation systems (SMT) built for the participation of the National Institute of Information and Communications Technology (NICT) in the WAT2019 (Nakazawa et al., 2019) Myanmar-English (my-en) and Khmer-English (km-en) translation tasks.1 We present supervised systems built using the parallel data provided by the organizers and external additional monolingual data. For all the translation directions, we trained supervised NMT and SMT systems, and combined them through n-best list reranking using several informative features (Marie and Fujita, 2018a), as in our previous participation to WAT2018 (Marie et al., 2018). This simple combination method achieved the best results among the submitted MT systems for these tasks according to BLEU (Papineni et al., 1 2 Data preprocessing To train our systems, we used all the bilingual data provided by the organizers. The provided bilingual data comprises different types of corpora: the training data provided by the ALT project2 and additional training data. These additional data are the UCSY corpus, constructed by the University of Computer Studies, Yangon (UCSY),3 for the my-en task, and the ECCC"
D19-5206,Y18-3007,1,0.640277,"slation systems (SMT) built for the participation of the National Institute of Information and Communications Technology (NICT) in the WAT2019 (Nakazawa et al., 2019) Myanmar-English (my-en) and Khmer-English (km-en) translation tasks.1 We present supervised systems built using the parallel data provided by the organizers and external additional monolingual data. For all the translation directions, we trained supervised NMT and SMT systems, and combined them through n-best list reranking using several informative features (Marie and Fujita, 2018a), as in our previous participation to WAT2018 (Marie et al., 2018). This simple combination method achieved the best results among the submitted MT systems for these tasks according to BLEU (Papineni et al., 1 2 Data preprocessing To train our systems, we used all the bilingual data provided by the organizers. The provided bilingual data comprises different types of corpora: the training data provided by the ALT project2 and additional training data. These additional data are the UCSY corpus, constructed by the University of Computer Studies, Yangon (UCSY),3 for the my-en task, and the ECCC corpus, collected by National Institute of Posts, Telecoms & ICT (NI"
D19-5206,N12-1047,0,0.0345309,"ur MT systems. 8 We used a Myanmar dictionary that contains a list of unique 41,343 Myanmar words from https: //github.com/chanmratekoko/Awesome-Myanmar. 70 right) lexicalized reordering models. We also used the default distortion limit of 6. We trained two 4-gram language models, one on the WMT monolingual data for English, on the Common Crawl corpus for Khmer, and on the Wikipedia data for Myanmar, concatenated to the target side of the parallel data, and another one on the target side of the parallel data, using LMPLZ (Heafield et al., 2013). To tune the SMT model weights, we used kb-mira (Cherry and Foster, 2012) and selected the weights giving the best BLEU score for the development data during 15 iterations. --type transformer --max-length 80 --mini-batch-fit --valid-freq 5000 --save-freq 5000 --workspace 10000 --disp-freq 500 --beam-size 12 --normalize 1 --valid-mini-batch 16 --overwrite --early-stopping 5 --cost-type ce-mean-words --valid-metrics ce-mean-words perplexity translation --keep-best --enc-depth 4 --dec-depth 4 --transformer-dropout 0.1 --learn-rate 0.001 --dropout-src 0.1 --dropout-trg 0.1 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --label-smoothing 0.1 --devices 0 1 2 3 4"
D19-5206,W19-5330,1,0.784097,"contrasted when ensembling 7 NMT models during decoding (#4). While we observe an improvement of 3.3 BLEU points for (my→en), the improvements for the other directions were limited to 1.0 BLEU points or less. Considering the cost of independently training 7 NMT models and the cost of decoding with 7 models, ensembling does not seem to offer a cost-effective solution. Finally, combining SMT and NMT (#5) provides the best results with improvements over #4 ranging from 0.9 (en→km) to 2.4 BLEU points (my→en). Our results for unsupervised SMT (#6) follow the same trend as the results presented by Marie et al. (2019) for English-Gujarati and English-Kazakh at WMT19: while unsupervised MT has shown promising results for European languages, it is far from being useful for real-world applications, i.e., truly low-resource distant language pairs. We assume that training useful bilingual weaklysupervised/unsupervised bilingual word embeddings for initializing the system remains one of the main challenges. Results Table 6 presents the results for different versions of our SMT and NMT systems. We can observe that NMT (#2) is significantly better than SMT (#1) for my-en while we can observe the reverse for km-en."
D19-5206,P13-2121,0,0.0393413,"Missing"
D19-5206,P02-1040,0,0.117309,"Missing"
D19-5206,P16-1009,0,0.0322957,"alid-metrics ce-mean-words perplexity translation --keep-best --enc-depth 4 --dec-depth 4 --transformer-dropout 0.1 --learn-rate 0.001 --dropout-src 0.1 --dropout-trg 0.1 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --label-smoothing 0.1 --devices 0 1 2 3 4 5 6 7 --dim-vocabs 8000 8000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --sync-sgd --exponential-smoothing 4 Back-Translation of Monolingual Data for NMT Parallel data for training NMT can be augmented with synthetic parallel data, generated through a so-called back-translation, to significantly improve translation quality (Sennrich et al., 2016a). We used an NMT system, trained on the parallel data provided by the organizers, to translate target monolingual sentences into the source language. Then, these back-translated sentences were simply mixed with the original parallel data to train from scratch a new source-to-target NMT system. We back-translated 2M sentences randomly sampled from WMT18 English data for my→en and km→en, our Myanmar Wikipedia corpus for en→my, and our Khmer Common Crawl corpus for en→km. Table 4: Parameters of Marian used for training our NMT systems. 3 Supervised MT Systems 3.1 NMT To build competitive NMT sy"
D19-5206,P16-1162,0,0.048767,"alid-metrics ce-mean-words perplexity translation --keep-best --enc-depth 4 --dec-depth 4 --transformer-dropout 0.1 --learn-rate 0.001 --dropout-src 0.1 --dropout-trg 0.1 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --label-smoothing 0.1 --devices 0 1 2 3 4 5 6 7 --dim-vocabs 8000 8000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --sync-sgd --exponential-smoothing 4 Back-Translation of Monolingual Data for NMT Parallel data for training NMT can be augmented with synthetic parallel data, generated through a so-called back-translation, to significantly improve translation quality (Sennrich et al., 2016a). We used an NMT system, trained on the parallel data provided by the organizers, to translate target monolingual sentences into the source language. Then, these back-translated sentences were simply mixed with the original parallel data to train from scratch a new source-to-target NMT system. We back-translated 2M sentences randomly sampled from WMT18 English data for my→en and km→en, our Myanmar Wikipedia corpus for en→my, and our Khmer Common Crawl corpus for en→km. Table 4: Parameters of Marian used for training our NMT systems. 3 Supervised MT Systems 3.1 NMT To build competitive NMT sy"
D19-5206,P18-1072,0,0.0254507,"highest scores were kept in the phrase table for each source phrase. As a result, the induced phrase table contains a total of 90M (300k×300) phrase pairs. For this induction, bilingual word embeddings of 300 dimensions were obtained using word embeddings trained with fastText13 and aligned in the same space using unsupervised Vecmap (Artetxe et al., 2018a). This alignment is the most critical step for unsupervised MT since it is used for initializing the training. It is expected to be extremely difficult for distant languages such as Myanmar, Khmer, and English, as reported by previous work (Søgaard et al., 2018). For each phrase pair, a total of four scores, to be used as features in the phrase tables were computed to mimic phraseReranking Framework and Features We rescored all the hypotheses in the list with a reranking framework using features to better model the fluency and the adequacy of each hypothesis. This method can find a better hypothesis in these merged n-best lists than the one-best hypothesis originated by the individual systems. We chose kb-mira as a rescoring framework and used a subset of the features proposed in Marie and Fujita (2018a). All the following features we used are descri"
I05-1079,P98-1013,0,0.0126582,"phrasing has recently been attracting increasing attention due to its potential in a broad range of natural language processing tasks. For example, a system that is capable of simplifying a given text, or showing the user several alternative expressions conveying the same content, would be useful for assisting a reader. There are several classes of paraphrase that exhibit a degree of regularity. For example, paraphrasing associated with verb alternation, lexical derivation, compound word decomposition, and paraphrasing of light-verb constructions (LVC(s)) all fall into such classes. Examples1 (1) and (2) appear to exhibit the same transformation pattern, in which a compound noun is transformed into a verb phrase. Likewise, paraphrases involving an LVC as in (3) and (4) (from [4]) have considerable similarities. (1) s. t. (2) s. t. (3) s. t. (4) s. t. 1 His machine operation is very good. He operates the machine very well. My son’s bat control is unskillful yet. My son controls his bat poorly yet. Steven made an attempt to stop playing. Steven attempted to stop playing. It had a noticeable effect on the trade. It noticeably affected the trade. For each example, “s” and “t” denote an or"
I05-1079,W04-2412,0,0.0472235,"has recently been attracting increasing attention due to its potential in a broad range of natural language processing tasks. For example, a system that is capable of simplifying a given text, or showing the user several alternative expressions conveying the same content, would be useful for assisting a reader. There are several classes of paraphrase that exhibit a degree of regularity. For example, paraphrasing associated with verb alternation, lexical derivation, compound word decomposition, and paraphrasing of light-verb constructions (LVC(s)) all fall into such classes. Examples1 (1) and (2) appear to exhibit the same transformation pattern, in which a compound noun is transformed into a verb phrase. Likewise, paraphrases involving an LVC as in (3) and (4) (from [4]) have considerable similarities. (1) s. t. (2) s. t. (3) s. t. (4) s. t. 1 His machine operation is very good. He operates the machine very well. My son’s bat control is unskillful yet. My son controls his bat poorly yet. Steven made an attempt to stop playing. Steven attempted to stop playing. It had a noticeable effect on the trade. It noticeably affected the trade. For each example, “s” and “t” denote an original s"
I05-1079,J02-3001,0,0.122308,"Missing"
I05-1079,J94-3013,0,0.0841281,"19] have shown that the theory of the LCS provides a systematic explanation of semantic decomposition as well as syntax determines. In particular, Kageyama [9] has shown that even a simple typology of LCS can explain a wide variety of linguistic phenomena including word association within compounds, transitivity alternation, and lexical derivation. Second, large-scale LCS dictionaries have been developed through practical use on machine translation and compound noun analysis [3,19]. The LCS dictionary for English [3] (4,163-verbs with 468 LCS types) was tailored based on a verb classification [12] with an expansion for the semantic role delivered to arguments. For Japanese, Takeuchi et al. [19] developed a 1,210-verbs LCS dictionary (with 12 LCS types) called the T-LCS dictionary, following Kageyama’s analysis [9]. In this paper, we make use of the current version of the T-LCS dictionary, because it provides a set of concrete rules for LCS assignment, which ensures the reliability of the dictionary. Examples of LCS in the T-LCS dictionary are shown in Table 1. An LCS consists of a combination of semantic predicates (“CONTROL,” “BE AT,” etc.) and their argument slots (x, y, and z). Each"
I05-1079,J87-3006,0,0.040094,"Missing"
I05-1079,J05-1004,0,0.0407037,"Missing"
I05-1079,W02-1409,0,0.0544453,"ularities underlying paraphrases can be explained by means of lexical semantics and how, and (ii) how lexical semantics theories can be enhanced with feedback from practical use, namely, paraphrase generation. We make an attempt to exploit the LCS among several lexical semantics frameworks, and propose a paraphrase generation model which utilizes LCS combining with syntactic transformation. 2 Lexical Conceptual Structure 2.1 Basic Framework Among several frameworks of lexical semantics, we focus on the Lexical Conceptual Structure (LCS) [8] due to the following reasons. First, several studies [9,3,19] have shown that the theory of the LCS provides a systematic explanation of semantic decomposition as well as syntax determines. In particular, Kageyama [9] has shown that even a simple typology of LCS can explain a wide variety of linguistic phenomena including word association within compounds, transitivity alternation, and lexical derivation. Second, large-scale LCS dictionaries have been developed through practical use on machine translation and compound noun analysis [3,19]. The LCS dictionary for English [3] (4,163-verbs with 468 LCS types) was tailored based on a verb classification [12"
I05-1079,C98-1013,0,\N,Missing
I05-1079,W05-0620,0,\N,Missing
I05-5004,N01-1009,0,0.0295265,"neration and recognition has drawn the attention of an increasing number of researchers because of its potential contribution to a broad range of natural language applications. Paraphrases can be viewed as monolingual translations. From this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (MT), such as transformation algorithms (Lavoie et al., 2000; Takahashi et al., 2001), corpus-based techniques for paraphrase pattern acquisition (Barzilay and McKeown, 2001; Shinyama and Sekine, 2003; Quirk et al., 2004), and fluency measurements (Lapata, 2001; Fujita et al., 2004). One thing the paraphrasing community is still lacking is shared collections of paraphrase ex• We define a set of paraphrase classes based on the syntactic features of transformation patterns. • We separately collect paraphrase examples for each paraphrase class that are considered to be linguistically explainable. • We use a paraphrase generation system to exhaustively collect candidate paraphrases from a given text collection, which are then manually labeled. 1 5801 sentence pairs from their comparable corpus have been judged manually and available from http://research"
I05-5004,A00-1009,0,0.0121454,"results, which we have evaluated according to cost-efficiency, exhaustiveness, and reliability. 1 Introduction Paraphrases are alternative ways of conveying the same content. The technology for paraphrase generation and recognition has drawn the attention of an increasing number of researchers because of its potential contribution to a broad range of natural language applications. Paraphrases can be viewed as monolingual translations. From this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (MT), such as transformation algorithms (Lavoie et al., 2000; Takahashi et al., 2001), corpus-based techniques for paraphrase pattern acquisition (Barzilay and McKeown, 2001; Shinyama and Sekine, 2003; Quirk et al., 2004), and fluency measurements (Lapata, 2001; Fujita et al., 2004). One thing the paraphrasing community is still lacking is shared collections of paraphrase ex• We define a set of paraphrase classes based on the syntactic features of transformation patterns. • We separately collect paraphrase examples for each paraphrase class that are considered to be linguistically explainable. • We use a paraphrase generation system to exhaustively col"
I05-5004,J87-3006,0,0.0133468,"Missing"
I05-5004,W04-3219,0,0.123497,"same content. The technology for paraphrase generation and recognition has drawn the attention of an increasing number of researchers because of its potential contribution to a broad range of natural language applications. Paraphrases can be viewed as monolingual translations. From this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (MT), such as transformation algorithms (Lavoie et al., 2000; Takahashi et al., 2001), corpus-based techniques for paraphrase pattern acquisition (Barzilay and McKeown, 2001; Shinyama and Sekine, 2003; Quirk et al., 2004), and fluency measurements (Lapata, 2001; Fujita et al., 2004). One thing the paraphrasing community is still lacking is shared collections of paraphrase ex• We define a set of paraphrase classes based on the syntactic features of transformation patterns. • We separately collect paraphrase examples for each paraphrase class that are considered to be linguistically explainable. • We use a paraphrase generation system to exhaustively collect candidate paraphrases from a given text collection, which are then manually labeled. 1 5801 sentence pairs from their comparable corpus have been judged man"
I05-5004,W03-1609,0,0.101351,"ative ways of conveying the same content. The technology for paraphrase generation and recognition has drawn the attention of an increasing number of researchers because of its potential contribution to a broad range of natural language applications. Paraphrases can be viewed as monolingual translations. From this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (MT), such as transformation algorithms (Lavoie et al., 2000; Takahashi et al., 2001), corpus-based techniques for paraphrase pattern acquisition (Barzilay and McKeown, 2001; Shinyama and Sekine, 2003; Quirk et al., 2004), and fluency measurements (Lapata, 2001; Fujita et al., 2004). One thing the paraphrasing community is still lacking is shared collections of paraphrase ex• We define a set of paraphrase classes based on the syntactic features of transformation patterns. • We separately collect paraphrase examples for each paraphrase class that are considered to be linguistically explainable. • We use a paraphrase generation system to exhaustively collect candidate paraphrases from a given text collection, which are then manually labeled. 1 5801 sentence pairs from their comparable corpus"
I05-5004,P01-1008,0,0.0653514,"ction Paraphrases are alternative ways of conveying the same content. The technology for paraphrase generation and recognition has drawn the attention of an increasing number of researchers because of its potential contribution to a broad range of natural language applications. Paraphrases can be viewed as monolingual translations. From this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (MT), such as transformation algorithms (Lavoie et al., 2000; Takahashi et al., 2001), corpus-based techniques for paraphrase pattern acquisition (Barzilay and McKeown, 2001; Shinyama and Sekine, 2003; Quirk et al., 2004), and fluency measurements (Lapata, 2001; Fujita et al., 2004). One thing the paraphrasing community is still lacking is shared collections of paraphrase ex• We define a set of paraphrase classes based on the syntactic features of transformation patterns. • We separately collect paraphrase examples for each paraphrase class that are considered to be linguistically explainable. • We use a paraphrase generation system to exhaustively collect candidate paraphrases from a given text collection, which are then manually labeled. 1 5801 sentence pairs f"
I05-5004,N03-1003,0,0.0642753,"f judgement criteria. We use morpho-syntactic paraphrasing patterns derived from paraphrase samples in an analogous way to previous methods such as (Dras, 1999). For instance, from example (1), we derive a paraphrasing pattern for paraphrasing of light-verb constructions: (6) s. N -o(⇒V ) V 3.2 Automatic paraphrase acquisition Recently, paraphrase examples have been automatically collected as a source of acquiring paraphrase knowledge, such as pairs of synonymous phrases and syntactic transformation templates. Some studies exploit topically related articles derived from multiple news sources (Barzilay and Lee, 2003; Shinyama and Sekine, 2003; Quirk et al., 2004; Dolan et al., 2004). Sentence pairs that are likely to be paraphrases are automatically collected from the parallel or comparable corpora, using such clues as overlaps of content words and named entities, syntactic similarity, and reference description, such as date of the article and positions of sentences in the articles. N -ACC V t. V (N ) V (N ) where N is a variable which matches with a noun, V a verb, V (N ) denotes the verbalized form of 27 (c) second opinion (correct / incorrect) (a) source sentence Given Obligatory (b) automatically gen"
I05-5004,C04-1051,0,0.0259555,"ived from paraphrase samples in an analogous way to previous methods such as (Dras, 1999). For instance, from example (1), we derive a paraphrasing pattern for paraphrasing of light-verb constructions: (6) s. N -o(⇒V ) V 3.2 Automatic paraphrase acquisition Recently, paraphrase examples have been automatically collected as a source of acquiring paraphrase knowledge, such as pairs of synonymous phrases and syntactic transformation templates. Some studies exploit topically related articles derived from multiple news sources (Barzilay and Lee, 2003; Shinyama and Sekine, 2003; Quirk et al., 2004; Dolan et al., 2004). Sentence pairs that are likely to be paraphrases are automatically collected from the parallel or comparable corpora, using such clues as overlaps of content words and named entities, syntactic similarity, and reference description, such as date of the article and positions of sentences in the articles. N -ACC V t. V (N ) V (N ) where N is a variable which matches with a noun, V a verb, V (N ) denotes the verbalized form of 27 (c) second opinion (correct / incorrect) (a) source sentence Given Obligatory (b) automatically generated paraphrase (c) annotator’s judge (correct / incorrect) (f) fr"
I08-1070,P05-1074,0,0.114445,"y Y ⇒ Y announced the arrest of X b. X prevent Y ⇒ X lower the risk of Y They are actually called inference/entailment rules, and paraphrase is defined as bidirectional inference/entailment relation1 . While the similarity score in DIRT is symmetric for given pair of paths, the algorithm of TEASE considers the direction. The other utilizes a sort of parallel texts, such as multiple translation of the same text (Barzilay and McKeown, 2001; Pang et al., 2003), corresponding articles from multiple news sources (Barzilay and Lee, 2003; Dolan et al., 2004), and bilingual corpus (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005). This approach is, however, limited by the difficulty of obtaining parallel/comparable corpora. 2.3 Acquiring paraphrase instances As reviewed in Section 1, paraphrase rules generate incorrect paraphrases, because their applicability conditions are not specified. To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006). Although these clues restrict phenomena to those appearing in particular domain or those describing coordinated events, they have enabled us to collect 53"
I08-1070,P01-1008,0,0.748432,"a resource is to establish a practical method of computing semantic equivalence and syntactic substitutability, i.e., paraphrasability, between given pair of expressions. This paper addresses the issues of computing paraphrasability, focusing on syntactic variants of predicate phrases. Our model estimates paraphrasability based on traditional distributional similarity measures, where the Web snippets are used to overcome the data sparseness problem in handling predicate phrases. Several feature sets are evaluated through empirical experiments. a. burst into tears ⇔ cried b. comfort ⇔ console (Barzilay and McKeown, 2001) A number of studies have been carried out on both compositional (morpho-syntactic) and noncompositional (lexical and idiomatic) paraphrases (see Section 2). In most research, paraphrases have been represented with the similar templates, such as shown in (3) and (4). (3) a. N1 V N2 ⇔ N1 ’s V -ing of N2 b. N1 V N2 ⇔ N2 be V -en by N1 (Harris, 1957) (2) 1 Introduction One of the common characteristics of human languages is that the same concept can be expressed by various linguistic expressions. Such linguistic variations are called paraphrases. Handling paraphrases is one of the key issues in a"
I08-1070,N03-1003,0,0.180524,"stems often output directional rules such as exemplified in (5). (5) a. X is charged by Y ⇒ Y announced the arrest of X b. X prevent Y ⇒ X lower the risk of Y They are actually called inference/entailment rules, and paraphrase is defined as bidirectional inference/entailment relation1 . While the similarity score in DIRT is symmetric for given pair of paths, the algorithm of TEASE considers the direction. The other utilizes a sort of parallel texts, such as multiple translation of the same text (Barzilay and McKeown, 2001; Pang et al., 2003), corresponding articles from multiple news sources (Barzilay and Lee, 2003; Dolan et al., 2004), and bilingual corpus (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005). This approach is, however, limited by the difficulty of obtaining parallel/comparable corpora. 2.3 Acquiring paraphrase instances As reviewed in Section 1, paraphrase rules generate incorrect paraphrases, because their applicability conditions are not specified. To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006). Although these clues restrict phenomena to those appearin"
I08-1070,C04-1051,0,0.0917613,"tional rules such as exemplified in (5). (5) a. X is charged by Y ⇒ Y announced the arrest of X b. X prevent Y ⇒ X lower the risk of Y They are actually called inference/entailment rules, and paraphrase is defined as bidirectional inference/entailment relation1 . While the similarity score in DIRT is symmetric for given pair of paths, the algorithm of TEASE considers the direction. The other utilizes a sort of parallel texts, such as multiple translation of the same text (Barzilay and McKeown, 2001; Pang et al., 2003), corresponding articles from multiple news sources (Barzilay and Lee, 2003; Dolan et al., 2004), and bilingual corpus (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005). This approach is, however, limited by the difficulty of obtaining parallel/comparable corpora. 2.3 Acquiring paraphrase instances As reviewed in Section 1, paraphrase rules generate incorrect paraphrases, because their applicability conditions are not specified. To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006). Although these clues restrict phenomena to those appearing in particular domai"
I08-1070,W07-1425,1,0.655054,"s in handling paraphrases (Mel’ˇcuk and Polgu`ere, 1987). A bottleneck of MTT is that a huge amount of lexical knowledge is required to represent various relationships between lexical items. Jacquemin (1999) has represented the syntagmatic and paradigmatic correspondences between paraphrases with context-free transformation rules and morphological and/or semantic relations between lexical items, targeting at syntactic variants of technical terms that are typically noun phrases consisting of more than one word. We have proposed a framework of generating syntactic variants of predicate phrases (Fujita et al., 2007). Following the previous work, we have been developing three sorts of resources for Japanese. 2.2 Acquiring paraphrase rules Since the late 1990’s, the task of automatic acquisition of paraphrase rules has drawn the attention of an increasing number of researchers. Although most of the proposed methods do not explicitly eliminate compositional paraphrases, their output tends to be non-compositional paraphrase. Previous approaches to this task are two-fold. The first group espouses the distributional hypothesis (Harris, 1968). Among a number of models based on this hypothesis, two algorithms ar"
I08-1070,C04-1036,0,0.191918,"ents: (i) modifier or modifiee, (ii) dependency relation types (direct dependency, appositive, or parallel, c.f., RASP and MINIPAR), (iii) base form of the head-word, and (iv) case marker following noun, auxiliary verb and verbal suffixes if they appear. The last feature is employed to distinguish the subtle difference of meaning of predicate phrases, such as voice, tense, aspect, and modality. While Lin and Pantel (2001) have calculated similarities of paths based on slot fillers of subject and object slots, MOD targets at sub-trees and utilizes any modifiers and modifiees. Feature weighting Geffet and Dagan (2004) have reported on that the better quality of feature vector (weighting function) leads better results. So far, several weighting functions have been proposed, such as point-wise mutual information (Lin and Pantel, 2001) and Relative Feature Focus (Geffet and Dagan, 2004). While these functions compute weights using a small corpus for merely re-ranking samples, we are developing a measure that assesses the paraphrasability of arbitrary pair of phrases, where a more robust weighting function is necessary. Therefore we directly use frequencies of features within Web snippets as weight. Normalizat"
I08-1070,P05-1014,0,0.029649,"Although these clues restrict phenomena to those appearing in particular domain or those describing coordinated events, they have enabled us to collect 538 1 See http://nlp.cs.nyu.edu/WTEP/ paraphrases accurately. The notion of Inferential Selectional Preference (ISP) has been introduced by Pantel et al. (2007). ISP can capture more general phenomena than above two; however, it lacks abilities to distinguish antonym relations. 2.4 Computing semantic equivalence Semantic equivalence between given pair of expressions has so far been estimated under the distributional hypothesis (Harris, 1968). Geffet and Dagan (2005) have extended it to the distributional inclusion hypothesis for recognizing the direction of lexical entailment. Weeds et al. (2005), on the other hand, have pointed out the limitations of lexical similarity and syntactic transformation, and have proposed to directly compute the distributional similarity of pair of sub-parses based on the distributions of their modifiers and parents. We think it is worth examining if the Web can be used as the source for extracting features of phrases. 3 Computing paraphrasability between predicate phrases using Web snippets We define the concept of paraphras"
I08-1070,P99-1044,0,0.268798,"tion, have been represented with some grammar formalisms, such as transformational generative grammar (Harris, 1957) and synchronous tree adjoining grammar (Dras, 1999). These grammars, however, lack the information of applicability conditions. Word association within phrases has been an attractive topic. Meaning-Text Theory (MTT) is a framework which takes into account several types of lexical dependencies in handling paraphrases (Mel’ˇcuk and Polgu`ere, 1987). A bottleneck of MTT is that a huge amount of lexical knowledge is required to represent various relationships between lexical items. Jacquemin (1999) has represented the syntagmatic and paradigmatic correspondences between paraphrases with context-free transformation rules and morphological and/or semantic relations between lexical items, targeting at syntactic variants of technical terms that are typically noun phrases consisting of more than one word. We have proposed a framework of generating syntactic variants of predicate phrases (Fujita et al., 2007). Following the previous work, we have been developing three sorts of resources for Japanese. 2.2 Acquiring paraphrase rules Since the late 1990’s, the task of automatic acquisition of pa"
I08-1070,P99-1004,0,0.207581,"puting paraphrasability Paraphrasability is finally computed by two conventional distributional similarity measures. The first is the measure proposed in (Lin and Pantel, 2001):  f ∈Fs ∩Ft (w(s, f ) + w(t, f ))  , ParLin (s⇒t) =  f ∈Fs w(s, f ) + f ∈Ft w(t, f ) where Fs and Ft denote feature sets for s and t, respectively. w(x, f ) stands for the weight (frequency in our experiment) of f in Fx . While ParLin is symmetric, it has been argued that it is important to determine the direction of paraphrase. As an asymmetric measure, we examine αskew divergence defined by the following equation (Lee, 1999): dskew (t, s) = D (Ps αPt + (1 − α)Ps ) , where Px denotes a probability distribution estimated6 from a feature set Fx . How well Pt approximates Ps is calculated based on the KL divergence, D. The parameter α is set to 0.99, following tradition, because the optimization of α is difficult. To take consistent measurements, we define the paraphrasability score Parskew as follows: Parskew (s⇒t) = exp (−dskew (t, s)) . 6 We estimate them simply using P maximum likelihood estimation, i.e., Px (f ) = w(x, f )/ f  ∈Fx w(x, f  ). 540 Table 1: # of sampled source phrases and automatically generated"
I08-1070,J87-3006,0,0.0493003,"Missing"
I08-1070,N03-1024,0,0.0617865,"f representative entities for given binary relation templates. These systems often output directional rules such as exemplified in (5). (5) a. X is charged by Y ⇒ Y announced the arrest of X b. X prevent Y ⇒ X lower the risk of Y They are actually called inference/entailment rules, and paraphrase is defined as bidirectional inference/entailment relation1 . While the similarity score in DIRT is symmetric for given pair of paths, the algorithm of TEASE considers the direction. The other utilizes a sort of parallel texts, such as multiple translation of the same text (Barzilay and McKeown, 2001; Pang et al., 2003), corresponding articles from multiple news sources (Barzilay and Lee, 2003; Dolan et al., 2004), and bilingual corpus (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005). This approach is, however, limited by the difficulty of obtaining parallel/comparable corpora. 2.3 Acquiring paraphrase instances As reviewed in Section 1, paraphrase rules generate incorrect paraphrases, because their applicability conditions are not specified. To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005;"
I08-1070,N07-1071,0,0.546011,"hrases for given expressions is useful for text-transcoding tasks, such as machine translation and summarization, as well as beneficial to human, for instance, in text-to-speech, text simplification, and writing assistance. a. X wrote Y ⇔ X is the author of Y b. X solves Y ⇔ X deals with Y (Lin and Pantel, 2001) The weakness of these templates is that they should be applied only in some contexts. In other words, the lack of applicability conditions for slot fillers may lead incorrect paraphrases. One way to specify the applicability condition is to enumerate correct slot fillers. For example, Pantel et al. (2007) have harvested instances for the given paraphrase templates based on the co-occurrence statistics of slot fillers and lexicalized part of templates (e.g. “deal with” in (4b)). Yet, there is no method which assesses semantic equivalence and syntactic substitutability of resultant pairs of expressions. (4) 537 In this paper, we propose a method of directly computing semantic equivalence and syntactic substitutability, i.e., paraphrasability, particularly focusing on automatically generated compositional paraphrases (henceforth, syntactic variants) of predicate phrases. While previous studies ha"
I08-1070,I05-5011,0,0.0830551,"et al., 2003), corresponding articles from multiple news sources (Barzilay and Lee, 2003; Dolan et al., 2004), and bilingual corpus (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005). This approach is, however, limited by the difficulty of obtaining parallel/comparable corpora. 2.3 Acquiring paraphrase instances As reviewed in Section 1, paraphrase rules generate incorrect paraphrases, because their applicability conditions are not specified. To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006). Although these clues restrict phenomena to those appearing in particular domain or those describing coordinated events, they have enabled us to collect 538 1 See http://nlp.cs.nyu.edu/WTEP/ paraphrases accurately. The notion of Inferential Selectional Preference (ISP) has been introduced by Pantel et al. (2007). ISP can capture more general phenomena than above two; however, it lacks abilities to distinguish antonym relations. 2.4 Computing semantic equivalence Semantic equivalence between given pair of expressions has so far been estimated under the distributional hypothesi"
I08-1070,W04-3206,0,0.0708851,"rules has drawn the attention of an increasing number of researchers. Although most of the proposed methods do not explicitly eliminate compositional paraphrases, their output tends to be non-compositional paraphrase. Previous approaches to this task are two-fold. The first group espouses the distributional hypothesis (Harris, 1968). Among a number of models based on this hypothesis, two algorithms are referred to as the state-of-the-art. DIRT (Lin and Pantel, 2001) collects paraphrase rules consisting of a pair of paths between two nominal slots based on point-wise mutual information. TEASE (Szpektor et al., 2004) discovers binary relation templates from the Web based on sets of representative entities for given binary relation templates. These systems often output directional rules such as exemplified in (5). (5) a. X is charged by Y ⇒ Y announced the arrest of X b. X prevent Y ⇒ X lower the risk of Y They are actually called inference/entailment rules, and paraphrase is defined as bidirectional inference/entailment relation1 . While the similarity score in DIRT is symmetric for given pair of paths, the algorithm of TEASE considers the direction. The other utilizes a sort of parallel texts, such as mu"
I08-1070,P07-1058,0,0.144005,"Missing"
I08-1070,N06-1008,0,0.0607164,"corresponding articles from multiple news sources (Barzilay and Lee, 2003; Dolan et al., 2004), and bilingual corpus (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005). This approach is, however, limited by the difficulty of obtaining parallel/comparable corpora. 2.3 Acquiring paraphrase instances As reviewed in Section 1, paraphrase rules generate incorrect paraphrases, because their applicability conditions are not specified. To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006). Although these clues restrict phenomena to those appearing in particular domain or those describing coordinated events, they have enabled us to collect 538 1 See http://nlp.cs.nyu.edu/WTEP/ paraphrases accurately. The notion of Inferential Selectional Preference (ISP) has been introduced by Pantel et al. (2007). ISP can capture more general phenomena than above two; however, it lacks abilities to distinguish antonym relations. 2.4 Computing semantic equivalence Semantic equivalence between given pair of expressions has so far been estimated under the distributional hypothesis (Harris, 1968)."
I08-1070,W05-1202,0,0.426989,"d us to collect 538 1 See http://nlp.cs.nyu.edu/WTEP/ paraphrases accurately. The notion of Inferential Selectional Preference (ISP) has been introduced by Pantel et al. (2007). ISP can capture more general phenomena than above two; however, it lacks abilities to distinguish antonym relations. 2.4 Computing semantic equivalence Semantic equivalence between given pair of expressions has so far been estimated under the distributional hypothesis (Harris, 1968). Geffet and Dagan (2005) have extended it to the distributional inclusion hypothesis for recognizing the direction of lexical entailment. Weeds et al. (2005), on the other hand, have pointed out the limitations of lexical similarity and syntactic transformation, and have proposed to directly compute the distributional similarity of pair of sub-parses based on the distributions of their modifiers and parents. We think it is worth examining if the Web can be used as the source for extracting features of phrases. 3 Computing paraphrasability between predicate phrases using Web snippets We define the concept of paraphrasability as follows: A grammatical phrase s is paraphrasable with another phrase t, iff t satisfies the following three: • t is gramma"
I08-1070,P03-1016,0,0.232922,") a. X is charged by Y ⇒ Y announced the arrest of X b. X prevent Y ⇒ X lower the risk of Y They are actually called inference/entailment rules, and paraphrase is defined as bidirectional inference/entailment relation1 . While the similarity score in DIRT is symmetric for given pair of paths, the algorithm of TEASE considers the direction. The other utilizes a sort of parallel texts, such as multiple translation of the same text (Barzilay and McKeown, 2001; Pang et al., 2003), corresponding articles from multiple news sources (Barzilay and Lee, 2003; Dolan et al., 2004), and bilingual corpus (Wu and Zhou, 2003; Bannard and Callison-Burch, 2005). This approach is, however, limited by the difficulty of obtaining parallel/comparable corpora. 2.3 Acquiring paraphrase instances As reviewed in Section 1, paraphrase rules generate incorrect paraphrases, because their applicability conditions are not specified. To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006). Although these clues restrict phenomena to those appearing in particular domain or those describing coordinated events,"
I17-2019,D16-1215,0,0.0358178,"Missing"
I17-2019,P11-2087,0,0.072296,"Missing"
I17-2019,W15-1501,0,0.0598011,"Missing"
I17-2019,P11-2117,0,0.144051,"tence pair, while the other is to classify given sentence pair into one of the three classes (good, ok, and bad). In the classification task of the QATS workshop, systems based on deep neural networks (Paetzold and Specia, ˇ 2016a) and MT metrics (Stajner et al., 2016a) have achieved the best performance. However, deep neural networks are rather unstable because of the difficulty of training on a limited amount of data; for instance, the QATS dataset offers only 505 sentence pairs for training. MT metrics are incapable of properly capturing deletions that are prevalent in text simplification (Coster and Kauchak, 2011), as they are originally designed to gauge semantic equivalence. In fact, as shown in Table 1, well-known MT metrics are strongly biased by the length difference between original and simple sentences, even though it is rather unrelated with the quality of text simplification assessed by humans. In order to properly account for the surfacelevel inequivalency occurring in text simplification, we examine semantic similarity features based on word embeddings and paraphrase lexicons. Unlike end-to-end training with deep neural networks, we quantify word-level semantic correIntroduction Text simplif"
I17-2019,W16-4912,0,0.0354931,"a sentence, we used the averaged word embeddings (Adi et al., 2017). |x| |y| i=1 j=1 1 ∑ 1 ∑ xi − yj DWE(x, y) = |x| |y| 3.3 Baseline Systems As the baseline, we employed four types of sysˇ tems from the QATS workshop (Stajner et al., 2016b): two typical baselines and two top-ranked systems. “Majority-class” labels all the sentence pairs with the most frequent class in the training data. “MT-baseline” combines BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009), TER (Snover et al., 2006), and WER (Levenshtein, 1966), using a support vector machine (SVM) classifier. SimpleNets (Paetzold and Specia, 2016a) has two different forms of deep neural network architectures: multi-layer perceptron (SimpleNetsMLP) and recurrent neural network (SimpleNetsRNN). SimpleNets-MLP uses seven features of each sentence: the number of characters, tokens, and word types, 5-gram language model probabilities estimated on the basis of either SUBTLEX (Brysbaert and New, 2009), SubIMDB (Paetzold and Specia, 2016b), Wikipedia, and Simple Wikipedia (Kauchak, 2013). SimpleNets-RNN, which does not require such feature engineering, uses embeddings of word N -grams. ˇ SMH (Stajner et al., 2016a) has two types of classifier"
I17-2019,N15-1022,0,0.0128603,"on such features achieves good performance in the classification task. 2 noise by considering only the best word alignment for each word in one sentence as follows. |x| 1 ∑ MAS(x, y) = max cos(xi , yj ) j |x| As MAS is asymmetric, we calculate it for each direction, i.e., MAS(x, y) and MAS(y, x), unlike Kajiwara and Komachi (2016) who has averaged these two values. Semantic Features Based on Word Alignments We bring a total of seven types of features that are proven useful for the similar task, i.e., finding corresponding sentence pairs within English Wikipedia and Simple English Wikipedia (Hwang et al., 2015; Kajiwara and Komachi, 2016). Specifically, we assume that some of these features are useful to capture inequivalency between original sentence and its simplified version introduced during simplification, such as lexical paraphrases and deletion of words and phrases. Throughout this section, original sentence and its simplified version are referred to as x and y, respectively. 2.4 HAS: Hungarian Alignment Similarity AAS and MAS deal with many-to-many and oneto-many word alignments, respectively. On the other hand, HAS (Song and Roth, 2015) is based on one-to-one word alignments. The task of i"
I17-2019,P02-1040,0,0.098149,"nce of Word Embeddings We also introduce the difference between sentence embeddings so as to gauge their differences in terms of meaning and simplicity. As the representation of a sentence, we used the averaged word embeddings (Adi et al., 2017). |x| |y| i=1 j=1 1 ∑ 1 ∑ xi − yj DWE(x, y) = |x| |y| 3.3 Baseline Systems As the baseline, we employed four types of sysˇ tems from the QATS workshop (Stajner et al., 2016b): two typical baselines and two top-ranked systems. “Majority-class” labels all the sentence pairs with the most frequent class in the training data. “MT-baseline” combines BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009), TER (Snover et al., 2006), and WER (Levenshtein, 1966), using a support vector machine (SVM) classifier. SimpleNets (Paetzold and Specia, 2016a) has two different forms of deep neural network architectures: multi-layer perceptron (SimpleNetsMLP) and recurrent neural network (SimpleNetsRNN). SimpleNets-MLP uses seven features of each sentence: the number of characters, tokens, and word types, 5-gram language model probabilities estimated on the basis of either SUBTLEX (Brysbaert and New, 2009), SubIMDB (Paetzold and Specia, 2016b), Wikipedia, and Simple Wik"
I17-2019,C16-1109,1,0.887469,"Missing"
I17-2019,P15-2070,0,0.0621332,"Missing"
I17-2019,P13-1151,0,0.0192105,"(Lavie and Denkowski, 2009), TER (Snover et al., 2006), and WER (Levenshtein, 1966), using a support vector machine (SVM) classifier. SimpleNets (Paetzold and Specia, 2016a) has two different forms of deep neural network architectures: multi-layer perceptron (SimpleNetsMLP) and recurrent neural network (SimpleNetsRNN). SimpleNets-MLP uses seven features of each sentence: the number of characters, tokens, and word types, 5-gram language model probabilities estimated on the basis of either SUBTLEX (Brysbaert and New, 2009), SubIMDB (Paetzold and Specia, 2016b), Wikipedia, and Simple Wikipedia (Kauchak, 2013). SimpleNets-RNN, which does not require such feature engineering, uses embeddings of word N -grams. ˇ SMH (Stajner et al., 2016a) has two types of classifiers: logistic classifier (SMH-IBk/Logistic) and random forest classifier (SMH-RandForest, SMH-RandForest-b). Both are trained relying on the automatic evaluation metrics for MT, such as BLEU, METEOR, and TER, in combination with the QE features for MT (Specia et al., 2013). Instead of reimplementing the above baseline systems, we excerpted their performance scores ˇ from (Stajner et al., 2016b). (6) 2.7 PAS: Paraphrase Alignment Similarity"
I17-2019,N16-1131,0,0.0395215,"Missing"
I17-2019,2006.amta-papers.25,0,0.060982,"entence embeddings so as to gauge their differences in terms of meaning and simplicity. As the representation of a sentence, we used the averaged word embeddings (Adi et al., 2017). |x| |y| i=1 j=1 1 ∑ 1 ∑ xi − yj DWE(x, y) = |x| |y| 3.3 Baseline Systems As the baseline, we employed four types of sysˇ tems from the QATS workshop (Stajner et al., 2016b): two typical baselines and two top-ranked systems. “Majority-class” labels all the sentence pairs with the most frequent class in the training data. “MT-baseline” combines BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009), TER (Snover et al., 2006), and WER (Levenshtein, 1966), using a support vector machine (SVM) classifier. SimpleNets (Paetzold and Specia, 2016a) has two different forms of deep neural network architectures: multi-layer perceptron (SimpleNetsMLP) and recurrent neural network (SimpleNetsRNN). SimpleNets-MLP uses seven features of each sentence: the number of characters, tokens, and word types, 5-gram language model probabilities estimated on the basis of either SUBTLEX (Brysbaert and New, 2009), SubIMDB (Paetzold and Specia, 2016b), Wikipedia, and Simple Wikipedia (Kauchak, 2013). SimpleNets-RNN, which does not require"
I17-2019,N15-1138,0,0.171418,"within English Wikipedia and Simple English Wikipedia (Hwang et al., 2015; Kajiwara and Komachi, 2016). Specifically, we assume that some of these features are useful to capture inequivalency between original sentence and its simplified version introduced during simplification, such as lexical paraphrases and deletion of words and phrases. Throughout this section, original sentence and its simplified version are referred to as x and y, respectively. 2.4 HAS: Hungarian Alignment Similarity AAS and MAS deal with many-to-many and oneto-many word alignments, respectively. On the other hand, HAS (Song and Roth, 2015) is based on one-to-one word alignments. The task of identifying the best one-to-one word alignments H is regarded as a problem of bipartite graph matching, where the two sets of vertices respectively comprise words within each sentence x and y, and the weight of a edge between xi and yj is given by the cosine similarity calculated over their word embeddings. Given H identified using the Hungarian algorithm (Kuhn, 1955), HAS is computed by averaging the similarities between embeddings of the aligned pairs of words. 2.1 AES: Additive Embeddings Similarity Given two sentences, x and y, AES betwe"
I17-2019,P13-4014,0,0.0664536,"Missing"
I17-2019,Q14-1018,0,0.0247557,"pleNets-RNN, which does not require such feature engineering, uses embeddings of word N -grams. ˇ SMH (Stajner et al., 2016a) has two types of classifiers: logistic classifier (SMH-IBk/Logistic) and random forest classifier (SMH-RandForest, SMH-RandForest-b). Both are trained relying on the automatic evaluation metrics for MT, such as BLEU, METEOR, and TER, in combination with the QE features for MT (Specia et al., 2013). Instead of reimplementing the above baseline systems, we excerpted their performance scores ˇ from (Stajner et al., 2016b). (6) 2.7 PAS: Paraphrase Alignment Similarity PAS (Sultan et al., 2014, 2015) is computed based on lexical paraphrases. This feature has been proven useful in the semantic textual similarity task of SemEval-2015 (Agirre et al., 2015). PA(x, y) + PA(y, x) (7) |x |+ |y| { |x| ∑ 1 ∃j : xi ⇔ yj ∈ y PA(x, y) = 0 otherwise i=1 PAS(x, y) = where xi ⇔ yj holds if and only if the word pair (xi , yj ) is included in a given paraphrase lexicon. 3 Experiment The usefulness of the above features was evaluated through an empirical experiment using the QATS ˇ dataset (Stajner et al., 2016b). 3.1 Data The QATS dataset consists of 505 and 126 sentence pairs for training and test"
I17-2019,S15-2027,0,0.0429715,"Missing"
I17-2019,W14-1201,0,0.0511045,"Missing"
I17-2019,W16-3411,0,0.0470369,"Missing"
I17-2019,P12-1107,0,0.0809342,"Missing"
I17-2019,Q16-1029,0,0.0200772,"uch as learners (Petersen and Ostendorf, 2007) and children (Belder and Moens, 2010). Such systems would also improve the performance of other natural language processing tasks, such as information extraction (Evans, 2011) and machine ˇ translation (MT) (Stajner and Popovi´c, 2016). Similarly to other text-to-text generation tasks, such as MT and summarization, the outputs of text simplification systems have been evaluated subˇ jectively by humans (Wubben et al., 2012; Stajner et al., 2014) or automatically by comparing with handcrafted reference texts (Specia, 2010; Coster and Kauchak, 2011; Xu et al., 2016). However, the former is costly and not replicable, and the latter has achieved only a low correlation with human evaluation. On the basis of this backdrop, Quality Estimation (QE) (Specia et al., 2010), i.e., automatic evaluation without reference, has been drawing much attention in the research community. In the shared 1 http://qats2016.github.io/shared.html 109 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 109–115, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP spondences using two pre-compiled external resources: (a) word embed"
N15-1065,P05-1074,0,0.0671902,"gments). Monolingual comparable corpora are also useful sources of paraphrases. For instance, articles from different newswire services describing the same event can be used in that way (Shinyama et al., 2002; Barzilay and Elhadad, 2003; Dolan et al., 2004; Wubben et al., 2009). Chen and Dolan (2011) created such corpora by collecting multiple descriptions of short movies through crowdsourcing. Webharvested definition sentences of the same term often contain paraphrases (Hashimoto et al., 2011; Yan et al., 2013). Bilingual parallel corpora have been recognized as sources of paraphrases since (Bannard and Callison-Burch, 2005). First, a translation table is created using techniques developed for statistical machine translation. Then, pairs of expressions in the same language that share the same translations are extracted. For instance, a pair (“under control”, “in check”) will be extracted if they are both linked with the German translation “unter controlle.” Each paraphrase pair (e1 , e2 ) is assigned probabilities, p(e2 |e1 ) and p(e1 |e2 ), estimated by marginalizing over all the∑ translations F shared by e1 and e2 , i.e., p(e2 |e1 ) = f ∈F p(e2 |f )p(f |e1 ). This bilingual pivoting approach inspired further te"
N15-1065,P01-1008,0,0.122126,"odified words (Hagiwara et al., 2006). The similarity of a pair of expressions is calculated by comparing the distributions of their contexts. Despite the quantitative advantage, this approach tends to result in low accuracy. This is because contextual information alone often fails to differentiate paraphrases from expressions that have other semantic relations, e.g., antonyms and sibling words. 631 2.1.2 Parallel and Comparable Corpora Much effort has gone into compiling monolingual parallel corpora and extracting paraphrases from them by identifying corresponding parts of aligned sentences. Barzilay and McKeown (2001) and Pang et al. (2003) collected multiple human translations of the same source text. Multiple verbalizations of mathematical proofs were also used (Barzilay and Lee, 2002). This triangulating method provides solid anchors that guarantee the semantic equivalence of sentences (or text fragments). Monolingual comparable corpora are also useful sources of paraphrases. For instance, articles from different newswire services describing the same event can be used in that way (Shinyama et al., 2002; Barzilay and Elhadad, 2003; Dolan et al., 2004; Wubben et al., 2009). Chen and Dolan (2011) created s"
N15-1065,W02-1022,0,0.038503,", this approach tends to result in low accuracy. This is because contextual information alone often fails to differentiate paraphrases from expressions that have other semantic relations, e.g., antonyms and sibling words. 631 2.1.2 Parallel and Comparable Corpora Much effort has gone into compiling monolingual parallel corpora and extracting paraphrases from them by identifying corresponding parts of aligned sentences. Barzilay and McKeown (2001) and Pang et al. (2003) collected multiple human translations of the same source text. Multiple verbalizations of mathematical proofs were also used (Barzilay and Lee, 2002). This triangulating method provides solid anchors that guarantee the semantic equivalence of sentences (or text fragments). Monolingual comparable corpora are also useful sources of paraphrases. For instance, articles from different newswire services describing the same event can be used in that way (Shinyama et al., 2002; Barzilay and Elhadad, 2003; Dolan et al., 2004; Wubben et al., 2009). Chen and Dolan (2011) created such corpora by collecting multiple descriptions of short movies through crowdsourcing. Webharvested definition sentences of the same term often contain paraphrases (Hashimot"
N15-1065,W03-1004,0,0.0326337,"hrases from them by identifying corresponding parts of aligned sentences. Barzilay and McKeown (2001) and Pang et al. (2003) collected multiple human translations of the same source text. Multiple verbalizations of mathematical proofs were also used (Barzilay and Lee, 2002). This triangulating method provides solid anchors that guarantee the semantic equivalence of sentences (or text fragments). Monolingual comparable corpora are also useful sources of paraphrases. For instance, articles from different newswire services describing the same event can be used in that way (Shinyama et al., 2002; Barzilay and Elhadad, 2003; Dolan et al., 2004; Wubben et al., 2009). Chen and Dolan (2011) created such corpora by collecting multiple descriptions of short movies through crowdsourcing. Webharvested definition sentences of the same term often contain paraphrases (Hashimoto et al., 2011; Yan et al., 2013). Bilingual parallel corpora have been recognized as sources of paraphrases since (Bannard and Callison-Burch, 2005). First, a translation table is created using techniques developed for statistical machine translation. Then, pairs of expressions in the same language that share the same translations are extracted. For"
N15-1065,P08-1077,0,0.0175983,"us types of corpora. There are two major streams: one that uses monolingual corpora and one that uses parallel or comparable corpora. 2.1.1 Monolingual Corpora A monolingual corpus is the most promising resource when targeting increased coverage, thanks to the availability of Web-scale monolingual data. Techniques that use such corpora mostly extract pairs of expressions by exploiting the contextual similarity associated with the Distributional Hypothesis (Harris, 1954). A given expression is represented with its co-occurring expressions such as adjacent word n-grams (Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008; Marton, 2013), nominal elements (Lin and Pantel, 2001; Szpektor et al., 2004; De Saeger et al., 2011), and modifiers and modified words (Hagiwara et al., 2006). The similarity of a pair of expressions is calculated by comparing the distributions of their contexts. Despite the quantitative advantage, this approach tends to result in low accuracy. This is because contextual information alone often fails to differentiate paraphrases from expressions that have other semantic relations, e.g., antonyms and sibling words. 631 2.1.2 Parallel and Comparable Corpora Much effort has gone into compiling"
N15-1065,D08-1021,0,0.467313,"cal machine translation. Then, pairs of expressions in the same language that share the same translations are extracted. For instance, a pair (“under control”, “in check”) will be extracted if they are both linked with the German translation “unter controlle.” Each paraphrase pair (e1 , e2 ) is assigned probabilities, p(e2 |e1 ) and p(e1 |e2 ), estimated by marginalizing over all the∑ translations F shared by e1 and e2 , i.e., p(e2 |e1 ) = f ∈F p(e2 |f )p(f |e1 ). This bilingual pivoting approach inspired further techniques such as the use of syntactic information as the basis of constraints (Callison-Burch, 2008; Zhao et al., 2009), learning patterns using synchronous grammar (Ganitkevitch et al., 2011), uncovering missing links by combining multiple translation tables and other lexical resources (Kok and Brockett, 2010), and re-ranking candidate pairs on the basis of contextual similarity (Chan et al., 2011). Ganitkevitch and Callison-Burch (2014) compiled paraphrase lexicons for various languages on this approach. Parallel/comparable corpora are useful sources of highly accurate paraphrases. However, for most languages, only small paraphrase lexicons can be created due to the limited availability o"
N15-1065,W11-2504,0,0.0372043,"Missing"
N15-1065,P11-1020,0,0.0304676,"es. Barzilay and McKeown (2001) and Pang et al. (2003) collected multiple human translations of the same source text. Multiple verbalizations of mathematical proofs were also used (Barzilay and Lee, 2002). This triangulating method provides solid anchors that guarantee the semantic equivalence of sentences (or text fragments). Monolingual comparable corpora are also useful sources of paraphrases. For instance, articles from different newswire services describing the same event can be used in that way (Shinyama et al., 2002; Barzilay and Elhadad, 2003; Dolan et al., 2004; Wubben et al., 2009). Chen and Dolan (2011) created such corpora by collecting multiple descriptions of short movies through crowdsourcing. Webharvested definition sentences of the same term often contain paraphrases (Hashimoto et al., 2011; Yan et al., 2013). Bilingual parallel corpora have been recognized as sources of paraphrases since (Bannard and Callison-Burch, 2005). First, a translation table is created using techniques developed for statistical machine translation. Then, pairs of expressions in the same language that share the same translations are extracted. For instance, a pair (“under control”, “in check”) will be extracted"
N15-1065,D11-1076,0,0.0699988,"Missing"
N15-1065,W10-1751,0,0.0265859,"erably exhibits various lexical correspondences that our method will exploit. For this purpose, paraphrases acquired from bilingual or monolingual parallel corpora are preferable (see Section 2.1.2). In this study, we take the bilingual pivoting method as an example for the sake of reproducibility. However, the method also outputs a large number of non-paraphrases. To obtain further clean seeds, we apply several filters as described in (Fujita et al., 2012) and discard pairs that have low paraphrase probability, i.e., p(e2 |e1 ) &lt; 0.01, following the convention in (Du et al., 2010; Max, 2010; Denkowski and Lavie, 2010; Fujita et al., 2012). Previous work (Chan et al., 2011; Fujita et al., 2012; Ganitkevitch et al., 2013) has proved that the information obtained from monolingual data can be used for assessing bilingually originated paraphrases. Thus, pairs that have low contextual similarity are also filtered out. Among various recipes for computing contextual similarity, we use a simple one: cosine measure of two context vectors comprising adjacent word 1–4 grams of all of the phrase appearances in given monolingual data. For a fair com633 As exemplified by (“X:ment”, “X:ing”) in (4c), we represent pairs o"
N15-1065,C04-1051,0,0.0910284,"ing corresponding parts of aligned sentences. Barzilay and McKeown (2001) and Pang et al. (2003) collected multiple human translations of the same source text. Multiple verbalizations of mathematical proofs were also used (Barzilay and Lee, 2002). This triangulating method provides solid anchors that guarantee the semantic equivalence of sentences (or text fragments). Monolingual comparable corpora are also useful sources of paraphrases. For instance, articles from different newswire services describing the same event can be used in that way (Shinyama et al., 2002; Barzilay and Elhadad, 2003; Dolan et al., 2004; Wubben et al., 2009). Chen and Dolan (2011) created such corpora by collecting multiple descriptions of short movies through crowdsourcing. Webharvested definition sentences of the same term often contain paraphrases (Hashimoto et al., 2011; Yan et al., 2013). Bilingual parallel corpora have been recognized as sources of paraphrases since (Bannard and Callison-Burch, 2005). First, a translation table is created using techniques developed for statistical machine translation. Then, pairs of expressions in the same language that share the same translations are extracted. For instance, a pair (“"
N15-1065,D10-1041,0,0.0277839,"at has high quality and preferably exhibits various lexical correspondences that our method will exploit. For this purpose, paraphrases acquired from bilingual or monolingual parallel corpora are preferable (see Section 2.1.2). In this study, we take the bilingual pivoting method as an example for the sake of reproducibility. However, the method also outputs a large number of non-paraphrases. To obtain further clean seeds, we apply several filters as described in (Fujita et al., 2012) and discard pairs that have low paraphrase probability, i.e., p(e2 |e1 ) &lt; 0.01, following the convention in (Du et al., 2010; Max, 2010; Denkowski and Lavie, 2010; Fujita et al., 2012). Previous work (Chan et al., 2011; Fujita et al., 2012; Ganitkevitch et al., 2013) has proved that the information obtained from monolingual data can be used for assessing bilingually originated paraphrases. Thus, pairs that have low contextual similarity are also filtered out. Among various recipes for computing contextual similarity, we use a simple one: cosine measure of two context vectors comprising adjacent word 1–4 grams of all of the phrase appearances in given monolingual data. For a fair com633 As exemplified by (“X:ment”,"
N15-1065,W07-1425,1,0.743235,"l corpora have also been used as a source of paraphrases. 630 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 630–640, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics completely different from those of the seed paraphrases, e.g., (2a) and (2b). (2) a. investment of resources ⇔ investing resources b. recruitment of engineers ⇔ recruiting engineers While the generality underlying paraphrases has been exploited either by handcrafted rules (Harris, 1957; Mel’ˇcuk and Polgu`ere, 1987; Jacquemin, 1999; Fujita et al., 2007) or by data-driven techniques (Ganitkevitch et al., 2011; Fujita et al., 2012), we still lack a robust and accurate way of identifying various types of lexical variants. Our method tackles this issue using affix patterns that are also acquired from high-quality seed paraphrases in a fully empirical way. Consequently, our method has the potential to apply to many languages. 2 Previous Work 2.1 Creating Paraphrase Lexicons Researchers have been intensively studying methods for automatically creating paraphrase lexicons using various types of corpora. There are two major streams: one that uses mo"
N15-1065,D12-1058,1,0.728031,"Missing"
N15-1065,D11-1108,0,0.0343624,"Missing"
N15-1065,N13-1092,0,0.056361,"Missing"
N15-1065,ganitkevitch-callison-burch-2014-multilingual,0,0.0648965,"|e1 ) and p(e1 |e2 ), estimated by marginalizing over all the∑ translations F shared by e1 and e2 , i.e., p(e2 |e1 ) = f ∈F p(e2 |f )p(f |e1 ). This bilingual pivoting approach inspired further techniques such as the use of syntactic information as the basis of constraints (Callison-Burch, 2008; Zhao et al., 2009), learning patterns using synchronous grammar (Ganitkevitch et al., 2011), uncovering missing links by combining multiple translation tables and other lexical resources (Kok and Brockett, 2010), and re-ranking candidate pairs on the basis of contextual similarity (Chan et al., 2011). Ganitkevitch and Callison-Burch (2014) compiled paraphrase lexicons for various languages on this approach. Parallel/comparable corpora are useful sources of highly accurate paraphrases. However, for most languages, only small paraphrase lexicons can be created due to the limited availability of such corpora. 2.1.3 Combination of Multiple Corpora Unlike the above methods, which used only a single type of corpus as sources of paraphrases, Fujita et al. (2012) used both bilingual parallel and monolingual corpora as sources. In that method, paraphrase pairs, e.g., (3a), are first acquired from a bilingual parallel corpus using the bi"
N15-1065,W99-0904,0,0.506916,"Polgu`ere, 1987), propose a representation of paraphrases that involve alternations of lexical variants. Jacquemin (1999) and Fujita et al. (2007) addressed this type of paraphrase using manually described syntactic transformation patterns in combination with dictionaries of lexical variants. Catvar (Habash and Dorr, 2003) is a comprehensive lexical derivation database for English. WordNet (Fellbaum, 1998) also contains information of that kind and is currently available for various languages. Despite its high accuracy, manual creation of rich lexical resources requires a large human effort. Gaussier (1999) and Fujita et al. (2007) extracted groups of lexical derivations from a list of headwords of dictionaries through mining affix patterns. This approach significantly reduces human effort, maintaining reasonable accuracy, but the coverage is still limited because of the reliance on manually compiled dictionaries. 3 Proposed Method This study is the first attempt to exploit various types of lexical variants for acquiring paraphrases in a completely empirical way. Given a seed paraphrase lexicon (SSeed ) our method (henceforth LEXVAR) expands it in two steps (see also Figure 1). Step 1. Learning"
N15-1065,N03-1013,0,0.0592227,"an learn syntax-based patterns during the bilingual pivoting process (Ganitkevitch et al., 2011). 632 same word, e.g., {“color”, “colour”} and {“authorize”, “authorise”}. Several syntactic and semantic theories, such as transformational grammar (Harris, 1957) and Meaning-Text Theory (Mel’ˇcuk and Polgu`ere, 1987), propose a representation of paraphrases that involve alternations of lexical variants. Jacquemin (1999) and Fujita et al. (2007) addressed this type of paraphrase using manually described syntactic transformation patterns in combination with dictionaries of lexical variants. Catvar (Habash and Dorr, 2003) is a comprehensive lexical derivation database for English. WordNet (Fellbaum, 1998) also contains information of that kind and is currently available for various languages. Despite its high accuracy, manual creation of rich lexical resources requires a large human effort. Gaussier (1999) and Fujita et al. (2007) extracted groups of lexical derivations from a list of headwords of dictionaries through mining affix patterns. This approach significantly reduces human effort, maintaining reasonable accuracy, but the coverage is still limited because of the reliance on manually compiled dictionari"
N15-1065,P06-1045,0,0.0165478,"ngual corpus is the most promising resource when targeting increased coverage, thanks to the availability of Web-scale monolingual data. Techniques that use such corpora mostly extract pairs of expressions by exploiting the contextual similarity associated with the Distributional Hypothesis (Harris, 1954). A given expression is represented with its co-occurring expressions such as adjacent word n-grams (Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008; Marton, 2013), nominal elements (Lin and Pantel, 2001; Szpektor et al., 2004; De Saeger et al., 2011), and modifiers and modified words (Hagiwara et al., 2006). The similarity of a pair of expressions is calculated by comparing the distributions of their contexts. Despite the quantitative advantage, this approach tends to result in low accuracy. This is because contextual information alone often fails to differentiate paraphrases from expressions that have other semantic relations, e.g., antonyms and sibling words. 631 2.1.2 Parallel and Comparable Corpora Much effort has gone into compiling monolingual parallel corpora and extracting paraphrases from them by identifying corresponding parts of aligned sentences. Barzilay and McKeown (2001) and Pang"
N15-1065,P11-1109,0,0.0389204,"Missing"
N15-1065,P99-1044,0,0.185292,"nolingual parallel corpora have also been used as a source of paraphrases. 630 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 630–640, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics completely different from those of the seed paraphrases, e.g., (2a) and (2b). (2) a. investment of resources ⇔ investing resources b. recruitment of engineers ⇔ recruiting engineers While the generality underlying paraphrases has been exploited either by handcrafted rules (Harris, 1957; Mel’ˇcuk and Polgu`ere, 1987; Jacquemin, 1999; Fujita et al., 2007) or by data-driven techniques (Ganitkevitch et al., 2011; Fujita et al., 2012), we still lack a robust and accurate way of identifying various types of lexical variants. Our method tackles this issue using affix patterns that are also acquired from high-quality seed paraphrases in a fully empirical way. Consequently, our method has the potential to apply to many languages. 2 Previous Work 2.1 Creating Paraphrase Lexicons Researchers have been intensively studying methods for automatically creating paraphrase lexicons using various types of corpora. There are two major str"
N15-1065,N10-1017,0,0.0178677,"ked with the German translation “unter controlle.” Each paraphrase pair (e1 , e2 ) is assigned probabilities, p(e2 |e1 ) and p(e1 |e2 ), estimated by marginalizing over all the∑ translations F shared by e1 and e2 , i.e., p(e2 |e1 ) = f ∈F p(e2 |f )p(f |e1 ). This bilingual pivoting approach inspired further techniques such as the use of syntactic information as the basis of constraints (Callison-Burch, 2008; Zhao et al., 2009), learning patterns using synchronous grammar (Ganitkevitch et al., 2011), uncovering missing links by combining multiple translation tables and other lexical resources (Kok and Brockett, 2010), and re-ranking candidate pairs on the basis of contextual similarity (Chan et al., 2011). Ganitkevitch and Callison-Burch (2014) compiled paraphrase lexicons for various languages on this approach. Parallel/comparable corpora are useful sources of highly accurate paraphrases. However, for most languages, only small paraphrase lexicons can be created due to the limited availability of such corpora. 2.1.3 Combination of Multiple Corpora Unlike the above methods, which used only a single type of corpus as sources of paraphrases, Fujita et al. (2012) used both bilingual parallel and monolingual"
N15-1065,J10-3003,0,0.0511117,"Missing"
N15-1065,D10-1064,0,0.0155803,"ty and preferably exhibits various lexical correspondences that our method will exploit. For this purpose, paraphrases acquired from bilingual or monolingual parallel corpora are preferable (see Section 2.1.2). In this study, we take the bilingual pivoting method as an example for the sake of reproducibility. However, the method also outputs a large number of non-paraphrases. To obtain further clean seeds, we apply several filters as described in (Fujita et al., 2012) and discard pairs that have low paraphrase probability, i.e., p(e2 |e1 ) &lt; 0.01, following the convention in (Du et al., 2010; Max, 2010; Denkowski and Lavie, 2010; Fujita et al., 2012). Previous work (Chan et al., 2011; Fujita et al., 2012; Ganitkevitch et al., 2013) has proved that the information obtained from monolingual data can be used for assessing bilingually originated paraphrases. Thus, pairs that have low contextual similarity are also filtered out. Among various recipes for computing contextual similarity, we use a simple one: cosine measure of two context vectors comprising adjacent word 1–4 grams of all of the phrase appearances in given monolingual data. For a fair com633 As exemplified by (“X:ment”, “X:ing”) in"
N15-1065,J87-3006,0,0.030462,"Missing"
N15-1065,I05-1011,0,0.090456,"Missing"
N15-1065,N03-1024,0,0.0797274,"Missing"
N15-1065,W04-3206,0,0.0417493,"d one that uses parallel or comparable corpora. 2.1.1 Monolingual Corpora A monolingual corpus is the most promising resource when targeting increased coverage, thanks to the availability of Web-scale monolingual data. Techniques that use such corpora mostly extract pairs of expressions by exploiting the contextual similarity associated with the Distributional Hypothesis (Harris, 1954). A given expression is represented with its co-occurring expressions such as adjacent word n-grams (Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008; Marton, 2013), nominal elements (Lin and Pantel, 2001; Szpektor et al., 2004; De Saeger et al., 2011), and modifiers and modified words (Hagiwara et al., 2006). The similarity of a pair of expressions is calculated by comparing the distributions of their contexts. Despite the quantitative advantage, this approach tends to result in low accuracy. This is because contextual information alone often fails to differentiate paraphrases from expressions that have other semantic relations, e.g., antonyms and sibling words. 631 2.1.2 Parallel and Comparable Corpora Much effort has gone into compiling monolingual parallel corpora and extracting paraphrases from them by identify"
N15-1065,W09-0621,0,0.0622907,"Missing"
N15-1065,2003.mtsummit-systems.9,0,\N,Missing
N15-1065,N13-1007,0,\N,Missing
N19-1384,P18-1073,0,0.11261,"ese phrase pairs are collected from the same monolingual data from which we extract partial translations. Given partial translations Figure 2: The framework for extracting partial translations from monolingual data. the induced phrase table, we search for sentence pairs that are the most likely partial translations in the monolingual data (Section 2.2). Finally, the extracted sentence pairs are post-processed (Section 2.3). 2.1 Unsupervised Phrase Table Induction Recent methods addressed the task of finding word translations from monolingual data without any supervision (Lample et al., 2018a; Artetxe et al., 2018a). On the other hand, Marie and Fujita (2018) presented a method for inducing phrase tables from monolingual data using a weaklysupervised framework. To make our approach useful in as many translation tasks as possible, including very low-resource scenarios, we propose a fully unsupervised version of the method in Marie and Fujita (2018). Using phrases instead of only single tokens promotes the extraction of partial translations containing longer sequences of tokens, rather than those with potentially more but discontinuously translated tokens. Regarding all n-grams of tokens in the monolingu"
N19-1384,D18-1399,0,0.0878972,"ese phrase pairs are collected from the same monolingual data from which we extract partial translations. Given partial translations Figure 2: The framework for extracting partial translations from monolingual data. the induced phrase table, we search for sentence pairs that are the most likely partial translations in the monolingual data (Section 2.2). Finally, the extracted sentence pairs are post-processed (Section 2.3). 2.1 Unsupervised Phrase Table Induction Recent methods addressed the task of finding word translations from monolingual data without any supervision (Lample et al., 2018a; Artetxe et al., 2018a). On the other hand, Marie and Fujita (2018) presented a method for inducing phrase tables from monolingual data using a weaklysupervised framework. To make our approach useful in as many translation tasks as possible, including very low-resource scenarios, we propose a fully unsupervised version of the method in Marie and Fujita (2018). Using phrases instead of only single tokens promotes the extraction of partial translations containing longer sequences of tokens, rather than those with potentially more but discontinuously translated tokens. Regarding all n-grams of tokens in the monolingu"
N19-1384,P18-1008,0,0.0288374,"e it can easily learn to pay no attention to UNKPP and more attention to correctly translated tokens thanks to the multi-head attention mechanism. Moreover, the Transformer model does not memorize complete 4 This includes equations, rows of a table, titles, etc. We made this token different from the usual token reserved for unknown word in the vocabulary, since they are of a different nature. 5 sequences and makes time steps independent, unlike recurrent neural networks (RNN). It uses instead positional encodings and has a better ability in linking important features from the entire sequence (Chen et al., 2018), which may make easier the learning from noisy sequences, such as the ones created by introducing UNKPP. We show in Section 4 that using UNKPP tokens instead of dropping uncovered tokens leads to a better model. Nonetheless, a better strategy that we leave for future work could be to apply some forced-decoding, with an SMT system for instance, that translates in the source sentence the parts of the target sentence that are not translated, while preserving the partial translations detected by our algorithm in the source sentence. 3 Experiments We experimented on three language pairs with diffe"
N19-1384,W17-4715,0,0.0698768,"of NMT systems. All the evaluated systems used the parallel data used to train the baseline system, and other synthetic parallel data generated by back-translation (backtr), copy (copy), or from extracted partial translation (partial). each language pair in order to get best possible baseline systems. We then apply the same hyperparameters in all the experiments for the given language pair. To train systems with partial translations (partial), we simply mixed them with the original parallel data during training. We also evaluated the systems using back-translated (backtr) and copied11 (copy) (Currey et al., 2017) data, separately mixed with the original parallel data. Note that these data were generated from the same target sentences sampled for extracting partial translations: partial, backtr, and copy had the same target side but different source side.12 Our NMT systems were evaluated with detokenized BLEU-cased. 3.3 Results The results of our experiments are presented in Table 1. For en→de and en→tr, the baseline systems resulted in a poor translation quality below 10 BLEU points. This highlights how critical it is to get more training data to better train an NMT model for low-resource translation"
N19-1384,D18-1045,0,0.0427176,"Missing"
N19-1384,2012.amta-papers.7,0,0.0221762,"data that can be generated by the back-translation system trained on some existing parallel data. Previous work has also studied the extraction of translation pairs of source and target sentences from monolingual data in their respective languages. They have been shown to be useful to train better statistical machine translation (SMT) systems, especially in low-resource conditions. Existing methods on sentence pair extraction mainly rely on the availability of comparable corpora as the source of accurate sentence pairs (Abdul Rauf and Schwenk, 2011), or on the robustness of SMT against noise (Goutte et al., 2012) because sentence pairs extracted from unrelated monolingual corpora tend to be noisy (Tillmann and Xu, 2009; Marie and Fujita, 2017). Most of them also require pre-trained accurate translation models, those of SMT systems for instance, that we may not have in low-resource conditions. Moreover, unlike SMT, NMT has been shown to deal very poorly with noisy training data and still largely underperforms SMT for low-resource language pairs (Koehn and Knowles, 2017) for which comparable corpora are usually not available. Even without an accurate translation model, we still have the possibility of e"
N19-1384,W18-2703,0,0.0209173,"on an existing accurate translation model trained on large parallel data, introducing a strong bias in the retrieval of sentence pairs. Unlike existing methods, our algorithm for retrieving partial translations is efficient enough to work on large unrelated monolingual data without relying on any document-level information, and also fully unsupervised. Without any bias toward some existing parallel data, it is very suitable for low-resource scenarios. Previous work has also exploited monolingual data in the target language for improving NMT systems (Sennrich et al., 2016; Currey et al., 2017; Hoang et al., 2018). As demonstrated in this paper, our approach is complementary to previous work, since partial translations can introduce novel information into training. To the best of our knowledge, this is the first work to propose a method for extracting sentence pairs from source and target 14 For this experiment, the NMT system for backtranslation was also trained on the full parallel data. 15 Only 0.1 BLEU points lower than the best reported result at WMT17 for this task. The winning systems used much more back-translated data and an ensemble of several models for decoding. 3841 unrelated monolingual c"
N19-1384,Q17-1024,0,0.0837924,"Missing"
N19-1384,P18-4020,0,0.0546493,"Missing"
N19-1384,W17-3204,0,0.0344194,"of comparable corpora as the source of accurate sentence pairs (Abdul Rauf and Schwenk, 2011), or on the robustness of SMT against noise (Goutte et al., 2012) because sentence pairs extracted from unrelated monolingual corpora tend to be noisy (Tillmann and Xu, 2009; Marie and Fujita, 2017). Most of them also require pre-trained accurate translation models, those of SMT systems for instance, that we may not have in low-resource conditions. Moreover, unlike SMT, NMT has been shown to deal very poorly with noisy training data and still largely underperforms SMT for low-resource language pairs (Koehn and Knowles, 2017) for which comparable corpora are usually not available. Even without an accurate translation model, we still have the possibility of extracting sentence pairs from unrelated source and target monolingual data. However, this is very challenging since we have no guarantee that there are sentence pairs actually retrievable from a given 3834 Proceedings of NAACL-HLT 2019, pages 3834–3844 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Source sentence der Mann wurde festgenommen . Partial translation a man was arrested at the scene . Post-processed U"
N19-1384,J82-2005,0,0.680587,"Missing"
N19-1384,P17-2062,1,0.931915,"he extraction of translation pairs of source and target sentences from monolingual data in their respective languages. They have been shown to be useful to train better statistical machine translation (SMT) systems, especially in low-resource conditions. Existing methods on sentence pair extraction mainly rely on the availability of comparable corpora as the source of accurate sentence pairs (Abdul Rauf and Schwenk, 2011), or on the robustness of SMT against noise (Goutte et al., 2012) because sentence pairs extracted from unrelated monolingual corpora tend to be noisy (Tillmann and Xu, 2009; Marie and Fujita, 2017). Most of them also require pre-trained accurate translation models, those of SMT systems for instance, that we may not have in low-resource conditions. Moreover, unlike SMT, NMT has been shown to deal very poorly with noisy training data and still largely underperforms SMT for low-resource language pairs (Koehn and Knowles, 2017) for which comparable corpora are usually not available. Even without an accurate translation model, we still have the possibility of extracting sentence pairs from unrelated source and target monolingual data. However, this is very challenging since we have no guaran"
N19-1384,P16-1009,0,0.110169,"ion quality. 1 Introduction Neural machine translation (NMT) systems usually require a large quantity of high-quality bilingual parallel data for training. However, for most language pairs, we do not have such resources, or only in very small quantities, mainly because they are costly to produce. On the other hand, monolingual corpora are readily available in large quantity for many languages. Previous work has proposed various strategies to integrate monolingual data into NMT systems and has confirmed their usefulness to improve NMT systems. The so-called backtranslation of monolingual data (Sennrich et al., 2016) is undoubtedly the most prevalent one. This approach simply uses a target-to-source MT system to translate monolingual data in the target language into the source language. The produced new synthetic parallel corpus can be used together with the original parallel data to increase the size of the training data, and eventually to improve NMT systems significantly and consistently. However, on the source side, the synthetic data only contain data that can be generated by the back-translation system trained on some existing parallel data. Previous work has also studied the extraction of translati"
N19-1384,P18-1072,0,0.0563611,"Missing"
N19-1384,N09-2024,0,0.123557,"work has also studied the extraction of translation pairs of source and target sentences from monolingual data in their respective languages. They have been shown to be useful to train better statistical machine translation (SMT) systems, especially in low-resource conditions. Existing methods on sentence pair extraction mainly rely on the availability of comparable corpora as the source of accurate sentence pairs (Abdul Rauf and Schwenk, 2011), or on the robustness of SMT against noise (Goutte et al., 2012) because sentence pairs extracted from unrelated monolingual corpora tend to be noisy (Tillmann and Xu, 2009; Marie and Fujita, 2017). Most of them also require pre-trained accurate translation models, those of SMT systems for instance, that we may not have in low-resource conditions. Moreover, unlike SMT, NMT has been shown to deal very poorly with noisy training data and still largely underperforms SMT for low-resource language pairs (Koehn and Knowles, 2017) for which comparable corpora are usually not available. Even without an accurate translation model, we still have the possibility of extracting sentence pairs from unrelated source and target monolingual data. However, this is very challengin"
N19-1384,I17-1039,0,0.0224762,"ovel information into training. To the best of our knowledge, this is the first work to propose a method for extracting sentence pairs from source and target 14 For this experiment, the NMT system for backtranslation was also trained on the full parallel data. 15 Only 0.1 BLEU points lower than the best reported result at WMT17 for this task. The winning systems used much more back-translated data and an ensemble of several models for decoding. 3841 unrelated monolingual corpora that can be used to train better NMT systems, without requiring any modification of current NMT model architecture. Wang et al. (2017) proposed a method to train an RNN-based NMT system on partially aligned translations only. However, this method cannot straightforwardly be applied to the state-of-theart Transformer architecture. In contrast, our proposed method does not assume a particular architecture of NMT, nor requires any modifications of the NMT implementation. In addition, they assume not only that a phrase table is given for their low-resource language pairs, but also that the phrase pairs in the given phrase table are very accurate. We rather focus on augmenting the training data, without assuming phrase pairs of h"
N19-1384,I17-1016,0,0.0243999,"/len(T ), with S and T respectively a source and a target sentence, kt the number of tokens in T that are covered by Bt , and len(·) a function that returns the number of tokens in a sentence. With the harmonic mean Ft between cs and ct , the algorithm searches for target sentences containing as many words translating source tokens as possible while penalizing the retrieval of very long target sentences that may also contain many tokens having no counterparts in the source sentence. Then, the algorithm re-ranks the target sentences in Tm with the phrase-based forced decoding (PBFD) algorithm (Zhang et al., 2017) (l.12). PBFD searches for the best combination of phrase pairs covering S and each target sentence in Tm , using the phrase translation probability computed by Eq. (1). However, the original PBFD algorithm penalizes sentence pair with words that are not covered by any phrase pair. It tends to favor very short sentences that are potentially less exploitable for NMT systems, or not even be sentences as it may happen when dealing with noisy 3836 monolingual data.4 Therefore, we use a slightly modified version which does not penalize uncovered words on the target side in order to favor the extrac"
P17-2062,C16-1109,0,0.0459482,"ator of similarity between two sentences. |x| 1 X max φ(xemb , yjemb ) S(x, y) = i j |x| (1) i=1 2.1 Step 1: Filtering with sentence embeddings where x and y are respectively the source and target sentences, |x |the length of x, and φ the cosine similarity between the embeddings in the target language space of the i-th word in x, i.e., xemb , i emb and the j-th word in y, i.e., yj . The computation of this score can be highly costly, depending on the sentence length and the number of dimensions of the word embeddings. Thus, we compute this score only for the source to target direction, unlike Kajiwara and Komachi (2016). In many situations, we may also have an access to a lexical translation model trained on some parallel data. We therefore incorporate the scores proposed by Tillmann and Xu (2009), but considering one probability for each translation direction, instead of summing them up, so that our classifier can optimize their weight separately. Assuming the availability of large-scale monolingual data, our method exploits word embeddings (Mikolov et al., 2013b) that are fast to estimate. First, word embeddings for each language are learned from the given monolingual data. This enables us to evaluate arbi"
P17-2062,P07-2045,0,0.00502736,"et sentence for each source sentence was retained. We discarded sentence pairs having a score lower than a threshExperiments We evaluated our method in a scenario of domain adaptation for phrase-based SMT (PBSMT). In this scenario, we assumed a lot of general-domain parallel data to train a general-domain phrase table and a lot of in-domain monolingual data as our source of in-domain pseudo-parallel sentences. 3.1 Parameters for sentence pair extraction Data and SMT system We experimented with the French–English language pair, both translation directions, on the medical domain. We used Moses (Koehn et al., 2007) to train, tune, and test our PBSMT systems. The general-domain phrase table was trained on Europarl V74 (1.99M sentences). The in-domain monolingual data were prepared by applying the NLTK5 sentence segmenter to the concatenation of all the monolingual corpora provided for the WMT’14 medical translation task.6 As the source of extracting in-domain sentence pairs, we randomly sampled 1M sentences (33M tokens) from the French data and 5M sentences (164M tokens) from the English data. Given pseudo-parallel sentences extracted by our method from these data 7 http://statmt.org/wmt15/ http://word2v"
P17-2062,P11-2031,0,0.0139871,"= 0.955) (th = 0.700) 28.0 28.6 2,607 1,985 25.4 26.4 2,533 1,955 121k 361k 14.46M Proposed method (th = 0.600) w/ cov. constraint 26.1 3,064 23.2 3,077 11k 19.21M Proposed method Table 1: BLEU scores (Papineni et al., 2002) averaged over 3 tuning runs, obtained when added an in-domain phrase table to the system, created either by the baseline method or by our work with or without the coverage constraint activated (denoted “w/ cov. constraint”). Bold scores indicate statistical significance (p &lt; 0.01) of the score over the baseline system, measured by approximate randomization using MultEval (Clark et al., 2011). We also present the number of OOV tokens in the test set and the number of sentence pairs actually used to train the in-domain phrase table. The speed of the method to evaluate sentence pairs from monolingual data was measured with 100 CPU threads (Xeon E5-2600) on 1 trillion sentence pairs randomly sampled. with the optimal threshold of 0.7, extracted 361k sentence pairs from the in-domain monolingual data, while the baseline method extracted only 121k sentence pairs due presumably to the use of the coverage constraint that might remove source sentences with a high OOV ratio. Less OOV token"
P17-2062,P15-1107,0,0.0219101,"and Marcu (2005) randomly paired sentences from 2 Despite the availability of more accurate methods (Coulmance et al., 2015; Duong et al., 2016) we choose this method considering its low computational cost and its reasonable need of external resources to estimate the translation matrix, i.e., only a small bilingual dictionary. 3 As shown by Adi et al. (2016), this can be effective to encode sentence-level information such as content and length, while being computationally more efficient than other methods, such as inducing paragraph vectors (Le and Mikolov, 2014) and using LSTM auto-encoders (Li et al., 2015). Our decision also relies on the promising accuracy of linear projection of word (not sentence) embeddings across different languages (Mikolov et al., 2013a). 393 their parallel data using two constraints: a length ratio not greater than two, and a coverage constraint that considers a negative example only if more than half of the words of the source sentence has a translation in the given target sentence according to some bilingual lexicon. However, from a large parallel corpus, one can easily retrieve another target sentence, almost identical, containing most of the words that the true targ"
P17-2062,D15-1131,0,0.0309946,"nk them, we train a Maximum Entropy (ME) classifier, following Munteanu and Marcu (2005). ME classifier suits particularly well our situation, since we deal with a small number of dense features and have hundred millions of sentence pairs to classify quickly. Positive examples for training the classifier can be obtained straightforwardly: we use true sentence pairs sampled from parallel data, different from the one used to train the lexical translation model. As for negative examples, Munteanu and Marcu (2005) randomly paired sentences from 2 Despite the availability of more accurate methods (Coulmance et al., 2015; Duong et al., 2016) we choose this method considering its low computational cost and its reasonable need of external resources to estimate the translation matrix, i.e., only a small bilingual dictionary. 3 As shown by Adi et al. (2016), this can be effective to encode sentence-level information such as content and length, while being computationally more efficient than other methods, such as inducing paragraph vectors (Le and Mikolov, 2014) and using LSTM auto-encoders (Li et al., 2015). Our decision also relies on the promising accuracy of linear projection of word (not sentence) embeddings"
P17-2062,2015.iwslt-evaluation.11,0,0.0221806,"LEU scores, our method provides a better handling of OOV, ignored by other methods that strongly rely on already trained lexical translation models. Our method can further be speeded up by some approximation, such as local sensitive hashing, or by using a smaller number of dimensions for word embeddings. We leave the study of their impact to our future work. We believe that our work is also useful for other downstream tasks that need comparable or pseudo-parallel sentences, such as parallel phrase extraction (Hewavitharana and Vogel, 2016) and adaptation of neural machine translation systems (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). task are reported in Table 2. For both translation directions, the features that have the most important were the ones based on lexical translaiton probabilities and alignments between embeddings. For instance, in En→Fr translation, removing them led to a significant drop of 0.4 and 0.8 BLEU points, respectively. For the Fr→En translation direction, surprisingly, we observed improvements on the test set for all configurations, except when removing either of the above two types of features. However, we did not observe such improvements for the En→Fr translation"
P17-2062,D16-1136,0,0.022481,"mum Entropy (ME) classifier, following Munteanu and Marcu (2005). ME classifier suits particularly well our situation, since we deal with a small number of dense features and have hundred millions of sentence pairs to classify quickly. Positive examples for training the classifier can be obtained straightforwardly: we use true sentence pairs sampled from parallel data, different from the one used to train the lexical translation model. As for negative examples, Munteanu and Marcu (2005) randomly paired sentences from 2 Despite the availability of more accurate methods (Coulmance et al., 2015; Duong et al., 2016) we choose this method considering its low computational cost and its reasonable need of external resources to estimate the translation matrix, i.e., only a small bilingual dictionary. 3 As shown by Adi et al. (2016), this can be effective to encode sentence-level information such as content and length, while being computationally more efficient than other methods, such as inducing paragraph vectors (Le and Mikolov, 2014) and using LSTM auto-encoders (Li et al., 2015). Our decision also relies on the promising accuracy of linear projection of word (not sentence) embeddings across different lan"
P17-2062,W04-3208,0,0.0751069,"irs to train a translation system makes it able to produce better translations. However, for most language pairs and domains, parallel corpora remain scarce due mainly to the cost of their creation (Germann, 2001). In the last two decades, numerous methods have been proposed to extract parallel sentences from comparable corpora. In addition to comparable corpora in large quantity, to the best of our knowledge, all previous methods heavily rely on document-level information and/or lexical translation models, such as those for statistical machine translation (SMT) systems (Zhao and Vogel, 2002; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Tillmann and Xu, 2009) and manually-created bilingual lexicon (Utiyama and Isahara, 2003). The most successful approaches use cross-lingual information retrieval techniques (Abdul Rauf and Schwenk, 2011; S, tef˘anescu et al., 2012) to extract sentence pairs from comparable documents. Using such document pairs 2 Sentence pair extraction During the sentence pair extraction, we do not assume an access to document-level information. Our method thus has to be efficient in evaluating 1 As in previous work, we regard the sentence pairs extracted by our method as “pseudo-pa"
P17-2062,J05-4003,0,0.665711,"ion system makes it able to produce better translations. However, for most language pairs and domains, parallel corpora remain scarce due mainly to the cost of their creation (Germann, 2001). In the last two decades, numerous methods have been proposed to extract parallel sentences from comparable corpora. In addition to comparable corpora in large quantity, to the best of our knowledge, all previous methods heavily rely on document-level information and/or lexical translation models, such as those for statistical machine translation (SMT) systems (Zhao and Vogel, 2002; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Tillmann and Xu, 2009) and manually-created bilingual lexicon (Utiyama and Isahara, 2003). The most successful approaches use cross-lingual information retrieval techniques (Abdul Rauf and Schwenk, 2011; S, tef˘anescu et al., 2012) to extract sentence pairs from comparable documents. Using such document pairs 2 Sentence pair extraction During the sentence pair extraction, we do not assume an access to document-level information. Our method thus has to be efficient in evaluating 1 As in previous work, we regard the sentence pairs extracted by our method as “pseudo-parallel” because they are n"
P17-2062,W01-1409,0,0.0788266,"pairs and then a classifier to find the most reliable ones. We report significant improvements in domain adaptation for statistical machine translation when using a translation model trained on the sentence pairs extracted from in-domain monolingual corpora. 1 Introduction Parallel corpus is an indispensable resource for statistical and neural machine translation. Generally, using more sentence pairs to train a translation system makes it able to produce better translations. However, for most language pairs and domains, parallel corpora remain scarce due mainly to the cost of their creation (Germann, 2001). In the last two decades, numerous methods have been proposed to extract parallel sentences from comparable corpora. In addition to comparable corpora in large quantity, to the best of our knowledge, all previous methods heavily rely on document-level information and/or lexical translation models, such as those for statistical machine translation (SMT) systems (Zhao and Vogel, 2002; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Tillmann and Xu, 2009) and manually-created bilingual lexicon (Utiyama and Isahara, 2003). The most successful approaches use cross-lingual information retrieval te"
P17-2062,P02-1040,0,0.102059,"Missing"
P17-2062,2012.amta-papers.7,0,0.165885,"-created bilingual lexicon (Utiyama and Isahara, 2003). The most successful approaches use cross-lingual information retrieval techniques (Abdul Rauf and Schwenk, 2011; S, tef˘anescu et al., 2012) to extract sentence pairs from comparable documents. Using such document pairs 2 Sentence pair extraction During the sentence pair extraction, we do not assume an access to document-level information. Our method thus has to be efficient in evaluating 1 As in previous work, we regard the sentence pairs extracted by our method as “pseudo-parallel” because they are not necessarily parallel. As shown by Goutte et al. (2012), even very noisy parallel corpora may be useful for SMT. 392 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 392–398 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2062 trillions of sentence pairs hypothesized from two monolingual corpora, each containing millions of sentences. To achieve this computationally challenging task, we need a fast way to compute some similarity between the source and target sentences, without relying on large lexical translati"
P17-2062,2012.eamt-1.37,0,0.0346949,"Missing"
P17-2062,N09-2024,0,0.775307,"o produce better translations. However, for most language pairs and domains, parallel corpora remain scarce due mainly to the cost of their creation (Germann, 2001). In the last two decades, numerous methods have been proposed to extract parallel sentences from comparable corpora. In addition to comparable corpora in large quantity, to the best of our knowledge, all previous methods heavily rely on document-level information and/or lexical translation models, such as those for statistical machine translation (SMT) systems (Zhao and Vogel, 2002; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Tillmann and Xu, 2009) and manually-created bilingual lexicon (Utiyama and Isahara, 2003). The most successful approaches use cross-lingual information retrieval techniques (Abdul Rauf and Schwenk, 2011; S, tef˘anescu et al., 2012) to extract sentence pairs from comparable documents. Using such document pairs 2 Sentence pair extraction During the sentence pair extraction, we do not assume an access to document-level information. Our method thus has to be efficient in evaluating 1 As in previous work, we regard the sentence pairs extracted by our method as “pseudo-parallel” because they are not necessarily parallel."
P17-2062,P03-1010,0,0.120788,"and domains, parallel corpora remain scarce due mainly to the cost of their creation (Germann, 2001). In the last two decades, numerous methods have been proposed to extract parallel sentences from comparable corpora. In addition to comparable corpora in large quantity, to the best of our knowledge, all previous methods heavily rely on document-level information and/or lexical translation models, such as those for statistical machine translation (SMT) systems (Zhao and Vogel, 2002; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Tillmann and Xu, 2009) and manually-created bilingual lexicon (Utiyama and Isahara, 2003). The most successful approaches use cross-lingual information retrieval techniques (Abdul Rauf and Schwenk, 2011; S, tef˘anescu et al., 2012) to extract sentence pairs from comparable documents. Using such document pairs 2 Sentence pair extraction During the sentence pair extraction, we do not assume an access to document-level information. Our method thus has to be efficient in evaluating 1 As in previous work, we regard the sentence pairs extracted by our method as “pseudo-parallel” because they are not necessarily parallel. As shown by Goutte et al. (2012), even very noisy parallel corpora"
P17-2062,P16-1024,0,0.0443719,"Missing"
P19-1312,P18-1073,0,0.240961,"n several cross-lingual NLP tasks. 1 Introduction Bilingual word embeddings (BWE) represent the vocabulary of two languages in one common continuous vector space. They are known to be useful in a wide range of cross-lingual NLP tasks. The most prevalent methods for training BWE are so-called mapping methods (Mikolov et al., 2013a): word embeddings for two languages are separately trained on respective monolingual data and then mapped into one common embedding space. The mapping function is usually trained using a small bilingual lexicon for supervision. Recently, unsupervised mapping for BWE (Artetxe et al., 2018a; Lample et al., 2018a), i.e., trained without using any manually created bilingual resources, has been shown to reach a performance comparable to supervised BWE in several crosslingual NLP tasks. Unsupervised BWE are trained with a three-step approach. First, word embeddings are roughly mapped into an initial BWE space, for instance using adversarial training or an heuristic mapping. Then, using the initial BWE, a small synthetic bilingual lexicon is induced. Finally, a new BWE, which is expected to be better than the initial BWE, is learned from the induced lexicon through a pseudo-supervis"
P19-1312,D18-1399,0,0.220211,"n several cross-lingual NLP tasks. 1 Introduction Bilingual word embeddings (BWE) represent the vocabulary of two languages in one common continuous vector space. They are known to be useful in a wide range of cross-lingual NLP tasks. The most prevalent methods for training BWE are so-called mapping methods (Mikolov et al., 2013a): word embeddings for two languages are separately trained on respective monolingual data and then mapped into one common embedding space. The mapping function is usually trained using a small bilingual lexicon for supervision. Recently, unsupervised mapping for BWE (Artetxe et al., 2018a; Lample et al., 2018a), i.e., trained without using any manually created bilingual resources, has been shown to reach a performance comparable to supervised BWE in several crosslingual NLP tasks. Unsupervised BWE are trained with a three-step approach. First, word embeddings are roughly mapped into an initial BWE space, for instance using adversarial training or an heuristic mapping. Then, using the initial BWE, a small synthetic bilingual lexicon is induced. Finally, a new BWE, which is expected to be better than the initial BWE, is learned from the induced lexicon through a pseudo-supervis"
P19-1312,P19-1019,0,0.0426782,"ning framework is on top of existing unsupervised mapping methods. languages. As phrases, we also consider all the token types in each corpus. In our phrase table, each L1 phrase is paired with its k most probable translations in L2 determined based on a score computed from the given BWE.2 The phrase table and a language model trained on the L2 monolingual data compose the initial USMT. Then, the USMT is iteratively refined in the following manner. • Synthetic parallel data are generated by translating monolingual data using the USMT. Both L1-to-L2 and L2-to-L1 translations can be considered (Artetxe et al., 2019). Training on synthetic parallel data For an unsupervised training of BWE, the training data must also be generated in an unsupervised way. To this end, we chose unsupervised machine translation (MT). Recent work has shown significant progress in unsupervised MT (Artetxe et al., 2018b; Lample et al., 2018b) with generated translations of a reasonable quality. Both statistical (SMT) and neural MT (NMT) have been adapted to the unsupervised scenario. We chose unsupervised SMT (USMT) to generate synthetic parallel data since it generates better translations than unsupervised NMT (Lample et al., 2"
P19-1312,Q17-1010,0,0.0640516,"xts, facilitating the training of word embeddings and compensating, to some extent, for the noisiness of the translations. In Appendix A, we provide results of our preliminary experiments supporting this assumption. 3 Experiments Are BWE unsupervisedly and jointly trained on noisy synthetic data better than unsupervised mapped BWE? To answer this question, we conducted experiments in three different tasks with three language pairs: English–German (en-de), English–French (en-fr), and English–Indonesian (en-id). 3.1 Settings for training BWE We trained monolingual word embeddings with fastText (Bojanowski et al., 2017)3 separately on English (239M lines), German (237M lines), and French (38M lines) News Crawl corpora provided by WMT4 for en-de and en-fr. For enid, we used English (100M lines) and Indonesian (77M lines) Common Crawl corpora.5 We then mapped the word embeddings into a BWE space using VECMAP,6 one of the best and most robust methods for unsupervised mapping (Glavas et al., 2019). The resulting BWE were used as baselines in our evaluation tasks and also to bootstrap our USMT system. Our initial USMT systems were induced with the following configuration. Maximum phrase length was set to six (L ="
P19-1312,N13-1073,0,0.152935,"Missing"
P19-1312,P19-1070,0,0.0718199,"Missing"
P19-1312,P13-2121,0,0.156727,"Missing"
P19-1312,P18-1072,0,0.0551317,"Missing"
P19-1312,P16-1157,0,0.530076,"ion with some supervised mapping method. The last two steps can be repeated to refine the BWE. In spite of their success, unsupervised mapping methods are inherently limited by the dissimilarity between the original word embedding spaces to be mapped. The feasibility of aligning two embedding spaces relies on the assumption that they are isomorphic. However, Søgaard et al. (2018) showed that these spaces are, in general, far from being isomorphic, and thus they result in suboptimal or degenerated unsupervised mappings. On the other hand, supervised methods that jointly train BWE from scratch (Upadhyay et al., 2016), on parallel or comparable corpora, do not have such limits since no pre-existing embedding spaces and no mapping function are involved. These methods jointly train BWE by exploiting bilingual and monolingual contexts of words, materialized by sentence or document pairs, to learn a single BWE space. However, they require large bilingual resources for training. To the best of our knowledge, joint training of BWE has never been explored for unsupervised scenarios. In this paper, we propose unsupervised joint training of BWE. Our method is an extension of previous work on unsupervised BWE: we pr"
Q17-1034,N12-1047,0,0.0163922,"2 Conf. 3 √ √ √ √ √ √ √ √ Table 5: Multiple phrase table configurations. trained using lmplz (Heafield et al., 2013).18 To concentrate on the translation model, we did not use the lexical reordering model throughout the experiments, while we enabled distance-based reordering up to six words. Our systems used the multiple decoding paths ability of Moses; we used up to three phrase tables in one system, as summarized in Table 5. We did not add the features presented in Section 3.2 to the phrase pairs directly derived from the parallel data.19 Weights of the features were optimized with kb-mira (Cherry and Foster, 2012) using 200-best hypotheses on 15 iterations. The translation outputs were evaluated with BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). The results were averaged over three tuning runs. The statistical significance was measured by approximate randomization (Clark et al., 2011) using MultEval.20 4.4 Additional baseline systems To compare our work with a state-of-the-art phrase table induction method, we implemented the work of Zhao et al. (2015). Even though they did not propose their method to perform domain adaptation of an SMT system, their work is the closest to ours a"
Q17-1034,P11-2031,0,0.019304,"used the multiple decoding paths ability of Moses; we used up to three phrase tables in one system, as summarized in Table 5. We did not add the features presented in Section 3.2 to the phrase pairs directly derived from the parallel data.19 Weights of the features were optimized with kb-mira (Cherry and Foster, 2012) using 200-best hypotheses on 15 iterations. The translation outputs were evaluated with BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). The results were averaged over three tuning runs. The statistical significance was measured by approximate randomization (Clark et al., 2011) using MultEval.20 4.4 Additional baseline systems To compare our work with a state-of-the-art phrase table induction method, we implemented the work of Zhao et al. (2015). Even though they did not propose their method to perform domain adaptation of an SMT system, their work is the closest to ours and does not require other external resources than those we used, i.e., parallel data and monolingual data not necessarily comparable. We implemented both global (GLP) and local (LLP) linear projection strategies and collected source and target phrases as they did. The source phrase set contains all"
Q17-1034,D15-1131,0,0.015951,"effectively estimate the reliability of each pair. This section describes several features to characterize each phrase pair; they are used for evaluating phrase pairs and also added in the induced phrase table to guide the decoder. 3.2.1 Cross-lingual semantic similarity Many researchers tackled the problem of estimating cross-lingual semantic similarity between pairs of words or phrases by using their embeddings (Mikolov et al., 2013a; Chandar et al., 2014; Faruqui 3 This transformation is performed by simply replacing the space between the two tokens with an underscore. 490 and Dyer, 2014; Coulmance et al., 2015; Gouws et al., 2015; Duong et al., 2016) in combination with either a seed bilingual lexicon or a set of parallel sentence pairs. We estimate monolingual phrase embeddings via the element-wise addition of the word embeddings composing the phrase. This method performs well to estimate phrase embeddings (Mitchell and Lapata, 2010; Mikolov et al., 2013a), despite its simplicity and relatively low computational cost compared to state-of-the-art methods based on neural networks (Socher et al., 2013a; Socher et al., 2013b) or rich features (Lazaridou et al., 2015). This low computational cost is cr"
Q17-1034,P11-2071,0,0.0544768,"Missing"
Q17-1034,W14-3348,0,0.0287649,"model, we did not use the lexical reordering model throughout the experiments, while we enabled distance-based reordering up to six words. Our systems used the multiple decoding paths ability of Moses; we used up to three phrase tables in one system, as summarized in Table 5. We did not add the features presented in Section 3.2 to the phrase pairs directly derived from the parallel data.19 Weights of the features were optimized with kb-mira (Cherry and Foster, 2012) using 200-best hypotheses on 15 iterations. The translation outputs were evaluated with BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). The results were averaged over three tuning runs. The statistical significance was measured by approximate randomization (Clark et al., 2011) using MultEval.20 4.4 Additional baseline systems To compare our work with a state-of-the-art phrase table induction method, we implemented the work of Zhao et al. (2015). Even though they did not propose their method to perform domain adaptation of an SMT system, their work is the closest to ours and does not require other external resources than those we used, i.e., parallel data and monolingual data not necessarily comparable. We implemented both gl"
Q17-1034,D12-1025,0,0.0199514,", another possibility to find translations for OOV words is bilingual word lexicon induction using comparable or unaligned monolingual corpora (Fung, 1995; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013). This approach is especially useful in finding words and their translations specific to the given corpus. A recent and completely different trend of work uses an unsupervised method regarding translation as a decipherment problem to learn a bilingual word lexicon and use it as a translation model (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). However, all these methods deal only with 2 In machine translation (MT), words and phrases that do not appear in the training parallel data, i.e., out488 For instance, using these approaches on source and target monolingual data containing both 5 millions sentences means that we have to evaluate 25 × 1012 candidate sentence pairs. words, mainly owing to the computational complexity of dealing with arbitrary lengths of phrases. Translations of phrases can be induced using bilingual word lexicons and considering permutations of word ordering (Zhang and Zong, 2013; Irvine an"
Q17-1034,D16-1136,0,0.0250633,"h pair. This section describes several features to characterize each phrase pair; they are used for evaluating phrase pairs and also added in the induced phrase table to guide the decoder. 3.2.1 Cross-lingual semantic similarity Many researchers tackled the problem of estimating cross-lingual semantic similarity between pairs of words or phrases by using their embeddings (Mikolov et al., 2013a; Chandar et al., 2014; Faruqui 3 This transformation is performed by simply replacing the space between the two tokens with an underscore. 490 and Dyer, 2014; Coulmance et al., 2015; Gouws et al., 2015; Duong et al., 2016) in combination with either a seed bilingual lexicon or a set of parallel sentence pairs. We estimate monolingual phrase embeddings via the element-wise addition of the word embeddings composing the phrase. This method performs well to estimate phrase embeddings (Mitchell and Lapata, 2010; Mikolov et al., 2013a), despite its simplicity and relatively low computational cost compared to state-of-the-art methods based on neural networks (Socher et al., 2013a; Socher et al., 2013b) or rich features (Lazaridou et al., 2015). This low computational cost is crucial in our case, as we need to evaluate"
Q17-1034,E14-1049,0,0.0321268,"Missing"
Q17-1034,W04-3208,0,0.0520205,"cludes this work and proposes some possible improvements to our approach. 2 Motivation of-vocabulary (OOV) tokens, have been recognized as one of the fundamental issues, regardless of the scenario, such as adapting existing SMT systems to a new specific domain. One straightforward way to find translations of OOV words and phrases consists in enlarging the parallel data used to train the translation model. This can be done by retrieving parallel sentences from comparable corpora. However, these methods heavily rely on document-level information (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Fung and Cheung, 2004; Munteanu and Marcu, 2005) to reduce their search space by scoring only sentence pairs extracted from each pair of documents. Indeed, scoring all possible sentence pairs from two large monolingual corpora using costly features and a classifier, as proposed by Munteanu and Marcu (2005) for instance, is computationally too expensive.2 In many cases, we may not have access to document-level information in the given monolingual data for the targeted domain. Furthermore, even without considering computational cost, it is unlikely that a large number of parallel sentences can be retrieved from non-"
Q17-1034,W95-0114,0,0.474111,"ber of parallel sentences can be retrieved from non-comparable monolingual corpora. Hewavitharana and Vogel (2016) proposed to directly extract phrase pairs from comparable sentences. However, the number of retrievable phrase pairs is strongly limited, because one can collect such comparable sentences only on a relatively small scale for the targeted language pairs and domains. When in-domain parallel or comparable sentences can not be easily retrieved, another possibility to find translations for OOV words is bilingual word lexicon induction using comparable or unaligned monolingual corpora (Fung, 1995; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013). This approach is especially useful in finding words and their translations specific to the given corpus. A recent and completely different trend of work uses an unsupervised method regarding translation as a decipherment problem to learn a bilingual word lexicon and use it as a translation model (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). However, all these methods deal only with 2 In machine translation (MT), words and phrases that do not appear in"
Q17-1034,P08-1088,0,0.0446413,"non-comparable monolingual corpora. Hewavitharana and Vogel (2016) proposed to directly extract phrase pairs from comparable sentences. However, the number of retrievable phrase pairs is strongly limited, because one can collect such comparable sentences only on a relatively small scale for the targeted language pairs and domains. When in-domain parallel or comparable sentences can not be easily retrieved, another possibility to find translations for OOV words is bilingual word lexicon induction using comparable or unaligned monolingual corpora (Fung, 1995; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013). This approach is especially useful in finding words and their translations specific to the given corpus. A recent and completely different trend of work uses an unsupervised method regarding translation as a decipherment problem to learn a bilingual word lexicon and use it as a translation model (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). However, all these methods deal only with 2 In machine translation (MT), words and phrases that do not appear in the training parallel data, i.e., out488 For instance, usi"
Q17-1034,P13-2121,0,0.0173604,"://statmt.org/moses/, version 2.1.1 16 https://github.com/emjotde/symgiza-pp/ 17 The one exception is the system for the Science En→Fr task, which uses only two language models as we do not have any in-domain monolingual data in addition to the target side of the in-domain parallel data. 494 Phrase table Phrase table trained from general-domain parallel data Phrase table trained from in-domain parallel data In-domain bilingual lexicon Phrase table induced from in-domain monolingual data Conf. 1 Conf. 2 Conf. 3 √ √ √ √ √ √ √ √ Table 5: Multiple phrase table configurations. trained using lmplz (Heafield et al., 2013).18 To concentrate on the translation model, we did not use the lexical reordering model throughout the experiments, while we enabled distance-based reordering up to six words. Our systems used the multiple decoding paths ability of Moses; we used up to three phrase tables in one system, as summarized in Table 5. We did not add the features presented in Section 3.2 to the phrase pairs directly derived from the parallel data.19 Weights of the features were optimized with kb-mira (Cherry and Foster, 2012) using 200-best hypotheses on 15 iterations. The translation outputs were evaluated with BLE"
Q17-1034,N13-1056,0,0.0800995,"Vogel (2016) proposed to directly extract phrase pairs from comparable sentences. However, the number of retrievable phrase pairs is strongly limited, because one can collect such comparable sentences only on a relatively small scale for the targeted language pairs and domains. When in-domain parallel or comparable sentences can not be easily retrieved, another possibility to find translations for OOV words is bilingual word lexicon induction using comparable or unaligned monolingual corpora (Fung, 1995; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013). This approach is especially useful in finding words and their translations specific to the given corpus. A recent and completely different trend of work uses an unsupervised method regarding translation as a decipherment problem to learn a bilingual word lexicon and use it as a translation model (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). However, all these methods deal only with 2 In machine translation (MT), words and phrases that do not appear in the training parallel data, i.e., out488 For instance, using these approaches on source and target monolingual data contai"
Q17-1034,W14-1617,0,0.242065,"ght, 2012; Nuhn et al., 2012). However, all these methods deal only with 2 In machine translation (MT), words and phrases that do not appear in the training parallel data, i.e., out488 For instance, using these approaches on source and target monolingual data containing both 5 millions sentences means that we have to evaluate 25 × 1012 candidate sentence pairs. words, mainly owing to the computational complexity of dealing with arbitrary lengths of phrases. Translations of phrases can be induced using bilingual word lexicons and considering permutations of word ordering (Zhang and Zong, 2013; Irvine and Callison-Burch, 2014). However, it is costly to thoroughly investigate all combinations of a large number of word-level translation candidates and possible permutations of word ordering. To retain only appropriate phrase pairs, Irvine and Callison-Burch (2014) proposed to exploit a set of features. Some of them, including temporal, contextual, and topic similarity features, strongly relied on the comparability of Wikipedia articles and on the availability of news articles annotated with a timestamp (Klementiev et al., 2012). We may not have such useful resources in large quantity for the targeted language pairs an"
Q17-1034,Q13-1035,0,0.0284446,"Missing"
Q17-1034,E12-1014,0,0.0195555,"word lexicons and considering permutations of word ordering (Zhang and Zong, 2013; Irvine and Callison-Burch, 2014). However, it is costly to thoroughly investigate all combinations of a large number of word-level translation candidates and possible permutations of word ordering. To retain only appropriate phrase pairs, Irvine and Callison-Burch (2014) proposed to exploit a set of features. Some of them, including temporal, contextual, and topic similarity features, strongly relied on the comparability of Wikipedia articles and on the availability of news articles annotated with a timestamp (Klementiev et al., 2012). We may not have such useful resources in large quantity for the targeted language pairs and domains. Saluja et al. (2014) and Zhao et al. (2015) also proposed methods to induce a phrase table, focusing only on the OOV words and phrases: unigrams and bigrams in the source side of their development and test data that are unseen in the training data. In their approach, no new translation options are proposed for known source phrases. To generate candidate phrase pairs, for a given source phrase, Saluja et al. (2014) uses only phrases from the target side of their parallel data and their morphol"
Q17-1034,W02-0902,0,0.295991,"s can be retrieved from non-comparable monolingual corpora. Hewavitharana and Vogel (2016) proposed to directly extract phrase pairs from comparable sentences. However, the number of retrievable phrase pairs is strongly limited, because one can collect such comparable sentences only on a relatively small scale for the targeted language pairs and domains. When in-domain parallel or comparable sentences can not be easily retrieved, another possibility to find translations for OOV words is bilingual word lexicon induction using comparable or unaligned monolingual corpora (Fung, 1995; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013). This approach is especially useful in finding words and their translations specific to the given corpus. A recent and completely different trend of work uses an unsupervised method regarding translation as a decipherment problem to learn a bilingual word lexicon and use it as a translation model (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). However, all these methods deal only with 2 In machine translation (MT), words and phrases that do not appear in the training parallel data, i.e., o"
Q17-1034,P07-2045,0,0.00451671,"domain and translation direction, we have four word embedding spaces: those with 300 or 800 dimensions for source and target languages. The reliability of each phrase pair was estimated as described in Section 3.3 to compile phrase tables of reasonable size and quality. We used Vowpal Wabbit13 to perform logistic regression with one pass, default parameters, and --link logistic option to obtain a classification score for each phrase pair. In the final induced phrase table, we kept the 300 best target phrases14 for each source phrase according to this score. 4.3 SMT systems The Moses toolkit (Koehn et al., 2007)15 was used for training SMT models, parameter tuning, and decoding. The phrase tables were trained on the parallel corpus using SyMGIZA++ (Junczys-Dowmunt and Szał, 2012)16 with IBM-2 word alignment and the grow-diag-final-and heuristics. To obtain strong baseline systems, all SMT systems used three language models17 built on different sets of corpora as shown in Table 4; each language model is a 4-gram modified Kneser-Ney smoothed one 13 https://github.com/JohnLangford/ vowpal_wabbit/ 14 As in Irvine and Callison-Burch (2014), we obtained better results when favoring recall over precision. W"
Q17-1034,Q15-1014,0,0.0180977,"an underscore. 490 and Dyer, 2014; Coulmance et al., 2015; Gouws et al., 2015; Duong et al., 2016) in combination with either a seed bilingual lexicon or a set of parallel sentence pairs. We estimate monolingual phrase embeddings via the element-wise addition of the word embeddings composing the phrase. This method performs well to estimate phrase embeddings (Mitchell and Lapata, 2010; Mikolov et al., 2013a), despite its simplicity and relatively low computational cost compared to state-of-the-art methods based on neural networks (Socher et al., 2013a; Socher et al., 2013b) or rich features (Lazaridou et al., 2015). This low computational cost is crucial in our case, as we need to evaluate a large number of candidate phrase pairs. In order to make source and target phrase embeddings comparable, we perform a linear projection (Mikolov et al., 2013a) of the embeddings of source phrases to the target embedding space. To learn the projection, we use the method of Mikolov et al. (2013a) with the only exception that we deal with not only words but also phrases. Given training data, i.e., a gold bilingual lexicon, we obtain a ˆ by solving the following optitranslation matrix W mization problem with stochastic"
Q17-1034,J05-4003,0,0.0783957,"oposes some possible improvements to our approach. 2 Motivation of-vocabulary (OOV) tokens, have been recognized as one of the fundamental issues, regardless of the scenario, such as adapting existing SMT systems to a new specific domain. One straightforward way to find translations of OOV words and phrases consists in enlarging the parallel data used to train the translation model. This can be done by retrieving parallel sentences from comparable corpora. However, these methods heavily rely on document-level information (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Fung and Cheung, 2004; Munteanu and Marcu, 2005) to reduce their search space by scoring only sentence pairs extracted from each pair of documents. Indeed, scoring all possible sentence pairs from two large monolingual corpora using costly features and a classifier, as proposed by Munteanu and Marcu (2005) for instance, is computationally too expensive.2 In many cases, we may not have access to document-level information in the given monolingual data for the targeted domain. Furthermore, even without considering computational cost, it is unlikely that a large number of parallel sentences can be retrieved from non-comparable monolingual corp"
Q17-1034,P12-1017,0,0.0184944,"to find translations for OOV words is bilingual word lexicon induction using comparable or unaligned monolingual corpora (Fung, 1995; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013). This approach is especially useful in finding words and their translations specific to the given corpus. A recent and completely different trend of work uses an unsupervised method regarding translation as a decipherment problem to learn a bilingual word lexicon and use it as a translation model (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). However, all these methods deal only with 2 In machine translation (MT), words and phrases that do not appear in the training parallel data, i.e., out488 For instance, using these approaches on source and target monolingual data containing both 5 millions sentences means that we have to evaluate 25 × 1012 candidate sentence pairs. words, mainly owing to the computational complexity of dealing with arbitrary lengths of phrases. Translations of phrases can be induced using bilingual word lexicons and considering permutations of word ordering (Zhang and Zong, 2013; Irvine and Callison-Burch, 20"
Q17-1034,P02-1040,0,0.103346,"To concentrate on the translation model, we did not use the lexical reordering model throughout the experiments, while we enabled distance-based reordering up to six words. Our systems used the multiple decoding paths ability of Moses; we used up to three phrase tables in one system, as summarized in Table 5. We did not add the features presented in Section 3.2 to the phrase pairs directly derived from the parallel data.19 Weights of the features were optimized with kb-mira (Cherry and Foster, 2012) using 200-best hypotheses on 15 iterations. The translation outputs were evaluated with BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). The results were averaged over three tuning runs. The statistical significance was measured by approximate randomization (Clark et al., 2011) using MultEval.20 4.4 Additional baseline systems To compare our work with a state-of-the-art phrase table induction method, we implemented the work of Zhao et al. (2015). Even though they did not propose their method to perform domain adaptation of an SMT system, their work is the closest to ours and does not require other external resources than those we used, i.e., parallel data and monolingual data not necessa"
Q17-1034,P95-1050,0,0.380348,"lel sentences can be retrieved from non-comparable monolingual corpora. Hewavitharana and Vogel (2016) proposed to directly extract phrase pairs from comparable sentences. However, the number of retrievable phrase pairs is strongly limited, because one can collect such comparable sentences only on a relatively small scale for the targeted language pairs and domains. When in-domain parallel or comparable sentences can not be easily retrieved, another possibility to find translations for OOV words is bilingual word lexicon induction using comparable or unaligned monolingual corpora (Fung, 1995; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013). This approach is especially useful in finding words and their translations specific to the given corpus. A recent and completely different trend of work uses an unsupervised method regarding translation as a decipherment problem to learn a bilingual word lexicon and use it as a translation model (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). However, all these methods deal only with 2 In machine translation (MT), words and phrases that do not appear in the trainin"
Q17-1034,P11-1002,0,0.0207642,"not be easily retrieved, another possibility to find translations for OOV words is bilingual word lexicon induction using comparable or unaligned monolingual corpora (Fung, 1995; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013). This approach is especially useful in finding words and their translations specific to the given corpus. A recent and completely different trend of work uses an unsupervised method regarding translation as a decipherment problem to learn a bilingual word lexicon and use it as a translation model (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). However, all these methods deal only with 2 In machine translation (MT), words and phrases that do not appear in the training parallel data, i.e., out488 For instance, using these approaches on source and target monolingual data containing both 5 millions sentences means that we have to evaluate 25 × 1012 candidate sentence pairs. words, mainly owing to the computational complexity of dealing with arbitrary lengths of phrases. Translations of phrases can be induced using bilingual word lexicons and considering permutations of word ordering (Zhang and"
Q17-1034,P14-1064,0,0.0747613,"it is costly to thoroughly investigate all combinations of a large number of word-level translation candidates and possible permutations of word ordering. To retain only appropriate phrase pairs, Irvine and Callison-Burch (2014) proposed to exploit a set of features. Some of them, including temporal, contextual, and topic similarity features, strongly relied on the comparability of Wikipedia articles and on the availability of news articles annotated with a timestamp (Klementiev et al., 2012). We may not have such useful resources in large quantity for the targeted language pairs and domains. Saluja et al. (2014) and Zhao et al. (2015) also proposed methods to induce a phrase table, focusing only on the OOV words and phrases: unigrams and bigrams in the source side of their development and test data that are unseen in the training data. In their approach, no new translation options are proposed for known source phrases. To generate candidate phrase pairs, for a given source phrase, Saluja et al. (2014) uses only phrases from the target side of their parallel data and their morphological variants ranked and pruned according to the forward lexical translation probabilities given by their baseline system"
Q17-1034,P13-1045,0,0.0393756,"rmed by simply replacing the space between the two tokens with an underscore. 490 and Dyer, 2014; Coulmance et al., 2015; Gouws et al., 2015; Duong et al., 2016) in combination with either a seed bilingual lexicon or a set of parallel sentence pairs. We estimate monolingual phrase embeddings via the element-wise addition of the word embeddings composing the phrase. This method performs well to estimate phrase embeddings (Mitchell and Lapata, 2010; Mikolov et al., 2013a), despite its simplicity and relatively low computational cost compared to state-of-the-art methods based on neural networks (Socher et al., 2013a; Socher et al., 2013b) or rich features (Lazaridou et al., 2015). This low computational cost is crucial in our case, as we need to evaluate a large number of candidate phrase pairs. In order to make source and target phrase embeddings comparable, we perform a linear projection (Mikolov et al., 2013a) of the embeddings of source phrases to the target embedding space. To learn the projection, we use the method of Mikolov et al. (2013a) with the only exception that we deal with not only words but also phrases. Given training data, i.e., a gold bilingual lexicon, we obtain a ˆ by solving the fo"
Q17-1034,D13-1170,0,0.00276916,"rmed by simply replacing the space between the two tokens with an underscore. 490 and Dyer, 2014; Coulmance et al., 2015; Gouws et al., 2015; Duong et al., 2016) in combination with either a seed bilingual lexicon or a set of parallel sentence pairs. We estimate monolingual phrase embeddings via the element-wise addition of the word embeddings composing the phrase. This method performs well to estimate phrase embeddings (Mitchell and Lapata, 2010; Mikolov et al., 2013a), despite its simplicity and relatively low computational cost compared to state-of-the-art methods based on neural networks (Socher et al., 2013a; Socher et al., 2013b) or rich features (Lazaridou et al., 2015). This low computational cost is crucial in our case, as we need to evaluate a large number of candidate phrase pairs. In order to make source and target phrase embeddings comparable, we perform a linear projection (Mikolov et al., 2013a) of the embeddings of source phrases to the target embedding space. To learn the projection, we use the method of Mikolov et al. (2013a) with the only exception that we deal with not only words but also phrases. Given training data, i.e., a gold bilingual lexicon, we obtain a ˆ by solving the fo"
Q17-1034,P03-1010,0,0.0598005,"5.2. Finally, Section 6 concludes this work and proposes some possible improvements to our approach. 2 Motivation of-vocabulary (OOV) tokens, have been recognized as one of the fundamental issues, regardless of the scenario, such as adapting existing SMT systems to a new specific domain. One straightforward way to find translations of OOV words and phrases consists in enlarging the parallel data used to train the translation model. This can be done by retrieving parallel sentences from comparable corpora. However, these methods heavily rely on document-level information (Zhao and Vogel, 2002; Utiyama and Isahara, 2003; Fung and Cheung, 2004; Munteanu and Marcu, 2005) to reduce their search space by scoring only sentence pairs extracted from each pair of documents. Indeed, scoring all possible sentence pairs from two large monolingual corpora using costly features and a classifier, as proposed by Munteanu and Marcu (2005) for instance, is computationally too expensive.2 In many cases, we may not have access to document-level information in the given monolingual data for the targeted domain. Furthermore, even without considering computational cost, it is unlikely that a large number of parallel sentences can"
Q17-1034,P16-1024,0,0.0298283,"Missing"
Q17-1034,D10-1091,0,0.0329753,"Missing"
Q17-1034,P13-1140,0,0.0172851,"ght, 2011; Dou and Knight, 2012; Nuhn et al., 2012). However, all these methods deal only with 2 In machine translation (MT), words and phrases that do not appear in the training parallel data, i.e., out488 For instance, using these approaches on source and target monolingual data containing both 5 millions sentences means that we have to evaluate 25 × 1012 candidate sentence pairs. words, mainly owing to the computational complexity of dealing with arbitrary lengths of phrases. Translations of phrases can be induced using bilingual word lexicons and considering permutations of word ordering (Zhang and Zong, 2013; Irvine and Callison-Burch, 2014). However, it is costly to thoroughly investigate all combinations of a large number of word-level translation candidates and possible permutations of word ordering. To retain only appropriate phrase pairs, Irvine and Callison-Burch (2014) proposed to exploit a set of features. Some of them, including temporal, contextual, and topic similarity features, strongly relied on the comparability of Wikipedia articles and on the availability of news articles annotated with a timestamp (Klementiev et al., 2012). We may not have such useful resources in large quantity"
Q17-1034,N15-1176,0,0.102753,"y investigate all combinations of a large number of word-level translation candidates and possible permutations of word ordering. To retain only appropriate phrase pairs, Irvine and Callison-Burch (2014) proposed to exploit a set of features. Some of them, including temporal, contextual, and topic similarity features, strongly relied on the comparability of Wikipedia articles and on the availability of news articles annotated with a timestamp (Klementiev et al., 2012). We may not have such useful resources in large quantity for the targeted language pairs and domains. Saluja et al. (2014) and Zhao et al. (2015) also proposed methods to induce a phrase table, focusing only on the OOV words and phrases: unigrams and bigrams in the source side of their development and test data that are unseen in the training data. In their approach, no new translation options are proposed for known source phrases. To generate candidate phrase pairs, for a given source phrase, Saluja et al. (2014) uses only phrases from the target side of their parallel data and their morphological variants ranked and pruned according to the forward lexical translation probabilities given by their baseline system’s translation model. T"
Q17-1034,D11-1033,0,\N,Missing
Q17-1034,D14-1131,0,\N,Missing
W03-1602,P01-1008,0,0.0389931,"To create such a system, one needs to feed it with a large collection of paraphrase patterns. Very timely, the acquisition of paraphrase patterns has been actively studied in recent years:  Manual collection of paraphrases in the context of language generation, e.g. (Robin and McKeown, 1996),  Derivation of paraphrases through existing lexical resources, e.g. (Kurohashi et al., 1999),  Corpus-based statistical methods inspired by the work on information extraction, e.g. (Jacquemin, 1999; Lin and Pantel, 2001), and  Alignment-based acquisition of paraphrases from comparable corpora, e.g. (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). One remaining issue is how effectively these methods contribute to the generation of paraphrases in our application-oriented context. 2.3 Paraphrase representation One of the findings obtained in the previous studies for paraphrase acquisition is that the automatic acquisition of candidates of paraphrases is quite realizable for various types of source data but acquired collections tend to be rather noisy and need manual cleaning as reported in, for example, (Lin and Pantel, 2001). Given that, it turns out to be important to devise an effective"
W03-1602,N03-1003,0,0.0148725,"a large collection of paraphrase patterns. Very timely, the acquisition of paraphrase patterns has been actively studied in recent years:  Manual collection of paraphrases in the context of language generation, e.g. (Robin and McKeown, 1996),  Derivation of paraphrases through existing lexical resources, e.g. (Kurohashi et al., 1999),  Corpus-based statistical methods inspired by the work on information extraction, e.g. (Jacquemin, 1999; Lin and Pantel, 2001), and  Alignment-based acquisition of paraphrases from comparable corpora, e.g. (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). One remaining issue is how effectively these methods contribute to the generation of paraphrases in our application-oriented context. 2.3 Paraphrase representation One of the findings obtained in the previous studies for paraphrase acquisition is that the automatic acquisition of candidates of paraphrases is quite realizable for various types of source data but acquired collections tend to be rather noisy and need manual cleaning as reported in, for example, (Lin and Pantel, 2001). Given that, it turns out to be important to devise an effective way of facilitating manual correction and a sta"
W03-1602,P98-1056,0,0.15982,"lly expressible formalism for representing paraphrases at the level of tree-to-tree transformation and (b) devise an additional layer of representation on its top that is designed to facilitate handcoding transformation rules. 2.4 Post-transfer text revision In paraphrasing, the morpho-syntactic information of a source sentence should be accessible throughout the transfer process since a morphosyntactic transformation in itself can often be a motivation or goal of paraphrasing. Therefore, such an approach as semantic transfer, where morphosyntactic information is highly abstracted away as in (Dorna et al., 1998; Richardson et al., 2001), does not suit this task. Provided that the morphosyntactic stratum be an optimal level of abstraction for representing paraphrasing/transfer patterns, one must recall that semantic-transfer approaches such as those cited above were motivated mainly by the need for reducing the complexity of transfer knowledge, which could be unmanageable in morpho-syntactic transfer. Our approach to this problem is to (a) leave the description of each transfer pattern underspecified and (b) implement the knowledge about linguistic constraints that are independent of a particular tra"
W03-1602,W01-0814,1,0.810902,"Missing"
W03-1602,P99-1044,0,0.0104458,"ding assistance system is, therefore, hoped to be able to generate sufficient varieties of paraphrases of a given input. To create such a system, one needs to feed it with a large collection of paraphrase patterns. Very timely, the acquisition of paraphrase patterns has been actively studied in recent years:  Manual collection of paraphrases in the context of language generation, e.g. (Robin and McKeown, 1996),  Derivation of paraphrases through existing lexical resources, e.g. (Kurohashi et al., 1999),  Corpus-based statistical methods inspired by the work on information extraction, e.g. (Jacquemin, 1999; Lin and Pantel, 2001), and  Alignment-based acquisition of paraphrases from comparable corpora, e.g. (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). One remaining issue is how effectively these methods contribute to the generation of paraphrases in our application-oriented context. 2.3 Paraphrase representation One of the findings obtained in the previous studies for paraphrase acquisition is that the automatic acquisition of candidates of paraphrases is quite realizable for various types of source data but acquired collections tend to be rather noisy and need m"
W03-1602,P99-1062,0,0.025313,"Missing"
W03-1602,A00-1009,0,0.0218102,"Meaning-preserving vs. reference-preserving paraphrases It is also useful to distinguish reference-preserving paraphrases from meaningpreserving ones. The above example in (3) is of the reference-preserving type. This types of paraphrasing requires the computation of reference to objects outside discourse and thus should be excluded from our scope for the present purpose. 4.2 Dependency trees (MDSs) Previous work on transfer-based machine translation (MT) suggests that the dependency-based representation has the advantage of facilitating syntactic transforming operations (Meyers et al., 1996; Lavoie et al., 2000). Following this, we adopt dependency trees as the internal representations of target texts. We suppose that a dependency tree consists of a set of nodes each of which corresponds to a lexeme or compound and a set of edges each of which represents the dependency relation between its ends. We call such a dependency tree a morpheme-based dependency structure (MDS). Each node in an MDS is supposed to be annotated with an open set of typed features that indicate morpho-syntactic and semantic information. We also assume a type hierarchy in dependency relations that consists of an open set of depend"
W03-1602,W97-0508,0,0.0659332,"Missing"
W03-1602,C96-1078,0,0.0432722,"cope our discussion. Meaning-preserving vs. reference-preserving paraphrases It is also useful to distinguish reference-preserving paraphrases from meaningpreserving ones. The above example in (3) is of the reference-preserving type. This types of paraphrasing requires the computation of reference to objects outside discourse and thus should be excluded from our scope for the present purpose. 4.2 Dependency trees (MDSs) Previous work on transfer-based machine translation (MT) suggests that the dependency-based representation has the advantage of facilitating syntactic transforming operations (Meyers et al., 1996; Lavoie et al., 2000). Following this, we adopt dependency trees as the internal representations of target texts. We suppose that a dependency tree consists of a set of nodes each of which corresponds to a lexeme or compound and a set of edges each of which represents the dependency relation between its ends. We call such a dependency tree a morpheme-based dependency structure (MDS). Each node in an MDS is supposed to be annotated with an open set of typed features that indicate morpho-syntactic and semantic information. We also assume a type hierarchy in dependency relations that consists of"
W03-1602,P01-1051,0,0.0130024,"evision would make the system tolerant to flows in paraphrasing rules. While many researchers have addressed the issue of paraphrase acquisition reporting promising results as cited above, the other three issues have been left relatively unexplored in spite of their significance in the above sense. Motivated by this context, in the rest of this paper, we address these remaining three. 3 Readability assessment To the best of our knowledge, there have never been no reports on research to build a computational model of the language proficiency of deaf people, except for the remarkable reports by Michaud and McCoy (2001). As a subpart of their research aimed at developing the ICICLE system (McCoy and Masterman, 1997), a language-tutoring application for deaf learners of written English, Michaud and McCoy developed an architecture for modeling the writing proficiency of a user called SLALOM. SLALOM is designed to capture the stereotypic linear order of acquisition within certain categories of morphological and/or syntactic features of language. Unfortunately, the modeling method used in SLALOM cannot be directly applied to our domain for three reasons.  Unlike writing tutoring, in reading assistance, target s"
W03-1602,W03-2314,0,0.0708175,"Missing"
W03-1602,W03-2317,0,0.0406903,"ree subprocesses: a. Problem identification: identify which portions of a given text will be difficult for a given user to read, b. Paraphrase generation: generate possible candidate paraphrases from the identified portions, and c. Evaluation: re-assess the resultant texts to choose the one in which the problems have been resolved. Given this decomposition, it is clear that one of the key issues in reading assistance is the problem of assessing the readability or comprehensibility1 of text because it is involved in subprocesses (a) and (c). Readability assessment is doubtlessly a tough issue (Williams et al., 2003). In this project, however, we argue that, if one targets only a particular population segment and if an adequate collection of data is available, then corpus-based empirical approaches may well be feasible. We have already proven that one can collect such readability assessment data by conducting survey questionnaires targeting teachers at schools for the deaf. 1 In this paper, we use the terms readability and comprehensibility interchangeably, while strictly distinguishing them from legibility of each fragment (typically, a sentence or paragraph) of a given text. 2.2 Paraphrase acquisition O"
W03-1602,W01-1402,0,\N,Missing
W03-1602,C98-1054,0,\N,Missing
W04-0402,P01-1008,0,0.0672653,"as those indicating that “to make an attempt” can be paraphrased into “to attempt,” and that “potential” can be paraphrased into “possibility.” Several attempts have been made to develop such resources manually (Sato, 1999; Dras, 1999; Inui and Nogami, 2001); those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct a sufficiently comprehensive resource for practical applications. There is another trend in the research in this field, namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Pang et al., 2003; Shinyama and Sekine, 2003, etc.). This type of approach may be able to reduce the cost of resource development. There are problems that must be overcome, however, before they can work practically. First, automatically acquired patterns tend to be complex. For example, from the paraphrase of (4s) into (4t), we can naively obtain the pattern: “X is purchased by Y ⇒ Y buys X.” (4) s. This car was purchased by him. t. He bought this car. This could also, however, be regarded as a combination of a simpler pattern of lexical paraphrasing (“purchase ⇒ buy”)"
W04-0402,E99-1042,0,0.0232588,"ing LCS dictionary. Experimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases. 1 Introduction Automatic paraphrase generation technology offers the potential to bridge gaps between the authors and readers of documents. For example, a system that is capable of simplifying a given text, or showing the user several alternative expressions conveying the same content, would be useful for assisting a reader (Carroll et al., 1999; Inui et al., 2003). In Japanese, like other languages, there are several classes of paraphrasing that exhibit a degree of regularity that allows them to be explained by a handful of sophisticated general rules and lexical semantic knowledge. For example, paraphrases associated with voice alteration, verb/case alteration, compounds, and lexical derivations all fall into such classes. In this paper, we focus our discussion on another useful class of paraphrases, namely, the paraphrasing of light-verb constructions (LVCs), and propose a computational model for generating paraphrases of this cla"
W04-0402,W01-0814,1,0.592452,"iments (Section 5). Finally, we conclude this paper with a brief of description of work to be done in the future (Section 6). 2 Motivation, target, and related work 2.1 Motivation One of the critical issues that we face in paraphrase generation is how to develop and maintain knowledge resources that covers a sufficiently wide range of paraphrasing patterns such as those indicating that “to make an attempt” can be paraphrased into “to attempt,” and that “potential” can be paraphrased into “possibility.” Several attempts have been made to develop such resources manually (Sato, 1999; Dras, 1999; Inui and Nogami, 2001); those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct a sufficiently comprehensive resource for practical applications. There is another trend in the research in this field, namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Pang et al., 2003; Shinyama and Sekine, 2003, etc.). This type of approach may be able to reduce the cost of resource development. There are problems that must be overcome, however, before they can work practi"
W04-0402,W03-1602,1,0.844244,"perimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases. 1 Introduction Automatic paraphrase generation technology offers the potential to bridge gaps between the authors and readers of documents. For example, a system that is capable of simplifying a given text, or showing the user several alternative expressions conveying the same content, would be useful for assisting a reader (Carroll et al., 1999; Inui et al., 2003). In Japanese, like other languages, there are several classes of paraphrasing that exhibit a degree of regularity that allows them to be explained by a handful of sophisticated general rules and lexical semantic knowledge. For example, paraphrases associated with voice alteration, verb/case alteration, compounds, and lexical derivations all fall into such classes. In this paper, we focus our discussion on another useful class of paraphrases, namely, the paraphrasing of light-verb constructions (LVCs), and propose a computational model for generating paraphrases of this class. Sentence (1s) is"
W04-0402,W02-2016,1,0.676943,"alized verbs. We retrieved 1,210 nominalized verbs from the TLCS dictionary. Light-verbs: Since a verb takes different meanings when it is a part of LVCs with different case particles, we collected pairs c, v of case particle c and verb v in the following way: Step 1. We collected 876,101 types of triplets n, c, v of nominalized verb n, case particle c, and base form of verb v from the parsed5 sentences of newspaper articles6 . 4 A sahen-noun is a verbal noun in Japanese, which acts as a verb in the form of “sahen-noun + suru”. 5 We used the statistical Japanese dependency parser CaboCha (Kudo and Matsumoto, 2002) for parsing. http://chasen.naist.jp/˜taku/software/cabocha/ 6 Excerpts from 9 years of the Mainichi Shinbun and 10 years of the Nihon Keizai Shinbun, giving a total of 25,061,504 sentences, were used. Table 2: Extensions of LCS Ext.1 Verb hankou-suru (resist) Ext.2 ukeru (receive) Ext.3 motomeru (ask) Ext.4 kandou-suru (be impressed) Verb phrase and its LCS representation [[Ken]y BE AGAINST [parents]z] Ken-ga oya-ni hankou-suru. Ken-NOM parents-DAT resist- PRES (Ken resists his parents.) [BECOME [[salesclerk]z BE WITH [[complaint]y MOVE FROM [customer]x TO [salesclerk]z]]] Ten’in-ga kyaku-kar"
W04-0402,J87-3006,0,0.0613912,"Missing"
W04-0402,N03-1024,0,0.0405069,"be paraphrased into “to attempt,” and that “potential” can be paraphrased into “possibility.” Several attempts have been made to develop such resources manually (Sato, 1999; Dras, 1999; Inui and Nogami, 2001); those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct a sufficiently comprehensive resource for practical applications. There is another trend in the research in this field, namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Pang et al., 2003; Shinyama and Sekine, 2003, etc.). This type of approach may be able to reduce the cost of resource development. There are problems that must be overcome, however, before they can work practically. First, automatically acquired patterns tend to be complex. For example, from the paraphrase of (4s) into (4t), we can naively obtain the pattern: “X is purchased by Y ⇒ Y buys X.” (4) s. This car was purchased by him. t. He bought this car. This could also, however, be regarded as a combination of a simpler pattern of lexical paraphrasing (“purchase ⇒ buy”) and a voice activization (“X (b) Noun + C"
W04-0402,W03-1609,0,0.0123349,"o “to attempt,” and that “potential” can be paraphrased into “possibility.” Several attempts have been made to develop such resources manually (Sato, 1999; Dras, 1999; Inui and Nogami, 2001); those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct a sufficiently comprehensive resource for practical applications. There is another trend in the research in this field, namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Pang et al., 2003; Shinyama and Sekine, 2003, etc.). This type of approach may be able to reduce the cost of resource development. There are problems that must be overcome, however, before they can work practically. First, automatically acquired patterns tend to be complex. For example, from the paraphrase of (4s) into (4t), we can naively obtain the pattern: “X is purchased by Y ⇒ Y buys X.” (4) s. This car was purchased by him. t. He bought this car. This could also, however, be regarded as a combination of a simpler pattern of lexical paraphrasing (“purchase ⇒ buy”) and a voice activization (“X (b) Noun + Case Particle (c) Noun + Cas"
W06-1407,N03-1013,0,0.0475382,"Missing"
W06-1407,J87-3006,0,0.168148,"Missing"
W06-1407,2003.mtsummit-systems.9,0,\N,Missing
W07-1425,N03-1003,0,0.138996,"between two expressions are five-fold. The first approximates it based on the similarities between their constituent words. If two words belong to closer nodes in a thesaurus or semantic network, they are considered more likely to be similar. The second uses the family of tree kernels (Collins and Duffy, 2001; Takahashi, 2005). The degree of equivalence of two trees (sentences) is defined as the number of common subtrees included in both trees. The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text (Barzilay and Lee, 2003; Brockett and Dolan, 2005). The fourth espouses the distributional hypothesis (Harris, 1968): given two words are likely to be equivalent if distributions of their surrounding words are similar (Lin and Pantel, 2001; Weeds et al., 2005). The final regards two expressions equivalent if they can be associated by using a set of lexicosyntactic paraphrase patterns (Mel’ˇcuk, 1996; Dras, 1999; Yoshikane et al., 1999; Takahashi, 2005). Despite the results previous work has achieved, no system that robustly recognizes and generates paraphrases is established. We are not convinced of a hypothesis und"
W07-1425,I05-5001,0,0.0564217,"are five-fold. The first approximates it based on the similarities between their constituent words. If two words belong to closer nodes in a thesaurus or semantic network, they are considered more likely to be similar. The second uses the family of tree kernels (Collins and Duffy, 2001; Takahashi, 2005). The degree of equivalence of two trees (sentences) is defined as the number of common subtrees included in both trees. The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text (Barzilay and Lee, 2003; Brockett and Dolan, 2005). The fourth espouses the distributional hypothesis (Harris, 1968): given two words are likely to be equivalent if distributions of their surrounding words are similar (Lin and Pantel, 2001; Weeds et al., 2005). The final regards two expressions equivalent if they can be associated by using a set of lexicosyntactic paraphrase patterns (Mel’ˇcuk, 1996; Dras, 1999; Yoshikane et al., 1999; Takahashi, 2005). Despite the results previous work has achieved, no system that robustly recognizes and generates paraphrases is established. We are not convinced of a hypothesis underlying the word-based appr"
W07-1425,W04-3205,0,0.024333,"by discussion (Section 5), and conclusion (Section 6). 2 Dynamic phrasal thesaurus 2.1 Issue Toward realizing phrasal thesaurus, the following two issues should be discussed. • What sorts of phrases should be treated • How to cope with a variety of expressions Although technologies of shallow parsing have been dramatically improved in the last decade, it is still difficult to represent arbitrary expression in logical form. We therefore think it is reasonable to define the range relying on lexico-syntactic structure instead of using particular semantic representation. According to the work of (Chklovski and Pantel, 2004; Torisawa, 2006), predicate phrase (simple sentence) is a reasonable unit because it approximately corresponds to the meaning of single event. Combination of words and a variety of construction coerce us into handling an enormous number of expressions than word-based approaches. One may think taking phrase is like treading a thorny path because one of the arguments in Section 1 is 152 about coverage. On this issue, we speculate that one of the feasible approach to realize a robust system is to divide phenomena into compositional and non-compositional (idiosyncratic) ones1 , and separately dev"
W07-1425,I05-1079,1,0.925612,"seems tolerable approach to generate them dynamically by applying such rules. Our work is targeted at compositional paraphrases and the system can be called dynamic phrasal thesaurus. Hereafter, we refer to paraphrases that are likely to be explained compositionally as syntactic variants. 2.2 Target language: Japanese While the discussion above does not depend on particular language, our implementation of dynamic phrasal thesaurus is targeted at Japanese. Several methods for paraphrasing Japanese predicate phrases have been proposed (Kondo et al., 1999; Kondo et al., 2001; Kaji et al., 2002; Fujita et al., 2005). The range they treat is, however, relatively narrow because they tend to focus on particular paraphrase phenomena or to rely on existing resources. On the other hand, we define the range of phenomena from a top-down viewpoint. As a concrete definition of predicate phrase in Japanese, noun phrase + case marker + predicate is employed which is hereafter referred to “phrase.” Noun phrase and predicate in Japanese themselves subcategorize various syntactic variants as shown in Figure 1 and paraphrase phenomena for above phrase also involve those focused on their interaction. Thus the range of ph"
W07-1425,I05-5004,1,0.76295,"tence) is a reasonable unit because it approximately corresponds to the meaning of single event. Combination of words and a variety of construction coerce us into handling an enormous number of expressions than word-based approaches. One may think taking phrase is like treading a thorny path because one of the arguments in Section 1 is 152 about coverage. On this issue, we speculate that one of the feasible approach to realize a robust system is to divide phenomena into compositional and non-compositional (idiosyncratic) ones1 , and separately develop resources to handle them as described in (Fujita and Inui, 2005). To compute semantic equivalence of idiosyncratic paraphrases, pairs or groups of paraphrases have to be statically compiled into a dictionary as wordbased thesaurus. The corpus-based approach is valuable for that purpose, although they are not guaranteed to collect all idiosyncratic paraphrases. On the other hand, compositional paraphrases can be captured by a relatively small number of rules. Thus it seems tolerable approach to generate them dynamically by applying such rules. Our work is targeted at compositional paraphrases and the system can be called dynamic phrasal thesaurus. Hereafter"
W07-1425,P02-1028,1,0.928019,"r of rules. Thus it seems tolerable approach to generate them dynamically by applying such rules. Our work is targeted at compositional paraphrases and the system can be called dynamic phrasal thesaurus. Hereafter, we refer to paraphrases that are likely to be explained compositionally as syntactic variants. 2.2 Target language: Japanese While the discussion above does not depend on particular language, our implementation of dynamic phrasal thesaurus is targeted at Japanese. Several methods for paraphrasing Japanese predicate phrases have been proposed (Kondo et al., 1999; Kondo et al., 2001; Kaji et al., 2002; Fujita et al., 2005). The range they treat is, however, relatively narrow because they tend to focus on particular paraphrase phenomena or to rely on existing resources. On the other hand, we define the range of phenomena from a top-down viewpoint. As a concrete definition of predicate phrase in Japanese, noun phrase + case marker + predicate is employed which is hereafter referred to “phrase.” Noun phrase and predicate in Japanese themselves subcategorize various syntactic variants as shown in Figure 1 and paraphrase phenomena for above phrase also involve those focused on their interaction"
W07-1425,P98-1116,0,0.0157992,"ive–noun derivation. Using this clue, candidates are collected by two methods. • From dictionary: Retrieve all word pairs from the given set of words those satisfying the following four conditions: (i) beginning with kanji character, (ii) having different POSs, (iii) sharing at least the first character and the first sound, and (iv) having a suffix pattern which corresponds to at least two pairs. • Using dictionary and corpus: Generate candidates from a set of words by applying a set of typical suffix patterns, and then check if each candidate is an actual word using corpus. This is based on (Langkilde and Knight, 1998). Step 2. Manual selection: The set of word pairs collected in the previous step includes those do not have particular semantic relationship. This step involves human to discard noises. Table 4 shows the size of 10 dictionaries, where each column denotes the number of word pairs retrieved from IPADIC5 (|D|), those using IPADIC, seven patterns and the same corpus as in Section 3.4 (|C|), their union (|D ∪ C|), and those manually judged correct (|J|), respectively. The sets of word pairs J are used as bi-directional lexical functions, although manual screening for four dictionaries without dagge"
W07-1425,N07-1071,0,0.064312,"possibility that data has been biased. http://mecab.sourceforge.jp/ 157 Sufficient condition of equivalence In our system, transformation patterns and generation functions offer necessary conditions for generating syntactic variants for given input. However, we have no sufficient condition to control the application of such a knowledge. It has not been thoroughly clarified what clue can be sufficient condition to ensure semantic equivalence, even in a number of previous work. Though, at least, roles of participants in the event have to be preserved by some means, such as the way presented in (Pantel et al., 2007). Kaji et al. (2002) introduced a method of case frame alignment in paraphrase generation. In the model, arguments of main verb in the source are taken over by that of the target according to the similarities between arguments of the source and target. Fujita et al. (2005) employed a semantic representation of verb to realize the alignment of the role of participants governed by the source and target verbs. According to an empirical experiment in (Fujita et al., 2005), statistical language models do not contribute to calculating semantic equivalence, but to filtering out anomalies. We therefor"
W07-1425,N06-1008,0,0.0558059,"nd conclusion (Section 6). 2 Dynamic phrasal thesaurus 2.1 Issue Toward realizing phrasal thesaurus, the following two issues should be discussed. • What sorts of phrases should be treated • How to cope with a variety of expressions Although technologies of shallow parsing have been dramatically improved in the last decade, it is still difficult to represent arbitrary expression in logical form. We therefore think it is reasonable to define the range relying on lexico-syntactic structure instead of using particular semantic representation. According to the work of (Chklovski and Pantel, 2004; Torisawa, 2006), predicate phrase (simple sentence) is a reasonable unit because it approximately corresponds to the meaning of single event. Combination of words and a variety of construction coerce us into handling an enormous number of expressions than word-based approaches. One may think taking phrase is like treading a thorny path because one of the arguments in Section 1 is 152 about coverage. On this issue, we speculate that one of the feasible approach to realize a robust system is to divide phenomena into compositional and non-compositional (idiosyncratic) ones1 , and separately develop resources to"
W07-1425,W05-1202,0,0.152673,"r. The second uses the family of tree kernels (Collins and Duffy, 2001; Takahashi, 2005). The degree of equivalence of two trees (sentences) is defined as the number of common subtrees included in both trees. The third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text (Barzilay and Lee, 2003; Brockett and Dolan, 2005). The fourth espouses the distributional hypothesis (Harris, 1968): given two words are likely to be equivalent if distributions of their surrounding words are similar (Lin and Pantel, 2001; Weeds et al., 2005). The final regards two expressions equivalent if they can be associated by using a set of lexicosyntactic paraphrase patterns (Mel’ˇcuk, 1996; Dras, 1999; Yoshikane et al., 1999; Takahashi, 2005). Despite the results previous work has achieved, no system that robustly recognizes and generates paraphrases is established. We are not convinced of a hypothesis underlying the word-based approaches because the structure of words also conveys some meaning. Even tree kernels, which take structures into account, do not have a mechanism for identifying typical equivalents: e.g., dative alternation and"
W07-1425,C98-1112,0,\N,Missing
W10-3201,P98-1013,0,0.101887,"” can be described? The shared meaning of lend and give in the above sentences is that they are categorized to Giving Verbs, as in Levin’s English Verb Classes and Alternations (EVCA) (Levin, 1993), while the different meaning will be that lend does not imply ownership of the theme, i.e., a bicycle. One of the problematic issues with describing shared meaning among verbs is that semantic classes such as Giving Verbs should be dependent on the granularity of meanings we assumed. For example, the meaning of lend and give in the above sentences is not categorized into the same Frame in FrameNet (Baker et al., 1998). The reason for this different categorization can be considered to be that the granularity of the semantic class of Giving Verbs is larger than that of the Giving Frame in FrameNet1 . From the view of natural language processing, especially dealing the with propositional meaning of verbs, all of the above classes, i.e., the wider class of Giving Verbs containing lend and give as well as the narrower class of Giving Frame containing give and donate, are needed. Therefore, in this work, in order to describe verb meanings with several granularities of semantic classes, a thesaurus form is adopte"
W10-3201,J05-1004,0,0.0127323,"to solve ne disambiguation between verbs when we map a verb in a sentence to WordNet; (3) the basic verbs of Japanese (i.e., highly ambiguous verbs) are wrongly assigned to unrelated synsets because they are constructed by translation from English to Japanese. 2 Existing Lexical Resources and Drawbacks 2.1 Lexical Resources in Japanese Lexical Resources in English From the view of previous lexical databases In English, several well-considered lexical databases are available, e.g., EVCA, Dorr’s LCS (Dorr, 1997), FrameNet, WordNet (Fellbaum, 1998), VerbNet (Kipper-Schuler, 2005) and PropBank (Palmer et al., 2005). Besides there is the research project (Pustejovsky and Meyers, 2005) to nd general descriptional framework of predicate argument structure by merging several lexical databases such as PropBank, NomBank, TimeBank and PennDiscouse Treebank. Our approach corresponds partly to each lexical database, (i.e., FrameNet’s Frame and FrameElements correspond to our verb class and semantic role labels, and the way to organize verb similarity classes with thesaurus corresponds with WordNet’s synset), but is not exactly the same; namely, there is no lexical database describing several granularities of se"
W10-3201,W05-0302,0,0.0150667,"in a sentence to WordNet; (3) the basic verbs of Japanese (i.e., highly ambiguous verbs) are wrongly assigned to unrelated synsets because they are constructed by translation from English to Japanese. 2 Existing Lexical Resources and Drawbacks 2.1 Lexical Resources in Japanese Lexical Resources in English From the view of previous lexical databases In English, several well-considered lexical databases are available, e.g., EVCA, Dorr’s LCS (Dorr, 1997), FrameNet, WordNet (Fellbaum, 1998), VerbNet (Kipper-Schuler, 2005) and PropBank (Palmer et al., 2005). Besides there is the research project (Pustejovsky and Meyers, 2005) to nd general descriptional framework of predicate argument structure by merging several lexical databases such as PropBank, NomBank, TimeBank and PennDiscouse Treebank. Our approach corresponds partly to each lexical database, (i.e., FrameNet’s Frame and FrameElements correspond to our verb class and semantic role labels, and the way to organize verb similarity classes with thesaurus corresponds with WordNet’s synset), but is not exactly the same; namely, there is no lexical database describing several granularities of semantic classes between verbs with arguments. Of course, since the abov"
W10-3201,P06-4017,0,0.218472,"le inheritance. The most ne-grained verb class before individual verb sense is a little wider than alternations. Currently, for the ne-grained verb class, we are organizing what kind of differentiated classes can be assumed (e.g., manner, background, presupposition, and etc.). 3.3 To organize hierarchical semantic verb class, we take a top down and a bottom up approaches. As for a bottom up approach, we use verb senses dened by a dictionary as the most ne-grained meaning; and then we group verbs that can be considered to share some meaning. As for a dictionary, we use the Lexeed database (Fujita et al., 2006), which consists of more than 20,000 verbs with explanations of word sense and example sentences. As a top down approach, we take three semantic classes: State, Change of State, and Activity as top level semantic classes of the thesaurus according to Vendler’s aspectual analysis (Vendler, 1967) (See Figure 4). This is because the above three classes can be useful for dealing with the propositional, especially, resultative aspect of verbs. For example “He threw a ball” can be an Activity and have no special result; but “He broke the door” can be a Change of State and then we can imagine a resul"
W10-3201,C98-1013,0,\N,Missing
W17-0807,C08-1113,0,0.0380051,"ot an issue” Table 3: Our decision tree for classifying a given issue: we do not produce questions for distinguishing X1/X2/X3, and X8/X10/X11/X12/X13, considering that their definitions are clear enough. fer) and LA (language) issues, described in §2. The priority of the former over the latter, implicitly assumed in the MeLLANGE typology, is also largely preserved; the sole exceptions are X7 (incorrect translations of terms) and X4b (too literal). Table 2 also shows that our typology includes the following three issue types that are not covered by MQM. of partial/incomplete annotation, e.g., Tsuboi et al. (2008), our procedure is nevertheless different from these in the sense that we leave issues “unannotated” only when identical ones are already annotated. 5 Intrinsic Evaluation of the Scheme It is hard to make a fair and unbiased comparison between different annotation schemes that target the same phenomena, employing the same assessors. We thus evaluated whether our issue classification scheme leads to sufficiently high level of inter-assessor agreement, regarding those poor results described in §2 as baselines, and analyzed the tendencies of disagreements and the distribution of issues. • X6 (ind"
W17-0807,N06-2015,0,0.376172,"nglish and Japanese. Neither have their applicability to translations produced by less advanced learners, such as undergraduate students, been fully examined. Aiming at (i) a consistent human assessment, (ii) of English-to-Japanese translations, (iii) produced by learner translators, we manually constructed a scheme for classifying identified issues. We first collected English-to-Japanese translations from learners in order to assure and validate the applicability of our scheme (§3). We then manually created an issue typology and a decision tree through an application of the OntoNotes method (Hovy et al., 2006), i.e., an iteration of assessing learners’ translations and updating the typology and decision tree (§4). We adopted an existing typology, that of MNH-TT (Babych et al., 2012), as the starting point, because its origin (Castagnoli et al., 2006) was tailored to assessing university student learners’ translations and its applicability across several European languages had been demonstrated. We evaluated our scheme with inter-assessor agreement, employing four assessors and an undergraduate learner translator (§5). 4 http://www.atanet.org/certification/ aboutexams_error.php 5 SAE J2450, the stan"
W17-0807,P02-1040,0,0.114813,"arity of issues depend Introduction Assessing and assuring translation quality is one of the main concerns for translation services, machine translation (MT) industries, and translation teaching institutions.1 The assessment process for a given pair of source document (SD) and its translation, i.e., target document (TD), consists of two tasks. The first task is to identify erroneous text spans in the TD. In professional settings, when assessors consider a text span in a TD as erroneous, 2 While automated metrics for MT quality evaluation are often presented as objective, many, including BLEU (Papineni et al., 2002), rely on comparison with a one or more human reference translations whose quality and subjectivity are merely assumed and not independently validated. 3 http://www.qt21.eu/launchpad/content/ multidimensional-quality-metrics 1 These include both private companies and translationrelated departments in colleges and universities. 57 Proceedings of the 11th Linguistic Annotation Workshop, pages 57–66, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics on the purpose of translations and the aim of human assessments (e.g., formative or summative). However, the typology"
W17-5705,J82-2005,0,0.74764,"Missing"
W17-5705,P11-1022,0,0.0408254,"Missing"
W17-5705,W16-2378,0,0.0116472,"0.584 0.480 Table 12: Results for the APE task. Method Raw MT output (a) APE w/ gold data only (b) (a) + bitext back-off (c) (b) + pseudo training data Ja→En 43.74 43.38 44.00 43.90 BLEU (↑) Ja→Zh Ja→Ko 73.14 85.52 72.28 84.87 73.01 85.53 73.15 85.57 Ja→En 42.21 42.33 41.87 41.95 Ja→En 0.517 0.438 0.563 TER (↓) Ja→Zh 16.98 17.53 17.05 16.97 F1 -OK (↑) Ja→Zh Ja→Ko 0.711 0.817 0.667 0.770 0.702 0.825 Ja→Ko 9.87 10.31 9.87 9.82 4.4 APE 5 Conclusion The task of APE is to automatically post-edit MT outputs (hyp). Although there are a number of methods that also refer to src (B´echara et al., 2011; Junczys-Dowmunt and Grundkiewicz, 2016), we have so far examined only classic baseline methods based on phrase-based statistical MT. The first system (a) was trained only on the gold data (Simard et al., 2007a) using Moses. However, this system tended to deteriorate the translation quality in terms of BLUE and TER, presumably due to the scarcity of training data. Then, our second model (b) introduced identical pairs of sentences in the target side of our DLC corpus in order to conservatively retain grammatical fragment within hyp. By (re-)decoding the hyp using the multiple decoding path ability of Moses,20 this model significantly"
W17-5705,2011.mtsummit-papers.35,0,0.112321,"Missing"
W17-5705,N16-1059,0,0.079743,"ting HTER score, irrespective of the evaluation metrics: Pearson’s correlation coefficient r, mean average error (MAE), and root mean squared error (RMSE). Given a pair of source text (src) and MT output (hyp), the task of sentence-level QE is to predict how good the entire hyp is, with respect to src. We conducted experiments on both of the HTER prediction and binary classification tasks. 4.3.1 Prediction of HTER In WMT, this task is to predict the HTER score, directly from (src, hyp) pair (Specia et al., 2015), or indirectly through predicting the necessary edits in a similar manner to WQE (Kim and Lee, 2016). We implemented a tool to extract a set of 17 features16 of QuEst++ (Specia et al., 2015), which is regarded as the baseline of this task. To compute the features based on language models, we used the corresponding part of the DLC corpus. To estimate the translation-related features, such as the number of translations per word in src, we trained a phrase-table on the DLC corpus using Moses. Following the findings in Shah et al. (2016), we also incorporated the distributed representations of src and hyp. First, word embeddings with 300 dimensions were learned from each part of the DLC corpus u"
W17-5705,P09-5002,0,0.0164932,"lation (ref ) 4. Manual grading of MT output (grade) 5. Manual post-editing of MT output (pe) For the latter three tasks (detailed in Sections 2.3, 2.4 and 2.5, respectively), we allocated adult native speakers of the target language who also understand Japanese. 2.1 Collecting Japanese utterances (src) First, we collected the following two sets of utterances in Japanese that have been used with our speech translation service. 2.2 Generation of MT outputs (hyp) The collected Japanese segments (src) were then translated by our in-house MT systems, which implement a phrase-based statistical MT (Koehn, 2009). The Ja→En translations were obtained in 2013, with the system trained on 736k sentence pairs. The Ja→Zh and Ja→Ko translations were generated later in 2016, with the systems trained on 1.44M and 1.40M sentence pairs, respectively. Table 1 summarizes the statistics of src and hyp. These segments are relatively shorter than sentences in written texts, such as news articles and patent documents. Travel-related utterances (travel): From the log data that our speech translation service accumulates, we randomly sampled 20,000 identical transcribed segments5 that were identified as Japanese by its"
W17-5705,W15-3001,0,0.0220631,"orpus using Moses. 4.2 Word-level QE (WQE) Given a pair of source text (src) and MT output (hyp), the task of word-level QE is to predict a sequence of tags with the same length as hyp, where each tag indicates how good the corresponding word in hyp is. While some previous studies, such as Bach et al. (2011), addressed to gauge the quality of each word with a real-valued score, WMT adopted a coarse-grained binary tag, i.e., {OK, BAD}, presumably because this form of tags can be automatically generated as the byproduct of computing HTER score by comparing hyp with its post-edited version (pe) (Bojar et al., 2015). Following the recent convention in WMT, we automatically generated a sequence of binary tags for each pair of src and hyp using TERCOM. As the evaluation metrics, we used F1 score of detecting “OK” tags (F1 -OK), that for “BAD” tags (F1 -BAD), and their product (F1 -mult) as in Bojar et al. (2016). As a system for WQE, we adopted an implementation15 based on a feed-forward neural net15 Ja 274,746 14,388 Step 2. Japanese sentences in the remaining half of the DLC corpus were decoded by the MT systems. Step 3. Tag sequences for the MT outputs were given in the same manner as the manually creat"
W17-5705,P17-2062,1,0.874248,"Missing"
W17-5705,P02-1040,0,0.0983965,"Zh Ja→Ko 26.18 38.85 69.44 81.98 39.73 49.11 30.38 51.01 86.45 93.52 34.29 54.16 • If the grade is either “S” or “A” but pe is not identical to the given hyp, both grading and post-editing are performed again. TER (↓) Ja→Zh 50.81 19.20 38.79 48.54 8.63 43.78 Ja→Ko 43.43 12.25 34.75 32.44 4.12 30.00 First, the results of manual grading are summarized in Table 3. While MT outputs for the travel domain were much better than the hospital domain in the Ja→En task, the segments in the hospital domain were better translated by the Ja→Zh and Ja→Ko MT systems. Table 4 shows proximity in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), between translations obtained through different ways. (a) “hyp against ref ” presents what is measured in standard evaluation of MT outputs. The scores in these rows reflect the distribution of MT outputs shown in Table 3. On the other hand, (b) “hyp against pe” gauges the amount of post-edits. As we asked to perform only necessary edits to assure at least grade “A,” the scores in these rows should be good in general. Only the exception is the hospital domain in the Ja→En task. As most of the MT outputs were of low quality, the workers tended to abandon them rat"
W17-5705,potet-etal-2012-collection,0,0.0493193,"Missing"
W17-5705,P16-1009,0,0.134765,"Missing"
W17-5705,W16-2392,0,0.107576,"to predict the HTER score, directly from (src, hyp) pair (Specia et al., 2015), or indirectly through predicting the necessary edits in a similar manner to WQE (Kim and Lee, 2016). We implemented a tool to extract a set of 17 features16 of QuEst++ (Specia et al., 2015), which is regarded as the baseline of this task. To compute the features based on language models, we used the corresponding part of the DLC corpus. To estimate the translation-related features, such as the number of translations per word in src, we trained a phrase-table on the DLC corpus using Moses. Following the findings in Shah et al. (2016), we also incorporated the distributed representations of src and hyp. First, word embeddings with 300 dimensions were learned from each part of the DLC corpus using word2vec17 with its default parameters. Then, the embedding for a given segment is computed by averaging the embeddings of its constituent words, assuming the additive compositionality (Mikolov et al., 2013). During the computation, unknown words were mapped to a zero vector. Finally, values for each of 300 dimensions were regarded as additional features. 4.3.2 Binary Classification We assume that users of speech translation servi"
W17-5705,N07-1064,0,0.0397932,") Ja→Zh Ja→Ko 73.14 85.52 72.28 84.87 73.01 85.53 73.15 85.57 Ja→En 42.21 42.33 41.87 41.95 Ja→En 0.517 0.438 0.563 TER (↓) Ja→Zh 16.98 17.53 17.05 16.97 F1 -OK (↑) Ja→Zh Ja→Ko 0.711 0.817 0.667 0.770 0.702 0.825 Ja→Ko 9.87 10.31 9.87 9.82 4.4 APE 5 Conclusion The task of APE is to automatically post-edit MT outputs (hyp). Although there are a number of methods that also refer to src (B´echara et al., 2011; Junczys-Dowmunt and Grundkiewicz, 2016), we have so far examined only classic baseline methods based on phrase-based statistical MT. The first system (a) was trained only on the gold data (Simard et al., 2007a) using Moses. However, this system tended to deteriorate the translation quality in terms of BLUE and TER, presumably due to the scarcity of training data. Then, our second model (b) introduced identical pairs of sentences in the target side of our DLC corpus in order to conservatively retain grammatical fragment within hyp. By (re-)decoding the hyp using the multiple decoding path ability of Moses,20 this model significantly improved the naive baseline system (a), but the translation quality was not consistently better depending on the language pair. Finally, we introduced in the third syst"
W17-5705,W07-0728,0,0.0298995,") Ja→Zh Ja→Ko 73.14 85.52 72.28 84.87 73.01 85.53 73.15 85.57 Ja→En 42.21 42.33 41.87 41.95 Ja→En 0.517 0.438 0.563 TER (↓) Ja→Zh 16.98 17.53 17.05 16.97 F1 -OK (↑) Ja→Zh Ja→Ko 0.711 0.817 0.667 0.770 0.702 0.825 Ja→Ko 9.87 10.31 9.87 9.82 4.4 APE 5 Conclusion The task of APE is to automatically post-edit MT outputs (hyp). Although there are a number of methods that also refer to src (B´echara et al., 2011; Junczys-Dowmunt and Grundkiewicz, 2016), we have so far examined only classic baseline methods based on phrase-based statistical MT. The first system (a) was trained only on the gold data (Simard et al., 2007a) using Moses. However, this system tended to deteriorate the translation quality in terms of BLUE and TER, presumably due to the scarcity of training data. Then, our second model (b) introduced identical pairs of sentences in the target side of our DLC corpus in order to conservatively retain grammatical fragment within hyp. By (re-)decoding the hyp using the multiple decoding path ability of Moses,20 this model significantly improved the naive baseline system (a), but the translation quality was not consistently better depending on the language pair. Finally, we introduced in the third syst"
W17-5705,2006.amta-papers.25,0,0.299193,"oviders, such as doctors and nurses, and patients, containing 2,225 identical segments of utterances. They were surely spoken language, although they were manually written and more formal than those in the travel domain. Procedure of Corpus Construction We have created our QE/APE datasets, regarding Japanese as the source language. We have so far regarded English, Chinese, and Korean as the target languages, considering that the speakers of these languages hold the largest proportion of visitors to Japan (Japan National Tourism Organization, 2017). Following the procedure in previous studies (Snover et al., 2006; Potet et al., 2012) and practices in WMT (Bojar et al., 2014, 2015, 2016), we determined the following five-step process. We have been examining the installation of our speech translation service into several practical situations where such system helps cross-lingual communication between humans. For this purpose, we have manually created role-play dialogs between Japanese and non-Japanese speakers. The hospital data is one of them. The extracted segments, especially those in the travel domain, include ungrammatical ones, nonunderstandable ones, and those containing inappropriate expressions"
W17-5705,P15-4020,0,0.0854108,"justifies that sentence embeddings obtained by such a naive way19 can improve the performance of predicting HTER score, irrespective of the evaluation metrics: Pearson’s correlation coefficient r, mean average error (MAE), and root mean squared error (RMSE). Given a pair of source text (src) and MT output (hyp), the task of sentence-level QE is to predict how good the entire hyp is, with respect to src. We conducted experiments on both of the HTER prediction and binary classification tasks. 4.3.1 Prediction of HTER In WMT, this task is to predict the HTER score, directly from (src, hyp) pair (Specia et al., 2015), or indirectly through predicting the necessary edits in a similar manner to WQE (Kim and Lee, 2016). We implemented a tool to extract a set of 17 features16 of QuEst++ (Specia et al., 2015), which is regarded as the baseline of this task. To compute the features based on language models, we used the corresponding part of the DLC corpus. To estimate the translation-related features, such as the number of translations per word in src, we trained a phrase-table on the DLC corpus using Moses. Following the findings in Shah et al. (2016), we also incorporated the distributed representations of sr"
W17-5705,W14-3302,0,\N,Missing
W17-5705,W16-2301,0,\N,Missing
W17-5705,W17-4717,0,\N,Missing
W18-1811,D16-1025,0,0.0167618,"tical machine translation (PBSMT) systems in many translation tasks. NMT systems perform especially well with language pairs involving two distant languages or morphologically-rich languages. Translations produced by NMT systems are usually more ﬂuent than those produced by state-of-the-art PBSMT systems. However, NMT systems are still far from producing perfect translations. Many researchers have studied the weaknesses of the NMT approach and shown that NMT systems perform poorly compared to PBSMT systems in relatively common scenarios, especially those involving low-resource language pairs (Bentivogli et al., 2016; Koehn and Knowles, 2017). Several approaches have recently been proposed to combine PBSMT and NMT in order to exploit their complementarity and to produce better translations. In this paper, we study the most popular combination methods and empirically compare them, aiming at drawing a better picture of their strengths and weaknesses. We demonstrate that reranking the simple concatenation of n-best lists produced by each of the NMT and PBSMT systems, with a set of well-motived features, performs consistently the best compared to the more popular and complex methods proposed by previous work."
W18-1811,N12-1047,0,0.2233,". For instance, if we choose only the model scores from an NMT system as features, Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 113 it is likely that all PBSMT hypotheses will be ignored by the reranking framework by giving a high preference to the hypotheses with high NMT models score, which will be actually the ones produced by the NMT system. 3.2 Reranking Framework and Features Previous work on n-best list reranking has proposed many different training algorithms, including those used to optimize PBSMT systems, such as MERT (Och, 2003) and KB-MIRA (Cherry and Foster, 2012). We choose KB-MIRA since it is commonly used in reranking framework and provides stable performances. It can also handle many features as opposed to MERT. The features we used are commonly used for n-best list reranking, which are difﬁcult or impossible to use during NMT or PBSMT decoding. To the best of our knowledge, the following features have never been exploited together in the same reranking framework. 3.2.1 NMT Features NMT translation models can be used to score a translation produced by an arbitrary system. We only need the source sentence and the corresponding translation hypothesis"
W18-1811,W14-3348,0,0.0115539,"sk. 2.1 Confusion Network Decoding The ﬁrst application of machine translation (MT) system combination used a consensus decoding strategy relying on a confusion network (Bangalore et al., 2001). Since this ﬁrst work, this approach has been improved and remains one of the most popular methods to combine many translations produced by different MT systems (Freitag et al., 2014). To generate the confusion network, alignments are required between the tokens of all the translation hypotheses to combine. Previous work (Heaﬁeld and Lavie, 2011; Freitag et al., 2014) on system combination used METEOR (Denkowski and Lavie, 2014) to perform an accurate word alignment between translation hypotheses by making use of its ability to align synonyms, stems, and paraphrases. After building the confusion network, decoding is performed to ﬁnd the most consensual path with additional models such as a large language model. This approach ﬁnds usually a better translation hypothesis than the best translations produced by the individual systems. However, it becomes quickly prohibitive if one wants to combine hundreds of hypotheses, such as the n-best hypotheses generated by different systems, while using costly models to score the"
W18-1811,P14-1129,0,0.0925932,"Missing"
W18-1811,N13-1073,0,0.0421376,"on the target side of the parallel data and the entire News Crawl corpora. We used newstest2012 as a validation dataset during the training of the NMT system and to tune the model weights of the PBSMT system, and newstest2013 (N13) and newstest2014 (N14) for evaluation. The statistics of the data are presented in Table 1. 4.2 Baseline Systems To train and test our PBSMT systems and attention-based NMT systems, we respectively used Moses (Koehn et al., 2007) and Nematus (Sennrich et al., 2017b) frameworks. For our baseline PBSMT systems, word alignments were trained with mgiza and fast align (Dyer et al., 2013) respectively for Ja-En and Fr-En.3 After their training for both translation directions, word alignments are symmetrized using the grow-diag-final-and heuristic. We trained two 4-gram language models with lmplz (Heaﬁeld et al., 2013) for each translation direction, one trained on the target side of the parallel data, and the other on the monolingual data concatenated to the target side of the parallel data. We pruned all singletons for the Japanese and English second language models used for the NTCIR translation tasks, because the monolingual data are very large. The reordering models are MS"
W18-1811,P07-2026,0,0.0323558,"ontains hypotheses produced by two different decoders, we compute two different WPP: one based on the score computed by Equation (1), with direct translation probabilities, and the other based on the score computed by the NMT decoder. 3.2.5 Consensus Score The so-called minimum Bayes risk (MBR) decoding for n-best list is a popular method used in SMT to ﬁnd in an n-best list of hypotheses the one that is on average the most similar to the other hypotheses. Sentence-level BLEU (Papineni et al., 2002) (sBLEU) is usually considered as the metric used to measure the similarity between hypotheses (Ehling et al., 2007). This method has a common objective with confusion network decoding and WPP (Section 3.2.4), since we search for the hypothesis containing the most popular tokens or n-grams used by the decoder to construct its n-best hypotheses. 1 Applying forced word alignment on the NMT hypotheses would be an alternative, but we did not observe any signiﬁcant differences in our preliminary experiments. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 115 We gauge how each hypothesis is similar to all the other hypotheses, using two scores respectively based on sBLEU and"
W18-1811,E14-2008,0,0.157968,"), which combines MT system outputs using neural networks. This method outperformed confusion network decoding, but has been evaluated only on a Chinese-to-English translation task with PBSMT and NMT systems that performed comparably on this task. 2.1 Confusion Network Decoding The ﬁrst application of machine translation (MT) system combination used a consensus decoding strategy relying on a confusion network (Bangalore et al., 2001). Since this ﬁrst work, this approach has been improved and remains one of the most popular methods to combine many translations produced by different MT systems (Freitag et al., 2014). To generate the confusion network, alignments are required between the tokens of all the translation hypotheses to combine. Previous work (Heaﬁeld and Lavie, 2011; Freitag et al., 2014) on system combination used METEOR (Denkowski and Lavie, 2014) to perform an accurate word alignment between translation hypotheses by making use of its ability to align synonyms, stems, and paraphrases. After building the confusion network, decoding is performed to ﬁnd the most consensual path with additional models such as a large language model. This approach ﬁnds usually a better translation hypothesis tha"
W18-1811,D13-1111,0,0.012803,"on and returns the corresponding phrase-based translation probability. PBFD is extremely costly to perform during NMT decoding but rather feasible after it on a selected set of diverse hypotheses. Then, given the PBFD score and the original score given by the NMT system, the rescoring of the hypotheses is performed. Zhang et al. (2017) did not rerank n-best lists but instead reranked a sample of hypotheses extracted from the NMT decoder’s search space. This ampliﬁes the diversity among the hypotheses to rescore, and the increased diversity has been shown useful in training a reranking system (Gimpel et al., 2013). However, as a potential drawback, hypotheses of very bad quality could be chosen. 3 n-best List Reranking 3.1 n-Best List Combination Since the early age of PBSMT (Och et al., 2004), reranking the n-best lists of hypotheses produced by a PBSMT system has been shown to be a simple and efﬁcient way to use complex features that could not be used during decoding. Furthermore, this approach offers some good guarantee to ﬁnd a better translation, because rescoring is applied to the best part of the decoder’s search space, while making use of more, and potentially better, features than the decoder."
W18-1811,W11-2117,0,0.0258745,"h translation task with PBSMT and NMT systems that performed comparably on this task. 2.1 Confusion Network Decoding The ﬁrst application of machine translation (MT) system combination used a consensus decoding strategy relying on a confusion network (Bangalore et al., 2001). Since this ﬁrst work, this approach has been improved and remains one of the most popular methods to combine many translations produced by different MT systems (Freitag et al., 2014). To generate the confusion network, alignments are required between the tokens of all the translation hypotheses to combine. Previous work (Heaﬁeld and Lavie, 2011; Freitag et al., 2014) on system combination used METEOR (Denkowski and Lavie, 2014) to perform an accurate word alignment between translation hypotheses by making use of its ability to align synonyms, stems, and paraphrases. After building the confusion network, decoding is performed to ﬁnd the most consensual path with additional models such as a large language model. This approach ﬁnds usually a better translation hypothesis than the best translations produced by the individual systems. However, it becomes quickly prohibitive if one wants to combine hundreds of hypotheses, such as the n-be"
W18-1811,P13-2121,0,0.0565216,") and newstest2014 (N14) for evaluation. The statistics of the data are presented in Table 1. 4.2 Baseline Systems To train and test our PBSMT systems and attention-based NMT systems, we respectively used Moses (Koehn et al., 2007) and Nematus (Sennrich et al., 2017b) frameworks. For our baseline PBSMT systems, word alignments were trained with mgiza and fast align (Dyer et al., 2013) respectively for Ja-En and Fr-En.3 After their training for both translation directions, word alignments are symmetrized using the grow-diag-final-and heuristic. We trained two 4-gram language models with lmplz (Heaﬁeld et al., 2013) for each translation direction, one trained on the target side of the parallel data, and the other on the monolingual data concatenated to the target side of the parallel data. We pruned all singletons for the Japanese and English second language models used for the NTCIR translation tasks, because the monolingual data are very large. The reordering models are MSD lexicalized and 2 http://www.statmt.org/wmt15/translation-task.html 3 We did not use mgiza to train the word alignments for the Fr-En pair, since fast align is much more efﬁcient on large training data, while it has been shown to pe"
W18-1811,2008.amta-srw.3,0,0.187747,"MT n-best hypotheses does not give access to the PBSMT decoder’s search space and its potentially more adequate translations. Instead of a list of hypotheses produced by a single system or multiple but homogeneous systems, we merge two lists respectively produced by PBSMT and NMT decoders, and rescore all the hypotheses. Then, the reranking framework using a lot of features to better model the ﬂuency and the adequacy of the hypotheses can potentially ﬁnd a better hypothesis than the onebest hypotheses originated by either the PBSMT or NMT systems. This method is similar to the one proposed by Hildebrand and Vogel (2008). However, their work aims at combining n-best lists from any kind of MT systems, ignoring the speciﬁcities and models of the systems used to produce them. In contrast, we focus on PBSMT and NMT system combination by making use of their respective models. This method has never been evaluated in comparison with the state-of-the-art methods presented in Section 2. While this approach seems simple, mixing efﬁciently both kinds of hypotheses is actually challenging. For instance, if we choose only the model scores from an NMT system as features, Proceedings of AMTA 2018, vol. 1: MT Research Track"
W18-1811,W16-2316,0,0.0447448,"produced by the NMT system. In cases where the PBSMT system produces translation of poor quality, we can expect that such pre-translation will signiﬁcantly harm the training of the NMT system. 2.3 Rescoring PBSMT hypotheses with NMT Before the emergence of end-to-end NMT systems, it was a common practice to include neural network models in PBSMT for reranking the n-best translation hypotheses produced by the PBSMT system (Le et al., 2012) or to include them directly during decoding (Devlin et al., Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 112 2014; Junczys-Dowmunt et al., 2016). This strategy has been successfully exploited for PBSMT systems. However, it is currently less attractive, because NMT systems are often able to produce much better translations than PBSMT systems, even better than the best translations obtained after reranking the PBSMT system’s n-best hypotheses with NMT system’s models. 2.4 Phrase-based Forced Decoding Yet another recent method dedicated to combine PBSMT and NMT systems is called phrasebased forced decoding (Zhang et al., 2017) (henceforth, PBFD). The idea is to use the phrase table and its translation probabilities, which are commonly le"
W18-1811,P07-2045,0,0.0225384,"lel data used to train the systems comprise Europarl v7, 109 French–English, and news-commentary v10. The language models for French and English were trained on the target side of the parallel data and the entire News Crawl corpora. We used newstest2012 as a validation dataset during the training of the NMT system and to tune the model weights of the PBSMT system, and newstest2013 (N13) and newstest2014 (N14) for evaluation. The statistics of the data are presented in Table 1. 4.2 Baseline Systems To train and test our PBSMT systems and attention-based NMT systems, we respectively used Moses (Koehn et al., 2007) and Nematus (Sennrich et al., 2017b) frameworks. For our baseline PBSMT systems, word alignments were trained with mgiza and fast align (Dyer et al., 2013) respectively for Ja-En and Fr-En.3 After their training for both translation directions, word alignments are symmetrized using the grow-diag-final-and heuristic. We trained two 4-gram language models with lmplz (Heaﬁeld et al., 2013) for each translation direction, one trained on the target side of the parallel data, and the other on the monolingual data concatenated to the target side of the parallel data. We pruned all singletons for the"
W18-1811,W17-3204,0,0.0600825,"Missing"
W18-1811,N12-1005,0,0.0240909,"ested by Niehues et al. (2016), to improve an NMT system with a pre-translation, the PBSMT system must produce translations of a quality comparable to (or better than) those produced by the NMT system. In cases where the PBSMT system produces translation of poor quality, we can expect that such pre-translation will signiﬁcantly harm the training of the NMT system. 2.3 Rescoring PBSMT hypotheses with NMT Before the emergence of end-to-end NMT systems, it was a common practice to include neural network models in PBSMT for reranking the n-best translation hypotheses produced by the PBSMT system (Le et al., 2012) or to include them directly during decoding (Devlin et al., Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 112 2014; Junczys-Dowmunt et al., 2016). This strategy has been successfully exploited for PBSMT systems. However, it is currently less attractive, because NMT systems are often able to produce much better translations than PBSMT systems, even better than the best translations obtained after reranking the PBSMT system’s n-best hypotheses with NMT system’s models. 2.4 Phrase-based Forced Decoding Yet another recent method dedicated to combine PBSMT a"
W18-1811,C16-1172,0,0.130428,"such as the n-best hypotheses generated by different systems, while using costly models to score the decoding paths. Moreover, we have no guarantee that the output of the combination will be better than the best hypothesis generated by individual systems. The confusion network may allow the generation of many hypotheses of very poor quality, especially in cases where many of the translation systems perform much worse than the best systems used in the combination. 2.2 Pre-translation with a PBSMT system Pre-translation is a recent method dedicated to combine PBSMT and NMT in a simple pipeline (Niehues et al., 2016). First, a PBSMT system is trained and used to decode the source side of the training data of the NMT system. Then, a second-stage NMT system is trained, where the concatenation of the source sentence and the PBSMT-decoded translation is regarded as the new source side of training data, while the target side of the training data remains unchanged. The main motivation behind this work is that a pre-translation generated by a PBSMT system would be informative to better guide the training of NMT systems. However, as suggested by Niehues et al. (2016), to improve an NMT system with a pre-translati"
W18-1811,P03-1021,0,0.0262305,"is actually challenging. For instance, if we choose only the model scores from an NMT system as features, Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 113 it is likely that all PBSMT hypotheses will be ignored by the reranking framework by giving a high preference to the hypotheses with high NMT models score, which will be actually the ones produced by the NMT system. 3.2 Reranking Framework and Features Previous work on n-best list reranking has proposed many different training algorithms, including those used to optimize PBSMT systems, such as MERT (Och, 2003) and KB-MIRA (Cherry and Foster, 2012). We choose KB-MIRA since it is commonly used in reranking framework and provides stable performances. It can also handle many features as opposed to MERT. The features we used are commonly used for n-best list reranking, which are difﬁcult or impossible to use during NMT or PBSMT decoding. To the best of our knowledge, the following features have never been exploited together in the same reranking framework. 3.2.1 NMT Features NMT translation models can be used to score a translation produced by an arbitrary system. We only need the source sentence and th"
W18-1811,N04-1021,0,0.112024,"ypotheses. Then, given the PBFD score and the original score given by the NMT system, the rescoring of the hypotheses is performed. Zhang et al. (2017) did not rerank n-best lists but instead reranked a sample of hypotheses extracted from the NMT decoder’s search space. This ampliﬁes the diversity among the hypotheses to rescore, and the increased diversity has been shown useful in training a reranking system (Gimpel et al., 2013). However, as a potential drawback, hypotheses of very bad quality could be chosen. 3 n-best List Reranking 3.1 n-Best List Combination Since the early age of PBSMT (Och et al., 2004), reranking the n-best lists of hypotheses produced by a PBSMT system has been shown to be a simple and efﬁcient way to use complex features that could not be used during decoding. Furthermore, this approach offers some good guarantee to ﬁnd a better translation, because rescoring is applied to the best part of the decoder’s search space, while making use of more, and potentially better, features than the decoder. However, unlike pre-translation or confusion network decoding approaches, a simple reranking of the hypotheses produced by a single decoder, NMT or PBSMT, is limited in its ability t"
W18-1811,P02-1040,0,0.10175,"mputed given the decoder’s score of the hypotheses in which the word appears. Since our list of hypotheses to rerank contains hypotheses produced by two different decoders, we compute two different WPP: one based on the score computed by Equation (1), with direct translation probabilities, and the other based on the score computed by the NMT decoder. 3.2.5 Consensus Score The so-called minimum Bayes risk (MBR) decoding for n-best list is a popular method used in SMT to ﬁnd in an n-best list of hypotheses the one that is on average the most similar to the other hypotheses. Sentence-level BLEU (Papineni et al., 2002) (sBLEU) is usually considered as the metric used to measure the similarity between hypotheses (Ehling et al., 2007). This method has a common objective with confusion network decoding and WPP (Section 3.2.4), since we search for the hypothesis containing the most popular tokens or n-grams used by the decoder to construct its n-best hypotheses. 1 Applying forced word alignment on the NMT hypotheses would be an alternative, but we did not observe any signiﬁcant differences in our preliminary experiments. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 115 W"
W18-1811,W17-4770,0,0.0515802,"Missing"
W18-1811,E17-3017,0,0.0287332,"Missing"
W18-1811,P16-1162,0,0.129458,"arallel development N13 N14 monolingual 24M 3,003 3,000 3,003 - Ja #tokens Fr 110M 73k 74k 74k 99k 87k 27B 726M 82k 74k 81k 2B #token types Fr En En Ja 102M 67k 68k 70k 92k 80k 15B 169k 4k 5k 5k 6k 6k 9M 614M 73k 70k 71k 3B 275k 5k 6k 6k 7k 6k 22M 2M 11k 11k 11k 4M 2M 10k 9k 10k 6M Table 1: Statistics on train, development, and test data. bidirectional models. PBSMT systems are tuned with KB-MIRA using development data. The distortion limit was tuned and set to 16 for Ja-En.4 Our baseline NMT systems used the default training parameters of Nematus, with layer normalization, and performed BPE (Sennrich et al., 2016) to ﬁx the source and target vocabulary sizes at 50k. The BPE segmentation was jointly learned for French and English since they share the same alphabet. During Nematus training, we saved the model after every 5k mini-batch iterations. The 4-best models according to their performance on the development data were selected to perform ensemble decoding. The decoding were performed using a beam size of 100 to produce 100-best hypotheses. As suggested by Koehn and Knowles (2017), we normalized the hypothesis score by their length during decoding to prevent a drop of the NMT system performance when"
W18-1811,J07-1003,0,0.0226817,"ven by Equations (1) and (2) for both translation directions using the lexical translation probabilities trained on the same parallel data used to train the MT systems. 3.2.4 Word Posterior Probability Word posterior probability (WPP) is another feature that is commonly used in PBSMT to rerank lists of translation hypotheses. For all target tokens appearing in the list, it computes the probability for the token to appear in a translation hypothesis. Then, we can score an entire hypothesis by averaging the posterior probability of the tokens it contains. We use the count-based WPP as deﬁned by Uefﬁng and Ney (2007). WPP is computed given the decoder’s score of the hypotheses in which the word appears. Since our list of hypotheses to rerank contains hypotheses produced by two different decoders, we compute two different WPP: one based on the score computed by Equation (1), with direct translation probabilities, and the other based on the score computed by the NMT decoder. 3.2.5 Consensus Score The so-called minimum Bayes risk (MBR) decoding for n-best list is a popular method used in SMT to ﬁnd in an n-best list of hypotheses the one that is on average the most similar to the other hypotheses. Sentence-l"
W18-1811,W17-4742,0,0.0465689,"h controls the adequacy of the translation • a language model controlling the ﬂuency of the translation • a distortion score that controls how much the target phrases in the translation hypothesis have been reordered given their corresponding source phrases • a lexical reordering table to control three kinds of phrase-based reordering: monotonous, swap, or discontinuous (henceforth, MSD models) • a word penalty to penalize short translations • a phrase penalty to count the number of phrase pairs used to compose the translation While translation models (Zhang et al., 2017) and language models (Wang et al., 2017) are useful to rescore NMT hypotheses, this may not be the case for the reordering models. A stateof-the-art PBSMT decoder limits its search within a pre-determined distortion limit. This limit can be seen as a safeguard to prevent the decoder to generate very ungrammatical translations, since it does not have the ability to model long dependencies between tokens. In contrast, NMT decoders are free to perform long-distance reorderings. For language pairs that need long-distance reordering, this means that an NMT hypothesis of a good quality will have a high distortion score and many source phr"
W18-1811,I17-1016,0,0.40875,"t al., Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 112 2014; Junczys-Dowmunt et al., 2016). This strategy has been successfully exploited for PBSMT systems. However, it is currently less attractive, because NMT systems are often able to produce much better translations than PBSMT systems, even better than the best translations obtained after reranking the PBSMT system’s n-best hypotheses with NMT system’s models. 2.4 Phrase-based Forced Decoding Yet another recent method dedicated to combine PBSMT and NMT systems is called phrasebased forced decoding (Zhang et al., 2017) (henceforth, PBFD). The idea is to use the phrase table and its translation probabilities, which are commonly learned during the training of a PBSMT system, to rescore translations produced by an NMT system. This approach aims at alleviating the low adequacy of some of the translations produced by an NMT system. Since this approach relies directly on the phrase table usually used in PBSMT, it will promote hypotheses that matches phrase pairs associated with a high translation probability from the phrase table. The forced decoding searches for the best phrase-based segmentation and returns the"
W18-1811,P17-2060,0,0.0366947,"SMT and NMT This section reviews four different methods able to combine PBSMT and NMT: confusion network decoding (Section 2.1), pre-translation with a PBSMT system (Section 2.2) and rescoring PBSMT or NMT translation hypotheses using different models (Section 2.3 and Section 2.4). We do not include in our comparison the work of He et al. (2016), which uses SMT features during NMT decoding, because their method cannot use phrase translation probability or more complex models that cannot be used during decoding. Moreover, we leave for future work the study of the more recent method proposed by Zhou et al. (2017), which combines MT system outputs using neural networks. This method outperformed confusion network decoding, but has been evaluated only on a Chinese-to-English translation task with PBSMT and NMT systems that performed comparably on this task. 2.1 Confusion Network Decoding The ﬁrst application of machine translation (MT) system combination used a consensus decoding strategy relying on a confusion network (Bangalore et al., 2001). Since this ﬁrst work, this approach has been improved and remains one of the most popular methods to combine many translations produced by different MT systems (F"
W18-2707,P16-1185,0,0.0450977,"17) generated synthetic parallel sentences by copying target sentences to the source. This method utilizes a feature in which some words, such as named entities, are often identical across the source and target languages and do not require translation. However, this method provides no benefits to language pairs having different character sets, such as English and Japanese. On the other hand, the basis of source monolingual corpora, a pre-training method based on an autoencoder has been proposed to enhance the encoder (Zhang and Zong, 2016). However, the decoder is not enhanced by this method. Cheng et al. (2016) trained two autoencoders using source and target monolingual corpora, while translation models are trained using a parallel corpus. This method enhances both the encoder and decoder, but it requires two monolingual corpora, respectively. Our proposed method enhances not only the decoder but also the encoder and attention using target monolingual corpora. 3 Output Word Output Word Sampling Sampling Word Distribution Generator & Attention Mech. Word Distribution Generator & Attention Mech. Contexts Contexts LSTM States LSTM Figure 2: Decoding Process of Back-Translator by the back-translator to"
W18-2707,P11-2031,0,0.181239,"Missing"
W18-2707,P16-1009,0,0.612859,"ro Sumita National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan {kenji.imamura,atsushi.fujita,eiichiro.sumita}@nict.go.jp Abstract coder is accurately trained because the target side of the synthetic parallel texts consists of manually created (correct) sentences. Consequently, this method provides steady improvements. However, this approach may not contribute to the improvement of the encoder because the source side of the synthetic parallel texts are automatically generated. In this paper, we extend the method proposed by Sennrich et al. (2016a) to enhance the encoder and attention using target monolingual corpora. Our proposed method generates multiple source sentences by sampling when each target sentence is translated back. By using multiple source sentences, we aim to achieve the following. A large-scale parallel corpus is required to train encoder-decoder neural machine translation. The method of using synthetic parallel texts, in which target monolingual corpora are automatically translated into source sentences, is effective in improving the decoder, but is unreliable for enhancing the encoder. In this paper, we propose a me"
W18-2707,W17-4715,0,0.0716399,"lator Target → Source Synthetic Source Sentences Base Parallel Corpus Training (Forward) Translator Source → Target Training Filter Test Sentence Synthetic Parallel Corpus Translation Figure 1: Flow of Our Approach methods only enhance the decoder and require a modification of the NMT. Another approach of using monolingual corpora of the target language is to learn models using synthetic parallel sentences. The method of Sennrich et al. (2016a) generates synthetic parallel corpora through back-translation and learns models from such corpora. Our proposed method is an extension of this method. Currey et al. (2017) generated synthetic parallel sentences by copying target sentences to the source. This method utilizes a feature in which some words, such as named entities, are often identical across the source and target languages and do not require translation. However, this method provides no benefits to language pairs having different character sets, such as English and Japanese. On the other hand, the basis of source monolingual corpora, a pre-training method based on an autoencoder has been proposed to enhance the encoder (Zhang and Zong, 2016). However, the decoder is not enhanced by this method. Che"
W18-2707,P16-1162,0,0.855078,"ro Sumita National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan {kenji.imamura,atsushi.fujita,eiichiro.sumita}@nict.go.jp Abstract coder is accurately trained because the target side of the synthetic parallel texts consists of manually created (correct) sentences. Consequently, this method provides steady improvements. However, this approach may not contribute to the improvement of the encoder because the source side of the synthetic parallel texts are automatically generated. In this paper, we extend the method proposed by Sennrich et al. (2016a) to enhance the encoder and attention using target monolingual corpora. Our proposed method generates multiple source sentences by sampling when each target sentence is translated back. By using multiple source sentences, we aim to achieve the following. A large-scale parallel corpus is required to train encoder-decoder neural machine translation. The method of using synthetic parallel texts, in which target monolingual corpora are automatically translated into source sentences, is effective in improving the decoder, but is unreliable for enhancing the encoder. In this paper, we propose a me"
W18-2707,D17-1158,0,0.0425771,"nd, monolingual corpora are readily available in large quantities. Sennrich et al. (2016a) proposed a method using synthetic parallel texts, in which target monolingual corpora are translated back into the source language (Figure 1). The advantage of this method is that the de2 Related Work One approach of using target monolingual corpora is to construct a recurrent neural network language model and combine the model with the decoder (G¨ulc¸ehere et al., 2015; Sriram et al., 2017). Similarly, there is a method of training language models, jointly with the translator, using multitask learning (Domhan and Hieber, 2017). These 55 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 55–63 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics Target Monolingual Corpus Back-Translator Target → Source Synthetic Source Sentences Base Parallel Corpus Training (Forward) Translator Source → Target Training Filter Test Sentence Synthetic Parallel Corpus Translation Figure 1: Flow of Our Approach methods only enhance the decoder and require a modification of the NMT. Another approach of using monolingual corpora of the target language is to learn models us"
W18-2707,W16-2392,0,0.0290266,"r not. We train the support vector machines (SVMs) on the sentence-level data and regard the classifier’s score as the confidence score. The features of the SVM classifier include the 17 basic features of QuEst++ (Specia et al., 2015).4 They are roughly categorized into the following two types. • Language model features of each of the source and target sentences. • Features based on the parallel sentences such as the average number of translation hypotheses per word. In addition, we add the source and target word embeddings. The sentence features are computed by averaging all word embeddings (Shah et al., 2016). The hyperparameters for the training are set using the grid search on the development set. In the expriments of Section 4, features are extracted from the base parallel corpus. 4 Translation System The translation system used in this study was OpenNMT (Klein et al., 2017). We modified it to accept Sections 3.1 and 3.2. 5 http://www.quest.dcs.shef.ac.uk/ 58 http://pj.ninjal.ac.jp/corpus center/bccwj/en/ The encoder was comprised of a two-layer BiLSTM (500 + 500 units), the decoder included a two-layer LSTM (1,000 units), and the stochastic gradient descent was used for optimization. The learn"
W18-2707,W17-5705,1,0.838981,"nces, each of which contains less than 1024 characters. We assume practical situations in which the domains of parallel and monolingual corpora are not identical. All sentences were segmented into words using an in-house word segmenter. The words were further segmented into 16K sub-words based on the byte-pair encoding rules (Sennrich et al., 2016b) acquired from the base parallel corpus for each language independently. 3.3.2 Confidence Filtering The second method involves filtering with the confidence of translation used in the translation quality estimation task. We use the data provided by Fujita and Sumita (2017), which is a collection of manual labels indicating whether the translation is acceptable or not. We train the support vector machines (SVMs) on the sentence-level data and regard the classifier’s score as the confidence score. The features of the SVM classifier include the 17 basic features of QuEst++ (Specia et al., 2015).4 They are roughly categorized into the following two types. • Language model features of each of the source and target sentences. • Features based on the parallel sentences such as the average number of translation hypotheses per word. In addition, we add the source and ta"
W18-2707,P16-1159,0,0.0282914,"t changed. It must be noted that if the domains of the base parallel and the target monolingual corpora are different, it is better to perform “further training” using the base parallel corpus for domain adaptation (Freitag and Al-Onaizan, 2016; Servan et al., 2016). 3 (2) yt where samplingy (P ) denotes the sampling operation of y based on the probability distribution P . The decoding continues until the end-of-sentence symbol is generated.2 We repeat the above process to generate multiple synthetic sentences. Note that this generation method is the same as that of the minimum risk training (Shen et al., 2016). In NMT, even if a low-probability word is selected by the sampling, the subsequent word would become fluent because it is conditioned by the history. Table 1 presents examples of the synthetic source sentences produced by the back-translator. Most of the synthetic source sentences are identical, or close to, the manual backtranslation (i.e., the reference translation). On the other hand, the last example is quite different from the perspective of word order because the clauses are inverted. Such a synthetic sentence is usually not produced by the n-best translation because of the low likelih"
W18-2707,P15-4020,0,0.0128306,"ennrich et al., 2016b) acquired from the base parallel corpus for each language independently. 3.3.2 Confidence Filtering The second method involves filtering with the confidence of translation used in the translation quality estimation task. We use the data provided by Fujita and Sumita (2017), which is a collection of manual labels indicating whether the translation is acceptable or not. We train the support vector machines (SVMs) on the sentence-level data and regard the classifier’s score as the confidence score. The features of the SVM classifier include the 17 basic features of QuEst++ (Specia et al., 2015).4 They are roughly categorized into the following two types. • Language model features of each of the source and target sentences. • Features based on the parallel sentences such as the average number of translation hypotheses per word. In addition, we add the source and target word embeddings. The sentence features are computed by averaging all word embeddings (Shah et al., 2016). The hyperparameters for the training are set using the grid search on the development set. In the expriments of Section 4, features are extracted from the base parallel corpus. 4 Translation System The translation"
W18-2707,L18-1545,1,0.800428,"h an appropriate value, we can obtain synthetic sentences that are almost of the same length as the manual back-translation. We set the word penalty such that the lengths of the translation and reference translation on the development set are approximately equal, using line search. 3.3.3 Random Filtering The third method is random filtering. This is identical to the reduction of the number of synthetic source sentences to be generated. 4 Experiments 4.1 Experimental Settings Corpora The corpus sizes used here are shown in Table 2. We used the global communication plan corpus (the GCP corpus, (Imamura and Sumita, 2018)), which is an in-house parallel corpus of daily life conversations and consists of Japanese (Ja), English (En), and Chinese (Zh). The experiments were performed on Englishto-Japanese and Chinese-to-Japanese translation tasks. We randomly selected 400K sentences for the base parallel corpus, and the remaining (1.55M sentences) were used as the Japanese monolingual corpus. The reason for dividing the parallel corpus into two corpora is to measure the upper-bound of quality improvement by using existing parallel texts on the same domain as the manual backtranslation. We also used the Balanced Co"
W18-2707,P17-4012,0,0.0440349,"e following two types. • Language model features of each of the source and target sentences. • Features based on the parallel sentences such as the average number of translation hypotheses per word. In addition, we add the source and target word embeddings. The sentence features are computed by averaging all word embeddings (Shah et al., 2016). The hyperparameters for the training are set using the grid search on the development set. In the expriments of Section 4, features are extracted from the base parallel corpus. 4 Translation System The translation system used in this study was OpenNMT (Klein et al., 2017). We modified it to accept Sections 3.1 and 3.2. 5 http://www.quest.dcs.shef.ac.uk/ 58 http://pj.ninjal.ac.jp/corpus center/bccwj/en/ The encoder was comprised of a two-layer BiLSTM (500 + 500 units), the decoder included a two-layer LSTM (1,000 units), and the stochastic gradient descent was used for optimization. The learning rate for the base parallel corpus was 1.0 for the first 14 epochs, followed by the annealing of 6 epochs while decreasing the learning rate by half. The mini-batch size was 64. At the translation stage, we generated 10-best translations and selected the best among them"
W18-2707,W17-5706,0,0.157005,"Development Test Monolingual GCP Corpus (Japanese) BCCWJ out. Note that the likelihood is corrected with the length of the synthetic source sentence. We call this the length biased log-likelihood lllen (Oda et al., 2017). lllen (y|x) = ∑ Parallel log Pr(yt |x, y&lt;t )+W P ·T, (3) # Sentences 400,000 2,000 2,000 1,552,475 4,791,336 Table 2: Corpus Statistics t where the first term on the right-hand side is the log-likelihood, W P denotes the word penalty (W P ≥ 0), and T denotes the number of words in the synthetic source sentence. NMTs tend to generate shorter translations than the expectation (Morishita et al., 2017). The word penalty works to increase the likelihood of long hypotheses when it is set to a positive value. With an appropriate value, we can obtain synthetic sentences that are almost of the same length as the manual back-translation. We set the word penalty such that the lengths of the translation and reference translation on the development set are approximately equal, using line search. 3.3.3 Random Filtering The third method is random filtering. This is identical to the reduction of the number of synthetic source sentences to be generated. 4 Experiments 4.1 Experimental Settings Corpora Th"
W18-2707,D16-1160,0,0.0383788,"rpora. Our proposed method is an extension of this method. Currey et al. (2017) generated synthetic parallel sentences by copying target sentences to the source. This method utilizes a feature in which some words, such as named entities, are often identical across the source and target languages and do not require translation. However, this method provides no benefits to language pairs having different character sets, such as English and Japanese. On the other hand, the basis of source monolingual corpora, a pre-training method based on an autoencoder has been proposed to enhance the encoder (Zhang and Zong, 2016). However, the decoder is not enhanced by this method. Cheng et al. (2016) trained two autoencoders using source and target monolingual corpora, while translation models are trained using a parallel corpus. This method enhances both the encoder and decoder, but it requires two monolingual corpora, respectively. Our proposed method enhances not only the decoder but also the encoder and attention using target monolingual corpora. 3 Output Word Output Word Sampling Sampling Word Distribution Generator & Attention Mech. Word Distribution Generator & Attention Mech. Contexts Contexts LSTM States LS"
W18-2707,W17-5712,1,0.825262,"hood Filtering The first method is filtering by the likelihood output from the back-translator. We consider the likelihood as an indicator of translation quality, and low-likelihood synthetic sentences are filtered 2 The back-translator does not use the beam search because the sampling is independently performed for each word. 3 57 We did not perform “further training” in this paper. Type Base Development Test Monolingual GCP Corpus (Japanese) BCCWJ out. Note that the likelihood is corrected with the length of the synthetic source sentence. We call this the length biased log-likelihood lllen (Oda et al., 2017). lllen (y|x) = ∑ Parallel log Pr(yt |x, y&lt;t )+W P ·T, (3) # Sentences 400,000 2,000 2,000 1,552,475 4,791,336 Table 2: Corpus Statistics t where the first term on the right-hand side is the log-likelihood, W P denotes the word penalty (W P ≥ 0), and T denotes the number of words in the synthetic source sentence. NMTs tend to generate shorter translations than the expectation (Morishita et al., 2017). The word penalty works to increase the likelihood of long hypotheses when it is set to a positive value. With an appropriate value, we can obtain synthetic sentences that are almost of the same l"
W18-2707,D16-1163,0,0.0229488,"ic source sentences and the BLEU score on the GCP corpus of En-Ja and Zh-Ja translation tasks, respectively. The graphs and tables in the figures present the same data for overviews and for analyzing the data in detail. Note that the method of Sennrich et al. (2016a) corresponds to the case of one synthetic source 6 4.3 Results with BCCWJ Table 3 shows the results using BCCWJ as a monolingual corpus (the results of the GCP cor7 Unfortunately, it is unknown in this experiment whether the encoder or attention were enhanced. We plan to investigate which module is enhanced by freezing parameters (Zoph et al., 2016) of the encoder and attention through the training. https://github.com/jhclark/multeval 59 32.0 Manual Back-Translation 31.0 BLEU 30.0 29.0 28.0 Sennrich et al. (2016) Likelihood Filtering Confidence Filtering Random Filtering N-best Generation 27.0 26.0 Base Corpus Only 25.0 0 2 4 6 8 10 Number of Synthetic Source Sentences # of Synthetic Sentences Base Corpus Only Sennrich et al. (2016a) 1 2 4 6 8 10 Manual Back-Translation En-Ja Confidence Random Filtering Filtering 26.19 28.61 (+2.42) 28.49 (+2.30) 28.85 (+2.66) 29.26 (+3.07) 30.30 (+4.11) 30.26 (+4.07) 30.08 (+3.89) 30.59 (+4.40) 30.27 (+"
W18-2707,P02-1040,0,0.100513,"Missing"
W18-6419,N12-1047,0,0.0542077,"r training data. Consequently, for Tr-En and Zh-En we simply trained regular phrase-based models using MSLR (monotone, swap, discontinuous-left, discontinuous-right) lexicalized reordering models and used the default distortion limit of 6. We trained two 4-gram language models: one on the entire monolingual data concatenated to the target side of the parallel data, and another one on the in-domain “News Crawl” corpora only, using LMPLZ (Heafield et al., 2013). For English, all singletons were pruned due to the large size of the monolingual data. To tune the SMT model weights, we used KB-MIRA (Cherry and Foster, 2012) and selected the weights giving the best BLEU score on the development data after 15 decoding runs. 4 target monolingual data i=i+1 source to target NMT target to source NMT synthetic parallel data synthetic parallel data ki=rki-1 sentences ki=rki-1 sentences ki back-translated sentences ki back-translated sentences Figure 1: Our incremental training framework. systems are trained, from scratch, on their respective new training data comprising the mixture of the original parallel data and the synthetic parallel data whose source side is back-translated from the target side. At this stage, we"
W18-6419,P13-2121,0,0.05331,"Missing"
W18-6419,W18-2703,0,0.069259,"the mixture of the synthetic and original parallel data, we back-translate a larger number of monolingual sentences, including the same sentences backtranslated at the first iteration. Since we have better NMT systems than those at the first iteration, we can expect the back-translation to be of a better quality. We mix this new synthetic parallel data to the original one and train again from scratch a source-to-target and a target-to-source NMT systems to obtain further improved translation models. Note that this procedure is partially similar to the work proposed by Zhang et al. (2018) and Hoang et al. (2018), but differs in the sense that we increase incrementally our back-translated data. Given the number of sentences used in the first iteration, k1 , and an expansion factor, r, we determine ki , the number of monolingual sentences back-translated at iteration i, as follows: Back-translation of Monolingual Data 4.1 bilingual parallel data Incremental Back-Translation with Et-En, Fi-En, and Tr-En We introduced an incremental training framework for NMT aiming to iteratively increase the quality and quantity of the synthetic parallel data used for training. In this framework, we first simultaneousl"
W18-6419,P18-4020,0,0.0575771,"Missing"
W18-6419,I17-1016,1,0.84595,"the scores given by right-to-left NMT models that we trained for each translation direction with the same parameters as left-to-right NMT models. The two right-to-left NMT models, each achieving the best BLEU and the best perplexity scores on the development data, were selected, giving us two other features for each translation direction. Since the Tr-En training parallel data are much smaller, we were able to perform one more right-to-left train15 In practice, adding one more right-to-left model for reranking did not significantly improve the BLEU score on the development data. 453 7 score (Zhang et al., 2017) thanks to the small size of the phrase table learned for this language pair. Also only for this language pair, we computed the scores for each hypothesis given by the so-called minimum Bayes risk (MBR) decoding for n-best list using two metrics: sBLEU and chrF++ (Popovi´c, 2017). The reranking framework was trained on n-best lists produced by the decoding of the same development data that we used to validate NMT system’s training and to tune SMT’s model weights. 6 Conclusion We participated in eight translation directions and for all of them we did experiments to compare SMT and NMT performan"
W18-6419,P07-2045,0,0.0179829,"9M (Fi) 4.4M (Tr) 509.9M (Zh) 36.0M (En) 72.8M (En) 5.1M (En) 576.2M (En) Table 1: Statistics of our preprocessed parallel data. Language En Et Fi Tr Zh Table 2: data. #lines #tokens 338.7M 146.1M 177.1M 105.0M 130.5M 7.5B 3.6B 3.2B 1.8B 2.3B Statistics of our preprocessed monolingual used only 100 millions sentence pairs randomly extracted from “Common Crawl.” To tune/validate and evaluate our systems, we used Newstest2016 and Newstest2017 for Fi-En and Tr-En, Newsdev2017 and Newstest2017 for Zh-En, and Newsdev2018 for Et-En. 2.2 Tokenization, Truecasing and Cleaning We used Moses tokenizer (Koehn et al., 2007) and truecaser for English, Estonian, Finnish, and Turkish. The truecaser was trained on one million tokenized lines extracted randomly from the monolingual data. Truecasing was then performed on all the tokenized data. For Chinese, we used Jieba3 for tokenization but did not perform truecasing. For cleaning, we only applied the Moses script clean-n-corpus.perl to remove lines in the parallel data containing more than 80 tokens and replaced characters forbidden by Moses. Note that we did not perform any punctuation normalization. Tables 1 and 2 present the statistics of the parallel and monoli"
W18-6419,W17-3204,0,0.0263216,"rie and Fujita (2018), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even when SMT greatly underperforms NMT. Following Marie and Fujita (2018), our combination of NMT and SMT works as follows. 5.1 Generation of n-best Lists We first produced the 100-best translation hypotheses with our NMT and SMT systems, independently.11 Unlike Moses, Marian must use a beam of size k to produce a k-best list during decoding. However, using a larger beam size during decoding for NMT may worsen translation quality (Koehn and Knowles, 2017).12 Consequently, we also produced with Marian the 10-best lists, for Zh-En, and 12-best lists for the other language pairs, and merged them with Marian’s 100-best lists to obtain lists containing up to 110 or 112 hypotheses.13 In this way, we make sure that we still have hypotheses of good quality in the lists despite using a larger beam size.14 Then, we merged the lists produced by Marian and Moses. We rescored all the hypotheses in the resulting lists with a reranking framework using features to better model the fluency and the adequacy of each hySetting for Zh-En For the Zh-En language pai"
W18-6419,W18-1811,1,0.930099,"1M 1M 200k 2 2 2 2 2 4 #lines back-translated 10M 20M 40M Table 3: Parameters used for our incremental training. For each language pair, the same parameters were used for both translation directions. In our preliminary experiments, we found that setting r = 2 and k1 very close to, or smaller than, the size of the original parallel data consistently gives good results across language pairs. Fine-tuning r and k1 would result in a better translation quality but at a greater cost. our primary submissions for WMT18 are the results of a simple combination of NMT and SMT. Indeed, as demonstrated by Marie and Fujita (2018), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even when SMT greatly underperforms NMT. Following Marie and Fujita (2018), our combination of NMT and SMT works as follows. 5.1 Generation of n-best Lists We first produced the 100-best translation hypotheses with our NMT and SMT systems, independently.11 Unlike Moses, Marian must use a beam of size k to produce a k-best list during decoding. However, using a larger beam size during decoding for NMT may worsen translation quality (Koehn and Knowles, 20"
W18-6419,P02-1040,0,0.102926,"ords --valid-metrics ce-mean-words perplexity translation --keep-best --enc-depth 6 --dec-depth 6 --transformer-dropout 0.1 --learn-rate 0.0003 --dropout-src 0.1 --dropout-trg 0.1 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --label-smoothing 0.1 --devices 0 1 2 3 --dim-vocabs 50000 50000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --sync-sgd --tied-embeddings --exponential-smoothing. For ZhEn, we did not use --dropout-src 0.1 --dropout-trg 0.1 since the training data is much larger. We performed NMT decoding with an ensemble of a total of six models according to the best BLEU (Papineni et al., 2002) and the best perplexity scores,7 produced by three independent training runs. #tokens 29.4M (Et) 52.9M (Fi) 4.4M (Tr) 509.9M (Zh) 36.0M (En) 72.8M (En) 5.1M (En) 576.2M (En) Table 1: Statistics of our preprocessed parallel data. Language En Et Fi Tr Zh Table 2: data. #lines #tokens 338.7M 146.1M 177.1M 105.0M 130.5M 7.5B 3.6B 3.2B 1.8B 2.3B Statistics of our preprocessed monolingual used only 100 millions sentence pairs randomly extracted from “Common Crawl.” To tune/validate and evaluate our systems, we used Newstest2016 and Newstest2017 for Fi-En and Tr-En, Newsdev2017 and Newstest2017 for"
W18-6419,W17-4770,0,0.0236645,"Missing"
W18-6419,P16-1009,0,0.397624,"(WMT), Volume 2: Shared Task Papers, pages 449–455 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64046 Language pair Et-En Fi-En Tr-En Zh-En #sent. pairs 1.9M 3.1M 207.4k 24.8M Marian4 (Junczys-Dowmunt et al., 2018) to train and evaluate our NMT systems since it supports state-of-the-art features and is one of the fastest NMT framework publicly available.5 In order to limit the size of the vocabulary of the NMT models, we segmented tokens in the parallel data into subword units via byte pair encoding (BPE) (Sennrich et al., 2016b) using 50k operations. BPE segmentations were jointly learned on the training parallel data for source and target languages, except for Zh-En for which Chinese and English segmentations were trained separately. All our NMT systems for Et-En, Fi-En, and Tr-En were consistently trained on 4 GPUs,6 with the following parameters for Marian: --type transformer --max-length 80 --mini-batch-fit --valid-freq 5000 --save-freq 5000 --workspace 8000 --disp-freq 500 --beam-size 12 --normalize 1 --valid-mini-batch 16 --overwrite --early-stopping 5 --cost-type ce-mean-words --valid-metrics ce-mean-words p"
W18-6419,P16-1162,0,0.730129,"(WMT), Volume 2: Shared Task Papers, pages 449–455 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64046 Language pair Et-En Fi-En Tr-En Zh-En #sent. pairs 1.9M 3.1M 207.4k 24.8M Marian4 (Junczys-Dowmunt et al., 2018) to train and evaluate our NMT systems since it supports state-of-the-art features and is one of the fastest NMT framework publicly available.5 In order to limit the size of the vocabulary of the NMT models, we segmented tokens in the parallel data into subword units via byte pair encoding (BPE) (Sennrich et al., 2016b) using 50k operations. BPE segmentations were jointly learned on the training parallel data for source and target languages, except for Zh-En for which Chinese and English segmentations were trained separately. All our NMT systems for Et-En, Fi-En, and Tr-En were consistently trained on 4 GPUs,6 with the following parameters for Marian: --type transformer --max-length 80 --mini-batch-fit --valid-freq 5000 --save-freq 5000 --workspace 8000 --disp-freq 500 --beam-size 12 --normalize 1 --valid-mini-batch 16 --overwrite --early-stopping 5 --cost-type ce-mean-words --valid-metrics ce-mean-words p"
W19-5313,P18-4020,0,0.0305142,"Missing"
W19-5313,P07-2045,0,0.0106799,"Missing"
W19-5313,W18-6419,1,0.860496,"algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a equal contribution 168 Proceedings of the Fourth Conference on Machine Translation (WMT), Volu"
W19-5313,P02-1040,0,0.103864,"was set to 2048. The number of attention heads in each encoder and decoder layer was set to eight. During training, the value of label smoothing was set to 0.1, and the attention dropout and residual dropout were set to 0.1. The Adam optimizer (Kingma and Ba, 2014) was used to tune the parameters of the model. The learning rate was varied under a warm-up strategy with warm-up steps of 16,000. All NMT models for ZH↔EN tasks were consistently trained on four P100 GPUs. We validated the model with an interval of 5,000 batches on the development set and selected the best model according to BLEU (Papineni et al., 2002) score on the newsdev2018 data set. We performed the following training run independently for five times to obtain the models for ensembling. First, an initial model was trained on the provided parallel data and used to generate pseudo-parallel data through back-translation. A new model was then trained from scratch on the mixture of the original parallel data and the pseudo-parallel data. The new model was further 12 13 Results Table 2 shows the results of ZH↔EN tasks. It is obvious that the back-translation, fine-tuning, and ensemble methods are greatly effective for the ZH↔EN tasks. In part"
W19-5313,Y17-1038,1,0.807473,"ansfer Learning In addition to the approaches in Section 3.1, we also use fine-tuning for transfer learning. Zoph et al. (2016) proposed to train a robust L3→L1 parent model using a large L3–L1 parallel corpus and then fine-tune it on a small L2–L1 corpus to obtain a robust L2→L1 child model. The underlying assumption is that the pre-trained L3→L1 model contains prior probabilities for translation into L1. The prior information is divided into two parts: language modeling information (strong prior) and cross-lingual information (weak or strong depending on the relationship between L3 and L2). Dabre et al. (2017) have shown that linguistically similar L3 and L2 allow for better transfer learning. As such, we transliterate L3 to L2 before pre-training a parent model. This could help in faster convergence, ensure cognate overlap, and potentially lead to a better translation quality. In this participation, we used Hindi as the helping language, L3. Results Refer to rows 1 and 2 of Table 1 for the various automatic evaluation scores. For Kazakh→English our submitted system achieved a cased BLEU score of 26.2 placing our system at 3rd rank out of 9 primary systems. On the other hand, our English→Kazakh per"
W19-5313,P16-1009,0,0.456958,"al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transformer (Vaswani et al., 2"
W19-5313,N16-1101,0,0.279052,", 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transfo"
W19-5313,P16-1162,0,0.806805,"al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transformer (Vaswani et al., 2"
W19-5313,D16-1163,0,0.143248,"English corpus. Chinese↔English translation can benefit from back-translation, model ensembling, and fine-tuning based on the development data. Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which l"
W19-5330,P18-1073,0,0.354971,"sesdecoder 295 we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual word embeddings (BWE), and the 300 target phrases with the highest scores were kept in the phrase table for each source phrase. In total, the induced phrase table contains 90M phrase pairs. BWE of 512 dimensions were obtained using word embeddings trained with fastText9 and aligned in the same space using unsupervised Vecmap (Artetxe et al., 2018b)10 for this induction. In total four scores, to be used as features in the phrase table, for each of these phrase pairs were computed to mimic phrasebased SMT: forward and backward phrase and lexical translation probabilities. Then, the phrase table was plugged into a Moses system that was tuned on the development data using KB-MIRA. We performed four refinement steps to improve the system using at each step 3M synthetic parallel sentences generated by the forward and backward translation systems, instead of using only either forward (Marie and Fujita, 2018b) or backward translations (Artetx"
W19-5330,D18-1399,0,0.473732,"Missing"
W19-5330,P07-2045,0,0.00885169,"Missing"
W19-5330,W17-3204,0,0.021935,"work. To account for hypotheses length, we added the difference, and its absolute value, between the number of tokens in the translation hypothesis and the source sentence. The reranking framework was trained on n-best lists generated by decoding the first 3k sentence pairs of the development data that we also used to validate the training of UNMT and PNMT systems and to tune the weights of USMT models. Table 3: Parameters for training Marian. 4 Generation of n-best Lists 12 We generated n-best with different beam size for decoding since translation quality can decrease with larger beam size (Koehn and Knowles, 2017). https://marian-nmt.github.io/ 297 # Methods de-cs 1 2 Single UNMT system Single USMT system 15.5 11.1 3 4 5 6 7 Single NMT system pseudo-supervised by UNMT Single NMT system pseudo-supervised by USMT Single Pseudo-supervised MT system Ensemble Pseudo-supervised MT system Re-ranking Pseudo-supervised MT system 15.9 15.3 16.2 16.5 17.0 8 9 10 Fine-tuning Pseudo-supervised MT system Fine-tuning Pseudo-supervised MT system + fixed quotes Fine-tuning + re-ranking Pseudo-supervised MT system + fixed quotes 18.7 19.6 20.1 Table 4: BLEU scores of UMT. #10 is our primary system submitted to the organ"
W19-5330,N12-1047,0,0.0485966,"on 4 GPUs for 300,000 iterations, with the parameters listed by Table 3. Combination of PNMT and USMT Our primary submission for the task was the result of a simple combination of PNMT and USMT similarly to what we did last year in our participation to the supervised News Translation Task of WMT18 (Marie et al., 2018). As demonstrated by Marie and Fujita (2018a), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even though SMT greatly underperforms 11 Reranking Framework and Features We chose KB-MIRA (Cherry and Foster, 2012) as a rescoring framework and used a subset of the features proposed in Marie and Fujita (2018a). All the following features we used are described in details by Marie and Fujita (2018a). It includes the scores given by N PNMT models independently trained. We computed sentencelevel translation probabilities using the lexical translation probabilities learned by mgiza during the training of our USMT system. We also used two 4-gram language models to compute two features for each hypothesis. One is the same language model used by our USMT system while the other is a small model trained on all the"
W19-5330,P17-2061,0,0.0185166,"pus for Kazakh. Statistics of the data preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system according to BLEU. However, compared with pseudosupervised MT model trained only on pseudoparallel corpora generated by either UNMT ("
W19-5330,2015.iwslt-evaluation.11,0,0.0294481,"ge in-domain corpora. For Gujarati and Kazakh, we used Common Crawl and News Crawl corpora, in addition to the provided News Commentary corpus for Kazakh. Statistics of the data preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system a"
W19-5330,C18-1111,1,0.806407,"atistics of the data preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system according to BLEU. However, compared with pseudosupervised MT model trained only on pseudoparallel corpora generated by either UNMT (#3) or USMT (#4), me"
W19-5330,W18-1811,1,0.86005,"result of a simple combination of our unsupervised neural and statistical machine translation systems. Our system is ranked first for the German-to-Czech translation task, using only the data provided by the organizers (“constraint”), according to both BLEU-cased and human evaluation. We also performed contrastive experiments with other language pairs, namely, English-Gujarati and EnglishKazakh, to better assess the effectiveness of unsupervised machine translation in for distant language pairs and in truly low-resource conditions. reranking using different informative features as proposed by Marie and Fujita (2018a). This simple combination method performed the best among unsupervised MT systems at WMT19 by BLEU 1 and human evaluation (Bojar et al., 2019). In addition to the official track, we also present the unsupervised systems for English-Gujariti and English-Kazakh for contrastive experiments with much more distant language pairs. The remainder of this paper is organized as follows. In Section 2, we introduce the data preprocessing. In Section 3, we describe the details of our UNMT, USMT, and pseudosupervised MT systems. Then, the combination of pseudo-supervised NMT and USMT is described in Secti"
W19-5330,N16-1162,0,0.0471887,"The other parameters for training the language model were set as listed in Table 1. Then we trained a Transformer-based UNMT model with the pre-trained cross-lingual language model using XLM toolkit. The auto-encoder of UNMT architecture cannot learn useful knowledge without some constraints; it would merely become a copying task that learns to copy the input words one by one (Lample et al., 2018). To alleviate this issue, we utilized a denoising auto-encoder (Vincent et al., 2010), and added noise in the form of random token swapping in input sentences to improve the model learning ability (Hill et al., 2016; He et al., 2016). The denoising auto-encoder acts as a language model that has been trained in one language and Systems Our entire system is illustrated in Figure 1. 3.1 Unsupervised NMT To build competitive UNMT systems, we chose to rely on the Transformer-based UNMT initialized by a pre-trained cross-lingual language model (Lample and Conneau, 2019) since it had been shown to outperform UNMT initialized with word embeddings, in quality and efficiency. In order to limit the size of the vocabulary of the UNMT model, we segmented tokens in the training data into sub-word units via byte pair e"
W19-5330,W18-6419,1,0.803583,"pairs generated by UNMT. To train this pseudo-supervised NMT (PNMT) system, we chose Marian (Junczys-Dowmunt et al., 2018)11 since it supports state-of-the-art features and is one of the fastest NMT frameworks publicly available. Specifically, the pseudo-supervised NMT system for de-cs was trained on 4 GPUs for 300,000 iterations, with the parameters listed by Table 3. Combination of PNMT and USMT Our primary submission for the task was the result of a simple combination of PNMT and USMT similarly to what we did last year in our participation to the supervised News Translation Task of WMT18 (Marie et al., 2018). As demonstrated by Marie and Fujita (2018a), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even though SMT greatly underperforms 11 Reranking Framework and Features We chose KB-MIRA (Cherry and Foster, 2012) as a rescoring framework and used a subset of the features proposed in Marie and Fujita (2018a). All the following features we used are described in details by Marie and Fujita (2018a). It includes the scores given by N PNMT models independently trained. We computed sentencelevel translation pr"
W19-5330,P18-4020,0,0.0370204,"Missing"
W19-5330,P16-1009,0,0.474495,"16). The denoising auto-encoder acts as a language model that has been trained in one language and Systems Our entire system is illustrated in Figure 1. 3.1 Unsupervised NMT To build competitive UNMT systems, we chose to rely on the Transformer-based UNMT initialized by a pre-trained cross-lingual language model (Lample and Conneau, 2019) since it had been shown to outperform UNMT initialized with word embeddings, in quality and efficiency. In order to limit the size of the vocabulary of the UNMT model, we segmented tokens in the training data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b). We determined 60k BPE operations jointly on the training monolingual data for German and Czech, and used a shared vocabulary for both languages with 60k tokens based on BPE. We used 50M monolingual corpora to train a 6 https://github.com/facebookresearch/ XLM 7 NVIDIA @ Tesla @ P100 16Gb. 5 https://github.com/moses-smt/ mosesdecoder 295 we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual"
W19-5330,P16-1162,0,0.857696,"16). The denoising auto-encoder acts as a language model that has been trained in one language and Systems Our entire system is illustrated in Figure 1. 3.1 Unsupervised NMT To build competitive UNMT systems, we chose to rely on the Transformer-based UNMT initialized by a pre-trained cross-lingual language model (Lample and Conneau, 2019) since it had been shown to outperform UNMT initialized with word embeddings, in quality and efficiency. In order to limit the size of the vocabulary of the UNMT model, we segmented tokens in the training data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b). We determined 60k BPE operations jointly on the training monolingual data for German and Czech, and used a shared vocabulary for both languages with 60k tokens based on BPE. We used 50M monolingual corpora to train a 6 https://github.com/facebookresearch/ XLM 7 NVIDIA @ Tesla @ P100 16Gb. 5 https://github.com/moses-smt/ mosesdecoder 295 we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual"
W19-5330,P18-1072,0,0.0953411,"Missing"
W19-5330,P17-2089,1,0.888859,"Missing"
W19-5330,D17-1155,1,0.833261,"preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system according to BLEU. However, compared with pseudosupervised MT model trained only on pseudoparallel corpora generated by either UNMT (#3) or USMT (#4), merging pseudo-parall"
W19-5428,P18-1073,0,0.149925,"-final-and heuristic. We also trained MSLR (monotone, swap, discontinuousleft, discontinuous-right) lexicalized reordering model. We trained one 4-gram language models on the entire monolingual data concatenated to the target side of the parallel data using LMPLZ 1 https://marian-nmt.github.io/, 1.6.0 2 NVIDIA® Tesla® P100 16Gb. 3 https://github.com/moses-smt/ mosesdecoder/ Unsupervised SMT We also built an SMT system, without any supervision, i.e., using only but all the provided monolingual data for training. We chose unsupervised SMT (USMT) over unsupervised NMT (UNMT) since previous work (Artetxe et al., 2018b) has shown that USMT slightly outperforms UNMT and that we expect USMT to work well for this language pair that involves only very few word reorderings. We built USMT systems using a framework similar to the one proposed in Marie and Fujita (2018b). The first step of USMT is the induction of a phrase table from the monolingual corpora. We first collected phrases of up to six tokens from the monolingual News Crawl corpora version 209 using word2phrase.4 As phrases, we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpor"
W19-5428,D18-1399,0,0.300481,"-final-and heuristic. We also trained MSLR (monotone, swap, discontinuousleft, discontinuous-right) lexicalized reordering model. We trained one 4-gram language models on the entire monolingual data concatenated to the target side of the parallel data using LMPLZ 1 https://marian-nmt.github.io/, 1.6.0 2 NVIDIA® Tesla® P100 16Gb. 3 https://github.com/moses-smt/ mosesdecoder/ Unsupervised SMT We also built an SMT system, without any supervision, i.e., using only but all the provided monolingual data for training. We chose unsupervised SMT (USMT) over unsupervised NMT (UNMT) since previous work (Artetxe et al., 2018b) has shown that USMT slightly outperforms UNMT and that we expect USMT to work well for this language pair that involves only very few word reorderings. We built USMT systems using a framework similar to the one proposed in Marie and Fujita (2018b). The first step of USMT is the induction of a phrase table from the monolingual corpora. We first collected phrases of up to six tokens from the monolingual News Crawl corpora version 209 using word2phrase.4 As phrases, we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpor"
W19-5428,N12-1047,0,0.035276,"thetic sentence pairs, obtained through back-translating (Sennrich et al., 2016a) the first 5M sentences from the monolingual corpora, to the original parallel data for training. We performed NMT decoding with an ensemble of a total of four models according to the best BLEU (Papineni et al., 2002) scores on the development data produced by four independent training runs using the same training parameters. 3.2 Table 2: Parameters of Marian used for training our NMT systems. (Heafield et al., 2013). Our systems used the default distortion limit of 6. We tuned the SMT model weights with KB-MIRA (Cherry and Foster, 2012) and selected the weights giving the best BLEU score on the development data after 15 iterations. SMT 3.3 We trained SMT systems using Moses.3 Word alignments and phrase tables were obtained from the tokenized parallel data using mgiza. Source-to-target and target-to-source word alignments were symmetrized with the grow-diag-final-and heuristic. We also trained MSLR (monotone, swap, discontinuousleft, discontinuous-right) lexicalized reordering model. We trained one 4-gram language models on the entire monolingual data concatenated to the target side of the parallel data using LMPLZ 1 https://"
W19-5428,P13-2121,0,0.0488158,"Missing"
W19-5428,P18-4020,0,0.033502,"Missing"
W19-5428,P07-2045,0,0.0139968,"ry close languages with large monolingual data, and to compare it with supervised MT systems trained on large bilingual data. We participated under the team name “NICT.” All our systems were constrained, i.e., we used only the parallel and monolingual data provided by the organizers to train and tune the MT systems. For both translation directions, we trained supervised neural MT (NMT) and statistical MT (SMT) systems, and combined them through n-best list reranking using different informative features as 2.2 Tokenization, Truecasing, and Cleaning We used the tokenizer and truecaser of Moses (Koehn et al., 2007). The truecaser was trained on one million tokenized lines extracted randomly from the monolingual data. Truecasing was then performed on all the tokenized data. For cleaning, we only applied the Moses script clean-corpus-n.perl to remove lines in the parallel data containing more than 80 tokens and replaced characters forbidden by Moses. Note that we did not perform any punctuation normalization. Table 1 presents the statistics of the parallel and monolingual data, respectively, after preprocessing. 208 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task P"
W19-5428,W17-3204,0,0.0207584,"ation systems, instead of using only either forward (Marie and Fujita, 2018b) or backward translations (Artetxe et al., 2018b). We report on the performance of the systems obtained after the fourth refinement step. 4 4.1 Generation of n-best Lists We first produced the six 100-best lists of translation hypotheses generated by four NMT leftto-right models individually, by their ensemble, and by one right-to-left model. Unlike Moses, Marian must use a beam of size k to produce a kbest list during decoding. However, using a larger beam size during decoding for NMT may worsen translation quality (Koehn and Knowles, 2017). Consequently, we also produced with Marian the 12-best lists and merged them with Marian’s 100-best lists to obtain lists containing up to 112 hypotheses,6 or up to 672 hypotheses after merging all the lists produced by NMT. In this way, we make sure that we still have hypotheses of good quality in the lists despite using a larger beam size. We also generated 100-best translation hypotheses with SMT.7 Finally, we merged the lists produced by Marian and Moses. 4.2 Reranking Framework and Features We rescored all the hypotheses in the resulting lists with a reranking framework using SMT and NM"
W19-5428,W18-1811,1,0.872248,"data using LMPLZ 1 https://marian-nmt.github.io/, 1.6.0 2 NVIDIA® Tesla® P100 16Gb. 3 https://github.com/moses-smt/ mosesdecoder/ Unsupervised SMT We also built an SMT system, without any supervision, i.e., using only but all the provided monolingual data for training. We chose unsupervised SMT (USMT) over unsupervised NMT (UNMT) since previous work (Artetxe et al., 2018b) has shown that USMT slightly outperforms UNMT and that we expect USMT to work well for this language pair that involves only very few word reorderings. We built USMT systems using a framework similar to the one proposed in Marie and Fujita (2018b). The first step of USMT is the induction of a phrase table from the monolingual corpora. We first collected phrases of up to six tokens from the monolingual News Crawl corpora version 209 using word2phrase.4 As phrases, we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual word embeddings, and the 300 target phrases with the highest scores were kept in the phrase table for each source phrase"
W19-5428,P02-1040,0,0.115857,"encoding (BPE) (Sennrich et al., 2016b) using 30k operations. BPE segmentations were jointly learned on the training parallel data for the source and target languages. All our NMT systems were consistently trained on 4 GPUs,2 with the parameters for Marian listed in Table 2. To improve translation quality, we added 5M synthetic sentence pairs, obtained through back-translating (Sennrich et al., 2016a) the first 5M sentences from the monolingual corpora, to the original parallel data for training. We performed NMT decoding with an ensemble of a total of four models according to the best BLEU (Papineni et al., 2002) scores on the development data produced by four independent training runs using the same training parameters. 3.2 Table 2: Parameters of Marian used for training our NMT systems. (Heafield et al., 2013). Our systems used the default distortion limit of 6. We tuned the SMT model weights with KB-MIRA (Cherry and Foster, 2012) and selected the weights giving the best BLEU score on the development data after 15 iterations. SMT 3.3 We trained SMT systems using Moses.3 Word alignments and phrase tables were obtained from the tokenized parallel data using mgiza. Source-to-target and target-to-source"
W19-5428,P16-1009,0,0.0357656,"mizer-params 0.9 0.98 1e-09 --clip-norm 5 --sync-sgd --tied-embeddings --exponential-smoothing 84.69M 68,284 171.15M Table 1: Statistics of our preprocessed data. 3 3.1 MT Systems NMT For our NMT systems, we adopt the Transformer architecture (Vaswani et al., 2017). We chose Marian (Junczys-Dowmunt et al., 2018)1 to train our NMT systems since it supports state-of-the-art features and is one of the fastest NMT frameworks publicly available. In order to limit the size of the vocabulary of the NMT models, we segmented tokens in the parallel data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b) using 30k operations. BPE segmentations were jointly learned on the training parallel data for the source and target languages. All our NMT systems were consistently trained on 4 GPUs,2 with the parameters for Marian listed in Table 2. To improve translation quality, we added 5M synthetic sentence pairs, obtained through back-translating (Sennrich et al., 2016a) the first 5M sentences from the monolingual corpora, to the original parallel data for training. We performed NMT decoding with an ensemble of a total of four models according to the best BLEU (Papineni et al., 2002) scores on the d"
W19-6613,D18-1399,0,0.237594,"lel corpus is available. This restricts one of the source or the target languages to be English which, in our case, is not possible. Approaches involving bi-directional NMT modeling is shown to drasti1 News domain translation is also the most competitive tasks in WMT indicating its importance. 2 http://opus.nlpl.eu/News-Commentary-v11. php 3 https://github.com/aizhanti/JaRuNC Dublin, Aug. 19-23, 2019 |p. 129 cally improve low-resource translation (Niu et al., 2018). However, like most other, this work focuses on translation from and into English. Remaining options include (a) unsupervised MT (Artetxe et al., 2018; Lample et al., 2018; Marie and Fujita, 2018), (b) parallel sentence mining from non-parallel or comparable corpora (Utiyama and Isahara, 2003; Tillmann and Xu, 2009), (c) generating pseudo-parallel data (Sennrich et al., 2016), and (d) MT based on pivot languages (Utiyama and Isahara, 2007). The linguistic distance between Japanese and Russian makes it extremely difficult to learn bilingual knowledge, such as bilingual lexicons and bilingual word embeddings. Unsupervised MT is thus not promising yet, due to its heavy reliance on accurate bilingual word embeddings. Neither does parallel sente"
W19-6613,N12-1047,0,0.0289999,"s) = P exp(β cos(emb(t),emb(s)) , where emb(·) stands 0 t0 exp(β cos(emb(t ),emb(s)) for a bilingual embedding of a given phrase, obtained through averaging bilingual embeddings of constituent words learned from the two monolingual data using fastText17 and vecmap.18 For each of the retained phrase pair, p(s|t) was computed analogously. We also computed lexical translation probabilities relying on those learned from the given small parallel corpus. Up to four phrase tables were jointly exploited by the multiple decoding path ability of Moses. Weights for the features were tuned using KB-MIRA (Cherry and Foster, 2012) on the development set; we took the best weights after 15 iterations. Two hyper-parameters, namely, k for the number of pivot-based phrase pairs per source phrase and d for distortion limit, were determined by a grid search on k ∈ {10, 20, 40, 60, 80, 100} and d ∈ {8, 10, 12, 14, 16, 18, 20}. In contrast, we used predetermined hyper-parameters for phrase table induction from monolingual data, following 15 https://github.com/facebookresearch/ UnsupervisedMT 16 https://code.google.com/archive/p/ word2vec/ 17 https://fasttext.cc/ 18 https://github.com/artetxem/vecmap Proceedings of MT Summit XVI"
W19-6613,P17-2061,1,0.92136,"odel trained on a resource-rich language pair like French↔English (parent), which is to be fine-tuned for a low-resource language pair like Uzbek↔English (child). On the other hand, multilingual approaches (Johnson et al., 2017) propose to train a single model to translate multiple language pairs. However, these approaches are effective only when the parent target or source language is relatively resource-rich like English (En). Furthermore, the parents and children models should be trained on similar domains; otherwise, one has to take into account an additional problem of domain adaptation (Chu et al., 2017). In this paper, we work on a linguistically distant and thus challenging language pair Japanese↔Russian (Ja↔Ru) which has only 12k lines of news domain parallel corpus and hence is extremely resource-poor. Furthermore, the amount of indirect in-domain parallel corpora, i.e., Ja↔En and Ru↔En, are also small. As we demonstrate in Section 4, this severely limits the performance of prominent low-resource techniques, such as multilingual modeling, back-translation, and pivotbased PBSMT. To remedy this, we propose a novel multistage fine-tuning method for NMT that combines multilingual modeling (Jo"
W19-6613,J82-2005,0,0.634941,"Missing"
W19-6613,N16-1101,0,0.0717709,"system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios (Koehn and Knowles, 2017). c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Only by exploiting cross-lingual transfer learning techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT performance approach PBSMT performance in lowresource scenarios. However, such methods usually require an NMT model trained on a resource-rich language pair like French↔English (parent), which is to be fine-tuned for a low-resource language pair like Uzbek↔English (child). On the other hand, multilingual approaches (Johnson et al., 2017) propose to train a single model to translate multiple language pairs. However, these approaches are effective only when the parent target or source language is relatively resource-rich like English (En)."
W19-6613,D07-1103,0,0.0524556,"u and Ru→Ja. Cascade: 2-step decoding using the source-toEnglish and English-to-target systems. Synthesize: Obtain a new phrase table from synthetic parallel data generated by translating English side of the target–English training parallel data to the source language with the English-to-source system. Triangulate: Compile a new phrase table combining those for the source-to-English and English-to-target systems. Among these three, triangulation is the most computationally expensive method. Although we had filtered the component phrase tables using the statistical significance pruning method (Johnson et al., 2007), triangulation can generate an enormous number of phrase pairs. To reduce the computational cost during decoding and the negative effects of potentially noisy phrase pairs, we retained for each source phrase s only the k-best translations t according to the forward translation probability φ(t|s) calculated from the conditional probabilities in the component models as defined in Utiyama and Isahara (2007). For each of the retained phrase pairs, we also calculated the backward translation probability, φ(s|t), and lexical translation probabilities, φlex (t|s) and φlex (s|t), in the same manner a"
W19-6613,Q17-1024,0,0.0503006,"Missing"
W19-6613,W18-6325,0,0.124224,"rd alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios (Koehn and Knowles, 2017). c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Only by exploiting cross-lingual transfer learning techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT performance approach PBSMT performance in lowresource scenarios. However, such methods usually require an NMT model trained on a resource-rich language pair like French↔English (parent), which is to be fine-tuned for a low-resource language pair like Uzbek↔English (child). On the other hand, multilingual approaches (Johnson et al., 2017) propose to train a single model to translate multiple language pairs. However, these approaches are effective only when the parent target or source language is relatively resource-rich like English (En). Furthermore, the parents and children mode"
W19-6613,W17-3204,0,0.502618,"more than 3.7 BLEU points, over a strong baseline, for this extremely low-resource scenario. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios (Koehn and Knowles, 2017). c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Only by exploiting cross-lingual transfer learning techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT performance approach PBSMT performance in lowresource scenarios. However, such methods usually require an NMT model trained on a resource-rich language pair like French↔English (parent), which is to be fine-tuned for a low-resource language pair like Uzbek↔English (child). On the other hand, m"
W19-6613,P07-2045,0,0.0227712,"eudo-parallel data. Our approach, which combines domain adaptation, multilingualism, and back-translation, helps improve the translation quality by more than 3.7 BLEU points, over a strong baseline, for this extremely low-resource scenario. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios (Koehn and Knowles, 2017). c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Only by exploiting cross-lingual transfer learning techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT performance approach PBSMT performance in lowresource scenarios. However, such methods usually require an NMT model trained on a resource-ric"
W19-6613,C18-1054,0,0.0296836,"the poor quality of the bilingual word embeddings used to extract the phrase table, as envisaged in Section 3. None of pivot-based approaches with unidirectional NMT models could even remotely rival the M2M Transformer NMT model (b3). 4.4 Augmentation with Back-translation Given that the M2M Transformer NMT model (b3) achieved best results for most of the translation directions and competitive results for the rest, we further explored it through back-translation. We examined the utility of pseudo-parallel data for all the six translation directions, unlike the work of Lakew et al. (2017) and Lakew et al. (2018), which concentrate only on the zero-shot language pair, and the work of Niu et al. (2018), which compares only uni- or bi-directional models. We investigated whether each translation direction in M2M models will benefit from pseudoparallel data and if so, what kind of improvement takes place. Dublin, Aug. 19-23, 2019 |p. 133 ID System #1–#10 Ja∗→Ru and/or Ru∗→Ja Ja∗→En and/or En∗→Ja Ru∗→En and/or En∗→Ru All Pseudo 12k→82k 47k→82k 82k All of the above Parallel data Ja↔Ru Ja↔En 12k→82k 47k→82k×2 12k→82k×2 47k→82k 12k→82k×2 47k→82k×2 12k→82k 47k→82k Ru↔En 82k×2 82k×2 82k 82k Total size of traini"
W19-6613,P10-2041,0,0.0550434,"• 22.85 22.77 • 23.09 En→Ru 16.92 17.30 17.20 • 17.89 16.76 16.68 16.80 • 17.73 17.13 17.26 17.30 Table 8: BLEU scores of M2M Transformer NMT systems trained on the mixture of given parallel corpus and pseudo-parallel data generated by back-translation using (b3). Six “X∗→Y” columns show whether the pseudo-parallel data for each translation direction is involved. Bold indicates the scores higher than (b3) and “• ” indicates statistical significance of the improvement. First, we selected sentences to be backtranslated from in-domain monolingual data (Table 3), relying on the score proposed by Moore and Lewis (2010) via the following procedure. 1. For each language, train two 4-gram language models, using KenLM: an in-domain one on all the Global Voices data, i.e., both parallel and monolingual data, and a general-domain one on the concatenation of Global Voices, IWSLT, and Tatoeba data. 2. For each language, discard sentences containing OOVs according to the in-domain language model. 3. For each translation direction, select the T best monolingual sentences in the news domain, according to the difference between cross-entropy scores given by the in-domain and general-domain language models. Whereas Niu"
W19-6613,W18-2710,0,0.413744,"s opposed to PBSMT. Transfer learning approaches (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018) work well when a large helping parallel corpus is available. This restricts one of the source or the target languages to be English which, in our case, is not possible. Approaches involving bi-directional NMT modeling is shown to drasti1 News domain translation is also the most competitive tasks in WMT indicating its importance. 2 http://opus.nlpl.eu/News-Commentary-v11. php 3 https://github.com/aizhanti/JaRuNC Dublin, Aug. 19-23, 2019 |p. 129 cally improve low-resource translation (Niu et al., 2018). However, like most other, this work focuses on translation from and into English. Remaining options include (a) unsupervised MT (Artetxe et al., 2018; Lample et al., 2018; Marie and Fujita, 2018), (b) parallel sentence mining from non-parallel or comparable corpora (Utiyama and Isahara, 2003; Tillmann and Xu, 2009), (c) generating pseudo-parallel data (Sennrich et al., 2016), and (d) MT based on pivot languages (Utiyama and Isahara, 2007). The linguistic distance between Japanese and Russian makes it extremely difficult to learn bilingual knowledge, such as bilingual lexicons and bilingual w"
W19-6613,P02-1040,0,0.104547,".70 0.19 3.72 2.02 Ru→Ja 1.86 1.61 4.29 1.96 0.87 8.35 4.45 Ja→En 2.41 6.18 5.15 4.36 6.48 10.24 8.19 En→Ja 7.83 8.81 7.55 7.97 10.63 12.43 10.27 Ru→En 18.42 19.60 14.24 20.70 22.25 22.10 22.37 En→Ru 13.64 15.11 10.86 16.24 16.03 16.92 16.52 Table 5: BLEU scores of baseline systems. Bold indicates the best BLEU score for each translation direction. tuned by a linear search on the BLEU score for the development set. Similarly to PBSMT, we also evaluated “Cascade” and “Synthesize” methods with unidirectional NMT models. 4.3 Results We evaluated MT models using case-sensitive and tokenized BLEU (Papineni et al., 2002) on test sets, using Moses’s multi-bleu.perl. Statistical significance (p &lt; 0.05) on the difference of BLEU scores was tested by Moses’s bootstraphypothesis-difference-significance.pl. Tables 5 and 6 show BLEU scores of all the models, except the NMT systems augmented with back-translations. Whereas some models achieved reasonable BLEU scores for Ja↔En and Ru↔En translation, all the results for Ja↔Ru, which is our main concern, were abysmal. Among the NMT models, Transformer models (b∗) were proven to be better than RNMT models (a∗). RNMT models could not even outperform the uni-directional PB"
W19-6613,P16-1009,0,0.406018,"is also the most competitive tasks in WMT indicating its importance. 2 http://opus.nlpl.eu/News-Commentary-v11. php 3 https://github.com/aizhanti/JaRuNC Dublin, Aug. 19-23, 2019 |p. 129 cally improve low-resource translation (Niu et al., 2018). However, like most other, this work focuses on translation from and into English. Remaining options include (a) unsupervised MT (Artetxe et al., 2018; Lample et al., 2018; Marie and Fujita, 2018), (b) parallel sentence mining from non-parallel or comparable corpora (Utiyama and Isahara, 2003; Tillmann and Xu, 2009), (c) generating pseudo-parallel data (Sennrich et al., 2016), and (d) MT based on pivot languages (Utiyama and Isahara, 2007). The linguistic distance between Japanese and Russian makes it extremely difficult to learn bilingual knowledge, such as bilingual lexicons and bilingual word embeddings. Unsupervised MT is thus not promising yet, due to its heavy reliance on accurate bilingual word embeddings. Neither does parallel sentence mining, due to the difficulty of obtaining accurate bilingual lexicons. Pseudo-parallel data can be used to augment existing parallel corpora for training, and previous work has reported that such data generated by so-called"
W19-6613,N09-2024,0,0.0303021,"nal NMT modeling is shown to drasti1 News domain translation is also the most competitive tasks in WMT indicating its importance. 2 http://opus.nlpl.eu/News-Commentary-v11. php 3 https://github.com/aizhanti/JaRuNC Dublin, Aug. 19-23, 2019 |p. 129 cally improve low-resource translation (Niu et al., 2018). However, like most other, this work focuses on translation from and into English. Remaining options include (a) unsupervised MT (Artetxe et al., 2018; Lample et al., 2018; Marie and Fujita, 2018), (b) parallel sentence mining from non-parallel or comparable corpora (Utiyama and Isahara, 2003; Tillmann and Xu, 2009), (c) generating pseudo-parallel data (Sennrich et al., 2016), and (d) MT based on pivot languages (Utiyama and Isahara, 2007). The linguistic distance between Japanese and Russian makes it extremely difficult to learn bilingual knowledge, such as bilingual lexicons and bilingual word embeddings. Unsupervised MT is thus not promising yet, due to its heavy reliance on accurate bilingual word embeddings. Neither does parallel sentence mining, due to the difficulty of obtaining accurate bilingual lexicons. Pseudo-parallel data can be used to augment existing parallel corpora for training, and pre"
W19-6613,N07-1061,0,0.47734,"rtance of sharing news between different language speakers. Moreover, news domain is one of the most challenging tasks, Proceedings of MT Summit XVII, volume 1 Ru Ja En #sent. X X X X X X 913 173 276 0 4 287 1 1,654 X X X Total X X X test 600 - Usage development 313 173 276 - Table 1: Manually aligned News Commentary data. due to large presence of out-of-vocabulary (OOV) tokens and long sentences.1 To establish and evaluate existing methods, we also involved English as the third language. As direct parallel corpora are scarce, involving a language such as English for pivoting is quite common (Utiyama and Isahara, 2007). There has been no clean held-out parallel data for Ja↔Ru and Ja↔En news translation. Therefore, we manually compiled development and test sets using News Commentary data2 as a source. Since the given Ja↔Ru and Ja↔En data share many lines in the Japanese side, we first compiled tri-text data. Then, from each line, corresponding parts across languages were manually identified, and unaligned parts were split off into a new line. Note that we have never merged two or more lines. As a result, we obtained 1,654 lines of data comprising trilingual, bilingual, and monolingual segments (mainly senten"
W19-6613,D16-1163,0,0.0944142,"ing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios (Koehn and Knowles, 2017). c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Only by exploiting cross-lingual transfer learning techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT performance approach PBSMT performance in lowresource scenarios. However, such methods usually require an NMT model trained on a resource-rich language pair like French↔English (parent), which is to be fine-tuned for a low-resource language pair like Uzbek↔English (child). On the other hand, multilingual approaches (Johnson et al., 2017) propose to train a single model to translate multiple language pairs. However, these approaches are effective only when the parent target or source language is relatively resource-rich like English (En). Furthermore, the p"
W19-6613,P03-1010,0,\N,Missing
W19-6613,P07-1092,0,\N,Missing
W19-6613,L16-1350,0,\N,Missing
Y10-1075,I08-1007,1,0.872999,"Missing"
Y10-1075,P94-1038,0,0.256331,"Missing"
Y10-1075,P08-1047,1,0.887762,"Missing"
Y10-1075,D08-1047,0,0.0534269,"Missing"
Y10-1075,I08-1025,0,0.0329099,"Missing"
Y10-1075,C02-1119,0,0.053663,"Missing"
Y10-1075,J01-2002,0,0.052682,"Missing"
Y10-1075,P97-1017,0,\N,Missing
Y18-3003,D14-1179,0,0.016896,"Missing"
Y18-3003,P17-2061,1,0.937207,"ish-Japanese translation, UCSY MyanmarEnglish translation and Indic multilingual translation directions. The techniques we focused on for each translation task can be summarized as below: • For the ASPEC translation tasks, we mostly relied on multilingual Transformer (Vaswani et al., 2017) models and experimented with Recurrently Stacked NMT (RS-NMT) (Dabre and Fujita, 2018) models in order to determine the trade-off between compactness of models and the loss in their performance. • For the UCSY Myanmar-English translation task, we tried domain adaptation techniques such as Mixed Fine Tuning (Chu et al., 2017) since the the final objective was to achieve high quality translation for a low-resource domain (ALT). • For the Indic multilingual task, we explored the feasibility of bilingual, N -to-1, 1-to-N and N to-N way translation models. We also tried an approach where we mapped the scripts of all Indic languages to a common script (Devanagari) to see if it helps improve the performance of a multilingual model. For additional details of how our submissions are ranked relative to the submissions of other WAT 952 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on"
Y18-3003,P07-2045,0,0.00542266,"perience a large loss in translation quality despite having significantly fewer parameters compared to the vanilla NMT models. An interesting observation is that systems with the best BLEU might not be the best in terms of human evaluation. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although vanilla NMT is significantly better than PBSMT in resource-rich scenarios, PBSMT performs better in resource-poor scenarios (Zoph et Anoop Kunchukuttan Microsoft AI and Research, India ankunchu@microsoft.com Eiichiro Sumita NICT, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan eiichiro.sumita@nict.go.jp al., 2016). By exploiting transfer learning techniques, the performance of NMT approaches can be improved substantially. For WAT 2018, we participated as team “NICT5” and worked on ASPEC Chinese-Japanese and English-Japanese translation, UCSY MyanmarEnglish translation an"
Y18-3003,N15-3017,1,0.843788,"ingle model to translate from English to all the Indic languages. This is essentially the reverse of the XX-En model. We also trained this model for 500k iterations. gle model to translate from all the Indic languages to English and vice versa. Unlike the previous multilingual models, we trained this model only for 180k iterations due to lack of time. • Multilingual Shared Indic Script XX-En model: This model is similar to the XX-En model except that the scripts for all the Indic languages are mapped to a common script. We used Devanagari as the common script, and used the Indic NLP Library5 (Kunchukuttan et al., 2015) for script conversion. As such, this increases the chance of vocabulary sharing. Because the training corpus diversity is significantly reduced we trained this model for 100k iterations because it is technically equivalent 5 • Multilingual XX-YY model: We trained a sinhttps://github.com/anoopkunchukuttan/ indic_nlp_library/ 957 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 Task Bengali-English Bengali-English Bengali-English Bengali-English Hindi-Englis"
Y18-3003,Y18-3001,1,0.838941,"d the feasibility of bilingual, N -to-1, 1-to-N and N to-N way translation models. We also tried an approach where we mapped the scripts of all Indic languages to a common script (Devanagari) to see if it helps improve the performance of a multilingual model. For additional details of how our submissions are ranked relative to the submissions of other WAT 952 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 participants, kindly refer to the overview paper (Nakazawa et al., 2018). 2 NMT Models and Approaches We will first describe the Transformer which is the state-of-the-art NMT model we used for our experiments. 2.1 The Transformer The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a sequence-tosequence neural model that consists of two components, the encoder and the decoder. The encoder converts the input word sequence into a sequence of vectors of high dimensionality. The decoder, on the other hand, produces the target word sequence by predicting the words using a combination of the previously predicted word and relevant p"
Y18-3003,D16-1163,0,0.0754938,"Missing"
Y18-3007,N12-1047,0,0.0634823,"and target-to-source word alignments were symmetrized with the grow-diag-final-and heuristic. We trained phrase-based SMT models and MSLR (monotone, swap, discontinuous-left, discontinuous-right) lexicalized reordering models. We also used the default distortion limit of 6. We trained two 4-gram language models, one on the WMT monolingual data for English, and on the Wikipedia data for Myanmar, concatenated to the target side of the parallel data, and another one on the target side of the parallel data only, using LMPLZ (Heafield et al., 2013). To tune the SMT model weights, we used kb-mira (Cherry and Foster, 2012) and selected the weights giving the best BLEU score for the development data during 15 iterations. 8k and 32k. 7 R Tesla R P100 16Gb. NVIDIA Back-translation # backtr. my→en en→my None (baseline) 0 300k 1M 300k 1M 19.0 20.1 23.3 23.2 25.1∗ 27.6 29.0∗ 27.9 22.5 17.6 Wikipedia CommonCrawl Table 3: BLEU scores for our NMT systems on the official test set of the tasks. The “Corpus” column denotes the origin of the back-translated data and the “#backtr.” column denotes the number of back-translated sentences mixed with the bilingual data for training. “∗” indicates the best configuration for each"
Y18-3007,D18-1045,0,0.0257642,"Missing"
Y18-3007,P13-2121,0,0.0392325,"were trained on the tokenized parallel data using mgiza. Source-totarget and target-to-source word alignments were symmetrized with the grow-diag-final-and heuristic. We trained phrase-based SMT models and MSLR (monotone, swap, discontinuous-left, discontinuous-right) lexicalized reordering models. We also used the default distortion limit of 6. We trained two 4-gram language models, one on the WMT monolingual data for English, and on the Wikipedia data for Myanmar, concatenated to the target side of the parallel data, and another one on the target side of the parallel data only, using LMPLZ (Heafield et al., 2013). To tune the SMT model weights, we used kb-mira (Cherry and Foster, 2012) and selected the weights giving the best BLEU score for the development data during 15 iterations. 8k and 32k. 7 R Tesla R P100 16Gb. NVIDIA Back-translation # backtr. my→en en→my None (baseline) 0 300k 1M 300k 1M 19.0 20.1 23.3 23.2 25.1∗ 27.6 29.0∗ 27.9 22.5 17.6 Wikipedia CommonCrawl Table 3: BLEU scores for our NMT systems on the official test set of the tasks. The “Corpus” column denotes the origin of the back-translated data and the “#backtr.” column denotes the number of back-translated sentences mixed with the b"
Y18-3007,P18-4020,0,0.0379332,"Missing"
Y18-3007,P07-2045,0,0.0108559,"we decided to remove lines in both corpora that fulfill at least one of the following conditions: Data set Train Development Test • contains more than 80 tokens For contrastive experiments, we also prepared CommonCrawl and Wikipedia corpora for English and cleaned them in the same manner. For the CommonCrawl corpus, we sampled 2M lines from the entire CommonCrawl corpus provided by WMT18, while for the Wikipedia corpus we sampled 1M lines from the entire dump of the English Wikipedia of 2017/06/01. We tokenized and truecased English data respectively with the tokenizer and truecaser of Moses (Koehn et al., 2007). The truecaser was trained on the English side of the parallel data. Truecasing was performed on all the tokenized data. For Myanmar, the provided bilingual data were already tokenized into writing units and Romanized.4 However, we were not able to take advantage of this preprocessing and chose to reverse it and tokenize the bilingual and monolingual data by ourselves with an in-house tokenizer. We did not apply truecasing on the Myanmar data. Note that for the en→my task, the outputs generated in the Myanmar language must be processed as done by the organizers before submission. For cleaning"
Y18-3007,W18-1811,1,0.796036,"aper describes neural (NMT) and statistical machine translation systems (SMT) built for the participation of the National Institute of Information and Communications Technology (NICT) to the WAT2018 Myanmar–English translation task (Nakazawa et al., 2018).1 We present systems built using only the parallel data provided by the organizers. For contrastive experiments, we also present systems that use monolingual data not provided by the organizers. For both translation directions, we trained NMT and SMT systems, and combined them through n-best list reranking using several informative features (Marie and Fujita, 2018). This simple combination method achieved the best results among the submitted MT systems for this task according to BLEU (Papineni et al., 2002). We also 1 The team ID of our participation is “NICT-4”. show that the use of monolingual data can dramatically improve translation quality. The remainder of this paper is organized as follows. In Section 2, we introduce the data preprocessing. In Section 3, we describe the details of our NMT and SMT systems. The back-translation of monolingual data used by some of our systems is described in Section 4. Then, the combination of NMT and SMT is describ"
Y18-3007,P02-1040,0,0.101122,"and Communications Technology (NICT) to the WAT2018 Myanmar–English translation task (Nakazawa et al., 2018).1 We present systems built using only the parallel data provided by the organizers. For contrastive experiments, we also present systems that use monolingual data not provided by the organizers. For both translation directions, we trained NMT and SMT systems, and combined them through n-best list reranking using several informative features (Marie and Fujita, 2018). This simple combination method achieved the best results among the submitted MT systems for this task according to BLEU (Papineni et al., 2002). We also 1 The team ID of our participation is “NICT-4”. show that the use of monolingual data can dramatically improve translation quality. The remainder of this paper is organized as follows. In Section 2, we introduce the data preprocessing. In Section 3, we describe the details of our NMT and SMT systems. The back-translation of monolingual data used by some of our systems is described in Section 4. Then, the combination of NMT and SMT is described in Section 5. Empirical results achieved by our systems are showed and analyzed in Section 6, and Section 7 concludes this paper. 2 Data prepr"
Y18-3007,P16-1009,0,0.107692,"nsformer architecture (Vaswani et al., 2017) since it has been shown to outperform, in quality and efficiency, the two other mainstream architectures for NMT known as deep recurrent neural network (deep RNN) and convolutional neural network (CNN). We chose Marian5 (JunczysDowmunt et al., 2018) to train and evaluate our NMT systems since it supports state-of-the-art features and is one of the fastest NMT framework publicly available. In order to limit the size of the vocabulary of the NMT models, we further segmented tokens in the parallel data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b) using 8k operations for both languages.6 All 5 https://marian-nmt.github.io/, version 1.6 The number of operations was chosen among 8k,16k,32k according to the best BLEU score obtained on the development set. We observed around 2 BLEU points of difference between 6 976 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 our NMT systems were consistently trained on 4 GPUs,7 with the following parameters for Marian: --type transformer --max-length 80 --mini-b"
Y18-3007,P16-1162,0,0.356602,"nsformer architecture (Vaswani et al., 2017) since it has been shown to outperform, in quality and efficiency, the two other mainstream architectures for NMT known as deep recurrent neural network (deep RNN) and convolutional neural network (CNN). We chose Marian5 (JunczysDowmunt et al., 2018) to train and evaluate our NMT systems since it supports state-of-the-art features and is one of the fastest NMT framework publicly available. In order to limit the size of the vocabulary of the NMT models, we further segmented tokens in the parallel data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b) using 8k operations for both languages.6 All 5 https://marian-nmt.github.io/, version 1.6 The number of operations was chosen among 8k,16k,32k according to the best BLEU score obtained on the development set. We observed around 2 BLEU points of difference between 6 976 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 our NMT systems were consistently trained on 4 GPUs,7 with the following parameters for Marian: --type transformer --max-length 80 --mini-b"
