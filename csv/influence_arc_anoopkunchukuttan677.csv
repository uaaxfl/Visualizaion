2020.coling-tutorials.3,Q19-1007,1,0.817811,"can be combined. We will see how MT models for new languages can be rapidly adapted from pre-trained MNMT models. Additionally, we will spend some time on multi-source NMT which leverages multilingual redundancy in terms of input in order to yield high quality translations. We will end the tutorial with a discussion on possible future directions that we believe that MNMT research should take. 3 Tutorial Outline Some representative papers are mentioned against tutorial sections. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2"
2020.coling-tutorials.3,2020.findings-emnlp.445,1,0.721245,"or Applied Researcher in the MT team at Microsoft India, where he is involved in building MT systems for multiple Indian languages. His research interests span in different aspects of machine translation, particularly: multilingual models, low-resource translation and translation involving related languages. More broadly, he is interested in different multilingual, cross-lingual and multi-task NLP approaches. He is passionate about building software and resources for NLP in Indian languages. He is the developer of the Indic NLP Library (Kunchukuttan, 2020), co-developer of the IndicNLP-Suite (Kakwani et al., 2020) and a co-founder of the AI4Bharat-NLP Initiative, a community initiative to build Indian language NLP technologies. He has published papers on MT and multilingual learning at ACL, NAACL, EMNLP, TACL and IJCNLP. He has been a member of the organizing committees for COLING 2012 and Workshop on Asian Translation. He received his Ph.D from the Indian Institute of Technology Bombay. References Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively Multilingual Neural Machine Translation. In NAACL. Maruan Al-Shedivat and Ankur Parikh. 2019. Consistency by agreement in zero-shot neural machi"
2020.coling-tutorials.3,W18-6325,0,0.0229673,"3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 20"
2020.coling-tutorials.3,N19-1387,1,0.778287,"2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Fu"
2020.coling-tutorials.3,D18-1103,0,0.0161762,"ameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Future directions (15 min) 9. Summary and conclusion"
2020.coling-tutorials.3,I17-2050,0,0.0220177,", 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterj"
2020.coling-tutorials.3,W18-2711,0,0.020594,"cmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Future directions (15 min) 9. Summary and conclusion (10 min) Total Time: 180 minutes (excluding break). Type of the Tutorial: Cutting-edge. Pre-requisites: Familiarity with sequence to sequence learning. 4 Tutorial Instructors Dr. Raj Dabre Post-Doctoral Researcher, NICT, Kyoto, Japan email: raj.dabre@nict.go.jp website: https://prajdabre.wixsite.com/prajdabre Raj Dabre received his M.Tech. from IIT Bombay, India and his Ph.D. from Kyoto University, Japan. He is a post-doctoral researcher at NICT which is Japan’s natio"
2020.coling-tutorials.3,D18-1039,0,0.0186918,"ned against tutorial sections. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-re"
2020.coling-tutorials.3,W18-6327,0,0.0175966,"ntative papers are mentioned against tutorial sections. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 20"
2020.coling-tutorials.3,P19-1297,0,0.0197742,"Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Future directions (15 min) 9. Summary and conclusion (10 min) Total Time: 180 minutes (excluding break). Type of the Tutorial: Cutting-edge. Pre-requisites: Familiarity with sequence to sequence learning. 4 Tutorial Instructors Dr. Raj Dabre Post-Doctoral Researcher, NICT, Kyoto, Japan email: raj.dabre@nict.go.jp website: https://prajdabre.wixsite.com/prajdabre Raj Dabre received his M.Tech. fro"
2020.coling-tutorials.3,P16-1009,0,0.0502196,"tionally, we will spend some time on multi-source NMT which leverages multilingual redundancy in terms of input in order to yield high quality translations. We will end the tutorial with a discussion on possible future directions that we believe that MNMT research should take. 3 Tutorial Outline Some representative papers are mentioned against tutorial sections. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer"
2020.coling-tutorials.3,P16-1162,0,0.0231572,"tionally, we will spend some time on multi-source NMT which leverages multilingual redundancy in terms of input in order to yield high quality translations. We will end the tutorial with a discussion on possible future directions that we believe that MNMT research should take. 3 Tutorial Outline Some representative papers are mentioned against tutorial sections. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer"
2020.coling-tutorials.3,D18-1326,0,0.0261679,"tions. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30"
2020.coling-tutorials.3,P19-1579,0,0.0106457,"earning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Future directions (15 min) 9. Summary and conclusion (10 min) Total Time: 180 minutes (excluding break). Type of the Tutorial: Cutting-edge. Pre-requisites: Familiarity with sequence to sequence learning. 4 Tutorial Instructors Dr. Raj Dabre Post-Doctoral Researcher, NICT, Kyoto, Japan email: raj.dabre@nict.go.jp website: https://prajdabre.wixsite.com/prajdabre Raj Dabre recei"
2020.coling-tutorials.3,N16-1004,0,0.00916043,"2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Future directions (15 min) 9. Summary and conclusion (10 min) Total Time: 180 minutes (excluding break). Type of the Tutorial: Cutting-edge. Pre-requisites: Familiarity with sequence to sequence learning. 4 Tutorial Instructors Dr. Raj Dabre Post-Doctoral Researcher, NICT, Kyoto, Japan email: raj.dabre@nict.go.jp website: https://prajdabre.wixsite.com/prajdabre Raj Dabre received his M.Tech. from IIT Bombay, India and his Ph.D. from Kyoto University, Jap"
2020.coling-tutorials.3,D16-1163,0,0.0194072,"2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source transla"
2020.findings-emnlp.445,C16-1047,0,0.384219,"as IndicCorp, contains a total of 8.8 billion tokens across 11 major Indian languages and English. The articles in IndicCorp are primarily sourced from news crawls. Using IndicCorp, we first train and evaluate word embeddings for each of the 11 languages. Given the morphological richness of Indian languages we train FastText word embeddings which are known to be more effective for such languages. To evaluate these embeddings we curate a benchmark comprising of word similarity and analogy tasks (Akhtar et al., 2017; Grave et al., 2018), text classification tasks, sentence classification tasks (Akhtar et al., 2016; Mukku and Mamidi, 2017), and bilingual lexicon induction tasks. On most tasks, the word embeddings trained on our IndicCorp outperform similar embeddings trained on existing corpora for Indian languages. Next, we train multilingual language models for these 11 languages using the ALBERT model (Lan et al., 2020). We chose ALBERT as the base model as it is very compact and hence easier to use in downstream tasks. To evaluate these pretrained language models, we create an NLU benchmark comprising of the following tasks: article genre classification, headline prediction, named entity recognition"
2020.findings-emnlp.445,W17-0811,0,0.382274,"uation benchmark comprising of various NLU tasks. Our monolingual corpora, collectively referred to as IndicCorp, contains a total of 8.8 billion tokens across 11 major Indian languages and English. The articles in IndicCorp are primarily sourced from news crawls. Using IndicCorp, we first train and evaluate word embeddings for each of the 11 languages. Given the morphological richness of Indian languages we train FastText word embeddings which are known to be more effective for such languages. To evaluate these embeddings we curate a benchmark comprising of word similarity and analogy tasks (Akhtar et al., 2017; Grave et al., 2018), text classification tasks, sentence classification tasks (Akhtar et al., 2016; Mukku and Mamidi, 2017), and bilingual lexicon induction tasks. On most tasks, the word embeddings trained on our IndicCorp outperform similar embeddings trained on existing corpora for Indian languages. Next, we train multilingual language models for these 11 languages using the ALBERT model (Lan et al., 2020). We chose ALBERT as the base model as it is very compact and hence easier to use in downstream tasks. To evaluate these pretrained language models, we create an NLU benchmark comprising"
2020.findings-emnlp.445,W13-3520,0,0.0267876,"ommonCrawl, also contains much less data for most Indian languages than our crawls. The CC4949 1 https://commoncrawl.org Net (Wenzek et al., 2019) and C4 (Raffel et al., 2019) projects also provide tools to process common crawl, but the extracted corpora are not provided and require a large amount of processing power. Our monolingual corpora is about 4 times larger than the corresponding OSCAR corpus and two times larger than the corresponding CC-100 corpus (Conneau et al., 2020). Word Embeddings. Word embeddings have been trained for many Indian languages using limited corpora. The Polyglot (Al-Rfou et al., 2013) and FastText (Bojanowski et al., 2017) projects provide embeddings trained on Wikipedia. FastText also provides embeddings trained on Wikipedia + CommonCrawl corpora. We show that on most evaluation tasks IndicFT outperforms existing FastText based embeddings. Pretrained Transformers. Pre-trained transformers serve as general language understanding models that can be used in a wide variety of downstream NLP tasks (Radford et al., 2019). Several transformer-based language models such as GPT (Radford, 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), etc."
2020.findings-emnlp.445,Q17-1010,0,0.291047,"data for most Indian languages than our crawls. The CC4949 1 https://commoncrawl.org Net (Wenzek et al., 2019) and C4 (Raffel et al., 2019) projects also provide tools to process common crawl, but the extracted corpora are not provided and require a large amount of processing power. Our monolingual corpora is about 4 times larger than the corresponding OSCAR corpus and two times larger than the corresponding CC-100 corpus (Conneau et al., 2020). Word Embeddings. Word embeddings have been trained for many Indian languages using limited corpora. The Polyglot (Al-Rfou et al., 2013) and FastText (Bojanowski et al., 2017) projects provide embeddings trained on Wikipedia. FastText also provides embeddings trained on Wikipedia + CommonCrawl corpora. We show that on most evaluation tasks IndicFT outperforms existing FastText based embeddings. Pretrained Transformers. Pre-trained transformers serve as general language understanding models that can be used in a wide variety of downstream NLP tasks (Radford et al., 2019). Several transformer-based language models such as GPT (Radford, 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), etc. have been proposed. All these models r"
2020.findings-emnlp.445,bojar-etal-2014-hindencorp,0,0.0913868,"h sources). 2 Related Work Text Corpora. Few organized sources of monolingual corpora exist for most Indian languages. The EMILLE/CIIL corpus (McEnery et al., 2000) was an early effort to build corpora for South Asian languages, spanning 14 languages with a total of 92 million words. Wikipedia for Indian languages is small (the largest one, Hindi, has just 40 million words). The Leipzig corpus (Goldhahn et al., 2012) contains small collections of upto 1 million sentences for news and web crawls (average 300K sentences). In addition, there are some language specific corpora for Hindi and Urdu (Bojar et al., 2014; Jawaid et al., 2014). In particular, the HindMonoCorp (Bojar et al., 2014) is one of the few larger Indian language collections (787M tokens). The CommonCrawl 1 project crawls webpages in many languages by sampling various websites. Our analysis of a processed crawl for the years 2013-2016 (Buck et al., 2014) for Indian languages revealed that most Indian languages, with the exception of Hindi, Tamil and Malayalam, have few good sentences (≥10 words) - in the order of around 50 million words. The OSCAR project (Ortiz Suarez et al., 2019), a recent processing of CommonCrawl, also contains muc"
2020.findings-emnlp.445,buck-etal-2014-n,0,0.0803568,"Missing"
2020.findings-emnlp.445,2020.tacl-1.30,0,0.0641196,"ng et al., 2019), CLUE (Chinese) (Xu et al., 2020), and FLUE (French) (Le et al., 2020) are important for tracking the efficacy of NLP models across languages. Such a benchmark is missing for Indic languages and the goal of this work is to fill this void. Datasets are available for some tasks for a few languages. The following are some of the prominent publicly available datasets2 : word similarity (Akhtar et al., 2017), word analogy (Grave et al., 2018), text classification, sentiment analysis (Akhtar et al., 2016; Mukku and Mamidi, 2017), paraphrase detection (Anand Kumar et al., 2016), QA (Clark et al., 2020; Gupta et al., 2018), discourse mode classification (Dhanwal et al., 2020), etc.. We also create datasets for some tasks, most of which span all major Indian languages. We bun2 A comprehensive list of resources Indian language NLP can be found https://github.com/AI4Bharat/indicnlp_catalog for here: Language #S #T #V I/O Punjabi (pa) 29.2 773 3.0 22 Hindi (hi) 63.1 1,860 6.5 2 Bengali (bn) 39.9 836 6.6 2 Odia (or) 6.94 107 1.4 9 Assamese (as) 1.39 32.6 0.8 8 Gujarati (gu) 41.1 719 5.7 14 Marathi (mr) 34.0 551 5.8 7 Kannada (kn) 53.3 713 11.9 14 Telugu (te) 47.9 674 9.4 8 Malayalam (ml) 50.2 72"
2020.findings-emnlp.445,2020.acl-main.747,0,0.374404,"d it is a collection of (i) existing Indian language datasets for some tasks, (ii) manual translations of some English datasets into Indian languages done as a part of this work, and (iii) new datasets that were created semi-automatically for all major Indian languages as a part of this work. These new datasets were created using external metadata (such as website/Wikipedia structure) resulting in more complex NLU tasks. Across all these tasks, we show that our embeddings are competitive or better than existing pre-trained multilingual embeddings such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020). We hope that these embeddings and evaluations benchmarks will not only be useful in driving NLP research on Indic languages, but will also help in evaluating advances in NLP over a more diverse set of languages. In summary, this paper introduces IndicNLPSuite containing the following resources for Indic NLP which will be made publicly available: • IndicCorp: Large sentence-level monolingual corpora for 11 languages from two language families (Indo-Aryan branch and Dravidian) and Indian English with an average 9-fold increase in size over OSCAR. • IndicGLUE: An evaluation benchmark containing"
2020.findings-emnlp.445,2020.lrec-1.302,0,0.0348021,"ed language models such as GPT (Radford, 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), etc. have been proposed. All these models require large amounts of monolingual corpora for training. For Indic languages, two such multilingual models are available: XLM-R (Conneau et al., 2020) and multilingual BERT (Devlin et al., 2019). However, they are trained across ~100 languages and smaller Indic language corpora. NLU Benchmarks. Benchmarks such as GLUE (Wang et al., 2018), SuperGLUE (Wang et al., 2019), CLUE (Chinese) (Xu et al., 2020), and FLUE (French) (Le et al., 2020) are important for tracking the efficacy of NLP models across languages. Such a benchmark is missing for Indic languages and the goal of this work is to fill this void. Datasets are available for some tasks for a few languages. The following are some of the prominent publicly available datasets2 : word similarity (Akhtar et al., 2017), word analogy (Grave et al., 2018), text classification, sentiment analysis (Akhtar et al., 2016; Mukku and Mamidi, 2017), paraphrase detection (Anand Kumar et al., 2016), QA (Clark et al., 2020; Gupta et al., 2018), discourse mode classification (Dhanwal et al.,"
2020.findings-emnlp.445,P19-1493,0,0.158456,"oss entropy loss. Cloze-style Multiple-choice QA. We feed the masked text segment as input to the model and at the output we have a softmax layer which predicts a probability distribution over the given candidates. We fine-tune the model using cross entropy loss with the target label as 1 for the correct candidate and 0 for the incorrect candidates. Cross-lingual Sentence Retrieval. No finetuning is required for this task. We compute the representation of every sentence by mean-pooling the outputs in the last hidden layer and then using cosine distance to compute similarity between sentences (Libovický et al., 2019). Additionally, we also center the sentence vectors across each language to remove language-specific bias in the vectors (Reimers and Gurevych, 2019). Winograd NLI, COPA, Paraphrase Detection: We input the sentence pair into the model as segment A and segment B. The [CLS] representation from the last layer is fed into an output layer for classification into one of the categories. News Category Classification, Discourse Mode Classification, Sentiment Analysis. We feed the representation of the [CLS] token from the last layer to a linear classifier with a softmax layer to predict a probability d"
2020.findings-emnlp.445,2021.ccl-1.108,0,0.160203,"Missing"
2020.findings-emnlp.445,2000.bcs-1.11,0,0.431026,"nce-level monolingual corpora for 11 languages from two language families (Indo-Aryan branch and Dravidian) and Indian English with an average 9-fold increase in size over OSCAR. • IndicGLUE: An evaluation benchmark containing a variety of NLU tasks. • IndicFT and IndicBERT: FastText-based word embeddings (11 languages) and ALBERT-based language models (12 languages) trained on IndicCorp. The IndicBERT embeddings are multilingual (includes Indian English sources). 2 Related Work Text Corpora. Few organized sources of monolingual corpora exist for most Indian languages. The EMILLE/CIIL corpus (McEnery et al., 2000) was an early effort to build corpora for South Asian languages, spanning 14 languages with a total of 92 million words. Wikipedia for Indian languages is small (the largest one, Hindi, has just 40 million words). The Leipzig corpus (Goldhahn et al., 2012) contains small collections of upto 1 million sentences for news and web crawls (average 300K sentences). In addition, there are some language specific corpora for Hindi and Urdu (Bojar et al., 2014; Jawaid et al., 2014). In particular, the HindMonoCorp (Bojar et al., 2014) is one of the few larger Indian language collections (787M tokens). T"
2020.findings-emnlp.445,W17-5408,0,0.314831,"s a total of 8.8 billion tokens across 11 major Indian languages and English. The articles in IndicCorp are primarily sourced from news crawls. Using IndicCorp, we first train and evaluate word embeddings for each of the 11 languages. Given the morphological richness of Indian languages we train FastText word embeddings which are known to be more effective for such languages. To evaluate these embeddings we curate a benchmark comprising of word similarity and analogy tasks (Akhtar et al., 2017; Grave et al., 2018), text classification tasks, sentence classification tasks (Akhtar et al., 2016; Mukku and Mamidi, 2017), and bilingual lexicon induction tasks. On most tasks, the word embeddings trained on our IndicCorp outperform similar embeddings trained on existing corpora for Indian languages. Next, we train multilingual language models for these 11 languages using the ALBERT model (Lan et al., 2020). We chose ALBERT as the base model as it is very compact and hence easier to use in downstream tasks. To evaluate these pretrained language models, we create an NLU benchmark comprising of the following tasks: article genre classification, headline prediction, named entity recognition, Wikipedia section-title"
2020.findings-emnlp.445,P17-1178,0,0.0232335,"lingual Sentence Retrieval (#English to Indian language parallel sentences) 5,169 5,522 752 6,463 5,760 5,049 4,886 5,637 39,238 Table 2: IndicGLUE Datasets’ Statistics. The first four datasets have been created as part of this project. correct candidates from entities that occur in the same article and have the same type as the correct entity. The type of an entity is taken from Wikidata. This task is similar to the one proposed by Petroni et al. (2019) for English, and aims to check if language models can be used as knowledge bases. Named Entity Recognition. We use the WikiAnn NER dataset7 (Pan et al., 2017) which contains NER data for 282 languages. This dataset is created from Wikipedia by utilizing cross language links to propagate English named entity labels to other languages. We consider the following coarsegrained labels in this dataset: Person (PER), Organisation (ORG) and Location (LOC). Cross-lingual Sentence Retrieval. Given a sentence in English, the task is to retrieve its translation from a set of candidate sentences in an Indian language. We use the CVIT-Mann Ki Baat dataset8 (Siripragrada et al., 2020) for this task. Winograd NLI (WNLI). The WNLI task (Levesque et al., 2011) is pa"
2020.findings-emnlp.445,D14-1162,0,0.0892077,"guage on IndicCorp using FastText (Bojanowski et al., 2017). Since Indian languages are morphologically rich, we chose FastText, which is capable of integrating subword information by using character n-gram embeddings during training. We train skipgram models for 10 epochs with a window size of 5, minimum token count of 5 and 10 negative examples sampled for each instance. We chose these hyper-parameters based on suggestions by Grave et al. (2018). Based on previously published results, we expect FastText to be better than word-level algorithms like word2vec (Mikolov et al., 2013b) and GloVe (Pennington et al., 2014) for morphologically rich languages. 5.2 hi BBC Articles IITP+ Movie IITP Product bn gu ml mr ta te FT-W FT-WC IndicFT 72.29 41.61 58.32 67.44 44.52 57.17 77.02 45.81 61.57 Soham Articles 62.79 64.78 71.82 iNLTK Headlines 81.94 86.35 83.06 90.88 84.07 83.65 81.65 89.09 90.74 95.87 91.40 95.37 ACTSA 46.03 42.51 52.58 Average 69.25 68.32 75.80 Table 4: Text classification accuracy on public datasets. On average, IndicFT embeddings outperform the baseline embeddings. We train FastText word embeddings for each language using IndicCorp, and evaluate their quality on: (a) word similarity, (b) word a"
2020.findings-emnlp.445,N18-1202,0,0.0844237,"Missing"
2020.findings-emnlp.445,D19-1250,0,0.0184964,"431 109,508 8,687 6,295 39,708 108,579 28,854 te 24,000 ml ta total 6,000 11,700 125,630 63,415 100,000 74,767 880,311 41,338 81,627 138,888 186,423 787,462 Cross-lingual Sentence Retrieval (#English to Indian language parallel sentences) 5,169 5,522 752 6,463 5,760 5,049 4,886 5,637 39,238 Table 2: IndicGLUE Datasets’ Statistics. The first four datasets have been created as part of this project. correct candidates from entities that occur in the same article and have the same type as the correct entity. The type of an entity is taken from Wikidata. This task is similar to the one proposed by Petroni et al. (2019) for English, and aims to check if language models can be used as knowledge bases. Named Entity Recognition. We use the WikiAnn NER dataset7 (Pan et al., 2017) which contains NER data for 282 languages. This dataset is created from Wikipedia by utilizing cross language links to propagate English named entity labels to other languages. We consider the following coarsegrained labels in this dataset: Person (PER), Organisation (ORG) and Location (LOC). Cross-lingual Sentence Retrieval. Given a sentence in English, the task is to retrieve its translation from a set of candidate sentences in an Ind"
2020.findings-emnlp.445,D19-1410,0,0.0206159,"which predicts a probability distribution over the given candidates. We fine-tune the model using cross entropy loss with the target label as 1 for the correct candidate and 0 for the incorrect candidates. Cross-lingual Sentence Retrieval. No finetuning is required for this task. We compute the representation of every sentence by mean-pooling the outputs in the last hidden layer and then using cosine distance to compute similarity between sentences (Libovický et al., 2019). Additionally, we also center the sentence vectors across each language to remove language-specific bias in the vectors (Reimers and Gurevych, 2019). Winograd NLI, COPA, Paraphrase Detection: We input the sentence pair into the model as segment A and segment B. The [CLS] representation from the last layer is fed into an output layer for classification into one of the categories. News Category Classification, Discourse Mode Classification, Sentiment Analysis. We feed the representation of the [CLS] token from the last layer to a linear classifier with a softmax layer to predict a probability distribution over the categories. We fine-tune the model using multi-class cross entropy loss. 6.3 Evaluation We summarize the main observations from"
2020.findings-emnlp.445,2020.lrec-1.462,0,0.101151,"n be used as knowledge bases. Named Entity Recognition. We use the WikiAnn NER dataset7 (Pan et al., 2017) which contains NER data for 282 languages. This dataset is created from Wikipedia by utilizing cross language links to propagate English named entity labels to other languages. We consider the following coarsegrained labels in this dataset: Person (PER), Organisation (ORG) and Location (LOC). Cross-lingual Sentence Retrieval. Given a sentence in English, the task is to retrieve its translation from a set of candidate sentences in an Indian language. We use the CVIT-Mann Ki Baat dataset8 (Siripragrada et al., 2020) for this task. Winograd NLI (WNLI). The WNLI task (Levesque et al., 2011) is part of the GLUE benchmark. Each example in the dataset consists of a pair of sentences where the second sentence is constructed from the first sentence by replacing an ambiguous pronoun with a possible referent within the sentence. The task is to predict if the second sentence is entailed by the original sentence. We manually translated this dataset to 3 Indic languages (hi, mr, gu) with the help of skilled bilingual speakers. The annotators were paid 3 cents per word and the translations were then verified by an ex"
2020.findings-emnlp.445,W18-5446,0,0.0673258,"Missing"
2020.repl4nlp-1.6,J15-4004,0,0.0431968,"he performance of the proposed meta-embedding models. 3.1 Evaluation Tasks and Datasets We consider the following standard evaluation tasks (Yin and Shutze, 2016; Coates and Bollegala, 2018): • Word similarity: in this task, we compare the human-annotated similarity scores between pairs of words with the corresponding cosine similarity computed via the constructed metaembeddings. We report results on the following benchmark datasets: RG (Rubenstein and Goodenough, 1965), MC (Miller and Charles, 1991), WS (Finkelstein et al., 2001), MTurk (Halawi et al., 2012), RW (Luong et al., 2013), and SL (Hill et al., 2015). Following previous works (Yin and Shutze, 2016; Coates and Bollegala, 2018; O’Neill and Bollegala, 2020), we report the Spearman correlation score (higher is better) between the cosine similarity (computed via meta-embeddings) and the human scores. • Word analogy: in this task, the aim is to answer questions which have the form “A is to B as C is to ?” (Mikolov et al., 2013). After generating the meta-embeddings a, b, and c (corresponding to terms A, B, and C, respectively), the answer is chosen to be the term whose meta-embedding has the maximum cosine similarity with (b − a + c) (Mikolov e"
2020.repl4nlp-1.6,C18-1140,0,0.0633096,"d are typically used as features in training NLP models for diverse tasks like named entity tagging, sentiment analysis, and classification, to name a few. Word embeddings are learned in an unsupervised manner from large text corpora and a number of pre-trained embeddings are readily available. The quality of the word embeddings, however, depends on various factors like the size and genre of training corpora as well as the training method used. This has led to ensemble approaches for creating meta-embeddings from different original embeddings (Yin and Shutze, 2016; Coates and Bollegala, 2018; Bao and Bollegala, 2018; O’Neill and Bollegala, 2020). Meta-embeddings are appealing because they can improve quality of embeddings on account of noise cancellation and diversity of data sources and algorithms. Various approaches have been proposed to learn meta-embeddings and can be broadly classified into two categories: (a) simple linear methods like averaging or concatenation, or a low-dimensional projection via singular value projection (Yin and 39 Proceedings of the 5th Workshop on Representation Learning for NLP (RepL4NLP-2020), pages 39–44 c July 9, 2020. 2020 Association for Computational Linguistics the se"
2020.repl4nlp-1.6,Q19-1007,1,0.703347,"and zi ). Our framework imposes orthogonal transformations on the given source embeddings to enable alignment. To allow a more effective model for comparing similarity between different embeddings of a given word, we additionally induce this latent space with the Mahalanobis metric. The Mahalanobis similarity generalizes the cosine similarity measure, which is commonly used for evaluating the relatedness between word embeddings. Unlike cosine similarity, the Mahalanobis metric does not assume uncorrelated feature and it incorporates the feature correlation information from the training data (Jawanpuria et al., 2019). The combination of orthogonal transformation and Mahalanobis metric learning allows to capture any affine relationship that may exist between word embeddings. Mathematically, this relates to the singular value decomposition of a matrix (Bonnabel and Sepulchre, 2009; Mishra et al., 2014). Overall, we formulate the problem of learning geometric transformations – the orthogonal rotations and the metric scaling – via a binary classification problem (discussed later). The metaembeddings are subsequently computed using these transformations. The following sections formalize the proposed latent spa"
2020.repl4nlp-1.6,Q17-1010,0,0.314581,". . . , n and Yij = 0 for i 6= j. The proposed optimization problem employs the simple to optimize square loss function: 2 min X&gt; U&gt; BVZ − Y + C kBk2 , (1) show that the proposed geometrically aligned metaembeddings outperform strong baselines such as the plain averaging and the plain concatenation models. 2 Proposed Geometric Modeling Consider two (monolingual) embeddings xi ∈ Rd and zi ∈ Rd of a given word i in a d-dimensional space. As discussed earlier, embeddings generated from different algorithms (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014; Dhillon et al., 2015; Bojanowski et al., 2017) may express different characteristics (of the same word). Hence, the goal of learning a meta-embedding wi (corresponding to word i) is to generate a representation that inherits the properties of the different source embeddings (e.g., xi and zi ). Our framework imposes orthogonal transformations on the given source embeddings to enable alignment. To allow a more effective model for comparing similarity between different embeddings of a given word, we additionally induce this latent space with the Mahalanobis metric. The Mahalanobis similarity generalizes the cosine similarity measure, which i"
2020.repl4nlp-1.6,S12-1047,0,0.110447,"e report the Spearman correlation score (higher is better) between the cosine similarity (computed via meta-embeddings) and the human scores. • Word analogy: in this task, the aim is to answer questions which have the form “A is to B as C is to ?” (Mikolov et al., 2013). After generating the meta-embeddings a, b, and c (corresponding to terms A, B, and C, respectively), the answer is chosen to be the term whose meta-embedding has the maximum cosine similarity with (b − a + c) (Mikolov et al., 2013). The benchmark datasets include MSR (Gao et al., 2014), GL (Mikolov et al., 2013), and SemEval (Jurgens et al., 2012). Following previous works (Yin and Shutze, 2016; Coates and Bollegala, 2018; O’Neill and Bollegala, 2020), we report the percentage of correct answers for MSR and GL datasets, and the Spearman correlation score for SemEval. In both cases, a higher score implies better performance. We learn the meta-embeddings from the following publicly available 300-dimensional pre-trained word embeddings for English. • CBOW (Mikolov et al., 2013): has 929 023 word embeddings trained on Google News. • GloVe (Pennington et al., 2014): has Geometry-Aware Concatenation We next propose to concatenate the aligned"
2020.repl4nlp-1.6,W13-3512,0,0.0514646,"n this section, we evaluate the performance of the proposed meta-embedding models. 3.1 Evaluation Tasks and Datasets We consider the following standard evaluation tasks (Yin and Shutze, 2016; Coates and Bollegala, 2018): • Word similarity: in this task, we compare the human-annotated similarity scores between pairs of words with the corresponding cosine similarity computed via the constructed metaembeddings. We report results on the following benchmark datasets: RG (Rubenstein and Goodenough, 1965), MC (Miller and Charles, 1991), WS (Finkelstein et al., 2001), MTurk (Halawi et al., 2012), RW (Luong et al., 2013), and SL (Hill et al., 2015). Following previous works (Yin and Shutze, 2016; Coates and Bollegala, 2018; O’Neill and Bollegala, 2020), we report the Spearman correlation score (higher is better) between the cosine similarity (computed via meta-embeddings) and the human scores. • Word analogy: in this task, the aim is to answer questions which have the form “A is to B as C is to ?” (Mikolov et al., 2013). After generating the meta-embeddings a, b, and c (corresponding to terms A, B, and C, respectively), the answer is chosen to be the term whose meta-embedding has the maximum cosine similarity"
2020.repl4nlp-1.6,W17-0212,0,0.0357879,"Missing"
2020.repl4nlp-1.6,D14-1162,0,0.113114,"ar methods that aim to learn metaembeddings as shared representation using autoencoding or transformation between common representation and each embedding set (Murom¨agi et al., 2017; Bollegala et al., 2018; Bao and Bollegala, 2018; O’Neill and Bollegala, 2020). In this work, we focus on simple linear methods such as averaging and concatenation for computing meta-embeddings, which are very easy to implement and have shown highly competitive performance (Yin and Shutze, 2016; Coates and Bollegala, 2018). Due to the nature of the underlying embedding generation algorithms (Mikolov et al., 2013; Pennington et al., 2014), correspondences between dimensions, e.g., of two embeddings x ∈ Rd and z ∈ Rd of the same word, are usually not known. Hence, averaging may be detrimental in cases where the dimensions are negatively correlated. Consider the scenario where z := −x. Here, simple averaging of x and z would result in the zero vector. Similarly, when z is a (dimension-wise) permutation of x, simple averaging would result in a sub-optimal meta-embedding vector compared to averaging of aligned embeddings. Therefore, we propose to align the embeddings (of a given word) as an important first step towards generating"
2020.repl4nlp-1.6,P10-1040,0,0.104305,"to different words. In addition, let Y denote the label matrix, where Yii = 1 for i = 1, . . . , n and Yij = 0 for i 6= j. The proposed optimization problem employs the simple to optimize square loss function: 2 min X&gt; U&gt; BVZ − Y + C kBk2 , (1) show that the proposed geometrically aligned metaembeddings outperform strong baselines such as the plain averaging and the plain concatenation models. 2 Proposed Geometric Modeling Consider two (monolingual) embeddings xi ∈ Rd and zi ∈ Rd of a given word i in a d-dimensional space. As discussed earlier, embeddings generated from different algorithms (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014; Dhillon et al., 2015; Bojanowski et al., 2017) may express different characteristics (of the same word). Hence, the goal of learning a meta-embedding wi (corresponding to word i) is to generate a representation that inherits the properties of the different source embeddings (e.g., xi and zi ). Our framework imposes orthogonal transformations on the given source embeddings to enable alignment. To allow a more effective model for comparing similarity between different embeddings of a given word, we additionally induce this latent space with the Ma"
2020.repl4nlp-1.6,P16-1128,0,0.769084,"{pratik.jawanpuria,ankunchu,bamdevm}@microsoft.com 2 tvsatyadev@gmail.com Abstract Shutze, 2016; Coates and Bollegala, 2018) and (b) non-linear methods that aim to learn metaembeddings as shared representation using autoencoding or transformation between common representation and each embedding set (Murom¨agi et al., 2017; Bollegala et al., 2018; Bao and Bollegala, 2018; O’Neill and Bollegala, 2020). In this work, we focus on simple linear methods such as averaging and concatenation for computing meta-embeddings, which are very easy to implement and have shown highly competitive performance (Yin and Shutze, 2016; Coates and Bollegala, 2018). Due to the nature of the underlying embedding generation algorithms (Mikolov et al., 2013; Pennington et al., 2014), correspondences between dimensions, e.g., of two embeddings x ∈ Rd and z ∈ Rd of the same word, are usually not known. Hence, averaging may be detrimental in cases where the dimensions are negatively correlated. Consider the scenario where z := −x. Here, simple averaging of x and z would result in the zero vector. Similarly, when z is a (dimension-wise) permutation of x, simple averaging would result in a sub-optimal meta-embedding vector compared"
2020.repl4nlp-1.6,N18-2031,0,\N,Missing
2020.wat-1.1,L18-1548,1,0.762787,"emselves. Be cause our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. Table 13: Statistics for IITB Corpus. “Mono” indicates monolingual Hindi corpus. For the first year, WAT uses BSD Corpus 22 (The Business Scene Dialogue corpus) for the dataset including training, development and test data. Par ticipants of this taks must get a copy of BSD corpus by themselves. 2.12 IITB Hindi–English task In this task we use IIT Bombay EnglishHindi Cor pus (Kunchukuttan et al., 2018) which contains EnglishHindi parallel corpus as well as mono lingual Hindi corpus collected from a variety of sources and corpora (Bojar et al., 2014). This cor pus had been developed at the Center for Indian Language Technology, IIT Bombay over the years. The corpus is used for mixed domain tasks hi↔en. The statistics for the corpus are shown in Table 13. 3 4.1 Tokenization We used the following tools for tokenization. 4.1.1 For ASPEC, JPC, TDDC, JIJI, ALT, UCSY, ECCC, and IITB • Juman version 7.024 for Japanese segmenta tion. • Stanford Word Segmenter version 201401 0425 (Chinese Penn"
2020.wat-1.1,E06-1031,0,0.0814841,"ipants’ systems submitted for human evaluation. The sub mitted translations were evaluated by a profes sional translation company and Pairwise scores were given to the submissions by comparing with baseline translations (described in Section 4). Additional Automatic Scores in MultiModal and UFAL EnOdia Tasks For the multimodal task and UFAL EnOdia task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (this time calculated by Moses scorer43 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer44 on the can didate and reference before scoring. For all error metrics, i.e. metrics where better scores are lower, we reverse the score by taking 1 − x and indicate this by prepending “n” to the metric name. With this modification, higher scores always indicate a better translation result. Also, we multiply all met ric scores by 100 for better readability. These additional scores document again, that BLEU implementations (and the underlying tok enization schemes) heavily vary in their outcomes. The scores are thus comparable only wi"
2020.wat-1.1,W17-5701,1,0.728933,"on to documentlevel evaluation. 2.11 Documentlevel Translation Task In WAT2020, we set up 2 documentlevel transla tion tasks: ParaNatCom and BSD. 20 21 http://www2.nict.go.jp/astrec-att/member/ mutiyama/paranatcom/ https://github.com/nlabmpg/Flickr30kEntJP 7 Lang hien hi Train 1,609,682 – Dev 520 – Test 2,507 – Mono – 45,075,279 systems were published on the WAT web page.23 We also have SMT baseline systems for the tasks that started at WAT 2017 or before 2017. The base line systems are shown in Tables 16, 17, and 18. SMT baseline systems are described in the WAT 2017 overview paper (Nakazawa et al., 2017). The commercial RBMT systems and the online transla tion systems were operated by the organizers. We note that these RBMT companies and online trans lation companies did not submit themselves. Be cause our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. Table 13: Statistics for IITB Corpus. “Mono” indicates monolingual Hindi corpus. For the first year, WAT uses BSD Corpus 22 (The Business Scene Dialogue corpus) for the dataset including training"
2020.wat-1.1,Y18-3001,1,0.753826,"raining, develop ment, and test data of the KhmerEnglish transla tion tasks are listed in Table 6. 1 4 http://opus.nlpl.eu/ Lang.pair Ja↔Ru Ja↔En Ru↔En Table 8: task. Partition train development test train development test train development test #sent. 12,356 486 600 47,082 589 600 82,072 313 600 #tokens 341k / 229k 16k / 11k 22k / 15k 1.27M / 1.01M 21k / 16k 22k / 17k 1.61M / 1.83M 7.8k / 8.4k 15k / 17k #types 22k / 42k 2.9k / 4.3k 3.5k / 5.6k 48k / 55k 3.5k / 3.8k 3.5k / 3.8k 144k / 74k 3.2k / 2.3k 5.6k / 3.8k 2.7 Indic Multilingual Task In 2018, we had organized an Indic languages task (Nakazawa et al., 2018) but due to lack of reli able evaluation corpora we discontinued it in WAT 2019. However, in 2020, high quality publicly available evaluation (and training) corpora became available which motivated us to relaunch the task. The Indic task involves mixed domain corpora for evaluation consisting of various articles composed by Indian Prime Minister. The languages involved are Hindi (Hi), Marathi (Mr), Tamil (Ta), Telugu (Te), Gujarati (Gu), Malayalam (Ml), Bengali (Bg) and English (En). English is either the source or the target language during evaluation leading to a total of 14 translation dir"
2020.wat-1.1,W12-5611,1,0.853557,"Missing"
2020.wat-1.1,P11-2093,0,0.0137734,"eq_length = 150 src_vocab_size = 100000 tgt_vocab_size = 100000 src_words_min_frequency = 1 tgt_words_min_frequency = 1 30 https://github.com/tensorflow/ tensor2tensor https://taku910.github.io/mecab/ 12 (separated by 1000 batches) and performed decod ing with a beam of size 4 and a length penalty of 0.6. 4.2.3 Before the calculation of the automatic evalua tion scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmenta tion, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model33 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0.34 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 20140616 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.35 For Korean segmentation, we used mecabko.36 For Myanmar and Khmer segmen tations, we used myseg.py37 and kmseg.py38 . For English and Russian tokenizations, we used tokenizer.perl39 in the Moses toolkit. For Indonesian and Malay tokenizations, we used tokenizer.perl as same as the Engl"
2020.wat-1.1,2020.lrec-1.518,1,0.770784,") which are not publicly available at the time of WAT 2020. Note that phrasetoregion an notation is not included in the test data. There are two settings of submission: with and without resource constraints. In the constrained setting, external resources such as additional data and pretrained models (with external data) are not allowed to use, except for pretrained convo lutional neural networks (for visual analysis) and basic linguistic tools such as taggers, parsers, and morphological analyzers. As the baseline system to compute the Pairwise score, we implement the textonly model in (Nishihara et al., 2020) under the constrained setting. 2.11.1 Documentlevel Scientific Paper Translation Traditional ASPEC translation tasks are sentence level and the translation quality of them seem to be saturated. We think it’s high time to move on to documentlevel evaluation. For the first year, we use ParaNatCom 21 (Parallel EnglishJapanese ab stract corpus made from Nature Communications articles) for the development and test sets of the Documentlevel Scientific Paper Translation sub task. We cannot provide documentlevel training corpus, but you can use ASPEC and any other ex tra resources. 2.11.2 Do"
2020.wat-1.1,P02-1040,0,0.120093,"ase tophrase and phrasetoregion annotations avail able in the training dataset. 5 5.2 Automatic Evaluation System The automatic evaluation system receives transla tion results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure 2, the system requires participants to provide the following information for each sub mission: Automatic Evaluation 5.1 Procedure for Calculating Automatic Evaluation Score • Human Evaluation: whether or not they sub mit the results for human evaluation; We evaluated translation results by three met rics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.31 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2020 web page.32 All scores for each task were calculated using the corresponding reference translations. 33 http://www.phontron.com/kytea/model.html http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 35 http://nlp.stanford.edu/software/segmenter"
2020.wat-1.1,2020.lrec-1.462,0,0.0440733,"Missing"
2020.wat-1.1,2006.amta-papers.25,0,0.152126,"conducted pairwise evaluation for participants’ systems submitted for human evaluation. The sub mitted translations were evaluated by a profes sional translation company and Pairwise scores were given to the submissions by comparing with baseline translations (described in Section 4). Additional Automatic Scores in MultiModal and UFAL EnOdia Tasks For the multimodal task and UFAL EnOdia task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (this time calculated by Moses scorer43 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer44 on the can didate and reference before scoring. For all error metrics, i.e. metrics where better scores are lower, we reverse the score by taking 1 − x and indicate this by prepending “n” to the metric name. With this modification, higher scores always indicate a better translation result. Also, we multiply all met ric scores by 100 for better readability. These additional scores document again, that BLEU implementations (and the underlying tok enization schemes) heavily vary in their outcome"
2020.wat-1.1,J82-2005,0,0.692397,"Missing"
2020.wat-1.1,L16-1249,1,0.823126,"pur pose of this task was to test the feasibility of multi domain multilingual solutions for extremely low resource language pairs and domains. Naturally the solutions could be onetomany, manytoone or manytomany NMT models. The domains in question are Wikinews and IT (specifically, Soft ware Documentation). The total number of evalu ation directions are 16 (8 for each domain). There is very little clean and publicly available data for these domains and language pairs and thus we en couraged participants to not only utilize the small Asian Language Treebank (ALT) parallel corpora (Thu et al., 2016) but also the parallel corpora from OPUS1 . The ALT dataset contains 18,088, 1,000 and 1,018 training, development and test ing sentences. As for corpora for the IT domain we only provided evaluation (dev and test sets) 2016), consisting of twenty thousand Khmer English parallel sentences from news articles. • The ECCC corpus consists of 100 thousand KhmerEnglish parallel sentences extracted from document pairs of KhmerEnglish bi lingual records in Extraordinary Chambers in the Court of Cambodia, collected by National Institute of Posts, Telecoms & ICT, Cambo dia. The ALT corpus has been"
2020.wmt-1.19,W19-5206,0,0.0119895,"model distribution which results in very regular synthetic source sentences that do not properly cover the true data distribution. Following the approach proposed by Edunov et al. (2018), we apply noising to the beam search outputs. In particular, we transform source sentences with three types of noise: deleting words with probability 0.1, replacing words by a filler token with probability 0.1, and swapping words which is implemented as a random permutation over the tokens, drawn from the uniform distribution but restricted to swapping words no 203 Dataset further than three positions apart. Caswell et al. (2019) showed that main purpose of the synthetic noise is not to diversify the source but simply to indicate that the given source is synthetic. They proposed to prepend the input sequences of the synthetic data with a reserved token like &lt;BT&gt; to indicate that the given source is synthetic. In this paper, we experiment with both Noisy BT and Tagged BT. 5 Wikititles Wikimatrix PMIndia Tanzil (Koran) NLPC UOM PIB (CVIT@IIITH) MKB (CVIT@IIITH) UFAL Total Target Agreement # of Sentences 102,146 52,669 39,526 93,540 8,945 60,836 5,744 166,871 530,277 Table 1: Tamil-English parallel corpus statistics. Due"
2020.wmt-1.19,Y17-1038,0,0.0479242,"Missing"
2020.wmt-1.19,D18-1045,0,0.0145775,"ic but lossy since Backtranslation 4.1 Noisy and Tagged Backtranslation Backtranslation typically uses beam search (Sennrich et al., 2016a) or just greedy search (Lample et al., 2018a,b) to generate synthetic source sentences. Both are approximate algorithms to identify the maximum a-posteriori (MAP) output, i.e. the sentence with the largest estimated probability given an input. Beam is generally successful in finding high probability outputs (Ott et al., 2018). However, MAP prediction can lead to less rich translations since it always favors the most likely alternative in case of ambiguity. Edunov et al. (2018) argue that this is also problematic for a data augmentation scheme such as backtranslation. Beam and greedy search focus on the head of the model distribution which results in very regular synthetic source sentences that do not properly cover the true data distribution. Following the approach proposed by Edunov et al. (2018), we apply noising to the beam search outputs. In particular, we transform source sentences with three types of noise: deleting words with probability 0.1, replacing words by a filler token with probability 0.1, and swapping words which is implemented as a random permutati"
2020.wmt-1.19,P09-5002,0,0.0157938,"Missing"
2020.wmt-1.19,P07-2045,0,0.00529801,"e 1 for dataset details). For backtraslation, we randomly selected 10M sentences from the newscrawl 2019 English monolingual corpora. For Tamil monolingual corpora, used the entire newscrawl corpus (0.7M), Wikipedia corpus and part of the CommonCrawl data made available for a consolidated corpus of 10M sentences. We use IIT-Bombay Hindi-English parallel corpora v2.0 (Kunchukuttan et al., 2018) containing 1.5M parallel sentences to build our multilingual models. We used UFAL’s Tamil-English dev set containing 1,000 parallel sentences for tuning our models. 6.2 Data Processing We use the Moses (Koehn et al., 2007) toolkit1 for lowercasing, tokenization and cleaning the English side of the data. Both Tamil and Hindi data are first normalized and then tokenized. The Hindi data is mapped to Tamil script. We use the Indic NLP library2 (Kunchukuttan, 2020) for text processing of the Indic languages. We remove all sentences of length greater than 80 words from our training corpus. In all cases, we use BPE subword segmentation (Sennrich et al., 2016b) with 32k merge operations. In case of mulitlingual models, we learn the BPE vocabulary jointly on the Hindi and Tamil data. 6.3 Training Details For all of our"
2020.wmt-1.19,W17-3204,0,0.0337523,"Missing"
2020.wmt-1.19,D19-1167,0,0.0465199,"Missing"
2020.wmt-1.19,L18-1548,1,0.769601,"ally relevant to low-resource language scenarios where the model is not as robust to the decoder input distribution. 6 6.1 Experimental Settings Dataset We train our models only on the parallel data provided for the task (see Table 1 for dataset details). For backtraslation, we randomly selected 10M sentences from the newscrawl 2019 English monolingual corpora. For Tamil monolingual corpora, used the entire newscrawl corpus (0.7M), Wikipedia corpus and part of the CommonCrawl data made available for a consolidated corpus of 10M sentences. We use IIT-Bombay Hindi-English parallel corpora v2.0 (Kunchukuttan et al., 2018) containing 1.5M parallel sentences to build our multilingual models. We used UFAL’s Tamil-English dev set containing 1,000 parallel sentences for tuning our models. 6.2 Data Processing We use the Moses (Koehn et al., 2007) toolkit1 for lowercasing, tokenization and cleaning the English side of the data. Both Tamil and Hindi data are first normalized and then tokenized. The Hindi data is mapped to Tamil script. We use the Indic NLP library2 (Kunchukuttan, 2020) for text processing of the Indic languages. We remove all sentences of length greater than 80 words from our training corpus. In all c"
2020.wmt-1.19,J82-2005,0,0.69105,"Missing"
2020.wmt-1.19,D15-1166,0,0.398224,"Kejriwal, Siddharth Jain, Amit Bhagwat STC India, Microsoft, Hyderabad {vikgoyal, ankunchu, rakejriw, sija, amitb}@microsoft.com Abstract We describe our submission for the English→Tamil and Tamil→English news translation shared task. In this submission, we focus on exploring if a low-resource language (Tamil) can benefit from a high-resource language (Hindi) with which it shares contact relatedness. We show utilizing contact relatedness via multilingual NMT can significantly improve translation quality for English-Tamil translation. 1 Introduction In recent years, Neural Machine Translation (Luong et al., 2015; Bahdanau et al., 2015; Johnson et al., 2017; Wu et al., 2018; Vaswani et al., 2017) (NMT) has become the most prominent approach to Machine Translation (MT) due to its simplicity, generality and effectiveness. In NMT, a single neural network often consisting of an encoder and a decoder is used to directly maximize the conditional probabilities of target sentences given the source sentences in an end-to-end paradigm. NMT models have been shown to surpass the performance of previously dominant statistical machine translation (SMT) (Koehn, 2009) on many well-established translation tasks. Howev"
2020.wmt-1.19,W95-0115,0,0.117467,"Missing"
2020.wmt-1.19,I17-2050,0,0.0331063,"Missing"
2020.wmt-1.19,2020.emnlp-main.187,0,0.0382552,"Missing"
2020.wmt-1.19,N19-4009,0,0.0111772,"zation and cleaning the English side of the data. Both Tamil and Hindi data are first normalized and then tokenized. The Hindi data is mapped to Tamil script. We use the Indic NLP library2 (Kunchukuttan, 2020) for text processing of the Indic languages. We remove all sentences of length greater than 80 words from our training corpus. In all cases, we use BPE subword segmentation (Sennrich et al., 2016b) with 32k merge operations. In case of mulitlingual models, we learn the BPE vocabulary jointly on the Hindi and Tamil data. 6.3 Training Details For all of our experiments, we use the fairseq (Ott et al., 2019) toolkit3 . We use the Transformer model with 4 layers in both the encoder and decoder, each with 512 hidden units. The word embedding size is set to 512 and 8 attention heads are used. The training is done in batches of maximum 2048 tokens at a time with dropout set to 0.2. We use the Adam (Kingma and Ba, 2015) optimizer to optimize model parameters with β1 = 0.9, β2 = 0.98 and  = 1e − 9 and we use the same learning rate schedule as Vaswani et al. (2017). We validate the model after each epoch via label smoothed cross entropy loss and perplexity on the development set. We train all our NMT m"
2020.wmt-1.19,W18-6319,0,0.0350951,"Missing"
2020.wmt-1.19,P16-1009,0,0.481413,"l networks (RNN) or convolutional neural networks (CNN). In this work, we use the Transformer architecture in all of our NMT models. We use smaller capacity networks compared to Transformer-base, given the smaller size of the parallel data. 3 the Tamil character set is smaller than the Devanagari character set. Such mapping will help to utilize the lexical similarity between the two languages directly. Hence, we convert all Hindi data to Tamil script during a pre-processing step. We also report results of our MultiNMT models without script conversion of Hindi to Tamil. 4 Backtranslation (BT) (Sennrich et al., 2016a) is a widely used data augmentation method where the reverse direction is used to translate sentences from target-side monolingual data into the source language. This synthetic parallel data is combined with the actual parallel data to re-train the model leading to better language modelling on the target-side, regularization and target domain adaptation. Backtranslation is particularly useful for low-resource languages. We use backtranslation to augment our multilingual models. The backtranslation data is generated by multilingual models in the reverse direction, hence some implicit multilin"
2020.wmt-1.19,P16-1162,0,0.68144,"l networks (RNN) or convolutional neural networks (CNN). In this work, we use the Transformer architecture in all of our NMT models. We use smaller capacity networks compared to Transformer-base, given the smaller size of the parallel data. 3 the Tamil character set is smaller than the Devanagari character set. Such mapping will help to utilize the lexical similarity between the two languages directly. Hence, we convert all Hindi data to Tamil script during a pre-processing step. We also report results of our MultiNMT models without script conversion of Hindi to Tamil. 4 Backtranslation (BT) (Sennrich et al., 2016a) is a widely used data augmentation method where the reverse direction is used to translate sentences from target-side monolingual data into the source language. This synthetic parallel data is combined with the actual parallel data to re-train the model leading to better language modelling on the target-side, regularization and target domain adaptation. Backtranslation is particularly useful for low-resource languages. We use backtranslation to augment our multilingual models. The backtranslation data is generated by multilingual models in the reverse direction, hence some implicit multilin"
2021.eacl-main.303,P18-4020,0,0.0659338,"Missing"
2021.eacl-main.303,2020.findings-emnlp.445,1,0.828312,"Missing"
2021.eacl-main.303,L18-1548,1,0.834517,"Missing"
2021.eacl-main.303,N15-3017,1,0.830664,"exhibit a high degree of graphemetophoneme correspondence. There is a large overlap in the logical character sets of these scripts, though the visual appearance of the characters varies. The languages utilizing these scripts are said to exhibit orthographic similar ity on account of various shared characteristics (Kunchukuttan et al., 2018a). We undertake a systematic, largescale evalua tion of neural machine transliteration for 10 ma jor Indic languages from 2 major language fami lies (IndoAryan and Dravidian languages) spoken by more than a billion speakers. Other than Brah miNet (Kunchukuttan et al., 2015) and Dakshina (Roark et al., 2020), no other previous work has ex plored a wide range of Indic languages; Dakshina only explores transliteration into Indic languages. Our major contributions are: • For a largescale evaluation, we mine 600K transliteration pairs across 10 languages from pub licly available parallel and monolingual sources. This is much larger than existing corpora like MSRNEWS (Banchs et al., 2015), Brahminet (Kunchukuttan et al., 2015), Dakshina (Roark et al., 2020) and other small datasets (Banchs et al., 2015; Kunchukuttan et al., 2018b; Gupta et al., 2012; Khapra et al."
2021.eacl-main.303,W95-0115,0,0.710741,"Missing"
2021.eacl-main.303,W12-3152,0,0.0547356,"Missing"
2021.eacl-main.303,W12-5611,0,0.0718073,"Missing"
2021.eacl-main.303,2020.lrec-1.294,0,0.0501046,"Missing"
2021.eacl-main.303,P12-1049,0,0.0476573,"the Association for Computational Linguistics, pages 3469–3475 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Language pa hi bn or gu mr kn te ml ta Word pair count (×1000) 55.3 157.7 65.4 34.7 65.5 38.0 24.7 77.4 31.1 57.1 Mining Accuracy 81.2 NA 76.7 NA 93.0 89.0 87.1 86.2 82.3 77.9 Table 1: Statistics on mined transliteration corpora 1.0 hi bn pa gu mr or ta te kn ml tion corpora from different sources (7.4 million sentence pairs across all languages, see Appendix A for details). We use the Moses transliteration mining module (Durrani et al., 2014) implementa tion of Sajjad et al. (2012) to mine transliteration pairs using the default settings. 0.9 0.8 0.7 2.2 Mining from Monolingual Corpora 0.6 hi bn pa gu mr or ta te kn ml Figure 1: Orthographic Similarity: Indic languages dian languages from 2 major language families: (a) IndoAryan branch of IndoEuropean family (Hindi, Marathi, Gujarati, Bengali, Odia, Punjabi), (b) Dravidian family (Kannada, Telugu, Malay alam, Tamil). 2.1 Mining from Parallel Translation Corpus While very little transliteration corpora exists, a reasonable amount of parallel translation corpora between English and Indian languages are avail able in t"
2021.wat-1.1,2020.acl-main.703,0,0.187967,"news-commentary corpus.14 This year we also encouraged participants to use any corpora from WMT 202015 and WMT 202116 involving Japanese, Russian, and English as long as it did not belong to the news commentary domain to prevent any test set sentences from being unintentionally seen during training. 2,049 2,050 Table 5: The NICT-SAP task corpora splits. The corpora belong to two domains: wikinews (ALT) and software documentation (IT). The Wikinews corpora are Nway parallel. also encouraged the use of monolingual corpora expecting that it would be for pre-trained NMT models such as BART/MBART (Lewis et al., 2020; Liu et al., 2020). In Table 5 we give statistics of the aforementioned corpora which we used for the organizer’s baselines. Note that the evaluation corpora for both domains are created from documents and thus contain document level meta-data. Participants were encouraged to use document level approaches. Note that we do not exhaustively list6 all available corpora here and participants were not restricted from using any corpora as long as they are freely available. 2.7 Partition train development test train development test train development test 8 http://www.phontron.com/kftt/ https://data"
2021.wat-1.1,C18-2019,0,0.0200002,"task and 4 systems for the Japanese→ English.74 On the whole, all the submitted systems are basically lexical-constraint-aware NMT models with lexically constrained decoding method, where the restricted target vocabulary is concatenated into source sentences and, during the beam search at inference time, the models generate translation outputs containing the target vocabulary. We observed that these techniques boost the final translation performance of the NMT models in the restricted translation task. For human evaluation, we conducted the sourcebased direct assessment (Cettolo et al., 2017; Federmann, 2018) and source-based contrastive assessment (Sakaguchi and Van Durme, 2018; Federmann, 2018), to have the top-ranked systems of each team appraised by bilingual human annotators. In the human evaluation campaign, we also include the human reference data. Table 20 reports the final automatic evaluation score and the human evaluation results. In both tasks, the systems from the team “NTT” are the most highly evaluated in all the submitted systems in the final score and the human evaluation, consistently. We also note that our designed automation metric is well correlated Flickr30kEnt-JP Japanese↔En"
2021.wat-1.1,W18-1819,0,0.0552279,"Missing"
2021.wat-1.1,2007.mtsummit-papers.63,0,0.0610664,"om OPUS. We Test set II : A pair of test and reference sentences and context data that are articles including test sentences. The references were automatically extracted from English newswire sentences and manually selected. Therefore, the quality of the references of test set II is better than that of test set I. The statistics of JIJI Corpus are shown in Table 2. The definition of data use is shown in Table 3. Participants submit the translation results of one or more of the test data. The sentence pairs in each data are identified in the same manner as that for ASPEC using the method from (Utiyama and Isahara, 2007). 2.5 ALT and UCSY Corpus The parallel data for Myanmar-English translation tasks at WAT2021 consists of two corpora, the ALT corpus and UCSY corpus. • The ALT corpus is one part from the Asian Language Treebank (ALT) project (Riza et al., 2016), consisting of twenty thousand Myanmar-English parallel sentences from news articles. • The UCSY corpus (Yi Mon Shwe Sin and Khin Mar Soe, 2018) is constructed by the NLP Lab, University of Computer Studies, 3 http://opus.nlpl.eu/ http://www.statmt.org/wmt20/ 5 Software Domain Evaluation Splits 4 3 Task Use Training Test set I Japanese to English Test"
2021.wat-1.22,D18-1530,1,0.828232,"f digitization has been a bottleneck hindering any meaningful progress towards automatic translation systems. This has changed recently, at least for monolingual data, with the curation of digital libraries like GRETIL8 and DCS9 . Currently, the largest freely available repository of translations are for The Bhagavadgita (Prabhakar et al., 2000) and The R¯am¯ayana (Geervani et al., 1989). However, labeled datasets for other tasks, like the ones proposed in (Kulkarni, 2013; Bhardwaj et al., 2018; Krishnan et al., 2020) have resulted in parsers (Krishna et al., 2020, 2021) and sandhi splitters (Aralikatte et al., 2018; Krishnan and Kulkarni, 2020) which are pre-cursors to modular translation systems. Though there have been attempts at building Sanskrit translation tools (Bharati and Kulkarni, 2009), they are mostly rule-based and rely on manual intervention. We hope that the availability of the Itih¯asa corpus pushes the domain towards endto-end systems. 8 http://gretil.sub.uni-goettingen.de/ gretil.html 9 http://www.sanskrit-linguistics.org/ dcs/index.php 6 Conclusion In this work, we introduce Itih¯asa, a large-scale dataset containing more than 93,000 pairs of Sanskrit shlokas and their English translat"
2021.wat-1.22,2009.freeopmt-1.2,0,0.0623893,"ation of digital libraries like GRETIL8 and DCS9 . Currently, the largest freely available repository of translations are for The Bhagavadgita (Prabhakar et al., 2000) and The R¯am¯ayana (Geervani et al., 1989). However, labeled datasets for other tasks, like the ones proposed in (Kulkarni, 2013; Bhardwaj et al., 2018; Krishnan et al., 2020) have resulted in parsers (Krishna et al., 2020, 2021) and sandhi splitters (Aralikatte et al., 2018; Krishnan and Kulkarni, 2020) which are pre-cursors to modular translation systems. Though there have been attempts at building Sanskrit translation tools (Bharati and Kulkarni, 2009), they are mostly rule-based and rely on manual intervention. We hope that the availability of the Itih¯asa corpus pushes the domain towards endto-end systems. 8 http://gretil.sub.uni-goettingen.de/ gretil.html 9 http://www.sanskrit-linguistics.org/ dcs/index.php 6 Conclusion In this work, we introduce Itih¯asa, a large-scale dataset containing more than 93,000 pairs of Sanskrit shlokas and their English translations from The R¯am¯ayana and The Mah¯abh¯arata. First, we detail the extraction process which includes an automated OCR phase and a manual alignment phase. Next, we analyze the dataset"
2021.wat-1.22,L18-1712,0,0.0648864,"Missing"
2021.wat-1.22,W18-6111,0,0.0209163,"line is ignored. In general, print errors are corrected as much as possible, subjective errors are retained for originality, and other types of errors are corrected when encountered. 6 This was a time-consuming process and the first author inspected the output manually over the course of one year. 7 It was not feasible for the authors to correct every error, especially the lexical ones. The most common error that exists in the corpus is the swapping of e and c. For example, ‘thcir’ instead of ‘their’. Though these errors can easily be corrected using automated tools like the one proposed in (Boyd, 2018), it is out-of-scope of this paper and is left for future work. Train Chapters Shlokas Chapters Shlokas Chapters Shlokas Dev. Test R¯amayana 514 42 86 15,834 1,115 2,422 Mah¯abh¯arata 1,688 139 283 59,327 5,033 9,299 Overall 2202 181 369 75,161 6,148 11,721 Total 642 19,371 2,110 73,659 2,752 93,030 Table 1: Size of training, development, and test sets. 3 Analysis In total, we extract 19,371 translation pairs from 642 chapters of The R¯am¯ayana and 73,659 translation pairs from 2,110 chapters of The Mah¯abh¯arata. It should be noted that these numbers do not correspond to the number of shlokas"
2021.wat-1.22,C12-1062,0,0.0174373,"viz., The R¯am¯ayana and The Mah¯abh¯arata. We first describe the motivation behind the curation of such a dataset and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this corpus and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset.1 1 Introduction Sanskrit is one of the oldest languages in the world and most Indo-European languages are influenced by it (Beekes, 1995). There are about 30 million pieces of Sanskrit literature available to us today (Goyal et al., 2012), most of which have not been digitized. Among those that have been, few have been translated. The main reason for this is the lack of expertise and funding. An automatic translation system would not only aid and accelerate this process, but it would also help in democratizing the knowledge, history, and culture present in this literature. In this work, we present Itih¯asa, a large-scale Sanskrit-English translation corpus consisting of more than 93,000 shlokas and their translations. Itih¯asa, literally meaning ‘it happened this way’ is a collection of historical records of important events i"
2021.wat-1.22,2005.mtsummit-papers.11,0,0.152061,"Itih¯asa, with the best models scoring between 7-8 BLEU points. This indicates the complex nature of the dataset (see §3 for a detailed analysis of the dataset and its vocabulary). Motivation The main motivation behind this work is to provide an impetus for the Indic NLP community to build better translation systems for Sanskrit. Additionally, since The R¯am¯ayana and The Mah¯abh¯arata are so pervasive in Indian culture, and have been translated to all major Indian languages, there is a possibility of creating an n-way parallel corpus with Sanskrit as the pivot language, similar to Europarl (Koehn, 2005) and PMIndia (Haddow and Kirefu, 2020) datasets. The existence of Sanskrit-English parallel data has other advantages as well. Due to Sanskrit being a morphologically rich, agglutinative, and highly inflexive, complex concepts can be expressed in compact forms by combining individual words through Sandhi and Samasa.2 This also enables a speaker to potentially create an infinite number of unique words in Sanskrit. Having a parallel corpus can help us induce word translations through bilingual dictionary induction (Søgaard et al., 2018). It also allows us to use English as a surrogate language f"
2021.wat-1.22,P07-2045,0,0.0131653,"Missing"
2021.wat-1.22,2020.emnlp-main.388,0,0.0181067,"ike Ganguli (1883) have been published, the lack of digitization has been a bottleneck hindering any meaningful progress towards automatic translation systems. This has changed recently, at least for monolingual data, with the curation of digital libraries like GRETIL8 and DCS9 . Currently, the largest freely available repository of translations are for The Bhagavadgita (Prabhakar et al., 2000) and The R¯am¯ayana (Geervani et al., 1989). However, labeled datasets for other tasks, like the ones proposed in (Kulkarni, 2013; Bhardwaj et al., 2018; Krishnan et al., 2020) have resulted in parsers (Krishna et al., 2020, 2021) and sandhi splitters (Aralikatte et al., 2018; Krishnan and Kulkarni, 2020) which are pre-cursors to modular translation systems. Though there have been attempts at building Sanskrit translation tools (Bharati and Kulkarni, 2009), they are mostly rule-based and rely on manual intervention. We hope that the availability of the Itih¯asa corpus pushes the domain towards endto-end systems. 8 http://gretil.sub.uni-goettingen.de/ gretil.html 9 http://www.sanskrit-linguistics.org/ dcs/index.php 6 Conclusion In this work, we introduce Itih¯asa, a large-scale dataset containing more than 93,000"
2021.wat-1.22,W13-3718,0,0.0242224,"M¨uller, 1866; MonierWilliams, 1899). Over the years, though notable translation works like Ganguli (1883) have been published, the lack of digitization has been a bottleneck hindering any meaningful progress towards automatic translation systems. This has changed recently, at least for monolingual data, with the curation of digital libraries like GRETIL8 and DCS9 . Currently, the largest freely available repository of translations are for The Bhagavadgita (Prabhakar et al., 2000) and The R¯am¯ayana (Geervani et al., 1989). However, labeled datasets for other tasks, like the ones proposed in (Kulkarni, 2013; Bhardwaj et al., 2018; Krishnan et al., 2020) have resulted in parsers (Krishna et al., 2020, 2021) and sandhi splitters (Aralikatte et al., 2018; Krishnan and Kulkarni, 2020) which are pre-cursors to modular translation systems. Though there have been attempts at building Sanskrit translation tools (Bharati and Kulkarni, 2009), they are mostly rule-based and rely on manual intervention. We hope that the availability of the Itih¯asa corpus pushes the domain towards endto-end systems. 8 http://gretil.sub.uni-goettingen.de/ gretil.html 9 http://www.sanskrit-linguistics.org/ dcs/index.php 6 Con"
2021.wat-1.22,P02-1040,0,0.109083,"Missing"
2021.wat-1.22,P16-1162,0,0.0267965,"Missing"
2021.wat-1.22,2006.amta-papers.25,0,0.199483,"Missing"
2021.wat-1.22,P18-1072,1,0.865103,"Missing"
D16-1196,N16-4006,1,0.288206,"riable-length consonant-vowel sequence, as a basic unit of translation between related languages which use abugida or alphabetic scripts. We show that orthographic syllable level translation significantly outperforms models trained over other basic units (word, morpheme and character) when training over small parallel corpora. 1 Introduction Related languages exhibit lexical and structural similarities on account of sharing a common ancestry (Indo-Aryan, Slavic languages) or being in prolonged contact for a long period of time (Indian subcontinent, Standard Average European linguistic areas) (Bhattacharyya et al., 2016). Translation between related languages is an important requirement due to substantial government, business and social communication among people speaking these languages. However, most of these languages have few parallel corpora resources, an important requirement for building good quality SMT systems. Modelling the lexical similarity among related languages is the key to building good-quality SMT systems with limited parallel corpora. Lexical similarity implies that the languages share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi"
D16-1196,N12-1047,0,0.0882306,"s – training: 44,777, tuning 1K, test: 2K sentences. Language models for word-level systems were trained on the target side of training corpora plus monolingual corpora from various sources [hin: 10M (Bojar et al., 2014), tam: 1M (Ramasamy et al., 2012), mar: 1.8M (news websites), mal: 200K (Quasthoff et al., 2006) sentences]. We used the target language side of the 1914 parallel corpora for character, morpheme and OS level LMs. System details: PBSMT systems were trained using the Moses system (Koehn et al., 2007), with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character and OS level models. We used the BrahmiNet transliteration system (Kunchukuttan et al., 2015) for postediting, which is based on the transliteration Module in Moses (Durrani et al., 2014). We used unsupervised morphological segmenters trained with Morfessor (Virpioja et al., 2013) for obtaining morpheme representations. The unsupervised morphological segmenters were trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006).The morph"
D16-1196,P10-1048,0,0.244431,"Missing"
D16-1196,E14-4029,0,0.0372769,"the target language side of the 1914 parallel corpora for character, morpheme and OS level LMs. System details: PBSMT systems were trained using the Moses system (Koehn et al., 2007), with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character and OS level models. We used the BrahmiNet transliteration system (Kunchukuttan et al., 2015) for postediting, which is based on the transliteration Module in Moses (Durrani et al., 2014). We used unsupervised morphological segmenters trained with Morfessor (Virpioja et al., 2013) for obtaining morpheme representations. The unsupervised morphological segmenters were trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006).The morph-segmenters and our implementation of orthographic syllabification are made available as part of the Indic NLP Library1 . Evaluation: We use BLEU (Papineni et al., 2002) and Le-BLEU (Virpioja and Grönroos, 2015) for evaluation. Le-BLEU does fuzzy matches of words and hence is suitable for evaluating SMT systems that perform transfor"
D16-1196,P06-2025,0,0.0317112,"present a linguistically motivated, variable length unit of translation — orthographic syllable (OS) — which provides more context for translation while limiting the number of basic units. The OS consists of one or more consonants followed by a vowel and is inspired from the akshara, a consonant-vowel unit, which is the fundamental organizing principle of Indic scripts (Sproat, 2003; Singh, 2006). It can be thought of as an approximate syllable with the onset and nucleus, but no coda. While true syllabification is hard, orthographic syllabification can be easily done. Atreya et al. (2016) and Ekbal et al. (2006) have shown that the OS is a useful unit for transliteration involving Indian languages. We show that orthographic syllable-level trans1912 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1912–1917, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics lation significantly outperforms character-level and strong word-level and morpheme-level baselines over multiple related language pairs (Indian as well as others). Character-level approaches have been previously shown to work well for language pairs with high lexical si"
D16-1196,P07-2045,0,0.00772487,"sisting of a modest number of sentences from tourism and health domains. The data split is as follows – training: 44,777, tuning 1K, test: 2K sentences. Language models for word-level systems were trained on the target side of training corpora plus monolingual corpora from various sources [hin: 10M (Bojar et al., 2014), tam: 1M (Ramasamy et al., 2012), mar: 1.8M (news websites), mal: 200K (Quasthoff et al., 2006) sentences]. We used the target language side of the 1914 parallel corpora for character, morpheme and OS level LMs. System details: PBSMT systems were trained using the Moses system (Koehn et al., 2007), with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character and OS level models. We used the BrahmiNet transliteration system (Kunchukuttan et al., 2015) for postediting, which is based on the transliteration Module in Moses (Durrani et al., 2014). We used unsupervised morphological segmenters trained with Morfessor (Virpioja et al., 2013) for obtaining morpheme representations. The unsupervised morpholog"
D16-1196,N15-3017,1,0.711478,"y et al., 2012), mar: 1.8M (news websites), mal: 200K (Quasthoff et al., 2006) sentences]. We used the target language side of the 1914 parallel corpora for character, morpheme and OS level LMs. System details: PBSMT systems were trained using the Moses system (Koehn et al., 2007), with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character and OS level models. We used the BrahmiNet transliteration system (Kunchukuttan et al., 2015) for postediting, which is based on the transliteration Module in Moses (Durrani et al., 2014). We used unsupervised morphological segmenters trained with Morfessor (Virpioja et al., 2013) for obtaining morpheme representations. The unsupervised morphological segmenters were trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006).The morph-segmenters and our implementation of orthographic syllabification are made available as part of the Indic NLP Library1 . Evaluation: We use BLEU (Papineni et al., 2002) and Le-BLEU (Virpioja and Grönroos, 2015) for evaluation. Le-BLEU does"
D16-1196,W95-0115,0,0.0765572,"ravidian). These languages have been in contact for a long time, hence there are many lexical and grammatical similarities among them, leading to the subcontinent being considered a linguistic area (Emeneau, 1956). Specifically, there is overlap between the vocabulary of these languages to varying degrees due to cognates, language contact and loanwords from Sanskrit (throughout history) and English (in recent times). Table 2 lists the languages involved in the experiments and provides an indication of the lexical similarity between them in terms of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1995) between the parallel training sentences at character level. All these language have a rich inflectional morphology with Dravidian languages, and Marathi and Konkani to some degree, being agglutinative. kok-mar and pan-hin have a high degree of lexical similarity. Dataset: We used the multilingual ILCI corpus for our experiments (Jha, 2012), consisting of a modest number of sentences from tourism and health domains. The data split is as follows – training: 44,777, tuning 1K, test: 2K sentences. Language models for word-level systems were trained on the target side of training corpora plus mono"
D16-1196,P12-2059,0,0.186929,"uations are retained and a special word boundary marker character (_) is introduced to indicate word boundaries as shown here: W: O: राजू , घराबाहेर जाऊ नको . रा जू _ , _ घ रा बा हे र _ जा ऊ _ न को _ . For all units of representation, we trained phrasebased SMT (PBSMT) systems. Since related languages have similar word order, we used distance based distortion model and monotonic decoding. For character and orthographic syllable level models, we use higher order (10-gram) languages models since data sparsity is a lesser concern due to small vocabulary size (Vilar et al., 2007). As suggested by Nakov and Tiedemann (2012), we used word-level tuning for character and orthographic syllable level models by post-processing n-best lists in each tuning step to calculate the usual word-based BLEU score. While decoding, the word and morpheme level systems will not be able to translate OOV words. Since the languages involved share vocabulary, we transliterate the untranslated words resulting in the post-edited systems WX and MX corresponding to the systems W and M respectively. Following decoding, we used a simple method to regenerate words from sub-word level units: Since we represent word boundaries using a word boun"
D16-1196,quasthoff-etal-2006-corpus,0,0.335578,", and Marathi and Konkani to some degree, being agglutinative. kok-mar and pan-hin have a high degree of lexical similarity. Dataset: We used the multilingual ILCI corpus for our experiments (Jha, 2012), consisting of a modest number of sentences from tourism and health domains. The data split is as follows – training: 44,777, tuning 1K, test: 2K sentences. Language models for word-level systems were trained on the target side of training corpora plus monolingual corpora from various sources [hin: 10M (Bojar et al., 2014), tam: 1M (Ramasamy et al., 2012), mar: 1.8M (news websites), mal: 200K (Quasthoff et al., 2006) sentences]. We used the target language side of the 1914 parallel corpora for character, morpheme and OS level LMs. System details: PBSMT systems were trained using the Moses system (Koehn et al., 2007), with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character and OS level models. We used the BrahmiNet transliteration system (Kunchukuttan et al., 2015) for postediting, which is based on the transliterat"
D16-1196,W12-5611,0,0.120413,"e have a rich inflectional morphology with Dravidian languages, and Marathi and Konkani to some degree, being agglutinative. kok-mar and pan-hin have a high degree of lexical similarity. Dataset: We used the multilingual ILCI corpus for our experiments (Jha, 2012), consisting of a modest number of sentences from tourism and health domains. The data split is as follows – training: 44,777, tuning 1K, test: 2K sentences. Language models for word-level systems were trained on the target side of training corpora plus monolingual corpora from various sources [hin: 10M (Bojar et al., 2014), tam: 1M (Ramasamy et al., 2012), mar: 1.8M (news websites), mal: 200K (Quasthoff et al., 2006) sentences]. We used the target language side of the 1914 parallel corpora for character, morpheme and OS level LMs. System details: PBSMT systems were trained using the Moses system (Koehn et al., 2007), with the grow-diag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character and OS level models. We used the BrahmiNet transliteration system (Kunchukuttan et"
D16-1196,R13-1088,0,0.470971,"rrowings or loan words from other languages. Translation for such words can be A different paradigm is to drop the notion of word boundary and consider the character n-gram as the basic unit of translation (Vilar et al., 2007; Tiedemann, 2009a). Such character-level SMT bas been explored for closely related languages like Bulgarian-Macedonian, Indonesian-Malay with modest success, with the short context of unigrams being a limiting factor (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). In this work, we present a linguistically motivated, variable length unit of translation — orthographic syllable (OS) — which provides more context for translation while limiting the number of basic units. The OS consists of one or more consonants followed by a vowel and is inspired from the akshara, a consonant-vowel unit, which is the fundamental organizing principle of Indic scripts (Sproat, 2003; Singh, 2006). It can be thought of as an approximate syllable with the onset and nucleus, but no coda. While true syllabification is hard, orthographic syllabification can be easily done. Atreya"
D16-1196,2009.eamt-1.3,0,0.615229,"systems. Modelling the lexical similarity among related languages is the key to building good-quality SMT systems with limited parallel corpora. Lexical similarity implies that the languages share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Translation for such words can be A different paradigm is to drop the notion of word boundary and consider the character n-gram as the basic unit of translation (Vilar et al., 2007; Tiedemann, 2009a). Such character-level SMT bas been explored for closely related languages like Bulgarian-Macedonian, Indonesian-Malay with modest success, with the short context of unigrams being a limiting factor (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). In this work, we present a linguistically motivated, variable length unit of translation — orthographic syllable (OS) — which provides more context for translation while limiting the number of basic units. The OS c"
D16-1196,E12-1015,0,0.719407,"similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Translation for such words can be A different paradigm is to drop the notion of word boundary and consider the character n-gram as the basic unit of translation (Vilar et al., 2007; Tiedemann, 2009a). Such character-level SMT bas been explored for closely related languages like Bulgarian-Macedonian, Indonesian-Malay with modest success, with the short context of unigrams being a limiting factor (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). In this work, we present a linguistically motivated, variable length unit of translation — orthographic syllable (OS) — which provides more context for translation while limiting the number of basic units. The OS consists of one or more consonants followed by a vowel and is inspired from the akshara, a consonant-vowel unit, which is the fundamental organizing principle of Indic scripts (Sproat, 2003; Singh, 2006). It can be thou"
D16-1196,W07-0705,0,0.651908,"ing good quality SMT systems. Modelling the lexical similarity among related languages is the key to building good-quality SMT systems with limited parallel corpora. Lexical similarity implies that the languages share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Translation for such words can be A different paradigm is to drop the notion of word boundary and consider the character n-gram as the basic unit of translation (Vilar et al., 2007; Tiedemann, 2009a). Such character-level SMT bas been explored for closely related languages like Bulgarian-Macedonian, Indonesian-Malay with modest success, with the short context of unigrams being a limiting factor (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). In this work, we present a linguistically motivated, variable length unit of translation — orthographic syllable (OS) — which provides more context for translation while limiting the number of basi"
D16-1196,W15-3052,0,0.178484,"Missing"
D19-5201,E06-1031,0,0.0437063,"ion web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/wat2019.my-en.zip 28 http://lotus.kuee.kyoto-u.ac.jp/WAT/ km-en-d"
D19-5201,W16-4601,1,0.763733,"Missing"
D19-5201,W19-6613,1,0.928331,"Bible and Cinema. The statistics of the corpus are given in Table 9. 2.10 #sent. 12,356 486 600 47,082 589 600 82,072 313 600 Table 10: In-Domain data for the Russian– Japanese task. Table 9: Data for the Tamil↔English task. 2.9 Partition train development test train development test train development test 3 Baseline Systems Human evaluations of most of WAT tasks were conducted as pairwise comparisons between the translation results for a specific baseline system and translation results for each particJaRuNC Corpus For the Russian↔Japanese task we asked participants to use the JaRuNC corpus5 (Imankulova et al., 2019) which belongs to the news commentary domain. This dataset was manually aligned and cleaned and is trilingual. It can be used to evaluate Russian↔English 6 http://www.phontron.com/kftt/ https://datarepository.wolframcloud.com/ resources/Japanese-English-Subtitle-Corpus 8 https://wit3.fbk.eu/ 9 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 10 https://cms.unov.org/UNCorpus/ 11 https://translate.yandex.ru/corpus?lang=en 12 http://lotus.kuee.kyoto-u.ac.jp/WAT/ News-Commentary/news-commentary-v14.en-ru. filtered.tar.gz 7 4 http://ufal.mff.cuni.cz/~ramasamy/parallel/ html/ 5 https://github.com/aizhanti/JaR"
D19-5201,W17-5701,1,0.779618,"Missing"
D19-5201,D10-1092,0,0.105912,"t of the tasks. We used Transformer (Vaswani et al., 2017) (Tensor2Tensor)) for the News Commentary and English↔Tamil tasks and Transformer (OpenNMT-py) for the Multimodal task. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score 16 We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES https://bitbucket.org/eunjeon/mecab-ko/ https://bitbucket.org/anoopk/indic_nlp_library 18 https://github.com/rsennrich/subword-nmt 19 https://taku910.github.io/mecab/ 17 20 9 https://github.com/tensorflow/tensor2tensor 4.2 Automatic Evaluation System (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.21 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2019 web page.22 All scores for each task were calculated using the corresponding reference translations. The automatic evaluation system receives translation results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure 2, the system requires participants to provid"
D19-5201,W14-7001,1,0.794275,"t 400 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 • Open innovation platform Due to the fixed and open test data, we can repeatedly evaluate translation systems on the same dataset over years. WAT receives submissions at any time; i.e., there is no submission deadline of translation results w.r.t automatic evaluation of translation quality. Introduction The Workshop on Asian Translation (WAT) is an open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014-WAT2018 (Nakazawa et al., 2014, 2015, 2016, 2017, 2018), WAT2019 brings together machine 2 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/ 3 https://ufal.mff.cuni.cz/hindi-visual-genome/ wat-2019-multimodal-task 1 One paper was withdrawn post acceptance and hence only 6 papers will be in the proceedings. 1 Proceedings of the 6th Workshop on Asian Translation, pages 1–35 Hong Kong, China, November 4, 2019. ©2019 Association for Computational Linguistics Lang JE JC Train 3,008,500 672,315 Dev 1,790 2,090 DevTest 1,784 2,148 Lang zh-ja ko-ja en-ja Test 1,812 2,107 Lang zh-ja ko-ja en-ja Table 1: Statistics for ASPEC Dataset"
D19-5201,W15-5001,1,0.855752,"Missing"
D19-5201,P17-4012,0,0.147541,"MT RBMT Other Other ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ja-hi ✓ ✓ ✓ ✓ EnTam ta-en en-ta ✓ ✓ IITB en-hi hi-ja ✓ ✓ ✓ ✓ TDDC ja-en ✓ hi-en ✓ ✓ NewsCommentary ru-ja ja-ru JIJI ja-en en-ja ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ALT en-my km-en ✓ ✓ Multimodal en-hi ✓ my-en ✓ ✓ en-km ✓ model) for Chinese segmentation. • The Moses toolkit for English and Indonesian tokenization. • Mecab-ko16 for Korean segmentation. • Indic NLP Library17 for Indic language segmentation. • The tools included in the ALT corpus for Myanmar and Khmer segmentation. • subword-nmt18 for all languages. 3.3.1 NMT with Attention We used OpenNMT (Klein et al., 2017) as the implementation of the baseline NMT systems of NMT with attention (System ID: NMT). We used the following OpenNMT configuration. • • • • • • • • When we built BPE-codes, we merged source and target sentences and we used 100,000 for s option. We used 10 for vocabulary-threshold when subword-nmt applied BPE. 3.2.2 The default values were used for the other system parameters. For EnTam, News Commentary • The Moses toolkit for English and Russian only for the News Commentary data. 3.3.2 Transformer (Tensor2Tensor) For the News Commentary and English↔Tamil tasks, we used tensor2tensor’s20 im"
D19-5201,W18-1819,0,0.026575,"multilingual model. As for the English↔Tamil task, we train separate baseline models for each translation direction with 32,000 separate sub-word vocabularies. • Mecab19 for Japanese segmentation. • The EnTam corpus is not tokenized by any external toolkits. • Both corpora are further processed by tensor2tensor’s internal pre/postprocessing which includes sub-word segmentation. 3.2.3 For Multi-Modal Task • Hindi Visual Genome comes untokenized and we did not use or recommend any specific external tokenizer. 3.3.3 Transformer (OpenNMT-py) For the Multimodal task, we used the Transformer model (Vaswani et al., 2018) as implemented in OpenNMT-py (Klein et al., 2017) and used the “base” model with default parameters for the multi-modal task baseline. We have generated the vocabulary of 32k subword types jointly for both the source and target languages. The vocabulary is shared between the encoder and decoder. • The standard OpenNMT-py sub-word segmentation was used for pre/postprocessing for the baseline system and each participant used what they wanted. 3.3 encoder_type = brnn brnn_merge = concat src_seq_length = 150 tgt_seq_length = 150 src_vocab_size = 100000 tgt_vocab_size = 100000 src_words_min_freque"
D19-5201,P11-2093,0,0.0158555,"tion system receives translation results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure 2, the system requires participants to provide the following information for each submission: • Human Evaluation: whether or not they submit the results for human evaluation; Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model23 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0.24 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.25 For Korean segmentation, we used mecab-ko.26 For Myanmar and Khmer segmentations, we used myseg.py27 and kmseg.py28 . For English and Russian tokenizations, we used tokenizer.perl29 in the Moses toolkit. For Hindi and Tamil tokenizations, we used Indic NLP Library.30 The detailed procedu"
D19-5201,P02-1040,0,0.106619,"used what they wanted. 3.3 encoder_type = brnn brnn_merge = concat src_seq_length = 150 tgt_seq_length = 150 src_vocab_size = 100000 tgt_vocab_size = 100000 src_words_min_frequency = 1 tgt_words_min_frequency = 1 Baseline NMT Methods We used the following NMT with attention for most of the tasks. We used Transformer (Vaswani et al., 2017) (Tensor2Tensor)) for the News Commentary and English↔Tamil tasks and Transformer (OpenNMT-py) for the Multimodal task. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score 16 We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES https://bitbucket.org/eunjeon/mecab-ko/ https://bitbucket.org/anoopk/indic_nlp_library 18 https://github.com/rsennrich/subword-nmt 19 https://taku910.github.io/mecab/ 17 20 9 https://github.com/tensorflow/tensor2tensor 4.2 Automatic Evaluation System (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.21 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2019 web page.22 All scores for e"
D19-5201,W16-2342,0,0.0144454,"on results that participants permit to be published are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyot"
D19-5201,J82-2005,0,0.731235,"Missing"
D19-5201,D19-5224,0,0.0224104,"Missing"
D19-5201,W15-3049,0,0.0158524,"s permit to be published are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-da"
D19-5201,W12-5611,1,0.827974,"/ 17k #types 22k / 42k 2.9k / 4.3k 3.5k / 5.6k 48k / 55k 3.5k / 3.8k 3.5k / 3.8k 144k / 74k 3.2k / 2.3k 5.6k / 3.8k translation quality as well but this is beyond the scope of this years sub-task. Refer to Table 10 for the statistics of the in-domain parallel corpora. In addition we encouraged the participants to use out-of-domain parallel corpora from various sources such as KFTT,6 JESC,7 TED,8 ASPEC,9 UN,10 Yandex11 and Russian↔English news-commentary corpus.12 EnTam Corpus For Tamil↔English translation task we asked the participants to use the publicly available EnTam mixed domain corpus4 (Ramasamy et al., 2012). This corpus contains training, development and test sentences mostly from the news-domain. The other domains are Bible and Cinema. The statistics of the corpus are given in Table 9. 2.10 #sent. 12,356 486 600 47,082 589 600 82,072 313 600 Table 10: In-Domain data for the Russian– Japanese task. Table 9: Data for the Tamil↔English task. 2.9 Partition train development test train development test train development test 3 Baseline Systems Human evaluations of most of WAT tasks were conducted as pairwise comparisons between the translation results for a specific baseline system and translation r"
D19-5201,2006.amta-papers.25,0,0.0857626,"hed are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/wat2019.my-en.zip 28 htt"
D19-5201,2007.mtsummit-papers.63,0,0.678495,",000 2,000 Dev 2,000 2,000 2,000 Table 2: Statistics for JPC • Domain and language pairs WAT is the world’s first workshop that targets scientific paper domain, and Chinese↔Japanese and Korean↔Japanese language pairs. In the future, we will add more Asian languages such as Vietnamese, Thai and so on. 2 Train 1,000,000 1,000,000 1,000,000 ASPEC-JE The training data for ASPEC-JE was constructed by NICT from approximately two million Japanese-English scientific paper abstracts owned by JST. The data is a comparable corpus and sentence correspondences are found automatically using the method from Utiyama and Isahara (2007). Each sentence pair is accompanied by a similarity score calculated by the method and a field ID that indicates a scientific field. The correspondence between field IDs and field names, along with the 2.2 JPC JPO Patent Corpus (JPC) for the patent tasks was constructed by the Japan Patent Office (JPO) in collaboration with NICT. The corpus consists of Chinese-Japanese, KoreanJapanese and English-Japanese patent descriptions whose International Patent Classi2 Disclosure Period 2016-01-01 to 2017-12-31 2018-01-01 to 2018-06-30 Train 1,089,346 (614,817) 314,649 (218,495) Dev Texts Items 1,153 2,"
I17-2048,N16-4006,1,0.901348,"Missing"
I17-2048,W04-3250,0,0.0228205,"he Leipzig corpus (Quasthoff et al., 2006). The BPE vocabulary size was chosen to match OS vocab size. We use tmtriangulate4 for phrase-table triangulation and combine-ptables (Bisazza et al., 2011) for linear interpolation of phrase-tables. Evaluation: The primary evaluation metric is word-level BLEU (Papineni et al., 2002). We also report LeBLEU (Virpioja and Gr¨onroos, 2015) scores in the appendix. LeBLEU is a variant of BLEU that does soft-matching of words and has been shown to be better for morphologically rich languages. We use bootstrap resampling for testing statistical significance (Koehn, 2004). (2) i=1 where, s, pi and t are the source, ith best sourcepivot translation and target sentence respectively. Using Multiple Pivot Languages : We use multiple pivot languages by combining triangulated models corresponding to different pivot languages. Linear interpolation is used (Bisazza et al., 2011) for model combination. Interpolation weights are assigned to each phrase-table and the feature values for each phrase pair are interpolated using these weights as shown below: f j (¯ s, t¯) = s.t P i X i αi = 1, αi fij (¯ s, t¯) (3) αi ≥ 0 where, f j is feature j defined on the phrase pair (¯"
I17-2048,P07-2045,0,0.0100926,"Missing"
I17-2048,2011.iwslt-evaluation.18,0,0.17107,"moothing for word and morpheme-level models, and 10-gram LMs for OS, BPE, characterlevel models. We used the Indic NLP library2 for orthographic syllabification, the subword-nmt library3 for training BPE models and Morfessor (Virpioja et al., 2013) for morphological segmentation. These unsupervised morphological analyzers for Indian languages, described in Kunchukuttan et al. (2014), are trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006). The BPE vocabulary size was chosen to match OS vocab size. We use tmtriangulate4 for phrase-table triangulation and combine-ptables (Bisazza et al., 2011) for linear interpolation of phrase-tables. Evaluation: The primary evaluation metric is word-level BLEU (Papineni et al., 2002). We also report LeBLEU (Virpioja and Gr¨onroos, 2015) scores in the appendix. LeBLEU is a variant of BLEU that does soft-matching of words and has been shown to be better for morphologically rich languages. We use bootstrap resampling for testing statistical significance (Koehn, 2004). (2) i=1 where, s, pi and t are the source, ith best sourcepivot translation and target sentence respectively. Using Multiple Pivot Languages : We use multiple pivot languages by combin"
I17-2048,W16-4811,1,0.833622,"b}@cse.iitb.ac.in Abstract with similar form (spelling and pronunciation) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate language, called the pivot language, to build the source-target translation system. The pivot approach makes no assumptions about source, pivot, and target language relatedness. Our work brings together subword-level translation and pivot-based SMT in low resource scenarios. We refer"
I17-2048,D16-1196,1,0.800917,"b}@cse.iitb.ac.in Abstract with similar form (spelling and pronunciation) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate language, called the pivot language, to build the source-target translation system. The pivot approach makes no assumptions about source, pivot, and target language relatedness. Our work brings together subword-level translation and pivot-based SMT in low resource scenarios. We refer"
I17-2048,N12-1047,0,0.0202235,"irst translated into the pivot language, and the pivot language translation is further translated into the target language using the S-P and P-T translation models respectively. To reduce cascading errors due to pipelining, we consider the top-k source-pivot translations in the second stage of the pipeline (an approximation to expectation over all translation candidates). We used k = 20 in our experiments. The translation candidates are scored 284 as shown below: P (t|s) = k X P (t|pi )P (pi |s) et al., 2007) with grow-diag-final-and heuristic for symmetrization of alignments, and Batch MIRA (Cherry and Foster, 2012) for tuning. Subwordlevel representation of sentences is long, hence we speed up decoding by using cube pruning with a smaller beam size (pop-limit=1000) for OS and BPE-level models. This setting has been shown to have minimal impact on translation quality (Kunchukuttan and Bhattacharyya, 2016a). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme-level models, and 10-gram LMs for OS, BPE, characterlevel models. We used the Indic NLP library2 for orthographic syllabification, the subword-nmt library3 for training BPE models and Morfessor (Virpioja et al., 2013) for morphologi"
I17-2048,W17-4102,1,0.706625,"tion) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate language, called the pivot language, to build the source-target translation system. The pivot approach makes no assumptions about source, pivot, and target language relatedness. Our work brings together subword-level translation and pivot-based SMT in low resource scenarios. We refer to orthographic syllables and byte pair encoded units as subwords. W"
I17-2048,P07-1092,0,0.0320213,"hukuttan and Bhattacharyya, 2016b, 2017). While OS units are approximate syllables, BPE units are highly frequent character sequences, some of them representing different linguistic units like syllables, morphemes and affixes. While orthographic syllabification applies to writing systems which represent vowels (alphabets and Pivoting using related language: We use a language related to both the source and target language as the pivot language. We explore two widely used pivoting techniques: phrase-table triangulation and pipelining. Triangulation (Utiyama and Isahara, 2007; Wu and Wang, 2007; Cohn and Lapata, 2007) “joins” the source-pivot and pivot-target subword-level phrase-tables on the common phrases in the pivot columns, generating the pivot model’s phrasetable. It recomputes the probabilities in the new source-target phrase-table, after making a few independence assumptions, as shown below: P (t¯|¯ s) = X P (t¯|¯ p)P (¯ p|¯ s) (1) p¯ where, s¯, p¯ and t¯ are source, pivot and target phrases respectively. In the pipelining/transfer approach (Utiyama and Isahara, 2007), a source sentence is first translated into the pivot language, and the pivot language translation is further translated into the t"
I17-2048,N15-1125,1,0.752889,"ong related languages is the key to building good-quality SMT systems with limited parallel corpora. Lexical similarity means that the languages share many words 283 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 283–289, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP abugidas), BPE can be applied to text in any writing system. the restricted case of related languages). Previous work on morpheme and word-level pivot models with multiple pivot languages have reported lower translation scores than the direct model (More et al., 2015; Dabre et al., 2015). Tiedemann (2012)’s work uses a character-level model in just one language pair of the triple (source-pivot or pivot-target) when the pivot is related to either the source or target (but not both). 2 Training subword-level models: We segment the data into subwords during pre-processing and indicate word boundaries by a boundary marker ( ) as shown in the example for OS below: word: Childhood means simplicity . subword: Chi ldhoo d mea ns si mpli ci ty . For building subword-level phrase-based models, we use (a) monotonic decoding since related languages have similar word order, (b) higher ord"
I17-2048,W15-5944,1,0.935113,"xical similarity among related languages is the key to building good-quality SMT systems with limited parallel corpora. Lexical similarity means that the languages share many words 283 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 283–289, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP abugidas), BPE can be applied to text in any writing system. the restricted case of related languages). Previous work on morpheme and word-level pivot models with multiple pivot languages have reported lower translation scores than the direct model (More et al., 2015; Dabre et al., 2015). Tiedemann (2012)’s work uses a character-level model in just one language pair of the triple (source-pivot or pivot-target) when the pivot is related to either the source or target (but not both). 2 Training subword-level models: We segment the data into subwords during pre-processing and indicate word boundaries by a boundary marker ( ) as shown in the example for OS below: word: Childhood means simplicity . subword: Chi ldhoo d mea ns si mpli ci ty . For building subword-level phrase-based models, we use (a) monotonic decoding since related languages have similar word"
I17-2048,P10-1048,0,0.024108,"izing Lexical Similarity between Related, Low-resource Languages for Pivot-based SMT Anoop Kunchukuttan, Maulik Shah Pradyot Prakash, Pushpak Bhattacharyya Department of Computer Science and Engineering Indian Institute of Technology Bombay {anoopk,maulik.shah,pradyot,pb}@cse.iitb.ac.in Abstract with similar form (spelling and pronunciation) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate"
I17-2048,P12-2059,0,0.100195,"Low-resource Languages for Pivot-based SMT Anoop Kunchukuttan, Maulik Shah Pradyot Prakash, Pushpak Bhattacharyya Department of Computer Science and Engineering Indian Institute of Technology Bombay {anoopk,maulik.shah,pradyot,pb}@cse.iitb.ac.in Abstract with similar form (spelling and pronunciation) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate language, called the pivot language, to build"
I17-2048,P02-1040,0,0.0990581,"for orthographic syllabification, the subword-nmt library3 for training BPE models and Morfessor (Virpioja et al., 2013) for morphological segmentation. These unsupervised morphological analyzers for Indian languages, described in Kunchukuttan et al. (2014), are trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006). The BPE vocabulary size was chosen to match OS vocab size. We use tmtriangulate4 for phrase-table triangulation and combine-ptables (Bisazza et al., 2011) for linear interpolation of phrase-tables. Evaluation: The primary evaluation metric is word-level BLEU (Papineni et al., 2002). We also report LeBLEU (Virpioja and Gr¨onroos, 2015) scores in the appendix. LeBLEU is a variant of BLEU that does soft-matching of words and has been shown to be better for morphologically rich languages. We use bootstrap resampling for testing statistical significance (Koehn, 2004). (2) i=1 where, s, pi and t are the source, ith best sourcepivot translation and target sentence respectively. Using Multiple Pivot Languages : We use multiple pivot languages by combining triangulated models corresponding to different pivot languages. Linear interpolation is used (Bisazza et al., 2011) for mode"
I17-2048,quasthoff-etal-2006-corpus,0,0.0329732,"ls. This setting has been shown to have minimal impact on translation quality (Kunchukuttan and Bhattacharyya, 2016a). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme-level models, and 10-gram LMs for OS, BPE, characterlevel models. We used the Indic NLP library2 for orthographic syllabification, the subword-nmt library3 for training BPE models and Morfessor (Virpioja et al., 2013) for morphological segmentation. These unsupervised morphological analyzers for Indian languages, described in Kunchukuttan et al. (2014), are trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006). The BPE vocabulary size was chosen to match OS vocab size. We use tmtriangulate4 for phrase-table triangulation and combine-ptables (Bisazza et al., 2011) for linear interpolation of phrase-tables. Evaluation: The primary evaluation metric is word-level BLEU (Papineni et al., 2002). We also report LeBLEU (Virpioja and Gr¨onroos, 2015) scores in the appendix. LeBLEU is a variant of BLEU that does soft-matching of words and has been shown to be better for morphologically rich languages. We use bootstrap resampling for testing statistical significance (Koehn, 2004). (2) i=1 where, s, pi and t a"
I17-2048,W12-5611,0,0.0295708,"Missing"
I17-2048,2009.eamt-1.3,0,0.0635171,"ology Bombay {anoopk,maulik.shah,pradyot,pb}@cse.iitb.ac.in Abstract with similar form (spelling and pronunciation) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate language, called the pivot language, to build the source-target translation system. The pivot approach makes no assumptions about source, pivot, and target language relatedness. Our work brings together subword-level trans"
I17-2048,E12-1015,0,0.101031,"ce Languages for Pivot-based SMT Anoop Kunchukuttan, Maulik Shah Pradyot Prakash, Pushpak Bhattacharyya Department of Computer Science and Engineering Indian Institute of Technology Bombay {anoopk,maulik.shah,pradyot,pb}@cse.iitb.ac.in Abstract with similar form (spelling and pronunciation) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate language, called the pivot language, to build"
I17-2048,W07-0705,0,0.159847,"n Institute of Technology Bombay {anoopk,maulik.shah,pradyot,pb}@cse.iitb.ac.in Abstract with similar form (spelling and pronunciation) and meaning viz. cognates, lateral borrowings or loan words from other languages e.g. , blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. For translation, lexical similarity can be utilized by transliteration of untranslated words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative approach involves the use of subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016b) and byte pair encoded units (Kunchukuttan and Bhattacharyya, 2017) have been used with varying degrees of success. On the other hand, if no parallel corpus is available between two languages, pivot-based SMT (Gispert and Marino, 2006; Utiyama and Isahara, 2007) provides a systematic way of using an intermediate language, called the pivot language, to build the source-target translation system. The pivot approach makes no assumptions about source, pivot, and target language relatedness. Our work brings together s"
I17-2048,W15-3052,0,0.0300374,"Missing"
I17-2048,J99-1003,0,\N,Missing
I17-2048,jha-2010-tdil,0,\N,Missing
I17-2048,P07-1108,0,\N,Missing
I17-2048,bojar-etal-2014-hindencorp,0,\N,Missing
I17-2048,I08-1067,1,\N,Missing
K16-1027,W15-3902,0,0.0193051,"ing the ambiguity in the grapheme-to-phoneme mapping. 5 6 Experiments Data: We experimented on the following Indian language pairs representing two language families: Bengali→Hindi, Kannada→Hindi, Hindi→Kannada and Tamil→Kannada. Bengali (bn) and Hindi (hi) are Indo-Aryan languages, while Kannada (kn) and Tamil (ta) are Dravidian languages. We used 10k source language names as training corpus, which were collected from various sources. We evaluated our systems on the NEWS 2015 Indic dataset. We created this set from the English to Indian language training corpora of the NEWS 2015 shared task (Banchs et al., 2015) by mining name pairs which have English names in common. 1500 words were selected at random to create the testset. The remaining pairs are used to train and tune a skyline supervised transliteration system for comparison. The training sets are small, the number of name pairs being: 2556 (bn-hi), 4022 (knhi), 3586 (hi-kn) and 3230 (ta-kn). Bootstrapping substring-based models In the second stage, we train a discriminative, loglinear transliteration model which learns substring mappings. We use the log-linear model proposed by Och and Ney (2002) for statistical machine translation and analogous"
K16-1027,N09-1034,0,0.0209179,"SubstringFigure 1: Overview of Proposed Approach tion based interlingual projection for multilingual transliteration mining. To the best of our knowledge, ours is the first work to use phonetic feature vectors for transliteration as opposed to transliteration mining. We use a substring-based log-linear model in our second stage. There are some parallels to this approach in the transliteration mining litereature. Some transliteration mining approaches have used a log-linear classifier to incorporate features to distinguish transliterations from non-transliterations (Klementiev and Roth, 2006; Chang et al., 2009). Sajjad et al. (2011) use a substring-based log-linear model trained on a noisy, intermediate transliteration corpus to iteratively remove bad (lowscoring) transliteration pairs found in the discovery process. 3 Unsupervised Substring-based Transliteration In this section, we give a high-level overview of our approach for learning a substring-based transliteration model in an unsupervised setting (depicted in Figure 1). The inputs are monolingual lists of words, WF and WE , for the source (F) and target (E) languages respectively. Note that these are neither parallel nor comparable lists. We"
K16-1027,N12-1047,0,0.0499422,"Missing"
K16-1027,2010.amta-papers.12,0,0.071539,"Missing"
K16-1027,D12-1002,0,0.455144,"Missing"
K16-1027,N10-1065,1,0.749874,"Missing"
K16-1027,P06-1103,0,0.267201,"ransliteration ambiguities. SubstringFigure 1: Overview of Proposed Approach tion based interlingual projection for multilingual transliteration mining. To the best of our knowledge, ours is the first work to use phonetic feature vectors for transliteration as opposed to transliteration mining. We use a substring-based log-linear model in our second stage. There are some parallels to this approach in the transliteration mining litereature. Some transliteration mining approaches have used a log-linear classifier to incorporate features to distinguish transliterations from non-transliterations (Klementiev and Roth, 2006; Chang et al., 2009). Sajjad et al. (2011) use a substring-based log-linear model trained on a noisy, intermediate transliteration corpus to iteratively remove bad (lowscoring) transliteration pairs found in the discovery process. 3 Unsupervised Substring-based Transliteration In this section, we give a high-level overview of our approach for learning a substring-based transliteration model in an unsupervised setting (depicted in Figure 1). The inputs are monolingual lists of words, WF and WE , for the source (F) and target (E) languages respectively. Note that these are neither parallel nor"
K16-1027,P06-2065,0,0.059408,"Missing"
K16-1027,P07-2045,0,0.00456309,"Missing"
K16-1027,N15-3017,1,0.80408,"Missing"
K16-1027,P02-1038,0,0.184332,"training corpora of the NEWS 2015 shared task (Banchs et al., 2015) by mining name pairs which have English names in common. 1500 words were selected at random to create the testset. The remaining pairs are used to train and tune a skyline supervised transliteration system for comparison. The training sets are small, the number of name pairs being: 2556 (bn-hi), 4022 (knhi), 3586 (hi-kn) and 3230 (ta-kn). Bootstrapping substring-based models In the second stage, we train a discriminative, loglinear transliteration model which learns substring mappings. We use the log-linear model proposed by Och and Ney (2002) for statistical machine translation and analogous transliteration features. The features are: substring transliteration probabilities, weighted average character transliteration probabilities and character language model score. The conditional probability of the target word e given the source word f is: P (e|f) = N P ∏ i=1 P (e¯i |f¯i ) = N P ∏ i=1 exp NF ∑ λj gj (f¯i , e¯i ) j=0 (11) ¯ where, fi and e¯i are source and target substrings respectively, λj and gj are feature weight and feature function respectively for feature j, N P number of substrings and N F is number of features. We synthes"
K16-1027,N09-1005,0,0.14977,"the top-1 transliterations in the synthesized, pseudo-parallel corpus; no true parallel corpus is used. Monotone decoding was performed. We used a 5-gram character language model trained with Witten-Bell smoothing on 40k names for all target languages. We ran Stage 2 for 5 iterations. For a rule-based baseline, we used the script conversion method implemented in the Indic NLP Library2 (Kunchukuttan et al., 2015) which is based on phonemic correspondences. poorly as reported in their work too. We also experimented with re-ranking the results using a unigram word based LM - our approximation to Ravi and Knight (2009)’s use of a word based LM - and its accuracy is comparable to PC_Init. The unigram LM was trained on a corpus of 185 million and 42 million tokens for hi and kn respectively. Thus, this knowledge-lite approach cannot learn a transliteration model effectively. Rule-based transliteration (Rule) performs significantly better than PC_Init. The phonetic nature of Indic scripts makes the rule-based system a stronger baseline, yet this simple approach does not ensure high accuracy transliteration. Phonetic changes like changes in manner/place of articulation, voicing, etc. make transliteration non-tr"
K16-1027,P11-1044,0,0.0232627,"verview of Proposed Approach tion based interlingual projection for multilingual transliteration mining. To the best of our knowledge, ours is the first work to use phonetic feature vectors for transliteration as opposed to transliteration mining. We use a substring-based log-linear model in our second stage. There are some parallels to this approach in the transliteration mining litereature. Some transliteration mining approaches have used a log-linear classifier to incorporate features to distinguish transliterations from non-transliterations (Klementiev and Roth, 2006; Chang et al., 2009). Sajjad et al. (2011) use a substring-based log-linear model trained on a noisy, intermediate transliteration corpus to iteratively remove bad (lowscoring) transliteration pairs found in the discovery process. 3 Unsupervised Substring-based Transliteration In this section, we give a high-level overview of our approach for learning a substring-based transliteration model in an unsupervised setting (depicted in Figure 1). The inputs are monolingual lists of words, WF and WE , for the source (F) and target (E) languages respectively. Note that these are neither parallel nor comparable lists. We need a phonemic repres"
K16-1027,P12-1049,0,0.0529545,"Missing"
K16-1027,P07-1119,0,0.0347206,"allel corpora, co-occurrence is no longer a learning signal and it is not possible to learn the character transliteration probabilities reliably. To compensate for this, we define Dirichlet priors (De ) over each character transliteration probability distributions (Θe ), which can be used to encode linguistic knowledge. This leads to our proposed EM-MAP training objective for the Mstep over the entire training set (WF ). based models, which learn substring mappings like .mba → mba, are one way to incorporate contextual information and have been shown to perform better in a supervised setting (Sherif and Kondrak, 2007). Contextual information is especially important in an unsupervised setting. 4 ∑ {∑{ ∑[ QWF (Θ) = δe,f σa,f,e ∑ + e P (e) a i=1 θfai ,ei }} + log P (e) F| ,e ) (3) j=|CF | ∀e ∈ CE , ∑ θfj ,e = 1 j=1 where, δe,f = P (e|f), σa,f,e = P (a|e, f) are conditional probabilities of the latent variables computed in the E-step. These are computed using the previous iteration’s parameter values, whose values are fixed in the current iteration. nf,e,a is the number of times characters e and f are aligned in the alignment structure a. CF and CE are the character sets of the source and target languages resp"
K16-1027,P05-1044,0,0.158297,"Missing"
K16-1027,W06-1630,0,0.0614127,"Missing"
khapra-etal-2014-transliteration,W10-2405,0,\N,Missing
khapra-etal-2014-transliteration,N10-1065,1,\N,Missing
khapra-etal-2014-transliteration,W10-2403,1,\N,Missing
khapra-etal-2014-transliteration,D08-1027,0,\N,Missing
khapra-etal-2014-transliteration,W09-3505,0,\N,Missing
khapra-etal-2014-transliteration,W10-0710,0,\N,Missing
khapra-etal-2014-transliteration,W10-0701,0,\N,Missing
khapra-etal-2014-transliteration,W10-2401,0,\N,Missing
khapra-etal-2014-transliteration,P12-1049,0,\N,Missing
khapra-etal-2014-transliteration,W10-0731,0,\N,Missing
khapra-etal-2014-transliteration,P11-1044,0,\N,Missing
khapra-etal-2014-transliteration,W09-3506,0,\N,Missing
khapra-etal-2014-transliteration,W10-0728,0,\N,Missing
khapra-etal-2014-transliteration,W10-0708,0,\N,Missing
khapra-etal-2014-transliteration,W09-3504,0,\N,Missing
kunchukuttan-etal-2012-experiences,E06-1032,0,\N,Missing
kunchukuttan-etal-2012-experiences,ambati-etal-2010-active,0,\N,Missing
kunchukuttan-etal-2012-experiences,D08-1027,0,\N,Missing
kunchukuttan-etal-2012-experiences,D08-1056,0,\N,Missing
kunchukuttan-etal-2012-experiences,W10-0710,0,\N,Missing
kunchukuttan-etal-2012-experiences,W10-0701,0,\N,Missing
kunchukuttan-etal-2012-experiences,D09-1030,0,\N,Missing
kunchukuttan-etal-2012-experiences,W10-0734,0,\N,Missing
kunchukuttan-etal-2012-experiences,W10-1701,0,\N,Missing
kunchukuttan-etal-2014-shata,W12-3152,0,\N,Missing
kunchukuttan-etal-2014-shata,jha-2010-tdil,0,\N,Missing
kunchukuttan-etal-2014-shata,P02-1040,0,\N,Missing
kunchukuttan-etal-2014-shata,W13-2807,0,\N,Missing
kunchukuttan-etal-2014-shata,2005.mtsummit-papers.11,0,\N,Missing
kunchukuttan-etal-2014-shata,W14-0130,1,\N,Missing
kunchukuttan-etal-2014-shata,I08-1067,1,\N,Missing
kunchukuttan-etal-2014-shata,2009.mtsummit-papers.7,0,\N,Missing
L18-1548,abdelali-etal-2014-amara,0,0.0794058,"l communication in Hindi and interfacing with the rest of the word via English. Hence, there is immense potential for EnglishHindi machine translation. However, the parallel corpora available in the public domain is quite limited. This work is an effort to consolidate all publicly available parallel corpora for English-Hindi as well as significantly add to the available parallel corpus through corpora collected in the course of this work. 2. Dataset The parallel corpus has been compiled from a variety of existing sources (primarily OPUS (Tiedemann, 2012), HindEn (Bojar et al., 2014b) and TED (Abdelali et al., 2014)) as well as corpora developed at the Center for Indian Language Technology2 (CFILT), IIT Bombay over the years. The training corpus consists of sentences, phrases as well 1 2 as dictionary entries, spanning many applications and domains. 2.1. Corpus Details The details of the training corpus are shown in Table 1. We briefly describe the new sub-corpora we have added to the collection. For the corpora compiled from existing sources, please refer to the papers mentioned in the table. Judicial domain corpus - I contains translations of legal judgements by in-house translators with many years of"
L18-1548,W05-0909,0,0.0852849,"he dimension of input and output embedding layers is 256 units. Training details: The model is trained with a batch size of 50 sentences and maximum sentence length of 100 using Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.0001. The output parameters were saved after every 10,000 iterations. We used early-stopping based on validation loss with patience=10. Decoding: We used a beam size of 12. We decoded the test set with an ensemble of four models (best model and the last three saved models). 3.4. Results We evaluated our system using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). We used a METEOR-Indic8 , a customized version of METEOR Indic, for evaluation of Hindi as target language. METEORIndic can perform synonym matches for Indian languages using synsets from IndoWordNet (Bhattacharyya, 2010). It can also perform stem matches for Indian languages using a trie-based stemmer (Bhattacharyya et al., 2014). This is useful for a morphologically rich language like Hindi. Table 3 shows the results of our experiments. 4. Availability The homepage for the dataset can be accessed here: http: //www.cfilt.iitb.ac.in/iitb_parallel. The new corpora we release are available for"
L18-1548,W14-0130,1,0.820852,"g based on validation loss with patience=10. Decoding: We used a beam size of 12. We decoded the test set with an ensemble of four models (best model and the last three saved models). 3.4. Results We evaluated our system using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). We used a METEOR-Indic8 , a customized version of METEOR Indic, for evaluation of Hindi as target language. METEORIndic can perform synonym matches for Indian languages using synsets from IndoWordNet (Bhattacharyya, 2010). It can also perform stem matches for Indian languages using a trie-based stemmer (Bhattacharyya et al., 2014). This is useful for a morphologically rich language like Hindi. Table 3 shows the results of our experiments. 4. Availability The homepage for the dataset can be accessed here: http: //www.cfilt.iitb.ac.in/iitb_parallel. The new corpora we release are available for research and non-commercial use under a Creative Commons Attribution-NonCommercial-ShareAlike License 9 . The corpora we compiled from other sources are available under their respective licenses. The sub-corpora (in the corpus distribution that we make available) are in the same order as listed in the Table 1, so they can be separa"
L18-1548,bojar-etal-2014-hindencorp,0,0.0231507,"Missing"
L18-1548,N12-1047,0,0.0293747,"In the other case, a single Unicode character represents the composite character. We choose the former representation. The normalization script is part of the IndicNLP4 library . For English, we used true-cased representation for our experiments. However, the parallel corpus being distributed is available in the original case. Tokenization: We use the Moses tokenizer for English and the IndicNLP tokenizer for Hindi. 3.2. SMT Setup We trained PBSMT systems with Moses5 (Koehn et al., 2007). We used the grow-diag-final-and heuristic for extracting phrases, lexicalised reordering and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram language models with Kneser-Ney smoothing using KenLM (Heafield, 2011). We used the HindMono (Bojar et al., 2014b) corpus for Hindi and the WMT NEWS Crawl 2015 corpus for English as additional monolingual corpora to train language models. These contain roughly 44 million and 23 million sentence for Hindi and English respectively. 3.3. We trained baseline machine translation models using the parallel corpus with popular off-the-shelf machine translation toolkits to provide benchmark translation accuracies for comparison. We trained phrase-base"
L18-1548,W11-2123,0,0.0369898,"zation script is part of the IndicNLP4 library . For English, we used true-cased representation for our experiments. However, the parallel corpus being distributed is available in the original case. Tokenization: We use the Moses tokenizer for English and the IndicNLP tokenizer for Hindi. 3.2. SMT Setup We trained PBSMT systems with Moses5 (Koehn et al., 2007). We used the grow-diag-final-and heuristic for extracting phrases, lexicalised reordering and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram language models with Kneser-Ney smoothing using KenLM (Heafield, 2011). We used the HindMono (Bojar et al., 2014b) corpus for Hindi and the WMT NEWS Crawl 2015 corpus for English as additional monolingual corpora to train language models. These contain roughly 44 million and 23 million sentence for Hindi and English respectively. 3.3. We trained baseline machine translation models using the parallel corpus with popular off-the-shelf machine translation toolkits to provide benchmark translation accuracies for comparison. We trained phrase-based Statistical Machine Translation (PBSMT) systems as well as Neural Data Preparation NMT Setup We trained a subword-level"
L18-1548,P07-2045,0,0.0122134,"th nukta can have two Unicode representations. In one case, the character and nukta are represented as two Unicode characters. In the other case, a single Unicode character represents the composite character. We choose the former representation. The normalization script is part of the IndicNLP4 library . For English, we used true-cased representation for our experiments. However, the parallel corpus being distributed is available in the original case. Tokenization: We use the Moses tokenizer for English and the IndicNLP tokenizer for Hindi. 3.2. SMT Setup We trained PBSMT systems with Moses5 (Koehn et al., 2007). We used the grow-diag-final-and heuristic for extracting phrases, lexicalised reordering and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram language models with Kneser-Ney smoothing using KenLM (Heafield, 2011). We used the HindMono (Bojar et al., 2014b) corpus for Hindi and the WMT NEWS Crawl 2015 corpus for English as additional monolingual corpora to train language models. These contain roughly 44 million and 23 million sentence for Hindi and English respectively. 3.3. We trained baseline machine translation models using the parallel corpus with po"
L18-1548,P13-4030,1,0.83594,"Missing"
L18-1548,P02-1040,0,0.112156,"r, containing 512 GRU units each. The dimension of input and output embedding layers is 256 units. Training details: The model is trained with a batch size of 50 sentences and maximum sentence length of 100 using Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.0001. The output parameters were saved after every 10,000 iterations. We used early-stopping based on validation loss with patience=10. Decoding: We used a beam size of 12. We decoded the test set with an ensemble of four models (best model and the last three saved models). 3.4. Results We evaluated our system using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). We used a METEOR-Indic8 , a customized version of METEOR Indic, for evaluation of Hindi as target language. METEORIndic can perform synonym matches for Indian languages using synsets from IndoWordNet (Bhattacharyya, 2010). It can also perform stem matches for Indian languages using a trie-based stemmer (Bhattacharyya et al., 2014). This is useful for a morphologically rich language like Hindi. Table 3 shows the results of our experiments. 4. Availability The homepage for the dataset can be accessed here: http: //www.cfilt.iitb.ac.in/iitb_parallel. The ne"
L18-1548,I08-1067,1,0.795954,"lish-Hindi Parallel corpus version 1.0, and provided benchmark baseline SMT and NMT results on this corpus. This corpus has been used for the two shared tasks (Workshop on Asian Language Translation 2016 and 2017). The HindiEn component of the corpus has also been used for the WMT 2014 shared task. The corpus is available under a Creative Commons Licence. In future, we plan to enhance the corpus from additional sources, mostly websites of the Government of India which is still a largely untapped source of parallel corpora. We also plan to build stronger baselines like pre-ordering with PBSMT (Ramanathan et al., 2008) for English-Hindi translation, and use of synthetic corpora generated via backtranslation for NMT systems (Sennrich et al., 2016a). 6. Acknowledgements We thank past and present members of the Center for Indian Language Technology for their efforts in creating various parts of the corpora over the years: Pallabh Bhattacharjee, Kashyap Popat, Rahul Sharnagat, Mitesh Khapra, Jaya Jha, Rajita Shukla, Laxmi Kashyap, Gajanan Rane and many members of the Hindi WordNet team. We also thank the Technology Development for Indian Languages (TDIL) Programme and the Department of Electronics & Information"
L18-1548,P16-1162,0,0.0295925,"d Statistical Machine Translation (PBSMT) systems as well as Neural Data Preparation NMT Setup We trained a subword-level encoder-decoder architecture based NMT system with attention (Bahdanau et al., 2015). 3474 4 5 anoopkunchukuttan.github.io/indic_nlp_library www.statmt.org/moses System SMT NMT eng-hin 5. hin-eng BLEU METEOR BLEU METEOR 11.75 12.23 0.313 0.308 14.49 12.83 0.266 0.219 Table 3: Results for Baseline Systems We used Nematus 6 (Sennrich et al., 2017) for training our NMT systems. Vocabulary: We used Byte Pair Encoding (BPE) to learn the vocabulary (with 15500 merge operations) (Sennrich et al., 2016b). We used the subword-nmt 7 tool for learning the BPE vocabulary. Since the writing systems and vocabularies of English and Hindi are separate, BPE models are trained separately. Network parameters: The network contains a single hidden encoder and decoder RNN layer, containing 512 GRU units each. The dimension of input and output embedding layers is 256 units. Training details: The model is trained with a batch size of 50 sentences and maximum sentence length of 100 using Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.0001. The output parameters were saved after every 10,000"
L18-1548,E17-3017,0,0.061118,"Missing"
L18-1548,tiedemann-2012-parallel,0,0.367394,"speakers. Hence, there is a large requirement for digital communication in Hindi and interfacing with the rest of the word via English. Hence, there is immense potential for EnglishHindi machine translation. However, the parallel corpora available in the public domain is quite limited. This work is an effort to consolidate all publicly available parallel corpora for English-Hindi as well as significantly add to the available parallel corpus through corpora collected in the course of this work. 2. Dataset The parallel corpus has been compiled from a variety of existing sources (primarily OPUS (Tiedemann, 2012), HindEn (Bojar et al., 2014b) and TED (Abdelali et al., 2014)) as well as corpora developed at the Center for Indian Language Technology2 (CFILT), IIT Bombay over the years. The training corpus consists of sentences, phrases as well 1 2 as dictionary entries, spanning many applications and domains. 2.1. Corpus Details The details of the training corpus are shown in Table 1. We briefly describe the new sub-corpora we have added to the collection. For the corpora compiled from existing sources, please refer to the papers mentioned in the table. Judicial domain corpus - I contains translations o"
N15-3017,E14-4029,0,0.0274247,"is described by Antony, P. J. and Soman, K.P. (2011). 3.1 Transliteration Mining Statistical transliteration can address these challenges by learning transliteration divergences from a parallel transliteration corpus. For most Indian language pairs, parallel transliteration corpora are not publicly available. Hence, we mine transliteration corpora for 110 language pairs from the ILCI corpus, a parallel translation corpora of 11 Indian languages (Jha, 2012). Transliteration pairs are mined using the unsupervised approach proposed by Sajjad et al. (2012) and implemented in the Moses SMT system (Durrani et al., 2014). Their approach models parallel translation corpus generation as a generative process comprising an interpolation of a transliteration and a non-transliteration process. The parameters of the generative process are learnt using the EM procedure, followed by extraction of transliteration pairs from the parallel corpora. Table 1 shows the statistics of mined pairs. We mined a total of 1.69 million word pairs for 110 language pairs. We observed disparity in the counts of mined transliteration pairs across languages. Language pairs of the Indo-Aryan family from geographically contiguous regions h"
N15-3017,kunchukuttan-etal-2014-shata,1,0.843383,"in IndoWordNet3 Though this corpus does not reflect the diversity in the mined transliterations, evaluation on this corpus could be a pointer to utility of the transliteration corpus. We compare the accuracy of match for transliteration 3 http://www.cfilt.iitb.ac.in/indowordnet 84 Our work in developing the transliteration systems was initially motivated by the need for transliterating the untranslated words in SMT output. To evaluate the transliteration systems in the context of machine translation, we post-edited the phrase based system (PB-SMT) outputs of Indian language pairs provided by Kunchukuttan et al. (2014) using our transliteration systems. Each untranslated word was replaced by each of its top-1000 transliterations and the resulting candidate sentences were re-ranked using a language model. We observe a significant improvement in translation quality across language pairs, as measured by the BLEU evaluation metric. Due to space constraints, we present results for only 8 language pairs in Table 3. We observed that though the system&apos;s best transliteration is not always correct, the sentence context and the language model select the right transliteration from the top-k transliteration Lang Pair PB"
N15-3017,P12-1049,0,0.112805,"11). A summary of the challenges specific to Indian languages is described by Antony, P. J. and Soman, K.P. (2011). 3.1 Transliteration Mining Statistical transliteration can address these challenges by learning transliteration divergences from a parallel transliteration corpus. For most Indian language pairs, parallel transliteration corpora are not publicly available. Hence, we mine transliteration corpora for 110 language pairs from the ILCI corpus, a parallel translation corpora of 11 Indian languages (Jha, 2012). Transliteration pairs are mined using the unsupervised approach proposed by Sajjad et al. (2012) and implemented in the Moses SMT system (Durrani et al., 2014). Their approach models parallel translation corpus generation as a generative process comprising an interpolation of a transliteration and a non-transliteration process. The parameters of the generative process are learnt using the EM procedure, followed by extraction of transliteration pairs from the parallel corpora. Table 1 shows the statistics of mined pairs. We mined a total of 1.69 million word pairs for 110 language pairs. We observed disparity in the counts of mined transliteration pairs across languages. Language pairs of"
N15-3017,jha-2010-tdil,0,\N,Missing
N16-4006,P06-2112,0,\N,Missing
N16-4006,P07-1108,0,\N,Missing
N16-4006,W07-0705,0,\N,Missing
N16-4006,P15-2021,0,\N,Missing
N19-1387,W14-5105,1,0.916554,"e addressed before/during construction of the contextual embeddings in the Bi-LSTM layer. Joty et al. (2017) use adversarial training for cross-lingual questionquestion similarity ranking. The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations. Pre-ordering the source language sentences to match the target language word order has been found useful in addressing word-order divergence for Phrase-Based SMT (Collins et al., 2005; Ramanathan et al., 2008; Navratil et al., 2012; Chatterjee et al., 2014). For NMT, Ponti et al. (2018) and Kawara et al. (2018) have explored preordering. Ponti et al. (2018) demonstrated that by reducing the syntactic divergence between the source and the target languages, consistent improvements in NMT performance can be obtained. On the contrary, Kawara et al. (2018) reported drop in NMT performance due to pre-ordering. Note that these works address source-target divergence, not divergence between source languages in multilingual NMT scenario. 3 Proposed Solution Consider the task of translating for an extremely low-resource language pair. The parallel corpus b"
N19-1387,P05-1066,0,0.216311,"any significant improvements, possibly because the divergence has to be addressed before/during construction of the contextual embeddings in the Bi-LSTM layer. Joty et al. (2017) use adversarial training for cross-lingual questionquestion similarity ranking. The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations. Pre-ordering the source language sentences to match the target language word order has been found useful in addressing word-order divergence for Phrase-Based SMT (Collins et al., 2005; Ramanathan et al., 2008; Navratil et al., 2012; Chatterjee et al., 2014). For NMT, Ponti et al. (2018) and Kawara et al. (2018) have explored preordering. Ponti et al. (2018) demonstrated that by reducing the syntactic divergence between the source and the target languages, consistent improvements in NMT performance can be obtained. On the contrary, Kawara et al. (2018) reported drop in NMT performance due to pre-ordering. Note that these works address source-target divergence, not divergence between source languages in multilingual NMT scenario. 3 Proposed Solution Consider the task of tran"
N19-1387,Y17-1038,0,0.0240468,"ow that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to major improvements in the translation quality in extremely low-resource scenarios. 1 Introduction Transfer learning for multilingual Neural Machine Translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Nguyen and Chiang, 2017) attempts to improve the NMT performance on the source to target language pair (child task) using an assisting source language (assisting to target language translation is the parent task). Here, the parent model is trained on the assisting and target language parallel corpus and the trained weights are used to initialize the child model. If source-target language pair parallel corpus is available, the child model can further be fine-tuned. The weight initialization reduces the requirement on the training data for the source-target language pair by transferring knowle"
N19-1387,L18-1550,0,0.0214217,"etwork: We use OpenNMT-Torch (Klein et al., 2018) to train the NMT system. We use the standard encoder-attention-decoder architecture (Bahdanau et al., 2015) with input-feeding approach (Luong et al., 2015). The encoder has two layers of bidirectional LSTMs with 500 neurons each and the decoder contains two LSTM layers with 500 neurons each. We use a mini-batch of size 50 and a dropout layer. We begin with an initial learning rate of 1.0 and continue training with exponential decay till the learning rate falls below 0.001. The English input is initialized with pre-trained fastText embeddings (Grave et al., 2018) 2 . English and Hindi vocabularies consists of 0.27M and 50K tokens appearing at least 2 and 5 times in the English and Hindi training corpus respectively. For representing English and other source languages into a common space, we translate each word in the source language into English using a bilingual dictionary (we used Google Translate to get single word translations). In an end-to-end solution, it would be ideal to use bilingual embeddings or obtain word-by-word translations via bilingual embeddings (Xie et al., 2018). However, publicly available bilingual embeddings for English-Indian"
N19-1387,Q19-1007,1,0.861339,"the English and Hindi training corpus respectively. For representing English and other source languages into a common space, we translate each word in the source language into English using a bilingual dictionary (we used Google Translate to get single word translations). In an end-to-end solution, it would be ideal to use bilingual embeddings or obtain word-by-word translations via bilingual embeddings (Xie et al., 2018). However, publicly available bilingual embeddings for English-Indian languages are not good enough for obtaining good-quality, bilingual representations (Smith et al., 2017; Jawanpuria et al., 2019) and publicly available bilingual dictionaries have limited coverage. The focus of our study is the in1 The corpus is available on request from http:// tdil-dc.in/index.php?lang=en 2 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors.md Language BLEU No Pre-Order Bengali Gujarati Marathi Malayalam Tamil 6.72 9.81 8.77 5.73 4.86 LeBLEU Pre-Ordered HT G 8.83 14.34 10.18 6.49 6.04 9.19 13.90 10.30 6.95 6.00 No Pre-Order 37.10 43.21 40.21 33.27 29.38 Pre-Ordered HT 41.50 47.36 41.49 33.69 30.77 G 42.01 47.60 42.22 35.09 31.33 Table 2: Transfer learning results for X -Hind"
N19-1387,jha-2010-tdil,0,0.0821231,", Gujarati, Marathi, Malayalam and Tamil are the source languages, and translation from these to Hindi constitute the child tasks. Hindi, Bengali, Gujarati and Marathi are Indo-Aryan languages, while Malayalam and Tamil are Dravidian languages. All these languages have a canonical SOV word order. Datasets: For training English-Hindi NMT systems, we use the IITB English-Hindi parallel corpus (Kunchukuttan et al., 2018) (1.46M sentences from the training set) and the ILCI English-Hindi parallel corpus (44.7K sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus (Jha, 2010)1 spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB parallel corpus for validation. For each child task, we use 2K sentences from ILCI corpus as test set. Network: We use OpenNMT-Torch (Klein et al., 2018) to train the NMT system. We use the standard encoder-attention-decoder architecture (Bahdanau et al., 2015) with input-feeding approach (Luong et al., 2015). The encoder has two layers of bidirectional LSTMs with 500 neurons each and the decoder contains two LSTM layers with 500 neurons each. We use a mini-batch of size 50 and a"
N19-1387,K17-1024,0,0.0223941,"eedings of NAACL-HLT 2019, pages 3868–3873 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics for multilingual NMT. However, some work exists for other NLP tasks in a multilingual setting. For Named Entity Recognition (NER), Xie et al. (2018) use a self-attention layer after the Bi-LSTM layer to address word-order divergence for Named Entity Recognition (NER) task. The approach does not show any significant improvements, possibly because the divergence has to be addressed before/during construction of the contextual embeddings in the Bi-LSTM layer. Joty et al. (2017) use adversarial training for cross-lingual questionquestion similarity ranking. The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations. Pre-ordering the source language sentences to match the target language word order has been found useful in addressing word-order divergence for Phrase-Based SMT (Collins et al., 2005; Ramanathan et al., 2008; Navratil et al., 2012; Chatterjee et al., 2014). For NMT, Ponti et al. (2018) and Kawara et al. (2018) have explored preordering. P"
N19-1387,P18-3004,0,0.0468978,"Missing"
N19-1387,W18-1817,0,0.0252691,"uages have a canonical SOV word order. Datasets: For training English-Hindi NMT systems, we use the IITB English-Hindi parallel corpus (Kunchukuttan et al., 2018) (1.46M sentences from the training set) and the ILCI English-Hindi parallel corpus (44.7K sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus (Jha, 2010)1 spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB parallel corpus for validation. For each child task, we use 2K sentences from ILCI corpus as test set. Network: We use OpenNMT-Torch (Klein et al., 2018) to train the NMT system. We use the standard encoder-attention-decoder architecture (Bahdanau et al., 2015) with input-feeding approach (Luong et al., 2015). The encoder has two layers of bidirectional LSTMs with 500 neurons each and the decoder contains two LSTM layers with 500 neurons each. We use a mini-batch of size 50 and a dropout layer. We begin with an initial learning rate of 1.0 and continue training with exponential decay till the learning rate falls below 0.001. The English input is initialized with pre-trained fastText embeddings (Grave et al., 2018) 2 . English and Hindi vocabul"
N19-1387,W04-3250,0,0.381732,"Missing"
N19-1387,L18-1548,1,0.765156,"h, datasets used, the network hyperparameters used in our experiments. Languages: We experimented with English → Hindi translation as the parent task. English is 3869 the assisting source language. Bengali, Gujarati, Marathi, Malayalam and Tamil are the source languages, and translation from these to Hindi constitute the child tasks. Hindi, Bengali, Gujarati and Marathi are Indo-Aryan languages, while Malayalam and Tamil are Dravidian languages. All these languages have a canonical SOV word order. Datasets: For training English-Hindi NMT systems, we use the IITB English-Hindi parallel corpus (Kunchukuttan et al., 2018) (1.46M sentences from the training set) and the ILCI English-Hindi parallel corpus (44.7K sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus (Jha, 2010)1 spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB parallel corpus for validation. For each child task, we use 2K sentences from ILCI corpus as test set. Network: We use OpenNMT-Torch (Klein et al., 2018) to train the NMT system. We use the standard encoder-attention-decoder architecture (Bahdanau et al., 2015) with input-feeding approach (Luong"
N19-1387,kunchukuttan-etal-2014-shata,1,0.903943,"Missing"
N19-1387,Q17-1026,0,0.0287173,"anguages are related (Zoph et al., 2016; Nguyen and Chiang, 2017; Dabre et al., 2017). Zoph et al. (2016) studied the influence of language divergence between languages chosen for training the parent and the child model, and showed that choosing similar languages for training the parent and the child model leads to better improvements from transfer learning. Several studies have tried to address the lexical divergence between the source and the target languages either by using Byte Pair Encoding (BPE) as basic input representation units (Nguyen and Chiang, 2017) or character-level NMT system (Lee et al., 2017) or bilingual embeddings (Gu et al., 2018). However, the effect of word order divergence and its mitigation has not been explored. In a practical setting, it is not uncommon to have source and assisting languages with different word order. For instance, it is possible to find parallel corpora between English (SVO word order) and some Indian (SOV word order) languages, but very little parallel corpora between Indian languages. Hence, it is natural to use English as an assisting language for inter-Indian language translation. To address the word order divergence, we propose to pre-order the assi"
N19-1387,W13-2807,0,0.0175236,"der divergence on Multilingual NMT. We do not want bilingual embeddings quality or bilingual dictionary coverage to influence the experiments, rendering our conclusions unreliable. Hence, we use the above mentioned largecoverage bilingual dictionary. Pre-ordering: We use CFILT-preorder3 for prereordering English sentences. It contains two preordering configurations: (1) generic rules (G) that apply to all Indian languages (Ramanathan et al., 2008), and (2) hindi-tuned rules (HT) which improves generic rules by incorporating improvements found through error analysis of EnglishHindi reordering (Patel et al., 2013). The Hindituned rules improve translation for other English to Indian language pairs too (Kunchukuttan et al., 2014). 5 Results We experiment with two scenarios: (a) an extremely resource scarce scenario with no parallel corpus for child tasks, (b) varying amounts of parallel corpora available for child task. 5.1 No Parallel Corpus for Child Task The results from our experiments are presented in the Table 2. We report BLEU scores and LeBLEU4 3 https://github.com/anoopkunchukuttan/ cfilt_preorder 4 LeBLEU (Levenshtein Edit BLEU) is a variant of BLEU that does a soft-match of reference and outp"
N19-1387,P18-1142,0,0.0194136,"ion of the contextual embeddings in the Bi-LSTM layer. Joty et al. (2017) use adversarial training for cross-lingual questionquestion similarity ranking. The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations. Pre-ordering the source language sentences to match the target language word order has been found useful in addressing word-order divergence for Phrase-Based SMT (Collins et al., 2005; Ramanathan et al., 2008; Navratil et al., 2012; Chatterjee et al., 2014). For NMT, Ponti et al. (2018) and Kawara et al. (2018) have explored preordering. Ponti et al. (2018) demonstrated that by reducing the syntactic divergence between the source and the target languages, consistent improvements in NMT performance can be obtained. On the contrary, Kawara et al. (2018) reported drop in NMT performance due to pre-ordering. Note that these works address source-target divergence, not divergence between source languages in multilingual NMT scenario. 3 Proposed Solution Consider the task of translating for an extremely low-resource language pair. The parallel corpus between the two languages, if a"
N19-1387,I08-1067,1,0.878884,"ements, possibly because the divergence has to be addressed before/during construction of the contextual embeddings in the Bi-LSTM layer. Joty et al. (2017) use adversarial training for cross-lingual questionquestion similarity ranking. The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations. Pre-ordering the source language sentences to match the target language word order has been found useful in addressing word-order divergence for Phrase-Based SMT (Collins et al., 2005; Ramanathan et al., 2008; Navratil et al., 2012; Chatterjee et al., 2014). For NMT, Ponti et al. (2018) and Kawara et al. (2018) have explored preordering. Ponti et al. (2018) demonstrated that by reducing the syntactic divergence between the source and the target languages, consistent improvements in NMT performance can be obtained. On the contrary, Kawara et al. (2018) reported drop in NMT performance due to pre-ordering. Note that these works address source-target divergence, not divergence between source languages in multilingual NMT scenario. 3 Proposed Solution Consider the task of translating for an extremely"
N19-1387,W15-3052,0,0.0141808,"ranslation, which is also reflected in the BLEU scores and Table 4. 5.2 Parallel Corpus for Child Task We study the impact of child task parallel corpus on pre-ordering. To this end, we finetune the parent task model with the child task parallel corpus. Table 5 shows the results for Bengali-Hindi, Gujarati-Hindi, Marathi-Hindi, Malayalam-Hindi, and Tamil-Hindi translation. We observe that pre-ordering is beneficial when almost no child task corpus is available. As the child task corpus increases, the model learns the on edit distance, hence it can handle morphological variations and cognates (Virpioja and Grönroos, 2015). word order of the source language; hence, the non pre-ordering models perform almost as good as or sometimes better than the pre-ordered ones. The non pre-ordering model is able to forget the wordorder of English and learn the word order of Indian languages. We attribute this behavior of the non pre-ordered model to the phenomenon of catastrophic forgetting (McCloskey and Cohen, 1989; French, 1999) which enables the model to learn the word-order of the source language when sufficient child task parallel corpus is available. We also compare the performance of the finetuned model with the mode"
N19-1387,D18-1034,0,0.09813,"trained scenario, where there is no parallel corpus for the child task. From our experiments, we show that there is a significant increase in the translation accuracy for the unseen source-target language pair. 2 Related Work To the best of our knowledge, no work has addressed word order divergence in transfer learning 3868 Proceedings of NAACL-HLT 2019, pages 3868–3873 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics for multilingual NMT. However, some work exists for other NLP tasks in a multilingual setting. For Named Entity Recognition (NER), Xie et al. (2018) use a self-attention layer after the Bi-LSTM layer to address word-order divergence for Named Entity Recognition (NER) task. The approach does not show any significant improvements, possibly because the divergence has to be addressed before/during construction of the contextual embeddings in the Bi-LSTM layer. Joty et al. (2017) use adversarial training for cross-lingual questionquestion similarity ranking. The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations. Pre-orderi"
N19-1387,D16-1163,0,0.213976,"rce language. We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to major improvements in the translation quality in extremely low-resource scenarios. 1 Introduction Transfer learning for multilingual Neural Machine Translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Nguyen and Chiang, 2017) attempts to improve the NMT performance on the source to target language pair (child task) using an assisting source language (assisting to target language translation is the parent task). Here, the parent model is trained on the assisting and target language parallel corpus and the trained weights are used to initialize the child model. If source-target language pair parallel corpus is available, the child model can further be fine-tuned. The weight initialization reduces the requirement on the training data for the source-target language pair by"
N19-1387,D15-1166,0,0.03224,"2018) (1.46M sentences from the training set) and the ILCI English-Hindi parallel corpus (44.7K sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus (Jha, 2010)1 spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB parallel corpus for validation. For each child task, we use 2K sentences from ILCI corpus as test set. Network: We use OpenNMT-Torch (Klein et al., 2018) to train the NMT system. We use the standard encoder-attention-decoder architecture (Bahdanau et al., 2015) with input-feeding approach (Luong et al., 2015). The encoder has two layers of bidirectional LSTMs with 500 neurons each and the decoder contains two LSTM layers with 500 neurons each. We use a mini-batch of size 50 and a dropout layer. We begin with an initial learning rate of 1.0 and continue training with exponential decay till the learning rate falls below 0.001. The English input is initialized with pre-trained fastText embeddings (Grave et al., 2018) 2 . English and Hindi vocabularies consists of 0.27M and 50K tokens appearing at least 2 and 5 times in the English and Hindi training corpus respectively. For representing English and o"
N19-1387,C12-1125,0,0.0179425,"the divergence has to be addressed before/during construction of the contextual embeddings in the Bi-LSTM layer. Joty et al. (2017) use adversarial training for cross-lingual questionquestion similarity ranking. The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations. Pre-ordering the source language sentences to match the target language word order has been found useful in addressing word-order divergence for Phrase-Based SMT (Collins et al., 2005; Ramanathan et al., 2008; Navratil et al., 2012; Chatterjee et al., 2014). For NMT, Ponti et al. (2018) and Kawara et al. (2018) have explored preordering. Ponti et al. (2018) demonstrated that by reducing the syntactic divergence between the source and the target languages, consistent improvements in NMT performance can be obtained. On the contrary, Kawara et al. (2018) reported drop in NMT performance due to pre-ordering. Note that these works address source-target divergence, not divergence between source languages in multilingual NMT scenario. 3 Proposed Solution Consider the task of translating for an extremely low-resource language p"
N19-1387,I17-2050,0,0.222385,"rd order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to major improvements in the translation quality in extremely low-resource scenarios. 1 Introduction Transfer learning for multilingual Neural Machine Translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Nguyen and Chiang, 2017) attempts to improve the NMT performance on the source to target language pair (child task) using an assisting source language (assisting to target language translation is the parent task). Here, the parent model is trained on the assisting and target language parallel corpus and the trained weights are used to initialize the child model. If source-target language pair parallel corpus is available, the child model can further be fine-tuned. The weight initialization reduces the requirement on the training data for the source-target language pair by transferring knowledge from the parent task,"
P13-4030,D09-1030,0,0.484519,"hrase translations. TransDoop incorporates quality control mechanisms and easy-to-use worker user interfaces designed to address issues with translation crowdsourcing. We have evaluated the crowd’s output using the METEOR metric. For a complex domain like judicial proceedings, the higher scores obtained by the map-reduce based approach compared to complete sentence translation establishes the efficacy of our work. 1 Introduction Crowdsourcing is no longer a new term in the domain of Computational Linguistics and Machine Translation research (Callison-Burch and Dredze, 2010; Snow et al., 2008; Callison-Burch, 2009). Crowdsourcing - basically where task outsourcing is delegated to a largely unknown Internet audience - is emerging as a new paradigm of human in the loop approaches for developing sophisticated techniques for understanding and generating natural language content. Amazon Mechanical 1 http://www.mturk.com,http://www. crowdflower.com 2 http://www.lingotek.com,http:///www. gengo.com 3 http://en.wikipedia.org/wiki/List_of_ languages_by_total_number_of_speakers 175 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 175–180, c Sofia, Bulgaria, August 4-9"
P13-4030,W10-1701,0,0.0171192,"4 describes the system architecture and workflow, while Section 5 presents important aspects of the user interfaces in the system. We present our preliminary experiments and observations in Section 6. Section 7 concludes the paper, pointing to future directions. 2 Related Work Lately, crowdsourcing has been explored as a source for generating data for NLP tasks (Snow et al., 2008; Callison-Burch and Dredze, 2010). Specifically, it has been explored as a channel for collecting different resources for SMT - evaluations of MT output (Callison-Burch, 2009), word alignments in parallel sentences (Gao et al., 2010) and post-edited versions of MT output (Aikawa et al., 2012). Ambati and Vogel (2010), Kunchukuttan et al. (2012) have shown the feasibility of crowdsourcing for collecting parallel corpora and 3 Multi-Stage Crowdsourcing Pipeline Our system is based on a multi-stage pipeline, whose central idea is to simplify the translation task into smaller tasks. The high level block diagram of the system is shown in Figure 1. Source language documents are sentencified using standard NLP tokenizers and sentence splitters. Extracted sentences are then split into phrases using a standard chunker and rule-bas"
P13-4030,kunchukuttan-etal-2012-experiences,1,0.844063,"r interfaces in the system. We present our preliminary experiments and observations in Section 6. Section 7 concludes the paper, pointing to future directions. 2 Related Work Lately, crowdsourcing has been explored as a source for generating data for NLP tasks (Snow et al., 2008; Callison-Burch and Dredze, 2010). Specifically, it has been explored as a channel for collecting different resources for SMT - evaluations of MT output (Callison-Burch, 2009), word alignments in parallel sentences (Gao et al., 2010) and post-edited versions of MT output (Aikawa et al., 2012). Ambati and Vogel (2010), Kunchukuttan et al. (2012) have shown the feasibility of crowdsourcing for collecting parallel corpora and 3 Multi-Stage Crowdsourcing Pipeline Our system is based on a multi-stage pipeline, whose central idea is to simplify the translation task into smaller tasks. The high level block diagram of the system is shown in Figure 1. Source language documents are sentencified using standard NLP tokenizers and sentence splitters. Extracted sentences are then split into phrases using a standard chunker and rule-based merging of small chunks. This step creates small phrases 176 Figure 1: Multistage crowdsourced translation req"
P13-4030,W12-3152,0,0.0131872,"issue for successful translation crowdsourcing. The most popular methods for quality control of crowdsourced tasks are based on sampling and redundancy. For translation crowdsourcing, Ambati et al. (2010) use inter-translator agreement for selection of a good translation from multiple, redundant worker translations. Zaidan and CallisonBurch (2011) score translations using a feature based model comprising sentence level, worker level and crowd ranking based features. However, automatic evaluation of translation quality is difficult, such automatic methods being either inaccurate or expensive. Post et al. (2012) have collected Indic language corpora data utilizing the crowd for collecting translations as well as validations. The quality of the validations is ensured using goldstandard sentence translations. Our approach to quality control is similar to Post et al. (2012), but we work at the level of phrases. While most crowdsourcing activities for data gathering has been concerned with collecting simple annotations like relevance judgments, there has been work to explore the use of crowdsourcing for more complex tasks, of which translation is a good example. Little et al. (2010) propose that many com"
P13-4030,W10-0710,0,0.147473,"ortant aspects of the user interfaces in the system. We present our preliminary experiments and observations in Section 6. Section 7 concludes the paper, pointing to future directions. 2 Related Work Lately, crowdsourcing has been explored as a source for generating data for NLP tasks (Snow et al., 2008; Callison-Burch and Dredze, 2010). Specifically, it has been explored as a channel for collecting different resources for SMT - evaluations of MT output (Callison-Burch, 2009), word alignments in parallel sentences (Gao et al., 2010) and post-edited versions of MT output (Aikawa et al., 2012). Ambati and Vogel (2010), Kunchukuttan et al. (2012) have shown the feasibility of crowdsourcing for collecting parallel corpora and 3 Multi-Stage Crowdsourcing Pipeline Our system is based on a multi-stage pipeline, whose central idea is to simplify the translation task into smaller tasks. The high level block diagram of the system is shown in Figure 1. Source language documents are sentencified using standard NLP tokenizers and sentence splitters. Extracted sentences are then split into phrases using a standard chunker and rule-based merging of small chunks. This step creates small phrases 176 Figure 1: Multistage"
P13-4030,D08-1027,0,0.210716,"Missing"
P13-4030,W05-0909,0,0.0470973,"es may not map, making translation difficult. For instance, the vaala modifier in Hindi translates to a clause in English. It does not contain any tense information, therefore the tense of the English clause cannot be determined by the worker. e.g. Using TransDoop, we conducted a set of smallscale, preliminary translation experiments. We obtained translations for English-Hindi and EnglishMarathi language pairs for the Judicial and Tourism domains. For each experiment, 15 sentences were given as input to the pipeline. For evaluation, we chose METEOR, a well-known translation evaluation metric (Banerjee and Lavie, 2005). We compared the results obtained from the crowdsourcing system with a expert human translation and the output of Google Translate. We also compared two expert translations using METEOR to establish a skyline for the translation accuracy. Table 1 summarizes the results of our experiments. The translations with Quality Control and multistage pipeline are better than Google translations and translations obtained from the crowd without any quality control, as evaluated by METEOR. Multi-stage translation yields better than complete sentence translation. Moreover, the translation quality is compar"
P13-4030,P11-1122,0,0.026633,"Missing"
P13-4030,ambati-etal-2010-active,0,\N,Missing
P13-4030,W10-0701,0,\N,Missing
P18-2064,P10-2041,0,0.0684589,"ta to Spanish, the tag distribution of China is skewed towards Location entity in Spanish. This leads to a drop in named entity recognition performance. In this work, we address this problem of drift in tag distribution owing to adding training data from a supporting language. The problem is similar to the problem of data selection for domain adaptation of various NLP tasks, except that additional complexity is introduced due to the multilingual nature of the learning task. For domain adaptation in various NLP tasks, several approaches have been proposed to address drift in data distribution (Moore and Lewis, 2010; Axelrod et al., 2011; Ruder and Plank, 2017). For instance, in machine translation, sentences from out-of-domain data are selected based on a suitably defined metric (Moore and Lewis, 2010; Axelrod et al., 2011). The metric attempts to capture similarity of the out-of-domain sentences with the in-domain data. Out-of-domain sentences most similar to the in-domain data are added. Like the domain adaptation techniques summarized above, we propose to judiciously add sentences from the assisting language to the primary language data based on the divergence between the tag distributions of named e"
P18-2064,D11-1033,0,0.0745883,"distribution of China is skewed towards Location entity in Spanish. This leads to a drop in named entity recognition performance. In this work, we address this problem of drift in tag distribution owing to adding training data from a supporting language. The problem is similar to the problem of data selection for domain adaptation of various NLP tasks, except that additional complexity is introduced due to the multilingual nature of the learning task. For domain adaptation in various NLP tasks, several approaches have been proposed to address drift in data distribution (Moore and Lewis, 2010; Axelrod et al., 2011; Ruder and Plank, 2017). For instance, in machine translation, sentences from out-of-domain data are selected based on a suitably defined metric (Moore and Lewis, 2010; Axelrod et al., 2011). The metric attempts to capture similarity of the out-of-domain sentences with the in-domain data. Out-of-domain sentences most similar to the in-domain data are added. Like the domain adaptation techniques summarized above, we propose to judiciously add sentences from the assisting language to the primary language data based on the divergence between the tag distributions of named entities in the trainMu"
P18-2064,Q17-1010,0,0.0265068,"Missing"
P18-2064,D17-1038,0,0.091646,"is skewed towards Location entity in Spanish. This leads to a drop in named entity recognition performance. In this work, we address this problem of drift in tag distribution owing to adding training data from a supporting language. The problem is similar to the problem of data selection for domain adaptation of various NLP tasks, except that additional complexity is introduced due to the multilingual nature of the learning task. For domain adaptation in various NLP tasks, several approaches have been proposed to address drift in data distribution (Moore and Lewis, 2010; Axelrod et al., 2011; Ruder and Plank, 2017). For instance, in machine translation, sentences from out-of-domain data are selected based on a suitably defined metric (Moore and Lewis, 2010; Axelrod et al., 2011). The metric attempts to capture similarity of the out-of-domain sentences with the in-domain data. Out-of-domain sentences most similar to the in-domain data are added. Like the domain adaptation techniques summarized above, we propose to judiciously add sentences from the assisting language to the primary language data based on the divergence between the tag distributions of named entities in the trainMultilingual learning for"
P18-2064,W02-2024,0,0.0295684,"gineering IIT Bombay, India. ‡ Microsoft AI & Research, Hyderabad, India. {rudra,pb}@cse.iitb.ac.in, ankunchu@microsoft.com Abstract Existing approaches add all training sentences from the assisting language to the primary language and train the neural network on the combined data. However, data from assisting languages can introduce a drift in the tag distribution for named entities, since the common named entities from the two languages may have vastly divergent tag distributions. For example, the entity China appears in training split of Spanish (primary) and English (assisting) (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) with the corresponding tag frequencies, Spanish = { Loc : 20, Org : 49, Misc : 1 } and English = { Loc : 91, Org : 7 }. By adding English data to Spanish, the tag distribution of China is skewed towards Location entity in Spanish. This leads to a drop in named entity recognition performance. In this work, we address this problem of drift in tag distribution owing to adding training data from a supporting language. The problem is similar to the problem of data selection for domain adaptation of various NLP tasks, except that additional complexity is introd"
P18-2064,N16-1155,0,0.0520536,"opose a metric based on symmetric KL divergence to filter out the highly divergent training instances in the assisting language. We empirically show that our data selection strategy improves NER performance in many languages, including those with very limited training data. 1 Introduction Neural NER trains a deep neural network for the NER task and has become quite popular as they minimize the need for hand-crafted features and, learn feature representations from the training data itself. Recently, multilingual learning has been shown to benefit Neural NER in a resource-rich language setting (Gillick et al., 2016; Yang et al., 2017). Multilingual learning aims to improve the NER performance on the language under consideration (primary language) by adding training data from one or more assisting languages. The neural network is trained on the combined data of the primary (DP ) and the assisting languages (DA ). The neural network has a combination of languagedependent and language-independent layers, and, the network learns better cross-lingual features via these language-independent layers. ∗ This work began when the second author was a research scholar at IIT Bombay 401 Proceedings of the 56th Annual"
P18-2064,D16-1196,1,0.905778,"Missing"
P18-2064,N15-3017,1,0.82717,"ll Sub-word 89.66 89.94 German Data Selection All Primary Assisting Layers Language Language Shared Monolingual None 75.98 - English All Sub-word 76.22 79.44 76.91† 79.44 91.61† 89.10† Spanish All Sub-word 74.94 76.99 76.92† 77.45† 90.85† 90.11 Dutch All Sub-word 75.59 77.38 77.29† 77.56 SKL Italian Data Selection All SKL Table 2: F-Score for German and Italian Test data using Monolingual and Multilingual learning strategies. † indicates that the SKL results are statistically significant compared to adding all assisting language data with p-value &lt; 0.05 using two-sided Welch t-test. 3 brary3 (Kunchukuttan et al., 2015) thereby, allowing sharing of sub-word features across the Indian languages. For Indian languages, the annotated data followed the IOB format. Experimental Setup In this section we list the datasets used and the network configurations used in our experiments. 3.1 Datasets 3.2 The Table 1 lists the datasets used in our experiments along with pre-trained word embeddings used and other dataset statistics. For German NER, we use ep-96-04-16.conll to create train and development splits, and use ep-96-04-15.conll as test split. As Italian has a different tag set compared to English, Spanish and Dutc"
P18-2064,N16-1030,0,0.0699121,"s based on symmetric KLDivergence of overlapping entities (b) We demonstrate the benefits of multilingual Neural NER on low-resource languages. We compare the proposed data selection approach with monolingual Neural NER system, and the multilingual Neural NER system trained using all assisting language sentences. To the best of our knowledge, ours is the first work for judiciously selecting a subset of sentences from an assisting language for multilingual Neural NER. 2 Network Architecture Several deep learning models (Collobert et al., 2011; Ma and Hovy, 2016; Murthy and Bhattacharyya, 2016; Lample et al., 2016; Yang et al., 2017) have been proposed for monolingual NER in the literature. Apart from the model by Collobert et al. (2011), remaining approaches extract sub-word features using either Convolution Neural Networks (CNNs) or Bi-LSTMs. The proposed data selection strategy for multilingual Neural NER can be used with any of the existing models. We choose the model by Murthy and Bhattacharyya (2016)1 in our experiments. Judicious Selection of Assisting Language Sentences For every assisting language sentence, we calculate the sentence score based on the average symmetric KL-Divergence score of o"
P18-2064,P16-1101,0,0.02011,"mple approach to select assisting language sentences based on symmetric KLDivergence of overlapping entities (b) We demonstrate the benefits of multilingual Neural NER on low-resource languages. We compare the proposed data selection approach with monolingual Neural NER system, and the multilingual Neural NER system trained using all assisting language sentences. To the best of our knowledge, ours is the first work for judiciously selecting a subset of sentences from an assisting language for multilingual Neural NER. 2 Network Architecture Several deep learning models (Collobert et al., 2011; Ma and Hovy, 2016; Murthy and Bhattacharyya, 2016; Lample et al., 2016; Yang et al., 2017) have been proposed for monolingual NER in the literature. Apart from the model by Collobert et al. (2011), remaining approaches extract sub-word features using either Convolution Neural Networks (CNNs) or Bi-LSTMs. The proposed data selection strategy for multilingual Neural NER can be used with any of the existing models. We choose the model by Murthy and Bhattacharyya (2016)1 in our experiments. Judicious Selection of Assisting Language Sentences For every assisting language sentence, we calculate the sentence score ba"
Q18-1022,P15-1166,0,0.151875,"dels that are better at transliterating multiple language pairs. The training may also benefit from implicit data augmentation since training data from multiple language pairs is available. From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data. This is particularly beneficial in low-resource scenarios. Our work adds to the increasing body of work investigating multilingual training for various NLP 304 tasks like POS tagging (Gillick et al., 2016), NER (Yang et al., 2016; Rudramurthy et al., 2016) and machine translation (Dong et al., 2015; Firat et al., 2016; Lee et al., 2017; Zoph et al., 2016; Johnson et al., 2017) with a view to learn models that generalize across languages and make effective use of scarce training data. The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters. This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity. We empirically show that models w"
Q18-1022,2010.iwslt-papers.7,0,0.0294356,"zeroshot transliteration scenarios, our solutions and the results of experiments. Section 7 discusses incorporation of phonetic information for multilingual transliteration. Section 8 concludes the work and discusses future directions. 2 Related Work General Transliteration Methods Previous work on transliteration has focused on the scenario of bilingual training. Until recently, the bestperforming solutions were discriminative statistical transliteration methods based on phrase-based statistical machine translation (Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2009; Finch and Sumita, 2010). Recent work has explored bilingual neural transliteration using the standard neural encoder-decoder architecture (with attention mechanism) (Bahdanau et al., 2015) or its adaptions (Finch et al., 2015; Finch et al., 2016). Using target bidirectional LSTM with model ensembling, Finch et al. (2016) have outperformed the state-ofthe-art phrase-based systems on the NEWS shared task datasets. On the other hand, we focus on multilingual transliteration with the encoder-decoder architecture or its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration"
Q18-1022,W15-3909,0,0.118921,"discusses future directions. 2 Related Work General Transliteration Methods Previous work on transliteration has focused on the scenario of bilingual training. Until recently, the bestperforming solutions were discriminative statistical transliteration methods based on phrase-based statistical machine translation (Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2009; Finch and Sumita, 2010). Recent work has explored bilingual neural transliteration using the standard neural encoder-decoder architecture (with attention mechanism) (Bahdanau et al., 2015) or its adaptions (Finch et al., 2015; Finch et al., 2016). Using target bidirectional LSTM with model ensembling, Finch et al. (2016) have outperformed the state-ofthe-art phrase-based systems on the NEWS shared task datasets. On the other hand, we focus on multilingual transliteration with the encoder-decoder architecture or its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate trans"
Q18-1022,W16-2711,0,0.0547497,"rections. 2 Related Work General Transliteration Methods Previous work on transliteration has focused on the scenario of bilingual training. Until recently, the bestperforming solutions were discriminative statistical transliteration methods based on phrase-based statistical machine translation (Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2009; Finch and Sumita, 2010). Recent work has explored bilingual neural transliteration using the standard neural encoder-decoder architecture (with attention mechanism) (Bahdanau et al., 2015) or its adaptions (Finch et al., 2015; Finch et al., 2016). Using target bidirectional LSTM with model ensembling, Finch et al. (2016) have outperformed the state-ofthe-art phrase-based systems on the NEWS shared task datasets. On the other hand, we focus on multilingual transliteration with the encoder-decoder architecture or its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify"
Q18-1022,N16-1101,0,0.0730592,"r at transliterating multiple language pairs. The training may also benefit from implicit data augmentation since training data from multiple language pairs is available. From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data. This is particularly beneficial in low-resource scenarios. Our work adds to the increasing body of work investigating multilingual training for various NLP 304 tasks like POS tagging (Gillick et al., 2016), NER (Yang et al., 2016; Rudramurthy et al., 2016) and machine translation (Dong et al., 2015; Firat et al., 2016; Lee et al., 2017; Zoph et al., 2016; Johnson et al., 2017) with a view to learn models that generalize across languages and make effective use of scarce training data. The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters. This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity. We empirically show that models with maximal paramete"
Q18-1022,N16-1155,0,0.0723241,"Missing"
Q18-1022,S17-2033,0,0.030994,"er architecture or its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify the correct transliteration) across multiple languages using grapheme to IPA mappings. Note that their model cannot generate transliterations; it can only rank candidates. Some literature mentions multilingual transliteration (Surana and Singh, 2008; He et al., 2017; Prakash, 2012; Pouliquen et al., 2005) or multilingual transliteration mining (Klementiev and Roth, 2006; Yoon et al., 2007). In these cases, however, multilingual refer to methods which work with multiple languages (as opposed to joint training - the sense of the word multilingual as we use it). Multilingual Translation Our work on multilingual transliteration is motivated by recently proposed multilingual neural machine translation archi305 tectures (Firat et al., 2016). Broadly, these proposals can be categorized into two groups. One group consists of architectures that specialize parts o"
Q18-1022,D12-1002,0,0.0350666,"Missing"
Q18-1022,P08-1103,0,0.0451685,"tup, results and analysis. Section 6 discusses various zeroshot transliteration scenarios, our solutions and the results of experiments. Section 7 discusses incorporation of phonetic information for multilingual transliteration. Section 8 concludes the work and discusses future directions. 2 Related Work General Transliteration Methods Previous work on transliteration has focused on the scenario of bilingual training. Until recently, the bestperforming solutions were discriminative statistical transliteration methods based on phrase-based statistical machine translation (Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2009; Finch and Sumita, 2010). Recent work has explored bilingual neural transliteration using the standard neural encoder-decoder architecture (with attention mechanism) (Bahdanau et al., 2015) or its adaptions (Finch et al., 2015; Finch et al., 2016). Using target bidirectional LSTM with model ensembling, Finch et al. (2016) have outperformed the state-ofthe-art phrase-based systems on the NEWS shared task datasets. On the other hand, we focus on multilingual transliteration with the encoder-decoder architecture or its adaptations. The two strands of work are obviously"
Q18-1022,W09-3504,0,0.234378,"Section 6 discusses various zeroshot transliteration scenarios, our solutions and the results of experiments. Section 7 discusses incorporation of phonetic information for multilingual transliteration. Section 8 concludes the work and discusses future directions. 2 Related Work General Transliteration Methods Previous work on transliteration has focused on the scenario of bilingual training. Until recently, the bestperforming solutions were discriminative statistical transliteration methods based on phrase-based statistical machine translation (Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2009; Finch and Sumita, 2010). Recent work has explored bilingual neural transliteration using the standard neural encoder-decoder architecture (with attention mechanism) (Bahdanau et al., 2015) or its adaptions (Finch et al., 2015; Finch et al., 2016). Using target bidirectional LSTM with model ensembling, Finch et al. (2016) have outperformed the state-ofthe-art phrase-based systems on the NEWS shared task datasets. On the other hand, we focus on multilingual transliteration with the encoder-decoder architecture or its adaptations. The two strands of work are obviously complimentary. Multilingua"
Q18-1022,Q17-1024,0,0.446771,"g may also benefit from implicit data augmentation since training data from multiple language pairs is available. From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data. This is particularly beneficial in low-resource scenarios. Our work adds to the increasing body of work investigating multilingual training for various NLP 304 tasks like POS tagging (Gillick et al., 2016), NER (Yang et al., 2016; Rudramurthy et al., 2016) and machine translation (Dong et al., 2015; Firat et al., 2016; Lee et al., 2017; Zoph et al., 2016; Johnson et al., 2017) with a view to learn models that generalize across languages and make effective use of scarce training data. The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters. This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity. We empirically show that models with maximal parameter sharing are beneficial, without increasing the model size."
Q18-1022,N10-1065,1,0.79409,"se compatible scripts resulting in a shared vocabulary. We specialize just the output layer for target languages, but share the encoder, decoder and character embeddings across languages. In this respect, we differ from Johnson et al. (2017). They share all network components across languages, but add an artificial token at the beginning of the input sequence to indicate the target language. Zeroshot Transliteration We use the multilingual models to address zeroshot transliteration. Zeroshot transliteration using bridge/pivot language has been explored for statistical machine transliteration (Khapra et al., 2010) as well as neural machine transliteration (Saha et al., 2016). Unlike previous approaches which pivot over bilingual transliteration models, we propose zeroshot transliteration that pivots over multilingual transliteration models. We also propose a direct zeroshot transliteration method, a scenario which has been explored for machine translation by Johnson et al. (2017), but not investigated previously for transliteration. In our zeroshot model, sequences from multiple source languages are mapped to a common encoder representation without the need for a parallel corpus between the source lang"
Q18-1022,P06-1103,0,0.0550269,"nguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify the correct transliteration) across multiple languages using grapheme to IPA mappings. Note that their model cannot generate transliterations; it can only rank candidates. Some literature mentions multilingual transliteration (Surana and Singh, 2008; He et al., 2017; Prakash, 2012; Pouliquen et al., 2005) or multilingual transliteration mining (Klementiev and Roth, 2006; Yoon et al., 2007). In these cases, however, multilingual refer to methods which work with multiple languages (as opposed to joint training - the sense of the word multilingual as we use it). Multilingual Translation Our work on multilingual transliteration is motivated by recently proposed multilingual neural machine translation archi305 tectures (Firat et al., 2016). Broadly, these proposals can be categorized into two groups. One group consists of architectures that specialize parts of the network for particular languages: specialized encoders (Zoph et al., 2016), decoders (Dong et al., 2"
Q18-1022,W15-3912,1,0.896917,"Missing"
Q18-1022,N15-3017,1,0.810922,"he parallel corpora are roughly of the same size. Better training schedules could be explored in future. Languages: We experimented with two sets of orthographically similar languages: Indian languages: (i) Hindi (hi), Bengali (bn) from the Indo-Aryan branch of Indo-European family (ii) Kannada (kn), Tamil (ta) from the Dravidian family. We studied Indic-Indic transliteration and transliteration involving a non-Indian language (English↔Indic). We mapped equivalent characters in different Indic scripts in order to build a common vocabulary based on the common offsets of the Unicode codepoints (Kunchukuttan et al., 2015). Slavic languages: Czech (cs), Polish (pl), Slovenian (sl) and Slovak (sk). We studied Arabic↔Slavic transliteration. Arabic is a non-Slavic language (Semitic branch of Afro-Asiatic) and uses an abjad script in which vowel diacritics are omitted in general usage. The languages chosen are representative of languages spoken by some major groups of peoples en-Indic en-hi 12K en-bn 13K en-kn 10K en-ta 10K Indic-en Indic-Indic hi-en 18K bn kn ta bn-en 12K hi 3620 5085 5290 kn-en 15K bn 2720 2901 ta-en 15K kn 4216 ar-Slavic ar-cs 15K ar-pl 15K ar-sl 10K ar-sk 10K Pair Table 1: Training set statisti"
Q18-1022,K16-1027,1,0.859366,"ding the phonetic properties of the character, one bit for each value of every property. The multiplication of the phonetic feature vector with the weight matrix in the first layer generates phonetic embeddings for each character. These are inputs to the encoder. Apart from this input change, the rest of the network architecture is the same as described in Section 3.2. Experiments: We experimented with Indian languages (Indic→English and Indic-Indic transliteration). Indic scripts generally have a one-one correspondence from characters to phonemes. Hence, we use phonetic features described by Kunchukuttan et al. (2016) to generate phonetic feature vectors for characters (available via the Indic NLP Library1 ). These Indic languages are spoken by nearly a billion people and hence the use of phonetic features is useful for many of the world’s most widely spoken languages. Results and Discussion: Table 8 shows the results. We observe that phonetic feature input improves transliteration accuracy for Indic-English transliteration. The improvements are primarily due to reduction in errors related to similar consonants like (T,D), (P,B), (C,K) and the use of H for aspiration. 1 https://github.com/anoopkunchukuttan"
Q18-1022,Q17-1026,0,0.0551909,"multiple language pairs. The training may also benefit from implicit data augmentation since training data from multiple language pairs is available. From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data. This is particularly beneficial in low-resource scenarios. Our work adds to the increasing body of work investigating multilingual training for various NLP 304 tasks like POS tagging (Gillick et al., 2016), NER (Yang et al., 2016; Rudramurthy et al., 2016) and machine translation (Dong et al., 2015; Firat et al., 2016; Lee et al., 2017; Zoph et al., 2016; Johnson et al., 2017) with a view to learn models that generalize across languages and make effective use of scarce training data. The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters. This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity. We empirically show that models with maximal parameter sharing are bene"
Q18-1022,W12-4810,0,0.0317588,"r its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify the correct transliteration) across multiple languages using grapheme to IPA mappings. Note that their model cannot generate transliterations; it can only rank candidates. Some literature mentions multilingual transliteration (Surana and Singh, 2008; He et al., 2017; Prakash, 2012; Pouliquen et al., 2005) or multilingual transliteration mining (Klementiev and Roth, 2006; Yoon et al., 2007). In these cases, however, multilingual refer to methods which work with multiple languages (as opposed to joint training - the sense of the word multilingual as we use it). Multilingual Translation Our work on multilingual transliteration is motivated by recently proposed multilingual neural machine translation archi305 tectures (Firat et al., 2016). Broadly, these proposals can be categorized into two groups. One group consists of architectures that specialize parts of the network f"
Q18-1022,E17-3017,0,0.0348879,"ayer (stride size = 1 and SAME padding), followed by ReLU units and max pooling. We use filters of different sizes and concatenate their output to produce the encoder output. Figure 1b shows a schematic of the encoder. We chose CNN over the conventional bidirectional LSTM layer since the temporal dependencies for transliteration are mostly local, which can be handled by the CNN encoder. We observed that training and decoding are significantly faster, with little impact on accuracy. The decoder contains a layer of LSTM cells and their start state is the average of the encoder’s output vectors (Sennrich et al., 2017). set for English-Hindi, this model was used for reporting test set results for English-Hindi. We observed that this criterion performed better than choosing the model with least validation set loss over all language pairs. Parameter Sharing: The vocabulary of the orthographically similar languages (at input and/or output) is comprised of the union of character sets of all these languages. Since the character set of these languages overlaps to a large extent, we share their character embeddings too. The encoder is shared across all source languages and the decoder is shared across all target l"
Q18-1022,I08-1009,0,0.0326398,"n with the encoder-decoder architecture or its adaptations. The two strands of work are obviously complimentary. Multilinguality and Transliteration To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify the correct transliteration) across multiple languages using grapheme to IPA mappings. Note that their model cannot generate transliterations; it can only rank candidates. Some literature mentions multilingual transliteration (Surana and Singh, 2008; He et al., 2017; Prakash, 2012; Pouliquen et al., 2005) or multilingual transliteration mining (Klementiev and Roth, 2006; Yoon et al., 2007). In these cases, however, multilingual refer to methods which work with multiple languages (as opposed to joint training - the sense of the word multilingual as we use it). Multilingual Translation Our work on multilingual transliteration is motivated by recently proposed multilingual neural machine translation archi305 tectures (Firat et al., 2016). Broadly, these proposals can be categorized into two groups. One group consists of architectures that s"
Q18-1022,P07-1015,0,0.0213722,"n To the best of our knowledge, ours is the first work on multilingual transliteration. Jagarlamudi and Daumé III (2012) have proposed a method for transliteration mining (given a name and candidate transliterations, identify the correct transliteration) across multiple languages using grapheme to IPA mappings. Note that their model cannot generate transliterations; it can only rank candidates. Some literature mentions multilingual transliteration (Surana and Singh, 2008; He et al., 2017; Prakash, 2012; Pouliquen et al., 2005) or multilingual transliteration mining (Klementiev and Roth, 2006; Yoon et al., 2007). In these cases, however, multilingual refer to methods which work with multiple languages (as opposed to joint training - the sense of the word multilingual as we use it). Multilingual Translation Our work on multilingual transliteration is motivated by recently proposed multilingual neural machine translation archi305 tectures (Firat et al., 2016). Broadly, these proposals can be categorized into two groups. One group consists of architectures that specialize parts of the network for particular languages: specialized encoders (Zoph et al., 2016), decoders (Dong et al., 2015) or both (Firat"
Q18-1022,D16-1163,0,0.109551,"pairs. The training may also benefit from implicit data augmentation since training data from multiple language pairs is available. From the perspective of a single language pair, data from other language pairs can be seen as additional (noisy) training data. This is particularly beneficial in low-resource scenarios. Our work adds to the increasing body of work investigating multilingual training for various NLP 304 tasks like POS tagging (Gillick et al., 2016), NER (Yang et al., 2016; Rudramurthy et al., 2016) and machine translation (Dong et al., 2015; Firat et al., 2016; Lee et al., 2017; Zoph et al., 2016; Johnson et al., 2017) with a view to learn models that generalize across languages and make effective use of scarce training data. The following are the contributions of our work: (1) We propose a compact neural encoder-decoder model for multilingual transliteration, that is designed to ensure maximum sharing of parameters across languages while providing room for learning language-specific parameters. This allows greater sharing of knowledge across language pairs by leveraging orthographic similarity. We empirically show that models with maximal parameter sharing are beneficial, without inc"
Q19-1007,D16-1250,0,0.0678747,"Missing"
Q19-1007,P17-1042,0,0.728815,"n a second correction transformation by assuming the average of the projected source and target embeddings as the true latent representation, we make no such assumption and learn the similarity metric from the data. Kementchedjhieva et al. (2018), recently, employed the generalized Procrustes analysis (GPA) method (Gower, 1975) for the bilingual mapping problem. GPA maps both the source and target language embeddings to a latent space, which is constructed by averaging over the two language spaces. Unsupervised methods have shown promising results, matching supervised methods in many studies. Artetxe et al. (2017) proposed a bootstrapping method for bilingual lexicon induction problem by using a small-seed bilingual dictionary. Subsequently, Artetxe et al. (2018b) and Hoshen and Wolf (2018) have proposed initialization methods that eliminate the need for a seed dictionary. Zhang et al. (2017b) and Grave et al. (2018) proposed aligning the source and target language word embeddings by optimizing the Wasserstein distance. Unsupervised methods based on adversarial training objectives have also been proposed (Barone, 2016; Zhang et al., 2017a; Conneau et al., 2018; Chen and Cardie, 2018). A recent work by"
Q19-1007,P18-1073,0,0.270059,"nsformation from embeddings of one language to another can be learned from a bilingual dictionary and corresponding monolingual embeddings by performing linear least-squares regression. A popular modification to this formulation constrains the transformation matrix to be orthogonal (Xing et al., 2015; Smith et al., 2017b; 2018a). This is known as the orthogonal Procrustes problem (Sch¨onemann, 1966). Orthogonality preserves monolingual distances and ensures the transformation is reversible. Lazaridou et al. (2015) and Joulin et al. (2018) optimize alternative loss functions in this framework. Artetxe et al. (2018a) improves on the Procrustes solution and propose a multi-step framework consisting of a series of linear transformations to the data. Faruqui and Dyer (2014) • Our approach outperforms state-of-the-art supervised and unsupervised bilingual mapping methods on the bilingual lexicon induction as well as the cross-lingual word similarity tasks. • An ablation analysis reveals that the following contribute to our model’s improved performance: (a) aligning the embedding spaces of different languages, (b) learning a similarity metric which induces a latent space, (c) performing inference in the in10"
Q19-1007,W16-1614,0,0.0598723,"s have shown promising results, matching supervised methods in many studies. Artetxe et al. (2017) proposed a bootstrapping method for bilingual lexicon induction problem by using a small-seed bilingual dictionary. Subsequently, Artetxe et al. (2018b) and Hoshen and Wolf (2018) have proposed initialization methods that eliminate the need for a seed dictionary. Zhang et al. (2017b) and Grave et al. (2018) proposed aligning the source and target language word embeddings by optimizing the Wasserstein distance. Unsupervised methods based on adversarial training objectives have also been proposed (Barone, 2016; Zhang et al., 2017a; Conneau et al., 2018; Chen and Cardie, 2018). A recent work by Søgaard et al. (2018) discusses cases in which unsupervised bilingual lexicon induction does not lead to good performance. 3 Learning Latent Space Representation In this section, we first describe the proposed geometric framework to learn bilingual embeddings. We then present its generalization to the multilingual setting. 3.1 Geometry-aware Factorization We propose to transform the word embeddings from the source and target languages to a common space in which the similarity of word embeddings may be better"
Q19-1007,D18-1027,0,0.277981,"have been shown to perform better than CCA-based approaches (Artetxe et al., 2016, 2018a). We view the problem of mapping the source and target languages word embeddings as (a) aligning the two language spaces and (b) learning a similarity metric in this (learned) common space. We accomplish this by learning suitable language-specific orthogonal transformations (for alignment) and a symmetric positive-definite matrix (as Mahalanobis metric). The similarity metric is useful in addressing the limitations of mapping to a common latent space under orthogonality constraints, an issue discussed by Doval et al. (2018). Whereas Doval et al. (2018) learn a second correction transformation by assuming the average of the projected source and target embeddings as the true latent representation, we make no such assumption and learn the similarity metric from the data. Kementchedjhieva et al. (2018), recently, employed the generalized Procrustes analysis (GPA) method (Gower, 1975) for the bilingual mapping problem. GPA maps both the source and target language embeddings to a latent space, which is constructed by averaging over the two language spaces. Unsupervised methods have shown promising results, matching su"
Q19-1007,E17-1084,0,0.0301526,"used to jointly transform multiple languages to a common latent space. However, this requires an n-way dictionary to represent n languages. In contrast, the proposed approach requires only pairwise bilingual dictionaries such that every language under consideration is represented in at least one bilingual dictionary. The above-mentioned approaches are referred to as offline since the monolingual and bilingual embeddings are learned separately. In contrast, online approaches directly learn a bilingual/ multilingual embedding from parallel corpora (Hermann and Blunsom, 2014; Huang et al., 2015; Duong et al., 2017), optionally augmented with monolingual corpora (Klementiev et al., 2012; Chandar et al., 2014; Gouws et al., 2015). In this work, we focus on offline approaches. use Canonical Correlation Analysis (CCA) to learn linear projections from the source and target languages to a common space such that correlations between the embeddings projected to this space are maximized. Procrustes solution– based approaches have been shown to perform better than CCA-based approaches (Artetxe et al., 2016, 2018a). We view the problem of mapping the source and target languages word embeddings as (a) aligning the"
Q19-1007,S17-2002,0,0.102423,"Missing"
Q19-1007,E14-1049,0,0.383405,"ear least-squares regression. A popular modification to this formulation constrains the transformation matrix to be orthogonal (Xing et al., 2015; Smith et al., 2017b; 2018a). This is known as the orthogonal Procrustes problem (Sch¨onemann, 1966). Orthogonality preserves monolingual distances and ensures the transformation is reversible. Lazaridou et al. (2015) and Joulin et al. (2018) optimize alternative loss functions in this framework. Artetxe et al. (2018a) improves on the Procrustes solution and propose a multi-step framework consisting of a series of linear transformations to the data. Faruqui and Dyer (2014) • Our approach outperforms state-of-the-art supervised and unsupervised bilingual mapping methods on the bilingual lexicon induction as well as the cross-lingual word similarity tasks. • An ablation analysis reveals that the following contribute to our model’s improved performance: (a) aligning the embedding spaces of different languages, (b) learning a similarity metric which induces a latent space, (c) performing inference in the in108 proaches for representing embeddings of multiple languages in a common vector space by designating one of the languages as a pivot language. In this simple a"
Q19-1007,D18-1024,0,0.397549,"ds in many studies. Artetxe et al. (2017) proposed a bootstrapping method for bilingual lexicon induction problem by using a small-seed bilingual dictionary. Subsequently, Artetxe et al. (2018b) and Hoshen and Wolf (2018) have proposed initialization methods that eliminate the need for a seed dictionary. Zhang et al. (2017b) and Grave et al. (2018) proposed aligning the source and target language word embeddings by optimizing the Wasserstein distance. Unsupervised methods based on adversarial training objectives have also been proposed (Barone, 2016; Zhang et al., 2017a; Conneau et al., 2018; Chen and Cardie, 2018). A recent work by Søgaard et al. (2018) discusses cases in which unsupervised bilingual lexicon induction does not lead to good performance. 3 Learning Latent Space Representation In this section, we first describe the proposed geometric framework to learn bilingual embeddings. We then present its generalization to the multilingual setting. 3.1 Geometry-aware Factorization We propose to transform the word embeddings from the source and target languages to a common space in which the similarity of word embeddings may be better learned. To this end, we align the source and target languages embe"
Q19-1007,N18-1032,0,0.0487977,"Missing"
Q19-1007,P15-1027,0,0.0311665,"mary of our findings: 2 Related Work Bilingual Embeddings. Mikolov et al. (2013b) show that a linear transformation from embeddings of one language to another can be learned from a bilingual dictionary and corresponding monolingual embeddings by performing linear least-squares regression. A popular modification to this formulation constrains the transformation matrix to be orthogonal (Xing et al., 2015; Smith et al., 2017b; 2018a). This is known as the orthogonal Procrustes problem (Sch¨onemann, 1966). Orthogonality preserves monolingual distances and ensures the transformation is reversible. Lazaridou et al. (2015) and Joulin et al. (2018) optimize alternative loss functions in this framework. Artetxe et al. (2018a) improves on the Procrustes solution and propose a multi-step framework consisting of a series of linear transformations to the data. Faruqui and Dyer (2014) • Our approach outperforms state-of-the-art supervised and unsupervised bilingual mapping methods on the bilingual lexicon induction as well as the cross-lingual word similarity tasks. • An ablation analysis reveals that the following contribute to our model’s improved performance: (a) aligning the embedding spaces of different languages"
Q19-1007,P14-1006,0,0.0251258,"od (Kementchedjhieva et al., 2018) may also be used to jointly transform multiple languages to a common latent space. However, this requires an n-way dictionary to represent n languages. In contrast, the proposed approach requires only pairwise bilingual dictionaries such that every language under consideration is represented in at least one bilingual dictionary. The above-mentioned approaches are referred to as offline since the monolingual and bilingual embeddings are learned separately. In contrast, online approaches directly learn a bilingual/ multilingual embedding from parallel corpora (Hermann and Blunsom, 2014; Huang et al., 2015; Duong et al., 2017), optionally augmented with monolingual corpora (Klementiev et al., 2012; Chandar et al., 2014; Gouws et al., 2015). In this work, we focus on offline approaches. use Canonical Correlation Analysis (CCA) to learn linear projections from the source and target languages to a common space such that correlations between the embeddings projected to this space are maximized. Procrustes solution– based approaches have been shown to perform better than CCA-based approaches (Artetxe et al., 2016, 2018a). We view the problem of mapping the source and target langu"
Q19-1007,D18-1043,0,0.245681,"the similarity metric from the data. Kementchedjhieva et al. (2018), recently, employed the generalized Procrustes analysis (GPA) method (Gower, 1975) for the bilingual mapping problem. GPA maps both the source and target language embeddings to a latent space, which is constructed by averaging over the two language spaces. Unsupervised methods have shown promising results, matching supervised methods in many studies. Artetxe et al. (2017) proposed a bootstrapping method for bilingual lexicon induction problem by using a small-seed bilingual dictionary. Subsequently, Artetxe et al. (2018b) and Hoshen and Wolf (2018) have proposed initialization methods that eliminate the need for a seed dictionary. Zhang et al. (2017b) and Grave et al. (2018) proposed aligning the source and target language word embeddings by optimizing the Wasserstein distance. Unsupervised methods based on adversarial training objectives have also been proposed (Barone, 2016; Zhang et al., 2017a; Conneau et al., 2018; Chen and Cardie, 2018). A recent work by Søgaard et al. (2018) discusses cases in which unsupervised bilingual lexicon induction does not lead to good performance. 3 Learning Latent Space Representation In this section, w"
Q19-1007,D15-1127,0,0.0289109,"Missing"
Q19-1007,D18-1330,0,0.447319,"Missing"
Q19-1007,K18-1021,0,0.448488,"n induction as well as the cross-lingual word similarity tasks. • An ablation analysis reveals that the following contribute to our model’s improved performance: (a) aligning the embedding spaces of different languages, (b) learning a similarity metric which induces a latent space, (c) performing inference in the in108 proaches for representing embeddings of multiple languages in a common vector space by designating one of the languages as a pivot language. In this simple approach, bilingual mappings are learned independently from all other languages to the pivot language. A GPA-based method (Kementchedjhieva et al., 2018) may also be used to jointly transform multiple languages to a common latent space. However, this requires an n-way dictionary to represent n languages. In contrast, the proposed approach requires only pairwise bilingual dictionaries such that every language under consideration is represented in at least one bilingual dictionary. The above-mentioned approaches are referred to as offline since the monolingual and bilingual embeddings are learned separately. In contrast, online approaches directly learn a bilingual/ multilingual embedding from parallel corpora (Hermann and Blunsom, 2014; Huang e"
Q19-1007,C12-1089,0,0.516391,"Geometric Approach Pratik Jawanpuria1 , Arjun Balgovind2∗ , Anoop Kunchukuttan1 , Bamdev Mishra1 1 1 Microsoft, India 2 IIT Madras, India {pratik.jawanpuria,ankunchu,bamdevm}@microsoft.com 2 barjun@cse.iitm.ac.in Abstract different languages are mapped close to each other in a common embedding space. Hence, they are useful for joint/transfer learning and sharing annotated data across languages in different NLP applications such as machine translation (Gu et al., 2018), building bilingual dictionaries (Mikolov et al., 2013b), mining parallel corpora (Conneau et al., 2018), text classification (Klementiev et al., 2012), sentiment analysis (Zhou et al., 2015), and dependency parsing (Ammar et al., 2016). Mikolov et al. (2013b) empirically show that a linear transformation of embeddings from one language to another preserves the geometric arrangement of word embeddings. In a supervised setting, the transformation matrix, W, is learned given a small bilingual dictionary and their corresponding monolingual embeddings. Subsequently, many refinements to the bilingual mapping framework have been proposed (Xing et al., 2015; Smith et al., 2017b; Conneau et al., 2018; Artetxe et al., 2016, 2017, 2018a,b). In this wo"
Q19-1007,P18-1072,0,0.272229,"Missing"
Q19-1007,P17-1179,0,0.080538,"crustes analysis (GPA) method (Gower, 1975) for the bilingual mapping problem. GPA maps both the source and target language embeddings to a latent space, which is constructed by averaging over the two language spaces. Unsupervised methods have shown promising results, matching supervised methods in many studies. Artetxe et al. (2017) proposed a bootstrapping method for bilingual lexicon induction problem by using a small-seed bilingual dictionary. Subsequently, Artetxe et al. (2018b) and Hoshen and Wolf (2018) have proposed initialization methods that eliminate the need for a seed dictionary. Zhang et al. (2017b) and Grave et al. (2018) proposed aligning the source and target language word embeddings by optimizing the Wasserstein distance. Unsupervised methods based on adversarial training objectives have also been proposed (Barone, 2016; Zhang et al., 2017a; Conneau et al., 2018; Chen and Cardie, 2018). A recent work by Søgaard et al. (2018) discusses cases in which unsupervised bilingual lexicon induction does not lead to good performance. 3 Learning Latent Space Representation In this section, we first describe the proposed geometric framework to learn bilingual embeddings. We then present its ge"
Q19-1007,S17-2008,0,0.0674473,"Missing"
Q19-1007,D17-1207,0,0.10899,"crustes analysis (GPA) method (Gower, 1975) for the bilingual mapping problem. GPA maps both the source and target language embeddings to a latent space, which is constructed by averaging over the two language spaces. Unsupervised methods have shown promising results, matching supervised methods in many studies. Artetxe et al. (2017) proposed a bootstrapping method for bilingual lexicon induction problem by using a small-seed bilingual dictionary. Subsequently, Artetxe et al. (2018b) and Hoshen and Wolf (2018) have proposed initialization methods that eliminate the need for a seed dictionary. Zhang et al. (2017b) and Grave et al. (2018) proposed aligning the source and target language word embeddings by optimizing the Wasserstein distance. Unsupervised methods based on adversarial training objectives have also been proposed (Barone, 2016; Zhang et al., 2017a; Conneau et al., 2018; Chen and Cardie, 2018). A recent work by Søgaard et al. (2018) discusses cases in which unsupervised bilingual lexicon induction does not lead to good performance. 3 Learning Latent Space Representation In this section, we first describe the proposed geometric framework to learn bilingual embeddings. We then present its ge"
Q19-1007,P15-1042,0,0.026939,"Balgovind2∗ , Anoop Kunchukuttan1 , Bamdev Mishra1 1 1 Microsoft, India 2 IIT Madras, India {pratik.jawanpuria,ankunchu,bamdevm}@microsoft.com 2 barjun@cse.iitm.ac.in Abstract different languages are mapped close to each other in a common embedding space. Hence, they are useful for joint/transfer learning and sharing annotated data across languages in different NLP applications such as machine translation (Gu et al., 2018), building bilingual dictionaries (Mikolov et al., 2013b), mining parallel corpora (Conneau et al., 2018), text classification (Klementiev et al., 2012), sentiment analysis (Zhou et al., 2015), and dependency parsing (Ammar et al., 2016). Mikolov et al. (2013b) empirically show that a linear transformation of embeddings from one language to another preserves the geometric arrangement of word embeddings. In a supervised setting, the transformation matrix, W, is learned given a small bilingual dictionary and their corresponding monolingual embeddings. Subsequently, many refinements to the bilingual mapping framework have been proposed (Xing et al., 2015; Smith et al., 2017b; Conneau et al., 2018; Artetxe et al., 2016, 2017, 2018a,b). In this work, we propose a novel geometric approac"
Q19-1007,N15-1104,0,0.78327,"et al., 2013b), mining parallel corpora (Conneau et al., 2018), text classification (Klementiev et al., 2012), sentiment analysis (Zhou et al., 2015), and dependency parsing (Ammar et al., 2016). Mikolov et al. (2013b) empirically show that a linear transformation of embeddings from one language to another preserves the geometric arrangement of word embeddings. In a supervised setting, the transformation matrix, W, is learned given a small bilingual dictionary and their corresponding monolingual embeddings. Subsequently, many refinements to the bilingual mapping framework have been proposed (Xing et al., 2015; Smith et al., 2017b; Conneau et al., 2018; Artetxe et al., 2016, 2017, 2018a,b). In this work, we propose a novel geometric approach for learning bilingual embeddings. We rotate the source and target language embeddings from their original vector spaces to a common latent space via language-specific orthogonal transformations. Furthermore, we define a similarity metric, the Mahalanobis metric, in this common space to refine the notion of similarity between a pair of embeddings. We achieve the above by learning the transformation matrix as follows: W = Ut BU&gt; s , where Ut and Us are the ortho"
W12-5906,I08-1067,1,0.854885,"ke the decoder’s large search space also limit the possible reorderings that can be searched during decoding. Lexical binary reordering model (Koehn, 2008) proposes a limited reordering model conditioned on the phrases. An alternative approach which has been proposed is to reorder the source language sentence to conform to the target language word order before decoding. The search space for the decoder is thus simplified, and thus translation can be performed effectively with a simple distortion model. Many solutions for manipulating source side parse trees, with manual (Collins et al., 2005; Ananthakrishnan et al., 2008) or automatic rules (Xia and McCord, 2004), have shown improvement in the performance of PBSMT. However, these solutions are language pair specific, cannot be easily scaled to new language pairs and may require linguistic resources like parsers on the source/target sides. Therefore, recently, approaches have been explored to learn word reorderings on the source side in a language independent way. Visweswariah et al. (2011) model the word reordering as a Travelling salesperson problem whereas Tromble and Eisner (2009) model it as a linear ordering problem. Given the large number of re-orderings"
W12-5906,P05-1066,0,0.112516,"Missing"
W12-5906,D09-1105,0,0.0196911,"manipulating source side parse trees, with manual (Collins et al., 2005; Ananthakrishnan et al., 2008) or automatic rules (Xia and McCord, 2004), have shown improvement in the performance of PBSMT. However, these solutions are language pair specific, cannot be easily scaled to new language pairs and may require linguistic resources like parsers on the source/target sides. Therefore, recently, approaches have been explored to learn word reorderings on the source side in a language independent way. Visweswariah et al. (2011) model the word reordering as a Travelling salesperson problem whereas Tromble and Eisner (2009) model it as a linear ordering problem. Given the large number of re-orderings this problem is NP-hard. We explore the possibility of representing the problem as a reordering of word sequences, instead of words. To this end, we propose a sequence labelling framework to identify work sequences. We also model the reversal of word sequences as a sequence labelling problem. These transformations reduce the problem to a phrase reordering problem, which has a smaller search space. In Section 2, we discuss our word sequence based reordering model, and how it has been partially cast as a sequence labe"
W12-5906,D11-1045,0,0.28083,"thus translation can be performed effectively with a simple distortion model. Many solutions for manipulating source side parse trees, with manual (Collins et al., 2005; Ananthakrishnan et al., 2008) or automatic rules (Xia and McCord, 2004), have shown improvement in the performance of PBSMT. However, these solutions are language pair specific, cannot be easily scaled to new language pairs and may require linguistic resources like parsers on the source/target sides. Therefore, recently, approaches have been explored to learn word reorderings on the source side in a language independent way. Visweswariah et al. (2011) model the word reordering as a Travelling salesperson problem whereas Tromble and Eisner (2009) model it as a linear ordering problem. Given the large number of re-orderings this problem is NP-hard. We explore the possibility of representing the problem as a reordering of word sequences, instead of words. To this end, we propose a sequence labelling framework to identify work sequences. We also model the reversal of word sequences as a sequence labelling problem. These transformations reduce the problem to a phrase reordering problem, which has a smaller search space. In Section 2, we discuss"
W12-5906,C04-1073,0,0.0358961,"possible reorderings that can be searched during decoding. Lexical binary reordering model (Koehn, 2008) proposes a limited reordering model conditioned on the phrases. An alternative approach which has been proposed is to reorder the source language sentence to conform to the target language word order before decoding. The search space for the decoder is thus simplified, and thus translation can be performed effectively with a simple distortion model. Many solutions for manipulating source side parse trees, with manual (Collins et al., 2005; Ananthakrishnan et al., 2008) or automatic rules (Xia and McCord, 2004), have shown improvement in the performance of PBSMT. However, these solutions are language pair specific, cannot be easily scaled to new language pairs and may require linguistic resources like parsers on the source/target sides. Therefore, recently, approaches have been explored to learn word reorderings on the source side in a language independent way. Visweswariah et al. (2011) model the word reordering as a Travelling salesperson problem whereas Tromble and Eisner (2009) model it as a linear ordering problem. Given the large number of re-orderings this problem is NP-hard. We explore the p"
W13-3611,W13-1703,0,0.206511,"Missing"
W13-3611,D10-1094,0,\N,Missing
W13-3611,P11-1092,0,\N,Missing
W13-3611,N12-1067,0,\N,Missing
W13-3611,W13-3601,0,\N,Missing
W14-1708,C08-1022,0,0.0268234,"Missing"
W14-1708,P06-1032,0,0.074105,"or category. So, the custom development approach is infeasible for correcting a large number of error categories. Hence, for correction of all the error categories, generic methods have been investigated - generally using language models or statistical machine translation (SMT) systems. The language model based method (Lee and Seneff, 2006; Kao et al., 2013) scores sentences based on a language model or count ratios of n-grams obtained from a large native text corpus. But this method still needs a candidate generation mechanism for each error category. On the other hand, the SMT based method (Brockett et al., 2006) formulates the grammar correction problem as a problem of translation of incorrect sentences to correct sentences. SMT provides a natural unsupervised method for identifying candidate corrections in the form of the translation model, and a method for scoring them with a variety of measures including the language model score. However, the SMT method requires a lot of parallel non-native learner corpora. In addition, the machinery in phrase based SMT is optimized towards solving the language translation problem. Therefore, the community has explored approaches to adapt the In this paper, we pro"
W14-1708,W13-3606,0,0.0218238,"Missing"
W14-1708,N12-1067,0,0.0655488,"Missing"
W14-1708,W13-1703,0,0.053685,"l. (2013). SVA correction is done using a prioritized, conditional rule based system described by Kunchukuttan et al. (2013). 4 Table 2 shows the results on the development set for different experimental configurations generated by varying the tuning metrics, and the method of combining the SMT model and custom correction modules. Table 3 shows the same results on the official CoNLL 2014 dataset without alternative answers. 5.1 We used the NUCLE Corpus v3.1 to build a phrase based SMT system for grammar correction. The NUCLE Corpus contains 28 error categories, whose details are documented in Dahlmeier et al. (2013). We split the corpus into training, tuning and test sets are shown in Table 1. 5.2 Document Count 1330 20 47 Sentence Count 54284 854 2013 The phrase based system was trained using the Moses1 system, with the grow-diag-finaland heuristic for extracting phrases and the msdbidirectional-fe model for lexicalized reordering. We tuned the trained models using Minimum Error Rate Training (MERT) with default parameters (100 best list, max 25 iterations). Instead of BLEU, the tuning metric was the F-0.5 metric. We trained 5-gram language models on all the sentences from NUCLE corpus using the Kneser-"
W14-1708,P03-1021,0,0.0579705,"general, GEC may partly assist in solving natural language processing (NLP) tasks like Machine Translation, Natural Language Generation etc. However, a more evident application of GEC is in building automated grammar checkers thereby non-native speakers of a language. The goal is to have automated tools to help non-native 60 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 60–64, c Baltimore, Maryland, 26-27 July 2014. 2014 Association for Computational Linguistics al., 2002), tuned using the Minimum Error Rate Training (MERT) algorithm (Och, 2003). Since BLEU is a form of weighted precision, along with a brevity penalty to factor in recall, it is suitable in the language translation scenario, where fidelity of the translation is an important in evaluation of the translation. Tuning to BLEU ensures that the parameter weights are set such that the fidelity of translations is high. However, ensuring fidelity is not the major challenge in grammar correction since the meaning of most input sentences is clear and most don’t have any grammatical errors. The metric to be tuned must ensure that weights are learnt such that the features most rel"
W14-1708,P02-1040,0,0.0894239,"Missing"
W14-1708,D13-1074,0,0.012303,"used metrics for tuning is the BLEU score (Papineni et • Correct all error categories using the SMT method, followed by correction using the custom modules. • Correct only the error categories not handled by the custom modules using the SMT method, followed by correction using the custom modules. 61 5 The error categories for which we built custom modules are noun number, determiner and subject-verb agreement (SVA) errors. These errors are amongst the most common errors made by non-native speakers. The noun number and determiner errors are corrected using the classification model proposed by Rozovskaya and Roth (2013), where the label space is a cross-product of the label spaces of the possible noun number and determiners. We use the feature-set proposed by Kunchukuttan et al. (2013). SVA correction is done using a prioritized, conditional rule based system described by Kunchukuttan et al. (2013). 4 Table 2 shows the results on the development set for different experimental configurations generated by varying the tuning metrics, and the method of combining the SMT model and custom correction modules. Table 3 shows the same results on the official CoNLL 2014 dataset without alternative answers. 5.1 We used"
W14-1708,W13-3607,0,0.0444389,"relevant to correcting the grammar errors are given due importance and that the tuning focuses on the grammatically incorrect parts of the sentences. The F-β score, as defined for the CoNLL shared task, is the most obvious metric to measure the accuracy of grammar correction on the tuning set. We choose the F-β metric as a score to be optimized using MERT for the SMT based grammar correction model. By choosing an appropriate value of β, it is possible to tune the system to favour increased recall/precision or a balance of both. SMT method for grammar correction (Buys and van der Merwe, 2013; Yuan and Felice, 2013). These include use of factored SMT, syntax based SMT, pruning of the phrase table, disabling or reordering, etc. The generic SMT approach has performed badly as compared to the specific custom made approaches (Yuan and Felice, 2013). Our system also builds upon the SMT methods and tries to address the above mentioned lacunae in two ways: • Tuning the SMT model to a metric suitable for grammar correction (i.e.F-β metric), instead of the BLEU metric. • Combination of custom-engineered methods and SMT based methods, by using classifier based for some error categories. Section 2 describes our met"
W14-1708,W14-1701,0,\N,Missing
W14-1708,W13-3611,1,\N,Missing
W14-1708,W13-3603,0,\N,Missing
W14-3308,P08-1087,0,0.171182,"Missing"
W14-3308,kunchukuttan-etal-2014-shata,1,0.901107,"Missing"
W14-3308,I11-1013,0,0.0307465,"Missing"
W14-3308,W13-2807,0,0.0300554,"Missing"
W14-3308,C10-1043,0,0.0255003,"rect/oblique case of the parallel nouns in Hindi sentence. We use object of preposition, subject, direct object, tense as our features. These features are extracted using semantic relations provided by Stanford’s typed dependencies (Marneffe, 2008). 4.4 that pre-orders the source sentence to conform to target word order. A substantial volume of work has been done in the field of source-side reordering for machine translation. Most of the experiments are based on applying reordering rules at the nodes of the parse tree of the source sentence. These reordering rules can be automatically learnt (Genzel, 2010). But, many source languages do not have a good robust parser. Hence, instead we can use shallow parsing techniques to get chunks of words and then reorder them. Reordering rules can be learned automatically from chunked data (Zhang, 2007). Hindi does not have a functional constituency or dependency parser available, as of now. But, a shallow parser4 is available for Hindi. Hence, we follow a chunk-based pre-ordering approach, wherein, we develop a set of rules to reorder the chunks in a source sentence. The following are the chunks tags generated by this shallow parser: Noun chunks (NP), Verb"
W14-3308,P07-1037,0,0.0349605,"Missing"
W14-3308,N03-1033,0,0.0119603,"Missing"
W14-3308,P03-1054,0,0.0276253,"ut various experiments. Starting with Phrase Based Statistical Machine Translation (PBSMT)(Koehn et. al., 2003) as baseline system we go ahead with pre-order PBSMT described in Section 4.1. After pre-ordering, we train a Factor Based SMT(Koehn, 2007b) model, where we add factors on the pre-ordered source corpus. In Factor Based SMT we have two variations- (a) using Supertag as factor described in Section 4.2 and (b) using number, case as factors described in Section 4.3. Experimental Setup English Corpus Normalization To begin with, the English data was tokenized using the Stanford tokenizer (Klein and Manning, 2003) and then true-cased using truecase.perl provided in MOSES toolkit. 3.2 100,839 Before splitting the data, we first randomize the parallel corpus. We filter out English sentences longer than 50 words along with their parallel Hindi translations. After filtering, we select 5000 sentences which are 10 to 20 words long as the test data, while remaining 284,832 sentences are used for training. We process the corpus through appropriate filters for normalization and then create a train-test split. 3.1 Hindi 3,092,555 118,285 17,961,357 289,832 182,777 Table 1: en-hi corpora statistics, post normalis"
W14-3308,N03-1017,0,0.0661416,"Missing"
W14-3308,W07-0401,0,0.0666536,"Missing"
W14-3308,P05-1066,0,0.225273,"Missing"
W14-3308,J99-2004,0,\N,Missing
W14-3308,N09-2047,0,\N,Missing
W14-3308,P02-1040,0,\N,Missing
W14-3308,W05-0909,0,\N,Missing
W14-3308,I08-1067,1,\N,Missing
W14-5105,N09-2047,0,0.0218425,"or an adjunction node marked by ∗ . Substitution and adjunction are those nodes which can be replaced by another supertag. For an adjunction node, it is necessary for its label to match the label of root node of supertag. A supertag can have atmost one adjunction node but can have more than one substitution node. 3.2 Supertagging Supertagging refers to tagging each word of a sentence with a supertag. An example of a supertagged sentence is shown in ﬁgure 2. NP S G♢ NP ↓ 0 I NP VP V♢ A♢ NP1 ↓ dark NP∗ NP N♢ chocolate ate Figure 2: Supertagged sentence “I ate dark chocolate” We use MICA Parser (Bangalore et al., 2009)2 to obtain rich linguistic information like POS tag, supertag, voice, the presence of empty subjects (PRO), wh-movement, deep syntactic argument (deep subject, deep direct object, deepindirect object), whether a verb heads a relative clause and also dependency relation, for each word. From this rich set of features, for each word, we extract word ID (essentially word position in a sentence), supertag, dependency relation and deep syntactic argument. Given a dependency relation, these supertags can be assembled using composition operation of TAG to form a constituent tree. Composition operatio"
W14-5105,P05-1033,0,0.0588163,"ection 3. Section 4 describes our approach for preordering. In Section 5, we provide the methodology for pre-ordering. Experimental setup is explained in Section 6, while corresponding results are shown in Section 7. We conclude our work in Section 8 providing acknowledgement in Section 9. 2 Related Work Word order has direct impact on the ﬂuency of transaltion obtained using SMT systems. There are basically two paradigms for generating correct word order. The ﬁrst paradigm deals with developing a reordering model which is used during decoding. Different solutions such as syntax-based models (Chiang, 2005), lexicalized reordering (Och et al., 2004), and tree-to-string methods (Zhang et al., 2006) have been proposed. Most of these approaches use statistical models to learn reordering rules, but all of them have diﬀerent methods to solve the reordering problem. The next paradigm deals with developing a reordering model which is used as pre-processing step (also known as pre-ordering) in SMT systems. In pre-ordering, the objective is to re1 http://en.wikipedia.org/wiki/Languages_of_ 30 India D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, page"
W14-5105,P05-1066,0,0.147181,"to re1 http://en.wikipedia.org/wiki/Languages_of_ 30 India D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 30–38, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) order source language words to conform to the target language word order. Xia and McCord (2004) describe an approach for translation from French to English, where reordering rules are acquired automatically using source and target parses and word alignment. The reordering rules in their approach operate at the level of contextfree rules in the parse tree. Collins et al. (2005), describe clause restructuring for German to English machine translation. They use six transformations that are applied on German parsed text to reorder it before training with a phrase based system. Popovic and Ney (2006) use hand-made rules to reorder the source side based on POS information. Zhang et al. (2007) propose chunk level reordering, where reordering rules are automatically learned from source-side chunks and word alignments. They allow all possible reorderings to be used to create a lattice that is input to the decoder. Genzel (2010), shows automatic rule extraction for 8 languag"
W14-5105,C94-1024,0,0.153428,"mmar (TAG), a mildly context sensitive formalism as discussed in Section 3. 3 Introduction to TAG/Supertag Tree Adjoining Grammar (TAG) was introduced by Joshi et al. (1975). Tree Adjoining Languages (TALs) generate some strictly context-sensitive languages and fall in the class of the so-called mildly context-sensitive languages. Lexicalized Tree Adjoining Grammar (LTAG) (Joshi and Schabes, 1991) is the TAG formalism where each lexical item is associated with atleast one elementary structure. This elementary structure of LTAG is known as Supertag. The concept of Supertag was ﬁrst proposed by Joshi and Srinivas (1994). Supertag localize dependencies, including longdistance dependencies, by requiring that all and only the dependent elements be present within the same structure. They provide syn31 tactic as well as dependency information at the word level by imposing complex constraints in a local context. Supertags provide information like POS tag, subcategorization and other syntactic constraints at the level of agreement feature. Supertag can also be viewed as fragments of parse tree associated with each lexical item. An example of supertag is shown in Figure 1. This supertag is used for transitive verbs."
W14-5105,P07-2045,0,0.0041265,"1.1K. We train a 5 gram language model using SRILM on a 50K monolingual Hindi corpus from the same domain. 4 WMT14 resources:-http://ufallab.ms.mff. cuni.cz/~bojar/hindencorp/ 5 http://www.speech.sri.com/projects/srilm 35 The English data was tokenized using the Stanford tokenizer and then true-cased using truecase.perl provided in MOSES toolkit. We normalize Hindi corpus using NLP Indic Library (Kunchukuttan et. al.,2014)6 . Normalization is followed by tokenization, wherein we make use of the trivtokenizer.pl provided with WMT14 shared task. 6.2 List of Experiments We use the MOSES toolkit (Koehn et al., 2007) to train various statistical machine translation systems. For English to Hindi we train three systems, on each data set (WMT14 and ILCI), as follows: • Phrase Based Systems: We train phrase based model without pre-ordering. • Context Free Grammar based preorderding: In this model, we reorder both train and test set using Patel et al. (2013)’s source side reordering rules which is reﬁnement of Ramanathan et al. (2008)’s rule-based reordering system. • Supertag based pre-ordering: In this model, we reorder both train and test set using our supertag based pre-ordering method as discussed in Sect"
W14-5105,P02-1040,0,0.102235,"Missing"
W14-5105,W13-2807,0,0.386194,"lkit. We normalize Hindi corpus using NLP Indic Library (Kunchukuttan et. al.,2014)6 . Normalization is followed by tokenization, wherein we make use of the trivtokenizer.pl provided with WMT14 shared task. 6.2 List of Experiments We use the MOSES toolkit (Koehn et al., 2007) to train various statistical machine translation systems. For English to Hindi we train three systems, on each data set (WMT14 and ILCI), as follows: • Phrase Based Systems: We train phrase based model without pre-ordering. • Context Free Grammar based preorderding: In this model, we reorder both train and test set using Patel et al. (2013)’s source side reordering rules which is reﬁnement of Ramanathan et al. (2008)’s rule-based reordering system. • Supertag based pre-ordering: In this model, we reorder both train and test set using our supertag based pre-ordering method as discussed in Section 5. We provide systematic comparision among these systems in Section 7. As the reordering rules are developed to conform Hindi word order, we were interested to see how does it aﬀect other Indian languages which have the same word order as Hindi. So, we developed various translation systems from English to other Indian languages using ILC"
W14-5105,popovic-ney-2006-pos,0,0.0255179,"on of India (NLPAI) order source language words to conform to the target language word order. Xia and McCord (2004) describe an approach for translation from French to English, where reordering rules are acquired automatically using source and target parses and word alignment. The reordering rules in their approach operate at the level of contextfree rules in the parse tree. Collins et al. (2005), describe clause restructuring for German to English machine translation. They use six transformations that are applied on German parsed text to reorder it before training with a phrase based system. Popovic and Ney (2006) use hand-made rules to reorder the source side based on POS information. Zhang et al. (2007) propose chunk level reordering, where reordering rules are automatically learned from source-side chunks and word alignments. They allow all possible reorderings to be used to create a lattice that is input to the decoder. Genzel (2010), shows automatic rule extraction for 8 language pairs. They ﬁrst extract a dependency tree and then converts it to a shallow constituent tree. The trees are annotated by both POS tags and by Stanford dependency types, then they learn reordering rules given a set of fea"
W14-5105,I08-1067,1,0.883807,"al.,2014)6 . Normalization is followed by tokenization, wherein we make use of the trivtokenizer.pl provided with WMT14 shared task. 6.2 List of Experiments We use the MOSES toolkit (Koehn et al., 2007) to train various statistical machine translation systems. For English to Hindi we train three systems, on each data set (WMT14 and ILCI), as follows: • Phrase Based Systems: We train phrase based model without pre-ordering. • Context Free Grammar based preorderding: In this model, we reorder both train and test set using Patel et al. (2013)’s source side reordering rules which is reﬁnement of Ramanathan et al. (2008)’s rule-based reordering system. • Supertag based pre-ordering: In this model, we reorder both train and test set using our supertag based pre-ordering method as discussed in Section 5. We provide systematic comparision among these systems in Section 7. As the reordering rules are developed to conform Hindi word order, we were interested to see how does it aﬀect other Indian languages which have the same word order as Hindi. So, we developed various translation systems from English to other Indian languages using ILCI corpus and compare it with standard pharse based systems. 7 Results In this"
W14-5105,2006.amta-papers.25,0,0.0671824,"Missing"
W14-5105,C04-1073,0,0.0531965,"cal models to learn reordering rules, but all of them have diﬀerent methods to solve the reordering problem. The next paradigm deals with developing a reordering model which is used as pre-processing step (also known as pre-ordering) in SMT systems. In pre-ordering, the objective is to re1 http://en.wikipedia.org/wiki/Languages_of_ 30 India D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 30–38, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) order source language words to conform to the target language word order. Xia and McCord (2004) describe an approach for translation from French to English, where reordering rules are acquired automatically using source and target parses and word alignment. The reordering rules in their approach operate at the level of contextfree rules in the parse tree. Collins et al. (2005), describe clause restructuring for German to English machine translation. They use six transformations that are applied on German parsed text to reorder it before training with a phrase based system. Popovic and Ney (2006) use hand-made rules to reorder the source side based on POS information. Zhang et al. (2007)"
W14-5105,N06-1033,0,0.0259579,"he methodology for pre-ordering. Experimental setup is explained in Section 6, while corresponding results are shown in Section 7. We conclude our work in Section 8 providing acknowledgement in Section 9. 2 Related Work Word order has direct impact on the ﬂuency of transaltion obtained using SMT systems. There are basically two paradigms for generating correct word order. The ﬁrst paradigm deals with developing a reordering model which is used during decoding. Different solutions such as syntax-based models (Chiang, 2005), lexicalized reordering (Och et al., 2004), and tree-to-string methods (Zhang et al., 2006) have been proposed. Most of these approaches use statistical models to learn reordering rules, but all of them have diﬀerent methods to solve the reordering problem. The next paradigm deals with developing a reordering model which is used as pre-processing step (also known as pre-ordering) in SMT systems. In pre-ordering, the objective is to re1 http://en.wikipedia.org/wiki/Languages_of_ 30 India D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 30–38, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) order source lan"
W14-5105,W07-0401,0,0.022631,"a and McCord (2004) describe an approach for translation from French to English, where reordering rules are acquired automatically using source and target parses and word alignment. The reordering rules in their approach operate at the level of contextfree rules in the parse tree. Collins et al. (2005), describe clause restructuring for German to English machine translation. They use six transformations that are applied on German parsed text to reorder it before training with a phrase based system. Popovic and Ney (2006) use hand-made rules to reorder the source side based on POS information. Zhang et al. (2007) propose chunk level reordering, where reordering rules are automatically learned from source-side chunks and word alignments. They allow all possible reorderings to be used to create a lattice that is input to the decoder. Genzel (2010), shows automatic rule extraction for 8 language pairs. They ﬁrst extract a dependency tree and then converts it to a shallow constituent tree. The trees are annotated by both POS tags and by Stanford dependency types, then they learn reordering rules given a set of features. This paper discusses about creating manual reordering rules with the help of Tree Adjo"
W14-5105,jha-2010-tdil,0,\N,Missing
W14-5105,C10-1043,0,\N,Missing
W14-5105,N04-1021,0,\N,Missing
W15-3912,P12-1049,0,0.0552216,"uages. The BrahmiNet corpus contains transliteration corpora for 110 Indian language pairs mined from the ILCI corpus, a parallel translation corpora of 11 Indian languages (Jha, 2012). The rest of the paper is organized as follows. Section 2 and Section 3 describes our system and experimental setup respectively. Section 4 discusses the results of various data representation methods and the use of mined corpus respectively. Section 5 concludes the report. 2 2.2 Use of mined transliteration corpus We explore the use of transliteration corpora mined from translation corpora for transliteration. Sajjad et al. (2012) proposed an unsupervised method for mining transliteration pairs from parallel corpus. Their approach models parallel translation corpus generation as a generative process comprising an interpolation of a transliteration and a non-transliteration process. The parameters of the generative process are learnt using the EM procedure, followed by extraction of transliteration pairs from the parallel corpora by setting an appropriate threshold. We compare the quality of the transliteration systems built from such mined corpora with systems trained on manually created NEWS 2015 corpora for English-I"
W15-3912,E14-4029,0,0.05518,"Missing"
W15-3912,E12-1015,0,0.0138349,"ze. PBSMT learning of character sequence mappings is agnostic of the position of the character in the word. Hence, we explore to transform the data representation to encode position information. Zhang et al. (2012) did not report any benefit from such a representation for Chinese-English transliteration. We investigated if such encoding useful for alphabetic and consonantal scripts as opposed to logographic scripts like Chinese. It is generally believed that syllabification of the text helps improve transliteration systems. However, syllabification systems are not available for all languages. Tiedemann (2012) proposed a character-level, overlapping bigram representation in the context of machine translation using transliteration. We can view this as weak, coarse and language independent syllabification approach. We explore this overlapping, segmentation approach for the transliteration task. For many language pairs, parallel transliteration corpora are not publicly available. However, parallel translation corpora like Europarl (Koehn, 2005) and ILCI (Jha, 2012) are available for many language pairs. Transliteration corpora mined from such parallel corpora has been shown to be useful for machine tr"
W15-3912,2010.iwslt-papers.7,0,0.258723,"ng heat maps of confusion matrices. 1 Introduction Machine Transliteration can be viewed as a problem of transforming a sequence of characters in one alphabet to another. Transliteration can be seen as a special case of the general translation problem between two languages. The primary differences from the general translation problem are: (i) limited vocabulary size, and (ii) simpler grammar with no reordering. Phrase based statistical machine translation (PB-SMT) is a robust and well-understood technology and can be easily adopted for application to the transliteration problem (Noeman, 2009; Finch and Sumita, 2010). Our submission to the NEWS 2015 shared task is a PBSMT system. Over a baseline PBSMT system, we address two issues: (i) suitable data representation for training, and (ii) parallel transliteration corpus availability. In many writing systems, the same logical/phonetic symbols can have different charac78 Proceedings of the Fifth Named Entity Workshop, joint with 53rd ACL and the 7th IJCNLP, pages 78–82, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2015) for transliteration between English and Indian languages. The BrahmiNet corpus contains transliteration"
W15-3912,gupta-etal-2012-mining,0,0.0397785,"Missing"
W15-3912,W12-4407,0,0.0222038,"an scripts have different characters for independent vowels and vowel diacritics. Independent vowels typically occurs at the beginning of the word, while diacritics occur in medial and terminal positions. The pronounciation, and hence the transliteration could also depend on the position of the characters. For instance, the terminal ion in nation would be pronounced differently from initial one in ionize. PBSMT learning of character sequence mappings is agnostic of the position of the character in the word. Hence, we explore to transform the data representation to encode position information. Zhang et al. (2012) did not report any benefit from such a representation for Chinese-English transliteration. We investigated if such encoding useful for alphabetic and consonantal scripts as opposed to logographic scripts like Chinese. It is generally believed that syllabification of the text helps improve transliteration systems. However, syllabification systems are not available for all languages. Tiedemann (2012) proposed a character-level, overlapping bigram representation in the context of machine translation using transliteration. We can view this as weak, coarse and language independent syllabification"
W15-3912,P07-2045,0,0.0103237,"urce and target training corpus. We use character (P) as well as bigram representations (T). In character based system, the character is the basic unit of transliteration. In bigram-based system, the overlapping bigram is the basic unit of transliteration. We also augmented the word representation with word boundary markers (M) (ˆ for start of word and $ end of word). The various representations we experimented with are illustrated below: character (P) character+boundary marker (M) bigram (T) bigram+boundary marker (M+T) Src Tgt En En En Hi Ba Ta Size 10513 7567 3549 We use the Moses toolkit (Koehn et al., 2007) to train the transliteration system and the language models were estimated using the SRILM toolkit (Stolcke and others, 2002). The transliteration pairs are mined using the transliteration module in Moses (Durrani et al., 2014). 4 Results and Error Analysis 4.1 Effect of Data Representation methods HINDI ˆHINDI$ HI IN ND DI I ˆH HI IN ND DI I$ $ The abbreviations mentioned above are used subsequently to refer to these data representations. 79 Table 1 shows transliteration results for various data representation methods on the development set. We see improvements in transliteration accuracy of"
W15-3912,2005.mtsummit-papers.11,0,0.0117058,"ally believed that syllabification of the text helps improve transliteration systems. However, syllabification systems are not available for all languages. Tiedemann (2012) proposed a character-level, overlapping bigram representation in the context of machine translation using transliteration. We can view this as weak, coarse and language independent syllabification approach. We explore this overlapping, segmentation approach for the transliteration task. For many language pairs, parallel transliteration corpora are not publicly available. However, parallel translation corpora like Europarl (Koehn, 2005) and ILCI (Jha, 2012) are available for many language pairs. Transliteration corpora mined from such parallel corpora has been shown to be useful for machine translation, cross lingual information retrieval, etc. (Kunchukuttan et al., 2014). In this paper, we make an intrinsic evaluation of the performance of the automatically mined BrahmiNet transliteration corpus (Kunchukuttan et al., Our NEWS 2015 shared task submission is a PBSMT based transliteration system with the following corpus preprocessing enhancements: (i) addition of wordboundary markers, and (ii) languageindependent, overlapping"
W15-3912,N15-3017,1,0.899131,"Missing"
W15-3912,W09-3525,0,0.17505,"ons by analyzing heat maps of confusion matrices. 1 Introduction Machine Transliteration can be viewed as a problem of transforming a sequence of characters in one alphabet to another. Transliteration can be seen as a special case of the general translation problem between two languages. The primary differences from the general translation problem are: (i) limited vocabulary size, and (ii) simpler grammar with no reordering. Phrase based statistical machine translation (PB-SMT) is a robust and well-understood technology and can be easily adopted for application to the transliteration problem (Noeman, 2009; Finch and Sumita, 2010). Our submission to the NEWS 2015 shared task is a PBSMT system. Over a baseline PBSMT system, we address two issues: (i) suitable data representation for training, and (ii) parallel transliteration corpus availability. In many writing systems, the same logical/phonetic symbols can have different charac78 Proceedings of the Fifth Named Entity Workshop, joint with 53rd ACL and the 7th IJCNLP, pages 78–82, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2015) for transliteration between English and Indian languages. The BrahmiNet corpus"
W15-3912,P03-1021,0,0.00711804,"teration systems built from such mined corpora with systems trained on manually created NEWS 2015 corpora for English-Indian language pairs. System Description We use a standard PB-SMT model for transliteration between the various language pairs. It is a discriminative, log-linear model which uses standard SMT features viz. direct/inverse phrase translation probabilities, direct/inverse lexical translation probabilities, phrase penalty, word penalty and language model score. The feature weights are tuned to optimize BLEU (Papineni et al., 2002) using the Minimum Error Rate Training algorithm (Och, 2003). It would be better to explore optimizing metrics like accuracy or edit distance instead of using BLEU as a proxy for these metrics. We experiment with various transliteration units as discussed in Section 2.1. We use a 5-gram language model over the transliteration units estimated using Witten-Bell smoothing. Since transliteration does not require any reordering, monotone decoding was done. 2.1 3 Experimental Setup For building the transliteration model with the NEWS 2015 shared task corpus as well as the BrahmiNet corpus, we used 500 word pairs for tuning and the rest for SMT training. The"
W15-3912,P02-1040,0,0.0952953,"setting an appropriate threshold. We compare the quality of the transliteration systems built from such mined corpora with systems trained on manually created NEWS 2015 corpora for English-Indian language pairs. System Description We use a standard PB-SMT model for transliteration between the various language pairs. It is a discriminative, log-linear model which uses standard SMT features viz. direct/inverse phrase translation probabilities, direct/inverse lexical translation probabilities, phrase penalty, word penalty and language model score. The feature weights are tuned to optimize BLEU (Papineni et al., 2002) using the Minimum Error Rate Training algorithm (Och, 2003). It would be better to explore optimizing metrics like accuracy or edit distance instead of using BLEU as a proxy for these metrics. We experiment with various transliteration units as discussed in Section 2.1. We use a 5-gram language model over the transliteration units estimated using Witten-Bell smoothing. Since transliteration does not require any reordering, monotone decoding was done. 2.1 3 Experimental Setup For building the transliteration model with the NEWS 2015 shared task corpus as well as the BrahmiNet corpus, we used 5"
W15-3912,jha-2010-tdil,0,\N,Missing
W15-5902,C12-1038,0,0.0460358,"Missing"
W15-5902,D12-1052,0,0.0190584,"et al., 2013) or oversampling the incorrect instances (Xing et al., 2013). Rozovskaya et al. (2012) propose an error inflation method for preposition error correction, where a fraction of the correct prepositions are marked as incorrect prepositions and these new “erroneous” instances are distributed among different possible erroneous prepositions. This method is similar to the MetaCost (Domingos, 1999) approach of re-labelling examples in the training set according to a cost function. Some whole-sentence correction approaches (Kunchukuttan et al., 2014; JunczysDowmunt and Grundkiewicz, 2014; Dahlmeier and Ng, 2012) are tuned to maximize the Fβ scores. In all the work mentioned above, the systems maximize Fβ by tuning only the “hyperparameters” like sampling threshold, features weights for scores of underlying components —classifier (Dahlmeier and Ng, 2012) or SMT component scores (Kunchukuttan et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014) —on a tuning dataset using approximate methods like MERT (Och, 2003), PRO (Hopkins and May, 2011) and grid search. In contrast, we optimize all the model parameters exactly and efficiently to maximize Fβ . 4 Optimizing Performance Measures A loss function like"
W15-5902,W13-1703,0,0.0223347,"instances. For each sampling method, sampled datasets were created using different representative sampling ratios (p = 0.3, 0.5, 1.0) which span the entire range. The sampling ratio (p) refers to the ratio of incorrect to correct examples in the sampled dataset. For cost-senstive learning, the corresponding misclassification cost ratio (J) can be computed as J = pR, where R is the ratio of correct to incorrect instances in training set. An SVM is trained with hinge loss on each of these sampled datasets. 6 Experimental Setup We tested our GED systems on three annotated learner corpora: NUCLE (Dahlmeier et al., 2013), HOO11 (Dale and Kilgarriff, 2011) and HOO12 (Dale et al., 2012) shared task corpora. For the HOO12 dataset, noun number error detection was not done since the dataset did not have these annotations. For hinge loss, the classifiers were trained using the SVMLight package (Joachims, b). For other loss functions, classifiers were trained using the SVM-Perf package (Joachims, a) with extensions to optimize recall, precision and Fβ . We used a linear kernel for all our experiments.The evaluation was done using Precision, Recall, F0.5 , F1 and F2 metrics. The average scores over a 5-fold cross-val"
W15-5902,W11-2838,0,0.0295713,"hod, sampled datasets were created using different representative sampling ratios (p = 0.3, 0.5, 1.0) which span the entire range. The sampling ratio (p) refers to the ratio of incorrect to correct examples in the sampled dataset. For cost-senstive learning, the corresponding misclassification cost ratio (J) can be computed as J = pR, where R is the ratio of correct to incorrect instances in training set. An SVM is trained with hinge loss on each of these sampled datasets. 6 Experimental Setup We tested our GED systems on three annotated learner corpora: NUCLE (Dahlmeier et al., 2013), HOO11 (Dale and Kilgarriff, 2011) and HOO12 (Dale et al., 2012) shared task corpora. For the HOO12 dataset, noun number error detection was not done since the dataset did not have these annotations. For hinge loss, the classifiers were trained using the SVMLight package (Joachims, b). For other loss functions, classifiers were trained using the SVM-Perf package (Joachims, a) with extensions to optimize recall, precision and Fβ . We used a linear kernel for all our experiments.The evaluation was done using Precision, Recall, F0.5 , F1 and F2 metrics. The average scores over a 5-fold cross-validation are reported. 7 7.1 Results"
W15-5902,W12-2006,0,0.0171159,"ng different representative sampling ratios (p = 0.3, 0.5, 1.0) which span the entire range. The sampling ratio (p) refers to the ratio of incorrect to correct examples in the sampled dataset. For cost-senstive learning, the corresponding misclassification cost ratio (J) can be computed as J = pR, where R is the ratio of correct to incorrect instances in training set. An SVM is trained with hinge loss on each of these sampled datasets. 6 Experimental Setup We tested our GED systems on three annotated learner corpora: NUCLE (Dahlmeier et al., 2013), HOO11 (Dale and Kilgarriff, 2011) and HOO12 (Dale et al., 2012) shared task corpora. For the HOO12 dataset, noun number error detection was not done since the dataset did not have these annotations. For hinge loss, the classifiers were trained using the SVMLight package (Joachims, b). For other loss functions, classifiers were trained using the SVM-Perf package (Joachims, a) with extensions to optimize recall, precision and Fβ . We used a linear kernel for all our experiments.The evaluation was done using Precision, Recall, F0.5 , F1 and F2 metrics. The average scores over a 5-fold cross-validation are reported. 7 7.1 Results and Discussion Limitations of"
W15-5902,D11-1125,0,0.0339617,"les in the training set according to a cost function. Some whole-sentence correction approaches (Kunchukuttan et al., 2014; JunczysDowmunt and Grundkiewicz, 2014; Dahlmeier and Ng, 2012) are tuned to maximize the Fβ scores. In all the work mentioned above, the systems maximize Fβ by tuning only the “hyperparameters” like sampling threshold, features weights for scores of underlying components —classifier (Dahlmeier and Ng, 2012) or SMT component scores (Kunchukuttan et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014) —on a tuning dataset using approximate methods like MERT (Och, 2003), PRO (Hopkins and May, 2011) and grid search. In contrast, we optimize all the model parameters exactly and efficiently to maximize Fβ . 4 Optimizing Performance Measures A loss function like the induced Fβ loss is nondecomposable i.e. it cannot be expressed as the sum of losses over individual instances. We use the max-margin formulation proposed by Joachims (2005) for exactly optimizing such nondecomposable loss functions which can be computed from the contingency table1 . It is applicable only to binary classification problems. The training loss is described in terms of a loss function (∆) which measures the discrepan"
W15-5902,W14-1703,0,0.0145487,"ible erroneous prepositions. This method is similar to the MetaCost (Domingos, 1999) approach of re-labelling examples in the training set according to a cost function. Some whole-sentence correction approaches (Kunchukuttan et al., 2014; JunczysDowmunt and Grundkiewicz, 2014; Dahlmeier and Ng, 2012) are tuned to maximize the Fβ scores. In all the work mentioned above, the systems maximize Fβ by tuning only the “hyperparameters” like sampling threshold, features weights for scores of underlying components —classifier (Dahlmeier and Ng, 2012) or SMT component scores (Kunchukuttan et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014) —on a tuning dataset using approximate methods like MERT (Och, 2003), PRO (Hopkins and May, 2011) and grid search. In contrast, we optimize all the model parameters exactly and efficiently to maximize Fβ . 4 Optimizing Performance Measures A loss function like the induced Fβ loss is nondecomposable i.e. it cannot be expressed as the sum of losses over individual instances. We use the max-margin formulation proposed by Joachims (2005) for exactly optimizing such nondecomposable loss functions which can be computed from the contingency table1 . It is applicable only to binary classification pro"
W15-5902,W14-1708,1,0.836405,"nces (Dahlmeier et al., 2012; Putra and Szabo, 2013; Kunchukuttan et al., 2013) or oversampling the incorrect instances (Xing et al., 2013). Rozovskaya et al. (2012) propose an error inflation method for preposition error correction, where a fraction of the correct prepositions are marked as incorrect prepositions and these new “erroneous” instances are distributed among different possible erroneous prepositions. This method is similar to the MetaCost (Domingos, 1999) approach of re-labelling examples in the training set according to a cost function. Some whole-sentence correction approaches (Kunchukuttan et al., 2014; JunczysDowmunt and Grundkiewicz, 2014; Dahlmeier and Ng, 2012) are tuned to maximize the Fβ scores. In all the work mentioned above, the systems maximize Fβ by tuning only the “hyperparameters” like sampling threshold, features weights for scores of underlying components —classifier (Dahlmeier and Ng, 2012) or SMT component scores (Kunchukuttan et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014) —on a tuning dataset using approximate methods like MERT (Och, 2003), PRO (Hopkins and May, 2011) and grid search. In contrast, we optimize all the model parameters exactly and efficiently to maxim"
W15-5902,P03-1021,0,0.0112546,"e-labelling examples in the training set according to a cost function. Some whole-sentence correction approaches (Kunchukuttan et al., 2014; JunczysDowmunt and Grundkiewicz, 2014; Dahlmeier and Ng, 2012) are tuned to maximize the Fβ scores. In all the work mentioned above, the systems maximize Fβ by tuning only the “hyperparameters” like sampling threshold, features weights for scores of underlying components —classifier (Dahlmeier and Ng, 2012) or SMT component scores (Kunchukuttan et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014) —on a tuning dataset using approximate methods like MERT (Och, 2003), PRO (Hopkins and May, 2011) and grid search. In contrast, we optimize all the model parameters exactly and efficiently to maximize Fβ . 4 Optimizing Performance Measures A loss function like the induced Fβ loss is nondecomposable i.e. it cannot be expressed as the sum of losses over individual instances. We use the max-margin formulation proposed by Joachims (2005) for exactly optimizing such nondecomposable loss functions which can be computed from the contingency table1 . It is applicable only to binary classification problems. The training loss is described in terms of a loss function (∆)"
W15-5902,D10-1094,0,0.0712138,"Missing"
W15-5902,W12-2032,0,0.0169319,"rity class instances in the4 training set has shown good improvement in many applications (Chawla et al., 2002). For large feature spaces, the generation of synthetic examples can be computationally expensive since it involves a k-nearest neighbourhood search. Optimizing evaluation metrics directly does not incur this computational overhead. 3 Related Work The most common methods to handle imbalanced datasets in GED involve undersampling the correct instances (Dahlmeier et al., 2012; Putra and Szabo, 2013; Kunchukuttan et al., 2013) or oversampling the incorrect instances (Xing et al., 2013). Rozovskaya et al. (2012) propose an error inflation method for preposition error correction, where a fraction of the correct prepositions are marked as incorrect prepositions and these new “erroneous” instances are distributed among different possible erroneous prepositions. This method is similar to the MetaCost (Domingos, 1999) approach of re-labelling examples in the training set according to a cost function. Some whole-sentence correction approaches (Kunchukuttan et al., 2014; JunczysDowmunt and Grundkiewicz, 2014; Dahlmeier and Ng, 2012) are tuned to maximize the Fβ scores. In all the work mentioned above, the s"
W15-5902,W13-3605,0,0.0128181,"nterpolation of minority class instances in the4 training set has shown good improvement in many applications (Chawla et al., 2002). For large feature spaces, the generation of synthetic examples can be computationally expensive since it involves a k-nearest neighbourhood search. Optimizing evaluation metrics directly does not incur this computational overhead. 3 Related Work The most common methods to handle imbalanced datasets in GED involve undersampling the correct instances (Dahlmeier et al., 2012; Putra and Szabo, 2013; Kunchukuttan et al., 2013) or oversampling the incorrect instances (Xing et al., 2013). Rozovskaya et al. (2012) propose an error inflation method for preposition error correction, where a fraction of the correct prepositions are marked as incorrect prepositions and these new “erroneous” instances are distributed among different possible erroneous prepositions. This method is similar to the MetaCost (Domingos, 1999) approach of re-labelling examples in the training set according to a cost function. Some whole-sentence correction approaches (Kunchukuttan et al., 2014; JunczysDowmunt and Grundkiewicz, 2014; Dahlmeier and Ng, 2012) are tuned to maximize the Fβ scores. In all the w"
W15-5902,W14-1701,0,\N,Missing
W15-5902,W13-3611,1,\N,Missing
W15-5902,W12-2025,0,\N,Missing
W15-5902,W13-3612,0,\N,Missing
W15-5944,H05-1085,0,0.0339948,"similarity between resource-poor languages and resource-rich languages for the translation task. Dabre et al. (2014) used multiple decoding paths (MDP) to overcome D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 303–307, c Trivandrum, India. December 2015. 2015 NLP Association of India (NLPAI) the limitation of small sized corpora.Paul et al. (2013) discusses criteria to be considered for selection of good pivot language. Use of sourceside segmentation as pre-processing technique has been demonstrated by (Kunchukuttan et al., 2014). Goldwater and McClosky (2005) investigates several methods for incorporating morphological information to achieve better translation from Czech to English. Most of the pivot strategies mentioned above focus on the situation of resource-poor languages where direct translation is either very poor or not available. Our approach, like Dabre et al. (2014), tries to employ pivot strategy to help improve the performance of existing SMT systems. To the best of our knowledge, our work is the first attempt to integrate word segmentation with pivot-based SMT. 3 Our System We propose a system which integrates word segmentation with t"
W15-5944,D07-1091,0,0.0548085,"orpus Pivot-Target Corpus Morfessor SourceMorphPivotMorph PivotMorphTarget SMT Training SMT Training SourceMorphPivotMorph Phrase table PivotMorphTarget Phrase table Triangulation SourceMorphTarget Phrase table Tune And Test Figure 1: Integration of word segmentation with triangulation 3.4 Multiple Decoding Paths We use the triangulated phrase table to supplement the direct phrase table. In order to integrate these two phrase tables, we use the multiple decoding paths (MDP) feature provided by the 1 https://github.com/anoopkunchukuttan/indic_nlp_library Moses decoder. Multiple decoding paths (Koehn and Hoang, 2007) allows us to lookup multiple translation models for hypothesis at decoding time, and the choice of best hypothesis at decoding time based on available evidence. We use MDP to combine one or more pivot-based MT systems with the direct MT system. This constitutes our final decoding system. We preferred this option over offline linear interpolation of phrase tables since the framework can dynamically consider phrases from multiple phrase tables and wouldn’t need any hyperparameter tuning. 4 Experiments The aim of experiments is to study impact of pivot strategies and word segmentation, separatel"
W15-5944,N03-1017,0,0.0188284,"ively. Using these two models, we induce a source-target model. The two important components to be calculated are - 1) phrase translation probability and 2) lexical weight. The Phrase translation probability is estimated by marginalizing over all possible pivot phrase, along with the assumption that the target phrases are independent of the source phrase given the pivot phrase. The phrase translation probability can be calculated as shown below: ( ) ϕ s∥t = ∑ p  ( ) ϕ (s∥ p) ϕ p∥t (1) Where, s, p, t are phrases in languages Ls , Lp , Lt respectively. The Lexical Weight, according to Koehn et al. (2003), depends on - 1) word alignment information a in a phrase pair (s, t) and 2) lexical translation probability w(s|t). 304 Lexical weight can be modeled using following equation, ( ) pw f∥e, a = n ∏ ∑ 1 w (fi ∥ej ) ∥j |(i, j) ∈ a∥ ∀(i,j)∈a i=1 (2) Wu and Wang (2009) discuss in detail about alignments information and lexical translation probability. 3.2 Word segmentation We use unsupervised word segmentation as preprocessing technique. For this purpose, Morfessor (Virpioja et al., 2013) is used. It performs morphological segmentation of words of a natural language, based solely on raw text dat"
W15-5944,P02-1040,0,0.0925006,"g, 500 sentences are used for tuning and 2000 sentences are used for testing. For the experiments, we use phrase-based SMT training and 5-gram SRILM language model. Tuning is done using the MERT algorithm. The triangulated MT systems use default distance based reordering while direct systems use wbe-msdbidirectional-fe-allff model 4.2 Experimental Setup We trained various phrase based SMT systems by combining the basic systems mentioned in Section 3. We use a threshold of 0.001 for phrase translation probability to manage size of triangulated phrase table. The performance metric used is BLEU (Papineni et al., 2002). The following are the configurations we experimented with: 1. DIR: MT system trained on direct SourceTarget corpus. 2. DIR_Morph: DIR system with source-text word-segmented. 3. PIVOT: MT system based on triangulated phrase table of Source-Target using a single 305 Pivot language. 4. PIVOT_Morph: PIVOT system with both Source and Pivot texts segmented. 5. PIVOT_SourceMorph: PIVOT system with only Source text segmented. 6. DIR+PIVOT: MT system based on integration of DIR and PIVOT phrase tables using MDP. 7. DIR_Morph+PIVOT_Morph: MT system based on integration of DIR_Morph and PIVOT_Morph usi"
W15-5944,N07-1061,0,0.0200742,"t work done in the field. Section 3 explains the design of our system in detail. Section 4 describes the experimental setup. Results of the experiments are discussed in Section 5. Section 6 includes concluding remarks on the mal-hin translation task. 2 Related Work There is substantial amount on pivot-based SMT. De Gispert and Marino (2006) discuss translation tasks between Catalan and English with the use of Spanish as a pivot language. Pivoting is done using two techniques: pipelining of source-pivot and pivot-target SMT systems and direct translation using a synthesized Catalan-English. In Utiyama and Isahara (2007), the authors propose the use of pivot language through - phrase translation (phrase table creation) and sentence translation. Wu and Wang (2007) compare three pivot strategies viz. - phrase translation (i.e. triangulation), transfer method and synthetic method. Nakov and Ng (2012) try to exploit the similarity between resource-poor languages and resource-rich languages for the translation task. Dabre et al. (2014) used multiple decoding paths (MDP) to overcome D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 303–307, c Trivandrum, Ind"
W15-5944,P07-1108,0,0.453884,"re discussed in Section 5. Section 6 includes concluding remarks on the mal-hin translation task. 2 Related Work There is substantial amount on pivot-based SMT. De Gispert and Marino (2006) discuss translation tasks between Catalan and English with the use of Spanish as a pivot language. Pivoting is done using two techniques: pipelining of source-pivot and pivot-target SMT systems and direct translation using a synthesized Catalan-English. In Utiyama and Isahara (2007), the authors propose the use of pivot language through - phrase translation (phrase table creation) and sentence translation. Wu and Wang (2007) compare three pivot strategies viz. - phrase translation (i.e. triangulation), transfer method and synthetic method. Nakov and Ng (2012) try to exploit the similarity between resource-poor languages and resource-rich languages for the translation task. Dabre et al. (2014) used multiple decoding paths (MDP) to overcome D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 303–307, c Trivandrum, India. December 2015. 2015 NLP Association of India (NLPAI) the limitation of small sized corpora.Paul et al. (2013) discusses criteria to be consid"
W15-5944,P09-1018,0,0.0175912,", along with the assumption that the target phrases are independent of the source phrase given the pivot phrase. The phrase translation probability can be calculated as shown below: ( ) ϕ s∥t = ∑ p  ( ) ϕ (s∥ p) ϕ p∥t (1) Where, s, p, t are phrases in languages Ls , Lp , Lt respectively. The Lexical Weight, according to Koehn et al. (2003), depends on - 1) word alignment information a in a phrase pair (s, t) and 2) lexical translation probability w(s|t). 304 Lexical weight can be modeled using following equation, ( ) pw f∥e, a = n ∏ ∑ 1 w (fi ∥ej ) ∥j |(i, j) ∈ a∥ ∀(i,j)∈a i=1 (2) Wu and Wang (2009) discuss in detail about alignments information and lexical translation probability. 3.2 Word segmentation We use unsupervised word segmentation as preprocessing technique. For this purpose, Morfessor (Virpioja et al., 2013) is used. It performs morphological segmentation of words of a natural language, based solely on raw text data. Morfessor uses probabilistic machine learning methods to do the task of segmentation. The trained models for word segmentation of Indian languages are available to use1 . 3.3 Integrating word segmentation with Triangulation In our system, we use both phrase table"
W15-5950,P05-1066,0,0.070739,"using source-side reordering as a pre-processing step. In the core SMT system, word order can be tackled using a variety of models of varying complexity: word-level alignment models (Brown et al., 1993), lexicalized reordering models (Tillmann, 2004; Galley and Manning, 2008), hierarchical SMT (Chiang, 2005), syntax based SMT (Yamada and Knight, 2002), etc. SMT models like phrase-based SMT are not good at bridging word order divergence (Marie et al., 2014). 351 In these cases, source-side reordering is used as a preprocessing step to convert the source sentence to target language word order (Collins et al., 2005; Ramanathan et al., 2008). The best performing approaches generally rely on parse information on the source side to generate the correct word order. However, it has proved to be a very difficult problem which is far from being solved, especially when parse information is not forthcoming. The computational complexity of searching through a large space of potential reorderings and the need for incorporating higher level linguistic information are the primary challenges in tackling the reordering problem. While there is active research in preordering and in-decoder approaches, there has been lit"
W15-5950,W14-3348,0,0.0135241,"g the target text to match the source order, thus necessitating the second reordering stage. 3 Generating Oracle Translations To estimate an upper bound on the improvement in translation accuracy possible with post-ordering, we generate oracle reorderings of the baseline SMT output hypothesis. An oracle reordering is the best possible word order of the hypothesis, in terms of fluency and syntactic correctness. We propose the following algorithm for computing the oracle reordering. 1. Obtain word alignments between the hypothesis and reference using the monolingual aligner algorithm in METEOR (Denkowski and Lavie, 2014). 2. Construct a new sentence by rearranging aligned words from the hypothesis using the 353 word-order from the reference. 3. Use additional heuristics to include as many unaligned words from the hypothesis into the oracle reordering as possible. In our experiments, the words in the hypothesis that were not aligned by METEOR but found a stemmatch in the reference were inserted in the oracle sentence. The resulting oracle hypothesis is a permutation of a subset of words in the original MT decoding step, such that they reflect the word order in the reference. 4 Experimental Setup We studied dif"
W15-5950,W07-0414,0,0.0299061,"estimate the upper bound. Section 4 describes our experimental setup and 5 presents the results and discusses the observations. Section 6 concludes the paper and points out future work. 2 Related Work Oracle translations have been used by many researchers for diagnosing translation output. Auli et al. (2009) and Wisniewski and Yvon (2013) have used oracle translations to do reference reachability analysis and identify model and search errors. Wisniewski and Yvon (2013) have used the oracle translations to conduct various kinds of failure analysis and study effect of various search parameters. Dreyer et al. (2007), Li and Khudanpur (2009) and Wisniewski and Yvon (2013) use oracle translations to understand the limitations of various reordering constraints imposed on translation decoders. In the same spirit, we try to estimate an upper bound on the benefits of post-ordering the baseline SMT output. Though we do not tackle the problem of postordering in this work, we summarize the existing work on post-ordering for SMT. There has been work on post-editing of machine translation output. The method described in (Simard et al., 2007) is most commonly used. It involves automatically post-editing the output o"
W15-5950,D08-1089,0,0.0336209,"oring the post-ordering problem. 1 Introduction Word order divergence is a central problem in Statistical Machine Translation (SMT) and major stumbling block to generating comprehensible translations. Many solutions for reordering have been proposed to bridge this divergence. Word order divergence is generally handled within the core SMT model or using source-side reordering as a pre-processing step. In the core SMT system, word order can be tackled using a variety of models of varying complexity: word-level alignment models (Brown et al., 1993), lexicalized reordering models (Tillmann, 2004; Galley and Manning, 2008), hierarchical SMT (Chiang, 2005), syntax based SMT (Yamada and Knight, 2002), etc. SMT models like phrase-based SMT are not good at bridging word order divergence (Marie et al., 2014). 351 In these cases, source-side reordering is used as a preprocessing step to convert the source sentence to target language word order (Collins et al., 2005; Ramanathan et al., 2008). The best performing approaches generally rely on parse information on the source side to generate the correct word order. However, it has proved to be a very difficult problem which is far from being solved, especially when parse"
W15-5950,P12-2061,0,0.0225532,"वरदान स ध झाल Ref: these injections have proved to be a boon for heart patients . Output: this injection of heart patients are proved to be a boon for the . 0 Oracle: 1 2 3 5 4 7 6 8 9 10 11 12 13 this injection are proved to be a boon for heart patients . 5 re-ordered using alignments 6 7 8 9 10 11 re-ordered with heuristics 3 4 13 not aligned/used Figure 1: Construction of Oracle reordering post-ordering system in the sense in which we have defined it. Theirs is actually a two stage translation system that decomposes the translation problem into lexical transfer and reordering subproblems. Goto et al. (2012) and Goto et al. (2013) also propose post-ordering systems with the same architecture, but different reordering methods in the second stage. The motivation in these postordering methods is not to improve upon the word order. Rather, lexical mappings are learnt in the first stage after reordering the target text to match the source order, thus necessitating the second reordering stage. 3 Generating Oracle Translations To estimate an upper bound on the improvement in translation accuracy possible with post-ordering, we generate oracle reorderings of the baseline SMT output hypothesis. An oracle"
W15-5950,W06-3114,0,0.0220096,"on the training-set using Kneser-Ney smoothing with SRILM (Stolcke and others, 2002). The hierarchical systems were also trained with Moses using default parameters. For three phrase-based SMT systems with Hindi, Marathi and Malayalam respectively as source and English as target language, qualitative analysis was performed through manual evaluation of output sentences by native speakers of each of the source languages. Given the source and gold reference, the evaluators were asked to rate the adequacy and fluency of a system&apos;s output and oracle sentences on a scale of 1 to 5, as described in (Koehn and Monz, 2006) (see Table 1). The weighted average of the scores over all sentences was then calculated as: average_score = 5 ∑ s.f (s) Lang-pair hin-eng mar-eng ben-eng guj-eng kon-eng pan-eng urd-eng tam-eng tel-eng mal-eng Model PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT PBSMT HPBMT Original 22.77 23.87 12.6 14.65 16.24 14.69 17.66 15.96 15.56 13.67 19.98 20.12 17.31 19.05 10.54 10.3 12.63 11.9 8.3 8.46 Pruned 23.12 24.5 11.23 13.95 15.67 14.01 17.03 15.17 14.74 12.77 19.87 20.00 17.17 18.58 8.9 9.0 11.42 10.66 6.03 6.48 Oracle 36.74 37.36"
W15-5950,W09-0437,0,0.0200953,"ollowing is an outline of the paper. In Section 2, we describe related work. In the remainder of the paper, we estimate an upper bound on the potential gains in translation accuracy by postordering. Section 3 describes our method for computing oracle reorderings from the translation output, which is used to estimate the upper bound. Section 4 describes our experimental setup and 5 presents the results and discusses the observations. Section 6 concludes the paper and points out future work. 2 Related Work Oracle translations have been used by many researchers for diagnosing translation output. Auli et al. (2009) and Wisniewski and Yvon (2013) have used oracle translations to do reference reachability analysis and identify model and search errors. Wisniewski and Yvon (2013) have used the oracle translations to conduct various kinds of failure analysis and study effect of various search parameters. Dreyer et al. (2007), Li and Khudanpur (2009) and Wisniewski and Yvon (2013) use oracle translations to understand the limitations of various reordering constraints imposed on translation decoders. In the same spirit, we try to estimate an upper bound on the benefits of post-ordering the baseline SMT output."
W15-5950,2011.mtsummit-papers.35,0,0.0959409,"Missing"
W15-5950,P07-2045,0,0.00530632,". 4 Experimental Setup We studied different SMT systems from 10 Indian languages to English for quantifying the potential improvement in translation accuracy. The experiments were carried across 10 Indian languages included in the multilingual ILCI corpus (Jha, 2010), which contains nearly 50,000 parallel sentences . For each language pair, the corpus was split into 46,277 sentences for training, 500 sentences for tuning and 2000 sentences for testing. We trained phrase-based and hierarchical phrase-based systems on this data. The phrase-based systems were trained using the Moses SMT toolkit (Koehn et al., 2007) with the grow-diag-final-and heuristic for phrase extraction and the msd-bidirectional-fe model for lexicalized reordering. The trained models were tuned with Minimum Error Rate Training (MERT) (Och, 2003) with default parameters. We trained Score 1 2 3 4 5 Adequacy No meaning Little meaning Much meaning Most meaning All meaning Fluency Incomprehensible Disfluent Non-native fluency Good fluency Flawless fluency Table 1: Description of scores for manual evaluation 5-gram language models on the training-set using Kneser-Ney smoothing with SRILM (Stolcke and others, 2002). The hierarchical syste"
W15-5950,N09-2003,0,0.0237438,"nd. Section 4 describes our experimental setup and 5 presents the results and discusses the observations. Section 6 concludes the paper and points out future work. 2 Related Work Oracle translations have been used by many researchers for diagnosing translation output. Auli et al. (2009) and Wisniewski and Yvon (2013) have used oracle translations to do reference reachability analysis and identify model and search errors. Wisniewski and Yvon (2013) have used the oracle translations to conduct various kinds of failure analysis and study effect of various search parameters. Dreyer et al. (2007), Li and Khudanpur (2009) and Wisniewski and Yvon (2013) use oracle translations to understand the limitations of various reordering constraints imposed on translation decoders. In the same spirit, we try to estimate an upper bound on the benefits of post-ordering the baseline SMT output. Though we do not tackle the problem of postordering in this work, we summarize the existing work on post-ordering for SMT. There has been work on post-editing of machine translation output. The method described in (Simard et al., 2007) is most commonly used. It involves automatically post-editing the output of an MT system using anot"
W15-5950,D14-1133,0,0.123827,"nslations. Many solutions for reordering have been proposed to bridge this divergence. Word order divergence is generally handled within the core SMT model or using source-side reordering as a pre-processing step. In the core SMT system, word order can be tackled using a variety of models of varying complexity: word-level alignment models (Brown et al., 1993), lexicalized reordering models (Tillmann, 2004; Galley and Manning, 2008), hierarchical SMT (Chiang, 2005), syntax based SMT (Yamada and Knight, 2002), etc. SMT models like phrase-based SMT are not good at bridging word order divergence (Marie et al., 2014). 351 In these cases, source-side reordering is used as a preprocessing step to convert the source sentence to target language word order (Collins et al., 2005; Ramanathan et al., 2008). The best performing approaches generally rely on parse information on the source side to generate the correct word order. However, it has proved to be a very difficult problem which is far from being solved, especially when parse information is not forthcoming. The computational complexity of searching through a large space of potential reorderings and the need for incorporating higher level linguistic informa"
W15-5950,P03-1021,0,0.0628413,"included in the multilingual ILCI corpus (Jha, 2010), which contains nearly 50,000 parallel sentences . For each language pair, the corpus was split into 46,277 sentences for training, 500 sentences for tuning and 2000 sentences for testing. We trained phrase-based and hierarchical phrase-based systems on this data. The phrase-based systems were trained using the Moses SMT toolkit (Koehn et al., 2007) with the grow-diag-final-and heuristic for phrase extraction and the msd-bidirectional-fe model for lexicalized reordering. The trained models were tuned with Minimum Error Rate Training (MERT) (Och, 2003) with default parameters. We trained Score 1 2 3 4 5 Adequacy No meaning Little meaning Much meaning Most meaning All meaning Fluency Incomprehensible Disfluent Non-native fluency Good fluency Flawless fluency Table 1: Description of scores for manual evaluation 5-gram language models on the training-set using Kneser-Ney smoothing with SRILM (Stolcke and others, 2002). The hierarchical systems were also trained with Moses using default parameters. For three phrase-based SMT systems with Hindi, Marathi and Malayalam respectively as source and English as target language, qualitative analysis was"
W15-5950,P02-1040,0,0.0914798,"Missing"
W15-5950,I08-1067,1,0.831334,"rdering as a pre-processing step. In the core SMT system, word order can be tackled using a variety of models of varying complexity: word-level alignment models (Brown et al., 1993), lexicalized reordering models (Tillmann, 2004; Galley and Manning, 2008), hierarchical SMT (Chiang, 2005), syntax based SMT (Yamada and Knight, 2002), etc. SMT models like phrase-based SMT are not good at bridging word order divergence (Marie et al., 2014). 351 In these cases, source-side reordering is used as a preprocessing step to convert the source sentence to target language word order (Collins et al., 2005; Ramanathan et al., 2008). The best performing approaches generally rely on parse information on the source side to generate the correct word order. However, it has proved to be a very difficult problem which is far from being solved, especially when parse information is not forthcoming. The computational complexity of searching through a large space of potential reorderings and the need for incorporating higher level linguistic information are the primary challenges in tackling the reordering problem. While there is active research in preordering and in-decoder approaches, there has been little work on the problem of"
W15-5950,W07-0728,0,0.0339413,"s kinds of failure analysis and study effect of various search parameters. Dreyer et al. (2007), Li and Khudanpur (2009) and Wisniewski and Yvon (2013) use oracle translations to understand the limitations of various reordering constraints imposed on translation decoders. In the same spirit, we try to estimate an upper bound on the benefits of post-ordering the baseline SMT output. Though we do not tackle the problem of postordering in this work, we summarize the existing work on post-ordering for SMT. There has been work on post-editing of machine translation output. The method described in (Simard et al., 2007) is most commonly used. It involves automatically post-editing the output of an MT system using another phrase-based MT system trained on parallel data constructed from previously decoded output (e) and corresponding references (e&apos;). Béchara et al. (2011) improvizes on this approach by appending source words (f) to the output part of the parallel data (e), creating a new language (e&apos;#f) and retaining source context. Marie et al. (2014) use a second-pass decoder to improve translation quality. However, none of these works have focussed on word order and the effect on the word order has not been"
W15-5950,2011.mtsummit-papers.36,0,0.0337307,"of an MT system using another phrase-based MT system trained on parallel data constructed from previously decoded output (e) and corresponding references (e&apos;). Béchara et al. (2011) improvizes on this approach by appending source words (f) to the output part of the parallel data (e), creating a new language (e&apos;#f) and retaining source context. Marie et al. (2014) use a second-pass decoder to improve translation quality. However, none of these works have focussed on word order and the effect on the word order has not been explicitly evaluated. Post-ordering as a problem has been introduced by Sudoh et al. (2011). However, it is not a Source: दया या आहे त . णांसाठ ह इंजे शने एक वरदान स ध झाल Ref: these injections have proved to be a boon for heart patients . Output: this injection of heart patients are proved to be a boon for the . 0 Oracle: 1 2 3 5 4 7 6 8 9 10 11 12 13 this injection are proved to be a boon for heart patients . 5 re-ordered using alignments 6 7 8 9 10 11 re-ordered with heuristics 3 4 13 not aligned/used Figure 1: Construction of Oracle reordering post-ordering system in the sense in which we have defined it. Theirs is actually a two stage translation system that decomposes the tran"
W15-5950,N04-4026,0,0.0279876,"g effort in exploring the post-ordering problem. 1 Introduction Word order divergence is a central problem in Statistical Machine Translation (SMT) and major stumbling block to generating comprehensible translations. Many solutions for reordering have been proposed to bridge this divergence. Word order divergence is generally handled within the core SMT model or using source-side reordering as a pre-processing step. In the core SMT system, word order can be tackled using a variety of models of varying complexity: word-level alignment models (Brown et al., 1993), lexicalized reordering models (Tillmann, 2004; Galley and Manning, 2008), hierarchical SMT (Chiang, 2005), syntax based SMT (Yamada and Knight, 2002), etc. SMT models like phrase-based SMT are not good at bridging word order divergence (Marie et al., 2014). 351 In these cases, source-side reordering is used as a preprocessing step to convert the source sentence to target language word order (Collins et al., 2005; Ramanathan et al., 2008). The best performing approaches generally rely on parse information on the source side to generate the correct word order. However, it has proved to be a very difficult problem which is far from being so"
W15-5950,P02-1039,0,0.0636105,"tral problem in Statistical Machine Translation (SMT) and major stumbling block to generating comprehensible translations. Many solutions for reordering have been proposed to bridge this divergence. Word order divergence is generally handled within the core SMT model or using source-side reordering as a pre-processing step. In the core SMT system, word order can be tackled using a variety of models of varying complexity: word-level alignment models (Brown et al., 1993), lexicalized reordering models (Tillmann, 2004; Galley and Manning, 2008), hierarchical SMT (Chiang, 2005), syntax based SMT (Yamada and Knight, 2002), etc. SMT models like phrase-based SMT are not good at bridging word order divergence (Marie et al., 2014). 351 In these cases, source-side reordering is used as a preprocessing step to convert the source sentence to target language word order (Collins et al., 2005; Ramanathan et al., 2008). The best performing approaches generally rely on parse information on the source side to generate the correct word order. However, it has proved to be a very difficult problem which is far from being solved, especially when parse information is not forthcoming. The computational complexity of searching th"
W15-5950,jha-2010-tdil,0,\N,Missing
W15-5950,J93-2003,0,\N,Missing
W15-5950,P05-1033,0,\N,Missing
W16-4604,N12-1047,0,0.020397,"ge model motivated our approach of experimentation using NLM and NNJM as a feature in SMT. 2 System Description For our participation in WAT 2016 shared task for English ßà Indonesian language pair, we experimented with the following systems – 1. Phrase-Based SMT system : This was our baseline system for the WMT shared task. The standard Moses Toolkit (Koehn et al., 2007) was used with MGIZA++ (Gao and Vogel, 2008) for word alignment on training corpus followed by grow-diag-final-and symmetrization heuristics for extracting phrases and lexicalized reordering. Tuning was done using Batch MIRA (Cherry and Foster, 2012) with the default 60 passes over the data and –return-best-dev flag to get the highest scoring run into the final moses.ini file. A 5-gram language model using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing was trained. 2. Use of Neural Language Model : A neural probabilistic language model was trained and integrated as a feature for the phrase-based translation model. For this, the default NPLM4 implementation in Moses which is similar to the method described in Vaswani et al. (2013) was used. The goal was to examine if neural language models can improve the fluency for Indonesian-En"
W16-4604,P14-1129,0,0.162182,"rained and integrated as a feature for the phrase-based translation model. For this, the default NPLM4 implementation in Moses which is similar to the method described in Vaswani et al. (2013) was used. The goal was to examine if neural language models can improve the fluency for Indonesian-English translation and English-Indonesian translation by making use of distributed representations. We experimented with various word embedding sizes of 700, 750 and 800 for the first hidden layer in the network to get the optimal parameter while decoding. 3. Use of Bilingual Neural Joint Language Model : Devlin et al. (2014) have shown that including source side context information in the neural language model can lead to substantial improvement in translation quality. We experimented with Devlin&apos;s method which uses NPLM3 in the back-end to train a neural network joint language model (NNJM) using parallel data and integrated it as a feature for the phrase-based translation as implemented in Moses. A 5-gram language model augmented with 9 source context words and single hidden layer required for fast decoding was used as a parameter to train the joint model. 4. Use of Operational Sequence Model : Operation sequenc"
W16-4604,P11-1105,0,0.066572,"Missing"
W16-4604,N13-1001,0,0.0204767,"end to train a neural network joint language model (NNJM) using parallel data and integrated it as a feature for the phrase-based translation as implemented in Moses. A 5-gram language model augmented with 9 source context words and single hidden layer required for fast decoding was used as a parameter to train the joint model. 4. Use of Operational Sequence Model : Operation sequence model was trained as it integrates Ngram-based reordering and translation in a single generative process which can result in relatively improved translation over phrase based system. OSM approach as suggested in Durrani et al. (2013) considers both source and target information for generating a translation. It deals with minimum translation units i.e. words, along with context information of source and target sentence which spans across phrasal boundries. A 5-gram OSM was used for the experimentation here. 4 http://nlg.isi.edu/software/nplm/ 69 These 4 systems were trained for both directions of language pair and the test data was decoded and evaluated with BLEU points, RIBES scores, AMFM scores, Pairwise crowdsourcing scores and Adequacy scores for comparative performance evaluation. 3 Experimental Setup The data provide"
W16-4604,P13-2071,0,0.0164479,"end to train a neural network joint language model (NNJM) using parallel data and integrated it as a feature for the phrase-based translation as implemented in Moses. A 5-gram language model augmented with 9 source context words and single hidden layer required for fast decoding was used as a parameter to train the joint model. 4. Use of Operational Sequence Model : Operation sequence model was trained as it integrates Ngram-based reordering and translation in a single generative process which can result in relatively improved translation over phrase based system. OSM approach as suggested in Durrani et al. (2013) considers both source and target information for generating a translation. It deals with minimum translation units i.e. words, along with context information of source and target sentence which spans across phrasal boundries. A 5-gram OSM was used for the experimentation here. 4 http://nlg.isi.edu/software/nplm/ 69 These 4 systems were trained for both directions of language pair and the test data was decoded and evaluated with BLEU points, RIBES scores, AMFM scores, Pairwise crowdsourcing scores and Adequacy scores for comparative performance evaluation. 3 Experimental Setup The data provide"
W16-4604,W08-0509,0,0.0423785,"both the systems show that RNN model system outperforms SMT system with n-gram LM. The results of Hermanto et al.(2015) and various other research outcomes on different language pair using neural language model motivated our approach of experimentation using NLM and NNJM as a feature in SMT. 2 System Description For our participation in WAT 2016 shared task for English ßà Indonesian language pair, we experimented with the following systems – 1. Phrase-Based SMT system : This was our baseline system for the WMT shared task. The standard Moses Toolkit (Koehn et al., 2007) was used with MGIZA++ (Gao and Vogel, 2008) for word alignment on training corpus followed by grow-diag-final-and symmetrization heuristics for extracting phrases and lexicalized reordering. Tuning was done using Batch MIRA (Cherry and Foster, 2012) with the default 60 passes over the data and –return-best-dev flag to get the highest scoring run into the final moses.ini file. A 5-gram language model using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing was trained. 2. Use of Neural Language Model : A neural probabilistic language model was trained and integrated as a feature for the phrase-based translation model. For this, the"
W16-4604,P07-2045,0,0.00519506,"d on same data. The perplexity analysis of both the systems show that RNN model system outperforms SMT system with n-gram LM. The results of Hermanto et al.(2015) and various other research outcomes on different language pair using neural language model motivated our approach of experimentation using NLM and NNJM as a feature in SMT. 2 System Description For our participation in WAT 2016 shared task for English ßà Indonesian language pair, we experimented with the following systems – 1. Phrase-Based SMT system : This was our baseline system for the WMT shared task. The standard Moses Toolkit (Koehn et al., 2007) was used with MGIZA++ (Gao and Vogel, 2008) for word alignment on training corpus followed by grow-diag-final-and symmetrization heuristics for extracting phrases and lexicalized reordering. Tuning was done using Batch MIRA (Cherry and Foster, 2012) with the default 60 passes over the data and –return-best-dev flag to get the highest scoring run into the final moses.ini file. A 5-gram language model using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing was trained. 2. Use of Neural Language Model : A neural probabilistic language model was trained and integrated as a feature for the p"
W16-4604,W11-2124,0,0.0628648,"Missing"
W16-4604,C12-2104,0,0.0475729,"Missing"
W16-4604,D13-1140,0,0.0461239,"on heuristics for extracting phrases and lexicalized reordering. Tuning was done using Batch MIRA (Cherry and Foster, 2012) with the default 60 passes over the data and –return-best-dev flag to get the highest scoring run into the final moses.ini file. A 5-gram language model using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing was trained. 2. Use of Neural Language Model : A neural probabilistic language model was trained and integrated as a feature for the phrase-based translation model. For this, the default NPLM4 implementation in Moses which is similar to the method described in Vaswani et al. (2013) was used. The goal was to examine if neural language models can improve the fluency for Indonesian-English translation and English-Indonesian translation by making use of distributed representations. We experimented with various word embedding sizes of 700, 750 and 800 for the first hidden layer in the network to get the optimal parameter while decoding. 3. Use of Bilingual Neural Joint Language Model : Devlin et al. (2014) have shown that including source side context information in the neural language model can lead to substantial improvement in translation quality. We experimented with Dev"
W16-4811,N16-4006,1,0.498783,"increase in length is also impacted by the specific choice of data format for representing the sentences as subwords. In a phrase-based SMT framework, we investigate different choices of decoder parameters as well as data format and their impact on decoding time and translation accuracy. We suggest best options for these settings that significantly improve decoding time with little impact on the translation accuracy. 1 Introduction Related languages are those that exhibit lexical and structural similarities on account of sharing a common ancestry or being in contact for a long period of time (Bhattacharyya et al., 2016). Examples of languages related by common ancestry are Slavic and Indo-Aryan languages. Prolonged contact leads to convergence of linguistic properties even if the languages are not related by ancestry and could lead to the formation of linguistic areas (Thomason, 2000). Examples of such linguistic areas are the Indian subcontinent (Emeneau, 1956), Balkan (Trubetzkoy, 1928) and Standard Average European (Haspelmath, 2001) linguistic areas. Both forms of language relatedness lead to related languages sharing vocabulary and structural features. There is substantial government, commercial and cul"
W16-4811,N12-1047,0,0.0848712,"in our experiments. The OS is a linguistically motivated, variable length unit of translation, which consists of one or more consonants followed by a vowel (a C+ V unit). But our methodology is not specific to any subword unit. Hence, the results and observations should hold for other subword units also. We used the Indic NLP Library1 for orthographic syllabification. Phrase-based SMT systems were trained with OS as the basic unit. We used the Moses system (Koehn et al., 2007), with mgiza2 for alignment, the grow-diag-final-and heuristic for symmetrization of word alignments, and Batch MIRA (Cherry and Foster, 2012) for tuning. Since data sparsity is a lesser concern due to small vocabulary size and higher order n-grams are generally trained for translation using subword units (Vilar et al., 2007), we trained 10-gram language models. The language model was trained on the training split of the target language corpus. 1 2 http://anoopkunchukuttan.github.io/indic_nlp_library https://github.com/moses-smt/mgiza 84 Translation Accuracy Relative Decoding Time ben-hin hin-mal mal-hin tel-mal 33.10 11.68 19.86 9.39 Stack tl=10 tl=5 32.84 32.54 11.24 11.01 19.21 18.39 ss=50 ss=10 +tuning 33.10 33.04 32.83 11.69 11"
W16-4811,P14-2022,0,0.0138374,"le design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern´andez et al., 2016), efficient memory allocation (Hoang et al., 2016), alternative stack configurations (Hoang et al., 2016) and alternative decoding algorithms like cube pruning (Chiang, 2007). In this work, we have investigated stack decoding configurations and cube pruning as a way of optimizing decoder performance for the translation between related languages (with subword units and monotone decoding). Prior work on comparing stack decoding and cube-pruning has been limited to word-level models (Huang and Chiang, 2007; Heafield et al., 2014). 7 Conclusion and Future Work We systematically study the choice of data format for representing subword units in sentences and various decoder parameters which affect decoding time in a phrase-based SMT setting. Our studies (using OS and character as basic units) show that the use of cube-pruning during tuning as well as testing with a lower value of the stack pop limit parameter improves decoding time substantially with minimal change in translation quality. Two data formats, the space marker and the boundary marker, perform roughly equivalently in terms of translation accuracy as well as d"
W16-4811,W11-2123,0,0.0223512,"he phrase table by pruning, which actually improved translation quality for character level models. The authors have not reported the decoding speed, but it is possible that pruning may also improve decoding speed since fewer hypothesis may have to be looked up in the phrase table, and smaller phrase tables can be loaded into memory. There has been a lot of work looking at optimizing specific components of SMT decoders in a general setting. Hoang et al. (2016) provide a good overview of various approaches to optimizing decoders. Some of the prominent efforts include efficient language models (Heafield, 2011), lazy loading (Zens and Ney, 2007), phrase-table design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern´andez et al., 2016), efficient memory allocation (Hoang et al., 2016), alternative stack configurations (Hoang et al., 2016) and alternative decoding algorithms like cube pruning (Chiang, 2007). In this work, we have investigated stack decoding configurations and cube pruning as a way of optimizing decoder performance for the translation between related languages (with subword units and monotone decoding). Prior work on comparing stack decoding and cube-pruning has been limited"
W16-4811,2016.amta-researchers.4,0,0.011073,"l have shorter parallel segments, as parallel corpus for training subword-level models. Tiedemann and Nakov (2013) also investigated reducing the size of the phrase table by pruning, which actually improved translation quality for character level models. The authors have not reported the decoding speed, but it is possible that pruning may also improve decoding speed since fewer hypothesis may have to be looked up in the phrase table, and smaller phrase tables can be loaded into memory. There has been a lot of work looking at optimizing specific components of SMT decoders in a general setting. Hoang et al. (2016) provide a good overview of various approaches to optimizing decoders. Some of the prominent efforts include efficient language models (Heafield, 2011), lazy loading (Zens and Ney, 2007), phrase-table design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern´andez et al., 2016), efficient memory allocation (Hoang et al., 2016), alternative stack configurations (Hoang et al., 2016) and alternative decoding algorithms like cube pruning (Chiang, 2007). In this work, we have investigated stack decoding configurations and cube pruning as a way of optimizing decoder performance for the tra"
W16-4811,P07-1019,0,0.0479251,"d Ney, 2007), phrase-table design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern´andez et al., 2016), efficient memory allocation (Hoang et al., 2016), alternative stack configurations (Hoang et al., 2016) and alternative decoding algorithms like cube pruning (Chiang, 2007). In this work, we have investigated stack decoding configurations and cube pruning as a way of optimizing decoder performance for the translation between related languages (with subword units and monotone decoding). Prior work on comparing stack decoding and cube-pruning has been limited to word-level models (Huang and Chiang, 2007; Heafield et al., 2014). 7 Conclusion and Future Work We systematically study the choice of data format for representing subword units in sentences and various decoder parameters which affect decoding time in a phrase-based SMT setting. Our studies (using OS and character as basic units) show that the use of cube-pruning during tuning as well as testing with a lower value of the stack pop limit parameter improves decoding time substantially with minimal change in translation quality. Two data formats, the space marker and the boundary marker, perform roughly equivalently in terms of translati"
W16-4811,P07-2045,0,0.0108953,"As an example of subword level representation unit, we have studied the orthographic syllable (OS) (Kunchukuttan and Bhattacharyya, 2016) in our experiments. The OS is a linguistically motivated, variable length unit of translation, which consists of one or more consonants followed by a vowel (a C+ V unit). But our methodology is not specific to any subword unit. Hence, the results and observations should hold for other subword units also. We used the Indic NLP Library1 for orthographic syllabification. Phrase-based SMT systems were trained with OS as the basic unit. We used the Moses system (Koehn et al., 2007), with mgiza2 for alignment, the grow-diag-final-and heuristic for symmetrization of word alignments, and Batch MIRA (Cherry and Foster, 2012) for tuning. Since data sparsity is a lesser concern due to small vocabulary size and higher order n-grams are generally trained for translation using subword units (Vilar et al., 2007), we trained 10-gram language models. The language model was trained on the training split of the target language corpus. 1 2 http://anoopkunchukuttan.github.io/indic_nlp_library https://github.com/moses-smt/mgiza 84 Translation Accuracy Relative Decoding Time ben-hin hin-"
W16-4811,D16-1196,1,0.862751,"s share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Sub-word level transformations are an effective way for translation of such shared words. Using subwords as basic units of translation has been shown to be effective in improving translation quality with limited parallel corpora. Subword units like character (Vilar et al., 2007; Tiedemann, 2009a), character n-gram (Tiedemann and Nakov, 2013) and orthographic syllables (Kunchukuttan and Bhattacharyya, 2016) have been explored and have been shown to improve translation quality to varying degrees. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 82 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 82–88, Osaka, Japan, December 12 2016. Original Subword units Internal Marker Boundary Marker Space Marker this is an example of data formats for segmentation thi s i s a n e xa m p le o f da ta fo rma t s fo r se gme n ta tio n thi s i s a n e xa m p le o f da ta"
W16-4811,P12-2059,0,0.183394,"MT systems based on encoder decoder architectures, particularly without attention mechanism, are more sensitive to sentence length, so we presume that the boundary marker format may be more appropriate. 86 6 Related Work It has been recognized in the past literature on translation between related languages that the increased length of subword level translation is challenge for training as well as decoding (Vilar et al., 2007). Aligning long sentences is computationally expensive, hence most work has concentrated on corpora with short sentences (e.g. OPUS (Tiedemann, 2009b)) (Tiedemann, 2009a; Nakov and Tiedemann, 2012; Tiedemann, 2012). To make alignment feasible, Vilar et al. (2007) used the phrase table learnt from word-level alignment, which will have shorter parallel segments, as parallel corpus for training subword-level models. Tiedemann and Nakov (2013) also investigated reducing the size of the phrase table by pruning, which actually improved translation quality for character level models. The authors have not reported the decoding speed, but it is possible that pruning may also improve decoding speed since fewer hypothesis may have to be looked up in the phrase table, and smaller phrase tables can"
W16-4811,P02-1040,0,0.094949,"ed as a multiple of wordlevel decoding time. The following methods & parameters in Moses have been experimented with: (i) normal stack decoding - vary ss: stack-size, tt: table-limit; (ii) cube pruning: vary pl:cube-pruningpop-limit. +tuning indicates that the decoder settings mentioned on previous row were used for tuning too. Translation accuracy and decode time per sentence for word-level decoding (in milliseconds) is shown on the last line for comparison. The PBSMT systems were trained and decoded on a server with Intel Xeon processors (2.5 GHz) and 256 GB RAM. 3.3 Evaluation We use BLEU (Papineni et al., 2002) for evaluating translation accuracy. We use the sum of user and system time minus the time for loading the phrase table (all reported by Moses) to determine the time taken for decoding the test set. 4 Effect of decoder parameters We observed that the decoding time for OS-level models is approximately 70 times of the word-level model. This explosion in the decoding time makes translation highly compute intensive and difficult to perform in real-time. It also makes tuning MT systems very slow since tuning typically requires multiple decoding runs over the tuning set. Hence, we experimented with"
W16-4811,P16-1162,0,0.0192064,"which are illustrated in Table 1: • Boundary Marker: The subword at the boundary of a word is augmented with a marker character. There is one boundary subword, either the first or the last chosen as per convention. Such a representation has been used in previous work, mostly related to morpheme level representation. • Internal Marker: Every subword internal to the word is augmented with a marker character. This representation has been used rarely, one example being the Byte Code Encoding representation used by University of Edinburgh’s Neural Machine Translation system (Williams et al., 2016; Sennrich et al., 2016). • Space Marker: The subword units are not altered, but inter-word boundary is represented by a space marker. Most work on translation between related languages has used this format. 83 For boundary and internal markers, the addition of the marker character does not change the sentence length, but can create two representations for some subwords (corresponding to internal and boundary positions), thus introducing some data sparsity. On the other hand, space marker doubles the sentence length (in terms in words), but each subword has a unique representation. 2.3 Decoder Parameters Given the ba"
W16-4811,R13-1088,0,0.697482,"l corpora. Lexical similarity implies that the languages share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Sub-word level transformations are an effective way for translation of such shared words. Using subwords as basic units of translation has been shown to be effective in improving translation quality with limited parallel corpora. Subword units like character (Vilar et al., 2007; Tiedemann, 2009a), character n-gram (Tiedemann and Nakov, 2013) and orthographic syllables (Kunchukuttan and Bhattacharyya, 2016) have been explored and have been shown to improve translation quality to varying degrees. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 82 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 82–88, Osaka, Japan, December 12 2016. Original Subword units Internal Marker Boundary Marker Space Marker this is an example of data formats for segmentation thi s i s a n e xa m p le o f da ta fo r"
W16-4811,2009.eamt-1.3,0,0.647772,"lity SMT systems with limited parallel corpora. Lexical similarity implies that the languages share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Sub-word level transformations are an effective way for translation of such shared words. Using subwords as basic units of translation has been shown to be effective in improving translation quality with limited parallel corpora. Subword units like character (Vilar et al., 2007; Tiedemann, 2009a), character n-gram (Tiedemann and Nakov, 2013) and orthographic syllables (Kunchukuttan and Bhattacharyya, 2016) have been explored and have been shown to improve translation quality to varying degrees. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 82 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 82–88, Osaka, Japan, December 12 2016. Original Subword units Internal Marker Boundary Marker Space Marker this is an example of data formats for segme"
W16-4811,E12-1015,0,0.349268,"based on encoder decoder architectures, particularly without attention mechanism, are more sensitive to sentence length, so we presume that the boundary marker format may be more appropriate. 86 6 Related Work It has been recognized in the past literature on translation between related languages that the increased length of subword level translation is challenge for training as well as decoding (Vilar et al., 2007). Aligning long sentences is computationally expensive, hence most work has concentrated on corpora with short sentences (e.g. OPUS (Tiedemann, 2009b)) (Tiedemann, 2009a; Nakov and Tiedemann, 2012; Tiedemann, 2012). To make alignment feasible, Vilar et al. (2007) used the phrase table learnt from word-level alignment, which will have shorter parallel segments, as parallel corpus for training subword-level models. Tiedemann and Nakov (2013) also investigated reducing the size of the phrase table by pruning, which actually improved translation quality for character level models. The authors have not reported the decoding speed, but it is possible that pruning may also improve decoding speed since fewer hypothesis may have to be looked up in the phrase table, and smaller phrase tables can"
W16-4811,W07-0705,0,0.703327,"to building good-quality SMT systems with limited parallel corpora. Lexical similarity implies that the languages share many words with the similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Sub-word level transformations are an effective way for translation of such shared words. Using subwords as basic units of translation has been shown to be effective in improving translation quality with limited parallel corpora. Subword units like character (Vilar et al., 2007; Tiedemann, 2009a), character n-gram (Tiedemann and Nakov, 2013) and orthographic syllables (Kunchukuttan and Bhattacharyya, 2016) have been explored and have been shown to improve translation quality to varying degrees. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 82 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 82–88, Osaka, Japan, December 12 2016. Original Subword units Internal Marker Boundary Marker Space Marker this is an example of data"
W16-4811,W16-2327,0,0.0150808,"ts for representation, which are illustrated in Table 1: • Boundary Marker: The subword at the boundary of a word is augmented with a marker character. There is one boundary subword, either the first or the last chosen as per convention. Such a representation has been used in previous work, mostly related to morpheme level representation. • Internal Marker: Every subword internal to the word is augmented with a marker character. This representation has been used rarely, one example being the Byte Code Encoding representation used by University of Edinburgh’s Neural Machine Translation system (Williams et al., 2016; Sennrich et al., 2016). • Space Marker: The subword units are not altered, but inter-word boundary is represented by a space marker. Most work on translation between related languages has used this format. 83 For boundary and internal markers, the addition of the marker character does not change the sentence length, but can create two representations for some subwords (corresponding to internal and boundary positions), thus introducing some data sparsity. On the other hand, space marker doubles the sentence length (in terms in words), but each subword has a unique representation. 2.3 Decoder"
W16-4811,N07-1062,0,0.0127205,"ch actually improved translation quality for character level models. The authors have not reported the decoding speed, but it is possible that pruning may also improve decoding speed since fewer hypothesis may have to be looked up in the phrase table, and smaller phrase tables can be loaded into memory. There has been a lot of work looking at optimizing specific components of SMT decoders in a general setting. Hoang et al. (2016) provide a good overview of various approaches to optimizing decoders. Some of the prominent efforts include efficient language models (Heafield, 2011), lazy loading (Zens and Ney, 2007), phrase-table design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern´andez et al., 2016), efficient memory allocation (Hoang et al., 2016), alternative stack configurations (Hoang et al., 2016) and alternative decoding algorithms like cube pruning (Chiang, 2007). In this work, we have investigated stack decoding configurations and cube pruning as a way of optimizing decoder performance for the translation between related languages (with subword units and monotone decoding). Prior work on comparing stack decoding and cube-pruning has been limited to word-level models (Huang and Ch"
W16-4811,jha-2010-tdil,0,\N,Missing
W16-4811,J07-2003,0,\N,Missing
W17-4102,N16-4006,1,0.865643,"Missing"
W17-4102,N12-1047,0,0.0409206,"ize of the orthographic syllable encoded corpus. Since we could not do orthographic syllabification for Urdu, Korean and Japanese, we selected the merge operations as follows: For Urdu, number of merge operations were selected based on Hindi OS vocabulary since Hindi and Urdu are registers of the same language. For Korean and Japanese, the number of BPE merge operations was set to 3000, discovered by tuning on a separate validation set. We trained phrase-based SMT systems using the Moses system (Koehn et al., 2007), with the growdiag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character, OS and BPE-unit level models. Subword level representation of sentences is long, hence we speed up decoding by using cube pruning with a smaller beam size (pop-limit=1000). This setting has been shown to have minimal impact on translation quality (Kunchukuttan and Bhattacharyya, 2016a). We used unsupervised morphologicalsegmenters for generating morpheme representations (trained using Morfessor (Smit et al., 2014)). For Indian languages, we used th"
W17-4102,W16-4811,1,0.943419,"n with subword level basic units. Character-level SMT has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013). Unigram-level learning provides very little context for learning translation models (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). These results were demonstrated primarily for very close European languages. Kunchukuttan and Bhattacharyya (2016b) proposed orthographic syllables, a linguistically-motivated variable-length unit, which approximates a syllable. This unit has outperformed character n-gram, word and morpheme level models as well as transliteration post-editing approaches mentioned earlier. They also showed orthographic syllables can outperform other units even when: (i) the lexical distance between related languages is reasonably large, (ii) the languages do not have a genetic relation, but only a contact relation. Recently, subword level models have also genThe paper is organized as follows. Section 2 discusses related w"
W17-4102,D15-1249,0,0.0725485,"Section 5 reports the results of our experiments and analyses the results. Based on experimental results, we analyse why BPE units out15 erated interest for neural machine translation (NMT) systems. The motivation is the need to limit the vocabulary of neural MT systems in encoder-decoder architectures (Sutskever et al., 2014). It is in this context that Byte Pair Encoding, a data compression method (Gage, 1994), was adapted to learn subword units for NMT (Sennrich et al., 2016). Other subword units for NMT have also been proposed: character (Chung et al., 2016), Huffman encoding based units (Chitnis and DeNero, 2015), wordpieces (Schuster and Nakajima, 2012; Wu et al., 2016). Our hypothesis is that such subword units learnt from corpora are particularly suited for translation between related languages. In this paper, we test this hypothesis by using BPE to learn subword units. lable is a sequence of one or more consonants followed by a vowel, i.e. a C+ V unit, which approximates a linguistic syllable (e.g. spacious would be segmented as spa ciou s). Orthographic syllabification is rule based and applies to writing systems which represent vowels (alphabets and abugidas). Both OS and BPE units are variable"
W17-4102,D16-1196,1,0.71549,"n with subword level basic units. Character-level SMT has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013). Unigram-level learning provides very little context for learning translation models (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). These results were demonstrated primarily for very close European languages. Kunchukuttan and Bhattacharyya (2016b) proposed orthographic syllables, a linguistically-motivated variable-length unit, which approximates a syllable. This unit has outperformed character n-gram, word and morpheme level models as well as transliteration post-editing approaches mentioned earlier. They also showed orthographic syllables can outperform other units even when: (i) the lexical distance between related languages is reasonably large, (ii) the languages do not have a genetic relation, but only a contact relation. Recently, subword level models have also genThe paper is organized as follows. Section 2 discusses related w"
W17-4102,P16-1160,0,0.0314708,"dels. Section 4 describes our experimental set-up. Section 5 reports the results of our experiments and analyses the results. Based on experimental results, we analyse why BPE units out15 erated interest for neural machine translation (NMT) systems. The motivation is the need to limit the vocabulary of neural MT systems in encoder-decoder architectures (Sutskever et al., 2014). It is in this context that Byte Pair Encoding, a data compression method (Gage, 1994), was adapted to learn subword units for NMT (Sennrich et al., 2016). Other subword units for NMT have also been proposed: character (Chung et al., 2016), Huffman encoding based units (Chitnis and DeNero, 2015), wordpieces (Schuster and Nakajima, 2012; Wu et al., 2016). Our hypothesis is that such subword units learnt from corpora are particularly suited for translation between related languages. In this paper, we test this hypothesis by using BPE to learn subword units. lable is a sequence of one or more consonants followed by a vowel, i.e. a C+ V unit, which approximates a linguistic syllable (e.g. spacious would be segmented as spa ciou s). Orthographic syllabification is rule based and applies to writing systems which represent vowels (alp"
W17-4102,N15-3017,1,0.828649,"m,tel-mal, hin-mal,mal-hin urd-hin,ben-urd urd-mal,mal-urd bul-mac dan-swe may-ind kor-jpn,jpn-kor train tune test 44,777 1000 2000 38,162 843 1707 150k 150k 137k 1000 1000 1000 2000 2000 2000 69,809 1000 2000 Library1 (Kunchukuttan et al., 2014). We used orthographic syllabification rules from the Indic NLP Library for Indian languages, and custom rules for Latin and Slavic scripts. For training BPE models, we used the subword-nmt2 library. We used Juman3 and Mecab4 for Japanese and Korean tokenization respectively. For mapping characters across Indic scripts, we used the method described by Kunchukuttan et al. (2015) and implemented in the Indic NLP Library. (a) Parallel Corpora Size (no. of sentences) Language hin (Bojar et al., 2014) tam (Ramasamy et al., 2012) mal (Quasthoff et al., 2006) mac (Tiedemann, 2009b) Size Language 4.4 10M urd (Jawaid et al., 2014) 5M 1M mar (news websites) 1.8M 200K swe (OpenSubtitles2016) 2.4M 680K ind (Tiedemann, 2009b) 640K (b) Details of additional monolingual corpora for training word-level language models (source and size in number of sentences) Table 2: Training Corpus Statistics 5 for other pairs were obtained from the OpenSubtitles2016 section of the OPUS corpus col"
W17-4102,N10-1077,0,0.0157459,"re dialects/registers of the same language and exhibit high lexical similarity. At the other end, pairs like Hindi-Malayalam belong to different language families, but show many lexical and grammatical similarities due to contact for a long time (Subbarao, 2012). The chosen languages cover 5 types of writing systems. Of these, alphabetic and abugida writing systems represent vowels, logographic writing systems do not have vowels. The use of vowels is optional in abjad writing systems and depends on various factors and conventions. For instance, Urdu word segmentation can be very inconsistent (Durrani and Hussain, 2010) and generally short vowels are not denoted. The Korean Hangul writing system is syllabic, so the vowels are implicitly represented in the characters. (a) List of languages used in experiments along with ISO 6393 codes. These codes are used in the paper. Language Family Dravidian Type of writing system mal,tam,tel hin,urd,ben Indo-Aryan kok,mar,pan Slavic bul,mac Germanic dan,swe Polynesian may,ind Altaic jpn,kor dan1 ,swe1 ,may1 ind1 ,buc2 ,mac2 mal,tam,tel,hin Abugida ben,kok,mar,pan Syllabic kor Logographic jpn Abjad urd Alphabet (b) Classification of the languages and writing systems. (i)"
W17-4102,P10-1048,0,0.16581,"ranslation between related languages that leverage lexical similarity between source and target languages. The first approach involves transliteration of source words into the target languages. This can done by transliterating the untranslated words in a post-processing step (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014), a technique generally used for handling named entities in SMT. However, transliteration candidates cannot be scored and tuned along with other features used in the SMT system. This limitation can be overcome by integrating the transliteration module into the decoder (Durrani et al., 2010), so both translation and transliteration candidates can be evaluated and scored simultaneously. This also allows transliteration vs. translation choices to be made. Since a high degree of similarity exists at the subword level between related languages, the second approach looks at translation with subword level basic units. Character-level SMT has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013). Unigram-level learning provides very little context"
W17-4102,W95-0115,0,0.011889,"cation of Japanese and Korean into the Altaic family is debated, but various lexical and grammatical similarities are indisputable, either due to genetic or cognate relationship (Robbeets, 2005; Vovin, 2010). However, the source of lexical similarity is immaterial to the current work. For want of a better classification, we use the name Altaic to indicate relatedness between Japanese and Korean. The chosen language pairs also exhibit varying levels of lexical similarity. Table 3 shows an indication of the lexical similarity between them in terms of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1995). The LCSR has been computed over the parallel training sentences at character level (shown only for language pairs where the writing systems are the same or can be easily mapped in order to do the LCSR computation). At one end of the spectrum, MalayalamIndia, Urdu-Hindi, Macedonian-Bulgarian are dialects/registers of the same language and exhibit high lexical similarity. At the other end, pairs like Hindi-Malayalam belong to different language families, but show many lexical and grammatical similarities due to contact for a long time (Subbarao, 2012). The chosen languages cover 5 types of wri"
W17-4102,P12-2059,0,0.275814,"e, this is the largest experiment for translation over related languages and the broad coverage strongly supports our results. • We also show BPE units outperform other translation units in a cross-domain translation task. 2 Related Work There are two broad set of approaches that have been explored in the literature for translation between related languages that leverage lexical similarity between source and target languages. The first approach involves transliteration of source words into the target languages. This can done by transliterating the untranslated words in a post-processing step (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014), a technique generally used for handling named entities in SMT. However, transliteration candidates cannot be scored and tuned along with other features used in the SMT system. This limitation can be overcome by integrating the transliteration module into the decoder (Durrani et al., 2010), so both translation and transliteration candidates can be evaluated and scored simultaneously. This also allows transliteration vs. translation choices to be made. Since a high degree of similarity exists at the subword level between related languages, the second approach looks"
W17-4102,P02-1040,0,0.0981059,"ual corpora for training word-level language models (source and size in number of sentences) Table 2: Training Corpus Statistics 5 for other pairs were obtained from the OpenSubtitles2016 section of the OPUS corpus collection (Tiedemann, 2009b). Language models for wordlevel systems were trained on the target side of training corpora plus additional monolingual corpora from various sources (See Table 2b for details). We used just the target language side of the parallel corpora for character, morpheme, OS and BPE-unit level LMs. 4.3 Evaluation The primary evaluation metric is word-level BLEU (Papineni et al., 2002). We also report LeBLEU (Virpioja and Gr¨onroos, 2015) scores as an alternative evaluation metric. LeBLEU is a variant of BLEU that does an edit-distance based, soft-matching of words and has been shown to be better for morphologically rich languages. We used bootstrap resampling for testing statistical significance (Koehn, 2004). Size Results and Analysis This section describes the results of various experiments and analyses them. A comparison of BPE with other units across languages and writing systems, choice of number of merge operations and effect of domain change and training data size a"
W17-4102,quasthoff-etal-2006-corpus,0,0.110024,"Missing"
W17-4102,W12-5611,0,0.116087,"Missing"
W17-4102,W04-3250,0,0.0474643,"additional monolingual corpora from various sources (See Table 2b for details). We used just the target language side of the parallel corpora for character, morpheme, OS and BPE-unit level LMs. 4.3 Evaluation The primary evaluation metric is word-level BLEU (Papineni et al., 2002). We also report LeBLEU (Virpioja and Gr¨onroos, 2015) scores as an alternative evaluation metric. LeBLEU is a variant of BLEU that does an edit-distance based, soft-matching of words and has been shown to be better for morphologically rich languages. We used bootstrap resampling for testing statistical significance (Koehn, 2004). Size Results and Analysis This section describes the results of various experiments and analyses them. A comparison of BPE with other units across languages and writing systems, choice of number of merge operations and effect of domain change and training data size are studied. We also report initial results with a joint bilingual BPE model. 5.1 System details Comparison of BPE with other units Table 3 shows translation accuracies of all the language pairs under experimentation for different translation units, in terms of BLEU as well as LeBLEU scores. The number of BPE merge operations was"
W17-4102,P07-2045,0,0.00790685,"erations was chosen such that the resultant vocabulary size would be equivalent to the vocabulary size of the orthographic syllable encoded corpus. Since we could not do orthographic syllabification for Urdu, Korean and Japanese, we selected the merge operations as follows: For Urdu, number of merge operations were selected based on Hindi OS vocabulary since Hindi and Urdu are registers of the same language. For Korean and Japanese, the number of BPE merge operations was set to 3000, discovered by tuning on a separate validation set. We trained phrase-based SMT systems using the Moses system (Koehn et al., 2007), with the growdiag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for character, OS and BPE-unit level models. Subword level representation of sentences is long, hence we speed up decoding by using cube pruning with a smaller beam size (pop-limit=1000). This setting has been shown to have minimal impact on translation quality (Kunchukuttan and Bhattacharyya, 2016a). We used unsupervised morphologicalsegmenters for generating"
W17-4102,P16-1162,0,0.66759,"stems given the lack of parallel corpora. Modelling lexical similarity among related languages is the key to building good-quality SMT systems with limited parallel corpora. Lexical similarity implies related languages share many words with similar form (spelling/pronunciation) and meaning e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. These words could be cognates, lateral borrowings or loan words from other languages. Subword level transformations are an effective way for translation of such shared words. In this work, we propose use of Byte Pair Encoding (BPE) (Gage, 1994; Sennrich et al., 2016), a encoding method inspired from text compression literature, to learn basic translation units for translation between related languages. In previous work, the basic units of translation are either linguistically motivated (word, morpheme, syllable, etc.) or ad-hoc choices (character n-gram). In contrast, BPE is motivated by statistical properties of text. We explore the use of segments learnt using Byte Pair Encoding (referred to as BPE units) as basic units for statistical machine translation between related languages and compare it with orthographic syllables, which are currently the best"
W17-4102,E14-2006,0,0.16106,"Missing"
W17-4102,2009.eamt-1.3,0,0.241829,"can be overcome by integrating the transliteration module into the decoder (Durrani et al., 2010), so both translation and transliteration candidates can be evaluated and scored simultaneously. This also allows transliteration vs. translation choices to be made. Since a high degree of similarity exists at the subword level between related languages, the second approach looks at translation with subword level basic units. Character-level SMT has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013). Unigram-level learning provides very little context for learning translation models (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). These results were demonstrated primarily for very close European languages. Kunchukuttan and Bhattacharyya (2016b) proposed orthographic syllables, a linguistically-motivated variable-length unit, which approximates a syllable. This unit has outperformed character n-gram, word and morpheme level mo"
W17-4102,E12-1015,0,0.599155,"the largest experiment for translation over related languages and the broad coverage strongly supports our results. • We also show BPE units outperform other translation units in a cross-domain translation task. 2 Related Work There are two broad set of approaches that have been explored in the literature for translation between related languages that leverage lexical similarity between source and target languages. The first approach involves transliteration of source words into the target languages. This can done by transliterating the untranslated words in a post-processing step (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014), a technique generally used for handling named entities in SMT. However, transliteration candidates cannot be scored and tuned along with other features used in the SMT system. This limitation can be overcome by integrating the transliteration module into the decoder (Durrani et al., 2010), so both translation and transliteration candidates can be evaluated and scored simultaneously. This also allows transliteration vs. translation choices to be made. Since a high degree of similarity exists at the subword level between related languages, the second approach looks"
W17-4102,R13-1088,0,0.0162884,"y integrating the transliteration module into the decoder (Durrani et al., 2010), so both translation and transliteration candidates can be evaluated and scored simultaneously. This also allows transliteration vs. translation choices to be made. Since a high degree of similarity exists at the subword level between related languages, the second approach looks at translation with subword level basic units. Character-level SMT has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013). Unigram-level learning provides very little context for learning translation models (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). These results were demonstrated primarily for very close European languages. Kunchukuttan and Bhattacharyya (2016b) proposed orthographic syllables, a linguistically-motivated variable-length unit, which approximates a syllable. This unit has outperformed character n-gram, word and morpheme level models as well as transliterati"
W17-4102,W07-0705,0,0.481373,"tem. This limitation can be overcome by integrating the transliteration module into the decoder (Durrani et al., 2010), so both translation and transliteration candidates can be evaluated and scored simultaneously. This also allows transliteration vs. translation choices to be made. Since a high degree of similarity exists at the subword level between related languages, the second approach looks at translation with subword level basic units. Character-level SMT has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013). Unigram-level learning provides very little context for learning translation models (Tiedemann, 2012). The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013). These results were demonstrated primarily for very close European languages. Kunchukuttan and Bhattacharyya (2016b) proposed orthographic syllables, a linguistically-motivated variable-length unit, which approximates a syllable. This unit has outperformed character n-gram, word and"
W17-4102,W15-3052,0,0.222921,"Missing"
W17-4102,D12-1027,0,0.0501317,"Missing"
W17-5717,P02-1040,0,0.132838,"Missing"
W17-5717,P15-2116,0,0.0486369,"Missing"
Y18-3001,Y18-3013,1,0.888887,"Missing"
Y18-3001,Y18-3003,1,0.889659,"Missing"
Y18-3001,Y18-3005,0,0.0459044,"Missing"
Y18-3001,P17-4012,0,0.0426364,". 3 each participant’s system. That is, the specific baseline system was the standard for human evaluation. At WAT 2018, we adopted a neural machine translation (NMT) with attention mechanism as a baseline system except for the IITB tasks. We used a phrasebased statistical machine translation (SMT) system, which is the same system as that at WAT 2017, as the baseline system for the IITB tasks. The NMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page.5 We used OpenNMT (Klein et al., 2017) as the implementation of the baseline NMT systems. In addition to the NMT baseline systems, we have SMT baseline systems for the tasks that started at last year or before last year. The baseline systems are shown in Tables 8, 9, and 10. SMT baseline systems are described in the previous WAT overview paper (Nakazawa et al., 2017). The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online tra"
Y18-3001,Y18-3002,0,0.0375601,"Missing"
Y18-3001,P07-2045,0,0.0120426,"in frequency = 1 The default values were used for the other system parameters. For many to one, one to many, and many to many multilingual NMT (Johnson et al., 2017), we add &lt;2XX> tags, which indicate the target language (XX is replaced by the language code), to the head of the source language sentences. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.11 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2018 web page.12 All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model13 and MeC"
Y18-3001,W04-3250,0,0.312277,"Missing"
Y18-3001,Y18-3007,0,0.0326187,"Missing"
Y18-3001,Y18-3014,0,0.0304859,"Missing"
Y18-3001,W14-7001,1,0.458489,"ion (WAT2018) including Ja↔En, Ja↔Zh scientific paper translation subtasks, Zh↔Ja, K↔Ja, En↔Ja patent translation subtasks, Hi↔En, My↔En mixed domain subtasks and Bn/Hi/Ml/Ta/Te/Ur/Si↔En Indic languages multilingual subtasks. For the WAT2018, 17 teams participated in the shared tasks. About 500 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014-WAT2017 (Nakazawa et al., 2014; Nakazawa et al., 2015; Nakazawa et al., 2016; Nakazawa et al., 2017), WAT2018 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We have been working toward practical use of machine translation among all Asian countries. For the 5th WAT, we adopted new translation subtasks with Myanmar ↔ EnSadao Kurohashi Kyoto University kuro@i.kyoto-u.ac.jp glish mixed domain corpus1 and Bengali/Hindi/Malayalam/Tamil/Telugu/Urdu/Sinhalese ↔ English OpenSubtitles corpus2 in addition to the subtasks at WAT2017. WAT is the uniq"
Y18-3001,W16-4601,1,0.938773,"Missing"
Y18-3001,P11-2093,0,0.0504246,"leu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.11 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2018 web page.12 All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model13 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0.14 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.15 For Korean segmentation, we 11 http://www.kecl.ntt.co.jp/icl/lirg/ ribes/index.html 12 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2018/ 13 http://www.phontron.com/kytea/model. html 14 http://code.google.com/p/mecab/ downloads/detail?name=mecab-ipadic-2.7. 0-20070801.tar.gz 15 http://nlp.stanford.ed"
Y18-3001,Y18-3011,0,0.141475,"Missing"
Y18-3001,P02-1040,0,0.119424,"://bitbucket.org/anoopk/indic_nlp_ library 10 https://github.com/rsennrich/ subword-nmt • tgt vocab size = 100000 • src words min frequency = 1 • tgt words min frequency = 1 The default values were used for the other system parameters. For many to one, one to many, and many to many multilingual NMT (Johnson et al., 2017), we add &lt;2XX> tags, which indicate the target language (XX is replaced by the language code), to the head of the source language sentences. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.11 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2018 web page.12 All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanes"
Y18-3001,Y18-3010,0,0.0585011,"Missing"
Y18-3001,Y18-3012,0,0.0444067,"Missing"
Y18-3001,2007.mtsummit-papers.63,0,0.0425147,"itute of Information and Communications Technology (NICT). The corpus consists of a Japanese-English scientific paper abstract corpus (ASPEC-JE), which is used for ja↔en subtasks, and a Japanese-Chinese scientific paper excerpt corpus (ASPEC-JC), which is used for ja↔zh subtasks. The statistics for each corpus are shown in Table 1. 2.1.1 ASPEC-JE The training data for ASPEC-JE was constructed by NICT from approximately two million JapaneseEnglish scientific paper abstracts owned by JST. The data is a comparable corpus and sentence correspondences are found automatically using the method from (Utiyama and Isahara, 2007). Each sentence pair is accompanied by a similarity score that are calculated by the method and a field ID that indicates a scientific field. The correspondence between field IDs and field names, along with the frequency and occurrence ratios for the training data, are described in the README file of ASPEC-JE. The development, development-test and test data were extracted from parallel sentences from the Japanese-English paper abstracts that exclude the sentences in the training data. Each dataset consists of 400 documents and contains sentences in each field at the same rate. The document ali"
Y18-3001,Y18-3017,0,0.0591037,"Missing"
Y18-3001,Y18-3006,1,0.868302,"Missing"
Y18-3003,D14-1179,0,0.016896,"Missing"
Y18-3003,P17-2061,1,0.937207,"ish-Japanese translation, UCSY MyanmarEnglish translation and Indic multilingual translation directions. The techniques we focused on for each translation task can be summarized as below: • For the ASPEC translation tasks, we mostly relied on multilingual Transformer (Vaswani et al., 2017) models and experimented with Recurrently Stacked NMT (RS-NMT) (Dabre and Fujita, 2018) models in order to determine the trade-off between compactness of models and the loss in their performance. • For the UCSY Myanmar-English translation task, we tried domain adaptation techniques such as Mixed Fine Tuning (Chu et al., 2017) since the the final objective was to achieve high quality translation for a low-resource domain (ALT). • For the Indic multilingual task, we explored the feasibility of bilingual, N -to-1, 1-to-N and N to-N way translation models. We also tried an approach where we mapped the scripts of all Indic languages to a common script (Devanagari) to see if it helps improve the performance of a multilingual model. For additional details of how our submissions are ranked relative to the submissions of other WAT 952 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on"
Y18-3003,P07-2045,0,0.00542266,"perience a large loss in translation quality despite having significantly fewer parameters compared to the vanilla NMT models. An interesting observation is that systems with the best BLEU might not be the best in terms of human evaluation. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although vanilla NMT is significantly better than PBSMT in resource-rich scenarios, PBSMT performs better in resource-poor scenarios (Zoph et Anoop Kunchukuttan Microsoft AI and Research, India ankunchu@microsoft.com Eiichiro Sumita NICT, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan eiichiro.sumita@nict.go.jp al., 2016). By exploiting transfer learning techniques, the performance of NMT approaches can be improved substantially. For WAT 2018, we participated as team “NICT5” and worked on ASPEC Chinese-Japanese and English-Japanese translation, UCSY MyanmarEnglish translation an"
Y18-3003,N15-3017,1,0.843788,"ingle model to translate from English to all the Indic languages. This is essentially the reverse of the XX-En model. We also trained this model for 500k iterations. gle model to translate from all the Indic languages to English and vice versa. Unlike the previous multilingual models, we trained this model only for 180k iterations due to lack of time. • Multilingual Shared Indic Script XX-En model: This model is similar to the XX-En model except that the scripts for all the Indic languages are mapped to a common script. We used Devanagari as the common script, and used the Indic NLP Library5 (Kunchukuttan et al., 2015) for script conversion. As such, this increases the chance of vocabulary sharing. Because the training corpus diversity is significantly reduced we trained this model for 100k iterations because it is technically equivalent 5 • Multilingual XX-YY model: We trained a sinhttps://github.com/anoopkunchukuttan/ indic_nlp_library/ 957 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 Task Bengali-English Bengali-English Bengali-English Bengali-English Hindi-Englis"
Y18-3003,Y18-3001,1,0.838941,"d the feasibility of bilingual, N -to-1, 1-to-N and N to-N way translation models. We also tried an approach where we mapped the scripts of all Indic languages to a common script (Devanagari) to see if it helps improve the performance of a multilingual model. For additional details of how our submissions are ranked relative to the submissions of other WAT 952 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 participants, kindly refer to the overview paper (Nakazawa et al., 2018). 2 NMT Models and Approaches We will first describe the Transformer which is the state-of-the-art NMT model we used for our experiments. 2.1 The Transformer The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a sequence-tosequence neural model that consists of two components, the encoder and the decoder. The encoder converts the input word sequence into a sequence of vectors of high dimensionality. The decoder, on the other hand, produces the target word sequence by predicting the words using a combination of the previously predicted word and relevant p"
Y18-3003,D16-1163,0,0.0754938,"Missing"
Y18-3013,N16-4006,1,0.807804,"lugu and Malayalam belong to the Dravidian language family. These are the major language families, in addition to the Austro-Asiatic and TibetoBurman languages spoken by a small section of the population. In addition to the similarities between languages belonging to the same language families, there are many similarities between the four language families on account of contact over a long period of time. Hence, India is referred to as a linguistic area (Emeneau, 1956). This relatedness manifests itself in the form of lexical, structural and morphological similarities between these languages (Bhattacharyya et al., 2016). In the WAT 2018 shared task (Nakazawa et al., 2018), we participated as team ‘Anuvaad’ and trained Indic− →English and English− →Indic baseline SMT systems along with our proposed many-to-one Indic− →English system. In neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), multilingual transfer learning approaches (includes many-to-one, oneto-many, or many-to-many translation) have shown significant improvement in translation quality with minimal increase in network complexity, especially in the case of resource-poor languages (Johnson et al., 201"
Y18-3013,P15-1166,0,0.0284883,"d task (Nakazawa et al., 2018), we participated as team ‘Anuvaad’ and trained Indic− →English and English− →Indic baseline SMT systems along with our proposed many-to-one Indic− →English system. In neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), multilingual transfer learning approaches (includes many-to-one, oneto-many, or many-to-many translation) have shown significant improvement in translation quality with minimal increase in network complexity, especially in the case of resource-poor languages (Johnson et al., 2017; Firat et al., 2016; Dong et al., 2015). However, SMT is still superior, when the training corpus is not big enough (Koehn and Knowles, 2017). Hence, we experimented with a multilingual manyto-one SMT system for Indic language to English translation using significantly less amount of data as compared to NMT. 2 Many-to-one SMT Despite, the huge success of NMT, SMT can still be used to achieve comparable transnational outcome in case of data scarcity (Koehn and Knowles, 2017). In order to train a SMT system, the learning of language model and translation model requires lesser amount of data as compared to learning of NMT system (Koeh"
Y18-3013,E14-4029,0,0.017003,"stics of the data split. . 3.1 Pre-processing We process the corpus through appropriate filters for normalization, tokenization and truecasing using the scripts available in Moses (Koehn et al., 2007) and the Indic NLP Library1 . Further, the training sentence length was limited to 50 words. Following the above preprocessing steps, we generated corpora of all Indian languages transliterated in Devanagari script using the BrahmiNet transliteration system (Kunchukuttan et al., 2015), which is 1 https://github.com/anoopkunchukuttan/ indic_nlp_library based on the transliteration module in Moses (Durrani et al., 2014). This data was used to train our many-to-one SMT system. 4 Models trained For all our experiments, we trained the models using the Moses implementation (Koehn et al., 2007) with 3-gram language model and using the growdiag-final-and heuristic for extracting phrases. We trained two types of SMT systems, baseline SMT system and many-to-one SMT system. 14 baseline SMT systems were trained using 7 parallel corpora as shown in Table 2. In order to train the multilingual SMT system, first, we merged all these corpora into a single bilingual corpus, wherein, sentences of Indic languages were transli"
Y18-3013,N16-1101,0,0.0214297,"n the WAT 2018 shared task (Nakazawa et al., 2018), we participated as team ‘Anuvaad’ and trained Indic− →English and English− →Indic baseline SMT systems along with our proposed many-to-one Indic− →English system. In neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), multilingual transfer learning approaches (includes many-to-one, oneto-many, or many-to-many translation) have shown significant improvement in translation quality with minimal increase in network complexity, especially in the case of resource-poor languages (Johnson et al., 2017; Firat et al., 2016; Dong et al., 2015). However, SMT is still superior, when the training corpus is not big enough (Koehn and Knowles, 2017). Hence, we experimented with a multilingual manyto-one SMT system for Indic language to English translation using significantly less amount of data as compared to NMT. 2 Many-to-one SMT Despite, the huge success of NMT, SMT can still be used to achieve comparable transnational outcome in case of data scarcity (Koehn and Knowles, 2017). In order to train a SMT system, the learning of language model and translation model requires lesser amount of data as compared to learning"
Y18-3013,W17-3204,0,0.0275908,"d English− →Indic baseline SMT systems along with our proposed many-to-one Indic− →English system. In neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), multilingual transfer learning approaches (includes many-to-one, oneto-many, or many-to-many translation) have shown significant improvement in translation quality with minimal increase in network complexity, especially in the case of resource-poor languages (Johnson et al., 2017; Firat et al., 2016; Dong et al., 2015). However, SMT is still superior, when the training corpus is not big enough (Koehn and Knowles, 2017). Hence, we experimented with a multilingual manyto-one SMT system for Indic language to English translation using significantly less amount of data as compared to NMT. 2 Many-to-one SMT Despite, the huge success of NMT, SMT can still be used to achieve comparable transnational outcome in case of data scarcity (Koehn and Knowles, 2017). In order to train a SMT system, the learning of language model and translation model requires lesser amount of data as compared to learning of NMT system (Koehn and Knowles, 2017). Mathematically, SMT is represented aseˆ = arg max(P (e|f )) = arg max(P (e).P (f"
Y18-3013,P07-2045,0,0.0217185,"Indic Languages Multilingual Parallel Corpus comprised of OpenSubtitles domain data provided by WAT 2018 organizers for shared task experiment. The data-set contains parallel corpora of 7 Indic languages as mentioned above along with their English translation. We did not use any monolingual corpus for this experiment. We extracted the data in individual files for training, tuning and testing. Table 1 shows the statistics of the data split. . 3.1 Pre-processing We process the corpus through appropriate filters for normalization, tokenization and truecasing using the scripts available in Moses (Koehn et al., 2007) and the Indic NLP Library1 . Further, the training sentence length was limited to 50 words. Following the above preprocessing steps, we generated corpora of all Indian languages transliterated in Devanagari script using the BrahmiNet transliteration system (Kunchukuttan et al., 2015), which is 1 https://github.com/anoopkunchukuttan/ indic_nlp_library based on the transliteration module in Moses (Durrani et al., 2014). This data was used to train our many-to-one SMT system. 4 Models trained For all our experiments, we trained the models using the Moses implementation (Koehn et al., 2007) with"
Y18-3013,D16-1196,1,0.872086,"Missing"
Y18-3013,N15-3017,1,0.919933,"ong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 is a sentence of foreign language. An SMT system selects the best translated English sentence e given a foreign sentence f. The argmax computation is expressed as a product of language model P(e) and translation model P(f |e). It produces the English sentence which exhibits highest probability value for a given sentence f. In many-to-one SMT model, the translation model needs to be trained on merged corpus of all Indic− →English language-pair, wherein, all bilingual corpora are transliterated into a certain script-pair (Kunchukuttan et al., 2015). Further, the language model is trained on the merged form of the target language corpus. 3 Language-pairs bn-en hi-en ml-en ta-en te-en ur-en si-en Train Dev Test 337,428 84,557 359,423 26,217 22,165 26,619 521,726 500 500 500 500 500 500 500 1000 1000 1000 1000 1000 1000 1000 Table 1: Indic-English corpora (bn- Bengali, hi- Hindi, ml- Malayalam, ta- Tamil, te- Telugu, ur- Urdu, si- Sinhalese individually paired with en-English) split statistics. The number indicates the number of sentences in the split (train, dev an test). Experimental Setup We trained 14 bilingual SMT systems for 7 IndicE"
Y18-3013,P02-1040,0,0.102723,"- en− →bn en− →hi en− →ml en− →ta en− →te en− →ur en− →si 11.34 26.49 14.23 15.87 21.02 21.62 11.71 0.601570 0.692385 0.422574 0.668548 0.728584 0.628279 0.580957 0.532680 0.657180 0.567090 0.756890 0.744230 0.534550 0.535950 11.000 73.750 - - - - - Table 2: Translation accuracies of baseline and many-to-one SMT systems. bn- Bengali, hi- Hindi, ml- Malayalam, ta- Tamil, te- Telugu, ur- Urdu, si- Sinhalese individually paired with en-English. 6 Result and Discussion Table 2 shows translation accuracies of baseline PBSMT and many-to-one PBSMT in terms of Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002), Rank-based Intuitive Bilingual Evaluation Score (RIBES) (Group and others, 2013), Adequacy-Fluency Metrics (AMFM) (Banchs et al., 2015) and human evaluation score (HUMAN) (only for hi-en and ta-en translation models). From the results of our experiment, we did not get any discernible improvement in translation quality by using many-to-one PBSMT compared to the baseline PBSMT systems. The many-to-one SMT approach shows minor improvement in BLEU scores for 3 Indic languages namely Malayalam, Urdu and Sindhi, and minor degradation in BLEU scores for 2 Indic languages Bengali and Tamil. A notice"
