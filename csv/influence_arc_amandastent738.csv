2007.sigdial-1.29,C00-1027,0,0.322094,"adaptation is sometimes called entrainment or audience design (Brennan and Clark, 1996; Horton and Gerrig, 2002). • recency adaptation – adaptation due to the representations of words, concepts etc. being activated, or brought to the forefront during language production, by previous perception or comprehension. This type of adaptation is sometimes called convergence, priming or alignment (Brown and Dell, 1987; Pickering and Garrod, 2004; Chartrand and Bargh, 1999). In this paper, we consider measures used in corpus-based studies of adaptation such as (Dubey et al., 2006; Reitter et al., 2006; Church, 2000). These measures do not permit examination of whether adaptation is due to the partner or to recency, and do 166 Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue, pages 166–173, c Antwerp, September 2007. 2007 Association for Computational Linguistics not measure the strength of adaptation. We propose two new measures, one that measures the presence of adaptation and another that measures its strength. Together, these measures can identify adaptation within a single document or between documents; can identify the strength of adaptation as well as its presence; and can be used"
2007.sigdial-1.29,W06-1405,0,0.0601818,"Missing"
2007.sigdial-1.29,N06-2031,0,0.172366,", conversational partners adapt to each other’s choice of words, particularly referring expressions (Brennan and Clark, 1996), converge on certain syntactic choices (Pickering et al., 2000; C. Lockridge, 2002), adapt their prosody to help their partners disambiguate syntactic ambiguities (Kraljic and Brennan, 2005), and also adapt using audiovisual information (Kraut et al., 2003). Some of these results have been duplicated using corpus studies; for example, researchers have found evidence of within-speaker and betweenspeaker convergence to certain syntactic constructions (Dubey et al., 2006; Reitter et al., 2006). Corpus studies can be a good addition to more tightly • partner adaptation – adaptation based on a model of the partner. This type of adaptation is sometimes called entrainment or audience design (Brennan and Clark, 1996; Horton and Gerrig, 2002). • recency adaptation – adaptation due to the representations of words, concepts etc. being activated, or brought to the forefront during language production, by previous perception or comprehension. This type of adaptation is sometimes called convergence, priming or alignment (Brown and Dell, 1987; Pickering and Garrod, 2004; Chartrand and Bargh, 1"
2007.sigdial-1.29,N07-4008,1,0.79719,"cy 3.71 3.82 1.30 1.62* 5.49* 4.99 Table 9: Adaptation ratio and adaptation strength averaged over significant features listed in Tables 3, 4 and 5. * indicates a significant difference between partner and recency adaptation (p&lt;.05) Taking a different cut-off may influence the result. P. Brown and G. Dell. 1987. Adapting production to comprehension: The explicit mention of instruments. Cognitive Psychology, 19:441–472. We hope to address these issues in future work. In current work, we are incorporating models of adaptation to syntactic and lexical choice into our RavenCalendar dialog system (Stenchikova et al., 2007). We are creating a tight integration between parsing, dialog management and response generation so that words and syntactic constructions used by the user can be highly salient for the system, and ones used by the system are available for interpretation of user utterances (cf. (Isard et al., 2006)). In experiments with this system, we plan to use our adaptation measures to evaluate user adaptation to system behavior for different system adaptation rates. S. Brennan C. Lockridge. 2002. Addressees’ needs influence speakers’ early syntactic choices. Psychonomics Bulletin and Review. Psych. Refer"
2007.sigdial-1.29,H05-1104,0,\N,Missing
2020.aacl-main.71,D15-1075,0,0.070922,"ontradictory) Table 1: Example illustrating context (c) - hypothesis (h) pairs for the task of textual entailment. Textual entailment (TE) is the task of determining if a hypothesis sentence can be inferred from a given context sentence. Figure 1 shows examples of context-hypothesis pairs for TE. Previous works (Wang and Zhang, 2009; Tatu and Moldovan, 2005; Sammons et al., 2010) investigated several semantic approaches for TE and demonstrated how they can be used to evaluate inference-related tasks such as QuesResearchers have curated many resources3 and benchmark datasets for TE in English (Bowman et al., 2015; Williams et al., 2018; Khot et al., 2018). However, to our knowledge, there is only one TE dataset (XNLI) in Hindi, which was created by translating English data (Conneau et al., 2018) and another in HindiEnglish code-switched setting (Khanuja et al., 2020). Hindi is the language with the fourth most native speakers in the world4 . Despite its wide prevalence, Hindi is still considered a low-resource language by NLP practitioners because there are a rather limited number of publicly available annotated datasets. Developing models that can accurately process text from low-resource languages,"
2020.aacl-main.71,P19-4007,0,0.172184,"for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 706–719 c December 4 - 7, 2020. 2020 Association for Computational Linguistics tated sentence with each of the template hypotheses to create TE samples. Unlike XNLI, our dataset is based on the original Hindi text and is not translated. Furthermore, the multiple annotation artefacts (Tan et al., 2019) present in the original classification data are leveled out for the Textual entailment task on the recasted data due to label balance 5 . We evaluated state-of-the-art language models (Conneau et al., 2019) performance on the recasted TE data. We then combine the predictions of related pairs (same premise) from TE task to predict the classification labels of the original data (premise sentence), a twostep classification. We observed that a better TE performance on the recasted data leads to higher accuracy on the followed classification task. We also observed that TE models can make inconsistent predictions across samples derived from the same context sentence. Driven by these observations, we propose two improvements to TE and classification modeling. First, we introduce a regularisation constr"
2020.aacl-main.71,D17-1070,0,0.0302848,"tive not only ensures that the model predictions are correct but also ensures that they are correct for the right reasons. The true classification label can be retrieved from the entailment vector only when the model draws necessary inferences correctly. Otherwise the multi-class classification would fail. Furthermore, combining the joint objective (Equation 2) with consistency regulariser (Equation 1) for the intermediate TE prediction further force pairwise-consistency between prediction of related TE pairs. Baselines - For evaluating our approach, we use the following baselines: InferSent (Conneau et al., 2017), Sent2Vec (Pagliardini et al., 2018), Bag-of-words (BoW) and XLMRoBERTa (Conneau et al., 2019) which is state-of-the-art for multilingual language modelling. Also, we evaluate a hypothesis-only analogue for each one of them as well. For experiments with recasted data, we use embeddings of context-hypothesis pair for baselines whereas for the hypothesis-only (Poliak et al., 2018b) models, we only use embeddings of the hypothesis sentence, keeping it blind to the context. 711 Hypothesis only Baselines - Evaluating hypothesis-only models is motivated by irregularities and biases presented in ent"
2020.aacl-main.71,2020.calcs-1.2,0,0.0221868,"ples of context-hypothesis pairs for TE. Previous works (Wang and Zhang, 2009; Tatu and Moldovan, 2005; Sammons et al., 2010) investigated several semantic approaches for TE and demonstrated how they can be used to evaluate inference-related tasks such as QuesResearchers have curated many resources3 and benchmark datasets for TE in English (Bowman et al., 2015; Williams et al., 2018; Khot et al., 2018). However, to our knowledge, there is only one TE dataset (XNLI) in Hindi, which was created by translating English data (Conneau et al., 2018) and another in HindiEnglish code-switched setting (Khanuja et al., 2020). Hindi is the language with the fourth most native speakers in the world4 . Despite its wide prevalence, Hindi is still considered a low-resource language by NLP practitioners because there are a rather limited number of publicly available annotated datasets. Developing models that can accurately process text from low-resource languages, such as Hindi, is critical for the proliferation and broader adoption of NLP technologies. Creating a high-quality labeled corpus for TE in Hindi through crowd-sourcing could be challenging. In this paper, we employ a recasting technique from Poliak et al. (2"
2020.aacl-main.71,W18-3504,1,0.827641,"or resides in spoken dialect than texts. There have been recent efforts using curriculum learning for making pretrained language models for several multi-lingual tasks (Conneau et al., 2018, 2019). However, many such languages give rise to creoles, building new mixed languages at the interface of existing languages. One such example is Hinglish (Hindi + English) that has widely been taken over in the form of tweets and social media messages. Attempts have been made to study linguistic tasks like language identification, NER (Singh et al., 2018) and detection of hate speech from social media (Mathur et al., 2018). (Sitaram et al., 2019) looks at the challenges and opportunities of code-switching. Joshi et al. (2019) compares the current deep learning methods for classification tasks in Hindi and concludes the need of more efficient models for the same. Apart from that, low-resource languages also challenge us to shift from data-driven modelling to intelligent neural modelling. This improves language understanding from limited available data and also diminishes the need of hand-engineered feature representations similar to generative modelling. Some such efforts have been put forth by Kumar et al. (201"
2020.aacl-main.71,2020.lrec-1.149,1,0.72254,"marked as ‘non-entailed’. This process is summarized with an example in Figure 1. For more detailed recasting illustration, see Appendix Section A.1 Figure 5. BHAAV - The second dataset BHAAV (BH ) (Kumar et al., 2019) contains 20,304 sentences from Hindi short stories annotated for one of the following five emotion categories: joy, anger, suspense, sad, and neutral. We used a similar process as PR to recast BH using the following templates to create the hypothesis: ‘It is a matter of great &lt;label&gt;’ and ‘It is not a matter of great &lt;label&gt;’. Hindi Discourse Modes Dataset (HDA) - This dataset (Dhanwal et al., 2020) consists of 10, 472 sentences from Hindi short stories annotated for five different discourse modes argumentative, narrative, descriptive, dialogic and informative. Recasting Classification Datasets One of the main challenges for TE evaluation for low-resource languages is the lack of labeled data. In this work, we employ recasting to convert annotated classification datasets in Hindi to labeled TE samples. As in (Poliak et al., 2018a), we selected four different datasets for recasting thus introducing linguistic diversity in the resulting TE dataset. Product Review - The first dataset (PR) c"
2020.aacl-main.71,N18-1049,0,0.0150654,"del predictions are correct but also ensures that they are correct for the right reasons. The true classification label can be retrieved from the entailment vector only when the model draws necessary inferences correctly. Otherwise the multi-class classification would fail. Furthermore, combining the joint objective (Equation 2) with consistency regulariser (Equation 1) for the intermediate TE prediction further force pairwise-consistency between prediction of related TE pairs. Baselines - For evaluating our approach, we use the following baselines: InferSent (Conneau et al., 2017), Sent2Vec (Pagliardini et al., 2018), Bag-of-words (BoW) and XLMRoBERTa (Conneau et al., 2019) which is state-of-the-art for multilingual language modelling. Also, we evaluate a hypothesis-only analogue for each one of them as well. For experiments with recasted data, we use embeddings of context-hypothesis pair for baselines whereas for the hypothesis-only (Poliak et al., 2018b) models, we only use embeddings of the hypothesis sentence, keeping it blind to the context. 711 Hypothesis only Baselines - Evaluating hypothesis-only models is motivated by irregularities and biases presented in entailment Context (Hindi): वह रोया जब उ"
2020.aacl-main.71,2020.acl-main.560,0,0.0207709,"revolved around English language. Our approach is motivated by such studies to analyse NLU using current embeddings for low-resource languages like Hindi. Bhattacharyya (2012) discusses some of the key challenges associated with Hindi, for example, grammatical constraints for most words to be masculine/feminine (similar to French and unlike English), which makes 707 semantic tasks like pronoun resolution, paraphrasing tough. 2.2 NLP for Low-Resource Languages In a plethora of diverse languages, only a handful of them have plenty of labeled resources for data-driven analysis and advancements (Joshi et al., 2020). Data in low-resource languages is either unlabeled or resides in spoken dialect than texts. There have been recent efforts using curriculum learning for making pretrained language models for several multi-lingual tasks (Conneau et al., 2018, 2019). However, many such languages give rise to creoles, building new mixed languages at the interface of existing languages. One such example is Hinglish (Hindi + English) that has widely been taken over in the form of tweets and social media messages. Attempts have been made to study linguistic tasks like language identification, NER (Singh et al., 20"
2020.aacl-main.71,H05-1047,0,0.0616048,"together. We therefore highlight the benefits of data recasting and our approach 2 with supporting experimental results. Context-Hypothesis p : The kid exclaimed with joy. h : The kid is happy. p : I am feeling happy. h : I am angry. Label entailed not-entailed (contradictory) Table 1: Example illustrating context (c) - hypothesis (h) pairs for the task of textual entailment. Textual entailment (TE) is the task of determining if a hypothesis sentence can be inferred from a given context sentence. Figure 1 shows examples of context-hypothesis pairs for TE. Previous works (Wang and Zhang, 2009; Tatu and Moldovan, 2005; Sammons et al., 2010) investigated several semantic approaches for TE and demonstrated how they can be used to evaluate inference-related tasks such as QuesResearchers have curated many resources3 and benchmark datasets for TE in English (Bowman et al., 2015; Williams et al., 2018; Khot et al., 2018). However, to our knowledge, there is only one TE dataset (XNLI) in Hindi, which was created by translating English data (Conneau et al., 2018) and another in HindiEnglish code-switched setting (Khanuja et al., 2020). Hindi is the language with the fourth most native speakers in the world4 . Desp"
2020.aacl-main.71,D09-1082,0,0.0387386,"tual entailment tasks together. We therefore highlight the benefits of data recasting and our approach 2 with supporting experimental results. Context-Hypothesis p : The kid exclaimed with joy. h : The kid is happy. p : I am feeling happy. h : I am angry. Label entailed not-entailed (contradictory) Table 1: Example illustrating context (c) - hypothesis (h) pairs for the task of textual entailment. Textual entailment (TE) is the task of determining if a hypothesis sentence can be inferred from a given context sentence. Figure 1 shows examples of context-hypothesis pairs for TE. Previous works (Wang and Zhang, 2009; Tatu and Moldovan, 2005; Sammons et al., 2010) investigated several semantic approaches for TE and demonstrated how they can be used to evaluate inference-related tasks such as QuesResearchers have curated many resources3 and benchmark datasets for TE in English (Bowman et al., 2015; Williams et al., 2018; Khot et al., 2018). However, to our knowledge, there is only one TE dataset (XNLI) in Hindi, which was created by translating English data (Conneau et al., 2018) and another in HindiEnglish code-switched setting (Khanuja et al., 2020). Hindi is the language with the fourth most native spea"
2020.aacl-main.71,I17-1100,0,0.0509973,"Missing"
2020.aacl-main.71,N18-1101,0,0.0257341,": Example illustrating context (c) - hypothesis (h) pairs for the task of textual entailment. Textual entailment (TE) is the task of determining if a hypothesis sentence can be inferred from a given context sentence. Figure 1 shows examples of context-hypothesis pairs for TE. Previous works (Wang and Zhang, 2009; Tatu and Moldovan, 2005; Sammons et al., 2010) investigated several semantic approaches for TE and demonstrated how they can be used to evaluate inference-related tasks such as QuesResearchers have curated many resources3 and benchmark datasets for TE in English (Bowman et al., 2015; Williams et al., 2018; Khot et al., 2018). However, to our knowledge, there is only one TE dataset (XNLI) in Hindi, which was created by translating English data (Conneau et al., 2018) and another in HindiEnglish code-switched setting (Khanuja et al., 2020). Hindi is the language with the fourth most native speakers in the world4 . Despite its wide prevalence, Hindi is still considered a low-resource language by NLP practitioners because there are a rather limited number of publicly available annotated datasets. Developing models that can accurately process text from low-resource languages, such as Hindi, is criti"
2020.emnlp-main.645,S17-2091,0,0.0245806,"to further study the use of GANs for the task of keyphrase generation. 1 Introduction Keyphrases capture the most salient topics of a document and are often indexed in databases to help with search and information retrieval techniques. Researchers tag their scientific publications with high-quality keyphrases to ensure discoverability in scientific repositories. Automatic identification of keyphrases is of great interest to the scientific community as it helps to recommend relevant articles, suggest missing citations to authors, identify potential peer reviewers, and analyze research trends (Augenstein et al., 2017). Keyphrases could either be extractive (part of the document) or abstractive (not part of the document). Some prior works have referred to them as present and absent keyphrases, respectively. Keyphrase generation is the process of predicting both extractive and abstractive keyphrases from a given document. Most of the previous works in keyphrase domain, including both supervised and unsupervised techniques, primarily focus on extractive keyphrases (Hasan and Ng, 2014; Mahata et al., 2018; Sahrawat et al., 2020). Recent studies Meng et al. (2017); Ye and Wang (2018); Chan et al. (2019) have st"
2020.emnlp-main.645,N19-1070,0,0.0525614,"Missing"
2020.lrec-1.149,J08-4004,0,0.265856,"भी उसे अब फर से नई जान-पहचान करनी पड़ेगी। Maybe it was clear to him that his daughter must have grown up now, and that he would have to reacquaint with her again. The 53 stories contained 10,472 sentences and all sentences were annotated by the three annotators. We evaluated the annotations in terms of inter-annotator agreements using Krippendorff’s alpha (K-alpha) (Krippendorff, 2011) which is more robust than simple agreement measures because it accounts for chance correction and class distributions. We observed strong inter-annotator agreements (K-alpha of 0.87) and per recommendations in (Artstein and Poesio, 2008) conclude that the annotations are of good quality. We chose a straightforward majority decision for label aggregation: if two or more annotators agreed on a discourse mode for a sentence. In cases where there was no agreement between the annotators, they met in person to discuss and assign the final label. 2.4. Dataset statistics Figure 1 shows a distribution of the number of sentences for each discourse mode. There is fair amount of class imbalance in this domain with the most prevalent class Descriptive having 3,954 samples, and the two low prevalence classes (Informative and Argumentative)"
2020.lrec-1.149,Q17-1010,0,0.0206365,"Missing"
2020.lrec-1.149,K18-2017,0,0.0221198,"Missing"
2020.lrec-1.149,W07-1428,0,0.0452706,"t they have already talked about, change of topic and relationship between states, events, beliefs etc, in a given mode of communication (Webber et al., 2012). The discourse structures could be present in the form of a single sentence or span across multiple sentences. Understanding discourse structures and relationships between them in text could be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from writt"
2020.lrec-1.149,J81-2001,0,0.470376,"ut, indicating relationship to what they have already talked about, change of topic and relationship between states, events, beliefs etc, in a given mode of communication (Webber et al., 2012). The discourse structures could be present in the form of a single sentence or span across multiple sentences. Understanding discourse structures and relationships between them in text could be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify diff"
2020.lrec-1.149,P14-2047,1,0.805639,"nd relationship between states, events, beliefs etc, in a given mode of communication (Webber et al., 2012). The discourse structures could be present in the form of a single sentence or span across multiple sentences. Understanding discourse structures and relationships between them in text could be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 of"
2020.lrec-1.149,W16-3617,1,0.84438,"und Discourse in the context of linguistics is defined as exploitation of language features by speakers to express what they are talking about, indicating relationship to what they have already talked about, change of topic and relationship between states, events, beliefs etc, in a given mode of communication (Webber et al., 2012). The discourse structures could be present in the form of a single sentence or span across multiple sentences. Understanding discourse structures and relationships between them in text could be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Racha"
2020.lrec-1.149,W09-3029,0,0.0379755,"ration (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spoken languages in the world1 . In spite of its wide usage it is still considered as one of the low resource languages by NLP practitioners, which necessitates the creation of new resources and tools for computational linguists that enables them to understand the under1 https://en.wikipedia.org/wiki/List_of_ languages_by_number_of_native_speakers lying nuances of the language using natural l"
2020.lrec-1.149,prasad-etal-2008-penn,0,0.104598,"mode of communication (Webber et al., 2012). The discourse structures could be present in the form of a single sentence or span across multiple sentences. Understanding discourse structures and relationships between them in text could be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spo"
2020.lrec-1.149,W11-0414,0,0.0355923,"2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spoken languages in the world1 . In spite of its wide usage it is still considered as one of the low resource languages by NLP practitioners, which necessitates the creation of new resources and tools for computational linguists that enables them to understand the under1 https://en.wikipedia.org/wiki/List_of_ languages_by_number_of_"
2020.lrec-1.149,P17-1011,0,0.0249855,"of these short stories were originally written in Hindi but some of them were written in other Indian languages and later translated to Hindi. We chose against crowd-sourcing the annotation process because we wanted to directly work with the annotators for qualitative feedback and to also ensure high quality annotations. We employed three native Hindi speakers with college level education for the annotation task. We first selected two random stories from our corpus and had the three annotators work on them independently and classify each sentence based on the discourse mode taxonomy used in (Song et al., 2017). Song et al (Song et al., 2017) developed their taxonomy based on prior works in linguistics (Smith, 2003). This preliminary task helped the annotators familiarize themselves with discourse modes and also understand the scope of this annotation task. More importantly, this also helped us ascertain feedback about the class labels. Based on the annotators’ feedback we first observed that of five discourse modes used in (Song et al., 2017), Emotion was extremely prevalent: most of the sentences in these short stories could be associated with some sort of an emotion. We therefore decided to elimi"
2020.lrec-1.149,tonelli-etal-2010-annotation,0,0.0196186,"as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spoken languages in the world1 . In spite of its wide usage it is still considered as one of the low resource languages by NLP practitioners, which necessitates the creation of new resources and tools for computational linguists that enables them to understand the under1 https://en.wikipedia.org/"
2020.lrec-1.149,W16-6307,0,0.0226833,"w resources and tools for computational linguists that enables them to understand the under1 https://en.wikipedia.org/wiki/List_of_ languages_by_number_of_native_speakers lying nuances of the language using natural language processing techniques. The syntax and semantics of Hindi is often different from other high resource languages like English. Dependency of the meaning of expressions on word order, morphological variations, and spelling variations makes Hindi an interesting language to study and also pose additional challenges for linguistic modelling (Kumar et al., 2019). Tripathi et al. (Tripathi et al., 2016), created a Hindi corpus of 1960 sentences extracted from children stories that are annotated with discourse modes for improving storytelling experience using TTS systems. They annotated an already existing story speech corpus (Sarkar et al., 2014) for three discourse modes - dialogue, narrative and descriptive. The main motivation of their work was to develop an automated discourse mode identification system at sentence level that could be further used for enhancing the output of a TTS system by improving the performance of the prosody models. Motivated by the efforts of (Tripathi et al., 201"
2020.lrec-1.149,W10-1844,0,0.0371131,"al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spoken languages in the world1 . In spite of its wide usage it is still considered as one of the low resource languages by NLP practitioners, which necessitates the creation of new resources and tools for computational linguists that enables them to understand the under1 https://en.wikipedia.org/wiki/List_of_ languages_by_number_of_native_speakers lying nuances o"
2020.lrec-1.149,P12-1008,0,0.027183,"ld be useful for many natural language processing tasks such as summarization (Li et al., 2016), question answering (Verberne et al., 2007), natural language generation (Williams and Reiter, 2003), anaphora resolution (Hirst, 1981), textual entailment (Hickl and Bensley, 2007), and machine translation (Li et al., 2014). With the release of Penn Discourse Treebank (Prasad et al., 2008), there has been an increasing interest from the scientific community to study discourse relations holding between eventualities in text in different languages such as Arabic (Al-Saif and Markert, 2010), Chinese (Zhou and Xue, 2012), Czech (Mladová et al., 2008), Italian (Tonelli et al., 2010), Tamil (Rachakonda and Sharma, 2011), Turkish (Zeyrek et al., 2010), and Hindi (Oza et al., 2009). Very few studies exists that aims to identify different discourse modes (Smith, 2003) from written text. Hindi language is one of the 22 official languages of India and is among the top five most widely spoken languages in the world1 . In spite of its wide usage it is still considered as one of the low resource languages by NLP practitioners, which necessitates the creation of new resources and tools for computational linguists that e"
E09-1012,W05-0613,0,0.0191088,"ing (Soricut and Marcu, 2003), hierarchical agglomerative clustering (Sporleder and Lascarides, 2004), parsing from lexicalized tree-adjoining grammars (Cristea, 2000), and rulebased approaches that use rhetorical relations and discourse cues (Forbes et al., 2003; Polanyi et al., 2004; LeThanh et al., 2004). With the exception of Cristea (2000), most of this research has been limited to non-incremental parsing of textual monologues where, in contrast to incremental dialog parsing, predicting a system action is not relevant. The work on discourse parsing that is most similar to ours is that of Baldridge and Lascarides (2005). They used a probabilistic headdriven parsing method (described in (Collins, 2003)) to construct rhetorical structure trees for a spoken dialog corpus. However, their parser was Proceedings of the 12th Conference of the European Chapter of the ACL, pages 94–102, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 94 Order Placement Dialog Opening Task Task Contact Info Order Item Shipping Info Topic/Subtask DialogAct,Pred!Args Utterance Topic/Subtask DialogAct,Pred!Args Utterance Payment Info Summary Closing Task Delivery Info Topic/Subtask Figure 2: Samp"
E09-1012,P06-1026,1,0.865703,"Missing"
E09-1012,J98-4001,0,0.298606,"stics 94 Order Placement Dialog Opening Task Task Contact Info Order Item Shipping Info Topic/Subtask DialogAct,Pred!Args Utterance Topic/Subtask DialogAct,Pred!Args Utterance Payment Info Summary Closing Task Delivery Info Topic/Subtask Figure 2: Sample output (subtask tree) from a parse-based model for the catalog ordering domain. DialogAct,Pred!Args to parse plans. Their parser, however, was not probabilistic or targeted at dialog processing. Utterance 3 Clause Dialog Structure We consider a task-oriented dialog to be the result of incremental creation of a shared plan by the participants (Lochbaum, 1998). The shared plan is represented as a single tree T that incorporates the task/subtask structure, dialog acts, syntactic structure and lexical content of the dialog, as shown in Figure 1. A task is a sequence of subtasks ST ∈ S. A subtask is a sequence of dialog acts DA ∈ D. Each dialog act corresponds to one clause spoken by one speaker, customer (cu ) or agent (ca ) (for which we may have acoustic, lexical, syntactic and semantic representations). Figure 2 shows the subtask tree for a sample dialog in our domain (catalog ordering). An order placement task is typically composed of the sequenc"
E09-1012,J96-1002,0,0.0236082,"e ...... ......... Closing Ack may we deliver this order to your home thank you yes i would like yes one thank for calling to place an order second you yes please please XYZ catalog ...... this is mary how may I help you can i have your home telephone number with area code ...... may we deliver this order to your home yes please ...... Figure 4: An illustration of incremental evolution of dialog structure a feature vector containing contextual information for the parsing action (see Section 5.1). These feature vectors and the associated parser actions are used to train maximum entropy models (Berger et al., 1996). These models are then used to incrementally incorporate the utterances for a new dialog into that dialog’s subtask tree as the dialog progresses, as shown in Figure 3. 4.1 ing of this dialog using our shift-reduce dialog parser would proceed as follows: the STP model predicts shift for sta ; the DAP model predicts YNP(Promotions) for daa ; the generator outputs would you like a free magazine?; and the parser shifts a token representing this utterance onto the stack. Then, the customer says no. The DAC model classies dau as No; the STC model classies stu as shift and binary-reduce-special-o"
E09-1012,W04-2322,0,0.0225122,"Missing"
E09-1012,W97-0301,0,0.0311247,"d of the dialog, the output is a binary branching subtask tree. Consider the example subdialog A: would you like a free magazine? U: no. The process4.2 Start-Complete Method In the shift-reduce method, the dialog tree is constructed as a side effect of the actions performed on the stack: each reduce action on the stack introduces a non-terminal in the tree. By contrast, in the start-complete method the instructions to build the tree are directly encoded in the parser actions. A stack is used to maintain the global parse state. The actions the parser can take are similar to those described in (Ratnaparkhi, 1997). The parser must decide whether to join each new terminal onto the existing left-hand edge of the tree, or start a new subtree. The actions for the parser include start-X, n-start-X, complete-X, u-completeX and b-complete-X, where X is each of the nonterminals (subtask labels) in the tree. Start-X pushes a token representing the current utterance onto the stack; n-start-X pushes non-terminal X onto the stack; complete-X pushes a token representing the current utterance onto the stack, then 97 Type Call-level pops the top two tokens off the stack and pushes the non-terminal X; u-complete-X pop"
E09-1012,J03-4003,0,0.0354499,"), parsing from lexicalized tree-adjoining grammars (Cristea, 2000), and rulebased approaches that use rhetorical relations and discourse cues (Forbes et al., 2003; Polanyi et al., 2004; LeThanh et al., 2004). With the exception of Cristea (2000), most of this research has been limited to non-incremental parsing of textual monologues where, in contrast to incremental dialog parsing, predicting a system action is not relevant. The work on discourse parsing that is most similar to ours is that of Baldridge and Lascarides (2005). They used a probabilistic headdriven parsing method (described in (Collins, 2003)) to construct rhetorical structure trees for a spoken dialog corpus. However, their parser was Proceedings of the 12th Conference of the European Chapter of the ACL, pages 94–102, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 94 Order Placement Dialog Opening Task Task Contact Info Order Item Shipping Info Topic/Subtask DialogAct,Pred!Args Utterance Topic/Subtask DialogAct,Pred!Args Utterance Payment Info Summary Closing Task Delivery Info Topic/Subtask Figure 2: Sample output (subtask tree) from a parse-based model for the catalog ordering domain."
E09-1012,P06-2089,0,0.0169641,"the generator outputs would you like a free magazine?; and the parser shifts a token representing this utterance onto the stack. Then, the customer says no. The DAC model classies dau as No; the STC model classies stu as shift and binary-reduce-special-offer; and the parser shifts a token representing the utterance onto the stack, before popping the top two elements off the stack and adding the subtree for special-order into the dialog’s subtask tree. Shift-Reduce Method In this method, the subtask tree is recovered through a right-branching shift-reduce parsing process (Hall et al., 2006; Sagae and Lavie, 2006). The parser shifts each utterance on to the stack. It then inspects the stack and decides whether to do one or more reduce actions that result in the creation of subtrees in the subtask tree. The parser maintains two data structures – a stack and a tree. The actions of the parser change the contents of the stack and create nodes in the dialog tree structure. The actions for the parser include unaryreduce-X, binary-reduce-X and shift, where X is each of the non-terminals (subtask labels) in the tree. Shift pushes a token representing the utterance onto the stack; binary-reduce-X pops two token"
E09-1012,P04-1088,0,0.0758876,"Missing"
E09-1012,N03-1030,0,0.0413694,") includes the task structure of the dialog in the context, (4) can be trained from dialog data, and (5) runs incrementally, parsing the dialog as it occurs and interleaving generation and interpretation. At the core of our model is a parser that incrementally builds the dialog task structure as the Discourse Parsing Discourse parsing is the process of building a hierarchical model of a discourse from its basic elements (sentences or clauses), as one would build a parse of a sentence from its words. There has now been considerable work on discourse parsing using statistical bottom-up parsing (Soricut and Marcu, 2003), hierarchical agglomerative clustering (Sporleder and Lascarides, 2004), parsing from lexicalized tree-adjoining grammars (Cristea, 2000), and rulebased approaches that use rhetorical relations and discourse cues (Forbes et al., 2003; Polanyi et al., 2004; LeThanh et al., 2004). With the exception of Cristea (2000), most of this research has been limited to non-incremental parsing of textual monologues where, in contrast to incremental dialog parsing, predicting a system action is not relevant. The work on discourse parsing that is most similar to ours is that of Baldridge and Lascarides (200"
E09-1012,J86-3001,0,0.703203,"ordering domain that has been annotated for dialog acts and task/subtask information. We contrast the amount of context provided by each method and its impact on performance. 1 2 Introduction Related Work There are two threads of research that are relevant to our work: work on parsing (written and spoken) discourse, and work on plan-based dialog models. Corpora of spoken dialog are now widely available, and frequently come with annotations for tasks/games, dialog acts, named entities and elements of syntactic structure. These types of information provide rich clues for building dialog models (Grosz and Sidner, 1986). Dialog models can be built ofine (for dialog mining and summarization), or online (for dialog management). A dialog manager is the component of a dialog system that is responsible for interpreting user actions in the dialog context, and for generating system actions. Needless to say, a dialog manager operates incrementally as the dialog progresses. In typical commercial dialog systems, the interpretation and generation processes operate independently of each other, with only a small amount of shared context. By contrast, in this paper we describe a dialog model that (1) tightly integrates i"
E09-1012,C04-1007,0,0.0143956,"an be trained from dialog data, and (5) runs incrementally, parsing the dialog as it occurs and interleaving generation and interpretation. At the core of our model is a parser that incrementally builds the dialog task structure as the Discourse Parsing Discourse parsing is the process of building a hierarchical model of a discourse from its basic elements (sentences or clauses), as one would build a parse of a sentence from its words. There has now been considerable work on discourse parsing using statistical bottom-up parsing (Soricut and Marcu, 2003), hierarchical agglomerative clustering (Sporleder and Lascarides, 2004), parsing from lexicalized tree-adjoining grammars (Cristea, 2000), and rulebased approaches that use rhetorical relations and discourse cues (Forbes et al., 2003; Polanyi et al., 2004; LeThanh et al., 2004). With the exception of Cristea (2000), most of this research has been limited to non-incremental parsing of textual monologues where, in contrast to incremental dialog parsing, predicting a system action is not relevant. The work on discourse parsing that is most similar to ours is that of Baldridge and Lascarides (2005). They used a probabilistic headdriven parsing method (described in (C"
E09-1012,P06-2041,0,0.0146227,"omotions) for daa ; the generator outputs would you like a free magazine?; and the parser shifts a token representing this utterance onto the stack. Then, the customer says no. The DAC model classies dau as No; the STC model classies stu as shift and binary-reduce-special-offer; and the parser shifts a token representing the utterance onto the stack, before popping the top two elements off the stack and adding the subtree for special-order into the dialog’s subtask tree. Shift-Reduce Method In this method, the subtask tree is recovered through a right-branching shift-reduce parsing process (Hall et al., 2006; Sagae and Lavie, 2006). The parser shifts each utterance on to the stack. It then inspects the stack and decides whether to do one or more reduce actions that result in the creation of subtrees in the subtask tree. The parser maintains two data structures – a stack and a tree. The actions of the parser change the contents of the stack and create nodes in the dialog tree structure. The actions for the parser include unaryreduce-X, binary-reduce-X and shift, where X is each of the non-terminals (subtask labels) in the tree. Shift pushes a token representing the utterance onto the stack; binary"
E09-1012,C04-1048,0,\N,Missing
E09-1012,W04-0211,0,\N,Missing
K19-1053,N19-1423,0,0.0383268,"Missing"
K19-1053,P18-2081,0,0.27479,"gs, and can predict how a politician will vote on a bill. As demonstrated in this paper, this model works well when a politician’s voting track record has already been established. However, it fails for politicians not in the training data, such as those who have never been elected to office or voted on bills relevant to the issues in the target bill. Although we did not implement any Ideal Point Models, they obviously share this weakness. There have also been approaches that incorporate additional knowledge about politicians and bills to yield extra insight into politicians’ voting patterns. Kornilova et al. (2018) enhanced the embeddings learned by their model by providing bill sponsorship information along with a CNN architecture for learning bill embeddings, while Nguyen et al. (2015) supplemented their model by taking into consideration the type of language legislators use. However, these models share the inability of earlier models to predict voting behavior for new politicians or political candidates. In fact, Kornilova et al. (2018) explicitly note that their model cannot handle unobserved politicians: they say, ”During testing, we only include legislators present in the training data”. The contr"
K19-1053,D16-1221,0,0.279744,"tance with respect to specific issues. A KB such as Freebase is likely to contain rich information such as events a congressperson attends, people they are related to, and personal details such as schools they were educated at; this information may be correlated with a politician’s stance on specific issues (Sunshine Hillygus, 2005; Duckitt and Sibley, 2010; Kraut and Lewis, 1975). Information in a KB is likely to be more restricted, but more reliable, than information extracted from news articles. We integrate these sources of information into the embedding based prediction model proposed in Kraft et al. (2016). We experiment with two representations for news articles: as the mean of the embeddings of the words in the article, and as a bag of words. To represent information in KBs, we first capture the KB relations using Universal Schema (US) (Riedel et al., 2013), and then construct relation embeddings using a neural network. We evaluate the proposed approaches on multiple sessions of Congress under two settings: (1) with only politicians that are observed at trainThe official voting records of United States congresspeople are preserved as roll call votes. Prediction of voting behavior of politicia"
K19-1053,P15-1139,0,0.147067,". However, it fails for politicians not in the training data, such as those who have never been elected to office or voted on bills relevant to the issues in the target bill. Although we did not implement any Ideal Point Models, they obviously share this weakness. There have also been approaches that incorporate additional knowledge about politicians and bills to yield extra insight into politicians’ voting patterns. Kornilova et al. (2018) enhanced the embeddings learned by their model by providing bill sponsorship information along with a CNN architecture for learning bill embeddings, while Nguyen et al. (2015) supplemented their model by taking into consideration the type of language legislators use. However, these models share the inability of earlier models to predict voting behavior for new politicians or political candidates. In fact, Kornilova et al. (2018) explicitly note that their model cannot handle unobserved politicians: they say, ”During testing, we only include legislators present in the training data”. The contributions of Kornilova et al. (2018), among them the CNNbased bill representation and the incorporation of bill metadata, are orthogonal to those in this paper. We leave the exp"
K19-1053,N13-1008,1,0.886181,"lated with a politician’s stance on specific issues (Sunshine Hillygus, 2005; Duckitt and Sibley, 2010; Kraut and Lewis, 1975). Information in a KB is likely to be more restricted, but more reliable, than information extracted from news articles. We integrate these sources of information into the embedding based prediction model proposed in Kraft et al. (2016). We experiment with two representations for news articles: as the mean of the embeddings of the words in the article, and as a bag of words. To represent information in KBs, we first capture the KB relations using Universal Schema (US) (Riedel et al., 2013), and then construct relation embeddings using a neural network. We evaluate the proposed approaches on multiple sessions of Congress under two settings: (1) with only politicians that are observed at trainThe official voting records of United States congresspeople are preserved as roll call votes. Prediction of voting behavior of politicians for whom no voting record exists, such as individuals running for office, is important for forecasting key political decisions. Prior work has relied on past votes cast to predict future votes, and thus fails to predict voting patterns for politicians wit"
L16-1076,P05-1022,0,0.259903,"Missing"
L16-1076,P12-2071,0,0.0384176,"Missing"
L16-1076,P13-2045,1,0.879947,"Missing"
L16-1076,P14-5010,0,0.00506755,"Missing"
L16-1076,H05-1067,0,0.613636,"to gain insights into what differentiates funny captions from the rest. We developed a set of unsupervised methods for ranking captions based on features such as originality, centrality, sentiment, concreteness, grammaticality, humancenteredness, etc. We used each of these methods to independently rank all captions from our corpus and selected the top captions for each method. Then, we performed Amazon Mechanical Turk experiments in which we asked Turkers to judge which of the selected captions is funnier. Figure 1: Cartoon number 31 Figure 2: Cartoon number 32 2. Related Work In early work, Mihalcea and Strapparava (2005) investigate whether classification techniques can distinguish between humorous and non-humorous text. Training data consisted of humorous one-liners (15 words or less), and non-humorous one-liners, which are derived from Reuters news titles, proverbs, and sentences from the British National Corpus. They looked at features such as alliteration, antonymy and adult slang. Mihalcea and Pullman (2007) took this work further. They looked at four semantic classes relevant to humancenteredness: persons, social groups, social relationships, and personal pronouns. They showed that social relationships"
L16-1076,N12-2012,0,0.0309884,"Missing"
L16-1493,E09-1005,0,0.0698683,"Missing"
L16-1493,P05-1018,0,0.0189283,"nted as a set {x1 , . . . , xn } of n sentences. An extractive summary S is composed of sentences from this document, i.e., S ⊆ D. For any document, we seek to recover the highest-scoring summary Sb which can fits within a pre-determined budget b. Sb = arg max score(S, D) (1) S⊆D s.t. cost(S) &lt; b The tractability of this inference formulation depends on the factorization of the function score over the summary. In this work, we forego exact solutions to (1) in order to accommodate richer scoring functions that can model phenomena such as diversity (Carbonell and Goldstein, 1998) and coherence (Barzilay and Lapata, 2005; Christensen et al., 2013). Summaries are scored using a linear model score(S, D) = w> Φ(S, D) (2) where Φ is a feature map for summaries and w is a vector of learned parameters. In order to train the parameters w, we assume the existence of a training dataset D comprised of instances hD, S ∗ i Algorithm 1 Structured perceptron (Collins, 2002) Input: training dataset D, feature map Φ, learning rate η Output: vector of learned parameters w 1: w0 ← 0|Φ| 2: k ← 0 3: while not converged do 4: for instance hD, S ∗ i ∈ D do 5: Sb ← arg maxS wk> Φ(D, S) 6: if Sb 6= S ∗ then   b 7: wk+1 ← wk + η Φ("
L16-1493,N13-1136,0,0.0202871,"xn } of n sentences. An extractive summary S is composed of sentences from this document, i.e., S ⊆ D. For any document, we seek to recover the highest-scoring summary Sb which can fits within a pre-determined budget b. Sb = arg max score(S, D) (1) S⊆D s.t. cost(S) &lt; b The tractability of this inference formulation depends on the factorization of the function score over the summary. In this work, we forego exact solutions to (1) in order to accommodate richer scoring functions that can model phenomena such as diversity (Carbonell and Goldstein, 1998) and coherence (Barzilay and Lapata, 2005; Christensen et al., 2013). Summaries are scored using a linear model score(S, D) = w> Φ(S, D) (2) where Φ is a feature map for summaries and w is a vector of learned parameters. In order to train the parameters w, we assume the existence of a training dataset D comprised of instances hD, S ∗ i Algorithm 1 Structured perceptron (Collins, 2002) Input: training dataset D, feature map Φ, learning rate η Output: vector of learned parameters w 1: w0 ← 0|Φ| 2: k ← 0 3: while not converged do 4: for instance hD, S ∗ i ∈ D do 5: Sb ← arg maxS wk> Φ(D, S) 6: if Sb 6= S ∗ then   b 7: wk+1 ← wk + η Φ(D, S ∗ ) − Φ(D, S) 8: k ←k+"
L16-1493,W02-1001,0,0.295561,"information (Callan, 1994). Although this approach is simple and scalable, document style and structural differences when changing domains or publishers can significantly affect snippet quality. 3. System Our system takes as input an HTML document. We automatically extract the article text from the HTML, and then automatically preprocess the text to obtain sentences, tokens and part of speech tags. Then, we compute various features over the preprocessed document. Each sentence is scored using a combination of feature values and feature weights, which are learned using a structured perceptron (Collins, 2002). Finally, sentences are extracted in a greedy fashion based on their scores while respecting the length constraint. These steps are illustrated in Figure 2. 3.1. Features We implemented various features drawn from the summarization literature that capture aspects of salience, diversity, coverage, content and readability. Table 1 presents features from each category implemented in our framework. 3089 Category Position Length Content Lexical cues Syntactic cues Examples sentence position, paragraph position, in-paragraph position word length, character length, summary length similarity to query"
L16-1493,P06-1039,0,0.0790576,"Missing"
L16-1493,hong-etal-2014-repository,0,0.022483,"cument summarization has focused on scoring, ranking, and extracting the most “informative” sentences from a document using various supervised (e.g. (Conroy et al., 2004; Daum´e and Marcu, 2006; Lin, 1999; Svore et al., 2007)) and unsupervised (e.g. (Erkan and Radev, 2004; Mei et al., 2010; Mihalcea and Radev, 2011)) methods. Innovations fall into two broad categories: (a) finding ways to assess whether a sentence should be included in a summary; and (b) efficient algorithms for exploring the space of possible summaries. A recent study compared a number of extractive summarization algorithms (Hong et al., 2014). The best performing algorithm performed global optimization over the input sentence set. However, these algorithms were compared using the DUC 2004 task, (a) which is a multi-document summarization task; and (b) for which the reference summaries were abstractive. † Equal contribution. Figure 1: Our summarization system More related to our framework, snippet extraction is a popular approach for search engines and news aggregators to show some content related to the query and the original document (Li et al., 2008). A simple way to identify snippets is to extract a passage from specific areas"
L16-1493,N12-1015,0,0.0159613,"∗ i ∈ D do 5: Sb ← arg maxS wk> Φ(D, S) 6: if Sb 6= S ∗ then   b 7: wk+1 ← wk + η Φ(D, S ∗ ) − Φ(D, S) 8: k ←k+1 P return average weights k1 j wj where S ∗ represents a reference summary for document D. The parameters are estimated using the structured perceptron (Collins, 2002) which minimizes a 0/1 loss over D and incorporates parameter averaging for generalization. The basic learning procedure is described in Algorithm 1. When inference is inexact and carried out via search—as in the case of our framework—convergence and performance can be improved using violation-fixing weight updates (Huang and Feyong, 2012). In addition to greedy search, we also experimented with beams of various sizes to reduce search errors but did not observe performance improvements1 . 4. 4.1. Experiments - Systems Compared Baselines and Other Methods We compare the extractive summaries generated by our framework to three simple baselines (Lead-based and two variations of Greedy) as well as to an array of standard summarization methods from the literature. Lead-based The baseline, lead-based, algorithm takes the first sentences in the document that fit within the budget. Greedy The greedy algorithm is exactly the one describ"
L16-1493,W04-1013,0,0.0124222,"ntence to the document, computed using cosine similarity over term frequency vectors) and a binary feature— computed on the fly—to identify when a candidate sentence is adjacent to a sentence already present in the summary. The scoring function we used for the greedy and knapsack algorithms uses the same features. 4.3. Automatic Evaluation For evaluation, we produced 300-character extractive summaries from the input documents by passing them through the systems described earlier. We evaluate the performance of all systems against manually creative extractive reference summaries. We use ROUGE (Lin, 2004), a well-established automatic evaluation metric based on lexical overlap which has been widely used in the scientific community and has been shown to correlate well with human evaluations. We follow the standards suggested by Owczarzak et al. (2012). As is standard, we report ROUGE R-1, R-2 and R-4. Table 2 shows evaluation results. The lead-based baseline outperforms all the methods from the literature that we included. However, our framework outperforms this baseline. Analyzing the reference summaries, we observed that they are primarily lead-based unless one of the first sentences in the i"
L16-1493,W12-2601,0,0.0322969,"unction we used for the greedy and knapsack algorithms uses the same features. 4.3. Automatic Evaluation For evaluation, we produced 300-character extractive summaries from the input documents by passing them through the systems described earlier. We evaluate the performance of all systems against manually creative extractive reference summaries. We use ROUGE (Lin, 2004), a well-established automatic evaluation metric based on lexical overlap which has been widely used in the scientific community and has been shown to correlate well with human evaluations. We follow the standards suggested by Owczarzak et al. (2012). As is standard, we report ROUGE R-1, R-2 and R-4. Table 2 shows evaluation results. The lead-based baseline outperforms all the methods from the literature that we included. However, our framework outperforms this baseline. Analyzing the reference summaries, we observed that they are primarily lead-based unless one of the first sentences in the input document is a result of errors in article extraction from HTML (e.g. a byline is extracted as part of the article, or a photo caption is included as part of the article) or a repetition of the article title. 5. Experiments - Side by Side Editori"
L16-1493,D07-1047,0,0.0250912,"s without sacrificing meaning or grammaticality. We present evaluation results for the single-document news summarization use case, comparing performance on well written articles as well as on a sampling of news articles of random quality. 2. Related Work Research on single-document, extractive summarization has been conducted since the 1950s (Luhn, 1958). Traditionally, extractive single document summarization has focused on scoring, ranking, and extracting the most “informative” sentences from a document using various supervised (e.g. (Conroy et al., 2004; Daum´e and Marcu, 2006; Lin, 1999; Svore et al., 2007)) and unsupervised (e.g. (Erkan and Radev, 2004; Mei et al., 2010; Mihalcea and Radev, 2011)) methods. Innovations fall into two broad categories: (a) finding ways to assess whether a sentence should be included in a summary; and (b) efficient algorithms for exploring the space of possible summaries. A recent study compared a number of extractive summarization algorithms (Hong et al., 2014). The best performing algorithm performed global optimization over the input sentence set. However, these algorithms were compared using the DUC 2004 task, (a) which is a multi-document summarization task; a"
L16-1493,E09-1089,0,0.0268594,"ard MEAD, we implemented a variation in which the first sentence in the document is required to be in the summary (S1 + MEAD). Divrank (Mei et al., 2010) is based on a reinforced random walk over a lexical similarity graph. This model automatically balances the prestige and the diversity of the top ranked vertices in a principled way. Lexrank (Erkan and Radev, 2004) is based on eigenvector similarity over a lexical similarity graph. The nodes of the graph correspond to input sentences and the edges to weighted cosine similarity. The most central sentences are selected for the summary. MaxCov (Takamura and Okumura, 2009) applies approximation algorithms for the budgeted maximum coverage problem (Khuller et al., 1999) to document summarization. It assumes the existence of a vocabulary in which each word is associated with some positive profit (word score in the summary). Given a collection of subsets of this vocabulary (sentences), each associated with some cost (number of characters), the budgeted maximum coverage problem identifies a summary whose total cost remains within the budget and whose union maximizes the summary score. Personalized PageRank (PPR) (Agirre and Soroa, 2009) is a variation of the pagera"
L16-1493,J08-1001,0,\N,Missing
N07-4008,W07-0305,0,0.067998,"Missing"
N09-2048,2007.sigdial-1.23,0,0.0187575,"n flexible input systems (Danieli and Gerbino, 1995; Smith and Gordon, 1997). However, task completion rates and times are better in flexible input systems (ChuCarroll and Nickerson, 2000; Smith and Gordon, 1997). With user adaptation, in flexible input dialog systems prompts can be formulated to maximize ASR accuracy and reduce the number of ASR timeouts (Sheeder and Balogh, 2003). Previous research on user adaptation to dialog systems was conducted in laboratory settings. However, the behavior of recruited subjects in a quiet laboratory may differ from that of real users in the noisy world (Ai et al., 2007). Here we present the first study, to the best of our knowledge, that investigates the adaptive behavior of real users of a live dialog system. We analyze dialogs from CMU’s Let’s Go! dialog system (Raux et al., 2005). We look at the effects of the system’s lexical and syntactic choices on: 1) lexical and syntactic choices in user responses; and 2) concept identification rates for user responses. We confirm prior results showing that users adapt to the system’s lexical and syntactic choices. We also show that particular choices for system prompts can lead to higher concept identification rates"
N09-2048,A00-2027,0,0.0883196,"Missing"
N09-2048,P08-1044,0,0.0611808,"Missing"
N09-2048,J97-1006,0,0.0195965,"syntactic choices can be particularly useful in flexible input dialog systems. Limited input dialog systems, including most commercial systems, require the user to respond to each system prompt using only the concept and words currently requested by the system. Flexible input dialog systems allow the user to respond to system prompts with concepts and words in addition to or other than the ones currently requested, and may even allow the user to take task initiative. Speech recognition (ASR) accuracy in limited input systems is better than in flexible input systems (Danieli and Gerbino, 1995; Smith and Gordon, 1997). However, task completion rates and times are better in flexible input systems (ChuCarroll and Nickerson, 2000; Smith and Gordon, 1997). With user adaptation, in flexible input dialog systems prompts can be formulated to maximize ASR accuracy and reduce the number of ASR timeouts (Sheeder and Balogh, 2003). Previous research on user adaptation to dialog systems was conducted in laboratory settings. However, the behavior of recruited subjects in a quiet laboratory may differ from that of real users in the noisy world (Ai et al., 2007). Here we present the first study, to the best of our knowle"
N09-2058,D07-1001,0,0.0388033,"Missing"
N09-2058,H01-1065,0,0.0265389,"rm into a sequence of words. This task includes word choice, and word and constituent ordering. In English, the positions of required elements of a sentence, verb phrase or noun phrase are relatively fixed. However, many sentences also include adverbials whose position is not fixed (Figure 1). There may be several appropriate positions for an adverbial in a particular context, but other positions give output that is non-idiomatic or disfluent, ambiguous, or incoherent. Some computational research has included models for adjunct ordering (e.g. (Ringger et al., 2004; Marciniak and Strube, 2004; Elhadad et al., 2001)). However, this is the first computational study to look specifically at adverbials. Adverbial positioning has long been studied in linguistics (e.g. (Keyser, 1968; Allen and Cruttenden, 1974; Ernst, 1984; Haider, 2000)). Most linguistic research focuses on whether 229 In this paper, we compare three approaches to adverbial positioning: a simple baseline approach using lexical and syntactic features, and one- and twostage classification-based approaches using lexical, syntactic, semantic and sentence-level features. We apply these approaches in a hybrid surface realizer that uses a probabilis"
N09-2058,1997.tmi-1.11,0,0.104637,"Missing"
N09-2058,C04-1097,0,0.134513,"zer is to transform an input semantic/syntactic form into a sequence of words. This task includes word choice, and word and constituent ordering. In English, the positions of required elements of a sentence, verb phrase or noun phrase are relatively fixed. However, many sentences also include adverbials whose position is not fixed (Figure 1). There may be several appropriate positions for an adverbial in a particular context, but other positions give output that is non-idiomatic or disfluent, ambiguous, or incoherent. Some computational research has included models for adjunct ordering (e.g. (Ringger et al., 2004; Marciniak and Strube, 2004; Elhadad et al., 2001)). However, this is the first computational study to look specifically at adverbials. Adverbial positioning has long been studied in linguistics (e.g. (Keyser, 1968; Allen and Cruttenden, 1974; Ernst, 1984; Haider, 2000)). Most linguistic research focuses on whether 229 In this paper, we compare three approaches to adverbial positioning: a simple baseline approach using lexical and syntactic features, and one- and twostage classification-based approaches using lexical, syntactic, semantic and sentence-level features. We apply these approaches"
N16-1008,D07-1069,1,0.692207,"aselines on two publicly available datasets. 2 Related Work Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. ("
N16-1008,C08-1040,1,0.865783,"icly available datasets. 2 Related Work Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the"
N16-1008,D13-1068,0,0.0170224,"ls for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the first to consider latent variable models, even though it is difficult to incorporate features and high-dimensional latent states in a HMM-based model. Ahmed et al. (2011) proposed a hierarchical nonparametric model that integrates a Recurrent Chinese Restaurant Process with Latent Dirichlet Allocation to cluster words over time. The main issues with this approach are that it does not generate human-readable sentences, and that scaling nonparametric Bayesian models is often challenging. Similarly, Huang and Huang (2013) introduced a joint mixture-event-aspect model using a generative method. Navarro-Colorado and Saquete (2015) combined temporal information with topic modeling, and obtained the best performance in the crossdocument event ordering task of SemEval 2015. There has been prior work (Wang et al., 2008; Lee et al., 2009) using matrix factorization to perform sentence clustering. A key distinction between our work and this previous work is that our method requires no additional sentence selection steps after sentence clustering, so we avoid error cascades. 60 Zhu and Chen (2007) were among the first"
N16-1008,W04-1013,0,0.0596697,"tent factor models have had huge success in recommender systems. These latent factors, often in the form of low-rank embeddings, capture not only explicit information but also implicit context from the input data. In this work, we propose a novel matrix factorization framework to “recommend” key sentences to a timeline. Figure 2 shows an overview of the framework. More specifically, we formulate this task as a matrix completion problem. Given a news corpus, we assume that there are m total sentences, which are the rows in the matrix. The first column is the metric section, where we use ROUGE (Lin, 2004) as the metric to pre-compute a sentence importance 61 score between a candidate sentence and a humangenerated summary. During training, we use these scores to tune model parameters, and during testing, we predict the sentence importance scores given the features in other columns. That is, we learn the embedding of important sentences. The second set of columns is the text feature section. In our experiments, this includes word observations, subject-verb-object (SVO) events, and the publication date of the document from which the candidate sentence is extracted. In our preprocessing step, we r"
N16-1008,S15-2138,0,0.0165672,"e first to consider latent variable models, even though it is difficult to incorporate features and high-dimensional latent states in a HMM-based model. Ahmed et al. (2011) proposed a hierarchical nonparametric model that integrates a Recurrent Chinese Restaurant Process with Latent Dirichlet Allocation to cluster words over time. The main issues with this approach are that it does not generate human-readable sentences, and that scaling nonparametric Bayesian models is often challenging. Similarly, Huang and Huang (2013) introduced a joint mixture-event-aspect model using a generative method. Navarro-Colorado and Saquete (2015) combined temporal information with topic modeling, and obtained the best performance in the crossdocument event ordering task of SemEval 2015. There has been prior work (Wang et al., 2008; Lee et al., 2009) using matrix factorization to perform sentence clustering. A key distinction between our work and this previous work is that our method requires no additional sentence selection steps after sentence clustering, so we avoid error cascades. 60 Zhu and Chen (2007) were among the first to consider multimodal timeline summarization, but they focus on visualization, and do not make use of images"
N16-1008,P14-1087,0,0.0300454,"example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the first to consider latent variable models, even though it is difficult to incorporate feature"
N16-1008,nivre-etal-2006-maltparser,0,0.156814,"e sentence and a humangenerated summary. During training, we use these scores to tune model parameters, and during testing, we predict the sentence importance scores given the features in other columns. That is, we learn the embedding of important sentences. The second set of columns is the text feature section. In our experiments, this includes word observations, subject-verb-object (SVO) events, and the publication date of the document from which the candidate sentence is extracted. In our preprocessing step, we run the Stanford part-of-speech tagger (Toutanova et al., 2003) and MaltParser (Nivre et al., 2006) to generate SVO events based on dependency parses. Additional features can easily be incorporated into this framework; we leave the consideration of additional features for future work. Finally, for each sentence, we use an image search engine to retrieve a top-ranked relevant image, and then we use a convolutional neural network (CNN) architecture to extract visual features in an unsupervised fashion. We use a CNN model from Simonyan and Zisserman (2015), which is trained on the ImageNet Challenge 2014 dataset (Russakovsky et al., 2014). In our work, we keep the 16 convolutional layers and m"
N16-1008,radev-etal-2004-mead,1,0.359708,"date sentence in a news article, we take advantage of top-ranked relevant images from the Web and model the image using a convolutional neural network architecture. Finally, we propose a scalable low-rank approximation approach for learning joint embeddings of news stories and images. In experiments, we compare our model to various competitive baselines, and demonstrate the stateof-the-art performance of the proposed textbased and multimodal approaches. 1 To distill key insights from news reports, prior work in summarization often relies on feature engineering, and uses clustering techniques (Radev et al., 2004b) to select important events to be included in the final summary. While this approach is unsupervised, the process of feature engineering is always expensive, and the number of clusters is not easy to estimate. To present a complete summary, researchers from the natural language processing (NLP) community often solely rely on the textual information, while studies in the computer vision (CV) community rely solely on the image and video information. However, even though news images are abundantly available together with news stories, approaches that jointly learn textual and visual representat"
N16-1008,N03-1033,0,0.0186037,"e importance 61 score between a candidate sentence and a humangenerated summary. During training, we use these scores to tune model parameters, and during testing, we predict the sentence importance scores given the features in other columns. That is, we learn the embedding of important sentences. The second set of columns is the text feature section. In our experiments, this includes word observations, subject-verb-object (SVO) events, and the publication date of the document from which the candidate sentence is extracted. In our preprocessing step, we run the Stanford part-of-speech tagger (Toutanova et al., 2003) and MaltParser (Nivre et al., 2006) to generate SVO events based on dependency parses. Additional features can easily be incorporated into this framework; we leave the consideration of additional features for future work. Finally, for each sentence, we use an image search engine to retrieve a top-ranked relevant image, and then we use a convolutional neural network (CNN) architecture to extract visual features in an unsupervised fashion. We use a CNN model from Simonyan and Zisserman (2015), which is trained on the ImageNet Challenge 2014 dataset (Russakovsky et al., 2014). In our work, we ke"
N16-1008,N15-1112,0,0.147479,"rmance. Our main contributions are three-fold: • We propose a novel matrix factorization approach for extractive summarization, leveraging the success of collaborative filtering; • We are among the first to consider representation learning of a joint embedding for text and images in timeline summarization; • Our model significantly outperforms various competitive baselines on two publicly available datasets. 2 Related Work Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popul"
N16-1008,D11-1040,0,0.599874,"Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the first to consider latent variable mo"
N16-1008,W04-3252,0,\N,Missing
P02-1048,C00-1054,1,0.503361,"n the user has hit the click-to-speak button, when a speech result arrives, and whether or not the user is inking on the display. When a speech lattice arrives, if inking is in progress MMFST waits for the ink meaning lattice, otherwise it applies a short timeout (1 sec.) and treats the speech as unimodal. When an ink meaning lattice arrives, if the user has tapped click-to-speak MMFST waits for the speech lattice to arrive, otherwise it applies a short timeout (1 sec.) and treats the ink as unimodal. MMFST uses the finite-state approach to multimodal integration and understanding proposed by Johnston and Bangalore (2000). Possibilities for multimodal integration and understanding are captured in a three tape device in which the first tape represents the speech stream (words), the second the ink stream (gesture symbols) and the third their combined meaning (meaning symbols). In essence, this device takes the speech and ink meaning lattices as inputs, consumes them using the first two tapes, and writes out a multimodal meaning lattice using the third tape. The three tape finite-state device is simulated using two transducers: G:W which is used to align speech and ink and G W:M which takes a composite alphabet o"
P02-1048,C00-1053,1,0.772153,"alues such as area, point, line, arrow. MEANING indicates the meaning of that form; for example an area can be either a loc(ation) or a sel(ection). NUMBER and TYPE indicate the number of entities in a selection (1,2,3, many) and their type (rest(aurant), theatre). SEM is a place holder for the specific content of the gesture, such as the points that make up an area or the identifiers of objects in a selection. When multiple selection gestures are present an aggregation technique (Johnston and Bangalore, 2001) is employed to overcome the problems with deictic plurals and numerals described in Johnston (2000). Aggregation augments the ink meaning lattice with aggregate gestures that result from combining adjacent selection gestures. This allows a deictic expression like these three restaurants to combine with two area gestures, one which selects one restaurant and the other two, as long as their sum is three. For example, if the user makes two area gestures, one around a single restaurant and the other around two restaurants (Figure 3), the resulting ink meaning lattice will be as in Figure 8. The first gesture (node numbers 0-7) is either a reference to a location (loc.) (0-3,7) or a reference to"
P02-1048,1997.iwpt-1.19,0,0.0197684,"e the corresponding I symbol is the specific interpretation. After multimodal integration a projection G:M is taken from the result G W:M machine and composed with the original I:G in order to reincorporate the specific contents that were left out of the finite-state process (I:G o G:M = I:M). The multimodal finite-state transducers used at runtime are compiled from a declarative multimodal context-free grammar which captures the structure Figure 8: Ink Meaning Lattice and interpretation of multimodal and unimodal commands, approximated where necessary using standard approximation techniques (Nederhof, 1997). This grammar captures not just multimodal integration patterns but also the parsing of speech and gesture, and the assignment of meaning. In Figure 9 we present a small simplified fragment capable of handling MATCH commands such as phone numbers for these three restaurants. A multimodal CFG differs from a normal CFG in that the terminals are triples: W:G:M, where W is the speech stream (words), G the ink stream (gesture symbols) and M the meaning stream (meaning symbols). An XML representation for meaning is used to facilate parsing and logging by other system components. The meaning tape sy"
P02-1048,P99-1024,1,0.628273,"ir specific contents in I:G (I:G o G:M = I:M). The meaning read off I:M is <cmd> <phone> <restaurant> [id1,id2,id3] </restaurant> </phone> </cmd>. This is passed to the multimodal dialog manager (MDM) and from there to the Multimodal UI resulting in a display like Figure 4 with coordinated TTS output. Since the speech input is a lattice and there is also potential for ambiguity in the multimodal grammar, the output from MMFST to MDM is an N-best list of potential multimodal interpretations. Multimodal Dialog Manager (MDM) The MDM is based on previous work on speech-act based models of dialog (Stent et al., 1999; Rich and Sidner, 1998). It uses a Java-based toolkit for writing dialog managers that is similar in philosophy to TrindiKit (Larsson et al., 1999). It includes several rule-based S CMD DEICTICNP DDETPL RESTPL NUM !! ! !!! < > < > eps:eps: cmd CMD eps:eps: /cmd phone:eps: phone numbers:eps:eps for:eps:eps DEICTICNP eps:eps: /phone DDETPL eps:area:eps eps:selection:eps NUM RESTPL eps:eps: restaurant eps:SEM:SEM eps:eps: /restaurant these:G:eps restaurants:restaurant:eps three:3:eps < < > > < < > > Figure 9: Multimodal grammar fragment processes that operate on a shared state. The state include"
P02-1048,W02-2110,0,0.226817,"e restaurants in Figure 3 and writes phone, the system responds with a graphical callout on the display, synchronized with a text-to-speech (TTS) prompt of the phone number, for each restaurant in turn (Figure 4). Figure 3: Two area gestures Figure 4: Phone query callouts The system also provides subway directions. If the user says How do I get to this place? and circles one Figure 5: Multimodal subway route User-tailored generation MATCH can also provide a user-tailored summary, comparison, or recommendation for an arbitrary set of restaurants, using a quantitative model of user preferences (Walker et al., 2002). The system will only discuss restaurants that rank highly according to the user’s dining preferences, and will only describe attributes of those restaurants the user considers important. This permits concise, targeted system responses. For example, the user could say compare these restaurants and circle a large set of restaurants (Figure 6). If the user considers inexpensiveness and food quality to be the most important attributes of a restaurant, the system response might be: Compare-A: Among the selected restaurants, the following offer exceptional overall value. Uguale’s price is 33 dolla"
P02-1048,P00-1020,0,\N,Missing
P04-1011,A00-2023,0,0.0311426,"es portability across domains and dialog contexts by using general rules for each generation module. However, the quality of the output for a particular domain, or a particular dialog context, may be inferior to that of a templatebased system unless domain-specific rules are developed or general rules are tuned for the particular domain. Furthermore, full NLG may be too slow for use in dialog systems. A third, more recent, approach is trainable generation: techniques for automatically training NLG modules, or hybrid techniques that adapt NLG modules to particular domains or user groups, e.g. (Langkilde, 2000; Mellish, 1998; Walker, Rambow and Rogati, 2002). Open questions about the trainable approach include (1) whether the output quality is high enough, and (2) whether the techniques work well across domains. For example, the training method used in SPoT (Sentence Planner Trainable), as described in (Walker, Rambow and Rogati, 2002), was only shown to work in the travel domain, for the information gathering phase of the dialog, and with simple content plans involving no rhetorical relations. This paper describes trainable sentence planning for information presentation in the MATCH (Multimodal Ac"
P04-1011,P02-1048,1,0.318096,"d Rogati, 2002). Open questions about the trainable approach include (1) whether the output quality is high enough, and (2) whether the techniques work well across domains. For example, the training method used in SPoT (Sentence Planner Trainable), as described in (Walker, Rambow and Rogati, 2002), was only shown to work in the travel domain, for the information gathering phase of the dialog, and with simple content plans involving no rhetorical relations. This paper describes trainable sentence planning for information presentation in the MATCH (Multimodal Access To City Help) dialog system (Johnston et al., 2002). We provide evidence that the trainable approach is feasible by showing (1) that the training technique used for SPoT can be extended to a new domain (restaurant information); (2) that this technique, previously used for informationgathering utterances, can be used for information presentations, namely recommendations and comparisons; and (3) that the quality of the output is comparable to that of a template-based generator previously developed and experimentally evaluated with MATCH users (Walker et al., 2002; Stent et al., 2002). Section 2 describes SPaRKy (Sentence Planning with Rhetorical"
P04-1011,A97-1039,0,0.0665551,"ts a set of text plan trees (tp-trees), consisting of a set of speech acts to be communicated and the rhetorical relations that hold between them. For example, the two tp-trees in Figure 6 are generated for the content plan in Figure 2. Sentence plans such as alternative 25 in Figure 4 are avoided; it is clearly worse than alternatives 12, 13 and 20 since it neither combines information based on a restaurant entity (e.g Babbo) nor on an attribute (e.g. decor). The top ranked sentence plan output by the SPR is input to the RealPro surface realizer which produces a surface linguistic utterance (Lavoie and Rambow, 1997). A prosody assignment module uses the prior levels of linguistic representation to determine the appropriate prosody for the utterance, and passes a markedup string to the text-to-speech module. Sentence Plan Generation As in SPoT, the basis of the SPG is a set of clause-combining operations that operate on tptrees and incrementally transform the elementary predicate-argument lexico-structural representations (called DSyntS (Melcuk, 1988)) associated with the speech-acts on the leaves of the tree. The operations are applied in a bottom-up left-to-right fashion and the resulting representation"
P04-1011,W98-1411,0,0.0360157,"Missing"
P04-1011,A92-1006,0,0.089344,"tructuring phase, called infer, which holds for combinations of speech acts for which there is no rhetorical relation expressed in the content plan, as in (Marcu, 1997). By explicitly representing the discourse structure of the information presentation, we can generate information presentations with considerably more internal complexity than those generated in (Walker, Rambow and Rogati, 2002) and eliminate those that violate certain coherence principles, as described in Section 2. The clause-combining operations are general operations similar to aggregation operations used in other research (Rambow and Korelsky, 1992; Danlos, 2000). The operations and the 1 Although the probability distribution here is handcrafted based on assumed preferences for operations such as merge, relative-clause and with-reduction, it might also be possible to learn this probability distribution from the data by training in two phases. elaboration infer nucleus:&lt;1&gt;assert-com-list_exceptional contrast contrast nucleus:&lt;2&gt;assert-com-decor nucleus:&lt;4&gt;assert-com-service nucleus:&lt;3&gt;assert-com-decor contrast nucleus:&lt;6&gt;assert-com-cuisine nucleus:&lt;5&gt;assert-com-service nucleus:&lt;7&gt;assert-com-cuisine elaboration nucleus:&lt;1&gt;assert-com-list_"
P04-1011,P01-1056,1,0.92581,"Missing"
P04-1011,W02-2110,0,0.123126,"mation presentation in the MATCH (Multimodal Access To City Help) dialog system (Johnston et al., 2002). We provide evidence that the trainable approach is feasible by showing (1) that the training technique used for SPoT can be extended to a new domain (restaurant information); (2) that this technique, previously used for informationgathering utterances, can be used for information presentations, namely recommendations and comparisons; and (3) that the quality of the output is comparable to that of a template-based generator previously developed and experimentally evaluated with MATCH users (Walker et al., 2002; Stent et al., 2002). Section 2 describes SPaRKy (Sentence Planning with Rhetorical Knowledge), an extension of SPoT that uses rhetorical relations. SPaRKy consists of a randomized sentence plan generator (SPG) and a trainable sentence plan ranker (SPR); these are described in Sections 3 strategy:recommend items: Chanpen Thai relations:justify(nuc:1;sat:2); justify(nuc:1;sat:3); justify(nuc:1;sat:4) content: 1. assert(best(Chanpen Thai)) 2. assert(has-att(Chanpen Thai, decor(decent))) 3. assert(has-att(Chanpen Thai, service(good)) 4. assert(has-att(Chanpen Thai, cuisine(Thai))) Figure 1: A co"
P04-1011,P87-1022,0,\N,Missing
P06-1026,J99-2004,1,0.313683,"ompare two models for recovering the subtask structure – a chunk-based model and a parse-based model. In the chunk-based model, we recover the precedence relations (sequence) of the subtasks but not dominance relations (subtask structure) among the subtasks. Figure 3 shows a sample output from the chunk model. In the parse model, we recover the complete task structure from the sequence of utterances as shown in Figure 2. Here, we describe our two models. We present our experiments on subtask segmentation and labeling in Section 6.4. We automatically annotate a user’s utterance with supertags (Bangalore and Joshi, 1999). Supertags encapsulate predicate-argument information in a local structure. They are composed with each other using the substitution and adjunction operations of Tree-Adjoining Grammars (Joshi, 1987) to derive a dependency analysis of an utterance and its predicate-argument structure. 4.3 Contact Info Dialog Act Tagging We use a domain-specific dialog act tagging scheme based on an adapted version of DAMSL (Core, 1998). The DAMSL scheme is quite comprehensive, but as others have also found (Jurafsky et al., 1998), the multi-dimensionality of the scheme makes the building of models from DAMSL-"
P06-1026,P92-1008,0,0.0582372,"Missing"
P06-1026,J96-1002,0,0.0109453,"Missing"
P06-1026,W03-2123,0,0.028735,"Missing"
P06-1026,J98-4001,0,0.0163483,"berry, 2001; Bohus and Rudnicky, 2003)) and information state-based approaches (e.g. (Larsson et al., 1999; Bos et al., 2003; Lemon and Gruenstein, 2004)). In recent years, there has been considerable research on how to automatically learn models of both types from data. Researchers who treat dialog as a sequence of information states have used reinforcement learning and/or Markov decision processes to build stochastic models for dialog management 4 Structural Analysis of a Dialog We consider a task-oriented dialog to be the result of incremental creation of a shared plan by the participants (Lochbaum, 1998). The shared plan is represented as a single tree that encapsulates the task structure (dominance and precedence relations among tasks), dialog act structure (sequences of dialog acts), and linguistic structure of utterances (inter-clausal relations and predicateargument relations within a clause), as illustrated in Figure 1. As the dialog proceeds, an utterance from a participant is accommodated into the tree in an incremental manner, much like an incremental syntactic parser accommodates the next word into a partial parse tree (Alexandersson and Reithinger, 1997). With this model, we can tig"
P06-1026,N01-1016,0,0.00960766,"tion, order-item, related-offers, summary. Subtasks can be nested; the nesting structure can be as deep as five levels. Most often the nesting is at the left or right frontier of the subtask tree. DialogAct,Pred−Args Utterance Clause Figure 1: Structural analysis of a dialog 4.1 Order Placement Utterance Segmentation The task of ”cleaning up” spoken language utterances by detecting and removing speech repairs and dysfluencies and identifying sentence boundaries has been a focus of spoken language parsing research for several years (e.g. (Bear et al., 1992; Seneff, 1992; Shriberg et al., 2000; Charniak and Johnson, 2001)). We use a system that segments the ASR output of a user’s utterance into clauses. The system annotates an utterance for sentence boundaries, restarts and repairs, and identifies coordinating conjunctions, filled pauses and discourse markers. These annotations are done using a cascade of classifiers, details of which are described in (Bangalore and Gupta, 2004). 4.2 Opening Order Item Payment Info Shipping Info Summary Closing Delivery Info Figure 2: A sample task structure in our application domain. Opening Contact Info Order Item Payment Info Shipping Info Summary Closing Delivery Info Figu"
P06-1026,J01-2004,0,0.0987769,"toolkit LLAMA (Haffner, 2006) to estimate the conditional distribution using maxent. LLAMA encodes multiclass maxent as binary maxent, in order to increase the speed of training and to scale this method to large data sets. Each of the g classes in the set z{> is encoded as a bit vector such that, in the vector for class |, the |B}~ bit is one and all other bits are zero. Then, g one-vs-other binary classifiers are used as follows. ; =+  r ; = ^ _ ¡ ; = 798x 7 9 x 8  ¡`w ¢ For real-time dialog management we use a topdown incremental parser that incorporates bottomup information (Roark, 2001). We rewrite equation (6) to exploit the subtask sequence provided by the chunk model as shown in Equation 7. For the purpose of this paper, we approximate Equation 7 using one-best (or k-best) chunk output.1 (2) to estimate the conditional distribution e In  order  D H  we use the general technique of choos_ _ 5.2 Parse-based Model As seen in Figure 3, the chunk model does not capture dominance relations among subtasks, which are important for resolving anaphoric references (Grosz and Sidner, 1986). Also, the chunk model is representationally inadequate for centerembedded nestings of s"
P06-1026,J86-3001,0,0.104962,"-time dialog management we use a topdown incremental parser that incorporates bottomup information (Roark, 2001). We rewrite equation (6) to exploit the subtask sequence provided by the chunk model as shown in Equation 7. For the purpose of this paper, we approximate Equation 7 using one-best (or k-best) chunk output.1 (2) to estimate the conditional distribution e In  order  D H  we use the general technique of choos_ _ 5.2 Parse-based Model As seen in Figure 3, the chunk model does not capture dominance relations among subtasks, which are important for resolving anaphoric references (Grosz and Sidner, 1986). Also, the chunk model is representationally inadequate for centerembedded nestings of subtasks, which do occur in our domain, although less frequently than the more prevalent “tail-recursive” structures. In this model, we are e interested in finding the most likely plan tree (  ) given the sequence of utterances: to cope with the prediction errors of the classifier, we approximate J3ML  with an P -gram language model on sequences of the refined tag labels: &:&apos; Q ) +R,/.S0/2K,4 &(&apos; Q ; &lt;*= 5 61T 798 5 6ST1U V WYX[Z _ ] ; d=  ,/.S0/2K,4 ^`_ T 798baBc 5 6 5 6ST1U V WYX[Z ; =+ In this"
P06-1026,H92-1060,0,0.0142709,"of subtasks opening, contact-information, order-item, related-offers, summary. Subtasks can be nested; the nesting structure can be as deep as five levels. Most often the nesting is at the left or right frontier of the subtask tree. DialogAct,Pred−Args Utterance Clause Figure 1: Structural analysis of a dialog 4.1 Order Placement Utterance Segmentation The task of ”cleaning up” spoken language utterances by detecting and removing speech repairs and dysfluencies and identifying sentence boundaries has been a focus of spoken language parsing research for several years (e.g. (Bear et al., 1992; Seneff, 1992; Shriberg et al., 2000; Charniak and Johnson, 2001)). We use a system that segments the ASR output of a user’s utterance into clauses. The system annotates an utterance for sentence boundaries, restarts and repairs, and identifies coordinating conjunctions, filled pauses and discourse markers. These annotations are done using a cascade of classifiers, details of which are described in (Bangalore and Gupta, 2004). 4.2 Opening Order Item Payment Info Shipping Info Summary Closing Delivery Info Figure 2: A sample task structure in our application domain. Opening Contact Info Order Item Payment I"
P06-1026,P04-1010,0,0.107697,"Missing"
P06-1026,J00-3003,0,0.522205,"Missing"
P06-1026,2005.sigdial-1.4,0,0.0600817,"Missing"
P06-1026,J97-1002,0,\N,Missing
P15-1038,D14-1082,0,0.0182931,"http://www.clearnlp.com 5 388 Parser ClearNLP v2,37 GN138 LTDP v2.0.39 Mate v3.6.110 RBG11 Redshift12 spaCy13 SNN14 Turbo v2.215 Yara16 Approach Transition-based, selectional branching (Choi and McCallum, 2013) Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Maximum spanning tree, 3rd-order features (Bohnet, 2010) Tensor decomposition, randomized hill-climb (Lei et al., 2014) Transition-based, non-monotonic (Honnibal et al., 2013) Transition-based, greedy, dynamic oracle, Brown clusters Transition-based, word embeddings (Chen and Manning, 2014) Dual decomposition, 3rd-order features (Martins et al., 2013) Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Language Java Python Python Java Java Cython Cython Java C++ Java License Apache GPL v2 n/a GPL v2 MIT FOSS Dual GPL v2 GPL v2 Apache Table 3: Dependency parsers used in our experiments. 4 Parsers 64 produced the best accuracy, while a beam size of 1 for LTDT, ClearNLP, and Yara produced the best speed performance. Given this trend, we also include how those three parsers perform at beam 1 in our analyses. We compared ten state of the art parsers representi"
P15-1038,P13-1104,1,0.926373,"Missing"
P15-1038,P12-2071,1,0.342324,"Missing"
P15-1038,W08-1301,0,0.108481,"Missing"
P15-1038,Q13-1033,0,0.0475998,"Missing"
P15-1038,P11-2125,0,0.09249,"Missing"
P15-1038,W09-1201,0,0.0163968,"Missing"
P15-1038,D11-1037,0,0.019356,": newswire, PT : pivot text, TC : telephone conversation, WB : web text, ALL : all genres combined. eralize.1 Furthermore, a detailed comparative error analysis is typically lacking. The most detailed comparison of dependency parsers to date was performed by McDonald and Nivre (2007; 2011); they analyzed accuracy as a function of sentence length, dependency distance, valency, non-projectivity, part-of-speech tags and dependency labels.2 Since then, additional analyses of dependency parsers have been performed, but either with respect to specific linguistic phenomena (e.g. (Nivre et al., 2010; Bender et al., 2011)) or to downstream tasks (e.g. (Miwa and others, 2010; Petrov et al., 2010; Yuret et al., 2013)). 3 3.1 ture from the input, as these are often not available in non-annotated data. 3.2 Dependency Conversion OntoNotes provides annotation of constituency trees only. Several programs are available for converting constituency trees into dependency trees. Table 2 shows a comparison between three of the most widely used: the LTH (Johansson and Nugues, 2007),4 , Stanford (de Marneffe and Manning, 2008),5 and ClearNLP (Choi and Palmer, 2012b)6 dependency converters. Compared to the Stanford converter,"
P15-1038,W13-3518,0,0.028072,"2012). 3 conll.cemantix.org/2012/download/ids/ 4 http://nlp.cs.lth.se/software http://nlp.stanford.edu/software 6 http://www.clearnlp.com 5 388 Parser ClearNLP v2,37 GN138 LTDP v2.0.39 Mate v3.6.110 RBG11 Redshift12 spaCy13 SNN14 Turbo v2.215 Yara16 Approach Transition-based, selectional branching (Choi and McCallum, 2013) Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Maximum spanning tree, 3rd-order features (Bohnet, 2010) Tensor decomposition, randomized hill-climb (Lei et al., 2014) Transition-based, non-monotonic (Honnibal et al., 2013) Transition-based, greedy, dynamic oracle, Brown clusters Transition-based, word embeddings (Chen and Manning, 2014) Dual decomposition, 3rd-order features (Martins et al., 2013) Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Language Java Python Python Java Java Cython Cython Java C++ Java License Apache GPL v2 n/a GPL v2 MIT FOSS Dual GPL v2 GPL v2 Apache Table 3: Dependency parsers used in our experiments. 4 Parsers 64 produced the best accuracy, while a beam size of 1 for LTDT, ClearNLP, and Yara produced the best speed performance. Given this trend, we also in"
P15-1038,W14-6110,0,0.0237582,"Missing"
P15-1038,N12-1015,0,0.0184437,"Missing"
P15-1038,W07-2416,0,0.0132148,"Missing"
P15-1038,D12-1096,0,0.0247064,"Missing"
P15-1038,D10-1069,0,0.0128203,"L : all genres combined. eralize.1 Furthermore, a detailed comparative error analysis is typically lacking. The most detailed comparison of dependency parsers to date was performed by McDonald and Nivre (2007; 2011); they analyzed accuracy as a function of sentence length, dependency distance, valency, non-projectivity, part-of-speech tags and dependency labels.2 Since then, additional analyses of dependency parsers have been performed, but either with respect to specific linguistic phenomena (e.g. (Nivre et al., 2010; Bender et al., 2011)) or to downstream tasks (e.g. (Miwa and others, 2010; Petrov et al., 2010; Yuret et al., 2013)). 3 3.1 ture from the input, as these are often not available in non-annotated data. 3.2 Dependency Conversion OntoNotes provides annotation of constituency trees only. Several programs are available for converting constituency trees into dependency trees. Table 2 shows a comparison between three of the most widely used: the LTH (Johansson and Nugues, 2007),4 , Stanford (de Marneffe and Manning, 2008),5 and ClearNLP (Choi and Palmer, 2012b)6 dependency converters. Compared to the Stanford converter, the ClearNLP converter produces a similar set of dependency labels but ge"
P15-1038,D13-1116,0,0.029659,"Missing"
P15-1038,W13-3516,0,0.0362406,"Missing"
P15-1038,P14-1130,0,0.0156994,"cy parsing was performed by (Kummerfeld and others, 2012). 3 conll.cemantix.org/2012/download/ids/ 4 http://nlp.cs.lth.se/software http://nlp.stanford.edu/software 6 http://www.clearnlp.com 5 388 Parser ClearNLP v2,37 GN138 LTDP v2.0.39 Mate v3.6.110 RBG11 Redshift12 spaCy13 SNN14 Turbo v2.215 Yara16 Approach Transition-based, selectional branching (Choi and McCallum, 2013) Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Maximum spanning tree, 3rd-order features (Bohnet, 2010) Tensor decomposition, randomized hill-climb (Lei et al., 2014) Transition-based, non-monotonic (Honnibal et al., 2013) Transition-based, greedy, dynamic oracle, Brown clusters Transition-based, word embeddings (Chen and Manning, 2014) Dual decomposition, 3rd-order features (Martins et al., 2013) Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Language Java Python Python Java Java Cython Cython Java C++ Java License Apache GPL v2 n/a GPL v2 MIT FOSS Dual GPL v2 GPL v2 Apache Table 3: Dependency parsers used in our experiments. 4 Parsers 64 produced the best accuracy, while a beam size of 1 for LTDT, ClearNLP, and Yara produced"
P15-1038,N06-2033,0,0.0191273,"Missing"
P15-1038,J93-2004,0,0.0541048,"rted in these shared tasks are: labeled attachment score (LAS) – the percentage of predicted dependencies where the arc and the label are assigned correctly; unlabeled attachment score (UAS) – where the arc is assigned correctly; label accuracy score (LS) – where the label is assigned correctly; and exact match (EM) – the percentage of sentences whose predicted trees are entirely correct. Although shared tasks have been tremendously useful for advancing the state of the art in dependency parsing, most English evaluation has employed a single-genre corpus, the WSJ portion of the Penn Treebank (Marcus et al., 1993), so it is not immediately clear how these results genIntroduction Dependency parsing is a valuable form of syntactic processing for NLP applications due to its transparent lexicalized representation and robustness with respect to flexible word order languages. Thanks to over a decade of research on statistical dependency parsing, many dependency parsers are now publicly available. In this paper, we report on a comparative analysis of leading statistical dependency parsers using a multi-genre corpus. Our purpose is not to introduce a new parsing algorithm but to assess the performance of exist"
P15-1038,P13-2109,0,0.137596,"Missing"
P15-1038,W13-4917,1,0.529379,"Missing"
P15-1038,D07-1013,0,0.0163942,"6 11,467 10,976 8,969 1,634 1,366 WB 284,975 36,351 38,490 12,452 1,797 1,787 ALL 2,084,081 291,640 216,357 105,179 15,161 11,697 Table 1: Distribution of data used for our experiments. The first three/last three rows show the number of tokens/trees in each genre. BC: broadcasting conversation, BN: broadcasting news, MZ: news magazine, NW : newswire, PT : pivot text, TC : telephone conversation, WB : web text, ALL : all genres combined. eralize.1 Furthermore, a detailed comparative error analysis is typically lacking. The most detailed comparison of dependency parsers to date was performed by McDonald and Nivre (2007; 2011); they analyzed accuracy as a function of sentence length, dependency distance, valency, non-projectivity, part-of-speech tags and dependency labels.2 Since then, additional analyses of dependency parsers have been performed, but either with respect to specific linguistic phenomena (e.g. (Nivre et al., 2010; Bender et al., 2011)) or to downstream tasks (e.g. (Miwa and others, 2010; Petrov et al., 2010; Yuret et al., 2013)). 3 3.1 ture from the input, as these are often not available in non-annotated data. 3.2 Dependency Conversion OntoNotes provides annotation of constituency trees only"
P15-1038,E12-2021,0,0.0425666,"Missing"
P15-1038,J11-1007,0,0.0218317,"Missing"
P15-1038,W08-2121,0,0.021027,"Missing"
P15-1038,W10-1905,0,0.0467351,"Missing"
P15-1038,D11-1036,0,0.0159019,"ate non-projective dependencies. Development data ClearNLP, LTDP, SNN and Yara make use of the development data (for parameter tuning). Mate and Turbo self-tune parameter settings using the training data. The others were trained using their default/“standard” parameter settings. 5 D EPENDA BLE: Web-based Evaluation and Visualization Tool There are several very useful tools for evaluating the output of dependency parsers, including the venerable eval.pl18 script used in the CoNLL shared tasks, and newer Java-based tools that support visualization of and search over parse trees such as TedEval (Tsarfaty et al., 2011),19 MaltEval (Nilsson and Nivre, 2008)20 and “What’s wrong with my NLP?”.21 Recently, there is momentum towards web-based tools for annotation and visualization of NLP pipelines (Stenetorp and others, 2012). For this work, we used a new webbased tool, D EPENDA BLE, developed by the first author of this paper. It requires no installation and so provides a convenient way to evaluate and compare dependency parsers. The following are key features of D EPENDA BLE: Beam search ClearNLP, LTDP, Redshift and Yara have the option of different beam settings. The higher the beam size, the more accurate th"
P15-1038,nilsson-nivre-2008-malteval,0,0.0972031,"elopment data ClearNLP, LTDP, SNN and Yara make use of the development data (for parameter tuning). Mate and Turbo self-tune parameter settings using the training data. The others were trained using their default/“standard” parameter settings. 5 D EPENDA BLE: Web-based Evaluation and Visualization Tool There are several very useful tools for evaluating the output of dependency parsers, including the venerable eval.pl18 script used in the CoNLL shared tasks, and newer Java-based tools that support visualization of and search over parse trees such as TedEval (Tsarfaty et al., 2011),19 MaltEval (Nilsson and Nivre, 2008)20 and “What’s wrong with my NLP?”.21 Recently, there is momentum towards web-based tools for annotation and visualization of NLP pipelines (Stenetorp and others, 2012). For this work, we used a new webbased tool, D EPENDA BLE, developed by the first author of this paper. It requires no installation and so provides a convenient way to evaluate and compare dependency parsers. The following are key features of D EPENDA BLE: Beam search ClearNLP, LTDP, Redshift and Yara have the option of different beam settings. The higher the beam size, the more accurate the parser usually becomes, but typicall"
P15-1038,C10-1094,0,0.0281924,"Missing"
P15-1038,S14-2008,0,\N,Missing
P15-1038,W06-2920,0,\N,Missing
P15-1038,D07-1096,0,\N,Missing
P19-1047,P13-1025,0,0.0750404,"Missing"
P19-1047,D14-1148,0,0.055852,"Missing"
P19-1047,P10-4010,0,0.0800789,"Missing"
P19-1047,K17-1031,0,0.196311,"Missing"
P19-1047,W17-5102,0,0.0351482,"Missing"
P19-1047,P82-1020,0,0.749575,"Missing"
P19-1047,E17-1070,0,0.0582316,"Missing"
P19-1047,D18-1471,0,0.0472256,"Missing"
P19-1047,N06-2015,0,0.0111964,"tive sentiment 11 Negative sentiment Good morning, gentlemen. Nice job on the rebound quarter. And this is a slightly delicate question. With some of the terrible events that 0.33 0.15 have been happening, what is this duty or potential liability or cost of insurance? 12 Hedging It may vary Michael. So, some might be much better than that, but then 0.22 you got some of that – that’s not as much right. So, all-in, yeah. Table 3: Pragmatic features as highlighted tokens. Note, named entities are lexicalized (e.g. “4.6 million”). Feature numbers (No.) correspond to the text description in §4.1. (Hovy et al., 2006): (1) events, (2) numbers, (3) organizations/locations, (4) persons, and (5) products. We also calculate (6) a concreteness ratio: the number of named entities in the turn divided by the total number of tokens in the turn. Predicate-based temporal orientation. Temporal orientation is the emphasis individuals place on the past, present, or future. Previous work has shown correlations between “future intent” extracted from query logs and financial market volume volatility (Hasanuzzaman et al., 2016). We determine the temporal orientation of every predicate in a turn. We extract OpenIE predicates"
P19-1047,N18-1010,0,0.0343227,"Missing"
P19-1047,N09-1031,0,0.189462,"Missing"
P19-1047,Q16-1005,0,0.062478,"Missing"
P19-1047,N16-1041,0,0.0572171,"Missing"
P19-1047,W15-1203,0,0.069933,"Missing"
P19-1047,N16-1174,0,0.0265209,"Missing"
P19-1047,D17-1164,0,0.0396423,"Missing"
P19-1047,P17-1157,0,0.150959,"Missing"
P19-1047,J11-2001,0,0.222699,"erfection, enthrall, phenomenal accidents, recession, stagnant cheater, devastate, loathsome basically, generally, sometimes a little, kind of, more or less appears, could, possibly likely, probably, usually always, clearly, undoubtedly assume, deviate, turbulence bounded, earmark, indebted adjudicate, breach, felony, lawful LM T LM T PH PH LM LM LM LM LM LM Num. terms 354 2,507 2,353 3,692 79 39 27 14 19 297 184 903 Table 2: Detailed examples and the number of words for lexicons used as pragmatic features. LM is (Loughran and McDonald, 2011), PH is (Prokofieva and Hirschberg, 2014) and T is (Taboada et al., 2011). Feature numbers (No.) correspond to the text description in §4.1. No. Pragmatic Feat. Example Score 6 Concreteness Yes. Andrew for the quarter the total inter-company sales for 0.29 the first quarter was roughly 4.6 million and about 600,000 was related to medical, it was 4 million via DSS . 10 Positive sentiment 11 Negative sentiment Good morning, gentlemen. Nice job on the rebound quarter. And this is a slightly delicate question. With some of the terrible events that 0.33 0.15 have been happening, what is this duty or potential liability or cost of insurance? 12 Hedging It may vary Michae"
P19-1047,I13-1097,0,0.0620612,"Missing"
P19-1047,P14-1109,0,0.298064,"Missing"
P19-1047,D16-1177,0,0.0734637,"Missing"
P19-1047,P16-1150,0,\N,Missing
P98-2241,P98-2241,1,0.0522468,"forward-looking centers Cfn that includes all discourse entities in utterance n. Its first element is the &apos;preferred The Centering m o d e l The centering framework (Grosz et al., 1995) makes three main claims: 1) given an utterance Un, the * The authors would like to thank James Alien, Marflyn Walker, and the anonymous reviewers for many helpful comments on a preliminary draft of the paper. This material is based on work supported by NSF grant IRI-96-23665, ONR grant N00014-95-1-1088 and Columbia University grant OPG: 1307. IA more detailed report of this study is available as URCS TR #687 (Byron and Stent, 1998) center&apos;, Cpn. 2. A backward-looking center Cbn, the highest ranked element of C f n - 1 that is in Cfn. The framework defines a preference ordering on techniques for effecting a topic change, ranked according to the inference load each places on the addressee. The transitions are called &apos;shift&apos;, &apos;retain&apos; and &apos;continue&apos; and differ based on whether Cbn = Cbn+l and whether Cbn = Cpn. At the heart of the theory are two centering rules: Rule 1: If any member of Cfn is realized by a pronoun in Cfn+l, Cbn+l must be a pronoun. Rule 2: Sequences of continues are preferred over sequences of retains, an"
P98-2241,J95-2003,0,0.84938,"Missing"
P98-2241,P96-1036,0,0.245158,"ts utility for multiparty discourse has not been shown, and a variety of issues must be tackled to adapt the model for dialog. This paper reports our application of three naive models of centering theory for dialog. These results will be used as baselines for evaluating future models. 1 1 Introduction The centering framework (Grosz et al., 1995) is one of the most influential computational linguistics theories relating local focus to the form chosen for referring expressions. A number of studies have developed refinements and extensions of the theory (eg. Brennan et al., 1987; Kameyama, 1986; Strube and Hahn, 1996; Walker et al., 1998), but few have attempted to extend the model to multi-party discourse (cf. Brennan, 1998; Walker, 1998). For dialog systems, the benefits of using centering theory include improved reference resolution and generation of more coherent referring expressions. However, it is not at all clear how to adapt the theory for multi-party discourse. This paper examines some of the issues involved in adapting the theory, then describes the results of applying three alternative models to a corpus of 2-person dialogs. We chose very naive approximations to the original theory as a starti"
P98-2241,P87-1022,0,\N,Missing
P98-2241,P86-1031,0,\N,Missing
P99-1024,P97-1034,0,0.0161568,"e (OAA) (Martin et al., 1998). This architecture allows components to be developed independently, and then flexibly and dynamically combined to support distributed computation. Most of the agents that compose CommandTalk have been described elsewhere !for more detail, see (Moore et al., 1997)). This paper describes extensions to CommandTalk to support spoken dialogue. While we make no theoretical claims about the nature and structure of dialogue, we are influenced by the theoretical work of (Grosz and Sidner, 1986) and will use terminology from that tradition when appropriate. We also follow (Chu-Carroll and Brown, 1997) in distinguishing task initiative and dialogue initiative. Section 2 demonstrates the dialogue capabilities of CommandTalk by way of an extended example. Section 3 describes how language in CommandTalk is modeled for understanding and generation. Section 4 describes the architecture of the dialogue manager in detail. Section 5 compares CommandTalk with other spo* This research was supported by the Defense Advanced Research Projects Agency under Contract N66001-94-C6046 with the Space and Naval Warfare Systems Center. The views and conclusions contained in this document are those of the author"
P99-1024,P93-1008,1,0.266453,"ctive Bravo. The system then tests the guard, which succeeds because Objective Bravo now exists. The system therefore takes dialogue initiative by asking the operator in utterance 31 if that operator would like to carry out the original command. Although, in this case, the simulated world changed in direct response to a linguistic act, in general the world can change for a variety of reasons, including the operator's activities on the GUI or the activities of other operators. Language Interpretation and Generation The language used in CommandTalk is derived from a single grammar using Gemini (Dowding et al., 1993), a unification-based grammar formalism. This grammar is used to provide all the language modeling capabilities of the system, including the language model used in the speech recognizer, the syntactic and semantic interpretation of user utterances (Dowding et al., 1994), and the generation of system responses (Shieber et al., 1990). For speech recognition, Gemini uses the Nuance speech recognizer. Nuance accepts language models written in a Grammar Specification Language (GSL) format that allows context-free, as well as the more commonly used finite-state, models. 3 Using a technique described"
P99-1024,P94-1016,1,0.73156,"the simulated world changed in direct response to a linguistic act, in general the world can change for a variety of reasons, including the operator's activities on the GUI or the activities of other operators. Language Interpretation and Generation The language used in CommandTalk is derived from a single grammar using Gemini (Dowding et al., 1993), a unification-based grammar formalism. This grammar is used to provide all the language modeling capabilities of the system, including the language model used in the speech recognizer, the syntactic and semantic interpretation of user utterances (Dowding et al., 1994), and the generation of system responses (Shieber et al., 1990). For speech recognition, Gemini uses the Nuance speech recognizer. Nuance accepts language models written in a Grammar Specification Language (GSL) format that allows context-free, as well as the more commonly used finite-state, models. 3 Using a technique described in (Moore, 1999), we compile a contextfree covering grammar into GSL format from the main Gemini grammar. This approach of using a single grammar source for both sides of the dialogue has several advantages. First, although there are differences between the language us"
P99-1024,P98-2129,0,0.0224411,"xed-initiative dialogues, and dialogues involving multi-modality. 5 Related Work C o m m a n d T a l k differs from other recent spoken language systems in that it is a c o m m a n d and control application. It provides a particularly 188 interesting environment in which to design spoken dialogue systems in that it supports distributed stochastic simulations, in which one operator controls a certain collection of forces while other operators simultaneously control other allied a n d / o r opposing forces, and unexpected events can occur that require responses in real time. Other applications (Litman et al., 1998; Walker et al., 1998) have been in domains that were sufficiently limited (e.g., queries about train schedules, or reading email) that the system could presume much about the user's goals, and make significant contributions to task initiative. However, the high number of possible commands available in CommandTalk, and the more abstract nature of the user's high-level goals (to carry out a simulation of a complex military engagement) preclude the system from taking significant task initiative in most cases. The system most closely related to CommandTalk in terms of dialogue use is TRIPS (Fergu"
P99-1024,H92-1003,0,0.019807,"s that it has been impossible to perform a formal evaluation of the system. This is due to the difficulty of collecting data in this domain, which requires speakers who are both knowledgeable about the domain and familiar with ModSAF. CommandTalk has been used in simulations of real military exercises, but those exercises have always taken place in classified environments where data collection is not permitted. To facilitate such an evaluation, we are currently porting the CommandTalk dialogue manager to the domain of air travel planning. There is a large body of existing data in that domain (MADCOW, 1992), and speakers familiar with the domain are easily available. The internal representation of actions in CommandTalk is derived from ModSAF. We would like to port that to a domain-independent representation such as frames or explicit representations of plans. Finally, there are interesting options regarding the finite state model. We are investigating other representations for the semantic contents of a discourse segment, such as frames or active templates. 7 Acknowledgments We would like to thank Andrew Kehler, David Israel, Jerry Hobbs, and Sharon Goldwater for comments on an earlier version"
P99-1024,P98-2136,0,0.0164003,"task initiative in most cases. The system most closely related to CommandTalk in terms of dialogue use is TRIPS (Ferguson and Allen, 1998), although there are several important differences. In contrast to TRIPS, in CommandTalk gestures are fully incorporated into the dialogue state. Also, CommandTalk provides the same language capabilities for user and system utterances. Unlike other simulation systems, such as QuickSet (Cohen et al., 1997), CommandTalk has extensive dialogue capabilities. In QuickSet, the user is required to confirm each spoken utterance before it is processed by the system (McGee et al., 1998). Our earlier work on spoken dialogue in the air travel planning domain (Bratt et al., 1995) (and related systems) interpreted speaker utterances in context, but did not support structured dialogues. The technique of using dialogue context to control the speech recognition state is similar to one used in (Andry, 1992). 6 Future Work We have discussed some aspects of CommandTalk that make it especially suited to handle different kinds of interactions. We have looked at the use of a dialogue stack, salience information, and focus spaces to assist interpretation and generation. We have seen that"
P99-1024,A97-1001,0,0.0757605,"dSAF battlefield simulator that allows simulation operators to generate and execute military exercises by creating forces and control measures, assigning missions to forces, and controlling the display (Ceranowicz, 1994). CommandTalk consists of independent, cooperating agents interacting through SRI's Open Agent Architecture (OAA) (Martin et al., 1998). This architecture allows components to be developed independently, and then flexibly and dynamically combined to support distributed computation. Most of the agents that compose CommandTalk have been described elsewhere !for more detail, see (Moore et al., 1997)). This paper describes extensions to CommandTalk to support spoken dialogue. While we make no theoretical claims about the nature and structure of dialogue, we are influenced by the theoretical work of (Grosz and Sidner, 1986) and will use terminology from that tradition when appropriate. We also follow (Chu-Carroll and Brown, 1997) in distinguishing task initiative and dialogue initiative. Section 2 demonstrates the dialogue capabilities of CommandTalk by way of an extended example. Section 3 describes how language in CommandTalk is modeled for understanding and generation. Section 4 describ"
P99-1024,P98-2219,0,0.00572617,"ues, and dialogues involving multi-modality. 5 Related Work C o m m a n d T a l k differs from other recent spoken language systems in that it is a c o m m a n d and control application. It provides a particularly 188 interesting environment in which to design spoken dialogue systems in that it supports distributed stochastic simulations, in which one operator controls a certain collection of forces while other operators simultaneously control other allied a n d / o r opposing forces, and unexpected events can occur that require responses in real time. Other applications (Litman et al., 1998; Walker et al., 1998) have been in domains that were sufficiently limited (e.g., queries about train schedules, or reading email) that the system could presume much about the user's goals, and make significant contributions to task initiative. However, the high number of possible commands available in CommandTalk, and the more abstract nature of the user's high-level goals (to carry out a simulation of a complex military engagement) preclude the system from taking significant task initiative in most cases. The system most closely related to CommandTalk in terms of dialogue use is TRIPS (Ferguson and Allen, 1998),"
P99-1024,H93-1008,1,\N,Missing
P99-1024,C98-2214,0,\N,Missing
P99-1024,J86-3001,0,\N,Missing
P99-1024,C98-2124,0,\N,Missing
P99-1024,P89-1002,1,\N,Missing
P99-1024,C98-2131,0,\N,Missing
S13-2004,W05-0402,0,0.0709332,"Missing"
S13-2004,N12-1049,0,0.0128057,"rt et al., 2011) opensource NLP tools. From the Stanford CoreNLP tools we obtained a tokenization of the input text, the lemma and part of speech (POS) tag for each token, and dependency and constituency parses for each sentence. From SENNA, we obtained a semantic role labelling for each sentence. 3 Approach We were curious to explore the tradeoff between additional context on the one hand, and additional layers of representation on the other, for the event and time expression extraction tasks. Researchers have investigated the impacts of different sets of features (Adafre and de Rijke, 2005; Angeli et al., 2012; 20 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 20–24, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Feature type Lexical 1 Features token Lexical 2 Part of speech Dependency lemma POS tag governing verb, governing verb POS, governing preposition, phrase tag, path to root of parse tree, head word, head word lemma, head word POS governing verb, governing verb POS, governing preposition, phrase tag, path to root of parse tree semantic role label"
S13-2004,S10-1063,0,0.13059,"Missing"
S13-2004,llorens-etal-2012-timen,0,0.0120821,"all but a few (erroneous) cases in the silver data, for determining the event features, we used the same features as before, with the single addition of the event class (during testing, we used the dynamically assigned event class from the event segmentation classifier). As before, we experimented with ATT1, ATT2, and ATT3 models. TempEval3 only includes evaluation of tense and aspect features, so we only report for those. The tags assigned by each classifier are listed in Table 3. 6 Time Normalization To compute TIMEX3 standard based values for extracted time expressions, we used the TIMEN (Llorens et al., 2012) and TRIOS (UzZaman and Allen, 2010) time normalizers. Values from the normalizers were validated in post-processing (e.g. “T2445” is invalid) and, when the normalizers returned different non-nil values, TIMEN’s values were selected without further reasoning. Time normalization was out of scope in our research for this evaluation, but it remains as part of our future work. 7 Results and Discussion Our results for event segmentation/classification on the TempEval-3 test data are provided in Table 4. The absence of semantic features causes only small changes in F1. The absence of syntactic featu"
S13-2004,S10-1062,0,0.146085,"om Abstract AQUAINT TimeBank TE3-Silver In this paper we present the results of experiments comparing (a) rich syntactic and semantic feature sets and (b) big context windows, for the TempEval time expression and event segmentation and classification tasks. We show that it is possible for models using only lexical features to approach the performance of models using rich syntactic and semantic feature sets. 1 Type gold gold silver Files 73 183 2452 EVENT 4431 6698 81329 TIMEX 579 1243 12739 Table 1: Frequency of event and time expressions in the text portions of the TempEval-3 data sets 2010; UzZaman and Allen, 2010)). In this work, we show that with large windows of context, it is possible for models using only lexical features to approach the performance of models using rich syntactic and semantic feature sets. Introduction TempEval-3 Temporal Annotation Task (UzZaman et al., 2012) has three subtasks: A Time expression extraction and classification extract time expressions from input text, and determine the type and normalised value for each extracted time expression. B Event extraction and classification - extract event mentions from input text, and determine the class, tense and aspect features for ea"
W00-1433,J86-3001,0,0.0867177,"Missing"
W00-1433,J93-4004,0,0.0896332,"o the TRAINS corpus (Heeman and Allen, 1995) and the HCRC Mapta~sk corpus (Anderson et al., 1991). 247 we can program a conversational agent to produce something similar. Therefore, in examining our dialogs the question we must answer is ""Why did this speaker produce this?"". RST is a descriptive theory of hierarchical structure in discourse that identifies functional relationships between discourse parts based on the intentions behind their production (Mann and Thompson, 1987). It has been used in content planning systems for text (effectively text monolog) (e.g. (Cawsey, 1993), (How, 1993), (Moore and Paris, 1993)). It has not yet been used much in content planning for spoken dialog. Because the dialogs we are examining are taskoriented, they are hierarchically structured and so provide a natural place to use RST. In fact, in order to uncover the full structure behind discourse contributions, it is necessary for us to use a model of rhetorical structure. Certain dialog contributions are explained by the speaker&apos;s rhetorical goals, rather than by task goals. In example 1, utterance 3 is justification for utterance 1 but does not directly contribute to completing the task. Example 1 A 1 They can&apos;t fix th"
W00-1433,J92-4007,0,0.0821859,"Missing"
W00-1433,J96-3006,0,0.0645815,", Nakatani and Traum describe a hierarWe would also like more information, at times, chical annotation of dialog for I-units, based on the about the subject matter in the spans of a relation. .. domination and satisfaction-precedence relations of The relation between a ""When"" question and a n (Grosz and Sidner, 1986). Other researchers have swer is question-answer, as is that between a ""Why"" shown that Grosz and Sidner&apos;s model of discourse question and answer; but the first question-answer structure (GST) and RST are similar in many reforms part of an elaboration and the second forms spects [(Moser and Moore, 1996), (Marcu, 1999)]. part of a justification or motivation. In our ammtaHowever, RST provides more specific relations than tion scheme, we supply a list of content types, such GST, and this is useful for content planning. As as time. location and number. The annotator adds well as helping to specify generation goals, content the content type in I)arentheses after the relation tag and ordering constraints, the rhetorical information when required. This means that the annotator may is needed in case the system has to explain what it. have to mark three items for a given set of spans: &apos;the has said."
W08-1133,C00-1007,1,0.744769,"or both TUNA domains, while the use of recency constraints was not as effective for TUNA-style tasks. We then modified Dale and Reiter’s classic attribute selection algorithm (Dale and Reiter, 1995) to model speakerspecific constraints, and found performance gains in this more greedy approach as well. Then we looked at surface realization for referring expression generation. There are several approaches to surface realization described in the literature (Reiter and Dale, 2000) ranging from hand-crafted template-based realizers to data-driven syntax-based realizers (Langkilde and Knight, 2000; Bangalore and Rambow, 2000). Template-based realization involves the insertion of attribute values into predetermined templates. Data-driven syntax-based methods use syntactic relations between words (including long-distance relations) for word ordering. Other data-driven techniques exhaustively generate possible realizations with recourse to syntax in as much as it is reflected in local n-grams. Such techniques have the advantage of being robust although they are inadequate to capture long-range dependencies. In this paper, we explore three techniques for the task of referring expression generation that are different h"
W08-1133,2007.mtsummit-ucnlg.14,0,0.121161,"ng expression generation. However, most of this work did not take into account: a) stylistic differences between speakers; or b) trainable surface realization approaches that combine semantic and word order information. In this paper we describe and evaluate several end-to-end referring expression generation algorithms that take into consideration speaker style and use data-driven surface realization techniques. 1 Introduction There now exist numerous general-purpose algorithms for attribute selection used in referring expression generation (e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Belz and Gatt, 2007)). However, these algorithms by-and-large focus on the algorithmic aspects of referring expression generation rather than on psycholinguistic factors that influence language production. For example, we know that humans exhibit individual style differences during language production that can be quite pronounced (e.g. (Belz, 2007)). We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again (Levelt, 1989). In this paper, we first explore the impact of individual style and priming on"
W08-1133,N07-1021,0,0.0229201,"sideration speaker style and use data-driven surface realization techniques. 1 Introduction There now exist numerous general-purpose algorithms for attribute selection used in referring expression generation (e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Belz and Gatt, 2007)). However, these algorithms by-and-large focus on the algorithmic aspects of referring expression generation rather than on psycholinguistic factors that influence language production. For example, we know that humans exhibit individual style differences during language production that can be quite pronounced (e.g. (Belz, 2007)). We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again (Levelt, 1989). In this paper, we first explore the impact of individual style and priming on attribute selection for referring expression generation. To get an idea of the potential improvement when modeling these factors, we implemented a version of full brevity search (Dale, 1992) that uses speaker-specific constraints, and another version that also uses recency constraints. We found that using speaker-specific constr"
W08-1133,W05-0831,0,0.0208369,".01 .01 0 .02 Table 2: Results for realization rate speaker constraints, we again see a performance jump, although compared to the best possible case (full brevity) there is still room for improvement. Table 1: Results for attribute selection Unfortunately, the number of states of the minimal permutation automaton of even a linear automata (finite-state machine representation of a string) grows exponentially with the number of words of the string. So, instead of creating a full permutation automaton, we choose to constrain permutations to be within a local window of adjustable size (also see (Kanthak et al., 2005)). 4 Attribute Selection Experiments Data Preparation The training data were used to build the models outlined above. The development data were then processed one-by-one. For our final submissions, we use training and development data to build our models. Results Table 1 shows the results for variations of full brevity. As we would expect, all approaches achieve a perfect score on uniqueness. For both corpora, we see a large performance jump when we use speaker constraints. However, when we incorporate recency constraints as well performance declines slightly. We think this is due to two facto"
W08-1133,J03-1003,0,0.2085,"Missing"
W08-1133,A00-2023,0,0.0173701,"d to big performance gains for both TUNA domains, while the use of recency constraints was not as effective for TUNA-style tasks. We then modified Dale and Reiter’s classic attribute selection algorithm (Dale and Reiter, 1995) to model speakerspecific constraints, and found performance gains in this more greedy approach as well. Then we looked at surface realization for referring expression generation. There are several approaches to surface realization described in the literature (Reiter and Dale, 2000) ranging from hand-crafted template-based realizers to data-driven syntax-based realizers (Langkilde and Knight, 2000; Bangalore and Rambow, 2000). Template-based realization involves the insertion of attribute values into predetermined templates. Data-driven syntax-based methods use syntactic relations between words (including long-distance relations) for word ordering. Other data-driven techniques exhaustively generate possible realizations with recourse to syntax in as much as it is reflected in local n-grams. Such techniques have the advantage of being robust although they are inadequate to capture long-range dependencies. In this paper, we explore three techniques for the task of referring expression ge"
W08-1133,W04-3308,0,0.0199772,"a template for an input attribute list it is quite likely to be coherent. At generation time, we find all possible realizations of each attribute in the input attribute set, and fill in each possible template with each combination of the attribute realizations. We report results for two versions of this realizer: one with speakerspecific lexicon and templates (Template-S), and one without (Template). Dependency-Based Realizer To construct our dependency-based realizer, we first parse all the word strings from the training data using the dependency parser described in (Bangalore et al., 2005; Nasr and Rambow, 2004). Then, for every pair of words wi , wj that occur in the same referring expression (RE) in the training data, we compute: f req(i &lt; j), the frequency with which wi precedes wj in any RE; f req(i = j − 1), the frequency with which wi immediately precedes wj in any RE; f req(dep(wi , wj ) ∧ i &lt; j), the frequency with which wi depends on and precedes wj in any RE, and f req(dep(wi , wj ) ∧ j &lt; i), the frequency with which wi depends on and follows wj in any RE. At generation time, we find all possible realizations of each attribute in the input attribute set, and for each combination of attribut"
W08-2120,A00-2023,0,0.0252409,"Natural Language Learning, pages 151–158 Manchester, August 2008 cency constraints was not as effective for TUNAstyle tasks. We then modified Dale and Reiter’s classic attribute selection algorithm (Dale and Reiter, 1995) to model individual differences in style, and found performance gains in this more greedy approach as well. Then, we look at surface realization for referring expression generation. There are several approaches to surface realizations described in the literature (Reiter and Dale, 2000) ranging from hand-crafted template-based realizers to data-driven syntax-based realizers (Langkilde and Knight, 2000; Bangalore and Rambow, 2000). Template-based realization provides a straightforward method to fill out pre-defined templates with the current attribute values. Data-driven syntaxbased methods employ techniques that incorporate the syntactic relations between words which can potentially go beyond local adjacency relations. Syntactic information also helps in eliminating ungrammatical sentence realizations. At the other extreme, there are techniques that exhaustively generate possible realizations with recourse to syntax in as much as it is reflected in local n-grams. Such techniques have the a"
W08-2120,W04-3308,0,0.0243709,"s a weighted finite-state automaton. The weights are computed from the prior probability of each template and the prior probability of each lexical item realizing an attribute (Equation 2). We have two versions of this realizer: one with speaker-specific lexicons and templates (Template-S), and one without (Template). We report results for both. P (T |AS) = ! t P (t|AS) ∗ &quot;! a∈t P (l|a, t) (2) l 4.1.2 Dependency-Based Realizer To construct our dependency-based realizer, we first parse all the word strings from the training data using the dependency parser described in (Bangalore et al., 2005; Nasr and Rambow, 2004). Then, for every pair of words wi , wj that occur in the same referring expression (RE) in the training data, we compute: f req(i &lt; j), the frequency with which wi precedes wj in any RE; f req(dep(wi , wj ) ∧ i &lt; j), the frequency with which wi depends on and precedes wj in any RE, and f req(dep(wi , wj )∧j &lt; i), the frequency with which wi depends on and follows wj in any RE. At generation time, we find all possible realizations of each attribute value in the input attribute set, and for each combination of attribute realizations, we find the most likely set of dependencies and precedences g"
W08-2120,2001.mtsummit-papers.68,0,0.0265149,"e number of words that would have to be added, deleted, or replaced in order to transform the generated referring expression into the one produced by the human. As used in the REG 2008 shared challenge, it is unnormalized, so its values range from zero up. Accuracy (ACC) is binary-valued: 1 if the generated referring expression is identical to that produced by the human (after spelling correction and normalization), and 0 otherwise. Bleu is an n-gram based metric that counts the number of 1, 2 and 3 grams shared between the generated string and one or more (preferably more) reference strings (Papenini et al., 2001). Bleu values are normalized and range from 0 (no match) to 1 (perfect match). Finally, the NIST metric is a variation on the Bleu metric that, among other things, weights rare n-grams higher than frequently-occurring ones (Doddington, 2002). NIST values are unnormalized. 156 SED ACC Bleu NIST Furniture FB-sf DR-sf FB-sf DR-sf FB-sf DR-sf FB-sf DR-sf Permute&Rank 3.97 4.22 0.09 0.06 .291 .242 3.82 3.32 Dependency 4.80 5.03 0.04 0.03 .193 .105 3.32 2.46 Dependency-S 4.71 4.88 0.06 0.04 .201 .157 3.74 3.26 Template 3.89 4.56 0.09 0.05 .283 .213 3.48 3.22 Template-S 3.26 3.90 0.19 0.12 .362 .294"
W08-2120,C00-1007,1,0.736757,"pages 151–158 Manchester, August 2008 cency constraints was not as effective for TUNAstyle tasks. We then modified Dale and Reiter’s classic attribute selection algorithm (Dale and Reiter, 1995) to model individual differences in style, and found performance gains in this more greedy approach as well. Then, we look at surface realization for referring expression generation. There are several approaches to surface realizations described in the literature (Reiter and Dale, 2000) ranging from hand-crafted template-based realizers to data-driven syntax-based realizers (Langkilde and Knight, 2000; Bangalore and Rambow, 2000). Template-based realization provides a straightforward method to fill out pre-defined templates with the current attribute values. Data-driven syntaxbased methods employ techniques that incorporate the syntactic relations between words which can potentially go beyond local adjacency relations. Syntactic information also helps in eliminating ungrammatical sentence realizations. At the other extreme, there are techniques that exhaustively generate possible realizations with recourse to syntax in as much as it is reflected in local n-grams. Such techniques have the advantage of being robust alth"
W08-2120,P04-1052,0,0.0122269,"ibution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. parts of an NLG system. Traditionally, this problem is split into two parts. The task of selecting the attributes to use in referring to an entity is the attribute selection task, performed during content planning or sentence planning. The actual construction of the referring expression is part of surface realization. There now exist numerous general-purpose algorithms for attribute selection (e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Belz and Gatt, 2007; Siddharthan and Copestake, 2004)). However, these algorithms by-and-large focus on the algorithmic aspects of referring expression generation rather than on psycholinguistic factors that influence language production. For example, we know that humans exhibit individual differences in language production that can be quite pronounced (e.g. (Belz, 2007)). We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again (Levelt, 1989). In this paper, we look at attribute selection and surface realization for referring expr"
W08-2120,2007.mtsummit-ucnlg.14,0,0.0219509,"Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. parts of an NLG system. Traditionally, this problem is split into two parts. The task of selecting the attributes to use in referring to an entity is the attribute selection task, performed during content planning or sentence planning. The actual construction of the referring expression is part of surface realization. There now exist numerous general-purpose algorithms for attribute selection (e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Belz and Gatt, 2007; Siddharthan and Copestake, 2004)). However, these algorithms by-and-large focus on the algorithmic aspects of referring expression generation rather than on psycholinguistic factors that influence language production. For example, we know that humans exhibit individual differences in language production that can be quite pronounced (e.g. (Belz, 2007)). We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again (Levelt, 1989). In this paper, we look at attribute selection and surf"
W08-2120,N07-1021,0,0.0151406,"ng or sentence planning. The actual construction of the referring expression is part of surface realization. There now exist numerous general-purpose algorithms for attribute selection (e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Belz and Gatt, 2007; Siddharthan and Copestake, 2004)). However, these algorithms by-and-large focus on the algorithmic aspects of referring expression generation rather than on psycholinguistic factors that influence language production. For example, we know that humans exhibit individual differences in language production that can be quite pronounced (e.g. (Belz, 2007)). We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again (Levelt, 1989). In this paper, we look at attribute selection and surface realization for referring expression generation using the TUNA corpus 1 , an annotated corpus of human-produced referring expressions that describe furniture and people. We first explore the impact of individual style and priming on attribute selection for referring expression generation. To get an idea of the potential improvement when modeling th"
W08-2120,W05-0831,0,0.0196036,"possible surface realizations. In general, the number of states of the minimal permutation automaton of even a linear automaton (finite-state representation of a string) grows exponentially with the number of words of the string. Although creating the full permutation automaton for full natural language generation tasks could be computationally prohibitive, most attribute sets in our two domains contain no more than five attributes. So we choose to explore the full permutation space. A more general approach might constrain permutations to be within a local window of adjustable size (also see (Kanthak et al., 2005)). Figure 3 shows the minimal permutation automaton for an input sequence of 4 words and a window size of 2. Each state of the automaton is indexed by a bit vector of size equal to the number of words/phrases of the target sentence. Each bit of the bit vector is set to 1 if the word/phrase in that bit position is used on any path from the initial to the current state. The next word for permutation from a given state is restricted to be within the window size (2 in our case) positions counting from the first as-yet uncovered position in that state. For example, the state indexed with vector “10"
W08-2120,J03-1003,0,0.406206,"Missing"
W08-2120,P02-1040,0,\N,Missing
W09-0506,P98-1122,0,0.0508899,"for Computational Linguistics 42 2 Related Work 1 Sys When a dialog system requests a confirmation, the user’s subsequent corrections and topic change utterances are particularly likely to be misrecognized. Considerable research has now been done on the automatic detection of spoken corrections. Linguistic cues to corrections include the number of words in the post-confirmation utterance and the use of marked word order (Krahmer et al., 2001). Prosodic cues include F0 max, RMS max, RMS mean, duration, speech tempo, and percentage of silent frames(Litman et al., 2006; Hirschberg et al., 2004; Levow, 1998). Discourse cues include the removal, repetition, addition or modification of a concept, the system’s dialog act type, and information about error rates in the dialog so far (Krahmer et al., 2001; et al., 2002; Litman et al., 2006; Walker et al., 2000). In our experiments, we use most of these features as well as additional lexical features. We can use knowledge of the type or content of a user utterance to modify system behavior. For example, in this paper we use the concept type(s) in the user’s utterance to adapt the recognizer’s LM. It is now common practice to adapt the recognizer to the"
W09-0506,J06-3004,0,0.378514,"thens, Greece, 30 March 2009. 2009 Association for Computational Linguistics 42 2 Related Work 1 Sys When a dialog system requests a confirmation, the user’s subsequent corrections and topic change utterances are particularly likely to be misrecognized. Considerable research has now been done on the automatic detection of spoken corrections. Linguistic cues to corrections include the number of words in the post-confirmation utterance and the use of marked word order (Krahmer et al., 2001). Prosodic cues include F0 max, RMS max, RMS mean, duration, speech tempo, and percentage of silent frames(Litman et al., 2006; Hirschberg et al., 2004; Levow, 1998). Discourse cues include the removal, repetition, addition or modification of a concept, the system’s dialog act type, and information about error rates in the dialog so far (Krahmer et al., 2001; et al., 2002; Litman et al., 2006; Walker et al., 2000). In our experiments, we use most of these features as well as additional lexical features. We can use knowledge of the type or content of a user utterance to modify system behavior. For example, in this paper we use the concept type(s) in the user’s utterance to adapt the recognizer’s LM. It is now common p"
W09-0506,C98-1117,0,\N,Missing
W09-3921,N09-2048,1,0.649676,"hat human dialog partners exhibit lexical and syntactic convergence; that is, that in a human-human conversation the participants become more similar in their use of language over time (Brennan and Clark, 1996; Lockridge and Brennan, 2002; Pickering and others, 2000; Reitter et al., 2006). Several Wizard-of-Oz studies have also shown evidence of convergence in human-computer dialog (Branigan and others, 2003; Brennan, 1996; Gustafson and others, 1997). In recent work, we examined user adaptation1 to the system’s choice of verb and preposition using the deployed Let’s Go! spoken dialog system (Stoyanchev and Stent, 2009a). This was the first study to look at convergence with real users of a real dialog system and examined user adaptation to verbs and prepositions. The study described in this paper is a follow-on to our previous study. 1 In this paper, we use the term adaptation to indicate directional convergence, e.g. user adaptation to a system. We make no claims about the psycholinguistic models underlying this adaptation. Here we look at user adaptation to the system’s choice of realization of task-related concepts. In this paper, we: (1) Confirm our previous results showing that users adapt to the syste"
W09-3921,W09-0506,1,0.65532,"hat human dialog partners exhibit lexical and syntactic convergence; that is, that in a human-human conversation the participants become more similar in their use of language over time (Brennan and Clark, 1996; Lockridge and Brennan, 2002; Pickering and others, 2000; Reitter et al., 2006). Several Wizard-of-Oz studies have also shown evidence of convergence in human-computer dialog (Branigan and others, 2003; Brennan, 1996; Gustafson and others, 1997). In recent work, we examined user adaptation1 to the system’s choice of verb and preposition using the deployed Let’s Go! spoken dialog system (Stoyanchev and Stent, 2009a). This was the first study to look at convergence with real users of a real dialog system and examined user adaptation to verbs and prepositions. The study described in this paper is a follow-on to our previous study. 1 In this paper, we use the term adaptation to indicate directional convergence, e.g. user adaptation to a system. We make no claims about the psycholinguistic models underlying this adaptation. Here we look at user adaptation to the system’s choice of realization of task-related concepts. In this paper, we: (1) Confirm our previous results showing that users adapt to the syste"
W09-3941,W08-0111,0,0.13496,"less frequent ones in the newspaper domain may be more “correct” in another domain. Our purpose is to lay out sentence plan construction possibilities, not to reproduce the WSJ authorial voice. Second, because SPaRKy is a twostage sentence planner and we are focusing here on sentence plan construction, we can only evaluate the local decisions made during that stage, not the overall quality of SPaRKy’s output. Evaluations of sentence planning tasks for datato-text generation have tended to focus solely on discourse cues (e.g. (Eugenio et al., 1997; Grote and Stede, 1998; Moser and Moore, 1995; Nakatsu, 2008; Taboada, 2006)). By contrast, we want good coverage for all core sentence planning tasks. Although Walker et al. performed an evaluation of SPaRKy (Stent et al., 2004; Walker et al., 2007), they evaluated the output from the sentence planner as a whole, rather than evaluating each stage separately. Williams and Reiter, in the work most similar to ours, examined a subset of the RST-DT corpus to see if they could use it to perform span ordering, punctuation selection, and discourse cue selection and placement. However, they assumed that surface realization was already complete, so they used le"
W09-3941,J05-3002,0,0.069436,"r news-style text generation. On Evaluation There are two basic approaches to NLG, text-totext generation (in which a model learned from a text corpus is applied to produce new texts from text input) and data-to-text generation (in which non-text input is converted into text output). In text-to-text generation, there has been considerable work on sentence fusion and information ordering, which are partly sentence planning tasks. For evaluation, researchers typically compare automatically produced text to the original humanproduced text, which is assumed to be “correct” (e.g. (Karamanis, 2007; Barzilay and McKeown, 2005; Marsi and Krahmer, 2005)). However, an evaluation that considers the only “correct” answer for a sentence planning task to be the answer in the original text is overly harsh. First, although we assume that all the possibilities in the human-produced text are “reasonable”, some may be awkward or incorrect for particular domains, while other less frequent ones in the newspaper domain may be more “correct” in another domain. Our purpose is to lay out sentence plan construction possibilities, not to reproduce the WSJ authorial voice. Second, because SPaRKy is a twostage sentence planner and we a"
W09-3941,H01-1055,0,0.0200401,"can be difficult. In this paper, we automatically extract sentence plan construction rules from the RST-DT corpus. In our rules, we use only domainindependent features that are available to a sentence planner at runtime. We evaluate these rules, and outline ways in which they can be used for sentence planning. We have integrated them into a revised version of SPaRKy. 1 Introduction Most natural language generation (NLG) systems have a pipeline architecture consisting of four core stages: content selection, discourse planning, sentence planning, and surface realization (Reiter and Dale, 2000; Rambow et al., 2001). A sentence planner maps from an input discourse plan to an output sentence plan. As part of this process it performs several tasks, including sentence ordering, sentence aggregation, discourse cue insertion and perhaps referring expression generation (Stent et al., 2004; Walker et al., 2007; Williams and Reiter, 2003). The developer of a sentence planner must typically write rules by hand (e.g. (Stent et al., 2004; Walker et al., 2007)) or learn a domainspecific model from a corpus of training data (e.g. (Williams and Reiter, 2003)). Unfortunately, there are very few corpora annotated with d"
W09-3941,P97-1011,0,0.117998,"Missing"
W09-3941,P04-1011,1,0.942276,"hey can be used for sentence planning. We have integrated them into a revised version of SPaRKy. 1 Introduction Most natural language generation (NLG) systems have a pipeline architecture consisting of four core stages: content selection, discourse planning, sentence planning, and surface realization (Reiter and Dale, 2000; Rambow et al., 2001). A sentence planner maps from an input discourse plan to an output sentence plan. As part of this process it performs several tasks, including sentence ordering, sentence aggregation, discourse cue insertion and perhaps referring expression generation (Stent et al., 2004; Walker et al., 2007; Williams and Reiter, 2003). The developer of a sentence planner must typically write rules by hand (e.g. (Stent et al., 2004; Walker et al., 2007)) or learn a domainspecific model from a corpus of training data (e.g. (Williams and Reiter, 2003)). Unfortunately, there are very few corpora annotated with discourse 2 Sentence Planning in SPaRKy The only publicly available sentence planner for data-to-text generation is SPaRKy (Stent et al., 2004). SPaRKy takes as input a discourse plan (a tree with rhetorical relations on the internal nodes and a proposition representing a"
W09-3941,W09-0613,0,0.020635,"g comma, semicolon or N/A as full. 6.3.3 7 One way to use these results would be to model the sentence planning task as a cascade of classifiers, but this method does not permit the system developer to add his or her own rules. So we continue to use SPaRKy, which is rule-based. We have made several changes to the Java version of SPaRKy to support application of our sentence plan construction rules. We modified the classes for storing and managing rules to read our XML rule format and process rule conditions and patterns. We stripped out the dependence on RealPro and added hooks for SimpleNLG (Gatt and Reiter, 2009). We modified the rule application algorithm so that users can choose to use a single rule set with patterns covering all three sentence planning tasks, or one rule set for each sentence planning task. Also, since there are now many rules, we give the user the option to specify which relations jSPaRKy should load rules for at each run. Discourse cue selection We have one input feature vector for each relation instance having two children. We use the same features as in the previous experiment, and as in the previous experiment, we order the child spans as they appear in the data. The pattern i"
W09-3941,W98-1414,0,0.038031,"incorrect for particular domains, while other less frequent ones in the newspaper domain may be more “correct” in another domain. Our purpose is to lay out sentence plan construction possibilities, not to reproduce the WSJ authorial voice. Second, because SPaRKy is a twostage sentence planner and we are focusing here on sentence plan construction, we can only evaluate the local decisions made during that stage, not the overall quality of SPaRKy’s output. Evaluations of sentence planning tasks for datato-text generation have tended to focus solely on discourse cues (e.g. (Eugenio et al., 1997; Grote and Stede, 1998; Moser and Moore, 1995; Nakatsu, 2008; Taboada, 2006)). By contrast, we want good coverage for all core sentence planning tasks. Although Walker et al. performed an evaluation of SPaRKy (Stent et al., 2004; Walker et al., 2007), they evaluated the output from the sentence planner as a whole, rather than evaluating each stage separately. Williams and Reiter, in the work most similar to ours, examined a subset of the RST-DT corpus to see if they could use it to perform span ordering, punctuation selection, and discourse cue selection and placement. However, they assumed that surface realization"
W09-3941,A97-1039,0,0.470891,"Missing"
W09-3941,J93-2004,0,0.0330303,"propositions have been fairly well fleshed-out, so that one has information about predicate-argument structure, tense, and the information status of entities to be realized. A relation has a label as well as one or more child text spans. The features we extract from our data include both per-span and per-relation features. In our experiments we use a subset of these features which is fairly domain-independent and does not overly partition our data. The complete set of features (full) is as well as our reduced set are given in Table 1. Data We use the Wall Street Journal Penn Treebank corpus (Marcus et al., 1993), which is a corpus of text annotated for syntactic structure. We also use two additional annotations done on (parts of) that corpus: PropBank (Kingsbury and Palmer, 2003), which consists of annotations for predicateargument structure; and the RST-DT (Carlson et al., 2002), which consists of annotations for rhetorical structure. We had to process this data into a form suitable for feature extraction. First, we produced a flattened form of the syntactic annotations, in which 2 The Penn Treebank and the RST-DT segment words and punctuation slightly differently, which makes it hard to align the v"
W09-3941,W05-1612,0,0.0237731,". On Evaluation There are two basic approaches to NLG, text-totext generation (in which a model learned from a text corpus is applied to produce new texts from text input) and data-to-text generation (in which non-text input is converted into text output). In text-to-text generation, there has been considerable work on sentence fusion and information ordering, which are partly sentence planning tasks. For evaluation, researchers typically compare automatically produced text to the original humanproduced text, which is assumed to be “correct” (e.g. (Karamanis, 2007; Barzilay and McKeown, 2005; Marsi and Krahmer, 2005)). However, an evaluation that considers the only “correct” answer for a sentence planning task to be the answer in the original text is overly harsh. First, although we assume that all the possibilities in the human-produced text are “reasonable”, some may be awkward or incorrect for particular domains, while other less frequent ones in the newspaper domain may be more “correct” in another domain. Our purpose is to lay out sentence plan construction possibilities, not to reproduce the WSJ authorial voice. Second, because SPaRKy is a twostage sentence planner and we are focusing here on senten"
W09-3941,W01-1605,0,\N,Missing
W10-4237,P03-2030,0,0.0265808,"Arabic over five years. There are other resources which may be useful. Zettelmoyer and Collins (2009) have manually converted the original SQL meaning annotations of the ATIS corpus (et al., 1994)— some 4,637 sentences—into lambda-calculus expressions which were used for training and testing their semantic parser. This resource might make a good out-of-domain test set for generation systems trained on WSJ data. FrameNet, used for semantic parsing, see for example Gildea and Jurafsky (2002), identifies a sentence’s frame elements and assigns semantic roles to the frame elements. FrameNet data (Baker and Sato, 2003) was used for training and test sets in one of the SensEval-3 shared tasks in 2004 (Automatic Labeling of Semantic Roles). There has been some work combining FrameNet with other lexical resources. For example, Shi and Mihalcea (2005) integrated FrameNet with VerbNet and WordNet for the purpose of enabling more robust semantic parsing. The Semlink project (http://verbs.colorado. edu/semlink/) aims to integrate Propbank, FrameNet, WordNet and VerbNet. Other relevant work includes Moldovan and Rus (Moldovan and Rus, 2001; Rus, 2002) who developed a technique for parsing into logical forms and use"
W10-4237,P06-1130,1,0.915952,"Missing"
W10-4237,I05-1015,0,0.0150032,"underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use"
W10-4237,W96-0501,0,0.0418685,"cognisers, coreference resolvers, and many other tools. Not only is there a real choice between a range of different systems performing the same task, there are also evaluation methodologies to help determine what the state of the art is. Natural Language Generation (NLG) has not so far developed generic tools and methods for comparing them to the same extent as Natural Language Analysis (NLA) has. The subfield of NLG that has perhaps come closest to developing generic tools is surface realisation. Wide-coverage surface realisers such as PENMAN / NIGEL (Mann and Mathiesen, 1983), FUF / SURGE (Elhadad and Robin, 1996) and REALPRO (Lavoie and Rambow, 1997) were intended to be more or less offthe-shelf plug-and-play modules. But they tended to require a significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set. With the advent of statistical techniques in NLG surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings wi"
W10-4237,H94-1010,0,0.0443218,"Missing"
W10-4237,J02-3001,0,0.0132479,"ogy, and coreference. The current goal is to annotate over a million words each of English and Chinese, and half a million words of Arabic over five years. There are other resources which may be useful. Zettelmoyer and Collins (2009) have manually converted the original SQL meaning annotations of the ATIS corpus (et al., 1994)— some 4,637 sentences—into lambda-calculus expressions which were used for training and testing their semantic parser. This resource might make a good out-of-domain test set for generation systems trained on WSJ data. FrameNet, used for semantic parsing, see for example Gildea and Jurafsky (2002), identifies a sentence’s frame elements and assigns semantic roles to the frame elements. FrameNet data (Baker and Sato, 2003) was used for training and test sets in one of the SensEval-3 shared tasks in 2004 (Automatic Labeling of Semantic Roles). There has been some work combining FrameNet with other lexical resources. For example, Shi and Mihalcea (2005) integrated FrameNet with VerbNet and WordNet for the purpose of enabling more robust semantic parsing. The Semlink project (http://verbs.colorado. edu/semlink/) aims to integrate Propbank, FrameNet, WordNet and VerbNet. Other relevant work"
W10-4237,P98-1116,0,0.0286606,"more or less offthe-shelf plug-and-play modules. But they tended to require a significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set. With the advent of statistical techniques in NLG surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to pr"
W10-4237,W02-2103,0,0.0463994,"ical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence. Results are typically evaluated using BLEU, and, roughly speaking, BLEU scores go down as m"
W10-4237,A97-1039,0,0.0697101,"many other tools. Not only is there a real choice between a range of different systems performing the same task, there are also evaluation methodologies to help determine what the state of the art is. Natural Language Generation (NLG) has not so far developed generic tools and methods for comparing them to the same extent as Natural Language Analysis (NLA) has. The subfield of NLG that has perhaps come closest to developing generic tools is surface realisation. Wide-coverage surface realisers such as PENMAN / NIGEL (Mann and Mathiesen, 1983), FUF / SURGE (Elhadad and Robin, 1996) and REALPRO (Lavoie and Rambow, 1997) were intended to be more or less offthe-shelf plug-and-play modules. But they tended to require a significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set. With the advent of statistical techniques in NLG surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/"
W10-4237,W04-0413,0,0.0130197,"ank bracketing style allows extraction of simple predicate/argument structure. In addition to Treebank-1 material, Treebank-3 contains documents from the Switchboard and Brown corpora. 2. Propbank (Palmer et al., 2005): This is a semantic annotation of the Wall Street Journal section of Penn Treebank-2. More specifically, each verb occurring in the Treebank has been treated as a semantic predicate and the surrounding text has been annotated for arguments and adjuncts of the predicate. The verbs have also been tagged with coarse grained senses and with inflectional information. 3. NomBank 1.0 (Meyers et al., 2004): NomBank is an annotation project at New York University that provides argument structure for common nouns in the Penn Treebank. NomBank marks the sets of arguments that occur with nouns in PropBank I, just as the latter records such information for verbs. 4. BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005): supplements the Wall Street Journal corpus, adding annotation of pronoun coreference, and a variety of entity and numeric types. 5. FrameNet (Johnson et al., 2002): 150,000 sentences annotated for semantic roles and possible syntactic realisations. The annot"
W10-4237,P01-1052,0,0.0217028,"ame elements and assigns semantic roles to the frame elements. FrameNet data (Baker and Sato, 2003) was used for training and test sets in one of the SensEval-3 shared tasks in 2004 (Automatic Labeling of Semantic Roles). There has been some work combining FrameNet with other lexical resources. For example, Shi and Mihalcea (2005) integrated FrameNet with VerbNet and WordNet for the purpose of enabling more robust semantic parsing. The Semlink project (http://verbs.colorado. edu/semlink/) aims to integrate Propbank, FrameNet, WordNet and VerbNet. Other relevant work includes Moldovan and Rus (Moldovan and Rus, 2001; Rus, 2002) who developed a technique for parsing into logical forms and used this to transform WordNet concept definitions into logical forms. The same method (with additional manual correction) was used to produce the test set for another SensEval-3 shared task (Identification of Logic Forms in English). 4.1 CoNLL 2008 Shared Task Data Perhaps the most immediately promising resource is is the CoNLL shared task data from 2008 (Surdeanu et al., 2008) which has syntactic dependency annotations, named-entity boundaries and the semantic dependencies model roles of both verbal and nominal predica"
W10-4237,W05-1510,0,0.232262,"surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence. Results are typically evaluated using BLEU, and, roughly speaking, BLEU scores go down as more information is removed. While public"
W10-4237,P05-1008,0,0.0183244,"-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Tre"
W10-4237,J05-1004,0,0.0427927,"these include documents originally included in the Penn Treebank, and thus make it possible in principle to combine the various levels of annotation into a single commonground representation. The following is a (nonexhaustive) list of such resources: 1. Penn Treebank-3 (Marcus et al., 1999): one million words of hand-parsed 1989 Wall Street Journal material annotated in Treebank II style. The Treebank bracketing style allows extraction of simple predicate/argument structure. In addition to Treebank-1 material, Treebank-3 contains documents from the Switchboard and Brown corpora. 2. Propbank (Palmer et al., 2005): This is a semantic annotation of the Wall Street Journal section of Penn Treebank-2. More specifically, each verb occurring in the Treebank has been treated as a semantic predicate and the surrounding text has been annotated for arguments and adjuncts of the predicate. The verbs have also been tagged with coarse grained senses and with inflectional information. 3. NomBank 1.0 (Meyers et al., 2004): NomBank is an annotation project at New York University that provides argument structure for common nouns in the Penn Treebank. NomBank marks the sets of arguments that occur with nouns in PropBan"
W10-4237,W08-2121,0,0.0516008,"Missing"
W10-4237,N01-1001,0,0.0158715,"added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009)"
W10-4237,P09-1110,0,0.022058,"Missing"
W10-4237,D09-1043,0,\N,Missing
W10-4237,C98-1112,0,\N,Missing
W11-2832,W05-0909,0,0.0208607,"selected (and removed) from PTB Section 24. Note that a small number of sentences from the selected WSJ sections were not included in the CoNLL-08 data (and are thus not included in the SR Task data) due to difficulties in merging the various data sets (e.g. Section 23 has 17 fewer sentences). 3 Automatic Evaluations We computed scores using the following wellknown automatic evaluation metrics: implementations use smoothing to allow sentencelevel scores to be computed. 2. NIST:4,5 n-gram similarity weighted in favour of less frequent n-grams which are taken to be more informative. 3. METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2011):6 lexical similarity based on exact, stem, synonym, and paraphrase matches between words and phrases. 4. TER (Snover et al., 2006):7 a length-normalized edit distance metric where phrasal shifts are counted as one edit. For each metric, we calculated system-level scores, the mean of the sentence-level scores and weighted n-best scores (described below). Text normalisation: Output texts were normalised by lower-casing all tokens, removing any extraneous white space characters and ensuring consistent treatment of ampersands. 4 http://www.itl.nist.gov/iad/mig/tests/mt"
W11-2832,P11-2040,1,0.695361,"ccessful in choosing a single-best output that is more similar to the reference sentence than the others in the top 5. In the absence of multiple reference sentences or human evaluation results for the n-best list though, it is unclear to what extent the outputs in the n-best list might represent valid paraphrases versus clearly less acceptable outputs. 4 Human Evaluations 4.1 Experimental Set-up We assessed three criteria in the human evaluations: Clarity, Readability and Meaning Similarity. We used continuous sliders as rating tools (see Figures 1 and 2), because raters tend to prefer them (Belz and Kow, 2011). Slider positions were mapped to values from 0 to 100 (best). The instructions relating to Clarity and Readability read as follows:8 The first criterion you need to assess is Clarity. How clear (easy to understand) is the highlighted sentence within the context of the text extract? The second criterion to assess is Readability. This is sometimes called ’fluency’, and your task is to decide how well the highlighted sentence reads; is it good fluent English, or does it have grammatical errors, awkward constructions, etc. Note that you should assess Clarity separately from Readability: it is pos"
W11-2832,W10-4237,1,0.90252,"Missing"
W11-2832,C10-1012,0,0.117338,"ncies via the LTH Constituent-to-Dependency Conversion Tool for Penn-style Treebanks (Pennconverter) (Johansson and Nugues, 2007). The resulting dependency bank was then merged with the Nombank (Meyers et al., 2004) and Propbank (Palmer et al., 2005) corpora. Named entity information from the BBN Entity Type corpus was also integrated into the CoNLL-08 data. Our shallow representation is based on the Pennconverter dependencies. The deep representation is derived from the merged Nombank, Propbank and syntactic dependencies in a process similar to the graph completion 218 algorithm outlined in (Bohnet et al., 2010) (see Section 2.2 for differences). 2.1 Shallow representation The shallow data consists of unordered syntactic dependency trees. Each word and punctuation marker from the original sentence is represented as a node in a syntactic dependency tree. Nodes: The node information consists of a word’s lemma, a coarse-grained POS-tag, and, where appropriate, number, tense and participle features and a sense tag id (as a suffix to the lemma). In addition, two punctuation features encode the quotation and bracketing information for the sentence. The POS-tag set is slightly less fine-grained than the Pen"
W11-2832,P06-1130,0,0.108586,"Missing"
W11-2832,W11-2107,0,0.00555759,"om PTB Section 24. Note that a small number of sentences from the selected WSJ sections were not included in the CoNLL-08 data (and are thus not included in the SR Task data) due to difficulties in merging the various data sets (e.g. Section 23 has 17 fewer sentences). 3 Automatic Evaluations We computed scores using the following wellknown automatic evaluation metrics: implementations use smoothing to allow sentencelevel scores to be computed. 2. NIST:4,5 n-gram similarity weighted in favour of less frequent n-grams which are taken to be more informative. 3. METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2011):6 lexical similarity based on exact, stem, synonym, and paraphrase matches between words and phrases. 4. TER (Snover et al., 2006):7 a length-normalized edit distance metric where phrasal shifts are counted as one edit. For each metric, we calculated system-level scores, the mean of the sentence-level scores and weighted n-best scores (described below). Text normalisation: Output texts were normalised by lower-casing all tokens, removing any extraneous white space characters and ensuring consistent treatment of ampersands. 4 http://www.itl.nist.gov/iad/mig/tests/mt/doc /ngram-study.pdf 5 http"
W11-2832,W07-2416,0,0.0146291,"h are ordered). The shallow input representation is intended to be a more ‘surfacey’, syntactic represention of the sentence. The deep(er) input type is intended to be closer to a semantic, more abstract, representation of the meaning of the sentence. The input representations were created by postprocessing the CoNLL 2008 Shared Task data (Surdeanu et al., 2008). For the preparation of the CoNLL-08 Shared task data, selected sections of the Penn WSJ Treebank were converted to syntactic dependencies via the LTH Constituent-to-Dependency Conversion Tool for Penn-style Treebanks (Pennconverter) (Johansson and Nugues, 2007). The resulting dependency bank was then merged with the Nombank (Meyers et al., 2004) and Propbank (Palmer et al., 2005) corpora. Named entity information from the BBN Entity Type corpus was also integrated into the CoNLL-08 data. Our shallow representation is based on the Pennconverter dependencies. The deep representation is derived from the merged Nombank, Propbank and syntactic dependencies in a process similar to the graph completion 218 algorithm outlined in (Bohnet et al., 2010) (see Section 2.2 for differences). 2.1 Shallow representation The shallow data consists of unordered syntact"
W11-2832,W02-2103,0,0.249839,"aning Similarity. This report presents the evaluation results, along with descriptions of the SR Task Tracks and evaluation methods. For descriptions of the participating systems, see the separate system reports in this volume, immediately following this results report. 1 Introduction and Overview Many different surface realisers have been developed over the past three decades or so. While symbolic realisers dominated for much of this period, the past decade has seen the development of many different types of statistical surface realisers. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has produced results for regenerating the Penn Treebank (PTB) (Marcus et al., 1995). The basic approach in all this work was to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then 217 Dominic Espinosa2 Eric Kow1 Department of Linguistics Ohio State University Columbus, OH, 43210, US {espinosa,mwhite}@ling.osu.edu 2 Amanda Stent AT&T Labs Research Florham Park, NJ 07932, US stent@research.att.com c"
W11-2832,W04-2705,0,0.123598,"ic represention of the sentence. The deep(er) input type is intended to be closer to a semantic, more abstract, representation of the meaning of the sentence. The input representations were created by postprocessing the CoNLL 2008 Shared Task data (Surdeanu et al., 2008). For the preparation of the CoNLL-08 Shared task data, selected sections of the Penn WSJ Treebank were converted to syntactic dependencies via the LTH Constituent-to-Dependency Conversion Tool for Penn-style Treebanks (Pennconverter) (Johansson and Nugues, 2007). The resulting dependency bank was then merged with the Nombank (Meyers et al., 2004) and Propbank (Palmer et al., 2005) corpora. Named entity information from the BBN Entity Type corpus was also integrated into the CoNLL-08 data. Our shallow representation is based on the Pennconverter dependencies. The deep representation is derived from the merged Nombank, Propbank and syntactic dependencies in a process similar to the graph completion 218 algorithm outlined in (Bohnet et al., 2010) (see Section 2.2 for differences). 2.1 Shallow representation The shallow data consists of unordered syntactic dependency trees. Each word and punctuation marker from the original sentence is re"
W11-2832,W05-1510,0,0.0626047,"sents the evaluation results, along with descriptions of the SR Task Tracks and evaluation methods. For descriptions of the participating systems, see the separate system reports in this volume, immediately following this results report. 1 Introduction and Overview Many different surface realisers have been developed over the past three decades or so. While symbolic realisers dominated for much of this period, the past decade has seen the development of many different types of statistical surface realisers. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has produced results for regenerating the Penn Treebank (PTB) (Marcus et al., 1995). The basic approach in all this work was to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then 217 Dominic Espinosa2 Eric Kow1 Department of Linguistics Ohio State University Columbus, OH, 43210, US {espinosa,mwhite}@ling.osu.edu 2 Amanda Stent AT&T Labs Research Florham Park, NJ 07932, US stent@research.att.com convert and use these underspecified repr"
W11-2832,J05-1004,0,0.0968466,"e deep(er) input type is intended to be closer to a semantic, more abstract, representation of the meaning of the sentence. The input representations were created by postprocessing the CoNLL 2008 Shared Task data (Surdeanu et al., 2008). For the preparation of the CoNLL-08 Shared task data, selected sections of the Penn WSJ Treebank were converted to syntactic dependencies via the LTH Constituent-to-Dependency Conversion Tool for Penn-style Treebanks (Pennconverter) (Johansson and Nugues, 2007). The resulting dependency bank was then merged with the Nombank (Meyers et al., 2004) and Propbank (Palmer et al., 2005) corpora. Named entity information from the BBN Entity Type corpus was also integrated into the CoNLL-08 data. Our shallow representation is based on the Pennconverter dependencies. The deep representation is derived from the merged Nombank, Propbank and syntactic dependencies in a process similar to the graph completion 218 algorithm outlined in (Bohnet et al., 2010) (see Section 2.2 for differences). 2.1 Shallow representation The shallow data consists of unordered syntactic dependency trees. Each word and punctuation marker from the original sentence is represented as a node in a syntactic"
W11-2832,P02-1040,0,0.0951709,"sal shifts are counted as one edit. For each metric, we calculated system-level scores, the mean of the sentence-level scores and weighted n-best scores (described below). Text normalisation: Output texts were normalised by lower-casing all tokens, removing any extraneous white space characters and ensuring consistent treatment of ampersands. 4 http://www.itl.nist.gov/iad/mig/tests/mt/doc /ngram-study.pdf 5 http://www.itl.nist.gov/iad/mig/tests/mt/2009/ 6 http://www.cs.cmu.edu/ alavie/METEOR/ 3 7 http://www.itl.nist.gov/iad/mig/tests/mt/2009/ http://www.umiacs.umd.edu/ snover/terp/ 3 1. BLEU (Papineni et al., 2002): geometric mean of 1- to 4-gram precision with a brevity penalty; recent 220 N-best, ranked system outputs: Ranked 5-best outputs were scored using a weighted average of the sentence-level scores for each metric, with these sentence-level weighted sums averaged across all outputs. The weight wi assigned to the ith system output was in inverse proportion to its rank ri (K = 5): wi = PKK−ri +1 j=1 K−rj +1 Missing outputs: Missing outputs were scored as zero (one for TER); in the n-best evaluation, missing or duplicate outputs were scored as 0 (1 for TER). Since coverage was high for all systems"
W11-2832,2006.amta-papers.25,0,0.0358882,"hus not included in the SR Task data) due to difficulties in merging the various data sets (e.g. Section 23 has 17 fewer sentences). 3 Automatic Evaluations We computed scores using the following wellknown automatic evaluation metrics: implementations use smoothing to allow sentencelevel scores to be computed. 2. NIST:4,5 n-gram similarity weighted in favour of less frequent n-grams which are taken to be more informative. 3. METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2011):6 lexical similarity based on exact, stem, synonym, and paraphrase matches between words and phrases. 4. TER (Snover et al., 2006):7 a length-normalized edit distance metric where phrasal shifts are counted as one edit. For each metric, we calculated system-level scores, the mean of the sentence-level scores and weighted n-best scores (described below). Text normalisation: Output texts were normalised by lower-casing all tokens, removing any extraneous white space characters and ensuring consistent treatment of ampersands. 4 http://www.itl.nist.gov/iad/mig/tests/mt/doc /ngram-study.pdf 5 http://www.itl.nist.gov/iad/mig/tests/mt/2009/ 6 http://www.cs.cmu.edu/ alavie/METEOR/ 3 7 http://www.itl.nist.gov/iad/mig/tests/mt/200"
W11-2832,W08-2121,0,0.0614246,"Missing"
W11-2832,D09-1043,1,\N,Missing
W11-2832,H94-1020,0,\N,Missing
W11-2834,W00-2004,0,0.750201,"Missing"
W11-2834,J93-2004,0,0.0355413,"s the only information not found in the training data that we added to our system. During realization, each input lemma is assigned inflection by looking up a tuple consisting of its root, POS tag and features in the morphological dictionary, or by using the rules mentioned above. 2 Results and Discussion We evaluated the output of our surface realizer using the reference file and tool provided by Dominic Espinosa, which incorporates BLEU (Papenini and others, 2002), METEOR (Lavie and Denkowski, 2009) and TER (from TERp (Snover and others, 2009)). We used the subsets of the Penn Tree231 bank (Marcus et al., 1993) provided by the Linguistic Data Consortium and converted into dependency trees by Deirdre Hogan. Table 1 shows the output of the automatic metrics for the development data. The absence of lexicalized information in the tree paths causes only a slight drop in accuracy because the language model duplicates some of that information; it also adds efficiency. Tracking only one-best possibilities for all phrases also adds efficiency at a cost of accuracy. We have not done a formal error analysis, but we did notice during development that punctuation marks, especially those that need to be matched ("
W11-2834,P02-1040,0,0.0965567,"Missing"
W11-2834,W09-0441,0,0.0247044,"Missing"
W12-1809,E09-1012,1,0.797666,"alog systems. Future challenges of this type would build on this by incorporating (in order): (a) tasks other than information retrieval (e.g. survey tasks (Stent et al., 2008)); (b) task completion (tasks with subtasks that have side effects, e.g. purchasing a ticket after looking up a route); (c) task adaptation (during development, participants work with one task, and during evaluation, participants work with a different but related task); and (d) multi-task modeling. Participating systems could learn by doing (Jung et al., 2009), via user simulation (Rieser and Lemon, 2011), from corpora (Bangalore and Stent, 2009), or from scripts or other abstract task representations (Barbosa et al., 2011). Tools for the Community It has never been easier (with a little Web programming) to rapidly prototype dialog systems as mobile apps, or to use them to collect data. To enable researchers 18 to focus on dialog- and task-modeling rather than component development, AT&T is happy to offer its AT&T WATSONSM speech recognizer and Natural VoicesT M text-to-speech synthesis engine in the cloud, through its Speech Mashup platform (Di Fabbrizio et al., 2009), to participants in dialog challenges. The Speech Mashup supports"
W12-1809,W11-2015,0,0.0213847,"o types of (collaborative or competitive) “dialog challenge”: Dialog layer-focused – Participants focus on models for a particular dialog behavior, such as turn-taking, grounding, alignment, or coreference. Implementations cover both the interpretation and the generation aspects of the behavior. Evaluation may be based on a comparison of the implemented behaviors to human language behaviors (e.g. for turn-taking, inter-turn silence, turn-final and turn-initial prosodic cues), and/or on user error rates and satisfaction scores. An initial dialog layer-focused challenge could be on turn-taking (Baumann and Schlangen, 2011; Selfridge and Heeman, 2010). Task modeling focused – This type of challenge will move from modeling individual tasks, to automatic acquisition and use of task models for interactive tasks in dialog systems. Future challenges of this type would build on this by incorporating (in order): (a) tasks other than information retrieval (e.g. survey tasks (Stent et al., 2008)); (b) task completion (tasks with subtasks that have side effects, e.g. purchasing a ticket after looking up a route); (c) task adaptation (during development, participants work with one task, and during evaluation, participants"
W12-1809,W11-2002,0,0.0617121,"Missing"
W12-1809,2005.sigdial-1.20,0,0.0553726,"ssociation for Computational Linguistics most basic semantic representations. As the field moves forward, dialog behavior modeling will be increasingly separated from task modeling (Allen et al., 2001a; Allen et al., 2001b). Research on dialog modeling will focus on dialog layers, taskindependent dialog behaviors such as (incremental) turn-taking, grounding, and coreference that involve both participants. Research on task modeling can focus on the design of task models that are agnostic to the types or forms of interaction that will use them, on general models for interactive problem-solving (Blaylock and Allen, 2005), and on rapid acquisition and adaptation of task models (Jung et al., 2009). Within this space, there can be two types of (collaborative or competitive) “dialog challenge”: Dialog layer-focused – Participants focus on models for a particular dialog behavior, such as turn-taking, grounding, alignment, or coreference. Implementations cover both the interpretation and the generation aspects of the behavior. Evaluation may be based on a comparison of the implemented behaviors to human language behaviors (e.g. for turn-taking, inter-turn silence, turn-final and turn-initial prosodic cues), and/or"
W12-1809,W11-2033,0,0.0147538,"s, can involve rich context modeling, and have side effects in the “real world”: Multi-task – The system interacts with the user to accomplish a series of (possibly related) tasks. For example, a user might use the system to order a book and then say schedule it for book club - a different task (e.g. requiring different backend DB lookups) but related to the previous one by the book informa1 2 www.vlingo.com http://www.apple.com/iphone/features/siri.html tion. Multi-task interaction increases the difficulty of interpretation and task inference, and so requires new kinds of dialog model (e.g. (Lison, 2011)). Asynchronous – the user may give the system a command (e.g. Add Hunger Games with Mary for 3 pm), and the system may follow up on that command an hour later, after considerable intervening dialog (e.g. Mary texted you about the Hunger Games). Because the dialog is multi-task, it is more free-flowing, with less clear start and end points but more opportunities for adaptation and personalization. Rich context modeling – Mobile devices come with numerous sensors useful for collecting nonlinguistic context (e.g. GPS, camera, web browser actions), while the semi-continuous nature of the interact"
W12-1809,J11-1006,0,0.014834,"task models for interactive tasks in dialog systems. Future challenges of this type would build on this by incorporating (in order): (a) tasks other than information retrieval (e.g. survey tasks (Stent et al., 2008)); (b) task completion (tasks with subtasks that have side effects, e.g. purchasing a ticket after looking up a route); (c) task adaptation (during development, participants work with one task, and during evaluation, participants work with a different but related task); and (d) multi-task modeling. Participating systems could learn by doing (Jung et al., 2009), via user simulation (Rieser and Lemon, 2011), from corpora (Bangalore and Stent, 2009), or from scripts or other abstract task representations (Barbosa et al., 2011). Tools for the Community It has never been easier (with a little Web programming) to rapidly prototype dialog systems as mobile apps, or to use them to collect data. To enable researchers 18 to focus on dialog- and task-modeling rather than component development, AT&T is happy to offer its AT&T WATSONSM speech recognizer and Natural VoicesT M text-to-speech synthesis engine in the cloud, through its Speech Mashup platform (Di Fabbrizio et al., 2009), to participants in dial"
W12-1809,P10-1019,0,0.0240634,"competitive) “dialog challenge”: Dialog layer-focused – Participants focus on models for a particular dialog behavior, such as turn-taking, grounding, alignment, or coreference. Implementations cover both the interpretation and the generation aspects of the behavior. Evaluation may be based on a comparison of the implemented behaviors to human language behaviors (e.g. for turn-taking, inter-turn silence, turn-final and turn-initial prosodic cues), and/or on user error rates and satisfaction scores. An initial dialog layer-focused challenge could be on turn-taking (Baumann and Schlangen, 2011; Selfridge and Heeman, 2010). Task modeling focused – This type of challenge will move from modeling individual tasks, to automatic acquisition and use of task models for interactive tasks in dialog systems. Future challenges of this type would build on this by incorporating (in order): (a) tasks other than information retrieval (e.g. survey tasks (Stent et al., 2008)); (b) task completion (tasks with subtasks that have side effects, e.g. purchasing a ticket after looking up a route); (c) task adaptation (during development, participants work with one task, and during evaluation, participants work with a different but re"
W14-4408,P98-1013,0,0.0722621,"Missing"
W14-4408,W09-0613,0,0.0108735,"other, a generate-andrank approach may be best (allowing each component to express alternative ‘good’ choices and choosing the best combination of these choices at the end). Our text planner is responsible for analyzing the input text reviews, extracting perattribute rating distributions and other meta-data from each review, and synthesizing this information to produce one or more discourse plans. Our sentence planner, J SPA RK Y – a freely-available toolkit (Stent and Molina, 2009) – can produce several candidate sentence plans and their corresponding surface realizations through SimpleNLG (Gatt and Reiter, 2009). The candidate summaries are ranked by calculating their perplexity with a language model trained over a large number of sentences from additional restaurant reviews collected over the Web. Hybrid summarization Most NLG research has converged around a “consensus architecture” (Reiter, 1994; Rambow and Korelsky, 1992), a pipeline architecture including the following modules: 1) text planning, which determines how the presentation content is selected, structured, and ordered; 2) sentence planning, which assigns content to sentences, inserts discourse cues to communicate the structure of the pre"
W14-4408,W00-0405,0,0.0182323,"ctive summarizer: using extractive summarization techniques, it selects salient quotes from the input reviews and embeds them into an automatically generated abstractive summary to provide evidence for, exemplify or justify positive or negative opinions. We demonstrate that, compared to extractive methods, summaries generated with abstractive and hybrid summarization approaches are more readable and compact. 1 Introduction Text summarization is a well-established area of research. Many approaches are extractive, that is, they select and stitch together pieces of text from the input documents (Goldstein et al., 2000; Radev et al., 2004). Other approaches are abstractive; they use natural language generation (NLG) techniques to paraphrase and condense the content of the input documents (Radev and McKeown, 1998). Most summarization methods focus on distilling factual information by identifying the input documents’ main topics, removing redundancies, and coherently ordering extracted phrases or sentences. Summarization of sentiment-laden text (e.g., product or service reviews) is substantially different from the traditional text summarization task: instead of presenting facts, the summarizer must present th"
W14-4408,W04-1013,0,0.0164874,"the quote selection module; and 4) the hybrid summarizer S TARLET-H. We used the Amazon Mechanical Turk3 crowdEvaluating an abstractive review summarizer involves measuring how accurately the opinion content present in the reviews is reflected in the summary and how understandable the generated content is to the reader. Traditional multi-document summarization evaluation techniques utilize both qualitative and quantitative metrics. The former require human subjects to rate different evaluative characteristics on a Likert-like scale, while the latter relies on automatic metrics such as ROUGE (Lin, 2004), which is based on the common number of n-grams between a peer, and one or several gold-standard reference summaries. 3 59 http://www.mturk.com restaurant review documents. sourcing system to post subjective evaluation tasks, or HITs, for 20 restaurant summaries. Each HIT consists of a set of ten randomly ordered reviews for one restaurant, and four randomly ordered summaries of reviews for that restaurant, each one accompanied by a set of evaluation widgets for the different evaluation metrics described below. To minimize reading order bias, both reviews and summaries were shuffled each time"
W14-4408,radev-etal-2004-mead,0,0.0204223,"Missing"
W14-4408,C10-1039,0,0.265289,"n of S TARLET-H. Another potential area of future research concerns the ability to personalize summaries to the user’s needs. For instance, the text planner can adapt its communicative goals based on polarity orientation – a user can be more interested in exploring in detail negative reviews – or it can focus more on specific (user-tailored) aspects and change the order of the presentation accordingly. Finally, it could be interesting to customize the summarizer to provide an overview of what is available in a specific geographic neighborhood and compare and contrast the options. Related work Ganesan et al. (2010) propose a method to extract salient sentence fragments that are both highly frequent and syntactically well-formed by using a graph-based data structure to eliminate redundancies. However, this approach assumes that the input sentences are already selected in terms of aspect and with highly redundant opinion content. Also, the generated summaries are very short and cannot be compared to a full-length output of a typical multi-document summarizer (e.g., 100-200 words). A similar approach is described in Ganesan et al. (2012), where very short phrases (from two to five words) are collated toget"
W14-4408,J98-3005,0,0.038586,"e for, exemplify or justify positive or negative opinions. We demonstrate that, compared to extractive methods, summaries generated with abstractive and hybrid summarization approaches are more readable and compact. 1 Introduction Text summarization is a well-established area of research. Many approaches are extractive, that is, they select and stitch together pieces of text from the input documents (Goldstein et al., 2000; Radev et al., 2004). Other approaches are abstractive; they use natural language generation (NLG) techniques to paraphrase and condense the content of the input documents (Radev and McKeown, 1998). Most summarization methods focus on distilling factual information by identifying the input documents’ main topics, removing redundancies, and coherently ordering extracted phrases or sentences. Summarization of sentiment-laden text (e.g., product or service reviews) is substantially different from the traditional text summarization task: instead of presenting facts, the summarizer must present the range of opinions and the consensus opinion (if any), and instead of focusing on one topic, the summarizer must present information about multiple aspects of the target entity. ∗ Robert Gaizauskas"
W14-4408,W94-0319,0,0.171353,"eta-data from each review, and synthesizing this information to produce one or more discourse plans. Our sentence planner, J SPA RK Y – a freely-available toolkit (Stent and Molina, 2009) – can produce several candidate sentence plans and their corresponding surface realizations through SimpleNLG (Gatt and Reiter, 2009). The candidate summaries are ranked by calculating their perplexity with a language model trained over a large number of sentences from additional restaurant reviews collected over the Web. Hybrid summarization Most NLG research has converged around a “consensus architecture” (Reiter, 1994; Rambow and Korelsky, 1992), a pipeline architecture including the following modules: 1) text planning, which determines how the presentation content is selected, structured, and ordered; 2) sentence planning, which assigns content to sentences, inserts discourse cues to communicate the structure of the presentation, and performs sentence aggregation and optionally referring expression generation; and 3) surface realization, which performs lexical selection, resolves syntactic issues such as subject-verb and noun-determiner agreement, and assigns morphological inflection to produce the final"
W14-4408,W09-3941,1,0.816615,"the decisions to be made at each stage of the NLG process just outlined are complex, and because they are not truly independent of each other, a generate-andrank approach may be best (allowing each component to express alternative ‘good’ choices and choosing the best combination of these choices at the end). Our text planner is responsible for analyzing the input text reviews, extracting perattribute rating distributions and other meta-data from each review, and synthesizing this information to produce one or more discourse plans. Our sentence planner, J SPA RK Y – a freely-available toolkit (Stent and Molina, 2009) – can produce several candidate sentence plans and their corresponding surface realizations through SimpleNLG (Gatt and Reiter, 2009). The candidate summaries are ranked by calculating their perplexity with a language model trained over a large number of sentences from additional restaurant reviews collected over the Web. Hybrid summarization Most NLG research has converged around a “consensus architecture” (Reiter, 1994; Rambow and Korelsky, 1992), a pipeline architecture including the following modules: 1) text planning, which determines how the presentation content is selected, structured,"
W14-4408,A92-1006,0,\N,Missing
W14-4408,C98-1013,0,\N,Missing
W16-3602,W12-1606,1,0.825427,"irst, we applied TL-DST to the DSTC2 data which has a great deal of user goal changes, and obtained state-of-the-art performance. Second, in order to test TL-DST on more challenging data, we applied TL-DST to a set of pseudo-real datasets involving multiple interleaved tasks and complex constraints on user goals. To generate the datasets, we fed the DSTC3 data, which includes three different types of tasks in addition to goal changes, to simulation techniques which have been often adopted for the development and evaluation of dialog systems (Schatzmann et al., 2006; Pietquin and Dutoit, 2006; Lee and Eskenazi, 2012). The results of these experiments show that TL-DST can successfully handle complex multi-task interactions, largely outperforming the DSTC baseline tracker. The rest of this paper is organized as follows. In Section 2 we describe TL-DST. In Section 3 we discuss our experiments. In Section 4 we present a brief summary of related work. We finish with conclusions and future work in Section 5. 2 information. An augmented DAI has the form start time (conf idence score, DAIend time ). Usually task frames come in a collection, called a task frame parse, as a result of task frame parsing when there a"
W16-3602,W10-4336,0,0.177737,"urant, not Italian”:   Task  Constraints    DB  Timestamps ... Restaurant (0.7, food = thai)   (0.6, food 6= italian)  [“Thai To Go”, “Pa de Thai”]   01/01/2016 : 12-00-00 ... A task state is analogous to a dialog state in typical dialog systems. However, unlike in conventional dialog state tracking, we don’t assume a unique value for each slot. Instead, we adopt binary distributions for each constraint. This allows us to circumvent the exponential complexity in the number of values which otherwise would be caused by taking a power set of slot values to handle complex constraints (Crook and Lemon, 2010). Task Lineage A task lineage is a chronologically ordered list of task states, representing the agent’s hypotheses about what tasks were involved at each time point in a conversation. A task lineage can be consulted to provide crucial pieces of information for conversation structure. For instance, the most recent task frames in a lineage can indicate the current focus of conversation. In addition, when the user switches back to a previous task, the agent can trace back the lineage in reverse order to take recency into account. However, conversational agents often cannot determine exactly what"
W16-3602,W13-4066,1,0.900652,"Missing"
W16-3602,W13-4069,1,0.856899,"three context sets: • B(l0:t−1 ): A set of δ-latest belief estimates for each constraint that appears in lt . The δ-latest belief estimate means the latest belief estimate before t − δ. 2.3 Task State Update In this section, we describe the last component of TL-DST, task state update. A nice property of TL-DST is its ability to exploit alternative methods for dialog state tracking. For instance, by setting a large value to δ for the context fetcher, one can adopt various discriminative models that take advantage of expressive feature functions extracted from a collection of raw observations (Lee, 2013; Henderson et al., 2014c; Williams, 2014). On the other hand, with δ being 0, one can employ a method from a library of generative models which only requires to know the immediately prior belief estimates (Wang and Lemon, 2013; Zilka et al., 2013). Unlike in previous work, instead of predicting a unique goal value for a slot, we perform belief tracking for each individual slot-value constraint to allow complex goals. For the experiments presented here, we chose to use the rule-based algorithm from Zilka et al. (2013) for constraint-level belief tracking. The use of a rule-based algorithm • U("
W16-3602,J98-4001,0,0.167217,"has been largely confined to developing more robust approaches to other conversational agent technologies, such as automated speech recognition (ASR) and spoken language understanding (SLU), in a session-based dialog processing a single task with a relatively simple goal. Session-based, single task, simple goal dialog is easier for dialog system engineers and consistent with 25 years of commercial dialog system development, but does not match users’ real-world task needs as communicated with human conversational assistants or recognized in the dialog literature (e.g. (Grosz and Sidner, 1988; Lochbaum, 1998)), and is inconsistent with the mobile-centric, always-on, conversational assistant commercial vision that has emerged over the past few years. This gap between how humans most effectively converse about complex tasks and what conversaWe consider the gap between user demands for seamless handling of complex interactions, and recent advances in dialog state tracking technologies. We propose a new statistical approach, Task Lineage-based Dialog State Tracking (TL-DST), aimed at seamlessly orchestrating multiple tasks with complex goals across multiple domains in continuous interaction. TL-DST co"
W16-3602,W11-2004,0,0.0260345,". The closest work to our task frame parsing is frame semantic parsing task in NLP (Das, 2014). Differences include that the input here is a collection of potentially conflicting semantic hypotheses from different domain-specific SLUs. Also we are more interested in obtaining a N -best list of parses with well calibrated confidence scores than in getting only a top hypothesis. Recently there has been growing interest in multidomain and multitask dialog (Crook et al., 2016; Sun et al., 2016; Ramachandran and Ratnaparkhi, 2015; Gaˇsic et al., 2015; Wang et al., 2014; Hakkani-T¨ ur et al., 2012; Nakano et al., 2011). To our knowledge, however, there is no previous work that provides a holistic statistical approach for complex dialog state tracking that can cover the wide range of problems discussed in this paper. 5 Conclusions In this paper, we have proposed the TL-DST approach toward the goal of seamlessly orchestrating multiple tasks with complex goals across multiple domains in continuous interaction. The proposed method’s state-of-the-art performance on common benchmark datasets and purposefully simulated dialog corpora demonstrates the potential capacity of TL-DST. In the future, we want to apply TL"
W16-3602,W14-3007,0,0.0188824,"context fetcher. The comparative results suggest that there is much room for improvement in both the task frame parser and the context fetcher. Given the good performance on Avg. Accuracy, despite imperfect joint prediction, a TL-DST based agent should be able to successfully complete the conversation with extra exchanges. This also matches our empirical analysis of the tracker’s output; the tracker missed only a couple of constraints in its incorrect joint prediction. 4 recent survey by Williams et al. (2016). The closest work to our task frame parsing is frame semantic parsing task in NLP (Das, 2014). Differences include that the input here is a collection of potentially conflicting semantic hypotheses from different domain-specific SLUs. Also we are more interested in obtaining a N -best list of parses with well calibrated confidence scores than in getting only a top hypothesis. Recently there has been growing interest in multidomain and multitask dialog (Crook et al., 2016; Sun et al., 2016; Ramachandran and Ratnaparkhi, 2015; Gaˇsic et al., 2015; Wang et al., 2014; Hakkani-T¨ ur et al., 2012; Nakano et al., 2011). To our knowledge, however, there is no previous work that provides a hol"
W16-3602,W15-4609,0,0.378812,"depart from conventional dialog state tracking approaches. Unlike most methods where the DST keeps on overriding the content of the dialog state (hence losing past states) TLDST adopts a dynamically growing structure, providing a richer view to later processing. This is particularly important for continuous interaction involving multiple tasks. Interestingly, this is a crucial reason behind advances in deep neural network models using the attention mechanism (Bahdanau et al., 2014). Also unlike some approaches that use stack-like data structures for focus management (Larsson and Traum, 2000; Ramachandran and Ratnaparkhi, 2015) where the tracker pops out the tasks above the focused task, losing valuable information such as temporal ordering and partially filled constraints, TL-DST preserves all of the past task states by viewing the focus change as a side effect of generating a new updated task state each time. This allows for flexible task switching among a set of partially fulfilled tasks. 2.1 Task Frame Parsing In this section we formalize task frame parsing as a structure prediction problem. We use a probabilistic framework that employs a beam search technique using Monte Carlo Markov Chain (MCMC) with simulated"
W16-3602,W14-4337,0,0.34052,"ext sets: • B(l0:t−1 ): A set of δ-latest belief estimates for each constraint that appears in lt . The δ-latest belief estimate means the latest belief estimate before t − δ. 2.3 Task State Update In this section, we describe the last component of TL-DST, task state update. A nice property of TL-DST is its ability to exploit alternative methods for dialog state tracking. For instance, by setting a large value to δ for the context fetcher, one can adopt various discriminative models that take advantage of expressive feature functions extracted from a collection of raw observations (Lee, 2013; Henderson et al., 2014c; Williams, 2014). On the other hand, with δ being 0, one can employ a method from a library of generative models which only requires to know the immediately prior belief estimates (Wang and Lemon, 2013; Zilka et al., 2013). Unlike in previous work, instead of predicting a unique goal value for a slot, we perform belief tracking for each individual slot-value constraint to allow complex goals. For the experiments presented here, we chose to use the rule-based algorithm from Zilka et al. (2013) for constraint-level belief tracking. The use of a rule-based algorithm • U(l0:t−1 ): A set of all p"
W16-3602,W14-4340,0,0.122587,"ext sets: • B(l0:t−1 ): A set of δ-latest belief estimates for each constraint that appears in lt . The δ-latest belief estimate means the latest belief estimate before t − δ. 2.3 Task State Update In this section, we describe the last component of TL-DST, task state update. A nice property of TL-DST is its ability to exploit alternative methods for dialog state tracking. For instance, by setting a large value to δ for the context fetcher, one can adopt various discriminative models that take advantage of expressive feature functions extracted from a collection of raw observations (Lee, 2013; Henderson et al., 2014c; Williams, 2014). On the other hand, with δ being 0, one can employ a method from a library of generative models which only requires to know the immediately prior belief estimates (Wang and Lemon, 2013; Zilka et al., 2013). Unlike in previous work, instead of predicting a unique goal value for a slot, we perform belief tracking for each individual slot-value constraint to allow complex goals. For the experiments presented here, we chose to use the rule-based algorithm from Zilka et al. (2013) for constraint-level belief tracking. The use of a rule-based algorithm • U(l0:t−1 ): A set of all p"
W16-3602,W13-4067,0,0.0405037,"section, we describe the last component of TL-DST, task state update. A nice property of TL-DST is its ability to exploit alternative methods for dialog state tracking. For instance, by setting a large value to δ for the context fetcher, one can adopt various discriminative models that take advantage of expressive feature functions extracted from a collection of raw observations (Lee, 2013; Henderson et al., 2014c; Williams, 2014). On the other hand, with δ being 0, one can employ a method from a library of generative models which only requires to know the immediately prior belief estimates (Wang and Lemon, 2013; Zilka et al., 2013). Unlike in previous work, instead of predicting a unique goal value for a slot, we perform belief tracking for each individual slot-value constraint to allow complex goals. For the experiments presented here, we chose to use the rule-based algorithm from Zilka et al. (2013) for constraint-level belief tracking. The use of a rule-based algorithm • U(l0:t−1 ): A set of all previous SLU results within e t−1 }. δ, {e ut−δ , . . . , u • M(l0:t−1 ): A set of all previous agent DAIs within δ, {mt−δ , . . . , mt−1 }. By varying δ, the context fetcher controls the ratio of summari"
W16-3602,D14-1007,0,0.0518858,"tion. 4 recent survey by Williams et al. (2016). The closest work to our task frame parsing is frame semantic parsing task in NLP (Das, 2014). Differences include that the input here is a collection of potentially conflicting semantic hypotheses from different domain-specific SLUs. Also we are more interested in obtaining a N -best list of parses with well calibrated confidence scores than in getting only a top hypothesis. Recently there has been growing interest in multidomain and multitask dialog (Crook et al., 2016; Sun et al., 2016; Ramachandran and Ratnaparkhi, 2015; Gaˇsic et al., 2015; Wang et al., 2014; Hakkani-T¨ ur et al., 2012; Nakano et al., 2011). To our knowledge, however, there is no previous work that provides a holistic statistical approach for complex dialog state tracking that can cover the wide range of problems discussed in this paper. 5 Conclusions In this paper, we have proposed the TL-DST approach toward the goal of seamlessly orchestrating multiple tasks with complex goals across multiple domains in continuous interaction. The proposed method’s state-of-the-art performance on common benchmark datasets and purposefully simulated dialog corpora demonstrates the potential capa"
W16-3602,W13-4065,0,0.0606396,"Missing"
W16-3602,W14-4339,0,0.0589209,"set of δ-latest belief estimates for each constraint that appears in lt . The δ-latest belief estimate means the latest belief estimate before t − δ. 2.3 Task State Update In this section, we describe the last component of TL-DST, task state update. A nice property of TL-DST is its ability to exploit alternative methods for dialog state tracking. For instance, by setting a large value to δ for the context fetcher, one can adopt various discriminative models that take advantage of expressive feature functions extracted from a collection of raw observations (Lee, 2013; Henderson et al., 2014c; Williams, 2014). On the other hand, with δ being 0, one can employ a method from a library of generative models which only requires to know the immediately prior belief estimates (Wang and Lemon, 2013; Zilka et al., 2013). Unlike in previous work, instead of predicting a unique goal value for a slot, we perform belief tracking for each individual slot-value constraint to allow complex goals. For the experiments presented here, we chose to use the rule-based algorithm from Zilka et al. (2013) for constraint-level belief tracking. The use of a rule-based algorithm • U(l0:t−1 ): A set of all previous SLU result"
W16-3602,W13-4070,0,\N,Missing
W16-3617,P15-1153,0,0.013616,"ys S to keep loose particles and dust from causing soft errors, drop-outs. Figure 1: A RST discourse tree with EDUs as leaf nodes (example from Mann and Thompson (1988)). n-grams are frequently used for quantifying content salience and redundancy prior to summarization over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes, 2011; Cao et al., 2015). In contrast, when the task at hand is more abstractive, the units are more finegrained, e.g., n-grams and phrases in abstractive summarization (Kikuchi et al., 2014; Liu et al., 2015; Bing et al., 2015), n-grams and humanannotated concept units in summarization evaluation (Lin, 2004; Hovy et al., 2006). Recently, subject-verb-object triplets were used to automatically identify concept units (Yang et al., 2016) and in abstractive summarization (Li, 2015); however, this requires semantic processing while EDU segmentation is presently more accurate and scalable. Here, we explore EDUs as a middle ground between fine-grained lexical units and full sentences. While EDUs have been used in prior work to directly assemble output summaries (Marcu, 1999; Hirao et al., 2013; Yoshida et al., 2014), the f"
W16-3617,D07-1001,0,0.0266921,",000 # contributors 15,000 10,000 5,000 0 1 2 3 4 5 6 7 8 9 # overlapping EDUs per contributor Figure 3: Number of EDUs which overlap with each SCU contributor (single or multi-part) in the DUC/TAC reference summary datasets. Compressive summarization. To explore the utility of EDUs in summarization, we examine near-extractive summaries in the NYT corpus which are drawn from sentences in the document but omit at least one word or phrase from them. This setting is also explored in the summarization literature for techniques which combine extractive sentence selection with sentence compression (Clarke and Lapata, 2007; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Kikuchi et al., 2014). These approaches are typically evaluated against abstractive summaries and have not been studied with a natural compressive dataset such as the ones proposed here. We do not address techniques to generate compressive summaries in this work but instead attempt to quantify how the omitted content in a summary relates to its EDU segmentation. 3 Contiguous Multi-part 3.1 Data and settings In the DUC 2005–2007 and TAC 2008–2011 shared tasks on multi-document summarization, evaluations are c"
W16-3617,W02-1001,0,0.0527873,"Missing"
W16-3617,P02-1057,0,0.0252912,"Missing"
W16-3617,P14-1048,0,0.056046,"Missing"
W16-3617,W10-4327,0,0.0363832,"d related work CIRCUMSTANCE S Discourse structure in summarization Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) represents the discourse in a document in the form of a tree (Figure 1). The leaf nodes of RST trees are elementary discourse units (EDUs) which are a segmentation of sentences into independent clauses, including dependencies such as clausal subjects and complements. The more central units to each RST relation are nuclei while the more peripheral are satellites. Prior work in document compression (Daum´e and Marcu, 2002) and single-document summarization (Marcu, 1999; Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Yoshida et al., 2014) has shown that the structure of discourse trees, especially the nuclearity of non-terminal discourse relations in the tree, is valuable for content selection in summarization. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) on the other hand is theory-neutral and does not define a recursive structure for the entire document like RST. Discourse relations are lexically bound to explicit discourse connectives within a sentence or exist between adjacent sentences if there is no connective. Each relation is realized in two t"
W16-3617,C04-1057,0,0.015236,"tems, has seen accuracy and speed improvements in recent years (Hernault et al., 2010; Joty et al., 2015). It is now practical to segment document sentences into EDUs at scale as a preprocessing step for automated summarization. N As your floppy drive writes or reads PURPOSE N a Syncom diskette is working four ways S to keep loose particles and dust from causing soft errors, drop-outs. Figure 1: A RST discourse tree with EDUs as leaf nodes (example from Mann and Thompson (1988)). n-grams are frequently used for quantifying content salience and redundancy prior to summarization over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes, 2011; Cao et al., 2015). In contrast, when the task at hand is more abstractive, the units are more finegrained, e.g., n-grams and phrases in abstractive summarization (Kikuchi et al., 2014; Liu et al., 2015; Bing et al., 2015), n-grams and humanannotated concept units in summarization evaluation (Lin, 2004; Hovy et al., 2006). Recently, subject-verb-object triplets were used to automatically identify concept units (Yang et al., 2016) and in abstractive summarization (Li, 2015); however, this requires semantic processing whi"
W16-3617,W09-1802,0,0.022774,"Hernault et al., 2010; Joty et al., 2015). It is now practical to segment document sentences into EDUs at scale as a preprocessing step for automated summarization. N As your floppy drive writes or reads PURPOSE N a Syncom diskette is working four ways S to keep loose particles and dust from causing soft errors, drop-outs. Figure 1: A RST discourse tree with EDUs as leaf nodes (example from Mann and Thompson (1988)). n-grams are frequently used for quantifying content salience and redundancy prior to summarization over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes, 2011; Cao et al., 2015). In contrast, when the task at hand is more abstractive, the units are more finegrained, e.g., n-grams and phrases in abstractive summarization (Kikuchi et al., 2014; Liu et al., 2015; Bing et al., 2015), n-grams and humanannotated concept units in summarization evaluation (Lin, 2004; Hovy et al., 2006). Recently, subject-verb-object triplets were used to automatically identify concept units (Yang et al., 2016) and in abstractive summarization (Li, 2015); however, this requires semantic processing while EDU segmentation is presently more accurate and s"
W16-3617,D13-1158,0,0.273442,"MSTANCE S Discourse structure in summarization Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) represents the discourse in a document in the form of a tree (Figure 1). The leaf nodes of RST trees are elementary discourse units (EDUs) which are a segmentation of sentences into independent clauses, including dependencies such as clausal subjects and complements. The more central units to each RST relation are nuclei while the more peripheral are satellites. Prior work in document compression (Daum´e and Marcu, 2002) and single-document summarization (Marcu, 1999; Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Yoshida et al., 2014) has shown that the structure of discourse trees, especially the nuclearity of non-terminal discourse relations in the tree, is valuable for content selection in summarization. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) on the other hand is theory-neutral and does not define a recursive structure for the entire document like RST. Discourse relations are lexically bound to explicit discourse connectives within a sentence or exist between adjacent sentences if there is no connective. Each relation is realized in two text arguments, which"
W16-3617,D15-1011,0,0.188878,"tence with three human-labeled concepts (SCU contributors). rived from the New York Times (NYT) corpus1 that are orders of magnitude larger than the DUC dataset, featuring thousands of article summaries with varying degrees of extractiveness. Although the summaries in this dataset typically contain fewer than 100 words and are sometimes intended to serve as a teaser for the article rather than a distillation of its content, they were nevertheless created by professional editors for a highly-trafficked news website. Prior work has also demonstrated the utility of this corpus for summarization (Hong et al., 2015; Nye and Nenkova, 2015). This dataset therefore enables the study of summarization in a realistic setting. 20,000 # contributors 15,000 10,000 5,000 0 1 2 3 4 5 6 7 8 9 # overlapping EDUs per contributor Figure 3: Number of EDUs which overlap with each SCU contributor (single or multi-part) in the DUC/TAC reference summary datasets. Compressive summarization. To explore the utility of EDUs in summarization, we examine near-extractive summaries in the NYT corpus which are drawn from sentences in the document but omit at least one word or phrase from them. This setting is also explored in the s"
W16-3617,hovy-etal-2006-automated,0,0.060529,"tree with EDUs as leaf nodes (example from Mann and Thompson (1988)). n-grams are frequently used for quantifying content salience and redundancy prior to summarization over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes, 2011; Cao et al., 2015). In contrast, when the task at hand is more abstractive, the units are more finegrained, e.g., n-grams and phrases in abstractive summarization (Kikuchi et al., 2014; Liu et al., 2015; Bing et al., 2015), n-grams and humanannotated concept units in summarization evaluation (Lin, 2004; Hovy et al., 2006). Recently, subject-verb-object triplets were used to automatically identify concept units (Yang et al., 2016) and in abstractive summarization (Li, 2015); however, this requires semantic processing while EDU segmentation is presently more accurate and scalable. Here, we explore EDUs as a middle ground between fine-grained lexical units and full sentences. While EDUs have been used in prior work to directly assemble output summaries (Marcu, 1999; Hirao et al., 2013; Yoshida et al., 2014), the focus was on using discourse structure as features for sentence ranking, while our work is the first t"
W16-3617,N15-1166,0,0.21908,"man-labeled concepts (SCU contributors). rived from the New York Times (NYT) corpus1 that are orders of magnitude larger than the DUC dataset, featuring thousands of article summaries with varying degrees of extractiveness. Although the summaries in this dataset typically contain fewer than 100 words and are sometimes intended to serve as a teaser for the article rather than a distillation of its content, they were nevertheless created by professional editors for a highly-trafficked news website. Prior work has also demonstrated the utility of this corpus for summarization (Hong et al., 2015; Nye and Nenkova, 2015). This dataset therefore enables the study of summarization in a realistic setting. 20,000 # contributors 15,000 10,000 5,000 0 1 2 3 4 5 6 7 8 9 # overlapping EDUs per contributor Figure 3: Number of EDUs which overlap with each SCU contributor (single or multi-part) in the DUC/TAC reference summary datasets. Compressive summarization. To explore the utility of EDUs in summarization, we examine near-extractive summaries in the NYT corpus which are drawn from sentences in the document but omit at least one word or phrase from them. This setting is also explored in the summarization literature"
W16-3617,N12-1015,0,0.0607799,"Missing"
W16-3617,petrov-etal-2012-universal,0,0.025344,"Missing"
W16-3617,A00-2024,0,0.0279496,"human-labeled summarization concepts within sentences and also aligns with near-extractive summaries constructed by news editors. Finally, we show that using EDUs as units of content selection instead of sentences leads to stronger summarization performance in near-extractive scenarios, especially under tight budgets. 1 Introduction Document summarization has a wide variety of practical applications and is consequently a focus of much NLP research. When a human summarizes a document, they often edit its constituent sentences in order to succinctly capture the document’s meaning. For instance, Jing and McKeown (2000) observed that summary authors trimmed extraneous content, combined sentences, replaced phrases or clauses with more general or specific variants, etc. These abstractive summaries thus involve sentences which deviate from those of the source document in structure or content. In contrast, automated approaches to summarization generally produce extractive summaries by selecting complete sentences from the source document (Nenkova and McKeown, 2011) in order to ensure that the output is grammatical. • A demonstration that EDU segmentation preserves human-identified conceptual units in the context"
W16-3617,prasad-etal-2008-penn,0,0.0157072,"ntences into independent clauses, including dependencies such as clausal subjects and complements. The more central units to each RST relation are nuclei while the more peripheral are satellites. Prior work in document compression (Daum´e and Marcu, 2002) and single-document summarization (Marcu, 1999; Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Yoshida et al., 2014) has shown that the structure of discourse trees, especially the nuclearity of non-terminal discourse relations in the tree, is valuable for content selection in summarization. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) on the other hand is theory-neutral and does not define a recursive structure for the entire document like RST. Discourse relations are lexically bound to explicit discourse connectives within a sentence or exist between adjacent sentences if there is no connective. Each relation is realized in two text arguments, which are similar to EDUs. However, unlike EDUs, PDTB relation arguments have flexibility in size, ordering and arrangement and do not form a complete segmentation of the text. They are therefore not easily interpretable as textual units that can be combined to form sentences and su"
W16-3617,J15-3002,0,0.0189641,"gement and do not form a complete segmentation of the text. They are therefore not easily interpretable as textual units that can be combined to form sentences and summaries. In this paper, we focus on EDUs and explore their viability as basic units for summarization. We did not use PDTB-style arguments to make sure each part of a document belongs to a textual unit and that the units are strictly adjacent to each other. EDU segmentation, typically addressed as a tagging problem early in discourse parsing systems, has seen accuracy and speed improvements in recent years (Hernault et al., 2010; Joty et al., 2015). It is now practical to segment document sentences into EDUs at scale as a preprocessing step for automated summarization. N As your floppy drive writes or reads PURPOSE N a Syncom diskette is working four ways S to keep loose particles and dust from causing soft errors, drop-outs. Figure 1: A RST discourse tree with EDUs as leaf nodes (example from Mann and Thompson (1988)). n-grams are frequently used for quantifying content salience and redundancy prior to summarization over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes,"
W16-3617,P14-2052,0,0.107706,"structure in summarization Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) represents the discourse in a document in the form of a tree (Figure 1). The leaf nodes of RST trees are elementary discourse units (EDUs) which are a segmentation of sentences into independent clauses, including dependencies such as clausal subjects and complements. The more central units to each RST relation are nuclei while the more peripheral are satellites. Prior work in document compression (Daum´e and Marcu, 2002) and single-document summarization (Marcu, 1999; Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Yoshida et al., 2014) has shown that the structure of discourse trees, especially the nuclearity of non-terminal discourse relations in the tree, is valuable for content selection in summarization. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) on the other hand is theory-neutral and does not define a recursive structure for the entire document like RST. Discourse relations are lexically bound to explicit discourse connectives within a sentence or exist between adjacent sentences if there is no connective. Each relation is realized in two text arguments, which are similar to EDUs."
W16-3617,C08-1110,1,0.750944,"rovements in recent years (Hernault et al., 2010; Joty et al., 2015). It is now practical to segment document sentences into EDUs at scale as a preprocessing step for automated summarization. N As your floppy drive writes or reads PURPOSE N a Syncom diskette is working four ways S to keep loose particles and dust from causing soft errors, drop-outs. Figure 1: A RST discourse tree with EDUs as leaf nodes (example from Mann and Thompson (1988)). n-grams are frequently used for quantifying content salience and redundancy prior to summarization over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes, 2011; Cao et al., 2015). In contrast, when the task at hand is more abstractive, the units are more finegrained, e.g., n-grams and phrases in abstractive summarization (Kikuchi et al., 2014; Liu et al., 2015; Bing et al., 2015), n-grams and humanannotated concept units in summarization evaluation (Lin, 2004; Hovy et al., 2006). Recently, subject-verb-object triplets were used to automatically identify concept units (Yang et al., 2016) and in abstractive summarization (Li, 2015); however, this requires semantic processing while EDU segmentation is pres"
W16-3617,D15-1219,0,0.0131793,"ation over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes, 2011; Cao et al., 2015). In contrast, when the task at hand is more abstractive, the units are more finegrained, e.g., n-grams and phrases in abstractive summarization (Kikuchi et al., 2014; Liu et al., 2015; Bing et al., 2015), n-grams and humanannotated concept units in summarization evaluation (Lin, 2004; Hovy et al., 2006). Recently, subject-verb-object triplets were used to automatically identify concept units (Yang et al., 2016) and in abstractive summarization (Li, 2015); however, this requires semantic processing while EDU segmentation is presently more accurate and scalable. Here, we explore EDUs as a middle ground between fine-grained lexical units and full sentences. While EDUs have been used in prior work to directly assemble output summaries (Marcu, 1999; Hirao et al., 2013; Yoshida et al., 2014), the focus was on using discourse structure as features for sentence ranking, while our work is the first to examine the utility of EDUs themselves. Datasets. In this work, we address singledocument summarization. Standard datasets for the task were created for"
W16-3617,D12-1022,0,0.0123486,"8 9 # overlapping EDUs per contributor Figure 3: Number of EDUs which overlap with each SCU contributor (single or multi-part) in the DUC/TAC reference summary datasets. Compressive summarization. To explore the utility of EDUs in summarization, we examine near-extractive summaries in the NYT corpus which are drawn from sentences in the document but omit at least one word or phrase from them. This setting is also explored in the summarization literature for techniques which combine extractive sentence selection with sentence compression (Clarke and Lapata, 2007; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Kikuchi et al., 2014). These approaches are typically evaluated against abstractive summaries and have not been studied with a natural compressive dataset such as the ones proposed here. We do not address techniques to generate compressive summaries in this work but instead attempt to quantify how the omitted content in a summary relates to its EDU segmentation. 3 Contiguous Multi-part 3.1 Data and settings In the DUC 2005–2007 and TAC 2008–2011 shared tasks on multi-document summarization, evaluations are conducted under the pyramid method—a technique which quanti"
W16-3617,P11-1052,0,0.0214048,"ty et al., 2015). It is now practical to segment document sentences into EDUs at scale as a preprocessing step for automated summarization. N As your floppy drive writes or reads PURPOSE N a Syncom diskette is working four ways S to keep loose particles and dust from causing soft errors, drop-outs. Figure 1: A RST discourse tree with EDUs as leaf nodes (example from Mann and Thompson (1988)). n-grams are frequently used for quantifying content salience and redundancy prior to summarization over sentences (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008; Gillick and Favre, 2009; Lin and Bilmes, 2011; Cao et al., 2015). In contrast, when the task at hand is more abstractive, the units are more finegrained, e.g., n-grams and phrases in abstractive summarization (Kikuchi et al., 2014; Liu et al., 2015; Bing et al., 2015), n-grams and humanannotated concept units in summarization evaluation (Lin, 2004; Hovy et al., 2006). Recently, subject-verb-object triplets were used to automatically identify concept units (Yang et al., 2016) and in abstractive summarization (Li, 2015); however, this requires semantic processing while EDU segmentation is presently more accurate and scalable. Here, we expl"
W16-3617,W04-1013,0,0.121571,"pler problem than human summarization. This leads to a natural question: can extractive summarization techniques be used to produce more human-like summaries? We hypothesize that automated methods can generate a wider range of summaries by extracting over sub-sentential units of meaning from the source documents rather than whole sentences. Specifically, in this paper we investigate whether elementary discourse units (EDUs) from Rhetorical Structure Theory (Mann and Thompson, 1988) comprise viable textual units for summarization. Our focus is on recovering salient summary content under ROUGE (Lin, 2004) while the composition of EDUs into fluent output sentences is left to future work. We investigate this hypothesis in two complementary ways: by studying the compatibility of EDUs with human-labeled summarization units from pyramid evaluations (Nenkova et al., 2007) and by assessing their utility in reconstructing real-world document previews chosen by news editors in the New York Times corpus (Sandhaus, 2008). The contributions of this work include: Although human-written summaries of documents tend to involve significant edits to the source text, most automated summarizers are extractive and"
W16-3617,D14-1196,0,0.231527,"tion Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) represents the discourse in a document in the form of a tree (Figure 1). The leaf nodes of RST trees are elementary discourse units (EDUs) which are a segmentation of sentences into independent clauses, including dependencies such as clausal subjects and complements. The more central units to each RST relation are nuclei while the more peripheral are satellites. Prior work in document compression (Daum´e and Marcu, 2002) and single-document summarization (Marcu, 1999; Louis et al., 2010; Hirao et al., 2013; Kikuchi et al., 2014; Yoshida et al., 2014) has shown that the structure of discourse trees, especially the nuclearity of non-terminal discourse relations in the tree, is valuable for content selection in summarization. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) on the other hand is theory-neutral and does not define a recursive structure for the entire document like RST. Discourse relations are lexically bound to explicit discourse connectives within a sentence or exist between adjacent sentences if there is no connective. Each relation is realized in two text arguments, which are similar to EDUs. However, unlike EDUs, P"
W16-3617,P13-1020,0,\N,Missing
W16-3617,W01-0100,0,\N,Missing
W16-3617,N15-1114,0,\N,Missing
W16-3617,P11-1049,0,\N,Missing
