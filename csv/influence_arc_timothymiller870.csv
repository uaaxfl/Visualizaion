2020.bionlp-1.7,E17-2118,1,0.84358,"cal Temporal Relation Extraction Chen Lin1 , Timothy Miller1* , Dmitriy Dligach2 , Farig Sadeque1 , Steven Bethard3 and Guergana Savova1 * Co-first author Boston Children’s Hospital and Harvard Medical School 2 Loyola University Chicago 3 University of Arizona 1 {first.last}@childrens.harvard.edu 2 ddligach@luc.edu 3 bethard@email.arizona.edu the CONTAINS task remains a challenge for both Abstract 1 conventional learning approaches (Sun et al., 2013; Bethard et al., 2015, 2016, 2017) and neural models (structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long ShortTerm memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018; Galvan et al., 2018)). The difficulty is that the limited labeled data is insufficient for training deep neural models for complex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that"
2020.bionlp-1.7,W18-5607,0,0.0120221,"author Boston Children’s Hospital and Harvard Medical School 2 Loyola University Chicago 3 University of Arizona 1 {first.last}@childrens.harvard.edu 2 ddligach@luc.edu 3 bethard@email.arizona.edu the CONTAINS task remains a challenge for both Abstract 1 conventional learning approaches (Sun et al., 2013; Bethard et al., 2015, 2016, 2017) and neural models (structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long ShortTerm memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018; Galvan et al., 2018)). The difficulty is that the limited labeled data is insufficient for training deep neural models for complex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that is highly wasteful – the same sentence must be processed multiple times, once for each candidate relation pair. Inspired by recent work in Green AI"
2020.bionlp-1.7,E17-1108,0,0.355691,"Missing"
2020.bionlp-1.7,S15-2136,1,0.899432,"ed to the document creation time (DCT) as Document Time Relations (DocTimeRel), with possible values of BEFORE, AFTER, OVERLAP, and BEFORE OVERLAP (Styler IV et al., 2014). At a finer level, a narrative container (Pustejovsky and Stubbs, 2011) can temporally subsume an event as a contains relation. The THYME corpus (Styler IV et al., 2014) consists of EMR clinical text and is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011). It was used in the Clinical TempEval series (Bethard et al., 2015, 2016, 2017). While the performance of DocTimeRel models has reached above 0.8 F1 on the THYME corpus, 70 Proceedings of the BioNLP 2020 workshop, pages 70–75 c Online, July 9, 2020 2020 Association for Computational Linguistics the conventional token BERT inserts at the start of every input sequence and its embedding is viewed as the representation of the entire sequence. The concatenated embedding is passed to a linear classifier to predict the CONTAINS, CONTAINED-BY, or NONE relation, rˆij , as in eq. (1). P (rˆij |x, Ei , Ej )=sof tmax(W L [G : ei : ej ] + b) (1) where W L ∈ R3dz ×lr , dz"
2020.bionlp-1.7,W18-5619,1,0.808752,"Savova1 * Co-first author Boston Children’s Hospital and Harvard Medical School 2 Loyola University Chicago 3 University of Arizona 1 {first.last}@childrens.harvard.edu 2 ddligach@luc.edu 3 bethard@email.arizona.edu the CONTAINS task remains a challenge for both Abstract 1 conventional learning approaches (Sun et al., 2013; Bethard et al., 2015, 2016, 2017) and neural models (structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long ShortTerm memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018; Galvan et al., 2018)). The difficulty is that the limited labeled data is insufficient for training deep neural models for complex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that is highly wasteful – the same sentence must be processed multiple times, once for each candidate relation pair. Inspired by r"
2020.bionlp-1.7,W17-2341,1,0.77707,"Extraction Chen Lin1 , Timothy Miller1* , Dmitriy Dligach2 , Farig Sadeque1 , Steven Bethard3 and Guergana Savova1 * Co-first author Boston Children’s Hospital and Harvard Medical School 2 Loyola University Chicago 3 University of Arizona 1 {first.last}@childrens.harvard.edu 2 ddligach@luc.edu 3 bethard@email.arizona.edu the CONTAINS task remains a challenge for both Abstract 1 conventional learning approaches (Sun et al., 2013; Bethard et al., 2015, 2016, 2017) and neural models (structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long ShortTerm memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018; Galvan et al., 2018)). The difficulty is that the limited labeled data is insufficient for training deep neural models for complex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that is highly wasteful"
2020.bionlp-1.7,S17-2093,1,0.888897,"Missing"
2020.bionlp-1.7,W19-1908,1,0.405446,"bethard@email.arizona.edu the CONTAINS task remains a challenge for both Abstract 1 conventional learning approaches (Sun et al., 2013; Bethard et al., 2015, 2016, 2017) and neural models (structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long ShortTerm memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018; Galvan et al., 2018)). The difficulty is that the limited labeled data is insufficient for training deep neural models for complex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that is highly wasteful – the same sentence must be processed multiple times, once for each candidate relation pair. Inspired by recent work in Green AI (Schwartz et al., 2019; Strubell et al., 2019), and one-pass encodings for multiple relations extraction (Wang et al., 2019), we propose a one-pass encoding mechanism"
2020.bionlp-1.7,2021.ccl-1.108,0,0.151401,"Missing"
2020.bionlp-1.7,W11-0419,0,0.4223,"he THYME corpus and is much “greener” in computational cost. 1 Introduction The analysis of many medical phenomena (e.g., disease progression, longitudinal effects of medications, treatment regimen and outcomes) heavily depends on temporal relation extraction from the clinical free text embedded in the Electronic Medical Records (EMRs). At a coarse level, a clinical event can be linked to the document creation time (DCT) as Document Time Relations (DocTimeRel), with possible values of BEFORE, AFTER, OVERLAP, and BEFORE OVERLAP (Styler IV et al., 2014). At a finer level, a narrative container (Pustejovsky and Stubbs, 2011) can temporally subsume an event as a contains relation. The THYME corpus (Styler IV et al., 2014) consists of EMR clinical text and is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011). It was used in the Clinical TempEval series (Bethard et al., 2015, 2016, 2017). While the performance of DocTimeRel models has reached above 0.8 F1 on the THYME corpus, 70 Proceedings of the BioNLP 2020 workshop, pages 70–75 c Online, July 9, 2020 2020 Association for Computational Lingu"
2020.bionlp-1.7,P19-1355,0,0.0207776,"the limited labeled data is insufficient for training deep neural models for complex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that is highly wasteful – the same sentence must be processed multiple times, once for each candidate relation pair. Inspired by recent work in Green AI (Schwartz et al., 2019; Strubell et al., 2019), and one-pass encodings for multiple relations extraction (Wang et al., 2019), we propose a one-pass encoding mechanism for the CONTAINS relation extraction task, which can significantly increase the efficiency and scalability. The architecture is shown in Figure 1. The three novel modifications to the original one-pass relational model of Wang et al. (2019) are: (1) Unlike Wang et al. (2019), our model operates in the relation extraction setting, meaning it must distinguish between relations and nonrelations, as well as classifying by relation type. (2) We introduce a pooled embedding for re"
2020.bionlp-1.7,P17-2035,0,0.0158578,"arig Sadeque1 , Steven Bethard3 and Guergana Savova1 * Co-first author Boston Children’s Hospital and Harvard Medical School 2 Loyola University Chicago 3 University of Arizona 1 {first.last}@childrens.harvard.edu 2 ddligach@luc.edu 3 bethard@email.arizona.edu the CONTAINS task remains a challenge for both Abstract 1 conventional learning approaches (Sun et al., 2013; Bethard et al., 2015, 2016, 2017) and neural models (structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long ShortTerm memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018; Galvan et al., 2018)). The difficulty is that the limited labeled data is insufficient for training deep neural models for complex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that is highly wasteful – the same sentence must be processed multiple times, once for eac"
2020.bionlp-1.7,P19-1132,0,0.349241,"lex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that is highly wasteful – the same sentence must be processed multiple times, once for each candidate relation pair. Inspired by recent work in Green AI (Schwartz et al., 2019; Strubell et al., 2019), and one-pass encodings for multiple relations extraction (Wang et al., 2019), we propose a one-pass encoding mechanism for the CONTAINS relation extraction task, which can significantly increase the efficiency and scalability. The architecture is shown in Figure 1. The three novel modifications to the original one-pass relational model of Wang et al. (2019) are: (1) Unlike Wang et al. (2019), our model operates in the relation extraction setting, meaning it must distinguish between relations and nonrelations, as well as classifying by relation type. (2) We introduce a pooled embedding for relational classification across long distances. Wang et al. (2019) focused on s"
2020.clinicalnlp-1.21,C18-1139,0,0.018953,"Missing"
2020.clinicalnlp-1.21,S15-2136,1,0.739815,"publicly available wiki of cancer and blood disorder treatment regimens and interventions. Second, we included 73 radiotherapy descriptions from a state cancer registry. These are abstractions from patients’ EMR, often copied and pasted from clinician notes, describing details of cancer treatment for use by cancer registrars within a patient-level XML. We used the entire text of the XML categories that contained radiotherapy details as model input. Third, 79 completed breast and colorectal cancer clinician notes that contained radiotherapy details from the THYME corpus from Clinical TempEval (Bethard et al., 2015) and an internal corpus with breast cancer notes. Annotation guidelines for radiotherapy properties and treatment instances were developed (Bitterman et al., 2020).1 If an overall radiotherapy treatment course was delivered in more than one Dose-Treatment Site relationship is represented as: …She presented after a screening mammogram showed a nodule in the left breast upper outer quadrant. After lumpectomy, she was treated with radiation to a dose of 50 Gy in 25 fractions to the left breast, followed by a boost of RT_DosageStart 10 Gy RTDosage_End in 5 fractions to the TxSiteStart tumor bed Tx"
2020.clinicalnlp-1.21,P13-1162,0,0.0232758,"Missing"
2020.clinicalnlp-1.4,W18-5615,0,0.0729319,"ders (Hines et al., 2014). Readmissions are harmful both in being disruptive to patients and families, and as a major driver of health-care costs in psychiatry (Wu et al., 2005; Mangalore and Knapp, 2007). Also, premature discharge of patients contributes not only to rehospitalization but to increased risk of homelessness and the possibility of violent behavior or suicide. Thus, reducing rates of early hospital readmission has been recognized and identified as a key to improve quality of care and reduce costs. There are a number of risk factors that previous work compiled from the literature (Holderness et al., 2018) to be important for understanding readmission risk, including such factors as problems with substance abuse, ability to maintain work, relations with family. Studying readmission through 35 Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages 35–40 c November 19, 2020. 2020 Association for Computational Linguistics from this latter technique point in a direction that could allow for further improvements even without additional training data. 2 with the training data, as described in the previous study (Holderness et al., 2019). We divided the training instances into tra"
2020.clinicalnlp-1.4,2021.ccl-1.108,0,0.0557436,"Missing"
2020.clinicalnlp-1.4,N19-1423,0,0.0538196,"Missing"
2020.clinicalnlp-1.4,W19-1915,1,0.624943,"t are important for readmission; and 2) The possibility that a risk classifier that uses explicit risk factors will be more interpretable and trustworthy to providers, and thus more likely to be put into practical use. In this work, we make use of a publicly available data set of sentences from discharge summary that is annotated for seven risk factors, along with the “sentiment,” marked as Positive, Negative, or Neutral (more details in background). Previous work on this dataset showed that the task was approachable with neural methods, but performance suffered because of small dataset size (Holderness et al., 2019). Here, we address this issue with two advances. First, we show that transfer learning helps dramatically. While the previous work used smaller neural models trained from scratch, we start from pre-trained transformer models (RoBERTa (Liu et al., 2019)), and improve them for the task with architectural and data augmentation strategies. Second, we show that sharing data between the different risk factor domains is better for performance than training separately, at least with pretrained models. Previous work trained separate classifiers for each of the risk factor domains, due to the (legitimat"
2020.louhi-1.12,araki-etal-2014-detecting,0,0.0186208,"cal, non-identical, or mereologically (part-whole) related. In keeping with our primary goal of enabling timeline extraction, we implemented CON-SUB as a TLINK since it conveys true temporal containment. CON-SUB, however, differs from other TLINKs, which were constrained by proximity and lexical cues, as discussed in section 2. The fact that CON-SUB also represents structural information allowed us to treat it like a coreference/bridging relation in terms of permissible textual evidence for link creation: namely, semantic scripts. These may be defined as: “a stereotypical sequence of events” (Araki et al., 2014) or “prototypical schematic sequences of events” (Chambers and Jurafsky, 2008). We can expect a surgery, for example, to consist of certain, typical subevents (incisions, subprocedures, anesthesia administration, etc.), which therefore enables annotators to look throughout the whole document for lexical items with meanings that fit those subevents. The concept of semantic scripts is what facilitates attainable long-distance coreference /bridging linking, and therefore, long-distance CON-SUB linking. This is obviously not the case for nonsubevent CONTAINS relations. 4.2 Inter-annotator Agreemen"
2020.louhi-1.12,S17-2093,1,0.860183,"3 School of Information, University of Arizona, Tucson, AZ 4 Department of Computer Science, Loyola University Chicago, Chicago, IL 1 {first.last}@childrens.harvard.edu 2 {first.last}@colorado.edu 3 bethard@arizona.edu 4 ddligach@luc.edu 017 018 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 leading US medical center. This dataset has previously undergone a variety of annotation efforts, most notably temporal annotation (Styler IV et al., 2014). It has been part of several SemEval shared tasks such as Clinical TempEval (Bethard et al., 2017) where state-of-the-art results have been established. Our goal was to utilize this THYME corpus to enable the extraction of more extensive patient timelines by manually creating cross-document links that built off the pre-existing single file annotations. (Wright-Bettner et al., 2019) discuss that a subset of the THYME temporal annotations contributed to incompatible temporal inferences, thus reducing their ability to support meaningful temporal reasoning. Accuracy and informativeness of temporal relation gold annotations are essential for their effectiveness in training a system for temporal"
2020.louhi-1.12,P08-1090,0,0.0789589,"g with our primary goal of enabling timeline extraction, we implemented CON-SUB as a TLINK since it conveys true temporal containment. CON-SUB, however, differs from other TLINKs, which were constrained by proximity and lexical cues, as discussed in section 2. The fact that CON-SUB also represents structural information allowed us to treat it like a coreference/bridging relation in terms of permissible textual evidence for link creation: namely, semantic scripts. These may be defined as: “a stereotypical sequence of events” (Araki et al., 2014) or “prototypical schematic sequences of events” (Chambers and Jurafsky, 2008). We can expect a surgery, for example, to consist of certain, typical subevents (incisions, subprocedures, anesthesia administration, etc.), which therefore enables annotators to look throughout the whole document for lexical items with meanings that fit those subevents. The concept of semantic scripts is what facilitates attainable long-distance coreference /bridging linking, and therefore, long-distance CON-SUB linking. This is obviously not the case for nonsubevent CONTAINS relations. 4.2 Inter-annotator Agreement While the gold intra-document CON-SUB relations enabled high cross-document"
2020.louhi-1.12,N19-1423,0,0.0715314,"(different narratives have different goals, which in turn influences meaning interpretation). This is discussed in detail in Section 4. We empirically found it essential to take changes in discourse context into account and suggest the same would be true for any annotation project that is interested in temporal reasoning, particularly those dealing with longer timelines (i.e., beyond the single-document level). Recent developments in natural language processing establish neural approaches and more specifically transformer-based methods as the state of the art. Pre-trained models such as BERT (Devlin et al., 2019), BioBERT (Lee et al., 2020), Xlnet (Yang et al., 2019), ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019), BART (Lewis et al., 2019), and SpanBERT (Joshi et al., 2020) report significant gains on multiple tasks. Thus, we demonstrate the learnability of the refined temporal relations in the context of these recent methodological developments. 2 prepositions and adjectives (e.g., during, subsequent to, prior to), chronological narrative progression, and so forth. Additionally, the notes had been separately annotated for intra-document coreference (IDENTICAL) and bridging (SET-SUBSET, WHOLEP"
2020.louhi-1.12,W13-1203,0,0.191253,"r. ii. cancer CONTAINS tumor iii. cancer CONTAINS adenocarcinoma The merged THYME and coreference annotations2 for (1) and (2) were as follows:      January 17, 2009 CONTAINS CT CT CONTAINS metastases February 20, 2009 CONTAINS resected metastases OVERLAPS resected metastases IDENTICAL metastases Both sets of intra-document links are pragmatically appropriate. Discourse contexts can expand or reduce the level of granularity at which a sense is interpreted (Recasens et al., 2011; also see Hobbs, 1985). In Note A, the text supports a coarse-grained interpretation of adenocarcinoma, or what Hovy et al., 2013 term a “wide” reading; it refers generally to the patient’s cancer. Note B, however, requires a fine-grained (“narrow”) interpretation – adenocarcinoma here refers specifically to the new, inoperable tumor and is contrasted with the original, resected tumor. The quandary for the cross-document task lies in whether to link adenocarcinoma in A as IDENTICAL to adenocarcinoma in B. An IDENTICAL relation entails logical impossibilities: assuming we also link cancerA as IDENT to cancerB, the combined within- and crossdocument relations now say the recurrent adenocarcinoma temporally contains itself"
2020.louhi-1.12,2020.tacl-1.5,0,0.153352,"take changes in discourse context into account and suggest the same would be true for any annotation project that is interested in temporal reasoning, particularly those dealing with longer timelines (i.e., beyond the single-document level). Recent developments in natural language processing establish neural approaches and more specifically transformer-based methods as the state of the art. Pre-trained models such as BERT (Devlin et al., 2019), BioBERT (Lee et al., 2020), Xlnet (Yang et al., 2019), ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019), BART (Lewis et al., 2019), and SpanBERT (Joshi et al., 2020) report significant gains on multiple tasks. Thus, we demonstrate the learnability of the refined temporal relations in the context of these recent methodological developments. 2 prepositions and adjectives (e.g., during, subsequent to, prior to), chronological narrative progression, and so forth. Additionally, the notes had been separately annotated for intra-document coreference (IDENTICAL) and bridging (SET-SUBSET, WHOLEPART) relations, which were later merged with the temporal annotations (Wright-Bettner et al., 2019). Temporal relations alone are insufficient for timeline extraction; core"
2020.louhi-1.12,P14-1094,0,0.029063,"these pre-existing within-note annotations by manually adding coreference and bridging links across each set of three notes. In the process, we discovered a subset of the original CONTAINS relations contributed to temporally-conflicting information, which led to the addition of two new TLINKs: NOTED-ON and CON-SUB (Wright-Bettner et al., 2019). We discuss below how these updates contribute to more accurate and comprehensive temporal relations which facilitated cross-document linking. As such, this is one of the few studies in clinical NLP for cross-document temporal relation annotations (see Raghavan et al., 2014 and Wright-Bettner et al., 2019; also see Song et al., 2018 for general domain cross-document temporal annotation discussions). Dataset The 594 notes that make up the colon cancer part of the THYME corpus are grouped into sets, each set pertaining to a single patient and consisting of three notes written at different times during the patient’s course of care. These notes had been previously annotated for five different intradocument temporal relations (BEFORE, OVERLAP, BEGINS-ON, ENDS-ON and CONTAINS), a subset of the ISO-TimeML temporal link (TLINK) types (Pustejovsky et al., 2010, Styler IV"
2020.louhi-1.12,L18-1558,0,0.035104,"Missing"
2020.louhi-1.12,W19-1908,1,0.922329,"e refinements of the THYME+ annotations. Splitting CONTAINS into CONTAINS and CON-SUB relations and OVERLAP into OVERLAP and NOTEDON relations leads to better learnability: CONTAINS goes from 0.664 F1 on THYME to 0.748 F1 on THYME+, and OVERLAP goes from 0.179 on THYME to 0.416 on THYME+. The best results for the new categories of CON-SUB and NOTED-ON are 0.072 F1 and 0.744 F1 respectively – results that establish baselines for these two new temporal relations. The performance on all types of relations for THYME+ is 0.625 F1 compared to 0.548 for THYME (Table 2, Overall column, rows 1 and 2). Lin et al., 2019 report 0.684 F1 for THYME CONTAINS, however the result is achieved when training on and evaluating for only the CONTAINS links, and augmenting the training data with automatically generated CONTAINS relations. Thus, it is not a fair comparison to use for the results reported in Table 2. Of the models beyond BioBERT that we explored, BART-large was the most successful. The result with BART-large was 0.748 F1 (Table 2, CONTAINS column, row 3). In general, certain pre-trained models, like BioBERT and BART, yield better results than the other models. BioBERT is pre-trained on biomedical text and"
2020.louhi-1.12,W16-5706,1,0.878583,"Missing"
2020.louhi-1.12,pustejovsky-etal-2010-iso,0,0.037864,"otations (see Raghavan et al., 2014 and Wright-Bettner et al., 2019; also see Song et al., 2018 for general domain cross-document temporal annotation discussions). Dataset The 594 notes that make up the colon cancer part of the THYME corpus are grouped into sets, each set pertaining to a single patient and consisting of three notes written at different times during the patient’s course of care. These notes had been previously annotated for five different intradocument temporal relations (BEFORE, OVERLAP, BEGINS-ON, ENDS-ON and CONTAINS), a subset of the ISO-TimeML temporal link (TLINK) types (Pustejovsky et al., 2010, Styler IV et al., 2014)1. To keep annotation manageable and circumvent massively inferential temporal linking, the THYME guidelines constrained TLINK creation to events within the same sentence or adjacent sentences, and specifically prohibited TLINKing across sections (these are clinically-delineated sections separated from each other by numerical section IDs – History of Present Illness is section 20103, Vital Signs is section 20110, etc.) Linguistic evidence for creating these TLINKs included local cues such as temporal 3 Refined Temporal Relation: NOTEDON The THYME guidelines specified t"
2020.louhi-1.12,D19-6201,1,0.888309,"Missing"
2020.nlpmc-1.1,W19-1909,0,0.0249108,"Missing"
2020.nlpmc-1.1,D19-1371,0,0.0120517,") or traditional word embedding models (e.g. Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), etc.) in a wide variety NLP tasks. BERT learns contextual embeddings through pre-training on a large unlabeled corpus (including the BooksCorpus (800M words) and English Wikipedia (2,500M words)) via two tasks, a masked language model task and a next sentence prediction task (Devlin et al., 2019). Domain-specific BERT models have been released, including BioBERT (Lee et al., 2020), which started from a BERT checkpoint and extended pre-training on biomedical journal articles, SciBERT (Beltagy et al., 2019), which is pre-trained from scratch with its own vocabulary, and ClinicalBERT (Alsentzer et al., 2019) which started from BERT checkpoints and extended pretraining using intensive care unit documents from the MIMIC corpus (Johnson et al., 2016). In this work, we use vanilla BERT, SciBERT, and two versions of ClinicalBERT, Bio+Clinical BERT and Bio+Discharge Summary BERT1 . 1 Materials and Methods Data We use de-identified text data from 2008-2017 from the San Francisco Department of Public Health (SFDPH), for which we examined one specialty (gastroenterology and liver) with over 33,000 eConsul"
2020.nlpmc-1.1,N19-1423,0,0.0110815,"e to occur together. 3 Background 3.1 BERT (Bidirectional Encoder Representations from Transformers), along its variants, have been proven to outperform other contextual embedding (e.g. ELMo (Peters et al., 2018)) or traditional word embedding models (e.g. Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), etc.) in a wide variety NLP tasks. BERT learns contextual embeddings through pre-training on a large unlabeled corpus (including the BooksCorpus (800M words) and English Wikipedia (2,500M words)) via two tasks, a masked language model task and a next sentence prediction task (Devlin et al., 2019). Domain-specific BERT models have been released, including BioBERT (Lee et al., 2020), which started from a BERT checkpoint and extended pre-training on biomedical journal articles, SciBERT (Beltagy et al., 2019), which is pre-trained from scratch with its own vocabulary, and ClinicalBERT (Alsentzer et al., 2019) which started from BERT checkpoints and extended pretraining using intensive care unit documents from the MIMIC corpus (Johnson et al., 2016). In this work, we use vanilla BERT, SciBERT, and two versions of ClinicalBERT, Bio+Clinical BERT and Bio+Discharge Summary BERT1 . 1 Materials"
2020.nlpmc-1.1,D14-1162,0,0.0919447,"he question type annotations. Values close to 0 represent variables whose distributions are independent, values above 0 (up to 1) represent pairs of variables that are more likely than chance to occur together, and values below 0 (down to -1) represent pairs of variables that are less likely than chance to occur together. 3 Background 3.1 BERT (Bidirectional Encoder Representations from Transformers), along its variants, have been proven to outperform other contextual embedding (e.g. ELMo (Peters et al., 2018)) or traditional word embedding models (e.g. Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), etc.) in a wide variety NLP tasks. BERT learns contextual embeddings through pre-training on a large unlabeled corpus (including the BooksCorpus (800M words) and English Wikipedia (2,500M words)) via two tasks, a masked language model task and a next sentence prediction task (Devlin et al., 2019). Domain-specific BERT models have been released, including BioBERT (Lee et al., 2020), which started from a BERT checkpoint and extended pre-training on biomedical journal articles, SciBERT (Beltagy et al., 2019), which is pre-trained from scratch with its own vocabulary, and ClinicalBERT (Alsentzer"
2020.nlpmc-1.1,N18-1202,0,0.0235165,"ween these two augmentations. 2 Figure 2: Normalized pointwise mutual information between categories in the question type annotations. Values close to 0 represent variables whose distributions are independent, values above 0 (up to 1) represent pairs of variables that are more likely than chance to occur together, and values below 0 (down to -1) represent pairs of variables that are less likely than chance to occur together. 3 Background 3.1 BERT (Bidirectional Encoder Representations from Transformers), along its variants, have been proven to outperform other contextual embedding (e.g. ELMo (Peters et al., 2018)) or traditional word embedding models (e.g. Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), etc.) in a wide variety NLP tasks. BERT learns contextual embeddings through pre-training on a large unlabeled corpus (including the BooksCorpus (800M words) and English Wikipedia (2,500M words)) via two tasks, a masked language model task and a next sentence prediction task (Devlin et al., 2019). Domain-specific BERT models have been released, including BioBERT (Lee et al., 2020), which started from a BERT checkpoint and extended pre-training on biomedical journal articles, SciBERT ("
2021.adaptnlp-1.11,S17-2093,1,0.874823,"nding to, for example, decisions to not use a drug. DocTimeRel helps us distinguish mentions of drugs that are current from those that predate the current time period, or are being speculated about for future use. ALINK can model drug start, stop, and continuation events, which can help to distinguish whether a missing mention in the middle of a record corresponds to a stop and restart, or an incidentally omitted mention. Figure 1 shows an example instance of a drug mention to be classified for all three tasks. The THYME dataset (Styler IV et al., 2014), released as part of Clinical TempEval (Bethard et al., 2017), contains all three of these annotation types, on 1200 notes of patients with colon and brain cancer. We train all models on the colon cancer section (details on data are in Section 4). While our bigger 106 project is specific to drug mentions, the problem is not limited to drug mentions, so we train and evaluate on all annotated events in the THYME corpus. We also assume that events are given, to allow a straightforward metric of how many events we “get right” when combining all property predictions. In the real world, events will have to be automatically detected, so our metric will be an u"
2021.adaptnlp-1.11,N19-1423,0,0.430519,"ancer corpus and an internal corpus, and show that performance of the combined systems is unacceptable despite good performance of individual systems. Although domain adaptation shows improvements on each individual system, the model selection problem is a barrier to improving overall pipeline performance. 1 Introduction Advances in machine learning methods and the release of annotated datasets of clinical texts (Uzuner et al., 2011; Styler IV et al., 2014) in the past decade has led to an increase of available clinical NLP systems for interesting tasks. Recent advances in pre-trained models (Devlin et al., 2019; Liu et al., 2019) have made ever more accurate clinical NLP systems possible. Unsupervised domain adaptation algorithms (e.g., Ziser and Reichart (2019)) have made it possible to reduce performance degradation when applying trained models to new domains. The great promise of these developments is that these methods can be combined into pipelines that allow for sophisticated information extraction capabilities for downstream clinical use cases. Rather than building one-off datasets for each complex downstream task that arises, standard NLP components could potentially be used as “Lego”-style"
2021.adaptnlp-1.11,W06-1673,0,0.116081,"s to new domains. The great promise of these developments is that these methods can be combined into pipelines that allow for sophisticated information extraction capabilities for downstream clinical use cases. Rather than building one-off datasets for each complex downstream task that arises, standard NLP components could potentially be used as “Lego”-style building blocks that allow for flexibly approaching new tasks as they arise. However, the existence of the building blocks alone does not solve this problem. Combining individual components into NLP pipelines can lead to cascading errors (Finkel et al., 2006). The true error rate for structured extraction tasks is potentially as high as the sum of the component tasks’ errors. For example, if the goal is to extract normalized concepts with assertion status, the concept error can come from normalization error, negation detection error, uncertainty detection error, etc, and the errors may not be correlated. These problems are exacerbated in the common case where individual components are trained on data from different domains, and tested on data from yet another domain. In this work, we quantitatively examine the issues described above in the context"
2021.adaptnlp-1.11,2020.acl-main.740,0,0.0448906,"Missing"
2021.adaptnlp-1.11,D19-1433,0,0.0224521,"examples. Since large pre-trained transformer models have arrived, they have been shown to be quite robust to out-of-distribution examples (Hendrycks et al., 2020), including on clinical tasks (Lin et al., 2020), where it was shown that adding domain adaptation layers on top of BERT was no better than BERT itself for negation detection. One of the few effective methods for improving the out-of-distribution performance of pre-trained transformer models has been to continue to pre-train the language modeling objective on the target domain data, before any fine-tuning is done on the source data (Han and Eisenstein, 2019; Gururangan et al., 2020). In this work, we focus on that method, since this is currently the most promising direction for adapting large pre-trained transformers. Specifically, to use 107 this method, we run additional masked language model training steps on the target training data from the RoBERTa-base checkpoint, before fine-tuning on the labeled colon cancer data, and then testing on target test data. We tune the learning rate for the language model pre-training on target development set data, optimizing for perplexity. 4 Evaluation For the three tasks of interest, we evaluate indomain ("
2021.adaptnlp-1.11,2020.acl-main.244,0,0.0345826,"Missing"
2021.adaptnlp-1.11,2020.bionlp-1.17,0,0.0366554,"ven though we use a metric, accuracy, that is forgiving to the worst-performing individual model. 2 Background It is both formally and empirically understood that classifiers can suffer performance loss when the test data is drawn from a different distribution than the training data (sometimes called domain shift). This presents a difficult challenge in clinical NLP because data-sharing limitations make it difficult to create large and diverse training corpora. As a result, domain adaptation approaches have been applied to multiple tasks in clinical NLP (Miller et al., 2017; Liu et al., 2018; Hur et al., 2020). Recent work in the general domain has made use of transfer learning, which can attack the problem of domain shift, but by a different mechanism than domain adaptation; by training on massive corpora, large pre-trained models both learn general features, and are able to learn from smaller new datasets without overfitting. The most prominent of these models are based on the transformer architecture (Vaswani et al., 2017). BERT (Devlin et al., 2019) uses a transformer encoder, and has shown that pre-training with massive amounts of text on a language modeling task, then fine-tuning on a supervi"
2021.adaptnlp-1.11,W18-2315,0,0.0185871,"This is the case even though we use a metric, accuracy, that is forgiving to the worst-performing individual model. 2 Background It is both formally and empirically understood that classifiers can suffer performance loss when the test data is drawn from a different distribution than the training data (sometimes called domain shift). This presents a difficult challenge in clinical NLP because data-sharing limitations make it difficult to create large and diverse training corpora. As a result, domain adaptation approaches have been applied to multiple tasks in clinical NLP (Miller et al., 2017; Liu et al., 2018; Hur et al., 2020). Recent work in the general domain has made use of transfer learning, which can attack the problem of domain shift, but by a different mechanism than domain adaptation; by training on massive corpora, large pre-trained models both learn general features, and are able to learn from smaller new datasets without overfitting. The most prominent of these models are based on the transformer architecture (Vaswani et al., 2017). BERT (Devlin et al., 2019) uses a transformer encoder, and has shown that pre-training with massive amounts of text on a language modeling task, then fine-"
2021.adaptnlp-1.11,2021.ccl-1.108,0,0.0694764,"Missing"
2021.adaptnlp-1.11,W17-2320,1,0.790952,"om multiple systems. This is the case even though we use a metric, accuracy, that is forgiving to the worst-performing individual model. 2 Background It is both formally and empirically understood that classifiers can suffer performance loss when the test data is drawn from a different distribution than the training data (sometimes called domain shift). This presents a difficult challenge in clinical NLP because data-sharing limitations make it difficult to create large and diverse training corpora. As a result, domain adaptation approaches have been applied to multiple tasks in clinical NLP (Miller et al., 2017; Liu et al., 2018; Hur et al., 2020). Recent work in the general domain has made use of transfer learning, which can attack the problem of domain shift, but by a different mechanism than domain adaptation; by training on massive corpora, large pre-trained models both learn general features, and are able to learn from smaller new datasets without overfitting. The most prominent of these models are based on the transformer architecture (Vaswani et al., 2017). BERT (Devlin et al., 2019) uses a transformer encoder, and has shown that pre-training with massive amounts of text on a language modelin"
2021.adaptnlp-1.11,P19-1591,0,0.0187895,"s. Although domain adaptation shows improvements on each individual system, the model selection problem is a barrier to improving overall pipeline performance. 1 Introduction Advances in machine learning methods and the release of annotated datasets of clinical texts (Uzuner et al., 2011; Styler IV et al., 2014) in the past decade has led to an increase of available clinical NLP systems for interesting tasks. Recent advances in pre-trained models (Devlin et al., 2019; Liu et al., 2019) have made ever more accurate clinical NLP systems possible. Unsupervised domain adaptation algorithms (e.g., Ziser and Reichart (2019)) have made it possible to reduce performance degradation when applying trained models to new domains. The great promise of these developments is that these methods can be combined into pipelines that allow for sophisticated information extraction capabilities for downstream clinical use cases. Rather than building one-off datasets for each complex downstream task that arises, standard NLP components could potentially be used as “Lego”-style building blocks that allow for flexibly approaching new tasks as they arise. However, the existence of the building blocks alone does not solve this probl"
2021.bionlp-1.21,W19-1909,0,0.0260304,"Missing"
2021.bionlp-1.21,D19-1371,0,0.0358033,"Missing"
2021.bionlp-1.21,S15-2136,1,0.718023,"ilot data, MASCC score appears promising in determining suitability for outpatient management of NF in gynecologic oncology patients. Prospective study is ongoing to confirm safety and determine impact on cost. Figure 2: Sample instances for DocTimeRel(1), TLINK:event-time(2), TLINK:event-event(3), Negation (4), and PubMedQA (5). 2.3 Labeled Fine-tuning Data The following sections describe the labeled datasets that are used as fine-tuning tasks. Figure 2 shows examples of how we format inputs for these tasks (more details below). THYME The THYME corpus (Styler IV et al., 2014) is widely used (Bethard et al., 2015, 2016, 2017) for clinical temporal relation discovery. There are two types of temporal relations defined in it: (1) The document time relations (DocTimeRel), which link a clinical event (EVENT) to the document creation time (DCT) with possible values of BEFORE, AFTER, OVERLAP, and BEFORE_OVERLAP, and (2) pairwise temporal relations (TLINK) between two events (EVENT) or an event and a time expression (TIMEX3) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011). Recently, the TLINK annotations of (2) were refined with values of BEFORE, BEGINS-ON, CONTAINS, CONS"
2021.bionlp-1.21,E17-2118,1,0.933747,"e Beth Israel Deaconess Medical Center between 2001 and 2012. We process the MIMIC-III corpus with the sentence detection, tokenization, and temporal modules of Apache cTAKES (Savova et al., 2010)2 to identify all entities (events and time expressions) in the corpus. Events are recognized by cTAKES event annotator. Event types include diseases/disorders, signs/symptoms, medications, anatomical sites, and procedures. Time expressions are recognized by cTAKES timex annotator. Time classes includes: date, time, duration, quantifier, prepostesp, and set (Styler IV et al., 2014). Special XML tags (Dligach et al., 2017) are inserted into the text sequence to mark the position of identified entities. Time expressions are replaced by their time class (Lin et al., 2017, 2018) for better generalizability. All special XML-tags and time class tokens are added into the PubMedBERT vocabulary so that they can be recognized. The top line of Figure 1 shows a sample sentence from the MIMIC-III corpus. The entities of this sentence are identified by Apache cTAKES. The bottom line of Figure 1 shows the entities marked by XML tags and the temporal expression replaced by its class. We process the MIMIC corpus sentence by se"
2021.bionlp-1.21,2020.tacl-1.5,0,0.30533,"ent amounts of labeled data. Many clinical NLP tasks are centered around entities: clinical named entity recognition aims to detect clinical entities (Wu et al., 2017; Pradhan et al., 2014; Elhadad et al., 2015), clinical negation extraction decides if a certain clinical entity is negated (Chapman et al., 2001; Harkema et al., 2009; Mehrabi et al., 2015), clinical relation discovery extracts relations among clinical entities (Lv et al., 2016; Leeuwenberg and Moens, 2017), etc. Though various masking strategies have been employed during pretraining – masking contiguous spans of text (SpanBERT, Joshi et al., 2020; BART, Lewis et al., 2019), varying masking ratios (Raffel et al., 2019), building additional neural models to predict which words to mask (Gu et al., 2020b), incorporating knowledge graphs (Zhang et al., 2019), masking entities for a named entity recognition task (Ziyadi et al., 2020) – none of the masking techniques so far have investigated and focused on clinical entities. Besides transformer-based models, there are other efforts (Beam et al., 2019; Chen et al., 2020) to characterize the biomedical/clinical entities at the word embedding level. There are also other statistical methods appl"
2021.bionlp-1.21,E17-1108,0,0.0406111,"Missing"
2021.bionlp-1.21,S15-2051,1,0.741722,"represents one specialty in medicine – intensive care. Pretraining is agnostic to downstream tasks: it learns representations for all words using a selfsupervised data-rich task. Yet, not all words are important for downstream fine-tuning tasks. Numerous pretrained words are not even used in the fine-tuning step, while important words crucial for the downstream task are not well represented due to insufficient amounts of labeled data. Many clinical NLP tasks are centered around entities: clinical named entity recognition aims to detect clinical entities (Wu et al., 2017; Pradhan et al., 2014; Elhadad et al., 2015), clinical negation extraction decides if a certain clinical entity is negated (Chapman et al., 2001; Harkema et al., 2009; Mehrabi et al., 2015), clinical relation discovery extracts relations among clinical entities (Lv et al., 2016; Leeuwenberg and Moens, 2017), etc. Though various masking strategies have been employed during pretraining – masking contiguous spans of text (SpanBERT, Joshi et al., 2020; BART, Lewis et al., 2019), varying masking ratios (Raffel et al., 2019), building additional neural models to predict which words to mask (Gu et al., 2020b), incorporating knowledge graphs (Z"
2021.bionlp-1.21,J81-4005,0,0.637309,"Missing"
2021.bionlp-1.21,2020.emnlp-main.566,0,0.222855,"ppears to provide a vocabare pre-trained on large general domain corpora, ulary that is helpful to the clinical domain. Howmany efforts have been made to continue pre- ever, the language of biomedical literature is diftaining general-domain language models on clini- ferent from the language of the clinical documents cal/biomedical corpora to derive domain-specific found in electronic medical records (EMRs). In language models (Lee et al., 2020; Alsentzer et al., general, a clinical document is written by physi2019; Beltagy et al., 2019). cians who have very limited time to express the Yet, as Gu et al. (2020a) pointed out, in special- numerous details of a patient-physician encounter. ized domains such as biomedicine, continued pre- Many nonstandard expressions, abbreviations, astraining from generic language models is inferior sumptions and domain knowledge are used in clinito domain-specific pretraining from scratch. Con- cal notes which makes the text hard to understand tinued pre-training from a generic model would outside of the clinical community and presents 191 Transformer-based neural language models have led to breakthroughs for a variety of natural language processing (NLP) tasks. Howe"
2021.bionlp-1.21,D19-1259,0,0.369594,"oad representation of biomedical terminology (PubMedBERT) on a clinical corpus along with a novel entity-centric masking strategy to infuse domain knowledge in the learning process. We show that such a model achieves superior results on clinical extraction tasks by comparing our entity-centric masking strategy with classic random masking on three clinical NLP tasks: cross-domain negation detection (Wu et al., 2014), document time relation (DocTimeRel) classification (Lin et al., 2020b), and temporal relation extraction (Wright-Bettner et al., 2020). We also evaluate our models on the PubMedQA(Jin et al., 2019) dataset to measure the models’ performance on a nonentity-centric task in the biomedical domain. The language addressed in this work is English. Proceedings of the BioNLP 2021 workshop, pages 191–201 June 11, 2021. ©2021 Association for Computational Linguistics challenges for automated systems. Pretraining a language model specific to the clinical domain requires large amounts of unlabeled clinical text on par with what the generic models are trained on. Unfortunately, such data are not available to the community. The only available such corpus is MIMIC III used to train ClinicalBERT (Alsent"
2021.bionlp-1.21,2020.acl-main.703,0,0.0877052,"Missing"
2021.bionlp-1.21,W18-5619,1,0.903048,"Missing"
2021.bionlp-1.21,W17-2341,1,0.747659,"les of Apache cTAKES (Savova et al., 2010)2 to identify all entities (events and time expressions) in the corpus. Events are recognized by cTAKES event annotator. Event types include diseases/disorders, signs/symptoms, medications, anatomical sites, and procedures. Time expressions are recognized by cTAKES timex annotator. Time classes includes: date, time, duration, quantifier, prepostesp, and set (Styler IV et al., 2014). Special XML tags (Dligach et al., 2017) are inserted into the text sequence to mark the position of identified entities. Time expressions are replaced by their time class (Lin et al., 2017, 2018) for better generalizability. All special XML-tags and time class tokens are added into the PubMedBERT vocabulary so that they can be recognized. The top line of Figure 1 shows a sample sentence from the MIMIC-III corpus. The entities of this sentence are identified by Apache cTAKES. The bottom line of Figure 1 shows the entities marked by XML tags and the temporal expression replaced by its class. We process the MIMIC corpus sentence by sentence, and discard sentences that have fewer than two entities. The resulting set (MIMIC-BIG) has 15.6 million sentences, 728.6 million words (the b"
2021.bionlp-1.21,W19-1908,1,0.8231,"stejovsky and Stubbs, 2011). Recently, the TLINK annotations of (2) were refined with values of BEFORE, BEGINS-ON, CONTAINS, CONSUB, ENDS-ON, NOTED-ON, OVERLAP, with the revised corpus known as the THYME+ corpus (Wright-Bettner et al., 2020). For the DocTimeRel task, we mark all events in THYME+ corpus with XML tags (“&lt;e>”, “&lt;/e>”) and extract 10 tokens from each side of the event as the contextual information. The DocTimeRel 193 labels are predicted using the special [CLS] embedding and a softmax function. For the TLINK task, we use the THYME+ annotation and the same window-based processing (Lin et al., 2019; Wright-Bettner et al., 2020) for generating relational candidates. The two entities involved in a relation candidate are marked by XML tags following the style of Dligach et al. (2017). Time expressions are represented by their time classes. The TLINK labels are predicted using the special [CLS] embedding and a softmax function. Cross-domain Negation We use the same corpora as Miller et al. (2017); Lin et al. (2020a): (1) 2010 i2b2/VA NLP Challenge Corpus (i2b2: Uzuner et al., 2011), (2) the Multi-source Integrated Platform for Answering Clinical Questions Corpus (MiPACQ: Albright et al., 20"
2021.bionlp-1.21,2020.bionlp-1.7,1,0.888738,"ain data. We propose a methodology to produce a model focused on the clinical domain: continued pretraining of a model with a broad representation of biomedical terminology (PubMedBERT) on a clinical corpus along with a novel entity-centric masking strategy to infuse domain knowledge in the learning process. We show that such a model achieves superior results on clinical extraction tasks by comparing our entity-centric masking strategy with classic random masking on three clinical NLP tasks: cross-domain negation detection (Wu et al., 2014), document time relation (DocTimeRel) classification (Lin et al., 2020b), and temporal relation extraction (Wright-Bettner et al., 2020). We also evaluate our models on the PubMedQA(Jin et al., 2019) dataset to measure the models’ performance on a nonentity-centric task in the biomedical domain. The language addressed in this work is English. Proceedings of the BioNLP 2021 workshop, pages 191–201 June 11, 2021. ©2021 Association for Computational Linguistics challenges for automated systems. Pretraining a language model specific to the clinical domain requires large amounts of unlabeled clinical text on par with what the generic models are trained on. Unfortunat"
2021.bionlp-1.21,2021.ccl-1.108,0,0.0941984,"Missing"
2021.bionlp-1.21,W17-2320,1,0.823998,"Missing"
2021.bionlp-1.21,P19-3007,0,0.0287133,"verall 0.773 0.781 Table 5: Effect of masking strategy (Rand or Entity) on cross-domain negation detection. Performance is in terms of F1. PubMedBERT RandMask EntityBERT Accuracy 0.760 0.738 0.750 Table 7: Performance of models on PubMedQA. Model RandMask EntityBERT within-sentence cross-sentence 4,021 4,156 757 768 total 4,778 4,924 Table 8: Correctly predicted TLINK counts by EntityBERT and RandMask before temporal closure. centric task like the TLINK extraction task, these entities can be better utilized for reasoning relations which they are part of. In Figure 5 we visualize with BertViz (Vig, 2019) Model Domain after before bfr/ovlp overlap overall the attention weights of head zero from the last RandMask same 0.88 0.92 0.78 0.94 0.92 layer of the fine-tuned RandMask and EntityBERT EntityBERT same 0.88 0.92 0.79 0.94 0.92 models on the TLINK task for a relation that EntiRandMask cross 0.65 0.65 0.34 0.74 0.69 EntityBERT cross 0.64 0.66 0.40 0.77 0.72 tyBERT correctly predicted but RandMask missed. The context is he has had steroid &lt;e> injection Table 6: Effect of masking strategy (Rand or Entity) &lt;/e> &lt;t> date &lt;/t>. A plausible explanation is that for in-domain (same) and cross-domain s"
2021.bionlp-1.21,W19-5006,0,0.018031,"performance on a nonentity-centric task in the biomedical domain. The language addressed in this work is English. Proceedings of the BioNLP 2021 workshop, pages 191–201 June 11, 2021. ©2021 Association for Computational Linguistics challenges for automated systems. Pretraining a language model specific to the clinical domain requires large amounts of unlabeled clinical text on par with what the generic models are trained on. Unfortunately, such data are not available to the community. The only available such corpus is MIMIC III used to train ClinicalBERT (Alsentzer et al., 2019) and BlueBERT (Peng et al., 2019), but it is magnitudes smaller and represents one specialty in medicine – intensive care. Pretraining is agnostic to downstream tasks: it learns representations for all words using a selfsupervised data-rich task. Yet, not all words are important for downstream fine-tuning tasks. Numerous pretrained words are not even used in the fine-tuning step, while important words crucial for the downstream task are not well represented due to insufficient amounts of labeled data. Many clinical NLP tasks are centered around entities: clinical named entity recognition aims to detect clinical entities (Wu e"
2021.bionlp-1.21,S14-2007,1,0.875764,"Missing"
2021.bionlp-1.21,2020.louhi-1.12,1,0.852197,"focused on the clinical domain: continued pretraining of a model with a broad representation of biomedical terminology (PubMedBERT) on a clinical corpus along with a novel entity-centric masking strategy to infuse domain knowledge in the learning process. We show that such a model achieves superior results on clinical extraction tasks by comparing our entity-centric masking strategy with classic random masking on three clinical NLP tasks: cross-domain negation detection (Wu et al., 2014), document time relation (DocTimeRel) classification (Lin et al., 2020b), and temporal relation extraction (Wright-Bettner et al., 2020). We also evaluate our models on the PubMedQA(Jin et al., 2019) dataset to measure the models’ performance on a nonentity-centric task in the biomedical domain. The language addressed in this work is English. Proceedings of the BioNLP 2021 workshop, pages 191–201 June 11, 2021. ©2021 Association for Computational Linguistics challenges for automated systems. Pretraining a language model specific to the clinical domain requires large amounts of unlabeled clinical text on par with what the generic models are trained on. Unfortunately, such data are not available to the community. The only availa"
2021.bionlp-1.21,W11-0419,0,0.0145713,"t inputs for these tasks (more details below). THYME The THYME corpus (Styler IV et al., 2014) is widely used (Bethard et al., 2015, 2016, 2017) for clinical temporal relation discovery. There are two types of temporal relations defined in it: (1) The document time relations (DocTimeRel), which link a clinical event (EVENT) to the document creation time (DCT) with possible values of BEFORE, AFTER, OVERLAP, and BEFORE_OVERLAP, and (2) pairwise temporal relations (TLINK) between two events (EVENT) or an event and a time expression (TIMEX3) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011). Recently, the TLINK annotations of (2) were refined with values of BEFORE, BEGINS-ON, CONTAINS, CONSUB, ENDS-ON, NOTED-ON, OVERLAP, with the revised corpus known as the THYME+ corpus (Wright-Bettner et al., 2020). For the DocTimeRel task, we mark all events in THYME+ corpus with XML tags (“&lt;e>”, “&lt;/e>”) and extract 10 tokens from each side of the event as the contextual information. The DocTimeRel 193 labels are predicted using the special [CLS] embedding and a softmax function. For the TLINK task, we use the THYME+ annotation and the same window-based processing (Lin et al., 2019; Wright-Be"
2021.bionlp-1.21,D17-1035,0,0.0482038,"Missing"
2021.bionlp-1.21,P19-1139,0,0.143485,"), clinical negation extraction decides if a certain clinical entity is negated (Chapman et al., 2001; Harkema et al., 2009; Mehrabi et al., 2015), clinical relation discovery extracts relations among clinical entities (Lv et al., 2016; Leeuwenberg and Moens, 2017), etc. Though various masking strategies have been employed during pretraining – masking contiguous spans of text (SpanBERT, Joshi et al., 2020; BART, Lewis et al., 2019), varying masking ratios (Raffel et al., 2019), building additional neural models to predict which words to mask (Gu et al., 2020b), incorporating knowledge graphs (Zhang et al., 2019), masking entities for a named entity recognition task (Ziyadi et al., 2020) – none of the masking techniques so far have investigated and focused on clinical entities. Besides transformer-based models, there are other efforts (Beam et al., 2019; Chen et al., 2020) to characterize the biomedical/clinical entities at the word embedding level. There are also other statistical methods applied to the downstream tasks. We do not include these efforts in our discussion because the focus of our paper is the investigation of a novel entity-based masking strategy in a transformer-based setting. entity-"
2021.semeval-1.42,L16-1599,1,0.752609,"e train set is never distributed to the participants. data, we used the i2b2 2010 Challenge Dataset, a de-identified dataset of notes from Partners HealthCare. The evaluation dataset for this task consisted of de-identified intensive care unit progress notes from the MIMIC III corpus (Johnson et al., 2016). Time expression recognition has been a key component of previous temporal language related competitions, like TempEval 2010 (Pustejovsky and Verhagen, 2009) and TempEval 2013 (UzZaman et al., 2013). For this task, we followed the Compositional Annotation of Time Expressions (SCATE) schema (Bethard and Parker, 2016) used in in SemEval 2018 Task 6 (Laparra et al., 2018). As in negation detection, previous works have also oberved a significant performance degradation on domain shift (Xu et al., 2019). For time expression recognition, we provided a sequence tagging model, fine-tuned on deidentified clinical notes from the Mayo Clinic, which were available to the task organizers, but are difficult to gain access to due to the complex data use agreements necessary. (Models were approved to be distributed, as the data is deidentified.) The development data was the annotated news portion of the SemEval 2018 Tas"
2021.semeval-1.42,P07-1033,0,0.462609,"Missing"
2021.semeval-1.42,N19-1423,0,0.0139312,"y, generate 5 additional training examples using 5 new words with same entity type; 4) train the model on the resulting dataset. The same method was used by UArizona-2, but, in this case, they fixed some errors in the manual annotations. KISNLP-1 and KISNLP-2 used the development labeled data as a fine-tuning resource, which was complemented by a data augmentation process. They did not use the unlabeled test data, nor any other resource. YNU-HPCC-1 and YNU-HPCC-2 also used the labeled portion of the development set. They finetuned 4 popular transformer-based pre-trained models: RoBERTa, BERT (Devlin et al., 2019), DistilBERt (Sanh et al., 2020) and ALBERT (Lan et al., 2020). The final prediction was given by hard voting strategy, integrating the results of the 4 models along with Source-Trained. Observations: Self-learning (5 submissions) and data augmentation (4 submissions) were the most commonly followed approaches. 2 submissions extended a self-learning technique with manually created heuristics. Only 3 submissions proposed ensemble methods. In this task, the development set was more frequently exploited and 4 submissions made use of the labeled data to continue fine-tuning the provided model. The"
2021.semeval-1.42,C16-1038,0,0.0297124,"velop an accurate system for a target domain when annotations exist for a related domain but cannot be distributed. Instead of annotated training data, participants are given a model trained on the annotations. Then, given unlabeled target domain data, they are asked to make predictions. This is a challenging setting, and much previous work on domain adaptation does not apply, as it assumes access to source data (Ganin et al., 2016; Ziser and Reichart, 2017; Saito et al., 2017; Ruder and Plank, 2018), or assumes that labeled target domain data is available (Daum´e III, 2007; Xia et al., 2013; Kim et al., 2016; Peng and Dredze, 2017). Two different semantic tasks in English were created to explore this framework: negation detection and time expression recognition. These represent two common types of classification tasks: negation detection is typically formulated as predicting an attribute of a word or span given its context, and time expression recognition is typically formulated as a named entity tagging problem. Both of these tasks have previously been run as shared tasks, and had at least two different domains of data available, and we had access to experienced annotators for both tasks, allowi"
2021.semeval-1.42,2021.ccl-1.108,0,0.0322749,"Missing"
2021.semeval-1.42,W17-2612,0,0.0242437,"system for a target domain when annotations exist for a related domain but cannot be distributed. Instead of annotated training data, participants are given a model trained on the annotations. Then, given unlabeled target domain data, they are asked to make predictions. This is a challenging setting, and much previous work on domain adaptation does not apply, as it assumes access to source data (Ganin et al., 2016; Ziser and Reichart, 2017; Saito et al., 2017; Ruder and Plank, 2018), or assumes that labeled target domain data is available (Daum´e III, 2007; Xia et al., 2013; Kim et al., 2016; Peng and Dredze, 2017). Two different semantic tasks in English were created to explore this framework: negation detection and time expression recognition. These represent two common types of classification tasks: negation detection is typically formulated as predicting an attribute of a word or span given its context, and time expression recognition is typically formulated as a named entity tagging problem. Both of these tasks have previously been run as shared tasks, and had at least two different domains of data available, and we had access to experienced annotators for both tasks, allowing us to annotate data i"
2021.semeval-1.42,W09-2418,0,0.126969,"Missing"
2021.semeval-1.42,P18-1096,0,0.0266295,"cipants to develop semantic annotation systems in the face of data sharing constraints. A participant’s goal is to develop an accurate system for a target domain when annotations exist for a related domain but cannot be distributed. Instead of annotated training data, participants are given a model trained on the annotations. Then, given unlabeled target domain data, they are asked to make predictions. This is a challenging setting, and much previous work on domain adaptation does not apply, as it assumes access to source data (Ganin et al., 2016; Ziser and Reichart, 2017; Saito et al., 2017; Ruder and Plank, 2018), or assumes that labeled target domain data is available (Daum´e III, 2007; Xia et al., 2013; Kim et al., 2016; Peng and Dredze, 2017). Two different semantic tasks in English were created to explore this framework: negation detection and time expression recognition. These represent two common types of classification tasks: negation detection is typically formulated as predicting an attribute of a word or span given its context, and time expression recognition is typically formulated as a named entity tagging problem. Both of these tasks have previously been run as shared tasks, and had at le"
2021.semeval-1.42,S13-2001,0,0.0356015,"cuments Time entities 278 99 47 17 18,020 2,231 1,900 Table 2: Size of the time expression recognition datasets. The train set is never distributed to the participants. data, we used the i2b2 2010 Challenge Dataset, a de-identified dataset of notes from Partners HealthCare. The evaluation dataset for this task consisted of de-identified intensive care unit progress notes from the MIMIC III corpus (Johnson et al., 2016). Time expression recognition has been a key component of previous temporal language related competitions, like TempEval 2010 (Pustejovsky and Verhagen, 2009) and TempEval 2013 (UzZaman et al., 2013). For this task, we followed the Compositional Annotation of Time Expressions (SCATE) schema (Bethard and Parker, 2016) used in in SemEval 2018 Task 6 (Laparra et al., 2018). As in negation detection, previous works have also oberved a significant performance degradation on domain shift (Xu et al., 2019). For time expression recognition, we provided a sequence tagging model, fine-tuned on deidentified clinical notes from the Mayo Clinic, which were available to the task organizers, but are difficult to gain access to due to the complex data use agreements necessary. (Models were approved to be"
2021.semeval-1.42,S19-1008,1,0.844233,"k consisted of de-identified intensive care unit progress notes from the MIMIC III corpus (Johnson et al., 2016). Time expression recognition has been a key component of previous temporal language related competitions, like TempEval 2010 (Pustejovsky and Verhagen, 2009) and TempEval 2013 (UzZaman et al., 2013). For this task, we followed the Compositional Annotation of Time Expressions (SCATE) schema (Bethard and Parker, 2016) used in in SemEval 2018 Task 6 (Laparra et al., 2018). As in negation detection, previous works have also oberved a significant performance degradation on domain shift (Xu et al., 2019). For time expression recognition, we provided a sequence tagging model, fine-tuned on deidentified clinical notes from the Mayo Clinic, which were available to the task organizers, but are difficult to gain access to due to the complex data use agreements necessary. (Models were approved to be distributed, as the data is deidentified.) The development data was the annotated news portion of the SemEval 2018 Task 6 data whose source text is from the freely available TimeBank. For evaluation, we used a set of annotated documents extracted from food security warning systems. The main impact of th"
2021.semeval-1.42,K17-1040,0,0.0221133,"task presents a new framework that asks participants to develop semantic annotation systems in the face of data sharing constraints. A participant’s goal is to develop an accurate system for a target domain when annotations exist for a related domain but cannot be distributed. Instead of annotated training data, participants are given a model trained on the annotations. Then, given unlabeled target domain data, they are asked to make predictions. This is a challenging setting, and much previous work on domain adaptation does not apply, as it assumes access to source data (Ganin et al., 2016; Ziser and Reichart, 2017; Saito et al., 2017; Ruder and Plank, 2018), or assumes that labeled target domain data is available (Daum´e III, 2007; Xia et al., 2013; Kim et al., 2016; Peng and Dredze, 2017). Two different semantic tasks in English were created to explore this framework: negation detection and time expression recognition. These represent two common types of classification tasks: negation detection is typically formulated as predicting an attribute of a word or span given its context, and time expression recognition is typically formulated as a named entity tagging problem. Both of these tasks have previo"
C16-1092,D11-1059,0,0.0457986,"Missing"
C16-1092,P99-1065,0,0.685096,"Missing"
C16-1092,P02-1017,0,0.519781,"rarchical syntax through distributional statistics alone, by modeling two widely-accepted features of human language acquisition and sentence processing that have not been simultaneously modeled by any existing grammar induction algorithm: (1) a left-corner parsing strategy and (2) limited working memory capacity. To model realistic input to human language learners, we evaluate our system on a corpus of child-directed speech rather than typical newswire corpora. Results beat or closely match those of three competing systems. 1 Introduction The success of statistical grammar induction systems (Klein and Manning, 2002; Seginer, 2007; Ponvert et al., 2011; Christodoulopoulos et al., 2012) seems to suggest that sufficient statistical information is available in language to allow grammar acquisition on this basis alone, as has been argued for word segmentation (Saffran et al., 1999). But existing grammar induction systems make unrealistic assumptions about human learners, such as the availability of part-of-speech information and access to an indexaddressable parser chart, which are not independently cognitively motivated. This paper explores the possibility that a memory-limited incremental left-corner parse"
C16-1092,P04-1061,0,0.434513,"Missing"
C16-1092,P11-1108,0,0.460587,"statistics alone, by modeling two widely-accepted features of human language acquisition and sentence processing that have not been simultaneously modeled by any existing grammar induction algorithm: (1) a left-corner parsing strategy and (2) limited working memory capacity. To model realistic input to human language learners, we evaluate our system on a corpus of child-directed speech rather than typical newswire corpora. Results beat or closely match those of three competing systems. 1 Introduction The success of statistical grammar induction systems (Klein and Manning, 2002; Seginer, 2007; Ponvert et al., 2011; Christodoulopoulos et al., 2012) seems to suggest that sufficient statistical information is available in language to allow grammar acquisition on this basis alone, as has been argued for word segmentation (Saffran et al., 1999). But existing grammar induction systems make unrealistic assumptions about human learners, such as the availability of part-of-speech information and access to an indexaddressable parser chart, which are not independently cognitively motivated. This paper explores the possibility that a memory-limited incremental left-corner parser, of the sort independently motivate"
C16-1092,C92-1032,0,0.55902,"Missing"
C16-1092,J10-1001,1,0.916984,"Missing"
C16-1092,P07-1049,0,0.840112,"distributional statistics alone, by modeling two widely-accepted features of human language acquisition and sentence processing that have not been simultaneously modeled by any existing grammar induction algorithm: (1) a left-corner parsing strategy and (2) limited working memory capacity. To model realistic input to human language learners, we evaluate our system on a corpus of child-directed speech rather than typical newswire corpora. Results beat or closely match those of three competing systems. 1 Introduction The success of statistical grammar induction systems (Klein and Manning, 2002; Seginer, 2007; Ponvert et al., 2011; Christodoulopoulos et al., 2012) seems to suggest that sufficient statistical information is available in language to allow grammar acquisition on this basis alone, as has been argued for word segmentation (Saffran et al., 1999). But existing grammar induction systems make unrealistic assumptions about human learners, such as the availability of part-of-speech information and access to an indexaddressable parser chart, which are not independently cognitively motivated. This paper explores the possibility that a memory-limited incremental left-corner parser, of the sort"
D17-1255,N10-1116,0,0.107728,"e scheduled with respect to their easiness (Jiang et al., 2015), and the uniform scheduler of rote training (Rote) in which all instances are used for training at every epoch. For Lit, we experimented with different queue lengths, n = {3, 5, 7}, and set n = 5 in the experiments as this value led to the best performance of this scheduler across all datasets. Curriculum learning starts training with easy instances and gradually introduces more complex instances for training. Since easiness information is not readily available in most datasets, previous approaches have used heuristic techniques (Spitkovsky et al., 2010; Basu and Christensen, 2013) or optimization algorithms (Jiang et al., 2015, 2014) to quantify easiness of training instances. These approaches consider an instance as easy if its loss is smaller than a threshold (λ). We adopt this technique as follows: at each iteration e, we divide the entire training data into easy and hard sets using iteration-specific λe and the loss values of instances, obtained from the current partially-trained network. All easy instances in conjunction with αe ∈ [0, 1] fraction of easiest hard instances (those with smallest loss values greater than λe ) are used for"
D17-1255,E17-2068,0,0.166195,"Missing"
D18-1292,N10-1083,0,0.140493,"Missing"
D18-1292,D13-1195,0,0.0218808,"score calculation The complexity of the inside algorithm is cubic on the length of the sentence because it has to iterate over all start points i, all end points j and all split points k of a span. For a dense PCFG with a large number of states, the explicit looping is undesirable, especially when it can be formulated as matrix multiplication. The split point loop is therefore replaced with a matrix multiplication in order to take advantage of highly optimized GPU linear algebra packages like cuBLAS and cuSPARSE, whereas previous work explores how to parse efficiently on GPUs (Johnson, 2011; Canny et al., 2013; Hall et al., 2014). Inside likelihoods are propagated using a copy V0 of the inside likelihood tensor V with the first and second indices reversed: V0[ j,i,c] = V[i, j,c] (17) This reversal allows the sum over split points k ∈ {i+1, ..., j−1} to be calculated as a product of contiguous matrices, which can be efficiently implemented on a GPU: V[i, j,1..C] = GD vec(V[i,i+1.. j−1,1..C] > V0[ j,i+1.. j−1,1..C] ) (18) where vec(M) flattens a matrix M into a vector. 3.3 Posterior inference on constituents Prior work (Johnson et al., 2007a) shows that using EM-like algorithms, which seek to maximiz"
D18-1292,P07-3008,0,0.0398876,"t either CFG parameter estimation (Carroll and Charniak, 1992; Schabes and Pereira, 1992; Johnson et al., 2007b) or directly inducing a CFG as well as its probabilities (Liang et al., 2009; Tu, 2012) have not achieved 1 The public repository can be found at https:// github.com/lifengjin/dimi_emnlp18. as much success as experiments with other kinds of formalisms (Klein and Manning, 2004; Seginer, 2007; Ponvert et al., 2011). The assumption has been made that the space of grammars is so big that constraints must be applied to the learning process to reduce the burden of the learner (Gold, 1967; Cramer, 2007; Liang et al., 2009). One constraint that has been applied is recursion depth (Schuler et al., 2010; Ponvert et al., 2011; Shain et al., 2016; Noji and Johnson, 2016; Jin et al., 2018), motivated by human cognitive constraints on memory capacity (Chomsky and Miller, 1963). Recursion depth can be defined in a left-corner parsing paradigm (Rosenkrantz and Lewis, 1970; Johnson-Laird, 1983). Left-corner parsers require only minimal stack memory to process left-branching and right-branching structures, but require an extra stack element to process each center embedding in a structure. For example,"
D18-1292,P14-1020,0,0.0181031,"he complexity of the inside algorithm is cubic on the length of the sentence because it has to iterate over all start points i, all end points j and all split points k of a span. For a dense PCFG with a large number of states, the explicit looping is undesirable, especially when it can be formulated as matrix multiplication. The split point loop is therefore replaced with a matrix multiplication in order to take advantage of highly optimized GPU linear algebra packages like cuBLAS and cuSPARSE, whereas previous work explores how to parse efficiently on GPUs (Johnson, 2011; Canny et al., 2013; Hall et al., 2014). Inside likelihoods are propagated using a copy V0 of the inside likelihood tensor V with the first and second indices reversed: V0[ j,i,c] = V[i, j,c] (17) This reversal allows the sum over split points k ∈ {i+1, ..., j−1} to be calculated as a product of contiguous matrices, which can be efficiently implemented on a GPU: V[i, j,1..C] = GD vec(V[i,i+1.. j−1,1..C] > V0[ j,i+1.. j−1,1..C] ) (18) where vec(M) flattens a matrix M into a vector. 3.3 Posterior inference on constituents Prior work (Johnson et al., 2007a) shows that using EM-like algorithms, which seek to maximize the likelihood of"
D18-1292,D16-1073,0,0.15638,"ends a chart-based Bayesian PCFG induction model (Johnson et al., 2007b) to include depth bounding, which allows both bounded and unbounded PCFGs to be induced from unannotated text. Experiments reported in this paper confirm that 2721 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2721–2731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics S S0 sequences in place of word strings is popular in the dependency grammar induction literature (Klein and Manning, 2002, 2004; Berg-Kirkpatrick et al., 2010; Jiang et al., 2016; Noji and Johnson, 2016). Combinatory Categorial Grammar (CCG) induction also relies on POS tags to assign basic categories to words (Bisk and Hockenmaier, 2012, 2013), among other constraints such as CCG combinators. Other linguistic constraints such as constraints of root nodes (Noji and Johnson, 2016), attachment rules (Naseem et al., 2010) or acoustic cues (Pate, 2013) have also been used in induction. VP S C VP NP RC NP VP NP D N For parts the plant built to fail was awful Figure 1: Stack elements after the word the in a leftcorner parse of the sentence For parts the plant built to fail"
D18-1292,U11-1006,0,0.0253849,"fficient inside score calculation The complexity of the inside algorithm is cubic on the length of the sentence because it has to iterate over all start points i, all end points j and all split points k of a span. For a dense PCFG with a large number of states, the explicit looping is undesirable, especially when it can be formulated as matrix multiplication. The split point loop is therefore replaced with a matrix multiplication in order to take advantage of highly optimized GPU linear algebra packages like cuBLAS and cuSPARSE, whereas previous work explores how to parse efficiently on GPUs (Johnson, 2011; Canny et al., 2013; Hall et al., 2014). Inside likelihoods are propagated using a copy V0 of the inside likelihood tensor V with the first and second indices reversed: V0[ j,i,c] = V[i, j,c] (17) This reversal allows the sum over split points k ∈ {i+1, ..., j−1} to be calculated as a product of contiguous matrices, which can be efficiently implemented on a GPU: V[i, j,1..C] = GD vec(V[i,i+1.. j−1,1..C] > V0[ j,i+1.. j−1,1..C] ) (18) where vec(M) flattens a matrix M into a vector. 3.3 Posterior inference on constituents Prior work (Johnson et al., 2007a) shows that using EM-like algorithms, w"
D18-1292,N07-1018,0,0.109479,"bounding the recursive complexity of the induction model (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016; Jin et al., 2018). Modern depth-bounded grammar inducers have been shown to be more accurate than early unbounded PCFG inducers, but this technique has never been compared against unbounded induction within the same system, in part because most previous depthbounding models are built around sequence models, the complexity of which grows exponentially with the maximum allowed depth. The present work instead applies depth bounds within a chart-based Bayesian PCFG inducer (Johnson et al., 2007b), where bounding can be switched on and off, and then samples trees with and without bounding.1 Results show that depth-bounding is indeed significantly effective in limiting the search space of the inducer and thereby increasing the accuracy of the resulting parsing model. Moreover, parsing results on English, Chinese and German show that this bounded model with a new inference technique is able to produce parse trees more accurately than or competitively with state-ofthe-art constituency-based grammar induction models. 1 Introduction Unsupervised grammar inducers hypothesize hierarchical s"
D18-1292,Q13-1006,0,0.0152799,"4, 2018. 2018 Association for Computational Linguistics S S0 sequences in place of word strings is popular in the dependency grammar induction literature (Klein and Manning, 2002, 2004; Berg-Kirkpatrick et al., 2010; Jiang et al., 2016; Noji and Johnson, 2016). Combinatory Categorial Grammar (CCG) induction also relies on POS tags to assign basic categories to words (Bisk and Hockenmaier, 2012, 2013), among other constraints such as CCG combinators. Other linguistic constraints such as constraints of root nodes (Noji and Johnson, 2016), attachment rules (Naseem et al., 2010) or acoustic cues (Pate, 2013) have also been used in induction. VP S C VP NP RC NP VP NP D N For parts the plant built to fail was awful Figure 1: Stack elements after the word the in a leftcorner parse of the sentence For parts the plant built to fail was awful. depth-bounding does empirically have the effect of significantly limiting the search space of the inducer. Analyses of this model also show that the posterior samples are indicative of implicit depth limits in the data. This work also shows for the first time that it is possible to induce an accurate unbounded PCFG from raw text with no strong linguistic constrai"
D18-1292,P11-1108,0,0.881268,"Missing"
D18-1292,P80-1024,0,0.635866,"formalisms (Klein and Manning, 2004; Seginer, 2007; Ponvert et al., 2011). The assumption has been made that the space of grammars is so big that constraints must be applied to the learning process to reduce the burden of the learner (Gold, 1967; Cramer, 2007; Liang et al., 2009). One constraint that has been applied is recursion depth (Schuler et al., 2010; Ponvert et al., 2011; Shain et al., 2016; Noji and Johnson, 2016; Jin et al., 2018), motivated by human cognitive constraints on memory capacity (Chomsky and Miller, 1963). Recursion depth can be defined in a left-corner parsing paradigm (Rosenkrantz and Lewis, 1970; Johnson-Laird, 1983). Left-corner parsers require only minimal stack memory to process left-branching and right-branching structures, but require an extra stack element to process each center embedding in a structure. For example, a left-corner parser must add a stack element for each of the first three words in the sentence, For parts the plant built to fail was awful, shown in Figure 1. These kinds of depth bounds in sentence processing have been used to explain the relative difficulty of center-embedded sentences compared to more right-branching paraphrases like It was awful for the plant"
D18-1292,N18-1084,0,0.0431878,"Missing"
D18-1292,P92-1017,0,0.784554,"the search space of the inducer and thereby increasing the accuracy of the resulting parsing model. Moreover, parsing results on English, Chinese and German show that this bounded model with a new inference technique is able to produce parse trees more accurately than or competitively with state-ofthe-art constituency-based grammar induction models. 1 Introduction Unsupervised grammar inducers hypothesize hierarchical structures for strings of words. Using context-free grammars (CFGs) to define these structures, previous attempts at either CFG parameter estimation (Carroll and Charniak, 1992; Schabes and Pereira, 1992; Johnson et al., 2007b) or directly inducing a CFG as well as its probabilities (Liang et al., 2009; Tu, 2012) have not achieved 1 The public repository can be found at https:// github.com/lifengjin/dimi_emnlp18. as much success as experiments with other kinds of formalisms (Klein and Manning, 2004; Seginer, 2007; Ponvert et al., 2011). The assumption has been made that the space of grammars is so big that constraints must be applied to the learning process to reduce the burden of the learner (Gold, 1967; Cramer, 2007; Liang et al., 2009). One constraint that has been applied is recursion dep"
D18-1292,P02-1017,0,0.338333,"are the effects of depth-bounding more directly, this work extends a chart-based Bayesian PCFG induction model (Johnson et al., 2007b) to include depth bounding, which allows both bounded and unbounded PCFGs to be induced from unannotated text. Experiments reported in this paper confirm that 2721 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2721–2731 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics S S0 sequences in place of word strings is popular in the dependency grammar induction literature (Klein and Manning, 2002, 2004; Berg-Kirkpatrick et al., 2010; Jiang et al., 2016; Noji and Johnson, 2016). Combinatory Categorial Grammar (CCG) induction also relies on POS tags to assign basic categories to words (Bisk and Hockenmaier, 2012, 2013), among other constraints such as CCG combinators. Other linguistic constraints such as constraints of root nodes (Noji and Johnson, 2016), attachment rules (Naseem et al., 2010) or acoustic cues (Pate, 2013) have also been used in induction. VP S C VP NP RC NP VP NP D N For parts the plant built to fail was awful Figure 1: Stack elements after the word the in a leftcorner"
D18-1292,P04-1061,0,0.427954,"-art constituency-based grammar induction models. 1 Introduction Unsupervised grammar inducers hypothesize hierarchical structures for strings of words. Using context-free grammars (CFGs) to define these structures, previous attempts at either CFG parameter estimation (Carroll and Charniak, 1992; Schabes and Pereira, 1992; Johnson et al., 2007b) or directly inducing a CFG as well as its probabilities (Liang et al., 2009; Tu, 2012) have not achieved 1 The public repository can be found at https:// github.com/lifengjin/dimi_emnlp18. as much success as experiments with other kinds of formalisms (Klein and Manning, 2004; Seginer, 2007; Ponvert et al., 2011). The assumption has been made that the space of grammars is so big that constraints must be applied to the learning process to reduce the burden of the learner (Gold, 1967; Cramer, 2007; Liang et al., 2009). One constraint that has been applied is recursion depth (Schuler et al., 2010; Ponvert et al., 2011; Shain et al., 2016; Noji and Johnson, 2016; Jin et al., 2018), motivated by human cognitive constraints on memory capacity (Chomsky and Miller, 1963). Recursion depth can be defined in a left-corner parsing paradigm (Rosenkrantz and Lewis, 1970; Johnso"
D18-1292,J93-2004,0,0.0617007,"to set the hyperparameters of the model, this evaluation also uses it to explore interactions among parsing accuracy, model fit, depth limit and category domain. The first set of experiments explores various settings of D in the hope of acquiring a better picture of how depth-bounding affects the inducer. The second set of experiments uses the value of D tuned in the first experiments, and does PIoC on different sets of samples to examine the effect it has on parse quality. Optimal parameter values from these first two experiments are then applied in experiments on English (The Penn Treebank; Marcus et al., 1993), Chinese (The Chinese Treebank 5.0; Xia et al., 2000) and German (NEGRA 2.0; Skut et al., 1998) data to show how the model performs compared with competing systems. Each run in evaluation uses one sample of parse trees from the posterior samples after convergence. Preliminary experiments show that the samples after convergence are very similar within a run and their parsing accuracies differ very little. This evaluation follows Seginer (2007) by running unlabeled PARSEVAL on parse trees collected from each run. Punctuation is retained in the raw text in induction, and removed in evaluation, a"
D18-1292,D15-1160,0,0.0227263,"tly implemented on a GPU: V[i, j,1..C] = GD vec(V[i,i+1.. j−1,1..C] > V0[ j,i+1.. j−1,1..C] ) (18) where vec(M) flattens a matrix M into a vector. 3.3 Posterior inference on constituents Prior work (Johnson et al., 2007a) shows that using EM-like algorithms, which seek to maximize the likelihood of data marginalizing out the latent trees, does not yield good performance. Because trees are the main target for evaluation, it may be preferable to find the most probable tree structures given the marginal posterior of tree structures compared to finding the most probable grammar. Some recent work (McClosky and Charniak, 2015; Keith et al., 2018) explores how to use marginal distributions of tree structures from supervised parsers to create more accurate parse trees. Based on these arguments, this model performs maximum a posteriori (MAP) inference on constituents (PIoC) using approximate conditional posteriors of spans to create final parses for evaluation. Formally, let σ?i, j be an MAP unlabeled span of words in a sentence from a corpus σ, with start point i and end point j, and σi,k , σk, j its possible children. This algorithm iteratively looks for the best pair of children σ?i,k , σ?k, j according to the pos"
D18-1292,D10-1120,0,0.0140445,"ussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics S S0 sequences in place of word strings is popular in the dependency grammar induction literature (Klein and Manning, 2002, 2004; Berg-Kirkpatrick et al., 2010; Jiang et al., 2016; Noji and Johnson, 2016). Combinatory Categorial Grammar (CCG) induction also relies on POS tags to assign basic categories to words (Bisk and Hockenmaier, 2012, 2013), among other constraints such as CCG combinators. Other linguistic constraints such as constraints of root nodes (Noji and Johnson, 2016), attachment rules (Naseem et al., 2010) or acoustic cues (Pate, 2013) have also been used in induction. VP S C VP NP RC NP VP NP D N For parts the plant built to fail was awful Figure 1: Stack elements after the word the in a leftcorner parse of the sentence For parts the plant built to fail was awful. depth-bounding does empirically have the effect of significantly limiting the search space of the inducer. Analyses of this model also show that the posterior samples are indicative of implicit depth limits in the data. This work also shows for the first time that it is possible to induce an accurate unbounded PCFG from raw text with"
D18-1292,D16-1004,0,0.428047,"of Linguistics The Ohio State University jin.544@osu.edu Finale Doshi-Velez Timothy Miller Harvard University Boston Children’s Hospital & finale@seas.harvard.edu Harvard Medical School timothy.miller@childrens.harvard.edu William Schuler Department of Linguistics The Ohio State University schuler@ling.osu.edu Abstract Lane Schwartz Department of Linguistics University of Illinois at Urbana-Champaign lanes@illinois.edu There have been several recent attempts to improve the accuracy of grammar induction systems by bounding the recursive complexity of the induction model (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016; Jin et al., 2018). Modern depth-bounded grammar inducers have been shown to be more accurate than early unbounded PCFG inducers, but this technique has never been compared against unbounded induction within the same system, in part because most previous depthbounding models are built around sequence models, the complexity of which grows exponentially with the maximum allowed depth. The present work instead applies depth bounds within a chart-based Bayesian PCFG inducer (Johnson et al., 2007b), where bounding can be switched on and off, and then samples trees with and with"
D18-1292,J10-1001,1,0.785735,"hnson et al., 2007b) or directly inducing a CFG as well as its probabilities (Liang et al., 2009; Tu, 2012) have not achieved 1 The public repository can be found at https:// github.com/lifengjin/dimi_emnlp18. as much success as experiments with other kinds of formalisms (Klein and Manning, 2004; Seginer, 2007; Ponvert et al., 2011). The assumption has been made that the space of grammars is so big that constraints must be applied to the learning process to reduce the burden of the learner (Gold, 1967; Cramer, 2007; Liang et al., 2009). One constraint that has been applied is recursion depth (Schuler et al., 2010; Ponvert et al., 2011; Shain et al., 2016; Noji and Johnson, 2016; Jin et al., 2018), motivated by human cognitive constraints on memory capacity (Chomsky and Miller, 1963). Recursion depth can be defined in a left-corner parsing paradigm (Rosenkrantz and Lewis, 1970; Johnson-Laird, 1983). Left-corner parsers require only minimal stack memory to process left-branching and right-branching structures, but require an extra stack element to process each center embedding in a structure. For example, a left-corner parser must add a stack element for each of the first three words in the sentence, Fo"
D18-1292,P07-1049,0,0.785405,"rammar induction models. 1 Introduction Unsupervised grammar inducers hypothesize hierarchical structures for strings of words. Using context-free grammars (CFGs) to define these structures, previous attempts at either CFG parameter estimation (Carroll and Charniak, 1992; Schabes and Pereira, 1992; Johnson et al., 2007b) or directly inducing a CFG as well as its probabilities (Liang et al., 2009; Tu, 2012) have not achieved 1 The public repository can be found at https:// github.com/lifengjin/dimi_emnlp18. as much success as experiments with other kinds of formalisms (Klein and Manning, 2004; Seginer, 2007; Ponvert et al., 2011). The assumption has been made that the space of grammars is so big that constraints must be applied to the learning process to reduce the burden of the learner (Gold, 1967; Cramer, 2007; Liang et al., 2009). One constraint that has been applied is recursion depth (Schuler et al., 2010; Ponvert et al., 2011; Shain et al., 2016; Noji and Johnson, 2016; Jin et al., 2018), motivated by human cognitive constraints on memory capacity (Chomsky and Miller, 1963). Recursion depth can be defined in a left-corner parsing paradigm (Rosenkrantz and Lewis, 1970; Johnson-Laird, 1983)."
D18-1292,C16-1092,1,0.730574,"State University jin.544@osu.edu Finale Doshi-Velez Timothy Miller Harvard University Boston Children’s Hospital & finale@seas.harvard.edu Harvard Medical School timothy.miller@childrens.harvard.edu William Schuler Department of Linguistics The Ohio State University schuler@ling.osu.edu Abstract Lane Schwartz Department of Linguistics University of Illinois at Urbana-Champaign lanes@illinois.edu There have been several recent attempts to improve the accuracy of grammar induction systems by bounding the recursive complexity of the induction model (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016; Jin et al., 2018). Modern depth-bounded grammar inducers have been shown to be more accurate than early unbounded PCFG inducers, but this technique has never been compared against unbounded induction within the same system, in part because most previous depthbounding models are built around sequence models, the complexity of which grows exponentially with the maximum allowed depth. The present work instead applies depth bounds within a chart-based Bayesian PCFG inducer (Johnson et al., 2007b), where bounding can be switched on and off, and then samples trees with and without bounding.1 Resul"
D18-1292,xia-etal-2000-developing,0,0.0352094,"also uses it to explore interactions among parsing accuracy, model fit, depth limit and category domain. The first set of experiments explores various settings of D in the hope of acquiring a better picture of how depth-bounding affects the inducer. The second set of experiments uses the value of D tuned in the first experiments, and does PIoC on different sets of samples to examine the effect it has on parse quality. Optimal parameter values from these first two experiments are then applied in experiments on English (The Penn Treebank; Marcus et al., 1993), Chinese (The Chinese Treebank 5.0; Xia et al., 2000) and German (NEGRA 2.0; Skut et al., 1998) data to show how the model performs compared with competing systems. Each run in evaluation uses one sample of parse trees from the posterior samples after convergence. Preliminary experiments show that the samples after convergence are very similar within a run and their parsing accuracies differ very little. This evaluation follows Seginer (2007) by running unlabeled PARSEVAL on parse trees collected from each run. Punctuation is retained in the raw text in induction, and removed in evaluation, also following Seginer (2007). Analysis of model behavi"
D19-6201,cybulska-vossen-2014-using,0,0.0634294,"Missing"
D19-6201,W16-1701,1,0.897502,"Missing"
D19-6201,W13-1203,0,0.394035,"Missing"
D19-6201,W16-5706,1,0.928784,"Missing"
D19-6201,recasens-etal-2012-annotating,0,0.397119,"Missing"
D19-6201,Q14-1012,1,0.893257,"Missing"
E17-2118,S15-2136,1,0.513914,"ontains relation which is the most frequent temporal relation type in clinical data (Styler IV et al., 2014). Consider the sentence: Patient was diagnosed with a rectal cancer in May of 2010. It can be said that the temporal expression May of 2010 in this sentence contains the cancer event. The same relation can exist between two events: During the surgery the patient experienced severe tachycardia. Here, the surgery event contains the tachycardia event. The vast majority of systems in temporal information extraction challenges, such as the i2b2 (Sun et al., 2013) and Clinical TempEval tasks (Bethard et al., 2015; Bethard et al., 2016), used classifiers with a large number of manually engineered features. This is not ideal, as most NLP components used for feature extraction experience a significant accuracy drop when applied to out-of-domain data (Wu et al., 2014; McClosky et al., 2010; Daum´e III, 2009; Blitzer et al., 2006), propagating the error to the downstream components and ultimately leading to significant performance degradation. In this work, we propose a novel temporal relation extraction framework that requires minimal linguistic pre-processing and can operate on raw tokens. We experiment"
E17-2118,W06-1615,0,0.0383876,"t between two events: During the surgery the patient experienced severe tachycardia. Here, the surgery event contains the tachycardia event. The vast majority of systems in temporal information extraction challenges, such as the i2b2 (Sun et al., 2013) and Clinical TempEval tasks (Bethard et al., 2015; Bethard et al., 2016), used classifiers with a large number of manually engineered features. This is not ideal, as most NLP components used for feature extraction experience a significant accuracy drop when applied to out-of-domain data (Wu et al., 2014; McClosky et al., 2010; Daum´e III, 2009; Blitzer et al., 2006), propagating the error to the downstream components and ultimately leading to significant performance degradation. In this work, we propose a novel temporal relation extraction framework that requires minimal linguistic pre-processing and can operate on raw tokens. We experiment with two neural architectures for temporal relation extraction: a convolutional neural network (CNN) (LeCun et al., 1998) and a long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997). Little work exists on using these methods for relation extraction; to the best of our knowledge no work exists"
E17-2118,D14-1181,0,0.00672077,"Missing"
E17-2118,W16-2914,1,0.784286,", 2015; Bethard et al., 2016). The gold standard annotations include time expressions, events (both medical and general), and temporal relations. We used the standard split established by Clinical TempEval 2016, using the development set for evaluating models and tuning model parameters, and evaluating our best event-event and event-time models on the test set. Following Clinical TempEval, we focus only on the contains relation, which was the most common relation and had the highest inter-annotator agreement. 3.2 Experiments We compare the performance of our neural models to the THYME system (Lin et al., 2016a), 747 Model Argument representation THYME full system THYME tokens only CNN tokens CNN tokens CNN pos tags LSTM tokens LSTM pos tags CNN token + pos tags LSTM token + pos tags n/a n/a position embeddings XML tags XML tags XML tags XML tags XML tags XML tags Event-time relations P R F1 0.583 0.810 0.678 0.564 0.786 0.657 0.647 0.627 0.637 0.660 0.775 0.713 0.707 0.708 0.707 0.691 0.626 0.657 0.754 0.657 0.702 0.727 0.681 0.703 0.698 0.660 0.679 Event-event relations P R F1 0.569 0.574 0.572 0.562 0.539 0.550 0.580 0.324 0.416 0.566 0.522 0.543 0.630 0.204 0.309 0.610 0.418 0.496 0.603 0.212 0"
E17-2118,W15-1506,0,0.369979,"his work, we propose a novel temporal relation extraction framework that requires minimal linguistic pre-processing and can operate on raw tokens. We experiment with two neural architectures for temporal relation extraction: a convolutional neural network (CNN) (LeCun et al., 1998) and a long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997). Little work exists on using these methods for relation extraction; to the best of our knowledge no work exists on using LSTM models for relation extraction or CNN models for temporal information extraction. Zeng et al. (2014) and Nguyen and Grishman (2015) employ CNNs for non-temporal relation extraction and show that CNNs can be effective for relation classification and perform as well as token-based baselines for relation extraction. Our experiments, on the other hand, show that neural relation extraction models can compete with a complex featurebased state-of-the-art relation extraction system. Another important difference that sets our work apart is our representation of the argument positions: previous work used token position features (embedded in a 50-dimensional space) to encode the relative distance of the words in the sentence to the"
E17-2118,D14-1162,0,0.0986284,"Missing"
E17-2118,C14-1220,0,0.255061,"mance degradation. In this work, we propose a novel temporal relation extraction framework that requires minimal linguistic pre-processing and can operate on raw tokens. We experiment with two neural architectures for temporal relation extraction: a convolutional neural network (CNN) (LeCun et al., 1998) and a long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997). Little work exists on using these methods for relation extraction; to the best of our knowledge no work exists on using LSTM models for relation extraction or CNN models for temporal information extraction. Zeng et al. (2014) and Nguyen and Grishman (2015) employ CNNs for non-temporal relation extraction and show that CNNs can be effective for relation classification and perform as well as token-based baselines for relation extraction. Our experiments, on the other hand, show that neural relation extraction models can compete with a complex featurebased state-of-the-art relation extraction system. Another important difference that sets our work apart is our representation of the argument positions: previous work used token position features (embedded in a 50-dimensional space) to encode the relative distance of th"
E17-2118,Q14-1012,1,\N,Missing
E17-2118,S16-1165,1,\N,Missing
E17-2118,N10-1004,0,\N,Missing
N18-1182,D17-1255,1,0.791955,"15), studied the hypothesis of the exponential nature of forgetting in humans. Three major indicators were identified that affect memory retention in humans: delay since last review of learning materials and strength of human memory (Ebbinghaus, 1913; Dempster, 1989; Wixted, 1990; Cepeda et al., 2006; Novikoff et al., 2012), and, more recently, difficulty of learning materials (Reddy et al., 2016). The above findings show that human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materials (spaced repetition). In (Amiri et al., 2017), we built on these findings to develop efficient and effective training paradigms for neural networks. Previous research also investigated the development of cognitivelymotivated training paradigms named curriculum learning for artificial neural networks (Bengio et al., 2009; Kumar et al., 2010). The difference between the above models is in their views to learning: curriculum learning is inspired by the learning principle that training starts with easier concepts and gradually proceeds with more difficult ones (Bengio et al., 2009). On the other hand, spaced repetition models are inspired by"
N18-1182,E03-1068,0,0.403206,"made based on the data. Although the quality of language resources can be improved through good annotation guidelines, test questions, etc., annotation noise still exists (Gupta et al., 2012; Lasecki et al., 2013). For example, Figure 1 shows sample spurious instances (those with potentially wrong labels) in CIFAR-10 (Krizhevsky, 2009) which is a benchmark dataset for object classification. Spurious instances can mislead systems, and, if available in test data, lead to unrealistic comparison among competing systems. Previous works either directly identify noise in datasets (Hovy et al., 2013; Dickinson and Meurers, 2003; Eskin, 2000; Loftsson, 2009), or develop models that are more robust against noise (Guan et al., 2017; Natarajan et al., 2013; Zhu et al., 2003; Zhu and Wu, 2004). Furthermore, recent works on adversarial perturbation have tackled this problem (Goodfellow et al., 2015; Feinman et al., 2017). However, most previous approaches require either annotations generated by each individual annotator (Guan et al., 2017), or both task-specific and instance-type (genuine or adversarial) labels for training (Hendrik Metzen et al., 2017; Zheng et al., 2016), or noise-free data (Xiao et al., 2015). Such inf"
N18-1182,A00-2020,0,0.764786,"ugh the quality of language resources can be improved through good annotation guidelines, test questions, etc., annotation noise still exists (Gupta et al., 2012; Lasecki et al., 2013). For example, Figure 1 shows sample spurious instances (those with potentially wrong labels) in CIFAR-10 (Krizhevsky, 2009) which is a benchmark dataset for object classification. Spurious instances can mislead systems, and, if available in test data, lead to unrealistic comparison among competing systems. Previous works either directly identify noise in datasets (Hovy et al., 2013; Dickinson and Meurers, 2003; Eskin, 2000; Loftsson, 2009), or develop models that are more robust against noise (Guan et al., 2017; Natarajan et al., 2013; Zhu et al., 2003; Zhu and Wu, 2004). Furthermore, recent works on adversarial perturbation have tackled this problem (Goodfellow et al., 2015; Feinman et al., 2017). However, most previous approaches require either annotations generated by each individual annotator (Guan et al., 2017), or both task-specific and instance-type (genuine or adversarial) labels for training (Hendrik Metzen et al., 2017; Zheng et al., 2016), or noise-free data (Xiao et al., 2015). Such information is o"
N18-1182,N13-1132,0,0.627572,"ata, and decisions made based on the data. Although the quality of language resources can be improved through good annotation guidelines, test questions, etc., annotation noise still exists (Gupta et al., 2012; Lasecki et al., 2013). For example, Figure 1 shows sample spurious instances (those with potentially wrong labels) in CIFAR-10 (Krizhevsky, 2009) which is a benchmark dataset for object classification. Spurious instances can mislead systems, and, if available in test data, lead to unrealistic comparison among competing systems. Previous works either directly identify noise in datasets (Hovy et al., 2013; Dickinson and Meurers, 2003; Eskin, 2000; Loftsson, 2009), or develop models that are more robust against noise (Guan et al., 2017; Natarajan et al., 2013; Zhu et al., 2003; Zhu and Wu, 2004). Furthermore, recent works on adversarial perturbation have tackled this problem (Goodfellow et al., 2015; Feinman et al., 2017). However, most previous approaches require either annotations generated by each individual annotator (Guan et al., 2017), or both task-specific and instance-type (genuine or adversarial) labels for training (Hendrik Metzen et al., 2017; Zheng et al., 2016), or noise-free data"
N18-1182,E17-2068,0,0.0511091,"Missing"
N18-1182,E09-1060,0,0.0327692,"ty of language resources can be improved through good annotation guidelines, test questions, etc., annotation noise still exists (Gupta et al., 2012; Lasecki et al., 2013). For example, Figure 1 shows sample spurious instances (those with potentially wrong labels) in CIFAR-10 (Krizhevsky, 2009) which is a benchmark dataset for object classification. Spurious instances can mislead systems, and, if available in test data, lead to unrealistic comparison among competing systems. Previous works either directly identify noise in datasets (Hovy et al., 2013; Dickinson and Meurers, 2003; Eskin, 2000; Loftsson, 2009), or develop models that are more robust against noise (Guan et al., 2017; Natarajan et al., 2013; Zhu et al., 2003; Zhu and Wu, 2004). Furthermore, recent works on adversarial perturbation have tackled this problem (Goodfellow et al., 2015; Feinman et al., 2017). However, most previous approaches require either annotations generated by each individual annotator (Guan et al., 2017), or both task-specific and instance-type (genuine or adversarial) labels for training (Hendrik Metzen et al., 2017; Zheng et al., 2016), or noise-free data (Xiao et al., 2015). Such information is often not availabl"
N18-1182,P17-1107,0,0.425125,"se-free data (Xiao et al., 2015). Such information is often not available in the final release of most datasets. Current approaches utilize prediction probability/loss of instances to tackle the above challenges in identifying spurious instances. This is because prediction probability/loss of spurious instances tend to be lower than that of genuine instances (He and Garcia, 2009). In particular, the Bayesian Uncertainty model (Feinman et al., 2017) defines spurious instances as those that have greater uncertainty (variance) in their stochastic predictions, and the Variational Inference model (Rehbein and Ruppenhofer, 2017; Hovy et al., 2013) expects greater posterior entropy in predictions made for spurious instances. In this paper, our hypothesis is that spurious instances are frequently found to be difficult to 2006 Proceedings of NAACL-HLT 2018, pages 2006–2016 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics learn during training process. This difficulty in learning stems from the intrinsic discrepancy between spurious and the cohort of genuine instances which frequently makes a learner less confident in predicting the wrong labels of spurious instances. Based on t"
N18-1182,N10-1116,0,0.0201186,"instances based on the resulting scores S updated by Eq. (2). uine instances are mixed at training time and the network is only provided with task-specific but not genuine/spurious labels for the instances. 2.1 Curriculum Learning Bengio et al. (2009) and Kumar et al. (2010) developed training paradigms which are inspired by the learning principle that humans can learn more effectively when training starts with easier concepts and gradually proceeds with more difficult ones. Since easiness of information is not readily available in most datasets, previous approaches used heuristic techniques (Spitkovsky et al., 2010; Basu and Christensen, 2013) or optimization algorithms (Jiang et al., 2015, 2014) to quantify easiness for instances. These approaches consider an instance as easy if its prediction loss is smaller than a threshold (λ). Given a neural network as the learner, we adopt curriculum learning to identify spurious instances as follows (see Figure 2): At each iteration i, we divide all instances into easy and hard batches using the iteration-specific threshold λi and the loss values of instances at iteration i, obtained from the current partially-trained network. All instances with a loss smaller th"
N19-1039,P07-1056,0,0.553982,"e, which the SCL 2 Background This work builds off of existing work in unsupervised domain adaptation, starting with Blitzer’s work on structural correspondence learning (SCL) (Blitzer et al., 2006, 2007). In the UDA 414 Proceedings of NAACL-HLT 2019, pages 414–419 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics features, and the authors use a logistic regression classifier for the final sentiment classifier. One standard corpus used to develop new domain adaptation algorithms is the Amazon sentiment analysis dataset.1 This corpus was created by Blitzer et al. (2007), but we use the version included in the software release from Ziser and Reichart (2017)2 , along with their pre-processing steps, for ease of comparison with their results. This dataset contains reviews from four product categories on Amazon.com – books, DVDs, electronics, and kitchen appliances. Reviews are mapped to binary categories: positive if the review assigns the product &gt;3 stars (out of 5) and negative if it assigns the product &lt; 3 stars. This dataset also contains additional unlabeled instances for each category, used for training the pivot predictor. task setup, one is given two da"
N19-1039,W06-1615,0,0.595311,"d domain adaptation (UDA) is the task of modifying a statistical model trained on labeled data from a source domain to achieve better performance on data from a target domain, without access to any labeled data in the target domain. Supervised domain adaptation methods can obtain excellent performance from a small number of labeled examples in the target domain (Daum´e III, 2007), but UDA is attractive in cases where annotation requires specialized expertise or the number of meaningfully different sub-domains is large (e.g., both are true for clinical NLP). Structural correspondence learning (Blitzer et al., 2006) (SCL) is one widely-used method for UDA in natural language processing. The key idea in SCL is that a subset of features, believed to be predictive across domains, are selected as pivot features. For each selected pivot feature, SCL creates an auxiliary classification task of predicting the value of that feature in an instance, given the values of all the non-pivot features for that instance. The auxiliary classifiers therefore learn important cross-domain information about the structure of the feature space, which the SCL 2 Background This work builds off of existing work in unsupervised dom"
N19-1039,P07-1033,0,0.404107,"Missing"
N19-1039,P18-1031,0,0.0460379,"Missing"
N19-1039,C18-1070,0,0.0147623,"otivating factors. First, we would like to improve the performance of SCL using joint training. Existing SCL-based methods are successful in treating pivot prediction as a pre-training phase, but joint training may improve UDA by allowing the network to find representations that are equally good at pivot reconstruction but better for downstream task performance. Second, we would like to evaluate the quality of pivot selection methods and explore whether this step might be eliminated to simplify SCL. We focus on feature-based UDA methods, as opposed to approaches that rely on embeddings (e.g., Barnes et al., 2018; Ziser and Reichart, 2018), since our primary interest is in improving existing models developed with a feature engineering approach. Such methods allow us to quickly adapt a number of different models to new datasets (e.g., for already-existing NLP pipeline software), rather than engineering new neural models from scratch for each of the pipeline tasks. For that reason, we compare to the AE-SCL model of Ziser and Reichart, rather than their subsequent models that take embeddings as input. In any case, we show that with some tuning the AE-SCL model can obtain state-of-the-art performance for"
N19-1039,P16-2038,0,0.0812156,"Missing"
N19-1039,D16-1023,0,0.0367681,"t features, but only use source labels while training the network. Both the AE-SCLR model and our JointM I model were run for 10 iterations to minimize differences due to random initialization and to calculate significance statistics. Table 2 shows the results of our experiments. First, we note that our replication of AE-SCL im5 Discussion and Conclusion Our results show that by jointly learning representations and task networks, UDA can be greatly improved over existing neural UDA methods. We note that there are existing domain adaptation methods that use joint training with auxiliary tasks. Yu and Jiang (2016) use an auxiliary task of predicting whether a masked pivot word in a sentence is positive or negative sentiment, where they 417 introduce a new technique to select pivots that still is based on correlations with source labels. Our work is unique in showing that the standard task of mutual-information-selected pivot prediction is a high quality auxiliary task, though future work should explore whether their pivot selection algorithm is superior to MI in our joint model. We also showed that existing neural UDA methods can be improved significantly with minor changes to the training regimen. Fin"
N19-1039,K17-1040,0,0.772487,"domain adaptation, starting with Blitzer’s work on structural correspondence learning (SCL) (Blitzer et al., 2006, 2007). In the UDA 414 Proceedings of NAACL-HLT 2019, pages 414–419 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics features, and the authors use a logistic regression classifier for the final sentiment classifier. One standard corpus used to develop new domain adaptation algorithms is the Amazon sentiment analysis dataset.1 This corpus was created by Blitzer et al. (2007), but we use the version included in the software release from Ziser and Reichart (2017)2 , along with their pre-processing steps, for ease of comparison with their results. This dataset contains reviews from four product categories on Amazon.com – books, DVDs, electronics, and kitchen appliances. Reviews are mapped to binary categories: positive if the review assigns the product &gt;3 stars (out of 5) and negative if it assigns the product &lt; 3 stars. This dataset also contains additional unlabeled instances for each category, used for training the pivot predictor. task setup, one is given two datasets, the source DS = {Xs , ys }, with labels for each instance, and the target DT = {"
N19-1039,N18-1112,0,0.3708,"rst, we would like to improve the performance of SCL using joint training. Existing SCL-based methods are successful in treating pivot prediction as a pre-training phase, but joint training may improve UDA by allowing the network to find representations that are equally good at pivot reconstruction but better for downstream task performance. Second, we would like to evaluate the quality of pivot selection methods and explore whether this step might be eliminated to simplify SCL. We focus on feature-based UDA methods, as opposed to approaches that rely on embeddings (e.g., Barnes et al., 2018; Ziser and Reichart, 2018), since our primary interest is in improving existing models developed with a feature engineering approach. Such methods allow us to quickly adapt a number of different models to new datasets (e.g., for already-existing NLP pipeline software), rather than engineering new neural models from scratch for each of the pipeline tasks. For that reason, we compare to the AE-SCL model of Ziser and Reichart, rather than their subsequent models that take embeddings as input. In any case, we show that with some tuning the AE-SCL model can obtain state-of-the-art performance for many pairs. Recently, neura"
P14-2014,S07-1025,1,0.791506,"cat] recording the index of a neighbor node. VP 2.2 VP RB l=1: [NP-DT],[NP-NN], Figure 2: A parse tree (left) and its descending paths according to Definition 1 (l - length). S c) DT NP DT ADVP l=0: [NP],[DT],[NN] NP PRP VBZ ADVP she comes RB Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and ba"
P14-2014,S13-2002,1,0.913047,"by taking advantage of relatively strict English word ordering. Like SST and PTK, the DPK requires the root category of two subtrees to be the same for the similarity to be greater than zero. Unlike SST and PTK, once the root category comparison is successfully completed, DPK looks at all paths that go through it and accumulates their similarity scores independent of ordering – in other words, it will ignore the ordering of the children in its pro4 Evaluation We applied DPK to two published temporal relation extraction systems: (Miller et al., 2013) in the clinical domain and Cleartk-TimeML (Bethard, 2013) in the general domain respectively. 4.1 Narrative Container Discovery The task here as described by Miller et al. (2013) is to identify the C ONTAINS relation between a time expression and a same-sentence event from clinical notes in the THYME corpus, which has 78 notes of 26 patients. We obtained this corpus from the authors and followed their linear composite kernel setting: KC (s1 , s2 ) = τ P X p=1 KT (tp1 , tp2 )+KF (f1 , f2 ) (2) where si is an instance object composed of flat features fi and a syntactic tree ti . A syntactic tree ti 84 can have multiple representations, as in Bag Tree"
P14-2014,strassel-etal-2010-darpa,0,0.0132148,"sentations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between the event and time in addition to a set of baseline features, and improved the temporal linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). Miller at al. (2013) used PET tree, bag tree, and path tree (PT, which is similar to a PET tree with the internal nodes removed) to represent syntactic information and improved the temporal relation discovery performance on THYME data2 (Styler et al., 2014). In this paper, we also use syntactic structure-enriched temporal relation discovery as a vehicle to test our proposed kernel. here Figure 1: Three example tree pairs. matching (Moschitti, 2006). In the PTK, a subtree may or may not expand any child in a production rule, while maintaining the ordering of the child nodes. Thus it generates"
P14-2014,S13-2012,0,0.0248395,"2.2 VP RB l=1: [NP-DT],[NP-NN], Figure 2: A parse tree (left) and its descending paths according to Definition 1 (l - length). S c) DT NP DT ADVP l=0: [NP],[DT],[NN] NP PRP VBZ ADVP she comes RB Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between"
P14-2014,P07-1026,0,0.0305882,"discovery as a vehicle to test our proposed kernel. here Figure 1: Three example tree pairs. matching (Moschitti, 2006). In the PTK, a subtree may or may not expand any child in a production rule, while maintaining the ordering of the child nodes. Thus it generates a very large but sparse feature space. To Figure 1(b), the PTK generates fragments (i) [NP [DT a] [JJ fat]]; (ii) [NP [DT a] [NN cat]]; and (iii) [NP [JJ fat] [NN cat]], among others, for the second tree. This allows for partial matching – substructure (ii) – while also generating some fragments that violate grammatical intuitions. Zhang et al. (2007) address the restrictiveness of SST by allowing soft matching of production rules. They allow partial matching of optional nodes based on the Treebank. For example, the rule N P → DT JJ N N indicates a noun phrase consisting of a determiner, adjective, and common noun. Zhang et al.’s method designates the JJ as optional, since the Treebank contains instances of a reduced version of the rule without the JJ node (N P → DT N N ). They also allow node matching among similar preterminals such as JJ, JJR, and JJS, mapping them to one equivalence class. Other relevant approaches are the spectrum tree"
P14-2014,E12-1019,0,0.0156425,"m events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between the event and time in addition to a set of baseline features, and improved the temporal linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). Miller at al. (2013) used PET tree, bag tree, and path tree (PT, which is similar to a PET tree with the internal nodes removed) to represent syntactic information and improved the temporal relation discovery performance on THYME data2 (Styler et al., 2014). In this paper, we also use syntactic structure-enric"
P14-2014,S10-1063,0,0.0247263,"f a neighbor node. VP 2.2 VP RB l=1: [NP-DT],[NP-NN], Figure 2: A parse tree (left) and its descending paths according to Definition 1 (l - length). S c) DT NP DT ADVP l=0: [NP],[DT],[NN] NP PRP VBZ ADVP she comes RB Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech ta"
P14-2014,W13-1903,1,0.87219,"thinking of it as cheaply calculating rule production similarity by taking advantage of relatively strict English word ordering. Like SST and PTK, the DPK requires the root category of two subtrees to be the same for the similarity to be greater than zero. Unlike SST and PTK, once the root category comparison is successfully completed, DPK looks at all paths that go through it and accumulates their similarity scores independent of ordering – in other words, it will ignore the ordering of the children in its pro4 Evaluation We applied DPK to two published temporal relation extraction systems: (Miller et al., 2013) in the clinical domain and Cleartk-TimeML (Bethard, 2013) in the general domain respectively. 4.1 Narrative Container Discovery The task here as described by Miller et al. (2013) is to identify the C ONTAINS relation between a time expression and a same-sentence event from clinical notes in the THYME corpus, which has 78 notes of 26 patients. We obtained this corpus from the authors and followed their linear composite kernel setting: KC (s1 , s2 ) = τ P X p=1 KT (tp1 , tp2 )+KF (f1 , f2 ) (2) where si is an instance object composed of flat features fi and a syntactic tree ti . A syntactic tre"
P14-2014,Y09-1038,0,0.0260023,"comes RB Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between the event and time in addition to a set of baseline features, and improved the temporal linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). Miller at al."
P19-1234,P15-1135,0,0.0254808,"shows that inducing PCFGs from raw text is possible, and cognitive constraints are useful for helping the induction model to find good grammars. Closely related to PCFG induction is the task of unsupervised constituency parsing from raw text where trees are unlabeled. Earlier work by Seginer (2007) and Ponvert et al. (2011) induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures for unsupervised parsing and language modeling and also shows good results on English. Although unlabeled parsing evaluation is common, other work (Bisk and Hockenmaier, 2015) has argued for labeled parsing evaluation for grammar induction. Early unsupervised dependency grammars and part-of-speech induction models (Klein and Manning, 2004; Christodoulopoulos and Steedman, 2010) have been similarly augmented with neural networks and word embeddings (Tran et al., 2016; Jiang et al., 2016). Neural networks provide flexible ways to parameterize distributions, and word embeddings (Mikolov et al., 2013; Pennington et al., 2014) allow these models to use semantic information in these distributed representations. Results show that these improvements produce more accurate d"
P19-1234,W06-2912,0,0.348131,"Missing"
P19-1234,K18-2005,0,0.0188753,"rger corpora. Because the inside algorithm is quadratic on the length of the sentences, the batch size for training gets quadratically smaller from 400 to 1 as sentences get longer. We use the Adam optimizer (Kingma and Ba, 2015), initialized with learning rates 0.1 for d and N, and 0.001 for L and parameters in g−1 . Means and standard deviations of evaluation metrics are reported in tables with 10 runs of the proposed system. We use ELMo embeddings (Peters et al., 2018) with 1024 dimensions from averaging representations from two BiLSTM layers and the word encoder in ELMo for all languages (Che et al., 2018).4 These embeddings are each trained with 20 million words from Wikipedia and Common Crawl. We initialize d and N with multinomials drawn from a Dirichlet distribution with 0.2 as the concentration parameter, following PCFG induction work with Bayesian models (Jin et al., 2018b). We assign the same diagonal variance matrix to all latent Gaussian distributions, calculated empirically from embeddings from 5000 randomly sampled sentences. M is initialized with the empirical mean of the same sampled embeddings, but with random Gaussian noise added to each row. The parameters of the normalizing flo"
P19-1234,D10-1056,0,0.030481,"unsupervised constituency parsing from raw text where trees are unlabeled. Earlier work by Seginer (2007) and Ponvert et al. (2011) induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures for unsupervised parsing and language modeling and also shows good results on English. Although unlabeled parsing evaluation is common, other work (Bisk and Hockenmaier, 2015) has argued for labeled parsing evaluation for grammar induction. Early unsupervised dependency grammars and part-of-speech induction models (Klein and Manning, 2004; Christodoulopoulos and Steedman, 2010) have been similarly augmented with neural networks and word embeddings (Tran et al., 2016; Jiang et al., 2016). Neural networks provide flexible ways to parameterize distributions, and word embeddings (Mikolov et al., 2013; Pennington et al., 2014) allow these models to use semantic information in these distributed representations. Results show that these improvements produce more accurate dependencies and POS assignments, but these improvements have not been applied to PCFG induction. Normalizing flows have been shown to be powerful models for complex densities (Dinh et al., 2015, 2017; Reze"
P19-1234,P99-1065,0,0.474841,"Missing"
P19-1234,W18-5452,0,0.256397,"ping constituents that have a single incoming and no outgoing dependency arc. For example, constituents like noun phrases that are kept in conversion may only have one incoming arc from the main verb, and no outgoing arc to any modifier. Each dataset has 15,000 sentences randomly sampled from the dependency treebank (if the treebank has enough sentences), or is augmented with sentences randomly sampled from Wikipedia (if the treebank has fewer sentences). Finally, unlabeled parsing experiments on the three constituency treebanks are reported, one following Jin et al. (2018a) and one following Htut et al. (2018). The hyperparameters of the model for all experiments are tuned on the Brown Corpus portion of the Penn Treebank. We set the number of categories C to 30, the categorical distance constraint strength λ1 to be 0.0001, and the drifting penalty 3 WSJ20test is the second half of WSJ20. λ2 to be 10. Function g−1 is set to have 4 coupling layers with q(i) being a feed-forward network with one hidden layer for both NICE and Real NVP, following He et al. (2018). We train the system until the marginal likelihood over the whole training set starts to oscillate, around 10,000 batches for smaller corpora"
P19-1234,D16-1073,0,0.133786,") induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures for unsupervised parsing and language modeling and also shows good results on English. Although unlabeled parsing evaluation is common, other work (Bisk and Hockenmaier, 2015) has argued for labeled parsing evaluation for grammar induction. Early unsupervised dependency grammars and part-of-speech induction models (Klein and Manning, 2004; Christodoulopoulos and Steedman, 2010) have been similarly augmented with neural networks and word embeddings (Tran et al., 2016; Jiang et al., 2016). Neural networks provide flexible ways to parameterize distributions, and word embeddings (Mikolov et al., 2013; Pennington et al., 2014) allow these models to use semantic information in these distributed representations. Results show that these improvements produce more accurate dependencies and POS assignments, but these improvements have not been applied to PCFG induction. Normalizing flows have been shown to be powerful models for complex densities (Dinh et al., 2015, 2017; Rezende and Mohamed, 2015; Papa2449 makarios et al., 2017). He et al. (2018) showed improved performance on POS ind"
P19-1234,D18-1292,1,0.337993,"oys context embeddings (Peters et al., 2018) in a normalizing flow model (Dinh et al., 2015) to extend PCFG induction to use semantic and morphological information1 . Linguistically motivated similarity penalty and categorical distance constraints are imposed on the inducer as regularization. Experiments show that the PCFG induction model with normalizing flow produces grammars with state-of-the-art accuracy on a variety of different languages. Ablation further shows a positive effect of normalizing flow, context embeddings and proposed regularizers. 1 Introduction Unsupervised PCFG inducers (Jin et al., 2018b) automatically bracket sentences into nested spans, and label these spans with consistent, linguistically relevant syntactic categories, which may be useful in downstream applications or linguistic research on under-resourced languages. Their success also provides evidence for learnability of grammar in absence of strong linguistic universals (MacWhinney and Bates, 1993; Plunkett and Wood, 2004; Bannard et al., 2009). However, current PCFG induction models, using word tokens 1 The code can be found at https://github.com/ lifengjin/acl_flow as input, are unable to incorporate semantics and mo"
P19-1234,N07-1018,0,0.307333,"y-motivated regularization terms help the flow-based model perform even better. Most notably, the similarity performance helps the flow models greatly by restricting the freedom that the flow models have to change the context embeddings, indicating that the information in context embeddings is valuable for induction. The Real NVP model produces higher data likelihood but its performance is lower than other NICE-based models, indicating that the volume-preserving property of NICE is important for preventing overfitting. 6 Related work Earlier work on PCFG induction (Carroll and Charniak, 1992; Johnson et al., 2007; Liang et al., 2009; Tu, 2012) shows that directly inducing PCFGs from raw text is difficult. Recent work (Shain et al., 2016; Jin et al., 2018b,a) shows that inducing PCFGs from raw text is possible, and cognitive constraints are useful for helping the induction model to find good grammars. Closely related to PCFG induction is the task of unsupervised constituency parsing from raw text where trees are unlabeled. Earlier work by Seginer (2007) and Ponvert et al. (2011) induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures"
P19-1234,P02-1017,0,0.582533,"normalizing flow g−1 are initialized from a uniform distribution with 0 mean and √ a standard deviation of 1/D. For labeled constituency evaluation, we compare against the state-of-the-art PCFG induction system DIMI (D2K15: depth bounded at 2 and 15 categories; Jin et al., 2018a) which takes word tokens as input and produces labeled trees.5 For unlabeled constituency evaluation, results from other unsupervised systems are used for comparison, including CCL (Seginer, 2007), UPPARSE (Ponvert et al., 2011), PRPN (Shen et al., 2018), as well as systems which use gold part-of-speech tags: DMV+CCM (Klein and Manning, 2002) and UML-DOP (Bod, 2006). 4 https://github.com/HIT-SCIR/ ELMoForManyLangs. 5 The DB-PCFG system (Jin et al., 2018b) is formally equivalent to the DIMI system. 2445 Model WSJ20test µ(σ) max WSJ µ(σ) CTB20 max DIMI 23.0(6.5) 34.1 this work 22.8(6.0) 24.0 22.2(3.8) 27.0 µ(σ) CTB max µ(σ) NEGRA20 max 15.4(4.4) 20.7 19.7(1.9) 24.0 13.8(3.4) 20.2 µ(σ) max NEGRA µ(σ) max 13.6(1.6) 17.5 26.2(2.8) 30.4 24.5(2.7) 29.1 Table 1: Recall-V-Measure scores for labeled grammar induction models trained on the listed treebanks with punctuation. For all tables, µ (σ) means the mean (standard deviation) of the rep"
P19-1234,P04-1061,0,0.553182,"induction is the task of unsupervised constituency parsing from raw text where trees are unlabeled. Earlier work by Seginer (2007) and Ponvert et al. (2011) induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures for unsupervised parsing and language modeling and also shows good results on English. Although unlabeled parsing evaluation is common, other work (Bisk and Hockenmaier, 2015) has argued for labeled parsing evaluation for grammar induction. Early unsupervised dependency grammars and part-of-speech induction models (Klein and Manning, 2004; Christodoulopoulos and Steedman, 2010) have been similarly augmented with neural networks and word embeddings (Tran et al., 2016; Jiang et al., 2016). Neural networks provide flexible ways to parameterize distributions, and word embeddings (Mikolov et al., 2013; Pennington et al., 2014) allow these models to use semantic information in these distributed representations. Results show that these improvements produce more accurate dependencies and POS assignments, but these improvements have not been applied to PCFG induction. Normalizing flows have been shown to be powerful models for complex"
P19-1234,P09-1011,0,0.0412885,"tion terms help the flow-based model perform even better. Most notably, the similarity performance helps the flow models greatly by restricting the freedom that the flow models have to change the context embeddings, indicating that the information in context embeddings is valuable for induction. The Real NVP model produces higher data likelihood but its performance is lower than other NICE-based models, indicating that the volume-preserving property of NICE is important for preventing overfitting. 6 Related work Earlier work on PCFG induction (Carroll and Charniak, 1992; Johnson et al., 2007; Liang et al., 2009; Tu, 2012) shows that directly inducing PCFGs from raw text is difficult. Recent work (Shain et al., 2016; Jin et al., 2018b,a) shows that inducing PCFGs from raw text is possible, and cognitive constraints are useful for helping the induction model to find good grammars. Closely related to PCFG induction is the task of unsupervised constituency parsing from raw text where trees are unlabeled. Earlier work by Seginer (2007) and Ponvert et al. (2011) induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures for unsupervised pa"
P19-1234,N15-1144,0,0.026577,"Pennington et al., 2014) allow these models to use semantic information in these distributed representations. Results show that these improvements produce more accurate dependencies and POS assignments, but these improvements have not been applied to PCFG induction. Normalizing flows have been shown to be powerful models for complex densities (Dinh et al., 2015, 2017; Rezende and Mohamed, 2015; Papa2449 makarios et al., 2017). He et al. (2018) showed improved performance on POS induction and dependency induction by incorporating normalizing flows into baseline models (Klein and Manning, 2004; Lin et al., 2015). 7 Conclusion This work proposes a neural PCFG inducer which employs context embeddings (Peters et al., 2018) in a normalizing flow model (Dinh et al., 2015) to extend PCFG induction to use semantic and morphological information. Linguistically motivated similarity penalty and categorical distance constraints are also imposed on the inducer as regularization. Labeled and unlabeled evaluation shows that the PCFG induction model with normalizing flow and context embeddings produces grammars with state-of-the-art accuracy on a variety of different languages. Results show consistent and meaningfu"
P19-1234,J93-2004,0,0.0678061,"xη to penalize the output drifting too far from the input 2444 embedding. The final objective to maximize is: |σ| L(σ) = X 1 X > log P(σi ) + λ1 kδ> d M − δe Mk2 |σ |i=0 d,e X −1 − λ2 kg (xη ) − xη k2 , (10) η∈σi where σ is a minibatch of sentences, a, b, c, d, e are all category labels, λ1 and λ2 are the weights for the two regularization terms and k . . . kn is the n-norm. 5 Experiments We report results of labeled parsing evaluation and unlabeled parsing evaluation against existing grammar induction and unsupervised parsing models. We evaluate our models on full English (The Penn Treebank; Marcus et al., 1993), Chinese (The Chinese Treebank 5.0; Xia et al., 2000) and German (NEGRA 2.0; Skut et al., 1998) constituency treebanks and the 20-or-fewer-word subsets for labeled parsing performance.3 For unlabeled parsing evaluation, we first report results on a set of languages with complex morphology chosen prior to evaluation. This set includes Czech and Russian, which are fusional languages, Korean and Uyghur, which are agglutinative languages, and Finnish, which has elements of both types. Dependency trees from the Universal Dependency Treebank (Nivre et al., 2016) of these languages are converted int"
P19-1234,D14-1162,0,0.0858082,"nsupervised parsing and language modeling and also shows good results on English. Although unlabeled parsing evaluation is common, other work (Bisk and Hockenmaier, 2015) has argued for labeled parsing evaluation for grammar induction. Early unsupervised dependency grammars and part-of-speech induction models (Klein and Manning, 2004; Christodoulopoulos and Steedman, 2010) have been similarly augmented with neural networks and word embeddings (Tran et al., 2016; Jiang et al., 2016). Neural networks provide flexible ways to parameterize distributions, and word embeddings (Mikolov et al., 2013; Pennington et al., 2014) allow these models to use semantic information in these distributed representations. Results show that these improvements produce more accurate dependencies and POS assignments, but these improvements have not been applied to PCFG induction. Normalizing flows have been shown to be powerful models for complex densities (Dinh et al., 2015, 2017; Rezende and Mohamed, 2015; Papa2449 makarios et al., 2017). He et al. (2018) showed improved performance on POS induction and dependency induction by incorporating normalizing flows into baseline models (Klein and Manning, 2004; Lin et al., 2015). 7 Con"
P19-1234,N18-1202,0,0.324098,"linois.edu Abstract Unsupervised PCFG inducers hypothesize sets of compact context-free rules as explanations for sentences. These models not only provide tools for low-resource languages, but also play an important role in modeling language acquisition (Bannard et al., 2009; Abend et al., 2017). However, current PCFG induction models, using word tokens as input, are unable to incorporate semantics and morphology into induction, and may encounter issues of sparse vocabulary when facing morphologically rich languages. This paper describes a neural PCFG inducer which employs context embeddings (Peters et al., 2018) in a normalizing flow model (Dinh et al., 2015) to extend PCFG induction to use semantic and morphological information1 . Linguistically motivated similarity penalty and categorical distance constraints are imposed on the inducer as regularization. Experiments show that the PCFG induction model with normalizing flow produces grammars with state-of-the-art accuracy on a variety of different languages. Ablation further shows a positive effect of normalizing flow, context embeddings and proposed regularizers. 1 Introduction Unsupervised PCFG inducers (Jin et al., 2018b) automatically bracket sen"
P19-1234,P11-1108,0,0.426482,"Missing"
P19-1234,D07-1043,0,0.0269832,"13.8(3.4) 20.2 µ(σ) max NEGRA µ(σ) max 13.6(1.6) 17.5 26.2(2.8) 30.4 24.5(2.7) 29.1 Table 1: Recall-V-Measure scores for labeled grammar induction models trained on the listed treebanks with punctuation. For all tables, µ (σ) means the mean (standard deviation) of the reported scores. 5.1 Labeled parsing evaluation Metric: Labeled trees induced by DIMI (Jin et al., 2018a) and the flow-based system are evaluated on six different datasets. In this evaluation, predicted labels of induced constituents that are in gold trees are compared against gold labels of these constituents6 using V-Measure (Rosenberg and Hirschberg, 2007). Recall of the induced trees is used to weight these V-Measure scores. The final Recall-V-Measure (RVM) score is computed as the product of these two measures. RVM can be maximized when gold constituents are included in induced trees and their clustering is consistent with gold annotation. RVM is equal to unlabeled recall when the matching constituents have the same clustering of labels as the gold annotation. Results: Left- and right-branching baselines are constructed by assigning 21 random labels7 to constituents in purely left- and right-branching trees. However, both branching baselines"
P19-1234,P07-1049,0,0.874694,"itialized with the empirical mean of the same sampled embeddings, but with random Gaussian noise added to each row. The parameters of the normalizing flow g−1 are initialized from a uniform distribution with 0 mean and √ a standard deviation of 1/D. For labeled constituency evaluation, we compare against the state-of-the-art PCFG induction system DIMI (D2K15: depth bounded at 2 and 15 categories; Jin et al., 2018a) which takes word tokens as input and produces labeled trees.5 For unlabeled constituency evaluation, results from other unsupervised systems are used for comparison, including CCL (Seginer, 2007), UPPARSE (Ponvert et al., 2011), PRPN (Shen et al., 2018), as well as systems which use gold part-of-speech tags: DMV+CCM (Klein and Manning, 2002) and UML-DOP (Bod, 2006). 4 https://github.com/HIT-SCIR/ ELMoForManyLangs. 5 The DB-PCFG system (Jin et al., 2018b) is formally equivalent to the DIMI system. 2445 Model WSJ20test µ(σ) max WSJ µ(σ) CTB20 max DIMI 23.0(6.5) 34.1 this work 22.8(6.0) 24.0 22.2(3.8) 27.0 µ(σ) CTB max µ(σ) NEGRA20 max 15.4(4.4) 20.7 19.7(1.9) 24.0 13.8(3.4) 20.2 µ(σ) max NEGRA µ(σ) max 13.6(1.6) 17.5 26.2(2.8) 30.4 24.5(2.7) 29.1 Table 1: Recall-V-Measure scores for lab"
P19-1234,C16-1092,1,0.881982,"he flow models greatly by restricting the freedom that the flow models have to change the context embeddings, indicating that the information in context embeddings is valuable for induction. The Real NVP model produces higher data likelihood but its performance is lower than other NICE-based models, indicating that the volume-preserving property of NICE is important for preventing overfitting. 6 Related work Earlier work on PCFG induction (Carroll and Charniak, 1992; Johnson et al., 2007; Liang et al., 2009; Tu, 2012) shows that directly inducing PCFGs from raw text is difficult. Recent work (Shain et al., 2016; Jin et al., 2018b,a) shows that inducing PCFGs from raw text is possible, and cognitive constraints are useful for helping the induction model to find good grammars. Closely related to PCFG induction is the task of unsupervised constituency parsing from raw text where trees are unlabeled. Earlier work by Seginer (2007) and Ponvert et al. (2011) induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures for unsupervised parsing and language modeling and also shows good results on English. Although unlabeled parsing evaluation"
P19-1234,W16-5907,0,0.058664,"onvert et al. (2011) induces unlabeled trees and achieves good results. More recent work (Shen et al., 2018) utilizes complex neural architectures for unsupervised parsing and language modeling and also shows good results on English. Although unlabeled parsing evaluation is common, other work (Bisk and Hockenmaier, 2015) has argued for labeled parsing evaluation for grammar induction. Early unsupervised dependency grammars and part-of-speech induction models (Klein and Manning, 2004; Christodoulopoulos and Steedman, 2010) have been similarly augmented with neural networks and word embeddings (Tran et al., 2016; Jiang et al., 2016). Neural networks provide flexible ways to parameterize distributions, and word embeddings (Mikolov et al., 2013; Pennington et al., 2014) allow these models to use semantic information in these distributed representations. Results show that these improvements produce more accurate dependencies and POS assignments, but these improvements have not been applied to PCFG induction. Normalizing flows have been shown to be powerful models for complex densities (Dinh et al., 2015, 2017; Rezende and Mohamed, 2015; Papa2449 makarios et al., 2017). He et al. (2018) showed improved p"
P19-1234,xia-etal-2000-developing,0,0.0637969,"2444 embedding. The final objective to maximize is: |σ| L(σ) = X 1 X > log P(σi ) + λ1 kδ> d M − δe Mk2 |σ |i=0 d,e X −1 − λ2 kg (xη ) − xη k2 , (10) η∈σi where σ is a minibatch of sentences, a, b, c, d, e are all category labels, λ1 and λ2 are the weights for the two regularization terms and k . . . kn is the n-norm. 5 Experiments We report results of labeled parsing evaluation and unlabeled parsing evaluation against existing grammar induction and unsupervised parsing models. We evaluate our models on full English (The Penn Treebank; Marcus et al., 1993), Chinese (The Chinese Treebank 5.0; Xia et al., 2000) and German (NEGRA 2.0; Skut et al., 1998) constituency treebanks and the 20-or-fewer-word subsets for labeled parsing performance.3 For unlabeled parsing evaluation, we first report results on a set of languages with complex morphology chosen prior to evaluation. This set includes Czech and Russian, which are fusional languages, Korean and Uyghur, which are agglutinative languages, and Finnish, which has elements of both types. Dependency trees from the Universal Dependency Treebank (Nivre et al., 2016) of these languages are converted into constituency trees (Collins et al., 1999) by keeping"
Q14-1012,J86-2003,0,0.0356834,"l subdomain. 3 Interpreting ‘Event’ and Temporal Expressions in the Clinical Domain Much prior work has been done on standardizing the annotation of events and temporal expressions in text. The most widely used approach is the ISOTimeML specification (Pustejovsky et al., 2010), an ISO standard that provides a common framework for annotating and analyzing time, events, and event relations. As defined by ISO-TimeML, an E VENT refers to anything that can be said “to obtain or hold true, to happen or to occur”. This is a broad notion of event, consistent with Bach’s use of the term “eventuality” (Bach, 1986) as well as the notion of fluents in AI (McCarthy, 2002). Because the goals of the THYME project involve automatically identifying the clinical timeline for a patient from clincal records, the scope of what should be admitted into the domain of events is interpreted more broadly than in ISO-TimeML3 . Within the THYME-TimeML guideline, an E VENT is anything relevant to the clinical timeline, i.e., anything that would show up on a detailed timeline of the patient’s care or life. The best single-word syntactic head for the E VENT is then used as its span. For example, a diagnosis would certainly"
Q14-1012,S13-2002,1,0.783065,"rk A BEFORE C, then an equivalent inferred TLINK would be used to match it. E VENT and T IMEX 3 IAA was generated based on exact and overlapping spans, respectively. These results are reported in Table 3. The THYME corpus also differs from ISOTimeML in terms of E VENT properties, with the addition of DocTimeRel, ContextualModality and ContextualAspect. IAA for these properties is in Table 4. 7.3 Baseline Systems To get an idea of how much work will be necessary to adapt existing temporal information extraction systems to the clinical domain, we took the freely available ClearTK-TimeML system (Bethard, 2013), 151 which was among the top performing systems in TempEval 2013 (UzZaman et al., 2013), and evaluated its performance on the THYME corpus. ClearTK-TimeML uses support vector machine classifiers trained on the TempEval 2013 training data, employing a small set of features including character patterns, tokens, stems, part-of-speech tags, nearby nodes in the constituency tree, and a small time word gazetteer. For E VENTs and T IMEX 3s, the ClearTK-TimeML system could be applied directly to the THYME corpus. For DocTimeRels, the relation for an E VENT was taken from the TLINK between that E VENT"
Q14-1012,W13-1903,1,0.813475,"ation for these events, (3) the interaction of general and domain-specific events and their importance in the final timeline, and, more generally, (4) the importance of rough temporality and narrative containers as a step towards finer-grained timelines. We have several avenues of ongoing and future work. First, we are working to demonstrate the utility of the THYME corpus for training machine learning models. We have designed support vector machine models with constituency tree kernels that were able to reach an F1-score of 0.737 on an E VENT-T IMEX 3 narrative container identification task (Miller et al., 2013), and we are working on training models to identify events, times and the remaining types of temporal relations. Second, as per our motivating use cases, we are working to integrate this annotation data with timeline visualization tools and to use these annotations in quality-of-care research. For example, we are using temporal reasoning built on this work to investigate the liver toxicity of methotrexate across a large corpus of EHRs (Lin et al., under review)]. Finally, we plan to explore the application of our notion of an event (anything that should be visible on a domain-appropriate timel"
Q14-1012,miltsakaki-etal-2004-penn,0,0.0158666,"h the number of events and times, and the task quickly becomes unmanageable. There are, however, strategies that we can adopt to make this labeling task more tractable. Temporal ordering relations in text are of three kinds: 1. Relations between two events 2. Relations between two times 148 3. Relations between a time and an event. ISO-TimeML, as a formal specification of the temporal information conveyed in language, makes no distinction between these ordering types. Humans, however, do make distinctions, based on local temporal markers and the discourse relations established in a narrative (Miltsakaki et al., 2004; Poesio, 2004). Because of the difficulty of humans capturing every relationship present in the note (and the disagreement which arises when annotators attempt to do so), it is vital that the annotation guidelines describe an approach that reduces the number of relations that must be considered, but still results in maximally informative temporal links. We have found that many of the weaknesses in prior annotation approaches stem from interaction between two competing goals: • The guideline should specify certain types of annotations that should be performed; • The guideline should not force"
Q14-1012,W04-0210,0,0.0401525,"d times, and the task quickly becomes unmanageable. There are, however, strategies that we can adopt to make this labeling task more tractable. Temporal ordering relations in text are of three kinds: 1. Relations between two events 2. Relations between two times 148 3. Relations between a time and an event. ISO-TimeML, as a formal specification of the temporal information conveyed in language, makes no distinction between these ordering types. Humans, however, do make distinctions, based on local temporal markers and the discourse relations established in a narrative (Miltsakaki et al., 2004; Poesio, 2004). Because of the difficulty of humans capturing every relationship present in the note (and the disagreement which arises when annotators attempt to do so), it is vital that the annotation guidelines describe an approach that reduces the number of relations that must be considered, but still results in maximally informative temporal links. We have found that many of the weaknesses in prior annotation approaches stem from interaction between two competing goals: • The guideline should specify certain types of annotations that should be performed; • The guideline should not force annotations to"
Q14-1012,W11-0419,1,0.93622,"s, as well as to develop state-of-the-art algointerventions and diagnostics which have been thus rithms to train and test on this dataset. far attempted. In other sections, the doctor may outDeriving timelines from news text requires the conline her current plan for the patient’s treatment, then crete realization of context-dependent assumptions later describe the patient’s specific medical history, about temporal intervals, orderings and organization, allergies, care directives, and so forth. underlying the explicit signals marked in the text Most critically for temporal reasoning, each clin(Pustejovsky and Stubbs, 2011). Deriving patient ical note reflects a single time in the patient’s treathistory timelines from clinical notes also involves ment history at which all of the doctor’s statements these types of assumptions, but there are special deare accurate (the D OCTIME), and each section tends mands imposed by the characteristics of the clinical to describe events of a particular timeframe. For narrative. Due to both medical shorthand practices example, ‘History of Present illness’ predominantly and general domain knowledge, many event-event describes events occuring before D OCTIME, whereas relations are"
Q14-1012,pustejovsky-etal-2010-iso,1,0.785939,"e can 2 The Nature of Clinical Documents be both domain-specific and complex, and are often we have been examining left implicit, requiring significant domain knowledge In the THYME corpus, 1 notes from a large healthcare 1,254 de-identified to accurately detect and interpret. In this paper, we discuss the demands on accurately practice (the Mayo Clinic), representing two distinct annotating such temporal information in clinical fields within oncology: brain cancer, and colon cannotes. We describe an implementation and extension cer. To date, we have principally examined two difof ISO-TimeML (Pustejovsky et al., 2010), devel- ferent general types of clinical narrative in our EHRs: oped specifically for the clinical domain, which we clinical notes and pathology reports. Clinical notes are records of physician interactions refer to as the “THYME Guidelines to ISO-TimeML” (“THYME-TimeML”), where THYME stands for with a patient, and often include multiple, clearly “Temporal Histories of Your Medical Events”. A sim- delineated sections detailing different aspects of the plified version of these guidelines formed the basis patient’s care and present illness. These notes are for the 2012 i2b2 medical-domain tempo"
Q14-1012,S13-2001,1,0.830816,"Missing"
Q14-1012,W08-0606,0,0.0123269,"for situations where doctors proffer a diagnosis, but do so cautiously, to avoid legal liability for an incorrect diagnosis or for overlooking a correct one. For example: (3) a. The signal in the MRI is not inconsistent with a tumor in the spleen. b. The rash appears to be measles, awaiting antibody test to confirm. These H EDGED E VENTs are more real than a hypothetical diagnosis, and likely merit inclusion on a timeline as part of the diagnostic history, but must not be conflated with confirmed fact. These (and other forms of uncertainty in the medical domain) are discussed extensively in (Vincze et al., 2008). In contrast, G ENERIC E VENTs do not refer to the patient’s illness or treatment, but instead discuss illness or treatment in general (often in the patient’s specific demographic). For example: (4) In other patients without significant comorbidity that can tolerate adjuvant chemotherapy, there is a benefit to systemic adjuvant chemotherapy. These sections would be true if pasted into any patient’s note, and are often identical chunks of text repeatedly used to justify a course of action or treatment as well as to defend against liability. Contextual Aspect (to distinguish from grammatical as"
Q18-1016,N10-1083,0,0.138572,"Missing"
Q18-1016,W06-2912,0,0.167751,"ion outside the Bayesian modeling paradigm. The CCL system (Seginer, 2007a) uses deterministic scoring systems to generate bracketed output of raw text. UPPARSE (Ponvert et al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar induction. Although POS tags can also be induced, this separate induction has been criticized (Pate and Johnson, 2016) for missing an opportunity to leverage information learned in grammar induction to estimate POS tags. Moreover, most of these models explore a search space that includes syntactic analyses that may be ext"
Q18-1016,W12-1913,0,0.296332,"uction (Johnson et al., 2007; Liang et al., 2009), in which subtypes of categories like noun phrases and verb phrases are induced on a given tree structure. The model described in this paper is given only words and not only induces categories for constituents but also tree structures. There are a wide variety of approaches to grammar induction outside the Bayesian modeling paradigm. The CCL system (Seginer, 2007a) uses deterministic scoring systems to generate bracketed output of raw text. UPPARSE (Ponvert et al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar indu"
Q18-1016,W01-0713,0,0.17576,"ith those of other models when evaluated on parse accuracy. Moreover, grammars acquired from this model demonstrate a consistent use of category labels, something which has not been demonstrated by other acquisition models. 1 Introduction Grammar acquisition or grammar induction (Carroll and Charniak, 1992) has been of interest to linguists and cognitive scientists for decades. This task is interesting because a well-performing acquisition model can serve as a good baseline for examining factors of grounding (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010), or as a piece of evidence (Clark, 2001; Zuidema, 2003) about the Distributional Hypothesis (Harris, 1954) against the poverty of the stimulus (Chomsky, 1965). Unfortunately, previous attempts at inducing unbounded context-free grammars (Johnson et al., 2007; Liang et al., 2009) converged to weak modes of a very multimodal distribution of grammars. There has been recent interest in applying cognitively- or empirically-motivated bounds on recursion depth to limit the search space of grammar induction models (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016). Ponvert et al. (2011) and Shain et al. (2016) in particular"
Q18-1016,P99-1065,0,0.418249,"Missing"
Q18-1016,D17-1176,0,0.0601686,"al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar induction. Although POS tags can also be induced, this separate induction has been criticized (Pate and Johnson, 2016) for missing an opportunity to leverage information learned in grammar induction to estimate POS tags. Moreover, most of these models explore a search space that includes syntactic analyses that may be extensively center embedded and therefore are unlikely to be produced by human speakers. Unlike most of these approaches, the model described in this paper uses cognitively motivated"
Q18-1016,N09-1012,0,0.0322381,"2007a) uses deterministic scoring systems to generate bracketed output of raw text. UPPARSE (Ponvert et al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar induction. Although POS tags can also be induced, this separate induction has been criticized (Pate and Johnson, 2016) for missing an opportunity to leverage information learned in grammar induction to estimate POS tags. Moreover, most of these models explore a search space that includes syntactic analyses that may be extensively center embedded and therefore are unlikely to be produced by human spe"
Q18-1016,D16-1073,0,0.0850205,"UPPARSE (Ponvert et al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar induction. Although POS tags can also be induced, this separate induction has been criticized (Pate and Johnson, 2016) for missing an opportunity to leverage information learned in grammar induction to estimate POS tags. Moreover, most of these models explore a search space that includes syntactic analyses that may be extensively center embedded and therefore are unlikely to be produced by human speakers. Unlike most of these approaches, the model described in this paper uses cog"
Q18-1016,P02-1017,0,0.741086,"are a wide variety of approaches to grammar induction outside the Bayesian modeling paradigm. The CCL system (Seginer, 2007a) uses deterministic scoring systems to generate bracketed output of raw text. UPPARSE (Ponvert et al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar induction. Although POS tags can also be induced, this separate induction has been criticized (Pate and Johnson, 2016) for missing an opportunity to leverage information learned in grammar induction to estimate POS tags. Moreover, most of these models explore a search space that include"
Q18-1016,P04-1061,0,0.436796,"ree structure. The model described in this paper is given only words and not only induces categories for constituents but also tree structures. There are a wide variety of approaches to grammar induction outside the Bayesian modeling paradigm. The CCL system (Seginer, 2007a) uses deterministic scoring systems to generate bracketed output of raw text. UPPARSE (Ponvert et al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar induction. Although POS tags can also be induced, this separate induction has been criticized (Pate and Johnson, 2016) for missing an oppo"
Q18-1016,D10-1119,0,0.0369459,"d speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy. Moreover, grammars acquired from this model demonstrate a consistent use of category labels, something which has not been demonstrated by other acquisition models. 1 Introduction Grammar acquisition or grammar induction (Carroll and Charniak, 1992) has been of interest to linguists and cognitive scientists for decades. This task is interesting because a well-performing acquisition model can serve as a good baseline for examining factors of grounding (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010), or as a piece of evidence (Clark, 2001; Zuidema, 2003) about the Distributional Hypothesis (Harris, 1954) against the poverty of the stimulus (Chomsky, 1965). Unfortunately, previous attempts at inducing unbounded context-free grammars (Johnson et al., 2007; Liang et al., 2009) converged to weak modes of a very multimodal distribution of grammars. There has been recent interest in applying cognitively- or empirically-motivated bounds on recursion depth to limit the search space of grammar induction models (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016). Ponvert et al. (201"
Q18-1016,E12-1024,0,0.0244365,"bounds on left-corner configurations of dependency grammars (Noji and Johnson, 2016), but the use of a dependency grammar makes the system impractical for addressing questions of how category types such as noun phrases may be learned. Unlike these, the model described in this paper induces a PCFG directly and then bounds it with a model-to-model transform, which yields a smaller space of learnable parameters and directly models the acquisition of category types as labels. Some induction models learn semantic grammars from text annotated with semantic predicates (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2012). There is evidence humans use semantic bootstrapping during grammar acquisition (Naigles, 1990), but these models typically rely on a set of pre-defined universals, such as combinators (Steedman, 2000), which simplify the induction task. In order to help address the question of whether such universals are indeed necessary for grammar induction, the model described in this paper does not assume any strong universals except independently motivated limits on working memory. 3 Background Like Noji and Johnson (2016) and Shain et al. (2016), the model described in this paper defines bounding depth"
Q18-1016,J93-2004,0,0.0649123,"luation The DB-PCFG model described in Section 4 is evaluated first on synthetic data to determine whether it can reliably learn a recursive grammar from data with a known optimum solution, and to determine the hyper-parameter value for β for doing so. Two experiments on natural data are then carried out. First, the model is run on natural data from the Adam 12 Again, d¯ = maxd {adt−1 , ⊥}. and Eve parts of the CHILDES corpus (Macwhinney, 1992) to compare with other grammar induction systems on a human-like acquisition task. Then data from the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) is used for further comparison in a domain for which competing systems are optimized. The competing systems include UPPARSE (Ponvert et al., 2011)13 , CCL (Seginer, 2007a)14 , BMMM+DMV with undirected dependency features (Christodoulopoulos et al., 2012)15 and UHHMM (Shain et al., 2016).16 For the natural language datasets, the variously parametrized DB-PCFG systems17 are first validated on a development set, and the optimal system is then run until convergence with the chosen hyperparameters on the test set. In development experiments, the log-likelihood of the dataset plateaus usually after"
Q18-1016,D16-1004,0,0.512782,"uistics The Ohio State University jin.544@osu.edu Finale Doshi-Velez Timothy Miller Harvard University Boston Children’s Hospital & finale@seas.harvard.edu Harvard Medical School timothy.miller@childrens.harvard.edu William Schuler Department of Linguistics The Ohio State University schuler@ling.osu.edu Lane Schwartz Department of Linguistics University of Illinois at Urbana-Champaign lanes@illinois.edu Abstract There has been recent interest in applying cognitively- or empirically-motivated bounds on recursion depth to limit the search space of grammar induction models (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016). This work extends this depthbounding approach to probabilistic contextfree grammar induction (DB-PCFG), which has a smaller parameter space than hierarchical sequence models, and therefore more fully exploits the space reductions of depthbounding. Results for this model on grammar acquisition from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy. Moreover, grammars acquired from this model demonstrate a consistent use of category labels, something which has not been demonstrated by o"
Q18-1016,C16-1003,0,0.0153129,"ammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar induction. Although POS tags can also be induced, this separate induction has been criticized (Pate and Johnson, 2016) for missing an opportunity to leverage information learned in grammar induction to estimate POS tags. Moreover, most of these models explore a search space that includes syntactic analyses that may be extensively center embedded and therefore are unlikely to be produced by human speakers. Unlike most of these approaches, the model described in this paper uses cognitively motivated bounds on the depth of human recursive processing to constrain its search of possible trees for input sentences. Some previous work uses depth bounds in the form of sequence models (Ponvert et al., 2011; Shain et al"
Q18-1016,P11-1108,0,0.417115,"Missing"
Q18-1016,C92-1032,0,0.0489003,"ersals, such as combinators (Steedman, 2000), which simplify the induction task. In order to help address the question of whether such universals are indeed necessary for grammar induction, the model described in this paper does not assume any strong universals except independently motivated limits on working memory. 3 Background Like Noji and Johnson (2016) and Shain et al. (2016), the model described in this paper defines bounding depth in terms of memory elements required in a left-corner parse. A left-corner parser (Rosenkrantz and Lewis, 1970; JohnsonLaird, 1983; Abney and Johnson, 1991; Resnik, 1992) uses a stack of memory elements to store derivation fragments during incremental processing. Each derivation fragment represents a disjoint connected component of phrase structure a/b consisting of a top sign a lacking a bottom sign b yet to come. For example, Figure 1 shows the derivation fragments in a traversal of a phrase structure tree for the sentence The cart the horse the man bought pulled broke. Immediately before processing the word man, the traversal has recognized three fragments of tree structure: two from category NP to category RC (covering the cart and the horse) and one from"
Q18-1016,J10-1001,1,0.87507,"Missing"
Q18-1016,D14-1141,0,0.0469766,"Missing"
Q18-1016,P07-1049,0,0.935012,"describes a Bayesian Dirichlet model of depth-bounded probabilistic context-free grammar (PCFG) induction. Bayesian Dirichlet models have been applied to the related area of latent variable PCFG induction (Johnson et al., 2007; Liang et al., 2009), in which subtypes of categories like noun phrases and verb phrases are induced on a given tree structure. The model described in this paper is given only words and not only induces categories for constituents but also tree structures. There are a wide variety of approaches to grammar induction outside the Bayesian modeling paradigm. The CCL system (Seginer, 2007a) uses deterministic scoring systems to generate bracketed output of raw text. UPPARSE (Ponvert et al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headd"
Q18-1016,C16-1092,1,0.0661038,"niversity jin.544@osu.edu Finale Doshi-Velez Timothy Miller Harvard University Boston Children’s Hospital & finale@seas.harvard.edu Harvard Medical School timothy.miller@childrens.harvard.edu William Schuler Department of Linguistics The Ohio State University schuler@ling.osu.edu Lane Schwartz Department of Linguistics University of Illinois at Urbana-Champaign lanes@illinois.edu Abstract There has been recent interest in applying cognitively- or empirically-motivated bounds on recursion depth to limit the search space of grammar induction models (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016). This work extends this depthbounding approach to probabilistic contextfree grammar induction (DB-PCFG), which has a smaller parameter space than hierarchical sequence models, and therefore more fully exploits the space reductions of depthbounding. Results for this model on grammar acquisition from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy. Moreover, grammars acquired from this model demonstrate a consistent use of category labels, something which has not been demonstrated by other acquisition mode"
S16-1120,P03-1054,0,0.029583,"LexiM works to align chunks which have lexical overlap, while string similarity and semantic distance work in two ways. The first way is to align chunks that that cannot be handled with those rules and follow by assining labels to the aligned chunks. The second way is for the chunks that are aligned through LexiM, the string distance and semantic similarity rule will provide similarity and relatedness labels. There are existing tools which perform tasks such as string distance measurement like SecondString (Cohen et al., 2003) and stringmetric (Madden, 2013), PoS taggers like Stanford Parser (Klein and Manning, 2003), and dictionary for synonymous words like WordNet (Fellbaum, 1998). Cohen et al. (2003) shows that Jaro-Winkler is an effective string distance metric for name matching task. Liu et al. (2010) has shown that TESLA, a similarity metric that considers both PoS tags and semantic equivalence (based on WordNet synsets), is effective in the task of automatic evaluation of machine translation in English language. We used Jaro-Winkler proximity (SecondString implementation) for string distance measurement, and TESLA for semantic similarity measurement. We analyzed the chunks in terms of their string"
S16-1120,W10-1754,0,0.027913,"d follow by assining labels to the aligned chunks. The second way is for the chunks that are aligned through LexiM, the string distance and semantic similarity rule will provide similarity and relatedness labels. There are existing tools which perform tasks such as string distance measurement like SecondString (Cohen et al., 2003) and stringmetric (Madden, 2013), PoS taggers like Stanford Parser (Klein and Manning, 2003), and dictionary for synonymous words like WordNet (Fellbaum, 1998). Cohen et al. (2003) shows that Jaro-Winkler is an effective string distance metric for name matching task. Liu et al. (2010) has shown that TESLA, a similarity metric that considers both PoS tags and semantic equivalence (based on WordNet synsets), is effective in the task of automatic evaluation of machine translation in English language. We used Jaro-Winkler proximity (SecondString implementation) for string distance measurement, and TESLA for semantic similarity measurement. We analyzed the chunks in terms of their string distance and semantic similarity scores, relating the scores to the similarity and relatedness types and scores in the annotated data. We identified the lowest, average and highest values for t"
S18-2014,P15-1162,0,0.0720084,"Missing"
S18-2014,W16-2911,1,0.894182,"Missing"
S18-2014,N13-1090,0,0.22307,"s as input a set of UMLS concept unique identifiers (CUIs) derived from the text of the notes of a patient and jointly predicts all billing codes associated with the patient. CUIs are extracted from notes by mapping spans of clinically-relevant text (e.g. shortness of breath, appendectomy, MRI) to entries in the UMLS Metathesaurus. CUIs can be easily extracted by existing tools such as Apache cTAKES (http://ctakes.apache.org). Our neural network model (Figure 1) is inspired by Deep Averaging Network (DAN) (Iyyer et al., 2015), FastText (Joulin et al., 2016), and continuous bag-ofwords (CBOW) (Mikolov et al., 2013a,b) models. CUIn Embedding Layer Averaging Layer Hidden Layer Output Layer Figure 1: Neural network model for learning patient representations from text. Model Architecture: The model takes as input a set of CUIs. CUIs are mapped to 300dimensional concept embeddings which are averaged and passed on to a 1000-dimensional hidden layer, creating a vectorial representation of a patient. The final network layer consists of n sigmoid units that are used for joint billing code prediction. The output of each sigmoid unit is converted to a binary (1/0) outcome. The number of units n in the output laye"
W12-2409,W09-1901,0,0.201922,"unit of selection is a document, typically containing multiple coreference pairs each represented as a feature vector. The most obvious way to extend a single instance informativeness metric to the document scenario is to aggregate the informativeness scores. Several uncertainty metrics have been proposed that follow that route to adapt single instance selection to multiple instance scenarios (Settles et al., 2008; Tomanek et al., 2009). We borrow some of these metrics and propose several new ones. To the best of our knowledge only one work exists that explores AL for coreference resolution. Gasperin (2009) experiments with an instance based approach in which batches of anaphoric pairs are selected on each iteration of AL. In these experiments, AL did not outperform the passive learning baseline, probably due to selecting batches of large size. 2.2 Active Learning Active Learning (AL) is a popular approach to selecting unlabeled data for annotation (Settles, 2010) that can potentially lead to drastic reductions in the amount of annotation that is necessary for training an accurate statistical classifier. Unlike passive learning, where the data is sampled for annotation randomly, AL delegates dat"
W12-2409,P07-1107,0,0.0380504,"Missing"
W12-2409,P02-1014,0,0.183915,"t antecedent the classifier links with the highest probability. There are separate pairwise classifiers for named entity and pronominal anaphor types. In the domain of clinical narratives, person mentions and personal pronouns in particular are not especially challenging – the vast majority of person mentions are the patient. In addition, pronoun mentions, while important, are relatively rare. Thus we are primarily interested in named entity coreference classification, and we use that classifier as the basis of the work described here. The feature set of this system is similar to that used by Ng and Cardie (2002). That system includes features based on surface form of the mentions, shallow syntactic information, and lexical semantics from WordNet. The system used here has a similar feature set but uses Unified Medical Language System (UMLS)2 semantic features as it is intended for clinical text, and also incorporates several syntactic features extracted from constituency parses extracted from cTAKES. To generate training data for active learning simulations, mention detection is run first (cTAKES contains a rule-based NER system) to find named entities and a constituency parser situates entities in a"
W12-2409,P10-1142,0,0.0608072,"us pieces of information about an entity and fitting the information into a standard data structure that can be reasoned about. An example CEM template is that for Disease with attributes for Body Location, Associated Sign or Symptom, Subject, Negation, Uncertainty, and Severity. Since a given entity may have many different attributes and relations, it 1 Background http://intermountainhealthcare.org/cem Space does not permit a thorough review of coreference resolution, but recent publications covered the history and current state of the art for both the general domain and the clinical domain (Ng, 2010; Pradhan et al., 2011; Zheng et al., 2011). The system used here (Zheng et al., 2012) is an end-to-end coreference resolution system, meaning that the algorithm receives no gold standard information about mentions, named entity types, or any linguistic information. The coreference resolution system is a module of the clinical Textual Analysis and Knowledge Extraction System (cTAKES) (Savova et al., 2010) that is trained on clinical data. It takes advantage of named entity recognition (NER) and categorization to detect entity mentions, and uses several cTAKES modules as feature generators, inc"
W12-2409,W11-1901,0,0.0971816,"of information about an entity and fitting the information into a standard data structure that can be reasoned about. An example CEM template is that for Disease with attributes for Body Location, Associated Sign or Symptom, Subject, Negation, Uncertainty, and Severity. Since a given entity may have many different attributes and relations, it 1 Background http://intermountainhealthcare.org/cem Space does not permit a thorough review of coreference resolution, but recent publications covered the history and current state of the art for both the general domain and the clinical domain (Ng, 2010; Pradhan et al., 2011; Zheng et al., 2011). The system used here (Zheng et al., 2012) is an end-to-end coreference resolution system, meaning that the algorithm receives no gold standard information about mentions, named entity types, or any linguistic information. The coreference resolution system is a module of the clinical Textual Analysis and Knowledge Extraction System (cTAKES) (Savova et al., 2010) that is trained on clinical data. It takes advantage of named entity recognition (NER) and categorization to detect entity mentions, and uses several cTAKES modules as feature generators, including the NER module,"
W12-2409,W09-1902,0,0.0248103,"Missing"
W12-2409,N07-1011,0,\N,Missing
W12-2409,D07-1051,0,\N,Missing
W12-2409,W07-1516,0,\N,Missing
W12-2409,N06-2015,0,\N,Missing
W12-2409,P11-1079,0,\N,Missing
W12-2409,J01-4004,0,\N,Missing
W12-2409,S10-1001,0,\N,Missing
W12-2409,N04-1012,0,\N,Missing
W13-1903,W12-2404,0,0.0820558,"tion for this task. 1 In this work we focus on extracting a particular temporal relation, C ONTAINS, that holds between a time expression and an event expression. This level of representation is based on the computational discourse model of narrative containers (Pustejovsky and Stubbs, 2011), which are time expressions or events which are central to a section of a text, usually manifested by being relative hubs of temporal relation links. We argue that containment relations are useful as an intermediate level of granularity between full temporal relation extraction and “coarse” temporal bins (Raghavan et al., 2012) like before admission, on admission, and after admission. Correctly extracting C ON TAINS relations will, for example, allow for more accurate placement of events on a timeline, to the resolution possible by the number of time expressions in the document. We suspect that this finer grained information will also be more useful for downstream applications like coreference, for which coarse information was found to be useful. The approach we develop is a supervised machine Introduction Clinical narratives are a rich source of unstructured information that hold great potential for impacting clini"
W13-1903,strassel-etal-2010-darpa,0,0.107061,"Missing"
W13-1903,tannier-muller-2008-evaluation,0,0.0173461,"he same set of relations using different set of axioms. To take a very simple example, given a gold set of relations A<B and B<C, and given the system output A<B, A<C and B<C, if one were to compute a plain precision/recall metric, then the axiom A<C would be counted against the system, when one can easily infer from the gold set of relations that it is indeed correct. With more relations the inference process becomes more complex. Recently there has been some work trying to address the shortcomings of the plain F1 score (Muller and Tannier, 2004; Setzer et al., 2006; UzZaman and Allen, 2011; Tannier and Muller, 2008; Tannier and Muller, 2011). However, the community has not yet come to a consensus on the best evaluation approach. Two recent evaluations, TempEval-3 (UzZaman et al., 2013) and the 2012 i2b2 Challenge (Sun et al., 2013), used an implementation of the proposal by (UzZaman and Allen, 2011). However, as described in Cherry et al. (2013), this algorithm, which uses a greedy graph minimization approach, is sensitive to the order in which the temporal relations are presented to the scorer. In addition, the scorer is not able to give credit for non-redundant, nonminimum links (Cherry et al., 2013)"
W13-1903,P11-2061,0,0.0184812,"t is possible to define the same set of relations using different set of axioms. To take a very simple example, given a gold set of relations A<B and B<C, and given the system output A<B, A<C and B<C, if one were to compute a plain precision/recall metric, then the axiom A<C would be counted against the system, when one can easily infer from the gold set of relations that it is indeed correct. With more relations the inference process becomes more complex. Recently there has been some work trying to address the shortcomings of the plain F1 score (Muller and Tannier, 2004; Setzer et al., 2006; UzZaman and Allen, 2011; Tannier and Muller, 2008; Tannier and Muller, 2011). However, the community has not yet come to a consensus on the best evaluation approach. Two recent evaluations, TempEval-3 (UzZaman et al., 2013) and the 2012 i2b2 Challenge (Sun et al., 2013), used an implementation of the proposal by (UzZaman and Allen, 2011). However, as described in Cherry et al. (2013), this algorithm, which uses a greedy graph minimization approach, is sensitive to the order in which the temporal relations are presented to the scorer. In addition, the scorer is not able to give credit for non-redundant, nonminimum li"
W13-1903,S07-1014,0,0.206982,"Missing"
W13-1903,N06-1037,0,0.0278824,"lt for using constituency structure in tree kernels for temporal relation extraction. The Path Tree representation uses a sub-tree of the whole constituent tree, but removes all nodes that are not along the path between the two arguments. Path information has been used in standard feature kernels (Pradhan et al., 2008), with each individual path being a possible boolean feature. The Path-Enclosed Tree representation is based on the smallest sub-tree that encloses the two proposed arguments. This is a representation that has shown value in other work using tree kernels for relation extraction (Zhang et al., 2006; Mirroshandel et al., 2009). The information contained in the PET representation is a superset of that contained in the Path Tree representation, since it includes the full path between arguments as well as the structure between arguments and the argument text. This means that it can take into account path information while also considering constituent structure between arguments that may play a role in determining whether the two arguments are related. For example, temporal cue words like after or during may occur between arguments and will not be captured by Path Trees. Like the PT represen"
W13-1903,S07-1025,1,0.869954,"ata, annotated the I NCLUDES relation, but merged it with other relations for the evaluation due to low inter-annotator agreement. Since no narrative container-annotated corpora exist, there are also no existing models for extracting narrative container relations. However, we can draw on the various methods applied to related temporal relation tasks. Most relevant is the work on linking events to timestamps. This was one of the subtasks in TempEval 2007 and 2010, and systems used a variety of features including words, part-of-speech tags, and the syntactic path between the event and the time (Bethard and Martin, 2007; Llorens et al., 2010). Syntactic path features were also used in the 2012 i2b2 Challenge, where they provided gains especially for intra-sentential temporal links (Xu et al., 2013). Recent research has also looked to syntactic tree kernels for temporal relation extraction. Mirroshandel et al. (2009) used a path-enclosed tree (i.e., selecting only the sub-tree containing the event and time), and used various weighting scheme variants of this approach on the TimeBank (Pustejovsky et al., 2003a) and Opinion4 corpora. Hovy et al. (2012) used a flat tree structure for each event-time pair, includ"
W13-1903,E12-1019,0,0.205712,"Missing"
W13-1903,S10-1063,0,0.0415784,"ES relation, but merged it with other relations for the evaluation due to low inter-annotator agreement. Since no narrative container-annotated corpora exist, there are also no existing models for extracting narrative container relations. However, we can draw on the various methods applied to related temporal relation tasks. Most relevant is the work on linking events to timestamps. This was one of the subtasks in TempEval 2007 and 2010, and systems used a variety of features including words, part-of-speech tags, and the syntactic path between the event and the time (Bethard and Martin, 2007; Llorens et al., 2010). Syntactic path features were also used in the 2012 i2b2 Challenge, where they provided gains especially for intra-sentential temporal links (Xu et al., 2013). Recent research has also looked to syntactic tree kernels for temporal relation extraction. Mirroshandel et al. (2009) used a path-enclosed tree (i.e., selecting only the sub-tree containing the event and time), and used various weighting scheme variants of this approach on the TimeBank (Pustejovsky et al., 2003a) and Opinion4 corpora. Hovy et al. (2012) used a flat tree structure for each event-time pair, including only tokenbased inf"
W13-1903,Y09-1038,0,0.428527,"methods applied to related temporal relation tasks. Most relevant is the work on linking events to timestamps. This was one of the subtasks in TempEval 2007 and 2010, and systems used a variety of features including words, part-of-speech tags, and the syntactic path between the event and the time (Bethard and Martin, 2007; Llorens et al., 2010). Syntactic path features were also used in the 2012 i2b2 Challenge, where they provided gains especially for intra-sentential temporal links (Xu et al., 2013). Recent research has also looked to syntactic tree kernels for temporal relation extraction. Mirroshandel et al. (2009) used a path-enclosed tree (i.e., selecting only the sub-tree containing the event and time), and used various weighting scheme variants of this approach on the TimeBank (Pustejovsky et al., 2003a) and Opinion4 corpora. Hovy et al. (2012) used a flat tree structure for each event-time pair, including only tokenbased information (words, part of speech tags) between the event and time, and found that adding such tree kernels on top of a baseline set of features improved event-time linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). While Mirroshandel et"
W13-1903,C04-1008,0,0.0380021,"ial properties of temporal relations, because it is possible to define the same set of relations using different set of axioms. To take a very simple example, given a gold set of relations A<B and B<C, and given the system output A<B, A<C and B<C, if one were to compute a plain precision/recall metric, then the axiom A<C would be counted against the system, when one can easily infer from the gold set of relations that it is indeed correct. With more relations the inference process becomes more complex. Recently there has been some work trying to address the shortcomings of the plain F1 score (Muller and Tannier, 2004; Setzer et al., 2006; UzZaman and Allen, 2011; Tannier and Muller, 2008; Tannier and Muller, 2011). However, the community has not yet come to a consensus on the best evaluation approach. Two recent evaluations, TempEval-3 (UzZaman et al., 2013) and the 2012 i2b2 Challenge (Sun et al., 2013), used an implementation of the proposal by (UzZaman and Allen, 2011). However, as described in Cherry et al. (2013), this algorithm, which uses a greedy graph minimization approach, is sensitive to the order in which the temporal relations are presented to the scorer. In addition, the scorer is not able t"
W13-1903,J08-2006,1,0.41968,"of unrelated structure that adds noise to the classifier. Here we include it in an attempt to get to the root of an apparent discrepancy in the tree kernel literature, as explained in Section 2, in which Hovy et al. (2012) report a negative result and Mirroshandel et al. (2009) report a positive result for using constituency structure in tree kernels for temporal relation extraction. The Path Tree representation uses a sub-tree of the whole constituent tree, but removes all nodes that are not along the path between the two arguments. Path information has been used in standard feature kernels (Pradhan et al., 2008), with each individual path being a possible boolean feature. The Path-Enclosed Tree representation is based on the smallest sub-tree that encloses the two proposed arguments. This is a representation that has shown value in other work using tree kernels for relation extraction (Zhang et al., 2006; Mirroshandel et al., 2009). The information contained in the PET representation is a superset of that contained in the Path Tree representation, since it includes the full path between arguments as well as the structure between arguments and the argument text. This means that it can take into accoun"
W13-1903,W11-0419,0,0.0857915,"are related by narrative containers. We use support vector machines with composite kernels, which allows for integrating standard feature kernels with tree kernels for representing structured features such as constituency trees. Our experiments show that using tree kernels in addition to standard feature kernels improves F1 classification for this task. 1 In this work we focus on extracting a particular temporal relation, C ONTAINS, that holds between a time expression and an event expression. This level of representation is based on the computational discourse model of narrative containers (Pustejovsky and Stubbs, 2011), which are time expressions or events which are central to a section of a text, usually manifested by being relative hubs of temporal relation links. We argue that containment relations are useful as an intermediate level of granularity between full temporal relation extraction and “coarse” temporal bins (Raghavan et al., 2012) like before admission, on admission, and after admission. Correctly extracting C ON TAINS relations will, for example, allow for more accurate placement of events on a timeline, to the resolution possible by the number of time expressions in the document. We suspect th"
W13-1903,N07-1070,1,\N,Missing
W13-1903,S10-1010,0,\N,Missing
W13-1903,S13-2001,0,\N,Missing
W13-5101,W09-1107,0,0.0140136,"tween areas under the curve (Active - Passive) is observed in several cases (e.g. 1d, 2b, 3d), it is unlikely to be of consequence in practice. Active learning would typically be stopped at a much earlier stage, e.g. when 1/3 or 1/2 of the data has been annotated. Nevertheless, it would still be interesting to uncover the conditions leading to this behavior and we leave this investigation for future work. Both of these scenarios, where active learning performed worse than random sampling, highlight the need for developing stopping criteria for active learning such as (Laws and Sch¨atze, 2008; Bloodgood and Vijay-Shanker, 2009). In a practical application of active learning, a held-out test set is unlikely to be available and some automated means of tracking the progress of active learning is needed. We plan to pursue this avenue of research in the future. In addition to that, we plan to explore the portability of the models trained via active learning. It would also be interesting to investigate the effect of swapping the base classifier: in this work we collect the data for annotation using a multinomial Naive Bayes model. It is still not clear whether the gains obtained by active learning would be preserved if a"
W13-5101,C08-1059,0,0.0472528,"Missing"
W13-5101,W12-2409,1,0.787453,"text of several text classification tasks and find that active learning did not always perform better than random sampling. The use of SVMs restricted their evaluation to binary classification only, limiting the applicability of their findings for many clinical NLP tasks. Chen et al. (Chen et al., 2011) investigate the use of active learning for assertion classification and show that active learning outperforms random sampling. Both of the above mentioned studies experiment with datasets that are quite different from ours in that they annotate relatively short snippets of text. Miller et al. (Miller et al., 2012) develop a series of active learning methods that are highly tailored to coreference resolution in clinical texts. Finally, Hahn et al. (Hahn et al., 2012) utilize active learning in practice for a corpus annotation task that involves labeling pathological phenomena in MEDLINE abstracts. Unfortunately they do not compare the performance of their active learning Active Learning Active learning is an approach to selecting unlabeled data for annotation that can potentially lead to large reductions in the amount of manual labeling that is necessary for training an accurate classifier. Unlike passi"
W13-5101,D07-1082,0,0.0286922,"n NLP for Medicine and Biology associated with RANLP 2013, pages 1–8, Hissar, Bulgaria, 13 September 2013. pioneered the use of active learning for text categorization. Their scenario, known as pool-based active learning, corresponds to a setting where an abundant supply of text documents is available but only a small sample can be economically annotated by a human labeler. Pool-based active learning has since been explored for many problem domains such as text classification (McCallum and Nigam, 1998; Tong and Chang, 2001; Tong and Koller, 2002), word-sense disambiguation (Chen et al., 2006; Zhu and Hovy, 2007; Dligach and Palmer, 2011), information extraction (Thompson et al., 1999; Settles et al., 2008), and image classification (Tong and Chang, 2001; Hoi et al., 2006). curacy when compared to passive learning. 2 2.1 Background Phenotyping as Document Classification Phenotyping can be viewed as a document classification task in which a document consists of all EHR documents and other associated data (labs, ordered medications, etc.) for the given patient. Initial filtering is usually performed based on a set of inclusion and exclusion criteria (ICD-9 codes, CPT codes, laboratory results, medicati"
W13-5101,N06-1016,0,\N,Missing
W13-5101,P11-2002,1,\N,Missing
W15-3809,bethard-etal-2014-cleartk,1,0.831713,"terministically extracted from those assignments. The CRF tagger processes one sentence at a time, assigning labels to all tokens within that sentence simultaneously. Systems We implemented a variety of systems in an attempt to empirically evaluate the best way to model the time span classification task. For all systems, the temporal expression extractor is implemented within Apache cTAKES3 (clinical Text Analysis and Knowledge Extraction System) (Savova et al., 2011), making use of its components for feature generation as well as its interface to the source general-domain NLP system ClearTK (Bethard et al., 2014) which in turn interfaces with different machine learning libraries, including LibSVM (Chang and Lin, 2011) and CRFSuite (Okazaki, 2007). 2.2.1 Sequence Models We developed three sequence-based models for this task, each with different perceived strengths. The first system is perhaps the simplest, a standard BIO (Begin-Inside-Outside) tagger using an off the shelf support vector machine (SVM) classifier (Cortes and Vapnik, 1995). BIO taggers work by labeling every token in a sentence as the beginning (B), inside (I), or outside (O) of some subsequence in the data (in this case a temporal expre"
W15-3809,S13-2002,1,0.885603,"Missing"
W15-3809,S10-1071,0,0.0716282,"Missing"
W15-3809,S13-2003,0,0.0273532,"Missing"
W15-3809,Q14-1012,1,0.917189,"Missing"
W15-3809,W13-1903,1,0.851141,"tejovsky et al., 2003) spurred much of the early research by providing a manually annotated corpus of events, times and temporal relations. Shared tasks such as TERN1 , which focused on time expressions, and TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), which included events and temporal relations as well, helped build a comIntroduction Temporal information is ubiquitous in clinical narratives, and accurately extracting temporal information has recently been the focus of a great deal of work in clinical natural language processing (NLP) (Raghavan et al., 2012; Miller et al., 2013; Sun et al., 2013). Relevant temporal information includes events, time expressions, and temporal relations between pairs of events and/or times. The accurate extraction of temporal information would be enabling technology for sophisticated downstream processing that requires temporal awareness of patient status. One promising application is question answering, where a physician can directly ask questions about a patient’s medical record. Many question types of interest are explicitly temporal (When was the patient’s last colonoscopy? ), but almost all are implicitly temporal in the sense tha"
W15-3809,S13-2001,0,0.024644,"stexp type, which is specific to the clinical domain. Exemplified by terms like postoperatively, this type represents time spans relative to some event, often an operation. Temporal information extraction has been a topic of a great deal of work both in the clinical and general NLP domains. In the general NLP domain, the TimeBank (Pustejovsky et al., 2003) spurred much of the early research by providing a manually annotated corpus of events, times and temporal relations. Shared tasks such as TERN1 , which focused on time expressions, and TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), which included events and temporal relations as well, helped build a comIntroduction Temporal information is ubiquitous in clinical narratives, and accurately extracting temporal information has recently been the focus of a great deal of work in clinical natural language processing (NLP) (Raghavan et al., 2012; Miller et al., 2013; Sun et al., 2013). Relevant temporal information includes events, time expressions, and temporal relations between pairs of events and/or times. The accurate extraction of temporal information would be enabling technology for sophisticated downstream processing th"
W15-3809,S07-1014,0,0.0265878,"ation from general domain methods is the Prepostexp type, which is specific to the clinical domain. Exemplified by terms like postoperatively, this type represents time spans relative to some event, often an operation. Temporal information extraction has been a topic of a great deal of work both in the clinical and general NLP domains. In the general NLP domain, the TimeBank (Pustejovsky et al., 2003) spurred much of the early research by providing a manually annotated corpus of events, times and temporal relations. Shared tasks such as TERN1 , which focused on time expressions, and TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), which included events and temporal relations as well, helped build a comIntroduction Temporal information is ubiquitous in clinical narratives, and accurately extracting temporal information has recently been the focus of a great deal of work in clinical natural language processing (NLP) (Raghavan et al., 2012; Miller et al., 2013; Sun et al., 2013). Relevant temporal information includes events, time expressions, and temporal relations between pairs of events and/or times. The accurate extraction of temporal information would be enabling technol"
W15-3809,W12-2404,0,\N,Missing
W15-3809,S10-1010,0,\N,Missing
W16-2911,E12-1021,0,0.0400658,"Missing"
W16-2911,D09-1026,0,0.0610051,"009) developed optimization procedures for α and β and found that optimization of the document-topic prior α led to improved results, as measured by perplexity on heldout data. Jagarlamudi et al. (2012) found that priors on both α and β allowed them to incorporate information into the LDA inference, though they found that a more complex model structure was necessary to properly incorporate the information, which requires a more complex inference procedure. Other relevant topic modeling work involves the augmentation of LDA-style models for labeling documents with multiple topics. Labeled LDA (Ramage et al., 2009) creates a topic for each label in a multi-label setting, and takes advantage of gold standard labels to learn topic distributions for each label. In the author-topic model (RosenZvi et al., 2004), a document is generated by a set 1 The i2b2 Challenge datasets are publicly available with a Data Use Agreement at https://i2b2.org/NLP/DataSets/. 85 3.2 Informed LDA Possible thresholds include 0 and 1/K. We found that thresholds allowed for too much variation, and led to some severely skewed label distributions, so that the next stage classifier may have only a few positive examples to work with."
W16-2914,S15-2136,1,0.767766,"Missing"
W16-2914,S16-1165,1,0.926678,"n a pair of arguments. Algorithm 1 Expansion for event-time relations 1: Given a gold-standard annotated event-time relation r(e,t), where e is an event, t is a temporal expression, r ∈ {C ONTAINS, B EFORE, . . . , N ONE} 2: for UMLS entity u ∈ E XPAND(e) do 3: Create relation r0 (u, t), r0 ← r 4: Add r0 to training data 5: end for 3 3.1 Experiments Dataset We tested our event expansion technique on a publicly available clinical corpus: the colon cancer set of the THYME corpus (Styler et al., 2014) used in SemEval 2015 Task 6 (Bethard et al., 2015) and SemEval 2016 Task 12: Clinical TempEval (Bethard et al., 2016). It contains 600 documents (400 oncology notes and 200 pathology notes) of 200 colon cancer patients. The gold standard annotations contain events (including both medical and general events, all annotated by head words), temporal expressions (e.g. tomorrow, postoperative, and March-11-2009), and temporal relations. We used the same training/development/test split as Clinical TempEval. The development set was used for testing research questions and building final models. Once the models were deemed finalized, they were rebuilt on the combined training and development sets and tested on the tes"
W16-2914,P11-2047,1,0.850314,"nguage System (UMLS) (Lindberg et al., 1993). It bridges the gap between different syntactic annotations of events in clinical corpora. We show that this method is superior to representing the same information as additional features, that it differs from plain upsampling, and that the primary mechanism of improvement is in the better representation of between-argument features. Our method can be viewed as a new form of data augmentation, akin to the generation of image variants for vision recognition (Krizhevsky et al., 2012) or the generation of word substitutions for information extraction (Kolomiyets et al., 2011). 108 Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 108–113, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics Contains E VENT Algorithm 2 Expansion for event-event relations T IME A CT- scan of abdomen and pelvis performed on Mar 11 ⇓ UMLS UMLS C ONTAINS UMLS C ONTAINS 1: Given a gold-standard annotated event-event relation r(ea ,eb ), where ea , eb are events, r ∈ {C ONTAINS, N ONE} 2: for UMLS entity ua ∈ E XPAND(ea ) do 3: if not overlaps(span(ua ), span(eb )) then 4: Create relation r0 (ua , eb ), r0 ← r 5: Add r0 to tra"
W16-2914,Q14-1012,1,0.911843,"Missing"
W16-2914,P11-2061,0,0.177114,"AINS(scan, March 11) is duplicated in Figure 1. Thus we also compare our UMLS-informed expansion of instances to simple duplication of instances1 . 4. Which types of features benefit most from the expansion? There are three groups of token We test all research questions by training on the training set and testing on the development set with token-based features for the event-time relations. Note that expansion is applied only to the training set, not to the development or test set. 3.4 Evaluation For results on the development set, we calculate closure-enhanced precision, recall and F1-score (UzZaman and Allen, 2011) on just the withinsentence relations (since that’s what our models are able to predict). Precision is the percentage of system-generated relations that can be verified in the transitive closure of the gold standard relations. Recall is the percentage of gold standard relations that can be found in the transitive closure of the system-generated relations. The final F1-score is the harmonic mean of the transitiveclosure-processed precision and recall. For results on the test set, we used the official Clinical TempEval evaluation scripts so that our results are directly comparable with the outco"
W16-2914,S13-2001,0,0.0395915,"linical temporal relation extraction, works beyond featurizing or duplicating the same information, can generalize between-argument signals in a more effective and robust fashion. We also report a new state-of-the-art result, which is a two point improvement over the best Clinical TempEval 2016 system. 1 Introduction Temporal relation extraction is important for understanding ordering of events from a narrative text. Recent years have seen annotated corpora created for temporal information extraction, from newspaper text (Pustejovsky et al., 2003; Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), to clinical narratives (Savova et al., 2009; Sun et al., 2013; Styler et al., 2014), all with the aim of developing systems for building event timelines from textual descriptions of events. Such narrative timelines are important for information extraction tasks such as question answering (Kahn et al., 1990), clinical outcomes prediction (Schmidt et al., 2005; Lin et al., 2014), and the identification of temporal patterns (Zhou and Hripcsak, 2007) among many. In a typical supervised approach to the temporal relation extraction task, argument pairs consist of pairs of events or temporal expres"
W16-2914,S07-1014,0,0.0247806,"System (UMLS). This method notably improves clinical temporal relation extraction, works beyond featurizing or duplicating the same information, can generalize between-argument signals in a more effective and robust fashion. We also report a new state-of-the-art result, which is a two point improvement over the best Clinical TempEval 2016 system. 1 Introduction Temporal relation extraction is important for understanding ordering of events from a narrative text. Recent years have seen annotated corpora created for temporal information extraction, from newspaper text (Pustejovsky et al., 2003; Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), to clinical narratives (Savova et al., 2009; Sun et al., 2013; Styler et al., 2014), all with the aim of developing systems for building event timelines from textual descriptions of events. Such narrative timelines are important for information extraction tasks such as question answering (Kahn et al., 1990), clinical outcomes prediction (Schmidt et al., 2005; Lin et al., 2014), and the identification of temporal patterns (Zhou and Hripcsak, 2007) among many. In a typical supervised approach to the temporal relation extraction task, argument pairs"
W16-2914,S10-1010,0,\N,Missing
W17-2320,W06-1615,0,0.545828,"is the task of using labeled data from one domain (the source domain) to train a classifier that will be applied to a new domain (the target domain). When there is some labeled data available in the target domain, this is referred to as supervised domain adaptation, and when there is no labeled data in the target domain, the task is called unsupervised domain adaptation (UDA). As the unsupervised version of the problem more closely aligns to real-world clinical use cases, we focus on that setting. One common UDA method in natural language processing is structural correspondence learning (SCL; Blitzer et al. (2006)). SCL hypothesizes that some features act consistently across domains (socalled pivot features) while others are still informative but are domain-dependent. The SCL method 165 Proceedings of the BioNLP 2017 workshop, pages 165–170, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics combines source and target extracted feature sets, and trains classifiers to predict the value of pivot features, uses singular value decomposition to reduce the dimensionality of the pivot feature space, and uses this reduced dimensionality space as an additional set of features. T"
W17-2320,W07-1011,0,0.0866677,"Missing"
W17-2320,P07-1034,0,0.0748422,"ied to negation detection (or any other biomedical NLP tasks). One difficulty of SCL is in selecting the pivot features, for which most existing approaches use heuristics about what features are likely to be domain independent. Another approach to UDA, known as bootstrapping or self-training, uses a classifier trained in the source domain to label target instances, and adds confidently predicted target instances to the training data with the predicted label. This method has been successfully applied to POS tagging, spam email classification, named entity classification, and syntactic parsing (Jiang and Zhai, 2007; McClosky et al., 2006). Clinical negation detection has a long history because of its importance to clinical information extraction. Rule-based systems such as Negex (Chapman et al., 2001) and its successor, ConText (Harkema et al., 2009) contain manually curated lists of negation cue words and apply rules about their scopes based on word distance and intervening cues. While these methods do not learn, the word distance parameter can be tuned by experts to apply to their own datasets. The DepNeg system (Sohn et al., 2012) used manually curated dependency path features in a rule-based system"
W17-2320,N06-1020,0,0.0189948,"ion (or any other biomedical NLP tasks). One difficulty of SCL is in selecting the pivot features, for which most existing approaches use heuristics about what features are likely to be domain independent. Another approach to UDA, known as bootstrapping or self-training, uses a classifier trained in the source domain to label target instances, and adds confidently predicted target instances to the training data with the predicted label. This method has been successfully applied to POS tagging, spam email classification, named entity classification, and syntactic parsing (Jiang and Zhai, 2007; McClosky et al., 2006). Clinical negation detection has a long history because of its importance to clinical information extraction. Rule-based systems such as Negex (Chapman et al., 2001) and its successor, ConText (Harkema et al., 2009) contain manually curated lists of negation cue words and apply rules about their scopes based on word distance and intervening cues. While these methods do not learn, the word distance parameter can be tuned by experts to apply to their own datasets. The DepNeg system (Sohn et al., 2012) used manually curated dependency path features in a rule-based system to abstract away from su"
W17-2320,P16-1210,1,0.8902,"Missing"
W17-2320,N15-1010,1,0.836611,"ent. The SCL method 165 Proceedings of the BioNLP 2017 workshop, pages 165–170, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics combines source and target extracted feature sets, and trains classifiers to predict the value of pivot features, uses singular value decomposition to reduce the dimensionality of the pivot feature space, and uses this reduced dimensionality space as an additional set of features. This method has been successful for part of speech tagging (Blitzer et al., 2006), sentiment analysis (Blitzer et al., 2007), and authorship attribution (Sapkota et al., 2015), among others, but to our knowledge has not been applied to negation detection (or any other biomedical NLP tasks). One difficulty of SCL is in selecting the pivot features, for which most existing approaches use heuristics about what features are likely to be domain independent. Another approach to UDA, known as bootstrapping or self-training, uses a classifier trained in the source domain to label target instances, and adds confidently predicted target instances to the training data with the predicted label. This method has been successfully applied to POS tagging, spam email classification"
W17-2320,P15-2028,0,0.0156238,"rization with 10x penalty, SCL A+N is structural correspondence learning with all features in addition to projected (new) features, SCL P+N is SCL with pivot features and projected features, BS-All=Bootstrapping with instances of all classes added to source, BS-Minority=Bootstrapping with only instances of minority class added to source, ISF=Instance similarity features. label. We therefore experiment with only adding minority class instances, enriching the training data to have a more even class distribution. The final UDA algorithm we experiment with uses instance similarity features (ISF) (Yu and Jiang, 2015). This method extends the feature space in the source domain with a set of similarity features computed by comparison to features extracted from target domain instances. Formally, the method selects a random subset of K exemplar instances from Dt and normalizes them as ~ˆe = ||~~ee ||. Similarity feature k for instance i in the source data set is computed as the dot product Xt [i] · ~ˆe[k]. Following Yu and Jiang, we set K = 50 and concatenate the similarity features to the full set of extracted features for each source instance at training. These exemplar instances must be kept around past tr"
W17-2320,P07-1056,0,\N,Missing
W17-2341,D13-1078,1,0.809433,"an as many as 10 tokens. CNNs, which represent meaning through fragments of word sequences, might struggle to compose these fragments to represent the meaning of time expressions. For example, can a CNN properly generalize that May 7 as a date is closer to April 30 than May 20? Can it embed years like 2012 and 2040 to recognize that the former was in the past, while the latter is in the future? Time normalization systems can handle such phenomena, but they are complex and language-specific, and often require significant manual effort to re-engineer for a new domain (Str¨otgen and Gertz, 2013; Bethard, 2013). Fortunately, not all tasks require full time normalization, so if the CNN can at least embed a meaningful subset of the time expression semantics, it may still be helpful in such tasks. An open question then, is how to best feed time expressions to the CNN so that it can usefully generalize over them as part of its solution to a larger task. We propose representing time expressions as single pseudo-tokens, with single vector representations (as in Figure 1), that encode easily extractable information about the time expression that is valuable for the task of temporal relation extraction. The"
W17-2341,E17-2118,1,0.828981,"traction. 2 Methods We trained two CNN-based classifiers for recognizing two types of within-sentence temporal relations, event-event and event-time relations, as they usually call for different temporal cues (Lin et al., 2016a). The input to our classifiers was manually annotated (gold) events and time expressions during both training and testing stages. That way we isolated the task of time expression representation for temporal relation extraction from the tasks of event and time expression recognition. We adopted the same xml-tag marked-up token sequence representation and model setup as (Dligach et al., 2017). Figure 2(1) illustrates the marked-up token sequence for an event-time instance, in which the event is marked by hei and h/ei and the time expression is marked by hti and h/ti. Event-event instances are handled similarly, e.g. a he1i surgery h/e1i is he2i scheduled h/e2i on march 11. We tried different ways of representing a time expression as a one-token tag. The most coarse option would be to represent all time expressions with one universal tag, htimexi, as in Figure 2(2). For more granular options, we experimented with these additional representations: 1) The time class1 of a time expres"
W17-2341,P14-1062,0,0.00458677,"not be an ideal representation for time expressions, which are long, highly varied, and semantically complex. We describe a method for representing time expressions with single pseudotokens for CNNs. With this method, we establish a new state-of-the-art result for a clinical temporal relation extraction task. 1 Introduction Convolutional Neural Networks (CNNs) utilize convolving filters and pooling layers for exploring and subsampling a feature space, and show excellent results in tasks such as semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), sentence modeling (Kalchbrenner et al., 2014), and many other natural language processing (NLP) tasks (Collobert et al., 2011). Token sequences are often used as the input for a CNN model in NLP. Each token is represented as a vector. Such vectors could be either word embeddings trained on the fly (Kalchbrenner et al., 2014), pre-trained on a corpus (Pennington et al., 2014; Mikolov et al., 2013), or one-hot vectors that index the token into a vocabulary (Johnson and Zhang, 2014). CNN filters then act as n-grams over continuous representations. Subsequent network layers learn to combine these n-gram filters to detect patterns in the inpu"
W17-2341,W16-2914,1,0.909188,"ring (Das and Musen, 1995; Kahn et al., 1990), clinical outcomes prediction (Schmidt et al., 2005), and the recognition of temporal patterns and timelines (Zhou and Hripcsak, 2007; Lin et al., 2014). Through experiments, we not only demonstrate the usefulness of one-tag representations for time expressions, but also establish a new state-of-theart result for clinical temporal relation extraction. 2 Methods We trained two CNN-based classifiers for recognizing two types of within-sentence temporal relations, event-event and event-time relations, as they usually call for different temporal cues (Lin et al., 2016a). The input to our classifiers was manually annotated (gold) events and time expressions during both training and testing stages. That way we isolated the task of time expression representation for temporal relation extraction from the tasks of event and time expression recognition. We adopted the same xml-tag marked-up token sequence representation and model setup as (Dligach et al., 2017). Figure 2(1) illustrates the marked-up token sequence for an event-time instance, in which the event is marked by hei and h/ei and the time expression is marked by hti and h/ti. Event-event instances are"
W17-2341,W15-3809,1,0.831139,"scheduled on hti mar 11 h/ti . 2: a hei surgery h/ei is scheduled on hti htimexi h/ti . 3: a hei surgery h/ei is scheduled on hti hdatei h/ti . 4: a hei surgery h/ei is scheduled on hti hnn cdi h/ti . 5: a hei surgery h/ei is scheduled on hti hdate nn cdi h/ti . 6: a hei surgery h/ei is scheduled on hti hindex 721i h/ti . 7: a hei surgery h/ei is scheduled on hti mar 11 hdatei h/ti . 8: hoi hoi hoi hoi hoi hbi hii hoi hoi 9: hoi hoi hoi hoi hoi hb datei hi datei hoi hoi 10: a he1i surgery h/e1i is he2i scheduled h/e2i on . Figure 2: Representations of an input sequence head and high accuracy (Miller et al., 2015). 2) CNN filters are more effective because they operate over the time expression as one unit. The filter process can thus focus on the informative surrounding context to catch generalizable patterns instead of being trapped within lengthy time expressions. We explored a variety of one-tag representations for time expressions, from very specific to very general. We also experimented with other ways to inject temporal information into the CNN models and compared them with our one-tag representations. We picked a challenging learning task where time expressions are critical cues for evaluating o"
W17-2341,D14-1162,0,0.124571,"ional Neural Networks (CNNs) utilize convolving filters and pooling layers for exploring and subsampling a feature space, and show excellent results in tasks such as semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), sentence modeling (Kalchbrenner et al., 2014), and many other natural language processing (NLP) tasks (Collobert et al., 2011). Token sequences are often used as the input for a CNN model in NLP. Each token is represented as a vector. Such vectors could be either word embeddings trained on the fly (Kalchbrenner et al., 2014), pre-trained on a corpus (Pennington et al., 2014; Mikolov et al., 2013), or one-hot vectors that index the token into a vocabulary (Johnson and Zhang, 2014). CNN filters then act as n-grams over continuous representations. Subsequent network layers learn to combine these n-gram filters to detect patterns in the input sequence. This token vector sequence representation has worked for many NLP tasks, but has not been wellstudied for temporal relation extraction. Time expressions are complex linguistic expressions that are challenging to represent because of their length and variety. For example, for the time expressions in the THYME (Styler I"
W17-2341,Q14-1012,1,0.302362,"Missing"
W17-2341,P14-2105,0,0.0108944,"tional Neural Networks (CNNs) in natural language processing. However, they might not be an ideal representation for time expressions, which are long, highly varied, and semantically complex. We describe a method for representing time expressions with single pseudotokens for CNNs. With this method, we establish a new state-of-the-art result for a clinical temporal relation extraction task. 1 Introduction Convolutional Neural Networks (CNNs) utilize convolving filters and pooling layers for exploring and subsampling a feature space, and show excellent results in tasks such as semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), sentence modeling (Kalchbrenner et al., 2014), and many other natural language processing (NLP) tasks (Collobert et al., 2011). Token sequences are often used as the input for a CNN model in NLP. Each token is represented as a vector. Such vectors could be either word embeddings trained on the fly (Kalchbrenner et al., 2014), pre-trained on a corpus (Pennington et al., 2014; Mikolov et al., 2013), or one-hot vectors that index the token into a vocabulary (Johnson and Zhang, 2014). CNN filters then act as n-grams over continuous representations. Sub"
W17-2341,C16-1223,0,0.0297418,"nt relations have lower inter-annotator agreement and usually leverage more of the syntactic information and event properties (Xu et al., 2013), which are not perfectly captured by token sequences. The class imbalance issues are more severe for event-event relations than for event-time relations as well (Dligach et al., 2017). These likely lead to a lower performance for event-event CNNs. In the future, we will investigate methods to improve the event-event model including incorporating syntactic information and event properties into a deep neural framework, and positive instance augmentation Yu and Jiang (2016). Word embeddings trained by conventional methods such as word2vec and GloVe did not prove to be useful in our preliminary experiments. This is likely due to (1) lack of sufficiently large publicly available domain-specific corpora, and (2) inability of the conventional methods to capture the semantic properties of events that are key for the relation extraction task (such as event durations). Currently, when we combined our encoded CNN-based event-time model with the THYME event-event model, we achieved the state-of-theart performance (0.621F) on the colon cancer data. The best 2016 Clinical"
W17-2341,S16-1165,1,\N,Missing
W18-5619,E17-2118,1,0.624941,"multiple external data sources to train word embeddings that form the input layer of the model. Since our 165 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 165–176 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics oped for this task. Neural networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framework task is in the clinical setting, we use available clinical data sources, but also experiment with general domain sources trained on much larger datasets. Besides showing that neural network approaches to i"
W18-5619,W06-0204,0,0.0465686,"14; Plank and Moschitti, 2013). Semi-supervised learning has been a popular approach for improving coverage and model generalizability for various information extraction tasks by exploring unlabeled data. Besides semi-supervised methods developed for feature-based learners (Le and Kim, 2015; Li and Zhou, 2010), there are such algorithms for deep neural network structures (DNN) (Laine and Aila, 2016; Kingma et al., 2014). Self-training or bootstrapping is a standard and straightforward semi-supervised learning method and widely used (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006; Greenwood and Stevenson, 2006; Rosenfeld and Feldman, 2007; Xu, 2008; Xu et al., 2007, 2010). To our best knowledge, we are the first to use self-training in a deep neural network setting for a clinical relation extraction task. Our motivation lies in two folds: 1) Self-training is computationally efficient as there is no other parallel learning goals such 166 as minimizing the reconstruction errors in Generative Adversarial Networks-based semi-supervised learning. With primitive features, DNN-based selftraining can effectively and efficiently evaluate a large amount of instances; 2) We hypothesize that not all unlabeled"
W18-5619,guthrie-etal-2006-closer,0,0.0342654,"nal unlabeled out-of-domain data (i.e. brain cancer clinical notes). 3.2.1 Clinical Word Embeddings To train word embeddings with good vocabulary coverage and high representational power, we took advantage of the clinical notes from MIMIC-III (Medical Information Mart for Intensive Care) dataset (Johnson et al., 2016). The publicly available MIMIC III contains 879 million words from Beth Israel Deaconess Medical Center’s Intensive Care Unit. We merged MIMIC-III data with the unlabeled colon cancer set above and trained 300dimension embeddings with fastText (Joulin et al., 2016) and skip-gram (Guthrie et al., 2006) models. 3.2.2 Social Media Word Embeddings While unlabeled clinical data provides a domainmatched source for training embeddings, additional data can be freely obtained from social media posts about colon cancer. To explore the benefits of extra coverage of such datasets versus the domain specificity of clinical embeddings, we obtain another set of embeddings using user-generated content about colon cancer from two social media platforms, namely Twitter and Reddit. For this purpose, we first generate a keyword list from two sources: a) the most frequent medical terms in the unlabeled colon ca"
W18-5619,D13-1137,0,0.0278187,"pes used by these networks (i.e., tokens) are computationally more efficient to obtain than the sophisticated features typically used by feature engineering methods. For pre-training, we investigate the use of multiple external data sources to train word embeddings that form the input layer of the model. Since our 165 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 165–176 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics oped for this task. Neural networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framewo"
W18-5619,S15-2136,1,0.871968,"ral networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framework task is in the clinical setting, we use available clinical data sources, but also experiment with general domain sources trained on much larger datasets. Besides showing that neural network approaches to information extraction can outperform featureengineering approaches, we find that self-training works better in the neural network setting than with existing state-of-the-art feature-engineering approaches. Finally, we show that these methods generalize to new clinical domains better than the feature-engineering approaches we compare them"
W18-5619,S17-2093,1,0.910128,"Missing"
W18-5619,W16-2914,1,0.827441,"ing that such performance parity is not obtainable without extensive feature engineering. Unlike other settings that have seen performance gains, information extraction tasks related to text typically have much smaller supervised training sets, and the neural network algorithms presumably do not see enough instances to optimally tune the large parameter space. In this paper, we examine the important information extraction task of temporal relation extraction from clinical text. The state-of-the-art for this task is a machine learner with a heavily-engineered set of features (Sun et al., 2013; Lin et al., 2016a). The identification of temporal relations from the clinical text in the electronic medical records has been drawing growing attention because of its potential to provide accurate fine-grained analyses of many medical phenomena (e.g., disease progression, longitudinal effects of medications), with many clinical applications such as question answering (Das and Musen, 1995; Kahn et al., 1990), clinical outcomes prediction (Schmidt et al., 2005), and recognition of temporal patterns and timelines (Zhou and Hripcsak, 2007; Lin et al., 2014). Obtaining large supervised datasets for clinical tasks"
W18-5619,W17-2341,1,0.609658,"sources to train word embeddings that form the input layer of the model. Since our 165 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 165–176 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics oped for this task. Neural networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framework task is in the clinical setting, we use available clinical data sources, but also experiment with general domain sources trained on much larger datasets. Besides showing that neural network approaches to information extracti"
W18-5619,E17-1108,0,0.372144,"re engineering methods. For pre-training, we investigate the use of multiple external data sources to train word embeddings that form the input layer of the model. Since our 165 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 165–176 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics oped for this task. Neural networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framework task is in the clinical setting, we use available clinical data sources, but also experiment with general domain sources trained on much larger d"
W18-5619,S17-2180,0,0.112986,"sources to train word embeddings that form the input layer of the model. Since our 165 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 165–176 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics oped for this task. Neural networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framework task is in the clinical setting, we use available clinical data sources, but also experiment with general domain sources trained on much larger datasets. Besides showing that neural network approaches to information extracti"
W18-5619,P04-3028,0,0.053647,"available gold standard datasets for temporal information extraction. The results of Clinical TempEval 2017 (Bethard et al., 2017) strongly support this latter point, as the performance of submitted systems drops severely when trained on gold instances in one domain and tested on a new domain. We are thus inspired to make use of unlabeled data in addition to gold standard data with a simple semisupervised learning method–self-training and combine it with varieties of pre-trained word embeddings to overcome gaps in training data coverage. In self-training (Yarowsky, 1995; Riloff et al., 2003; Maeireizo et al., 2004), a classifier is first trained on existing labeled data, and then applied to unlabeled data (typically a much larger amount). The predicted instances above a confidence threshold are added to the training set and the classifier is re-trained. Self-training is especially attractive in a neural network setting because the primitive feature types used by these networks (i.e., tokens) are computationally more efficient to obtain than the sophisticated features typically used by feature engineering methods. For pre-training, we investigate the use of multiple external data sources to train word em"
W18-5619,P06-1095,0,0.0208379,"esentations for all three relational candidates, in which the event in an event-time relation pair is marked by hei and h/ei and the time expression is marked by hti and h/ti. The time expression is further encoded by its time class, hti hdatei h/ti, which is a gold standard attribute of a time expression annotation (Styler IV et al., 2014). Event-event instances are marked with additional indexes 1 and 2, e.g. a he1i surgery h/e1i is he2i scheduled h/e2i on march 11. We also follow previous best practice in applying transitive closure to existing gold CONTAINS relations on the training data (Mani et al., 2006; Lin et al., 2016a). Depending on the order of the relational arguments, there are three types of gold standard relational labels, CONTAINS, CONTAINED3 http://ctakes.apache.org E VENT 2 T IME A surgery was scheduled on March 11, 2014 ⇓ Candidate 1: a hei surgery h/ei was scheduled on hti hdatei h/ti; Candidate 2: a surgery was hei scheduled h/ei on hti hdatei h/ti; Candidate 3: a he1i surgery h/e1i was he2i scheduled h/e2i on march Figure 2: Representations of event-event and eventtime relational candidates in a sentence BY, and NONE. 4.3 Bidirectional RNN Classifier We use a bi-directional r"
W18-5619,W15-3809,1,0.841909,"E VENT 1 tion, since we use the official Clinical TempEval 2017 scoring tool, our models are penalized for the missed cross-sentence relations. 4.1 Preprocessing We process the labeled and unlabeled clinical data through the sentence detection and tokenization modules of Apache cTAKES3 . For the labeled clinical data, we use gold standard event and time expression annotations and their time classes (Styler IV et al., 2014) for both model development and final validation. For the unlabeled clinical text data, we use the cTAKES event annotator (Lin et al., 2016a) and time expression annotator (Miller et al., 2015) to automatically annotate event and time expressions along with their time classes (e.g., TIME, DATE, SET). Both labeled and unlabeled corpora are transformed to lower case as shown in Figure 2. 4.2 Instance Representation We first create a dataset of within-sentence CONTAINS-relation candidates from the colon cancer text of the labeled clinical data. Given all gold standard events and time expressions within a sentence, we link every pair of events, and every event to a time expression (if present) to form CONTAINS candidates. To mark the position of the relational arguments in a candidate p"
W18-5619,P14-1076,0,0.0203719,"it was trained on. The source domain of Clinical TempEval 2017 is colon cancer clinical text while the target domain is brain cancer clinical text. Few domain adaptation techniques are applied by the participants: 1) modeling unknown words to accommodate unseen vocabulary in the new domain; 2) using pretrained domain-independent word embeddings; 3) for supervised domain adaptation, assigning higher weights to samples from the new domain during model training. The performance on the domain adaptation task plummetted. Other domain adaptation methods used in general relation extraction include (Nguyen et al., 2014; Nguyen and Grishman, 2014; Plank and Moschitti, 2013). Semi-supervised learning has been a popular approach for improving coverage and model generalizability for various information extraction tasks by exploring unlabeled data. Besides semi-supervised methods developed for feature-based learners (Le and Kim, 2015; Li and Zhou, 2010), there are such algorithms for deep neural network structures (DNN) (Laine and Aila, 2016; Kingma et al., 2014). Self-training or bootstrapping is a standard and straightforward semi-supervised learning method and widely used (Agichtein and Gravano, 2000; Pantel"
W18-5619,P14-2012,0,0.0200696,"he source domain of Clinical TempEval 2017 is colon cancer clinical text while the target domain is brain cancer clinical text. Few domain adaptation techniques are applied by the participants: 1) modeling unknown words to accommodate unseen vocabulary in the new domain; 2) using pretrained domain-independent word embeddings; 3) for supervised domain adaptation, assigning higher weights to samples from the new domain during model training. The performance on the domain adaptation task plummetted. Other domain adaptation methods used in general relation extraction include (Nguyen et al., 2014; Nguyen and Grishman, 2014; Plank and Moschitti, 2013). Semi-supervised learning has been a popular approach for improving coverage and model generalizability for various information extraction tasks by exploring unlabeled data. Besides semi-supervised methods developed for feature-based learners (Le and Kim, 2015; Li and Zhou, 2010), there are such algorithms for deep neural network structures (DNN) (Laine and Aila, 2016; Kingma et al., 2014). Self-training or bootstrapping is a standard and straightforward semi-supervised learning method and widely used (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006; Gr"
W18-5619,P06-1015,0,0.0372333,"., 2014; Nguyen and Grishman, 2014; Plank and Moschitti, 2013). Semi-supervised learning has been a popular approach for improving coverage and model generalizability for various information extraction tasks by exploring unlabeled data. Besides semi-supervised methods developed for feature-based learners (Le and Kim, 2015; Li and Zhou, 2010), there are such algorithms for deep neural network structures (DNN) (Laine and Aila, 2016; Kingma et al., 2014). Self-training or bootstrapping is a standard and straightforward semi-supervised learning method and widely used (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006; Greenwood and Stevenson, 2006; Rosenfeld and Feldman, 2007; Xu, 2008; Xu et al., 2007, 2010). To our best knowledge, we are the first to use self-training in a deep neural network setting for a clinical relation extraction task. Our motivation lies in two folds: 1) Self-training is computationally efficient as there is no other parallel learning goals such 166 as minimizing the reconstruction errors in Generative Adversarial Networks-based semi-supervised learning. With primitive features, DNN-based selftraining can effectively and efficiently evaluate a large amount of instances; 2) We hypo"
W18-5619,P13-1147,0,0.0212914,"l TempEval 2017 is colon cancer clinical text while the target domain is brain cancer clinical text. Few domain adaptation techniques are applied by the participants: 1) modeling unknown words to accommodate unseen vocabulary in the new domain; 2) using pretrained domain-independent word embeddings; 3) for supervised domain adaptation, assigning higher weights to samples from the new domain during model training. The performance on the domain adaptation task plummetted. Other domain adaptation methods used in general relation extraction include (Nguyen et al., 2014; Nguyen and Grishman, 2014; Plank and Moschitti, 2013). Semi-supervised learning has been a popular approach for improving coverage and model generalizability for various information extraction tasks by exploring unlabeled data. Besides semi-supervised methods developed for feature-based learners (Le and Kim, 2015; Li and Zhou, 2010), there are such algorithms for deep neural network structures (DNN) (Laine and Aila, 2016; Kingma et al., 2014). Self-training or bootstrapping is a standard and straightforward semi-supervised learning method and widely used (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006; Greenwood and Stevenson, 2006;"
W18-5619,W11-0419,0,0.325967,"o, obtaining state-of-the-art performance in an unsupervised domain adaptation setting. 2 Related Work In recent years, several shared tasks on temporal relation extraction from clinical text have been organized. Among them, the i2b2 temporal challenge evaluates the i2b2 corpus (Sun et al., 2013), and Clinical TempEval series (Bethard et al., 2015, 2016, 2017) evaluate systems using the THYME corpus (Styler IV et al., 2014), which is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) per an extension of the TimeML specifications (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011). Challenge participants develop methods to extract EVENT and TIMEX3 entities, CONTAINS relations and document creation time relations. Herein, we focus on CONTAINS relation, which signals an EVENT occurs entirely within the temporal bounds of an narrative container. The narrative container is either another EVENT or TIMEX3. Conventional learning methods, such as support vector machines (SVM) and conditional random fields (CRF) (Sun et al., 2013), have been develClinical TempEval 2017 introduces the task of domain adaptation, as the most frequent use case would be the application of a model on"
W18-5619,W03-0404,0,0.14125,"Missing"
W18-5619,P07-1076,0,0.0197391,"Semi-supervised learning has been a popular approach for improving coverage and model generalizability for various information extraction tasks by exploring unlabeled data. Besides semi-supervised methods developed for feature-based learners (Le and Kim, 2015; Li and Zhou, 2010), there are such algorithms for deep neural network structures (DNN) (Laine and Aila, 2016; Kingma et al., 2014). Self-training or bootstrapping is a standard and straightforward semi-supervised learning method and widely used (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006; Greenwood and Stevenson, 2006; Rosenfeld and Feldman, 2007; Xu, 2008; Xu et al., 2007, 2010). To our best knowledge, we are the first to use self-training in a deep neural network setting for a clinical relation extraction task. Our motivation lies in two folds: 1) Self-training is computationally efficient as there is no other parallel learning goals such 166 as minimizing the reconstruction errors in Generative Adversarial Networks-based semi-supervised learning. With primitive features, DNN-based selftraining can effectively and efficiently evaluate a large amount of instances; 2) We hypothesize that not all unlabeled data are useful. Our goal is"
W18-5619,D12-1110,0,0.0274721,"ks (i.e., tokens) are computationally more efficient to obtain than the sophisticated features typically used by feature engineering methods. For pre-training, we investigate the use of multiple external data sources to train word embeddings that form the input layer of the model. Since our 165 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 165–176 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics oped for this task. Neural networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framework task is in the clin"
W18-5619,P17-2035,0,0.624158,"the model. Since our 165 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 165–176 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics oped for this task. Neural networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framework task is in the clinical setting, we use available clinical data sources, but also experiment with general domain sources trained on much larger datasets. Besides showing that neural network approaches to information extraction can outperform featureengineering approaches, we find that self"
W18-5619,P11-2061,0,0.0343755,"YME corpus and silver instances predicted from the unlabeled colon cancer data. Models were tested on the gold colon cancer and gold brain cancer development sets of the THYME corpus, comparing in-domain and cross-domain performance to select the best models for testing. The best models were tested on the gold colon cancer and brain cancer test sets (Clinical TempEval 2017 test sets). All models were evaluated with the metrics precision (P), recall (R) and F1-score (F), using the standard Clinical TempEval evaluation script, where the P and R definitions are enhanced through temporal closure (UzZaman and Allen, 2011; UzZaman et al., 2012): when calculating precision, we run temporal closure on the gold relations but not on the system-generated ones; when calculating recall, we run temporal closure on the systemgenerated relations but not on the gold ones. 169 6 Results Table 3 shows performance of the THYME system and various bi-directional RNN methods on the colon cancer and brain cancer development sets. For RNNs, we evaluated both LSTM and GRU models. For embedding combinations, we tested using the clinical embedding alone (C), using both clinical and cancer-related social media embeddings (CS), using"
W18-5619,C10-2155,0,0.0593427,"Missing"
W18-5619,P07-1074,0,0.0303758,"pular approach for improving coverage and model generalizability for various information extraction tasks by exploring unlabeled data. Besides semi-supervised methods developed for feature-based learners (Le and Kim, 2015; Li and Zhou, 2010), there are such algorithms for deep neural network structures (DNN) (Laine and Aila, 2016; Kingma et al., 2014). Self-training or bootstrapping is a standard and straightforward semi-supervised learning method and widely used (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006; Greenwood and Stevenson, 2006; Rosenfeld and Feldman, 2007; Xu, 2008; Xu et al., 2007, 2010). To our best knowledge, we are the first to use self-training in a deep neural network setting for a clinical relation extraction task. Our motivation lies in two folds: 1) Self-training is computationally efficient as there is no other parallel learning goals such 166 as minimizing the reconstruction errors in Generative Adversarial Networks-based semi-supervised learning. With primitive features, DNN-based selftraining can effectively and efficiently evaluate a large amount of instances; 2) We hypothesize that not all unlabeled data are useful. Our goal is to use a straightforward me"
W18-5619,P95-1026,0,0.678514,"much broader than what is covered by available gold standard datasets for temporal information extraction. The results of Clinical TempEval 2017 (Bethard et al., 2017) strongly support this latter point, as the performance of submitted systems drops severely when trained on gold instances in one domain and tested on a new domain. We are thus inspired to make use of unlabeled data in addition to gold standard data with a simple semisupervised learning method–self-training and combine it with varieties of pre-trained word embeddings to overcome gaps in training data coverage. In self-training (Yarowsky, 1995; Riloff et al., 2003; Maeireizo et al., 2004), a classifier is first trained on existing labeled data, and then applied to unlabeled data (typically a much larger amount). The predicted instances above a confidence threshold are added to the training set and the classifier is re-trained. Self-training is especially attractive in a neural network setting because the primitive feature types used by these networks (i.e., tokens) are computationally more efficient to obtain than the sophisticated features typically used by feature engineering methods. For pre-training, we investigate the use of m"
W19-1903,E17-2118,1,0.724348,"eason entities directly, we first train DrugADE and Drug-Reason relation classifiers, where the candidates for ADE and Reason arguments are all signs/symptoms and disease/disorders detected by cTAKES. If the relation classifier classifies a candidate pair as a Drug-ADE relation, we not only create the Drug-ADE relation but we create an ADE entity out of the non-Drug argument (and the Reason entity detector works the same way). For relation extraction with the Flair neural model, we use a representation based on previous work on extracting temporal narrative container relations from sentences (Dligach et al., 2017). For each relation candidate consisting of a (Drug, Attribute) tuple, we insert xml-like start and stop tokens into the sentence around each of the candidate arguments indicating their position. For example, the sentence: He does feel episodes of hypoglycemia if he does not eat following insulin becomes: He does feel episodes of <ADE> hypoglycemia </ADE> if he does not eat following <Drug> insulin </Drug>. This augmented sentence representation is then passed into the pre-trained Flair bi-directional LSTM sequence model, and the final states in each direction are concatenated into a feature v"
W19-1903,C18-1139,0,0.0303899,"to classify the relation. For Track 3 (end-to-end relation extraction), the During development, we manually partitioned the data so that we could empirically optimize the value of C in the linear SVM classifier on held out data. We tuned a single value of C that optimized the micro-F score on the held-out part of the training data. It may be possible to squeeze out slightly better performance by tuning C separately for each classifier, but the classifiers were pretty stable in the range we experimented with. We compare this system to an off-the-shelf neural network-based system called Flair (Akbik et al., 2018). This system is pre-trained using one billion words of text (Chelba et al., 2013) to learn a multi-layer Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) network language model. Given the pre-trained network, this system passes in the tokens for an input sequence, and receives back the values at the deepest hidden layer at each index of the multilayer LSTM, and this sequence of vectors is called contextual embeddings. Like regular word embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014), there is one vector per input token, but since they are extrac"
W19-1903,D14-1162,0,0.0864382,"system to an off-the-shelf neural network-based system called Flair (Akbik et al., 2018). This system is pre-trained using one billion words of text (Chelba et al., 2013) to learn a multi-layer Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) network language model. Given the pre-trained network, this system passes in the tokens for an input sequence, and receives back the values at the deepest hidden layer at each index of the multilayer LSTM, and this sequence of vectors is called contextual embeddings. Like regular word embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014), there is one vector per input token, but since they are extracted from the output layer of the pre-trained LSTM they are expected to contain more information about the surrounding sentence context. To train an entity extractor in Flair, we again model the task as a BIO tagging task, but instead of using linguistic features we simply pass the contextual embeddings for each token to a standard LSTM tagger. This LSTM has a hidden state with 256 dimensions, and is optimized with Adam (Kingma and Ba, 2014). We train for 50 epochs, and the model that performs best on the held out validation set du"
W19-1903,P10-1040,0,0.109977,"Missing"
W19-1908,E17-1108,0,0.0592049,"d for specific domains (Lee et al., 2019) or serve as a backbone model to be fine-tuned with one output layer for a wide range of tasks. Figure 1: Representations of three candidate relations produced from an example token sequence. sentence-boundary detection errors. The input sequences of arbitrary lengths that BERT operates on cover both within-sentence and cross-sentence situations, enabling us to design a universal model that is sentence boundary agnostic. For the task of clinical temporal relation extraction, recent years have seen the rise of neural approaches – structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018) – where minimally-engineered inputs have been adopted over heavily featureengineered techniques (Sun et al., 2013). The THYME corpus (Styler IV et al., 2014), which is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011), is a popular choice for evaluation and was used in the Clinical Temp"
W19-1908,C18-1139,0,0.0215405,"es of BERT motivate us to apply it to a traditionally sentence-level task – temporal relation extraction from clinical text. The identification of temporal relations in the clinical narrative can lead to accurate fine-grained analyses of many medical phenomena (e.g., disease progression, longitudinal effects of medications), with a variety of clinical applications such as question answering (Das and Musen, 1995; Kahn et al., 2 Background Recently, several pre-trained general-purposed language encoders have been proposed, including CoVe (McCann et al., 2017), ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), GPT (Radford et al., 2018), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2018). These models are trained on vast amounts of unlabeled text to achieve 65 Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages 65–71 c Minneapolis, Minnesota, June 7, 2019. 2019 Association for Computational Linguistics E VENT 1 generalizable contextualized word embeddings, and some can be fine-tuned to fit a supervised task. E VENT 2 T IME . A surgery was scheduled on March 11, 2014. ⇓ #1: . a es surgery ee was scheduled on ts date te . #2: . a surgery was es scheduled ee on ts dat"
W19-1908,S15-2136,1,0.956288,"thard@email.arizona.edu Abstract 1990), clinical outcomes prediction (Schmidt et al., 2005), and recognition of temporal patterns and timelines (Zhou and Hripcsak, 2007; Lin et al., 2014). However, the labeled instances for this clinical information extraction task are limited, so neural models trained from scratch may not be able to learn complex linguistic phenomena. Pre-trained models like BERT could potentially provide rich representations as they are trained on massive data. Classic models for clinical temporal relation extraction have framed the task within a sentence (Sun et al., 2013; Bethard et al., 2015, 2016, 2017), making them susceptible to sentence detection errors. Using BERT, on the other hand, eliminates this sensitivity to sentence boundary errors. The key contributions of this paper are: (1) introducing BERT to the challenging task of clinical temporal relation extraction and evaluating its performance on a widely used testbed (THYME corpus; Styler IV et al., 2014), (2) developing a universal processing mechanism based on a fixed, sentenceboundary agnostic window of contiguous tokens, (3) pre-training BERT on MIMIC-III (Medical Information Mart for Intensive Care) dataset (Johnson e"
W19-1908,W18-5619,1,0.837014,"ed from an example token sequence. sentence-boundary detection errors. The input sequences of arbitrary lengths that BERT operates on cover both within-sentence and cross-sentence situations, enabling us to design a universal model that is sentence boundary agnostic. For the task of clinical temporal relation extraction, recent years have seen the rise of neural approaches – structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018) – where minimally-engineered inputs have been adopted over heavily featureengineered techniques (Sun et al., 2013). The THYME corpus (Styler IV et al., 2014), which is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011), is a popular choice for evaluation and was used in the Clinical TempEval series (Bethard et al., 2015, 2016, 2017). 3 3.1 Methods Task definition We process the THYME corpus using the segmentation and tokenization modules of Apache cTAKES (http://ctakes.a"
W19-1908,S17-2093,1,0.901456,"Missing"
W19-1908,W17-2341,1,0.929808,"th one output layer for a wide range of tasks. Figure 1: Representations of three candidate relations produced from an example token sequence. sentence-boundary detection errors. The input sequences of arbitrary lengths that BERT operates on cover both within-sentence and cross-sentence situations, enabling us to design a universal model that is sentence boundary agnostic. For the task of clinical temporal relation extraction, recent years have seen the rise of neural approaches – structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018) – where minimally-engineered inputs have been adopted over heavily featureengineered techniques (Sun et al., 2013). The THYME corpus (Styler IV et al., 2014), which is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011), is a popular choice for evaluation and was used in the Clinical TempEval series (Bethard et al., 2015, 2016, 2017). 3 3.1 Methods Task definition W"
W19-1908,N18-1202,0,0.0331039,"ple sentences. These advantages of BERT motivate us to apply it to a traditionally sentence-level task – temporal relation extraction from clinical text. The identification of temporal relations in the clinical narrative can lead to accurate fine-grained analyses of many medical phenomena (e.g., disease progression, longitudinal effects of medications), with a variety of clinical applications such as question answering (Das and Musen, 1995; Kahn et al., 2 Background Recently, several pre-trained general-purposed language encoders have been proposed, including CoVe (McCann et al., 2017), ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), GPT (Radford et al., 2018), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2018). These models are trained on vast amounts of unlabeled text to achieve 65 Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages 65–71 c Minneapolis, Minnesota, June 7, 2019. 2019 Association for Computational Linguistics E VENT 1 generalizable contextualized word embeddings, and some can be fine-tuned to fit a supervised task. E VENT 2 T IME . A surgery was scheduled on March 11, 2014. ⇓ #1: . a es surgery ee was scheduled on ts date te . #2: . a surgery w"
W19-1908,E17-2118,1,0.936835,"el to be fine-tuned with one output layer for a wide range of tasks. Figure 1: Representations of three candidate relations produced from an example token sequence. sentence-boundary detection errors. The input sequences of arbitrary lengths that BERT operates on cover both within-sentence and cross-sentence situations, enabling us to design a universal model that is sentence boundary agnostic. For the task of clinical temporal relation extraction, recent years have seen the rise of neural approaches – structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018) – where minimally-engineered inputs have been adopted over heavily featureengineered techniques (Sun et al., 2013). The THYME corpus (Styler IV et al., 2014), which is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011), is a popular choice for evaluation and was used in the Clinical TempEval series (Bethard et al., 2015, 2016, 2017). 3 3.1 Method"
W19-1908,W18-5607,0,0.317533,"tandard entities, of which two are events, “surgery” and “scheduled”, and one is a time expression, “March 11, 2014”, whose time class is “date”. One can form three candidate relations for these three entities. CONTAINS relations are by far the most frequent type of relation in the THYME corpus. They signal that an EVENT occurs entirely within the temporal bounds of a narrative container (Pustejovsky and Stubbs, 2011). The THYME corpus is limited in size so models developed on it may suffer from low generalizability. Recent efforts to improve performance have attempted tree-structured models (Galvan et al., 2018) or assistance from unlabeled data (Lin et al., 2018). Years of shared work on this problem and plateauing scores may have suggested that performance on this task is at its peak. However, given the successful application of BERT on many different tasks in the general domain, as well as more recent work in relation extraction tasks (Wang et al., 2019; Lee et al., 2019), we wanted to explore applying this new model to the clinical temporal relation extraction task. 3.2 Window-based processing We aim to build a BERT-based model for both within- and cross-sentence relations. Figure 2 presents the"
W19-1908,W11-0419,0,0.659617,"ears have seen the rise of neural approaches – structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018) – where minimally-engineered inputs have been adopted over heavily featureengineered techniques (Sun et al., 2013). The THYME corpus (Styler IV et al., 2014), which is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011), is a popular choice for evaluation and was used in the Clinical TempEval series (Bethard et al., 2015, 2016, 2017). 3 3.1 Methods Task definition We process the THYME corpus using the segmentation and tokenization modules of Apache cTAKES (http://ctakes.apache.org). We consume gold standard event annotations, gold time expressions and their classes (Styler IV et al., 2014) for generating instances of containment relation candidates. Each instance consists of a pair of event entities, or an event entity and a time expression entity. We preserve the natural order of the two entities in their o"
W19-1908,P17-2035,0,0.150458,"entations of three candidate relations produced from an example token sequence. sentence-boundary detection errors. The input sequences of arbitrary lengths that BERT operates on cover both within-sentence and cross-sentence situations, enabling us to design a universal model that is sentence boundary agnostic. For the task of clinical temporal relation extraction, recent years have seen the rise of neural approaches – structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018) – where minimally-engineered inputs have been adopted over heavily featureengineered techniques (Sun et al., 2013). The THYME corpus (Styler IV et al., 2014), which is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011), is a popular choice for evaluation and was used in the Clinical TempEval series (Bethard et al., 2015, 2016, 2017). 3 3.1 Methods Task definition We process the THYME corpus using the segmentation and tokenization"
W19-1908,P19-1132,0,0.0558117,"Missing"
