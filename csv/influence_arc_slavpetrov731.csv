D07-1072,A00-2018,0,0.055178,"typically a poor model as it makes overly strong independence assumptions. As a result, many generative approaches to parsing construct refinements of the treebank grammar which are more suitable for the modeling task. Lexical methods split each pre-terminal symbol into many subsymbols, one for each word, and then focus on smoothing sparse lexical statis688 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 688–697, Prague, June 2007. 2007 Association for Computational Linguistics tics (Collins, 1999; Charniak, 2000). Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006). We apply our HDP-PCFG-GR model to automatically learn the number of subsymbols for each symbol. 2 Models based on Dirichlet processes In document clustering, for example, each data point xi is a document represented by its termfrequency vector. Each component (cluster) z has multinomial parameters φz which specifies a distribution F (·; φz ) over words. It is"
D07-1072,P07-1035,0,0.551821,"Missing"
D07-1072,P06-1085,0,0.0253881,"treebank parsing (Charniak, 1996; Collins, 1999). An important question when learning PCFGs is how many grammar symbols to allocate to the learning algorithm based on the amount of available data. The question of “how many clusters (symbols)?” has been tackled in the Bayesian nonparametrics literature via Dirichlet process (DP) mixture models (Antoniak, 1974). DP mixture models have since been extended to hierarchical Dirichlet processes (HDPs) and HDP-HMMs (Teh et al., 2006; Beal et al., 2002) and applied to many different types of clustering/induction problems in NLP (Johnson et al., 2006; Goldwater et al., 2006). In this paper, we present the hierarchical Dirichlet process PCFG (HDP-PCFG). a nonparametric As models increase in complexity, so does the uncertainty over parameter estimates. In this regime, point estimates are unreliable since they do not take into account the fact that there are different amounts of uncertainty in the various components of the parameters. The HDP-PCFG is a Bayesian model which naturally handles this uncertainty. We present an efficient variational inference algorithm for the HDP-PCFG based on a structured mean-field approximation of the true posterior over parameters. T"
D07-1072,P03-1054,1,0.0589178,"ore suitable for the modeling task. Lexical methods split each pre-terminal symbol into many subsymbols, one for each word, and then focus on smoothing sparse lexical statis688 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 688–697, Prague, June 2007. 2007 Association for Computational Linguistics tics (Collins, 1999; Charniak, 2000). Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006). We apply our HDP-PCFG-GR model to automatically learn the number of subsymbols for each symbol. 2 Models based on Dirichlet processes In document clustering, for example, each data point xi is a document represented by its termfrequency vector. Each component (cluster) z has multinomial parameters φz which specifies a distribution F (·; φz ) over words. It is customary to use a conjugate Dirichlet prior G0 = Dirichlet(α0 , . . . , α0 ) over the multinomial parameters, which can be interpreted as adding α0 − 1 pseudocounts for each word. 2.2 At th"
D07-1072,P05-1010,0,0.344389,"ling task. Lexical methods split each pre-terminal symbol into many subsymbols, one for each word, and then focus on smoothing sparse lexical statis688 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 688–697, Prague, June 2007. 2007 Association for Computational Linguistics tics (Collins, 1999; Charniak, 2000). Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006). We apply our HDP-PCFG-GR model to automatically learn the number of subsymbols for each symbol. 2 Models based on Dirichlet processes In document clustering, for example, each data point xi is a document represented by its termfrequency vector. Each component (cluster) z has multinomial parameters φz which specifies a distribution F (·; φz ) over words. It is customary to use a conjugate Dirichlet prior G0 = Dirichlet(α0 , . . . , α0 ) over the multinomial parameters, which can be interpreted as adding α0 − 1 pseudocounts for each word. 2.2 At the heart of the HDP-PCFG"
D07-1072,P06-1055,1,0.592342,"ds split each pre-terminal symbol into many subsymbols, one for each word, and then focus on smoothing sparse lexical statis688 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 688–697, Prague, June 2007. 2007 Association for Computational Linguistics tics (Collins, 1999; Charniak, 2000). Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006). We apply our HDP-PCFG-GR model to automatically learn the number of subsymbols for each symbol. 2 Models based on Dirichlet processes In document clustering, for example, each data point xi is a document represented by its termfrequency vector. Each component (cluster) z has multinomial parameters φz which specifies a distribution F (·; φz ) over words. It is customary to use a conjugate Dirichlet prior G0 = Dirichlet(α0 , . . . , α0 ) over the multinomial parameters, which can be interpreted as adding α0 − 1 pseudocounts for each word. 2.2 At the heart of the HDP-PCFG is the Dirichlet proce"
D07-1072,J03-4003,0,\N,Missing
D07-1094,P05-1010,0,0.132895,"Missing"
D07-1094,P06-1055,1,0.930124,"er (Young and Woodland, 1994). Finally, to model complex emission densities, states emit mixtures of multivariate Gaussians. This standard structure is shown schematically in Figure 1. While this rich structure is phonetically well-motivated and empirically successful, so much structural bias may be unnecessary, or even harmful. For example in the domain of syntactic parsing with probabilistic context-free grammars (PCFGs), a surprising recent result is that automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure (Petrov et al., 2006). In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling, analagous to the approach taken by Petrov et al. (2006) for learning PCFGs. We start with a minimal monophone HMM in which there is a single state for each (context-independent) phone. Moreover, the emission model for each state is a single multivariate Gaussian (over the standard MFCC acoustic features). We then iteratively refine this minimal HMM through state splitting, adding complexity as needed. States in the refined HMMs are always substates of the original HMM and a"
D08-1012,J92-4003,0,0.0340262,"gham and Mannila, 2001). Although our best performance does not come from random projections, we still obtain substantial speed-ups over a single pass fine decoder when using random projections in coarse passes. 4.2 Frequency clustering In frequency clustering, we allocate words to clusters by frequency. At each level, the most frequent words go into one cluster and the rarest words go into another one. Concretely, we sort the words in a given cluster by frequency and split the cluster so that the two halves have equal token mass. This approach can be seen as a radically simplified version of Brown et al. (1992). It can, and does, result in highly imbalanced cluster hierarchies. 4.3 HMM clustering An approach found to be effective by Petrov and Klein (2007) for coarse-to-fine parsing is to use likelihood-based hierarchical EM training. We adopt this approach here by identifying each cluster with a latent state in an HMM and determinizing the emissions so that each word type is emitted 90000 70000 60000 29.2 29 BLEU Perplexity 29.4 HMM JCluster Frequency Random 80000 50000 40000 30000 28.2 10000 0 0 28 100 1 2 3 4 5 6 7 8 9 10 Number of bits in coarse language model Figure 5: Results of coarse languag"
D08-1012,N06-1022,0,0.0389392,"s, Zhang and Gildea (2008) consider an approach in which the results of a bigram pass are used as an A* heuristic to guide a trigram pass. In their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. 109 0,1-NP-0,1 LM Order To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model. We show that, on a range of languages, our coarse-tofine decoding approach greatly outperfor"
D08-1012,P05-1033,0,0.0934293,"ation via a backpointer. See Wu (1996) or Melamed (2004) for a detailed exposition. Once we integrate an n-gram language model, the state space becomes lexicalized and combining dynamic programming items becomes more difficult. Each state is now parametrized by the initial and final n−1 words in the target language hypothesis: ln−1 , ..., l1 -i Xj -r1 , ..., rn−1 . Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). Decoding with an integrated language model is computationally expensive for two reasons: (1) the need to keep track of a large number of lexicalized hypotheses for each source span, and (2) the need to frequently query the large language model for each hypothesis combination. Multipass coarse-to-fine decoding can alleviate both computational issues. We start by decoding in an extremely coarse bigram search space, where there are very few possible translations. We compute standard inside/outside probabilities (iS/oS), as follows. Consider the application of non-inverted binary rule: we combin"
D08-1012,W97-0302,0,0.043808,"mework like ours, Zhang and Gildea (2008) consider an approach in which the results of a bigram pass are used as an A* heuristic to guide a trigram pass. In their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. 109 0,1-NP-0,1 LM Order To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model. We show that, on a range of languages, our coarse-tofine decoding app"
D08-1012,N07-1052,1,0.832804,"n their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. 109 0,1-NP-0,1 LM Order To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model. We show that, on a range of languages, our coarse-tofine decoding approach greatly outperforms baseline beam pruning and bigram-to-trigram pruning on time-to-BLEU plots, reducing decoding times by up to a factor of 50 compared to sin"
D08-1012,P07-1019,0,0.0685306,"thogonal axis of abstraction: the size of the target language. The introduction of abstract languages gives better control over the granularity of the search space and provides a richer set of intermediate problems, allowing us to adapt the level of refinement of the intermediate, coarse passes to minimize total computation. Beyond coarse-to-fine approaches, other related approaches have also been demonstrated for syntactic MT. For example, Venugopal et al. (2007) considers a greedy first pass with a full model followed by a second pass which bounds search to a region near the greedy results. Huang and Chiang (2007) searches with the full model, but makes assumptions about the the amount of reordering the language model can trigger in order to limit exploration. 2.2 Language Model Projections When decoding in a syntactic translation model with an n-gram language model, search states are specified by a grammar nonterminal X as well as the the n-1 left-most target side words ln−1 , . . . , l1 and right-most target side words r1 , . . . , rn−1 of the generated hypothesis. We denote the resulting lexicalized state as ln−1 , . . . , l1 -X-r1 , . . . , rn−1 . Assuming a vocabulary V and grammar symbol set G, t"
D08-1012,N03-1016,1,0.831454,"o guide a trigram pass. In their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. 109 0,1-NP-0,1 LM Order To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model. We show that, on a range of languages, our coarse-tofine decoding approach greatly outperforms baseline beam pruning and bigram-to-trigram pruning on time-to-BLEU plots, reducing decoding times by up to a fact"
D08-1012,P07-2045,0,0.0103353,"Missing"
D08-1012,2005.mtsummit-papers.11,0,0.0299132,"eed to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. 109 0,1-NP-0,1 LM Order To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model. We show that, on a range of languages, our coarse-tofine decoding approach greatly outperforms baseline beam pruning and bigram-to-trigram pruning on time-to-BLEU plots, reducing decoding times by up to a factor of 50 compared to single pass decoding. In addition, coarse-to-fine decoding increases BLEU scores by up to 0.4 points. This increase is a mixture of improved search and subtly advantageous coarseto-fine effects which are further discussed below. 01,10-NP-00,10 010,100-NP-000,100 the,report-NP-these,states 3 π ..."
D08-1012,P04-1083,0,0.0161719,"ally, binary inverted productions invert the order of their children (X → hY Zi). These productions are associated with rewrite weights in the 111 standard way. Without a language model, SCFG decoding is just like (monolingual) CFG parsing. The dynamic programming states are specified by i Xj , where hi, ji is a source sentence span and X is a nonterminal. The only difference is that whenever we apply a CFG production on the source side, we need to remember the corresponding synchronous production on the target side and store the best obtainable translation via a backpointer. See Wu (1996) or Melamed (2004) for a detailed exposition. Once we integrate an n-gram language model, the state space becomes lexicalized and combining dynamic programming items becomes more difficult. Each state is now parametrized by the initial and final n−1 words in the target language hypothesis: ln−1 , ..., l1 -i Xj -r1 , ..., rn−1 . Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). Decoding with an integrated language mode"
D08-1012,P03-1021,0,0.00535411,"be attributed primarily to the substantially richer distortion model used by Moses. The multipass coarse-to-fine architecture that we have introduced presents many choice points. In the following, we investigate various axes individually. We present our findings as BLEU-to-time plots, where the tradeoffs were generated by varying the complexity and the number of coarse passes, as well as the pruning thresholds and beam sizes. Unless otherwise noted, the experiments are on SpanishEnglish using trigram language models. When different decoder settings are applied to the same model, MERT weights (Och, 2003) from the unprojected single pass setup are used and are kept constant across runs. In particular, the same MERT weights are used for all coarse passes; note that this slightly disadvantages the multipass runs, which use MERT weights optimized for the single pass decoder. models perform at pruning, we ran our decoder several times, varying only the clustering source. In each case, we used a 2-bit trigram model as a single coarse pass, followed by a fine output pass. Figure 6 shows that we can obtain significant improvements over the single-pass baseline regardless of the clustering. To no grea"
D08-1012,N07-1051,1,0.772129,"08) consider an approach in which the results of a bigram pass are used as an A* heuristic to guide a trigram pass. In their two-pass approach, the coarse bigram pass becomes computationally dominant. Our work differs in two ways. First, we use posterior pruning rather than A* search. Unlike A* search, posterior pruning allows multipass methods. Not only are posterior pruning methods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. 109 0,1-NP-0,1 LM Order To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model. We show that, on a range of languages, our coarse-tofine decoding approach greatly outperforms baseline beam pruning"
D08-1012,P06-1055,1,0.434994,"pproach most relevant to the current work is Zhang and Gildea (2008), which begins with an initial bigram pass and uses the resulting chart to guide Central to coarse-to-fine language projection is the construction of sequences of word clusterings (see Figure 1). The clusterings are deterministic mappings from words to clusters, with the property that each clustering refines the previous one. There are many choice points in this process, including how these clusterings are obtained and how much refinement is optimal for each pass. We demonstrate that likelihood-based hierarchical EM training (Petrov et al., 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods. In addition, we demonstrate that more than two passes are beneficial and show that our computation is equally distributed over all passes. In our experiments, passes with less than 16-cluster language models are most advantageous, and even a single pass with just two word clusters can reduce decoding time greatly. 108 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 108–116, c Honolulu, October 2008. 2008 Association for Comput"
D08-1012,N07-1063,0,0.0344929,"ts used to encode the target language vocabulary is varied along the x-axis. The language model order is varied along the y-axis. Second, we focus on an orthogonal axis of abstraction: the size of the target language. The introduction of abstract languages gives better control over the granularity of the search space and provides a richer set of intermediate problems, allowing us to adapt the level of refinement of the intermediate, coarse passes to minimize total computation. Beyond coarse-to-fine approaches, other related approaches have also been demonstrated for syntactic MT. For example, Venugopal et al. (2007) considers a greedy first pass with a full model followed by a second pass which bounds search to a region near the greedy results. Huang and Chiang (2007) searches with the full model, but makes assumptions about the the amount of reordering the language model can trigger in order to limit exploration. 2.2 Language Model Projections When decoding in a syntactic translation model with an n-gram language model, search states are specified by a grammar nonterminal X as well as the the n-1 left-most target side words ln−1 , . . . , l1 and right-most target side words r1 , . . . , rn−1 of the gene"
D08-1012,P96-1021,0,0.0677399,"→ [Y Z]). Finally, binary inverted productions invert the order of their children (X → hY Zi). These productions are associated with rewrite weights in the 111 standard way. Without a language model, SCFG decoding is just like (monolingual) CFG parsing. The dynamic programming states are specified by i Xj , where hi, ji is a source sentence span and X is a nonterminal. The only difference is that whenever we apply a CFG production on the source side, we need to remember the corresponding synchronous production on the target side and store the best obtainable translation via a backpointer. See Wu (1996) or Melamed (2004) for a detailed exposition. Once we integrate an n-gram language model, the state space becomes lexicalized and combining dynamic programming items becomes more difficult. Each state is now parametrized by the initial and final n−1 words in the target language hypothesis: ln−1 , ..., l1 -i Xj -r1 , ..., rn−1 . Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). Decoding with an integr"
D08-1012,J97-3002,0,0.21094,"ethods simpler (for example, there is no need to have complex multipart bounds), but they can be much more effective. For example, in monolingual parsing, posterior pruning methods (Goodman, 1997; Charniak et al., 2006; Petrov and Klein, 2007) have led to greater speedups than their more cautious A* analogues (Klein and Manning, 2003; Haghighi et al., 2007), though at the cost of guaranteed optimality. 109 0,1-NP-0,1 LM Order To follow related work and to focus on the effects of the language model, we present translation results under an inversion transduction grammar (ITG) translation model (Wu, 1997) trained on the Europarl corpus (Koehn, 2005), described in detail in Section 3, and using a trigram language model. We show that, on a range of languages, our coarse-tofine decoding approach greatly outperforms baseline beam pruning and bigram-to-trigram pruning on time-to-BLEU plots, reducing decoding times by up to a factor of 50 compared to single pass decoding. In addition, coarse-to-fine decoding increases BLEU scores by up to 0.4 points. This increase is a mixture of improved search and subtly advantageous coarseto-fine effects which are further discussed below. 01,10-NP-00,10 010,100-N"
D08-1012,P03-1019,0,0.0142955,"itional memory requirement, but it is negligible. As we will see in our experiments (Section 5) the largest gains can be obtained with extremely coarse language models. In particular, the largest coarse model we use in our best multipass decoder uses a 4-bit encoding and hence has only 16 distinct words (or at most 4096 trigrams). 3 Inversion Transduction Grammars While our approach applies in principle to a variety of machine translation systems (phrase-based or syntactic), we will use the inversion transduction grammar (ITG) approach of Wu (1997) to facilitate comparison with previous work (Zens and Ney, 2003; Zhang and Gildea, 2008) as well as to focus on language model complexity. ITGs are a subclass of synchronous context-free grammars (SCFGs) where there are only three kinds of rules. Preterminal unary productions produce terminal strings on both sides (words or phrases): X → e/f . Binary in-order productions combine two phrases monotonically (X → [Y Z]). Finally, binary inverted productions invert the order of their children (X → hY Zi). These productions are associated with rewrite weights in the 111 standard way. Without a language model, SCFG decoding is just like (monolingual) CFG parsing"
D08-1012,P08-1025,0,0.365941,"a synchronous CFG translation model is very efficient, requiring only a variant of the CKY algorithm. As in monolingual parsing, dynamic programming items are simply indexed by a source language span and a syntactic label. Complexity arises when n-gram language model scoring is added, because items must now be distinguished by their initial and final few target language words for purposes of later combination. This lexically exploded search space is a root cause of inefficiency in decoding, and several methods have been suggested to combat it. The approach most relevant to the current work is Zhang and Gildea (2008), which begins with an initial bigram pass and uses the resulting chart to guide Central to coarse-to-fine language projection is the construction of sequences of word clusterings (see Figure 1). The clusterings are deterministic mappings from words to clusters, with the property that each clustering refines the previous one. There are many choice points in this process, including how these clusterings are obtained and how much refinement is optimal for each pass. We demonstrate that likelihood-based hierarchical EM training (Petrov et al., 2006) and cluster-based language modeling methods (Go"
D08-1091,abeille-etal-2000-building,0,0.148692,"Missing"
D08-1091,P05-1038,0,0.0263235,"Missing"
D08-1091,P05-1022,0,0.810022,"best parsing numbers on several metrics, for several domains and languages. Discriminative parsing has been investigated before, such as in Johnson (2001), Clark and Curran (2004), Henderson (2004), Koo and Collins (2005), Turian et al. (2007), Finkel et al. (2008), and, most similarly, in Petrov and Klein (2008). However, in all of these cases, the final parsing performance fell short of the best generative models by several percentage points or only short sentences were used. Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (Charniak and Johnson, 2005; Huang, 2008). Multi-scale grammars, in contrast, give higher accuracies using smaller grammars than previous work in this direction, outperforming top generative models in grammar size and in parsing accuracy. 2 Latent Variable Parsing Treebanks are typically not annotated with fully detailed syntactic structure. Rather, they present only a coarse trace of the true underlying processes. As a result, learning a grammar for parsing requires the estimation of a more highly articulated model than the naive CFG embodied by such treebanks. A manual approach might take the category NP and subdivide"
D08-1091,N06-1022,0,0.047385,"Missing"
D08-1091,A00-2018,0,0.13979,"parsing accuracies with the smallest reported grammars. 1 Introduction In latent variable approaches to parsing (Matsuzaki et al., 2005; Petrov et al., 2006), one models an observed treebank of coarse parse trees using a grammar over more refined, but unobserved, derivation trees. The parse trees represent the desired output of the system, while the derivation trees represent the typically much more complex underlying syntactic processes. In recent years, latent variable methods have been shown to produce grammars which are as good as, or even better than, earlier parsing work (Collins, 1999; Charniak, 2000). In particular, in Petrov et al. (2006) we exhibited a very accurate We introduce multi-scale grammars, in which some productions reference fine categories, while others reference coarse categories (see Figure 2). We use the general framework of hidden variable CRFs (Lafferty et al., 2001; Koo and Collins, 2005), where gradient-based optimization maximizes the likelihood of the observed variables, here parse trees, summing over log-linearly scored derivations. With multi-scale grammars, it is natural to refine productions rather than categories. As a result, a category such as NP can be compl"
D08-1091,P04-1014,0,0.0245713,"put features integrally in our dynamic program. Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of Petrov et al. (2006). 867 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 867–876, c Honolulu, October 2008. 2008 Association for Computational Linguistics In addition, we exhibit the best parsing numbers on several metrics, for several domains and languages. Discriminative parsing has been investigated before, such as in Johnson (2001), Clark and Curran (2004), Henderson (2004), Koo and Collins (2005), Turian et al. (2007), Finkel et al. (2008), and, most similarly, in Petrov and Klein (2008). However, in all of these cases, the final parsing performance fell short of the best generative models by several percentage points or only short sentences were used. Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (Charniak and Johnson, 2005; Huang, 2008). Multi-scale grammars, in contrast, give higher accuracies using smaller grammars than previous work in this direction, outperforming top g"
D08-1091,P08-1109,0,0.159821,"magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of Petrov et al. (2006). 867 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 867–876, c Honolulu, October 2008. 2008 Association for Computational Linguistics In addition, we exhibit the best parsing numbers on several metrics, for several domains and languages. Discriminative parsing has been investigated before, such as in Johnson (2001), Clark and Curran (2004), Henderson (2004), Koo and Collins (2005), Turian et al. (2007), Finkel et al. (2008), and, most similarly, in Petrov and Klein (2008). However, in all of these cases, the final parsing performance fell short of the best generative models by several percentage points or only short sentences were used. Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (Charniak and Johnson, 2005; Huang, 2008). Multi-scale grammars, in contrast, give higher accuracies using smaller grammars than previous work in this direction, outperforming top generative models in grammar size and in parsing accuracy. 2 Latent Variable Parsing Tr"
D08-1091,W01-0521,0,0.0561878,"Missing"
D08-1091,P04-1013,0,0.0179826,"n our dynamic program. Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of Petrov et al. (2006). 867 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 867–876, c Honolulu, October 2008. 2008 Association for Computational Linguistics In addition, we exhibit the best parsing numbers on several metrics, for several domains and languages. Discriminative parsing has been investigated before, such as in Johnson (2001), Clark and Curran (2004), Henderson (2004), Koo and Collins (2005), Turian et al. (2007), Finkel et al. (2008), and, most similarly, in Petrov and Klein (2008). However, in all of these cases, the final parsing performance fell short of the best generative models by several percentage points or only short sentences were used. Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (Charniak and Johnson, 2005; Huang, 2008). Multi-scale grammars, in contrast, give higher accuracies using smaller grammars than previous work in this direction, outperforming top generative models i"
D08-1091,P08-1067,0,0.1807,"eral metrics, for several domains and languages. Discriminative parsing has been investigated before, such as in Johnson (2001), Clark and Curran (2004), Henderson (2004), Koo and Collins (2005), Turian et al. (2007), Finkel et al. (2008), and, most similarly, in Petrov and Klein (2008). However, in all of these cases, the final parsing performance fell short of the best generative models by several percentage points or only short sentences were used. Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (Charniak and Johnson, 2005; Huang, 2008). Multi-scale grammars, in contrast, give higher accuracies using smaller grammars than previous work in this direction, outperforming top generative models in grammar size and in parsing accuracy. 2 Latent Variable Parsing Treebanks are typically not annotated with fully detailed syntactic structure. Rather, they present only a coarse trace of the true underlying processes. As a result, learning a grammar for parsing requires the estimation of a more highly articulated model than the naive CFG embodied by such treebanks. A manual approach might take the category NP and subdivide it into one s"
D08-1091,J98-4004,0,0.194826,"r grammars than previous work in this direction, outperforming top generative models in grammar size and in parsing accuracy. 2 Latent Variable Parsing Treebanks are typically not annotated with fully detailed syntactic structure. Rather, they present only a coarse trace of the true underlying processes. As a result, learning a grammar for parsing requires the estimation of a more highly articulated model than the naive CFG embodied by such treebanks. A manual approach might take the category NP and subdivide it into one subcategory NPˆS for subjects and another subcategory NPˆVP for objects (Johnson, 1998; Klein and Manning, 2003). However, rather than devising linguistically motivated features or splits, latent variable parsing takes a fully automated approach, in which each symbol is split into unconstrained subcategories. 2.1 Latent Variable Grammars Latent variable grammars augment the treebank trees with latent variables at each node. This creates a set of (exponentially many) derivations over split categories for each of the original parse trees over unsplit categories. For each observed category A we now have a set of latent subcategories Ax . For example, NP might be split into NP1 thr"
D08-1091,P01-1042,0,0.131099,"nefit of some input features integrally in our dynamic program. Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of Petrov et al. (2006). 867 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 867–876, c Honolulu, October 2008. 2008 Association for Computational Linguistics In addition, we exhibit the best parsing numbers on several metrics, for several domains and languages. Discriminative parsing has been investigated before, such as in Johnson (2001), Clark and Curran (2004), Henderson (2004), Koo and Collins (2005), Turian et al. (2007), Finkel et al. (2008), and, most similarly, in Petrov and Klein (2008). However, in all of these cases, the final parsing performance fell short of the best generative models by several percentage points or only short sentences were used. Only in combination with a generative model was a discriminative component able to produce high parsing accuracies (Charniak and Johnson, 2005; Huang, 2008). Multi-scale grammars, in contrast, give higher accuracies using smaller grammars than previous work in this direc"
D08-1091,P03-1054,1,0.0687685,"previous work in this direction, outperforming top generative models in grammar size and in parsing accuracy. 2 Latent Variable Parsing Treebanks are typically not annotated with fully detailed syntactic structure. Rather, they present only a coarse trace of the true underlying processes. As a result, learning a grammar for parsing requires the estimation of a more highly articulated model than the naive CFG embodied by such treebanks. A manual approach might take the category NP and subdivide it into one subcategory NPˆS for subjects and another subcategory NPˆVP for objects (Johnson, 1998; Klein and Manning, 2003). However, rather than devising linguistically motivated features or splits, latent variable parsing takes a fully automated approach, in which each symbol is split into unconstrained subcategories. 2.1 Latent Variable Grammars Latent variable grammars augment the treebank trees with latent variables at each node. This creates a set of (exponentially many) derivations over split categories for each of the original parse trees over unsplit categories. For each observed category A we now have a set of latent subcategories Ax . For example, NP might be split into NP1 through NP8 . The parameters"
D08-1091,H05-1064,0,0.0495169,"the desired output of the system, while the derivation trees represent the typically much more complex underlying syntactic processes. In recent years, latent variable methods have been shown to produce grammars which are as good as, or even better than, earlier parsing work (Collins, 1999; Charniak, 2000). In particular, in Petrov et al. (2006) we exhibited a very accurate We introduce multi-scale grammars, in which some productions reference fine categories, while others reference coarse categories (see Figure 2). We use the general framework of hidden variable CRFs (Lafferty et al., 2001; Koo and Collins, 2005), where gradient-based optimization maximizes the likelihood of the observed variables, here parse trees, summing over log-linearly scored derivations. With multi-scale grammars, it is natural to refine productions rather than categories. As a result, a category such as NP can be complex in some regions of the grammar while remaining simpler in other regions. Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al., 2004), giving the benefit of some input features integrally in our"
D08-1091,D07-1072,1,0.509376,"1 Latent Variable Grammars Latent variable grammars augment the treebank trees with latent variables at each node. This creates a set of (exponentially many) derivations over split categories for each of the original parse trees over unsplit categories. For each observed category A we now have a set of latent subcategories Ax . For example, NP might be split into NP1 through NP8 . The parameters of the refined productions Ax → By Cz , where Ax is a subcategory of A, By of B, and Cz of C, can then be estimated in various ways; past work has included both generative 868 (Matsuzaki et al., 2005; Liang et al., 2007) and discriminative approaches (Petrov and Klein, 2008). We take the discriminative log-linear approach here. Note that the comparison is only between estimation methods, as Smith and Johnson (2007) show that the model classes are the same. 2.2 Log-Linear Latent Variable Grammars In a log-linear latent variable grammar, each production r = Ax → By Cz is associated with a multiplicative weight φr (Johnson, 2001; Petrov and Klein, 2008) (sometimes we will use the log-weight θr when convenient). The probability of a derivation t of a sentence w is proportional to the product of the weights of its"
D08-1091,J93-2004,0,0.0418058,"Missing"
D08-1091,P05-1010,0,0.52518,"grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of Petrov et al. (2006). In addition, our discriminative approach integrally admits features beyond local tree configurations. We present a multiscale training method along with an efficient CKY-style dynamic program. On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars. 1 Introduction In latent variable approaches to parsing (Matsuzaki et al., 2005; Petrov et al., 2006), one models an observed treebank of coarse parse trees using a grammar over more refined, but unobserved, derivation trees. The parse trees represent the desired output of the system, while the derivation trees represent the typically much more complex underlying syntactic processes. In recent years, latent variable methods have been shown to produce grammars which are as good as, or even better than, earlier parsing work (Collins, 1999; Charniak, 2000). In particular, in Petrov et al. (2006) we exhibited a very accurate We introduce multi-scale grammars, in which some p"
D08-1091,N06-1040,0,0.0110067,"not appeal to the underlying single scale grammar. However, in the present work, we use our multiscale grammars only to compute expectations of the underlying grammars in an efficient, implicit way. 5 Learning Sparse Multi-Scale Grammars We now consider how to discriminatively learn multi-scale grammars by iterative splitting productions. There are two main concerns. First, because multi-scale grammars are most effective when many productions share the same weight, sparsity is very desirable. In the present work, we exploit L1 -regularization, though other techniques such as structural zeros (Mohri and Roark, 2006) could also potentially be used. Second, training requires repeated parsing, so we use coarse-to-fine chart caching to greatly accelerate each iteration. 870 We directly optimize this non-convex objective function using a numerical gradient based method (LBFGS (Nocedal and Wright, 1999) in our implementation). To handle the non-diferentiability of the L1 -regularization term R(θ) we use the orthant-wise method of Andrew and Gao (2007). Fitting the loglinear model involves the following derivatives:   ∂Lcond (θ) X = Eθ [fr (t)|Ti ] − Eθ [fr (t)|wi ] ∂θr i where the first term is the expected"
D08-1091,N07-1051,1,0.623478,"y are still dense at the production level, which we address here. As in Petrov et al. (2006), we arrange our subcategories into a hierarchy, as shown in Figure 1. In practice, the construction of the hierarchy is tightly coupled to a split-based learning process (see Section 5). We use the naming convention that an original category A becomes A0 and A1 in the first round; A0 then becoming A00 and A01 in the second round, and so on. We will use x ˆ ≻ x to indicate that the subscript or subcategory x is a refinement of x ˆ.1 We 1 Conversely, x ˆ is a coarser version of x, or, in the language of Petrov and Klein (2007), x ˆ is a projection of x. θr¯ * S i n g l e  s c a l e p r o d u c t i o n s u M l t i  s c a l e p r o d u c t i o n θrˆ s → D T 0 0 0 → t h e + D T 0 0 1 D T 0 1 D T 0 1 → t h 5 0 → t 0 e + h 5 . 0 } e + 0 . 7 . * D T 0 0 → t h e + D T 0 1 D T 0 1 0 → t h 5 . 0 . 3 → e 3 + 7 1 0 1 + 1 → t h e + D T 1 0 0 → t h 0 + 0 0 0 0 5 0 . 0 + 0 1 5 0 . 0 + 1 1 1 0 7 0 . 3 + 1 1 1 1 2 + 0 1 0 0 2 1 . 1 + 0 1 2 1 . 1 + 2 0 2 1 . 1 + 1 . 1 D T 1 D T 1 1 D T 1 0 1 1 → 0 t → h t e h + 2 . 1 + 2 . 1 + 2 . 1 e 1 2 . 1 1 → t h → t h e } 1 0 + T 1 → t . 1 2 0 D 2 e + 1 1 1 2 e + 0 1 h e + 2 . 5 . 0 1 0 1 0"
D08-1091,P06-1055,1,0.760875,"scriminative utility. In this paper, we present a discriminative approach which addresses both of these limitations. We present a discriminative, latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement. The model is formally a latent variable CRF grammar over trees, learned by iteratively splitting grammar productions (not categories). Different regions of the grammar are refined to different degrees, yielding grammars which are three orders of magnitude smaller than the single-scale baseline and 20 times smaller than the split-and-merge grammars of Petrov et al. (2006). In addition, our discriminative approach integrally admits features beyond local tree configurations. We present a multiscale training method along with an efficient CKY-style dynamic program. On a variety of domains and languages, this method produces the best published parsing accuracies with the smallest reported grammars. 1 Introduction In latent variable approaches to parsing (Matsuzaki et al., 2005; Petrov et al., 2006), one models an observed treebank of coarse parse trees using a grammar over more refined, but unobserved, derivation trees. The parse trees represent the desired output"
D08-1091,A97-1014,0,0.0727901,"Missing"
D08-1091,J07-4003,0,0.0169931,"or each of the original parse trees over unsplit categories. For each observed category A we now have a set of latent subcategories Ax . For example, NP might be split into NP1 through NP8 . The parameters of the refined productions Ax → By Cz , where Ax is a subcategory of A, By of B, and Cz of C, can then be estimated in various ways; past work has included both generative 868 (Matsuzaki et al., 2005; Liang et al., 2007) and discriminative approaches (Petrov and Klein, 2008). We take the discriminative log-linear approach here. Note that the comparison is only between estimation methods, as Smith and Johnson (2007) show that the model classes are the same. 2.2 Log-Linear Latent Variable Grammars In a log-linear latent variable grammar, each production r = Ax → By Cz is associated with a multiplicative weight φr (Johnson, 2001; Petrov and Klein, 2008) (sometimes we will use the log-weight θr when convenient). The probability of a derivation t of a sentence w is proportional to the product of the weights of its productions r: Y P (t|w) ∝ φr r∈t The score of a parse T is then the sum of the scores of its derivations: X P (t|w) P (T |w) = t∈T 3 Hierarchical Refinement Grammar refinement becomes challenging"
D08-1091,W04-3201,1,0.951658,"framework of hidden variable CRFs (Lafferty et al., 2001; Koo and Collins, 2005), where gradient-based optimization maximizes the likelihood of the observed variables, here parse trees, summing over log-linearly scored derivations. With multi-scale grammars, it is natural to refine productions rather than categories. As a result, a category such as NP can be complex in some regions of the grammar while remaining simpler in other regions. Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al., 2004), giving the benefit of some input features integrally in our dynamic program. Our multi-scale grammars are 3 orders of magnitude smaller than the fully-split baseline grammar and 20 times smaller than the generative split-and-merge grammars of Petrov et al. (2006). 867 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 867–876, c Honolulu, October 2008. 2008 Association for Computational Linguistics In addition, we exhibit the best parsing numbers on several metrics, for several domains and languages. Discriminative parsing has been investigated befo"
D08-1091,J03-4003,0,\N,Missing
D10-1002,W08-2102,0,0.357336,"Missing"
D10-1002,N01-1016,0,0.010271,"Test Unlabeled Newswire # sentences # words length Avg./Std. 39.8k 950.0k 28.9/11.2 1.7k 40.1k 25.1/11.8 2.4k 56.7k 25.1/12.0 1,769.1k 43,057.0k 24.3/10.9 Broadcast News # sentences # words length Avg./Std. 59.0k 1,281.1k 17.3/11.3 1.0k 17.1k 17.4/11.3 1.1k 19.4k 17.7/11.4 4,386.5k 77,687.9k 17.7/12.8 Table 1: The number of words and sentences, together with average (Avg.) sentence length and its standard deviation (Std.), for the data sets used in our experiments. duces scores that are identical to those produced by EVALB for WSJ. For Broadcast News, SParseval applies Charniak and Johnson’s (Charniak and Johnson, 2001) scoring method for EDITED nodes3 . Using this method, BN scores were slightly (.05-.1) lower than if EDITED constituents were treated like any other, as in EVALB. 2.3 Latent Variable Grammars We use the latent variable grammar (Matsuzaki et al., 2005; Petrov et al., 2006) implementation of Huang and Harper (2009) in this work. Latent variable grammars augment the observed parse trees in the treebank with a latent variable at each tree node. This effectively splits each observed category into a set of latent subcategories. An EM-algorithm is used to fit the model by maximizing the joint likeli"
D10-1002,P05-1022,0,0.104018,"ccuracies can be achieved by training grammars on disjoint sets of automatically labeled data. Two primary factors appear to be determining the efficacy of our self-training approach. First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars. Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model. Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative PCFG. Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009). In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models. One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models. It would also be interesting to determine whether further increasing the accuracy of the model u"
D10-1002,A00-2018,0,0.220478,"Missing"
D10-1002,D09-1116,1,0.56437,"thod for injecting some additional diversity into the individual grammars to determine whether a product model is most successful when there is more variance among the individual models. Our initial experiments and analysis will focus on the development set of WSJ. We will then follow up with an analysis of broadcast news (BN) to determine whether the findings generalize to a second, less structured type of data. It is important to construct grammars capable of parsing this type of data accurately and consistently in order to support structured language modeling (e.g., (Wang and Harper, 2002; Filimonov and Harper, 2009)). 4 Newswire Experiments In this section, we compare single grammars and their products that are trained in the standard way with gold WSJ training data, as well as the three self-training scenarios discussed in Section 3. We 15 4.1 Regular Training We begin by training ten latent variable models initialized with different random seeds using the gold WSJ training set. Results are presented in Table 2. The best F score attained by the individual SM6 grammars on the development set is 90.8, with an average score of 90.5. The product of grammars achieves a significantly improved accuracy at 92.0"
D10-1002,N09-2064,0,0.115286,"Missing"
D10-1002,D09-1087,1,0.925811,"cently, Petrov (2010) showed that substantial differences between the learned grammars remain, even if the hierarchical splitting reduces the variance across independent runs of EM. In order to counteract the overfitting behavior, Petrov et al. (2006) introduced a linear smoothing procedure that allows training grammars for 6 splitmerge (SM) rounds without overfitting. The increased expressiveness of the model, combined with the more robust parameter estimates provided by the smoothing, results in a nice increase in parsing accuracy on a held-out set. However, as reported by Petrov (2009) and Huang and Harper (2009), an additional 7th SM round actually hurts performance. Huang and Harper (2009) addressed the issue of data sparsity and overfitting from a different angle. They showed that self-training latent variable grammars on their own output can mitigate data sparsity issues and improve parsing accuracy. Because the capacity of the model can grow with the size of the training data, latent variable grammars are able to benefit from the additional training data, even though it is not perfectly labeled. Consequently, they also found that a 7th round of SM training was beneficial in the presence of large"
D10-1002,P08-1067,0,0.309603,"Missing"
D10-1002,D07-1072,1,0.65979,". A hierarchical split-and-merge algorithm introduces grammar complexity gradually, iteratively splitting (and potentially merging back) each observed treebank category into a number of increasingly refined latent subcategories. The Expectation Maximization (EM) algorithm is used to train the model, guaranteeing that each EM iteration will increase the training likelihood. However, because the latent variable grammars are not explicitly regularized, EM keeps fitSlav Petrov∗ Google Research 76 Ninth Avenue New York, NY slav@google.com ∗ ting the training data and eventually begins overfitting (Liang et al., 2007). Moreover, EM is a local method, making no promises regarding the final point of convergence when initialized from different random seeds. Recently, Petrov (2010) showed that substantial differences between the learned grammars remain, even if the hierarchical splitting reduces the variance across independent runs of EM. In order to counteract the overfitting behavior, Petrov et al. (2006) introduced a linear smoothing procedure that allows training grammars for 6 splitmerge (SM) rounds without overfitting. The increased expressiveness of the model, combined with the more robust parameter est"
D10-1002,P05-1010,0,0.398785,"k 19.4k 17.7/11.4 4,386.5k 77,687.9k 17.7/12.8 Table 1: The number of words and sentences, together with average (Avg.) sentence length and its standard deviation (Std.), for the data sets used in our experiments. duces scores that are identical to those produced by EVALB for WSJ. For Broadcast News, SParseval applies Charniak and Johnson’s (Charniak and Johnson, 2001) scoring method for EDITED nodes3 . Using this method, BN scores were slightly (.05-.1) lower than if EDITED constituents were treated like any other, as in EVALB. 2.3 Latent Variable Grammars We use the latent variable grammar (Matsuzaki et al., 2005; Petrov et al., 2006) implementation of Huang and Harper (2009) in this work. Latent variable grammars augment the observed parse trees in the treebank with a latent variable at each tree node. This effectively splits each observed category into a set of latent subcategories. An EM-algorithm is used to fit the model by maximizing the joint likelihood of parse trees and sentences. To allocate the grammar complexity only where needed, a simple split-and-merge procedure is applied. In every splitmerge (SM) round, each latent variable is first split in two and the model is re-estimated. A likelih"
D10-1002,N06-1020,0,0.854687,"odels’ mistakes are independent to some extent, multiple grammars can be effectively combined into an unweighted product model of much higher accuracy. We build upon this line of work and investigate methods to exploit products of latent variable grammars in the context of self-training. 3 Self-training Methodology Different types of parser self-training have been proposed in the literature over the years. All of them involve parsing a set of unlabeled sentences with a baseline parser and then estimating a new parser by combining this automatically parsed data with the original training data. McClosky et al. (2006) presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker. In their approach, a large amount of unlabeled text is parsed by the two-stage system and the parameters of the first-stage lexicalized parser are then re-estimated taking the counts from the automatically parsed data into consideration. More recently Huang and Harper (2009) presented a self-training procedure based on an EMalgorithm. They showed that the EM-algorithm that is typically used to fit a latent variable"
D10-1002,N07-1051,1,0.885056,"Missing"
D10-1002,P06-1055,1,0.639362,"the latent variable grammars are not explicitly regularized, EM keeps fitSlav Petrov∗ Google Research 76 Ninth Avenue New York, NY slav@google.com ∗ ting the training data and eventually begins overfitting (Liang et al., 2007). Moreover, EM is a local method, making no promises regarding the final point of convergence when initialized from different random seeds. Recently, Petrov (2010) showed that substantial differences between the learned grammars remain, even if the hierarchical splitting reduces the variance across independent runs of EM. In order to counteract the overfitting behavior, Petrov et al. (2006) introduced a linear smoothing procedure that allows training grammars for 6 splitmerge (SM) rounds without overfitting. The increased expressiveness of the model, combined with the more robust parameter estimates provided by the smoothing, results in a nice increase in parsing accuracy on a held-out set. However, as reported by Petrov (2009) and Huang and Harper (2009), an additional 7th SM round actually hurts performance. Huang and Harper (2009) addressed the issue of data sparsity and overfitting from a different angle. They showed that self-training latent variable grammars on their own o"
D10-1002,N10-1003,1,0.0899128,"into a number of increasingly refined latent subcategories. The Expectation Maximization (EM) algorithm is used to train the model, guaranteeing that each EM iteration will increase the training likelihood. However, because the latent variable grammars are not explicitly regularized, EM keeps fitSlav Petrov∗ Google Research 76 Ninth Avenue New York, NY slav@google.com ∗ ting the training data and eventually begins overfitting (Liang et al., 2007). Moreover, EM is a local method, making no promises regarding the final point of convergence when initialized from different random seeds. Recently, Petrov (2010) showed that substantial differences between the learned grammars remain, even if the hierarchical splitting reduces the variance across independent runs of EM. In order to counteract the overfitting behavior, Petrov et al. (2006) introduced a linear smoothing procedure that allows training grammars for 6 splitmerge (SM) rounds without overfitting. The increased expressiveness of the model, combined with the more robust parameter estimates provided by the smoothing, results in a nice increase in parsing accuracy on a held-out set. However, as reported by Petrov (2009) and Huang and Harper (200"
D10-1002,roark-etal-2006-sparseval,1,0.786696,"Missing"
D10-1002,N06-2033,0,0.301023,"Missing"
D10-1002,P05-1003,0,0.00841046,"The power of the product model comes directly from the diversity in log p(r|s, G) among individual grammars. If there is little diversity, the individual grammars would make similar predictions and there would be little or no benefit from using a product model. We use the average empirical variance of the log posterior probabilities of the rules among the learned grammars over a held-out set S as a proxy of the diversity among the grammars: P P P p(r|s, G)VAR(log(p(r|s, G))) s∈S G∈G r∈R(G,s) P P Diversity From the perspective of Products of Experts (Hinton, 1999) or Logarithmic Opinion Pools (Smith et al., 2005), each individual expert learns complementary aspects of the training data and the veto power of product models enforces that the joint prediction of their product has to be licensed by all individual experts. One possible explanation of the observation in the previous subsection is that with the addition of more latent variables, the individual grammars become more deeply specialized on certain aspects of the training data. This specialization leads to greater diversity in their prediction preferences, especially in the presence of a small training set. On the other hand, the self-labeled tra"
D10-1002,W02-1031,1,0.777259,"iment investigates a method for injecting some additional diversity into the individual grammars to determine whether a product model is most successful when there is more variance among the individual models. Our initial experiments and analysis will focus on the development set of WSJ. We will then follow up with an analysis of broadcast news (BN) to determine whether the findings generalize to a second, less structured type of data. It is important to construct grammars capable of parsing this type of data accurately and consistently in order to support structured language modeling (e.g., (Wang and Harper, 2002; Filimonov and Harper, 2009)). 4 Newswire Experiments In this section, we compare single grammars and their products that are trained in the standard way with gold WSJ training data, as well as the three self-training scenarios discussed in Section 3. We 15 4.1 Regular Training We begin by training ten latent variable models initialized with different random seeds using the gold WSJ training set. Results are presented in Table 2. The best F score attained by the individual SM6 grammars on the development set is 90.8, with an average score of 90.5. The product of grammars achieves a significan"
D10-1002,D09-1161,0,0.175583,"resulting single self-trained grammars. Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model. Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010), despite being a single generative PCFG. Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009). In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models. One possibility would be to utilize more unlabeled data or to identify additional ways to bias the models. It would also be interesting to determine whether further increasing the accuracy of the model used for automatically labeling the unlabeled data can enhance performance even more. A simple but computationally expensive way to do this would be to parse the data with an SM7 product model. Finally, for this work, we always used products of 10 grammars, but we sometimes observe"
D10-1017,N09-1014,0,0.0434584,"which is unlikely to scale up because its dual formulation requires the inversion of a matrix whose size depends on the graph size. Gupta et al. (2009) also constrain similar trigrams to have similar POS tags by forming cliques of similar trigrams and maximizing the agreement score over these cliques. Computing clique agreement potentials however is NP-hard and so they propose approximation algorithms that are still quite complex computationally. We achieve similar effects by using our simple, scalable convex graph regularization framework. Further, unlike other graph-propagation algorithms (Alexandrescu and Kirchhoff, 2009), our approach is inductive. While one might be able to make inductive extensions of transductive approaches (Sindhwani et al., 2005), these usually require extensive computational resources at test time. 6 Experiments and Results We use the Wall Street Journal (WSJ) section of the Penn Treebank as our labeled source domain training set. We follow standard setup procedures for this task and train on sections 00-18, comprising of 38,219 POS-tagged sentences with a total of 912,344 words. To evaluate our domain-adaptation approach, we consider two different target domains: questions and biomedic"
D10-1017,W06-1615,1,0.263887,"here α is a mixing coefficient which reflects the relative confidence between the original posteriors from the CRF and the smoothed posteriors from the graph. We discuss how we set α in Section 6. 172 −η l+u X  2 log p(yi∗ |xi ; Λ(t) )+γkΛk (5) n i=l+1 where η and γ are hyper-parameters whose setting we discuss in Section 6. Given the new CRF parameters Λ we loop back to step 1 (Section 4.1) and iterate until convergence. It is important to note that every step of our algorithm is convex, although their combination clearly is not. 5 Related Work Our work differs from previous studies of SSL (Blitzer et al., 2006; III, 2007; Huang and Yates, 2009) for improving POS tagging in several ways. First, our algorithm can be generalized to other structured semi-supervised learning problems, although POS tagging is our motivating task and test application. Unlike III (2007), we do not require target domain labeled data. While the SCL algorithm (Blitzer et al., 2006) has been evaluated without target domain labeled data, that evaluation was to some extent transductive in that the target test data (unlabeled) was included in the unsupervised stage of SCL training that creates the structural correspondence betwee"
D10-1017,P09-1056,0,0.0267525,"ich reflects the relative confidence between the original posteriors from the CRF and the smoothed posteriors from the graph. We discuss how we set α in Section 6. 172 −η l+u X  2 log p(yi∗ |xi ; Λ(t) )+γkΛk (5) n i=l+1 where η and γ are hyper-parameters whose setting we discuss in Section 6. Given the new CRF parameters Λ we loop back to step 1 (Section 4.1) and iterate until convergence. It is important to note that every step of our algorithm is convex, although their combination clearly is not. 5 Related Work Our work differs from previous studies of SSL (Blitzer et al., 2006; III, 2007; Huang and Yates, 2009) for improving POS tagging in several ways. First, our algorithm can be generalized to other structured semi-supervised learning problems, although POS tagging is our motivating task and test application. Unlike III (2007), we do not require target domain labeled data. While the SCL algorithm (Blitzer et al., 2006) has been evaluated without target domain labeled data, that evaluation was to some extent transductive in that the target test data (unlabeled) was included in the unsupervised stage of SCL training that creates the structural correspondence between the two domains. We mentioned alr"
D10-1017,P07-1033,0,0.0159588,"Missing"
D10-1017,P06-1063,0,0.0202101,"Missing"
D10-1017,N03-1033,0,0.0271084,"Missing"
D10-1017,P95-1026,0,0.149289,"t-of-domain data may lag far behind. Annotating training data for all sub-domains of a varied domain such as all of Web text is impractical, giving impetus to the development of SSL techniques that can learn from unlabeled data to perform well across domains. The earliest SSL algorithm is self-training (Scudder, 1965), where one makes use of a previously trained model to annotate unlabeled data which is then used to re-train the model. While self-training is widely Fernando Pereira Google Research Mountain View, CA 94043 pereira@google.com used and can yield good results in some applications (Yarowsky, 1995), it has no theoretical guarantees except under certain stringent conditions, which rarely hold in practice(Haffari and Sarkar, 2007). Other SSL methods include co-training (Blum and Mitchell, 1998), transductive support vector machines (SVMs) (Joachims, 1999), and graph-based SSL (Zhu et al., 2003). Several surveys cover a broad range of methods (Seeger, 2000; Zhu, 2005; Chapelle et al., 2007; Blitzer and Zhu, 2008). A majority of SSL algorithms are computationally expensive; for example, solving a transductive SVM exactly is intractable. Thus we have a conflict between wanting to use SSL wit"
D10-1017,P07-1096,0,\N,Missing
D10-1069,W06-1615,0,0.134917,"nterpretations from which they cannot recover (McDonald and Nivre, 2007). Our approach will therefore be to use a large amount of unlabeled data to bias the model towards the appropriate distribution for the target domain. Rather than looking for feature correspondences between the domains 709 80 75 WSJ+QB WSJ 70 0 10 100 1K 10K 100K 1M 85 80 LAS Some Labeled Target Domain Data UAS 85 3.2 75 70 65 WSJ+QB WSJ 60 0 10 100 1K 10K 100K 1M Number of unlabeled questions Figure 2: Uptraining with large amounts of unlabeled data gives significant improvements over two different supervised baselines. (Blitzer et al., 2006), we propose to use automatically labeled target domain data to learn the target domain distribution directly. 4.1 Uptraining vs. Self-training The idea of training parsers on their own output has been around for as long as there have been statistical parsers, but typically does not work well at all (Charniak, 1997). Steedman et al. (2003) and Clark et al. (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. McClosky et al. (2006a) were the first to improve a state-of-the-art constituency parsing system"
D10-1069,A00-1031,0,0.0843199,"rkeleyParser) of Petrov et al. (2006), as well as the recent product of latent variable grammars version (Petrov, 2010). To facilitate comparisons between constituency and dependency parsers, we convert the output of the constituency parsers to labeled dependencies using the same procedure that is applied to the treebanks. We also report their F1 scores for completeness. While the constituency parsers used in our experiments view part-of-speech (POS) tagging as an integral part of parsing, the dependency parsers require the input to be tagged with a separate POS tagger. We use the TnT tagger (Brants, 2000) in our experi707 ments, because of its efficiency and ease of use. Tagger and parser are always trained on the same data. 3 Parsing Questions We consider two domain adaptation scenarios in this paper. In the first scenario (sometimes abbreviated as WSJ), we assume that we do not have any labeled training data from the target domain. In practice, this will always be the case when the target domain is unknown or very diverse. The second scenario (abbreviated as WSJ+QB) assumes a small amount of labeled training data from the target domain. While this might be expensive to obtain, it is certainl"
D10-1069,J92-4003,0,0.175368,"Missing"
D10-1069,W08-2102,0,0.0366411,"Missing"
D10-1069,P05-1022,0,0.36016,"le parsers, as well as our own implementation of a deterministic shiftreduce parser in our experiments. The dependency parsers that we compare are the deterministic shift-reduce MaltParser (Nivre et al., 2007) and the second-order minimum spanning tree algorithm based MstParser (McDonald et al., 2006). Our shiftreduce parser is a re-implementation of the MaltParser, using a standard set of features and a linear kernel SVM for classification. We also train and evaluate the generative lexicalized parser of Charniak (2000) on its own, as well as in combination with the discriminative reranker of Charniak and Johnson (2005). Finally, we run the latent variable parser (a.k.a. BerkeleyParser) of Petrov et al. (2006), as well as the recent product of latent variable grammars version (Petrov, 2010). To facilitate comparisons between constituency and dependency parsers, we convert the output of the constituency parsers to labeled dependencies using the same procedure that is applied to the treebanks. We also report their F1 scores for completeness. While the constituency parsers used in our experiments view part-of-speech (POS) tagging as an integral part of parsing, the dependency parsers require the input to be tag"
D10-1069,A00-2018,0,0.296255,"ries have a maximum length of 160 characters. 2.2 Parsers We use multiple publicly available parsers, as well as our own implementation of a deterministic shiftreduce parser in our experiments. The dependency parsers that we compare are the deterministic shift-reduce MaltParser (Nivre et al., 2007) and the second-order minimum spanning tree algorithm based MstParser (McDonald et al., 2006). Our shiftreduce parser is a re-implementation of the MaltParser, using a standard set of features and a linear kernel SVM for classification. We also train and evaluate the generative lexicalized parser of Charniak (2000) on its own, as well as in combination with the discriminative reranker of Charniak and Johnson (2005). Finally, we run the latent variable parser (a.k.a. BerkeleyParser) of Petrov et al. (2006), as well as the recent product of latent variable grammars version (Petrov, 2010). To facilitate comparisons between constituency and dependency parsers, we convert the output of the constituency parsers to labeled dependencies using the same procedure that is applied to the treebanks. We also report their F1 scores for completeness. While the constituency parsers used in our experiments view part-of-s"
D10-1069,W03-0407,0,0.0250743,"t Domain Data UAS 85 3.2 75 70 65 WSJ+QB WSJ 60 0 10 100 1K 10K 100K 1M Number of unlabeled questions Figure 2: Uptraining with large amounts of unlabeled data gives significant improvements over two different supervised baselines. (Blitzer et al., 2006), we propose to use automatically labeled target domain data to learn the target domain distribution directly. 4.1 Uptraining vs. Self-training The idea of training parsers on their own output has been around for as long as there have been statistical parsers, but typically does not work well at all (Charniak, 1997). Steedman et al. (2003) and Clark et al. (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. McClosky et al. (2006a) were the first to improve a state-of-the-art constituency parsing system by utilizing unlabeled data for self-training. In subsequent work, they show that the same idea can be used for domain adaptation if the unlabeled data is chosen accordingly (McClosky et al., 2006b). Sagae and Tsujii (2007) co-train two dependency parsers by adding automatically parsed sentences for which the parsers agree to the training data. Finally, Suzuki e"
D10-1069,de-marneffe-etal-2006-generating,0,0.0159226,"Missing"
D10-1069,C96-1058,0,0.52879,"Missing"
D10-1069,N10-1060,0,0.132693,"Missing"
D10-1069,W01-0521,0,0.243423,"Missing"
D10-1069,D09-1087,0,0.0495384,"Missing"
D10-1069,P06-1063,0,0.291529,"detailed error analysis in Section 5, showing that the errors of the WSJ-trained model are primarily caused by sharp changes in syntactic configurations and only secondarily due to lexical shifts. Uptraining leads to large improvements across all error metrics and especially on important dependencies like subjects (nsubj). 2.1 Our main training set consists of Sections 02-21 of the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), with Section 22 serving as development set for source domain comparisons. For our target domain experiments, we evaluate on the QuestionBank (Judge et al., 2006), which includes a set of manually annotated questions from a TREC question answering task. The questions in the QuestionBank are very different from our training data in terms of grammatical constructions and vocabulary usage, making this a rather extreme case of domainadaptation. We split the 4,000 questions contained in this corpus in three parts: the first 2,000 questions are reserved as a small target-domain training set; the remaining 2,000 questions are split in two equal parts, the first serving as development set and the second as our final test set. We report accuracies on the develo"
D10-1069,P10-1001,0,0.0273823,"Missing"
D10-1069,P08-1068,0,0.122256,"Missing"
D10-1069,J93-2004,0,0.03499,"Missing"
D10-1069,P09-1039,0,0.0204109,"Missing"
D10-1069,N06-1020,0,0.742823,"Johnson, 2005; Petrov et al., 2006; Carreras et al., 2008; Koo and Collins, 2010). Quite impressively, models based on deterministic shift-reduce parsing algorithms are able to rival the other computationally more expensive models (see Nivre (2008) and references therein for more details). Their linear running time makes them ideal candidates for large scale text processing, and our model of choice for this paper. Unfortunately, the parsing accuracies of all models have been reported to drop significantly on outof-domain test sets, due to shifts in vocabulary and grammar usage (Gildea, 2001; McClosky et al., 2006b; Foster, 2010). In this paper, we focus our attention on the task of parsing questions. Questions pose interesting challenges for WSJ-trained parsers because they are heavily underrepresented in the training data (there are only 334 questions among the 39,832 training sentences). At the same time, questions are of particular interest for user facing applications like question answering or web search, which necessitate parsers that can process questions in a fast and accurate manner. We start our investigation in Section 3 by training several state-of-the-art (dependency and constituency) par"
D10-1069,P06-1043,0,0.804412,"Johnson, 2005; Petrov et al., 2006; Carreras et al., 2008; Koo and Collins, 2010). Quite impressively, models based on deterministic shift-reduce parsing algorithms are able to rival the other computationally more expensive models (see Nivre (2008) and references therein for more details). Their linear running time makes them ideal candidates for large scale text processing, and our model of choice for this paper. Unfortunately, the parsing accuracies of all models have been reported to drop significantly on outof-domain test sets, due to shifts in vocabulary and grammar usage (Gildea, 2001; McClosky et al., 2006b; Foster, 2010). In this paper, we focus our attention on the task of parsing questions. Questions pose interesting challenges for WSJ-trained parsers because they are heavily underrepresented in the training data (there are only 334 questions among the 39,832 training sentences). At the same time, questions are of particular interest for user facing applications like question answering or web search, which necessitate parsers that can process questions in a fast and accurate manner. We start our investigation in Section 3 by training several state-of-the-art (dependency and constituency) par"
D10-1069,D07-1013,0,0.0130183,"ndicating that 2,000 sentences are not sufficient to train accurate parsers, even for quite narrow domains. 4 Uptraining for Domain-Adaptation The results in the previous section suggest that parsers without global constraints have difficulties dealing with the syntactic differences between declarative sentences and questions. A possible explanation is that similar word configurations can appear in both types of sentences, but with very different syntactic interpretation. Local models without global constraints are therefore mislead into deadend interpretations from which they cannot recover (McDonald and Nivre, 2007). Our approach will therefore be to use a large amount of unlabeled data to bias the model towards the appropriate distribution for the target domain. Rather than looking for feature correspondences between the domains 709 80 75 WSJ+QB WSJ 70 0 10 100 1K 10K 100K 1M 85 80 LAS Some Labeled Target Domain Data UAS 85 3.2 75 70 65 WSJ+QB WSJ 60 0 10 100 1K 10K 100K 1M Number of unlabeled questions Figure 2: Uptraining with large amounts of unlabeled data gives significant improvements over two different supervised baselines. (Blitzer et al., 2006), we propose to use automatically labeled target do"
D10-1069,P05-1012,0,0.113042,"might be expensive to obtain, it is certainly feasible for narrow domains (e.g. questions), or when a high parsing accuracy is really important. 3.1 No Labeled Target Domain Data We first trained all parsers on the WSJ training set and evaluated their performance on the two domain specific evaluation sets (newswire and questions). As can be seen in the left columns of Table 1, all parsers perform very well on the WSJ development set. While there are differences in the accuracies, all scores fall within a close range. The table also confirms the commonly known fact (Yamada and Matsumoto, 2003; McDonald et al., 2005) that constituency parsers are more accurate at producing dependencies than dependency parsers (at least when the dependencies were produced by a deterministic transformation of a constituency treebank, as is the case here). This picture changes drastically when the performance is measured on the QuestionBank development set (right columns in Table 1). As one Evaluating on QuestionBank Nivre et al. (2007) McDonald et al. (2006) Charniak (2000) Petrov et al. (2006) Petrov (2010) Our shift-reduce parser Our shift-reduce parser (gold POS) Training on WSJ + QB F1 UAS LAS POS — 83.54 78.85 91.32 —"
D10-1069,W06-2932,0,0.0105863,"question test sets. similar in style to the questions in the QuestionBank: (i) the queries must start with an English function word that can be used to start a question (what, who when, how, why, can, does, etc.), and (ii) the queries have a maximum length of 160 characters. 2.2 Parsers We use multiple publicly available parsers, as well as our own implementation of a deterministic shiftreduce parser in our experiments. The dependency parsers that we compare are the deterministic shift-reduce MaltParser (Nivre et al., 2007) and the second-order minimum spanning tree algorithm based MstParser (McDonald et al., 2006). Our shiftreduce parser is a re-implementation of the MaltParser, using a standard set of features and a linear kernel SVM for classification. We also train and evaluate the generative lexicalized parser of Charniak (2000) on its own, as well as in combination with the discriminative reranker of Charniak and Johnson (2005). Finally, we run the latent variable parser (a.k.a. BerkeleyParser) of Petrov et al. (2006), as well as the recent product of latent variable grammars version (Petrov, 2010). To facilitate comparisons between constituency and dependency parsers, we convert the output of the"
D10-1069,J08-4003,0,0.0655965,"Missing"
D10-1069,P06-1055,1,0.550711,"nts. The dependency parsers that we compare are the deterministic shift-reduce MaltParser (Nivre et al., 2007) and the second-order minimum spanning tree algorithm based MstParser (McDonald et al., 2006). Our shiftreduce parser is a re-implementation of the MaltParser, using a standard set of features and a linear kernel SVM for classification. We also train and evaluate the generative lexicalized parser of Charniak (2000) on its own, as well as in combination with the discriminative reranker of Charniak and Johnson (2005). Finally, we run the latent variable parser (a.k.a. BerkeleyParser) of Petrov et al. (2006), as well as the recent product of latent variable grammars version (Petrov, 2010). To facilitate comparisons between constituency and dependency parsers, we convert the output of the constituency parsers to labeled dependencies using the same procedure that is applied to the treebanks. We also report their F1 scores for completeness. While the constituency parsers used in our experiments view part-of-speech (POS) tagging as an integral part of parsing, the dependency parsers require the input to be tagged with a separate POS tagger. We use the TnT tagger (Brants, 2000) in our experi707 ments,"
D10-1069,N10-1003,1,0.812884,"(Nivre et al., 2007) and the second-order minimum spanning tree algorithm based MstParser (McDonald et al., 2006). Our shiftreduce parser is a re-implementation of the MaltParser, using a standard set of features and a linear kernel SVM for classification. We also train and evaluate the generative lexicalized parser of Charniak (2000) on its own, as well as in combination with the discriminative reranker of Charniak and Johnson (2005). Finally, we run the latent variable parser (a.k.a. BerkeleyParser) of Petrov et al. (2006), as well as the recent product of latent variable grammars version (Petrov, 2010). To facilitate comparisons between constituency and dependency parsers, we convert the output of the constituency parsers to labeled dependencies using the same procedure that is applied to the treebanks. We also report their F1 scores for completeness. While the constituency parsers used in our experiments view part-of-speech (POS) tagging as an integral part of parsing, the dependency parsers require the input to be tagged with a separate POS tagger. We use the TnT tagger (Brants, 2000) in our experi707 ments, because of its efficiency and ease of use. Tagger and parser are always trained o"
D10-1069,W06-1616,0,0.0209052,"Missing"
D10-1069,N06-2033,0,0.0437763,"Missing"
D10-1069,D07-1111,0,0.0685014,"around for as long as there have been statistical parsers, but typically does not work well at all (Charniak, 1997). Steedman et al. (2003) and Clark et al. (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. McClosky et al. (2006a) were the first to improve a state-of-the-art constituency parsing system by utilizing unlabeled data for self-training. In subsequent work, they show that the same idea can be used for domain adaptation if the unlabeled data is chosen accordingly (McClosky et al., 2006b). Sagae and Tsujii (2007) co-train two dependency parsers by adding automatically parsed sentences for which the parsers agree to the training data. Finally, Suzuki et al. (2009) present a very effective semi-supervised approach in which features from multiple generative models estimated on unlabeled data are combined in a discriminative system for structured prediction. All of these approaches have in common that their ultimate goal is to improve the final performance. Our work differs in that instead of improving the Uptraining with different base parsers Baseline Self-training Uptraining on Petrov et al. (2006) Upt"
D10-1069,E03-1008,0,0.199536,"5 80 LAS Some Labeled Target Domain Data UAS 85 3.2 75 70 65 WSJ+QB WSJ 60 0 10 100 1K 10K 100K 1M Number of unlabeled questions Figure 2: Uptraining with large amounts of unlabeled data gives significant improvements over two different supervised baselines. (Blitzer et al., 2006), we propose to use automatically labeled target domain data to learn the target domain distribution directly. 4.1 Uptraining vs. Self-training The idea of training parsers on their own output has been around for as long as there have been statistical parsers, but typically does not work well at all (Charniak, 1997). Steedman et al. (2003) and Clark et al. (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. McClosky et al. (2006a) were the first to improve a state-of-the-art constituency parsing system by utilizing unlabeled data for self-training. In subsequent work, they show that the same idea can be used for domain adaptation if the unlabeled data is chosen accordingly (McClosky et al., 2006b). Sagae and Tsujii (2007) co-train two dependency parsers by adding automatically parsed sentences for which the parsers agree to the training"
D10-1069,D09-1058,0,0.00896614,". (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. McClosky et al. (2006a) were the first to improve a state-of-the-art constituency parsing system by utilizing unlabeled data for self-training. In subsequent work, they show that the same idea can be used for domain adaptation if the unlabeled data is chosen accordingly (McClosky et al., 2006b). Sagae and Tsujii (2007) co-train two dependency parsers by adding automatically parsed sentences for which the parsers agree to the training data. Finally, Suzuki et al. (2009) present a very effective semi-supervised approach in which features from multiple generative models estimated on unlabeled data are combined in a discriminative system for structured prediction. All of these approaches have in common that their ultimate goal is to improve the final performance. Our work differs in that instead of improving the Uptraining with different base parsers Baseline Self-training Uptraining on Petrov et al. (2006) Uptraining on Petrov (2010) Using only WSJ data UAS LAS POS 72.23 60.06 88.48 73.62 61.63 89.60 86.02 76.94 90.75 85.21 76.19 90.74 Using WSJ + QB data UAS"
D10-1069,W03-3023,0,0.115131,"e target domain. While this might be expensive to obtain, it is certainly feasible for narrow domains (e.g. questions), or when a high parsing accuracy is really important. 3.1 No Labeled Target Domain Data We first trained all parsers on the WSJ training set and evaluated their performance on the two domain specific evaluation sets (newswire and questions). As can be seen in the left columns of Table 1, all parsers perform very well on the WSJ development set. While there are differences in the accuracies, all scores fall within a close range. The table also confirms the commonly known fact (Yamada and Matsumoto, 2003; McDonald et al., 2005) that constituency parsers are more accurate at producing dependencies than dependency parsers (at least when the dependencies were produced by a deterministic transformation of a constituency treebank, as is the case here). This picture changes drastically when the performance is measured on the QuestionBank development set (right columns in Table 1). As one Evaluating on QuestionBank Nivre et al. (2007) McDonald et al. (2006) Charniak (2000) Petrov et al. (2006) Petrov (2010) Our shift-reduce parser Our shift-reduce parser (gold POS) Training on WSJ + QB F1 UAS LAS PO"
D11-1006,P10-1131,0,0.0631278,"e of the languages that we considered: • USR: The weakly supervised system of Naseem et al. (2010), in which manually defined universal syntactic rules (USR) are used to constrain a probabilistic Bayesian model. In addition to their original results, we also report results using the same part-of-speech tagset as the systems described in this paper (USR†). This is useful for two reasons. First, it makes the comparison more direct. Second, we can generate USR results for all eight languages and not just for the languages that they report. • PGI: The phylogenetic grammar induction (PGI) model of Berg-Kirkpatrick and Klein (2010), in which the parameters of completely 69 unsupervised DMV models for multiple languages are coupled via a phylogenetic prior. • PR: The posterior regularization (PR) approach of Ganchev et al. (2009), in which a supervised English parser is used to generate constraints that are projected using a parallel corpus and used to regularize a target language parser. We report results without treebank specific rules. Table 4 gives results comparing the models presented in this work to those three systems. For this comparison we use sentences of length 10 or less after punctuation has been removed in"
D11-1006,D10-1117,0,0.0262077,"ew domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large advances in quality, but parsing accuracies still significantly lag behind those of supervised systems. Furthermore, they are often trained and evaluated under idealized conditions, e.g., only on short sentences or assuming the existence of gold-standard part-ofspeech (POS) tags.1 The reason for these assumptions is clear. Unsupervised grammar induction is difficult given the complexity of the analysis space. These assumptions help to give the model traction. The study of unsupervised grammar induction has many merits. Most notably, it increases our"
D11-1006,J93-2003,0,0.027115,"ions), P RT (particles), P UNC (punctuation marks) and X (a catch-all tag). Similar tagsets are used by other studies on grammar induction and projection (Naseem et al., 2010; Zeman and Resnik, 2008). For all our experiments we replaced the language specific part-of-speech tags in the treebanks with these universal tags. Like all treebank projection studies we require a corpus of parallel text for each pair of languages we study. For this we used the Europarl corpus version 5 (Koehn, 2005). The corpus was preprocessed in standard ways and word aligned by running six iterations of IBM Model 1 (Brown et al., 1993), followed by six iterations of the HMM model (Vogel et al., 1996) in both directions. We then intersect word alignments to generate one-to-one alignments. 2.2 Parsing Model All of our parsing models are based on the transition-based dependency parsing paradigm (Nivre, 2008). Specifically, all models use an arc-eager transition strategy and are trained using the averaged perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. The features used by all models are: the part-of-speech tags of the first four words on the buffer and of the top two words on the stack; the word identi"
D11-1006,W06-2920,0,0.843095,"000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large advances in quality, but parsing accuracies still significantly lag behind those of supervise"
D11-1006,W10-2906,1,0.522648,"vein to Hwa et al. (2005), Ganchev et al. (2009), Smith and Eisner (2009) inter alia. The algorithm is based on the work of Hall et al. (2011) for training extrinsic parser objective functions and borrows heavily from ideas in learning with weak supervision including work on learning with constraints (Chang et al., 2007) and posterior regularization (Ganchev et al., 2010). In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al. (2010). The algorithm is given in Figure 2. It starts by labeling a set of target language sentences with a parser, which in our case is the direct transfer parser from the previous section (line 1). Next, it uses these parsed target sentences to ‘seed’ a new parser by training a parameter vector using the predicted parses as a gold standard via standard perceptron updates for J rounds (lines 3-6). This generates a parser that emulates the direct transfer parser, but Notation: x: input sentence y: dependency tree a: alignment w: parameter vector φ(x, y): feature vector DP : dependency parser, i.e.,"
D11-1006,P07-1036,0,0.0538361,"h does not rely on projecting syntax across aligned parallel corpora (modulo the fact that non-gold tags come from a system that uses parallel corpora). In this section we describe a simple mechanism for projecting from the direct transfer system using large amounts of parallel data in a similar vein to Hwa et al. (2005), Ganchev et al. (2009), Smith and Eisner (2009) inter alia. The algorithm is based on the work of Hall et al. (2011) for training extrinsic parser objective functions and borrows heavily from ideas in learning with weak supervision including work on learning with constraints (Chang et al., 2007) and posterior regularization (Ganchev et al., 2010). In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al. (2010). The algorithm is given in Figure 2. It starts by labeling a set of target language sentences with a parser, which in our case is the direct transfer parser from the previous section (line 1). Next, it uses these parsed target sentences to ‘seed’ a new parser by training a parameter vector using the"
D11-1006,A00-2018,0,0.0218137,"resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buc"
D11-1006,P04-1014,0,0.00519879,"ng parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually"
D11-1006,N09-1009,0,0.0195959,"hers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large advances in quality, but parsing accuracies still significantly lag behind those of supervised systems. Furthermore, they are often trained and evaluated under idealized conditions, e.g., only on short sentences or assuming the existence of gold-standard part-ofspeech (POS) tags.1 The reason for these assumptions is clear. Unsupervised grammar induction is difficult given the complexity of the analysis space. These assumptions help to give the mode"
D11-1006,D11-1005,0,0.550093,"ti-source direct (multi-dir.) and projected (multi-proj.) transfer systems. best-source is the best source model from the languages in Table 2 (excluding the target language). avg-source is the mean UAS over the source models for the target (excluding target language). multi-source transfer already provides strong performance gains. We expect that more principled techniques will lead to further improvements. For example, recent work by Søgaard (2011) explores data set sub-sampling methods. Unlike our work, Søgaard found that simply concatenating all the data led to degradation in performance. Cohen et al. (2011) explores the idea learning language specific mixture coefficients for models trained independently on the target language treebanks. However, their results show that this method often did not significantly outperform uniform mixing. 5 Comparison Comparing unsupervised and parser projection systems is difficult as many publications use nonoverlapping sets of languages or different evaluation criteria. We compare to the following three systems that do not augment the treebanks and report results for some of the languages that we considered: • USR: The weakly supervised system of Naseem et al. ("
D11-1006,P99-1065,0,0.200331,"Missing"
D11-1006,P97-1003,0,0.0394505,"cting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins e"
D11-1006,W02-1001,0,0.157428,"Missing"
D11-1006,P11-1061,1,0.74025,"Missing"
D11-1006,P09-1042,0,0.766198,"esults in slightly lower accuracies on average. 5 This requires a transition-based parser with a beam greater than 1 to allow for ambiguity to be resolved at later stages. 65 3.2 Projected Transfer Unlike most language transfer systems for parsers, the direct transfer approach does not rely on projecting syntax across aligned parallel corpora (modulo the fact that non-gold tags come from a system that uses parallel corpora). In this section we describe a simple mechanism for projecting from the direct transfer system using large amounts of parallel data in a similar vein to Hwa et al. (2005), Ganchev et al. (2009), Smith and Eisner (2009) inter alia. The algorithm is based on the work of Hall et al. (2011) for training extrinsic parser objective functions and borrows heavily from ideas in learning with weak supervision including work on learning with constraints (Chang et al., 2007) and posterior regularization (Ganchev et al., 2010). In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al. (2010). The algorithm is given in"
D11-1006,W01-0521,0,0.0110295,"uction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010)."
D11-1006,D11-1138,1,0.682786,"beam greater than 1 to allow for ambiguity to be resolved at later stages. 65 3.2 Projected Transfer Unlike most language transfer systems for parsers, the direct transfer approach does not rely on projecting syntax across aligned parallel corpora (modulo the fact that non-gold tags come from a system that uses parallel corpora). In this section we describe a simple mechanism for projecting from the direct transfer system using large amounts of parallel data in a similar vein to Hwa et al. (2005), Ganchev et al. (2009), Smith and Eisner (2009) inter alia. The algorithm is based on the work of Hall et al. (2011) for training extrinsic parser objective functions and borrows heavily from ideas in learning with weak supervision including work on learning with constraints (Chang et al., 2007) and posterior regularization (Ganchev et al., 2010). In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al. (2010). The algorithm is given in Figure 2. It starts by labeling a set of target language sentences with a parser, which in our"
D11-1006,P04-1061,0,0.0621498,"for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large advances in quality, but parsing accuracies still significantly lag behind those of supervised systems. Furthermore, they are often trained and evaluated under idealized conditions, e.g., only on short sentences or assuming the existence of gold-standard part-ofspeech (POS) tags.1 The reason for these assumptions is clear. Unsupervised grammar induction is difficult given the complexity of the analysis"
D11-1006,2005.mtsummit-papers.11,0,0.023336,"adverbs), P RON (pronouns), D ET (determiners), A DP (prepositions or postpositions), N UM (numerals), C ONJ (conjunctions), P RT (particles), P UNC (punctuation marks) and X (a catch-all tag). Similar tagsets are used by other studies on grammar induction and projection (Naseem et al., 2010; Zeman and Resnik, 2008). For all our experiments we replaced the language specific part-of-speech tags in the treebanks with these universal tags. Like all treebank projection studies we require a corpus of parallel text for each pair of languages we study. For this we used the Europarl corpus version 5 (Koehn, 2005). The corpus was preprocessed in standard ways and word aligned by running six iterations of IBM Model 1 (Brown et al., 1993), followed by six iterations of the HMM model (Vogel et al., 1996) in both directions. We then intersect word alignments to generate one-to-one alignments. 2.2 Parsing Model All of our parsing models are based on the transition-based dependency parsing paradigm (Nivre, 2008). Specifically, all models use an arc-eager transition strategy and are trained using the averaged perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. The features used by all mod"
D11-1006,J93-2004,0,0.0417235,"el corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al"
D11-1006,P06-1043,0,0.00526051,"Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction syst"
D11-1006,P05-1012,1,0.789242,"ltiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in"
D11-1006,D10-1120,0,0.557983,"urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large advances in quality, but parsing accuracies still significantly lag behind those of supervised systems. Furthermore, they are often trained and evaluated under idealized conditions, e.g., only on short sentences or assuming the existence of gold-standard part-ofspeech (POS) tags.1 The reason for these assumptions is clear. Unsupervised grammar induction is difficult given the complexity of the analysis space. These assumptions help to give the model traction. The study of unsupervised grammar inductio"
D11-1006,P05-1013,0,0.0178333,"re trained using the averaged perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. The features used by all models are: the part-of-speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of 64 the top word on the stack (if available). All feature conjunctions are included. For treebanks with non-projective trees we use the pseudo-projective parsing technique to transform the treebank into projective structures (Nivre and Nilsson, 2005). We focus on using this parsing system for two reasons. First, the parser is near state-of-the-art on English parsing benchmarks and second, and more importantly, the parser is extremely fast to train and run, making it easy to run a large number of experiments. Preliminary experiments using a different dependency parser – MSTParser (McDonald et al., 2005) – resulted in similar empirical observations. 2.3 Evaluation All systems are evaluated using unlabeled attachment score (UAS), which is the percentage of words (ignoring punctuation tokens) in a corpus that modify the correct head (Buchholz"
D11-1006,nivre-etal-2006-maltparser,0,0.0980768,"can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical"
D11-1006,J08-4003,0,0.108689,"the treebanks with these universal tags. Like all treebank projection studies we require a corpus of parallel text for each pair of languages we study. For this we used the Europarl corpus version 5 (Koehn, 2005). The corpus was preprocessed in standard ways and word aligned by running six iterations of IBM Model 1 (Brown et al., 1993), followed by six iterations of the HMM model (Vogel et al., 1996) in both directions. We then intersect word alignments to generate one-to-one alignments. 2.2 Parsing Model All of our parsing models are based on the transition-based dependency parsing paradigm (Nivre, 2008). Specifically, all models use an arc-eager transition strategy and are trained using the averaged perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. The features used by all models are: the part-of-speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of 64 the top word on the stack (if available). All feature conjunctions are included. For treebanks with non-projective trees we use the pseudo-projective par"
D11-1006,P06-1055,1,0.504745,"how that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006;"
D11-1006,D10-1069,1,0.607013,"t al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large ad"
D11-1006,P07-1049,0,0.00664668,"ysis space. These assumptions help to give the model traction. The study of unsupervised grammar induction has many merits. Most notably, it increases our understanding of how computers (and possibly humans) learn in the absence of any explicit feedback. However, the gold POS tag assumption weakens any conclusions that can be drawn, as part-of-speech are also a form of syntactic analysis, only shallower. Furthermore, from a practical standpoint, it is rarely the case that we are completely devoid of resources for most languages. This point has been made by 1 A notable exception is the work of Seginer (2007). 62 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 62–72, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics studies that transfer parsers to new languages by projecting syntax across word alignments extracted from parallel corpora (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009). Although again, most of these studies also assume the existence of POS tags. In this work we present a method for creating dependency parsers for languages for which no labeled training data is available. First, w"
D11-1006,D08-1052,0,0.00824402,"em result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. Thi"
D11-1006,P05-1044,0,0.0095803,"t. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large advances in quality, but parsing accuracies still significantly lag behind those of supervised systems. Furthermore, they are often trained and evaluated under idealized conditions, e.g., only on short sentences or assuming the existence of gold-standard part-ofspeech (POS) tags.1 The reason for these assumptions is clear. Unsupervised grammar induction is difficult given the complexity of the analysis space. These assumption"
D11-1006,D09-1086,0,0.138219,"actic analysis, only shallower. Furthermore, from a practical standpoint, it is rarely the case that we are completely devoid of resources for most languages. This point has been made by 1 A notable exception is the work of Seginer (2007). 62 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 62–72, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics studies that transfer parsers to new languages by projecting syntax across word alignments extracted from parallel corpora (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009). Although again, most of these studies also assume the existence of POS tags. In this work we present a method for creating dependency parsers for languages for which no labeled training data is available. First, we train a source side English parser that, crucially, is delexicalized so that its predictions rely soley on the part-of-speech tags of the input sentence, in the same vein as Zeman and Resnik (2008). We empirically show that directly transferring delexicalized models (i.e. parsing a foreign language POS sequence with an English parser) already outperforms state-of-the-art unsupervi"
D11-1006,W04-3207,0,0.00665029,"parallel data in a similar vein to Hwa et al. (2005), Ganchev et al. (2009), Smith and Eisner (2009) inter alia. The algorithm is based on the work of Hall et al. (2011) for training extrinsic parser objective functions and borrows heavily from ideas in learning with weak supervision including work on learning with constraints (Chang et al., 2007) and posterior regularization (Ganchev et al., 2010). In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al. (2010). The algorithm is given in Figure 2. It starts by labeling a set of target language sentences with a parser, which in our case is the direct transfer parser from the previous section (line 1). Next, it uses these parsed target sentences to ‘seed’ a new parser by training a parameter vector using the predicted parses as a gold standard via standard perceptron updates for J rounds (lines 3-6). This generates a parser that emulates the direct transfer parser, but Notation: x: input sentence y: dependency tree a: alignment w: parameter vector φ(x, y): feature vector DP :"
D11-1006,N09-1010,0,0.0349353,"showing that these methods are robust to tagging errors. 4 Multi-Source Transfer The previous section focused on transferring an English parser to a new target language. However, there are over 20 treebanks available for a variety of language groups including Indo-European, Altaic (including Japanese), Semitic, and Sino-Tibetan. Many of these are even in standardized formats (Buchholz and Marsi, 2006; Nivre et al., 2007). Past studies have shown that for both part-of-speech tagging and grammar induction, learning with multiple comparable languages leads to improvements (Cohen and Smith, 2009; Snyder et al., 2009; BergKirkpatrick and Klein, 2010). In this section we exTarget Test Language da de el en es it nl pt sv da 79.2 34.3 33.3 34.4 38.1 44.8 38.7 42.5 44.5 de 45.2 83.9 52.5 37.9 49.4 56.7 43.7 52.0 57.0 el 44.0 53.2 77.5 45.7 57.3 66.8 62.1 66.6 57.8 Source Training Language en es it 45.9 45.0 48.6 47.2 45.8 53.4 63.9 41.6 59.3 82.5 28.5 38.6 53.3 79.7 68.4 57.7 64.7 79.3 60.8 40.9 50.4 69.2 68.5 74.7 58.3 46.3 53.4 nl 46.1 55.8 57.3 43.7 51.2 57.6 73.6 67.1 54.5 pt 48.1 55.5 58.6 42.3 66.7 69.1 58.5 84.6 66.8 sv 47.8 46.2 47.5 43.7 41.4 50.9 44.2 52.1 84.8 Table 2: UAS for all source-target lan"
D11-1006,P11-2120,0,0.151016,".3 68.0 61.1 63.8 pred-POS multi-dir. multi-proj. 46.2 47.5 51.7 52.0 58.5 63.0 55.6 56.5 56.8 58.9 54.3 64.4 67.7 70.3 58.3 62.1 56.1 59.3 Table 3: UAS for multi-source direct (multi-dir.) and projected (multi-proj.) transfer systems. best-source is the best source model from the languages in Table 2 (excluding the target language). avg-source is the mean UAS over the source models for the target (excluding target language). multi-source transfer already provides strong performance gains. We expect that more principled techniques will lead to further improvements. For example, recent work by Søgaard (2011) explores data set sub-sampling methods. Unlike our work, Søgaard found that simply concatenating all the data led to degradation in performance. Cohen et al. (2011) explores the idea learning language specific mixture coefficients for models trained independently on the target language treebanks. However, their results show that this method often did not significantly outperform uniform mixing. 5 Comparison Comparing unsupervised and parser projection systems is difficult as many publications use nonoverlapping sets of languages or different evaluation criteria. We compare to the following th"
D11-1006,N10-1116,0,0.0264635,"om ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large advances in quality, but parsing accuracies still significantly lag behind those of supervised systems. Furthermore, they are often trained and evaluated under idealized conditions, e.g., only on short sentences or assuming the existence of gold-standard part-ofspeech (POS) tags.1 The reason for these assumptions is clear. Unsupervised grammar induction is difficult given the complexity of the analysis space. These assumptions help to give the model traction. The study of unsupervised grammar induction has many merits. Most n"
D11-1006,C96-2141,0,0.0558518,"all tag). Similar tagsets are used by other studies on grammar induction and projection (Naseem et al., 2010; Zeman and Resnik, 2008). For all our experiments we replaced the language specific part-of-speech tags in the treebanks with these universal tags. Like all treebank projection studies we require a corpus of parallel text for each pair of languages we study. For this we used the Europarl corpus version 5 (Koehn, 2005). The corpus was preprocessed in standard ways and word aligned by running six iterations of IBM Model 1 (Brown et al., 1993), followed by six iterations of the HMM model (Vogel et al., 1996) in both directions. We then intersect word alignments to generate one-to-one alignments. 2.2 Parsing Model All of our parsing models are based on the transition-based dependency parsing paradigm (Nivre, 2008). Specifically, all models use an arc-eager transition strategy and are trained using the averaged perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. The features used by all models are: the part-of-speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on t"
D11-1006,W04-0307,0,0.0233826,"d parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources ava"
D11-1006,I08-3008,0,0.794089,"nship. An example of such a tree is given in Figure 1. Dependency tree arcs are often labeled with the role of the syntactic relationship, e.g., is to hearing might be labeled as S UBJECT. However, we focus on unlabeled parsing in order to reduce problems that arise due to different treebank annotation schemes. Of course, even for unlabeled dependencies, significant variations in the annotation schemes remain. For example, in the Danish treebank determiners govern adjectives and nouns in noun phrases, while in most other treebanks the noun is the head of the noun phrase. Unlike previous work (Zeman and Resnik, 2008; Smith and Eisner, 2009), we do not apply any transformations to the treebanks, which makes our results easier to reproduce, but systematically underestimates accuracy. 2.1 Data Sets The treebank data in our experiments are from the CoNLL shared-tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). We use English (en) only as a source language throughout the paper. Additionally, we use the following eight languages as both source and target languages: Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es) and Swedish (sv). For langu"
D11-1006,D08-1059,0,0.0486712,"ges we study. For this we used the Europarl corpus version 5 (Koehn, 2005). The corpus was preprocessed in standard ways and word aligned by running six iterations of IBM Model 1 (Brown et al., 1993), followed by six iterations of the HMM model (Vogel et al., 1996) in both directions. We then intersect word alignments to generate one-to-one alignments. 2.2 Parsing Model All of our parsing models are based on the transition-based dependency parsing paradigm (Nivre, 2008). Specifically, all models use an arc-eager transition strategy and are trained using the averaged perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. The features used by all models are: the part-of-speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of 64 the top word on the stack (if available). All feature conjunctions are included. For treebanks with non-projective trees we use the pseudo-projective parsing technique to transform the treebank into projective structures (Nivre and Nilsson, 2005). We focus on using this parsing system for two reasons"
D11-1006,P07-1002,0,\N,Missing
D11-1006,P01-1067,0,\N,Missing
D11-1006,P99-1010,0,\N,Missing
D11-1006,D07-1096,1,\N,Missing
D11-1017,P07-1036,0,0.00547072,"to additional features. Targeted self-training is also similar to the retraining of Burkett et al. (2010) in which they jointly parse unannotated bilingual text using a multiview learning objective, then retrain the monolingual parser models to include each side of the jointly parsed bitext as monolingual training data. Our approach is different in that it doesn’t use a second parser and bitext to guide the creation of new training data, and instead relies on n-best lists and an extrinsic metric. Our method can be considered an instance of weakly or distantly supervised structured prediction (Chang et al., 2007; Chang et al., 2010; Clarke et al., 2010; Ganchev et al., 2010). Those methods attempt to learn structure models from related external signals or aggregate data statistics. This work differs in two respects. First, we use the external signals not as explicit constraints, but to compute an oracle score used to re-rank a set of parses. As such, there are no requirements that it factor by the structure of the parse tree and can in fact be any arbitrary metric. Second, our final objective is different. In weakly/distantly supervised learning, the objective is to use external knowledge to build be"
D11-1017,A00-2018,0,0.0628137,"ween those extrinsic metrics, parsing quality as measured on the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 20"
D11-1017,W10-2903,0,0.016302,"aining is also similar to the retraining of Burkett et al. (2010) in which they jointly parse unannotated bilingual text using a multiview learning objective, then retrain the monolingual parser models to include each side of the jointly parsed bitext as monolingual training data. Our approach is different in that it doesn’t use a second parser and bitext to guide the creation of new training data, and instead relies on n-best lists and an extrinsic metric. Our method can be considered an instance of weakly or distantly supervised structured prediction (Chang et al., 2007; Chang et al., 2010; Clarke et al., 2010; Ganchev et al., 2010). Those methods attempt to learn structure models from related external signals or aggregate data statistics. This work differs in two respects. First, we use the external signals not as explicit constraints, but to compute an oracle score used to re-rank a set of parses. As such, there are no requirements that it factor by the structure of the parse tree and can in fact be any arbitrary metric. Second, our final objective is different. In weakly/distantly supervised learning, the objective is to use external knowledge to build better structured predictors. In our case t"
D11-1017,P05-1066,0,0.596748,"Missing"
D11-1017,P97-1003,0,0.0555975,"correlation between those extrinsic metrics, parsing quality as measured on the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petr"
D11-1017,de-marneffe-etal-2006-generating,0,0.0173365,"Missing"
D11-1017,N10-1060,0,0.0202747,"ng has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsing was We experiment with Japanese, Korean and Turkish, but never meant as a stand-alone task, but is rather a there is nothing language specific in our approach. 183 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183–192, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics which we refer to as targeted self-training (Section 2). Similar to self-training,"
D11-1017,N04-1035,0,0.046446,"ence reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 1 Introduction means to an end, towards the goal of building systems that can process natural language input. This is not to say that parsers are not used in larger systems. All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in machine translation (Yamada and Knight, 2001; Galley et al., 2004; Xu et al., 2009). While it has been repeatedly shown that using a parser can bring net gains on downstream application quality, it is often unclear how much intrinsic parsing accuracy actually matters. In this paper we try to shed some light on this issue by comparing different parsers in the context of machine translation (MT). We present experiments on translation from English to three Subject-ObjectVerb (SOV) languages,1 because those require extensive syntactic reordering to produce grammatical translations. We evaluate parse quality on a number of extrinsic metrics, including word reord"
D11-1017,C10-1043,0,0.387001,"ne way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into a set C of contiguous spans"
D11-1017,W01-0521,0,0.021726,"regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsing was We experiment with Japanese, Korean and Turkish, but never meant as a stand-alone task, but is rather a there is nothing language specific in our approach. 183 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183–192, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics which we refer to as targeted self-training (S"
D11-1017,2007.mtsummit-papers.29,0,0.312625,"le component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into a set C of co"
D11-1017,D11-1138,1,0.521563,"e parse ranked most highly in the n-best list. The motivation of this selection step is that good performance on the downstream external task, measured by the extrinsic metric, should be predictive of an intrinsically good parse. At the very least, even if the selected parse is not syntactically correct, or even if it goes against the original treebanking guidelines, it results in a higher extrinsic score and should therefore be preferred. One could imagine extending this framework by repeatedly running self-training on successively improving parsers in an EM-style algorithm. A recent work by Hall et al. (2011) on training a parser with multiple objective functions investigates a similar idea in the context of online learning. In this paper we focus our attention on machine translation as the final application, but one could envision applying our techniques to other applications such as information extraction or question answering. In particular, we explore one application of targeted self-training, where computing the extrinsic metric involves plugging the parse into an MT system’s reordering component and computing the accuracy of the reordering compared to a reference word order. We now direct ou"
D11-1017,D10-1092,0,0.0712737,"omputing the accuracy of the reordering compared to a reference word order. We now direct our attention to the details of this application. 3 The MT Reordering Task Determining appropriate target language word order for a translation is a fundamental problem in MT. When translating between languages with significantly different word order such as English and Japanese, it has been shown that metrics which explicitly account for word-order are much better correlated with human judgments of translation quality than those that give more weight to word choice, like BLEU (Lavie and Denkowski, 2009; Isozaki et al., 2010a; Birch and Osborne, 2010). This demonstrates the importance of getting reordering right. 3.1 Reordering as a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transforma"
D11-1017,W10-1736,0,0.160736,"Missing"
D11-1017,P06-1063,0,0.0599599,"ysis and is extremely sensitive to parser errors. 4 4.1 Experimental Setup Good parse Treebank data In our experiments the baseline training corpus is the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1993) using standard training/development/testing splits. We converted the treebank to match the tokenization expected by our MT system. In particular, we split tokens containing hyphens into multiple tokens and, somewhat simplistically, gave the original token’s part-of-speech tag to all newly created tokens. In Section 6 we make also use of the Question Treebank (QTB) (Judge et al., 2006), as a source of syntactically annotated out-of-domain data. Though we experiment with both dependency parsers and phrase structure parsers, our MT system assumes dependency parses as input. We use the Stanford converter (de Marneffe et al., 2006) to convert phrase structure parse trees to dependency parse trees (for both treebank trees and predicted trees). Reordered: 15 or greater of an SPF has that sunscreen Wear Reordering score: 1.0 (matches reference) Bad parse Reordered: 15 or greater of an SPF has that Wear sunscreen Reordering score: 0.78 (“Wear” is out of place) Figure 1: Examples of"
D11-1017,P06-1096,0,0.0282082,"Missing"
D11-1017,J93-2004,0,0.0436852,"hile there is a good correlation between those extrinsic metrics, parsing quality as measured on the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to t"
D11-1017,N06-1020,0,0.0229852,"Missing"
D11-1017,P05-1012,1,0.122844,"nsic metrics, parsing quality as measured on the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsi"
D11-1017,P08-1006,0,0.0120471,"ems between the WSJ-only shift-reduce parser and the QTB-Train targeted self-training 10x shift-reduce parser. ceptron model for an end-to-end MT system where the alignment parameters are updated based on selecting an alignment from a n-best list that leads to highest BLEU score. As mentioned earlier, this also makes our work similar to Hall et al. (2011) who train a perceptron algorithm on multiple objective functions with the goal of producing parsers that are optimized for extrinsic metrics. It has previously been observed that parsers often perform differently for downstream applications. Miyao et al. (2008) compared parser quality in the biomedical domain using a protein-protein interaction (PPI) identification accuracy metric. This allowed them to compare the utility of extant dependency parsers, phrase structure parsers, and deep structure parsers for the PPI identification task. One could apply the targeted self-training technique we describe to optimize any of these parsers for the PPI task, similar to how we have optimized our parser for the MT reordering task. 8 Conclusion reordering component of a machine translation system. This significantly improves the subjective quality of the system"
D11-1017,J08-4003,0,0.104441,"the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsing was We experiment with Japanese,"
D11-1017,P03-1021,1,0.0248517,"mework. 4.5 MT system We carried out all our translation experiments on a state-of-the-art phrase-based statistical MT system. During both training and testing, the system reorders source-language sentences in a preprocessing step using the above-mentioned rules. During decoding, we used an allowed jump width of 4 words. In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding. Overall for decoding, we use between 20 to 30 features, whose weights are optimized using MERT (Och, 2003). All experiments for a given lan187 guage pair use the same set of MERT weights tuned on a system using a separate parser (that is neither the baseline nor the experiment parser). This potentially underestimates the improvements that can be obtained, but also eliminates MERT as a possible source of improvement, allowing us to trace back improvements in translation quality directly to parser changes.2 For parallel training data, we use a custom collection of parallel documents. They come from various sources with a substantial portion coming from the web after using simple heuristics to identi"
D11-1017,P02-1040,0,0.108652,"th metrics.4 One explanation for the drops in LAS is that some parts of the parse tree are important for downstream reordering quality while others are not (or only to a lesser extent). Some distinctions between labels become less important; for example, arcs labeled “amod” and “advmod” are transformed identically by the reordering rules. Some semantic distinctions also become less important; for example, any sane interpretation of “red hot car” would be reordered the same, that is, not at all. 5.2 Translation quality improvement To put the improvement of the MT system in terms of BLEU score (Papineni et al., 2002), a widely used metric for automatic MT evaluation, we took 5000 sentences from Web-Test and had humans generate reference translations into Japanese, Korean, and 4 We did not attempt this experiment for the BerkeleyParser since training was too slow. 188 Turkish. We then trained MT systems varying only the parser used for reordering in training and decoding. Table 2 shows that targeted self-training data increases BLEU score for translation into all three languages. In addition to BLEU increase, a side-by-side human evaluation on 500 sentences (sampled from the 5000 used to compute BLEU score"
D11-1017,N07-1051,1,0.0360042,"1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsing was We experiment with Japanese, Korean and Turkish, but never meant as a stand-alone task, but is rather a there is nothing language specific in our approach. 183 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183–192, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics which we refer to as targeted self-training (Section 2). Similar to self-training, a baseline model is used to produce predictions on an unlabeled data set. However, rather than directly training on the output of the b"
D11-1017,P06-1055,1,0.0472315,"uality as measured on the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsing was We experiment"
D11-1017,D10-1069,1,0.571776,"to produce predictions on an unlabeled data set. However, rather than directly training on the output of the baseline model, we generate a list of hypotheses and use an external signal to select the best candidate. The selected parse trees are added to the training data and the model is then retrained. The experiments in Section 5 show that this simple procedure noticeably improves our parsers for the task at hand, resulting in significant improvements in downstream translation quality, as measured in a human evaluation on web text. This idea is similar in vein to McClosky. et al. (2006) and Petrov et al. (2010), except that we use an extrinsic quality metric instead of a second parsing model for making the selection. It is also similar to Burkett and Klein (2008) and Burkett et al. (2010), but again avoiding the added complexity introduced by the use of additional (bilingual) models for candidate selection. It should be noted that our extrinsic metric is computed from data that has been manually annotated with reference word reorderings. Details of the reordering metric and the annotated data we used are given in Sections 3 and 4. While this annotation requires some effort, such annotations are much"
D11-1017,W11-2102,1,0.704327,"s into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into a set C of contiguous spans whose content appears contiguously in the same order in the reference. The reordering score is then computed as ρ(esys , eref ) = 1 − |C |− 1 . |e |− 1 This metric assigns a score between 0"
D11-1017,D07-1077,0,0.168629,"mportance of getting reordering right. 3.1 Reordering as a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the"
D11-1017,C04-1073,0,0.448999,"s a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into"
D11-1017,N09-1028,1,0.710453,"uide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 1 Introduction means to an end, towards the goal of building systems that can process natural language input. This is not to say that parsers are not used in larger systems. All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in machine translation (Yamada and Knight, 2001; Galley et al., 2004; Xu et al., 2009). While it has been repeatedly shown that using a parser can bring net gains on downstream application quality, it is often unclear how much intrinsic parsing accuracy actually matters. In this paper we try to shed some light on this issue by comparing different parsers in the context of machine translation (MT). We present experiments on translation from English to three Subject-ObjectVerb (SOV) languages,1 because those require extensive syntactic reordering to produce grammatical translations. We evaluate parse quality on a number of extrinsic metrics, including word reordering accuracy, BL"
D11-1017,P01-1067,0,0.0915201,"s of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 1 Introduction means to an end, towards the goal of building systems that can process natural language input. This is not to say that parsers are not used in larger systems. All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in machine translation (Yamada and Knight, 2001; Galley et al., 2004; Xu et al., 2009). While it has been repeatedly shown that using a parser can bring net gains on downstream application quality, it is often unclear how much intrinsic parsing accuracy actually matters. In this paper we try to shed some light on this issue by comparing different parsers in the context of machine translation (MT). We present experiments on translation from English to three Subject-ObjectVerb (SOV) languages,1 because those require extensive syntactic reordering to produce grammatical translations. We evaluate parse quality on a number of extrinsic metrics,"
D11-1017,W06-3108,0,0.0368459,"icable to any language pair. We chose to use manually written rules to eliminate the variance induced by the automatic reordering-rule learning framework. 4.5 MT system We carried out all our translation experiments on a state-of-the-art phrase-based statistical MT system. During both training and testing, the system reorders source-language sentences in a preprocessing step using the above-mentioned rules. During decoding, we used an allowed jump width of 4 words. In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding. Overall for decoding, we use between 20 to 30 features, whose weights are optimized using MERT (Och, 2003). All experiments for a given lan187 guage pair use the same set of MERT weights tuned on a system using a separate parser (that is neither the baseline nor the experiment parser). This potentially underestimates the improvements that can be obtained, but also eliminates MERT as a possible source of improvement, allowing us to trace back improvements in translation quality directly to parser changes.2 For parallel training data, we use a custom collection of"
D11-1017,D08-1059,0,0.0478733,"derings is straightforward because annotators need little special background or training, as long as they can speak both the source and target languages. We chose Japanese as the target language through which to create the English reference reorderings because we had access to bilingual annotators fluent in English and Japanese. 186 Parsers The core dependency parser we use is an implementation of a transition-based dependency parser using an arc-eager transition strategy (Nivre, 2008). The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). The parser uses the following features: word identity of the first two words on the buffer, the top word on the stack and the head of the top word on the stack (if available); part-ofspeech identities of the first four words on the buffer and top two words on the stack; dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the leftmost modifier of the first word in the buffer. We also include conjunctions over all nonlexical features. We also give results for the latent variable parser (a.k.a. BerkeleyParser) of Petro"
D11-1017,W10-2906,1,\N,Missing
D11-1017,A00-1031,0,\N,Missing
D11-1017,W10-1749,0,\N,Missing
D11-1017,D08-1092,0,\N,Missing
D13-1049,J90-2002,0,0.846888,"zing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations. 1 Introduction Generating the appropriate word order for the target language has been one of the fundamental problems in machine translation since the ground setting work of Brown et al. (1990). Lexical reordering approaches (Tillmann, 2004; Zens and Ney, 2006) add a reordering component to standard phrase-based translation systems (Och and Ney, 2004). Because the reordering model is trained discriminatively, it can use a rich set of lexical features. However, it only has access to the local context which often times is insufficient to make the long-distance reordering decisions that are necessary for language pairs with significantly different word order. Models that use a source-side parser differ on two main dimensions: the way tree transformations are expressed, and whether they"
D13-1049,J93-2003,0,0.029433,"tional complexity of the learning algorithm and the incremental nature in which the rules are learned and applied. 3.1 WMT Setup In our first set of experiments, we use the data provided for the WMT 2010 shared task (CallisonBurch et al., 2010). We build systems for all language pairs: English to and from Czech, French, German, and Spanish. Since this is a publicly available dataset, it is easy to compare our results to other submissions to the shared task. During word alignment, we filter out sentences exceeding 60 words in the parallel texts and perform 6 iterations of IBM Model-1 training (Brown et al., 1993), followed by 6 iterations of HMM training (Vogel et al., 1996). We do not use Model-4 because it is slow and did not add much value to our systems in a pilot study. Standard phrase extraction heuristics (Koehn et al., 2003) are applied to extract phrase pairs with a length limit of 6 from alignments symmetrized with the “union” heuristic. Maximum jump width is set to 8. Rule extraction for the forest517 to-string system is limited to 16 rules per tree node. There are no length-based reordering constraints in the forest-to-string system. We train two 5-gram language models with Kneser-Ney smoo"
D13-1049,W06-2920,0,0.0270223,"urn. We only include the results 518 en: English1 cs: Czech2 de: German3 es: Spanish4 fr: French5 hu: Hungarian2 nl: Dutch3 pt: Portuguese4 UAS 92.28 84.66 89.30 86.24 88.57 87.66 86.09 90.22 LAS 90.28 72.01 86.98 82.32 86.40 82.51 82.31 87.26 POS 97.05 98.97 97.69 96.62 97.48 94.47 97.38 98.10 Table 1: Parsing accuracies on the retokenized treebanks. UAS is unlabeled attachment score, LAS is labeled attachment score, and POS is part-of-speech tagging accuracy. The treebank sources are (1): Marcus et al. (1993) + Judge et al. (2006) + Petrov and McDonald (2012), (2): Nivre et al. (2007), (3): Buchholz and Marsi (2006), (4): McDonald et al. (2013), (5): Abeill´e et al. (2003). from the forest-to-string system when they are better than the phrase-based results. We use * to denote results from the forest-to-string system. 4.1 WMT Experiments Table 2 presents detailed results on the WMT setup. Lexical reordering (Zens and Ney, 2006) never hurts and is thus included in all systems. Overall, our results are a little better than the best results of the WMT 2010 shared task for two language pairs and within reach of the best results in most other cases. The 2-step classifier preordering approach provides statistic"
D13-1049,W10-1703,0,0.0189316,"Missing"
D13-1049,P05-1066,0,0.827786,"Missing"
D13-1049,P97-1003,0,0.0891084,"Missing"
D13-1049,de-marneffe-etal-2006-generating,0,0.0062678,"Missing"
D13-1049,D11-1018,0,0.661613,"04; Collins et al., 2005) preprocess the input in such a way that the words on the source side appear closer to their final positions on the target side. Because preordering is performed prior to word alignment, it can improve the alignment process and can then be combined with any subsequent translation model. Most preordering models use a source-side syntactic parser and perform a series of tree transformations. Approaches that do not use a parser exist as well and typically induce a hierarchical representation that also allows them to perform longdistance changes (Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other lan"
D13-1049,N10-1128,0,0.187992,"ing gains of up to 1.4 BLEU over a state-of-the-art system. We also show gains of up to 0.5 BLEU over a strong directly comparable preordering system that is based on learning unlexicalized reordering rules. We obtain improvements of more than 2 BLEU in experiments on additional languages. The gains are especially large for languages where the sentence structure is very different from English. These positive results are confirmed in human side-by-side evaluations. When comparing our approach to syntax-based translation systems (Yamada and Knight, 2001; Galley et al., 2004; Huang et al., 2006; Dyer and Resnik, 2010) we note that both approaches use syntactic information for reordering decisions. Our preordering approach has several advantages. First, beFeature ROOT p attr det nsubj amod It was a real whirlwind NN VBD DT JJ NN NOUN VERB DET ADJ NOUN Weight PrevChild:tag=JJ,PrevSibling:a 0.448 PrevChild:cat=ADJ,PrevSibling:a 0.292 PrevChild:cat=ADJ,NoNextSibling 0.212 ... . . . Figure 2: An example where lexical information is necessary for choosing the correct word order. cause preordering is performed before learning word alignments, it has the potential to improve the word alignments. Second, by using d"
D13-1049,N04-1035,0,0.0785,"on the WMT 2010 shared task data, observing gains of up to 1.4 BLEU over a state-of-the-art system. We also show gains of up to 0.5 BLEU over a strong directly comparable preordering system that is based on learning unlexicalized reordering rules. We obtain improvements of more than 2 BLEU in experiments on additional languages. The gains are especially large for languages where the sentence structure is very different from English. These positive results are confirmed in human side-by-side evaluations. When comparing our approach to syntax-based translation systems (Yamada and Knight, 2001; Galley et al., 2004; Huang et al., 2006; Dyer and Resnik, 2010) we note that both approaches use syntactic information for reordering decisions. Our preordering approach has several advantages. First, beFeature ROOT p attr det nsubj amod It was a real whirlwind NN VBD DT JJ NN NOUN VERB DET ADJ NOUN Weight PrevChild:tag=JJ,PrevSibling:a 0.448 PrevChild:cat=ADJ,PrevSibling:a 0.292 PrevChild:cat=ADJ,NoNextSibling 0.212 ... . . . Figure 2: An example where lexical information is necessary for choosing the correct word order. cause preordering is performed before learning word alignments, it has the potential to imp"
D13-1049,C10-1043,0,0.607851,"fferent word order. Models that use a source-side parser differ on two main dimensions: the way tree transformations are expressed, and whether they are built manually or learned from data. One common type of tree transformation are rewrite rules. These typically involve some condition under which the transformation can be applied (e.g., a noun and an adjective found in the same clause) and the transformation itself (e.g., move the adjective after the noun). These rules can be designed manually (Collins et al., 2005; Wang et al., 2007) or learned from data (Xia and McCord, 2004; Habash, 2007; Genzel, 2010; Wu et al., 2011). Another type of tree transformations uses ranking functions to implement precedence-based reordering. Here, a function assigns a numerical value to every word in a clause, intended to express the precedence of the word in the target language. The reordering operation is then to sort the words according to their assigned values. The ranking function 513 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 513–523, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics can be designed manually (X"
D13-1049,2007.mtsummit-papers.29,0,0.168716,"gnificantly different word order. Models that use a source-side parser differ on two main dimensions: the way tree transformations are expressed, and whether they are built manually or learned from data. One common type of tree transformation are rewrite rules. These typically involve some condition under which the transformation can be applied (e.g., a noun and an adjective found in the same clause) and the transformation itself (e.g., move the adjective after the noun). These rules can be designed manually (Collins et al., 2005; Wang et al., 2007) or learned from data (Xia and McCord, 2004; Habash, 2007; Genzel, 2010; Wu et al., 2011). Another type of tree transformations uses ranking functions to implement precedence-based reordering. Here, a function assigns a numerical value to every word in a clause, intended to express the precedence of the word in the target language. The reordering operation is then to sort the words according to their assigned values. The ranking function 513 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 513–523, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics can be design"
D13-1049,2006.amta-papers.8,0,0.0396848,"ed task data, observing gains of up to 1.4 BLEU over a state-of-the-art system. We also show gains of up to 0.5 BLEU over a strong directly comparable preordering system that is based on learning unlexicalized reordering rules. We obtain improvements of more than 2 BLEU in experiments on additional languages. The gains are especially large for languages where the sentence structure is very different from English. These positive results are confirmed in human side-by-side evaluations. When comparing our approach to syntax-based translation systems (Yamada and Knight, 2001; Galley et al., 2004; Huang et al., 2006; Dyer and Resnik, 2010) we note that both approaches use syntactic information for reordering decisions. Our preordering approach has several advantages. First, beFeature ROOT p attr det nsubj amod It was a real whirlwind NN VBD DT JJ NN NOUN VERB DET ADJ NOUN Weight PrevChild:tag=JJ,PrevSibling:a 0.448 PrevChild:cat=ADJ,PrevSibling:a 0.292 PrevChild:cat=ADJ,NoNextSibling 0.212 ... . . . Figure 2: An example where lexical information is necessary for choosing the correct word order. cause preordering is performed before learning word alignments, it has the potential to improve the word alignm"
D13-1049,P06-1063,0,0.0121392,"ts and language pairs we divide the experiments into groups and discuss each in turn. We only include the results 518 en: English1 cs: Czech2 de: German3 es: Spanish4 fr: French5 hu: Hungarian2 nl: Dutch3 pt: Portuguese4 UAS 92.28 84.66 89.30 86.24 88.57 87.66 86.09 90.22 LAS 90.28 72.01 86.98 82.32 86.40 82.51 82.31 87.26 POS 97.05 98.97 97.69 96.62 97.48 94.47 97.38 98.10 Table 1: Parsing accuracies on the retokenized treebanks. UAS is unlabeled attachment score, LAS is labeled attachment score, and POS is part-of-speech tagging accuracy. The treebank sources are (1): Marcus et al. (1993) + Judge et al. (2006) + Petrov and McDonald (2012), (2): Nivre et al. (2007), (3): Buchholz and Marsi (2006), (4): McDonald et al. (2013), (5): Abeill´e et al. (2003). from the forest-to-string system when they are better than the phrase-based results. We use * to denote results from the forest-to-string system. 4.1 WMT Experiments Table 2 presents detailed results on the WMT setup. Lexical reordering (Zens and Ney, 2006) never hurts and is thus included in all systems. Overall, our results are a little better than the best results of the WMT 2010 shared task for two language pairs and within reach of the best res"
D13-1049,N03-1017,0,0.206592,"nBurch et al., 2010). We build systems for all language pairs: English to and from Czech, French, German, and Spanish. Since this is a publicly available dataset, it is easy to compare our results to other submissions to the shared task. During word alignment, we filter out sentences exceeding 60 words in the parallel texts and perform 6 iterations of IBM Model-1 training (Brown et al., 1993), followed by 6 iterations of HMM training (Vogel et al., 1996). We do not use Model-4 because it is slow and did not add much value to our systems in a pilot study. Standard phrase extraction heuristics (Koehn et al., 2003) are applied to extract phrase pairs with a length limit of 6 from alignments symmetrized with the “union” heuristic. Maximum jump width is set to 8. Rule extraction for the forest517 to-string system is limited to 16 rules per tree node. There are no length-based reordering constraints in the forest-to-string system. We train two 5-gram language models with Kneser-Ney smoothing for each of the target languages. One is trained on the target side of the parallel text, the other on a news corpus provided by the shared task. We tune the feature weights for every configuration with 10 rounds of hy"
D13-1049,P08-1068,0,0.0174249,"ot comparable to other numbers in the literature. When necessary, we projectivize the treebanks by raising arcs until the tree becomes projective, as described in Nivre and Nilsson (2005); we do not reconstruct non-projective arcs at parsing time, since our subsequent systems expect projective trees. Our part-of-speech tagger is a conditional random field model (Lafferty et al., 2001) with simple wordidentity and affix features. The parsing model is a shift-reduce dependency parser, using the higherorder features from Zhang and Nivre (2011). Additionally, we include 256 word-cluster features (Koo et al., 2008) trained on a large amount of unlabeled monolingual text (Uszkoreit and Brants, 2008). 4 Experiments Due to the large number of experiments and language pairs we divide the experiments into groups and discuss each in turn. We only include the results 518 en: English1 cs: Czech2 de: German3 es: Spanish4 fr: French5 hu: Hungarian2 nl: Dutch3 pt: Portuguese4 UAS 92.28 84.66 89.30 86.24 88.57 87.66 86.09 90.22 LAS 90.28 72.01 86.98 82.32 86.40 82.51 82.31 87.26 POS 97.05 98.97 97.69 96.62 97.48 94.47 97.38 98.10 Table 1: Parsing accuracies on the retokenized treebanks. UAS is unlabeled attachment"
D13-1049,P09-1019,0,0.0109447,"limit of 6 from alignments symmetrized with the “union” heuristic. Maximum jump width is set to 8. Rule extraction for the forest517 to-string system is limited to 16 rules per tree node. There are no length-based reordering constraints in the forest-to-string system. We train two 5-gram language models with Kneser-Ney smoothing for each of the target languages. One is trained on the target side of the parallel text, the other on a news corpus provided by the shared task. We tune the feature weights for every configuration with 10 rounds of hypergraph-based Minimum Error Rate Training (MERT) (Kumar et al., 2009). 3.2 Additional Languages In our second set of experiments, we explore the impact of classifier preordering for a number of languages with different word orders. Some of the languages included in our study are verb-subject-object (VSO) languages (Arabic, Irish, Welsh), subjectobject-verb (SOV) languages (Japanese, Korean), and fairly free word order languages (Dutch, Hungarian). Where a parser is available, we also conduct experiments on translating into English. Since there are no standard training sets for many of these language pairs, we use parallel data automatically mined from the web."
D13-1049,P07-1091,0,0.339957,"ed preordering model. Our model operates over dependency parse trees and is therefore able to perform long-distance reordering decisions, as is typical for preordering models. But instead of deterministic rules or ranking functions, we use discriminative classifiers to directly predict the final word order, using rich (bi-)lexical and syntactic features. We present two models. The first model uses a classifier to directly predict the permutation order in which a family of words (a head word and all its children) will appear on the target side. This approach is similar in spirit to the work of Li et al. (2007), except that they use constituency parse trees and consider only nodes with 2 or 3 children. We instead work with dependency trees and consider much larger head-children sets. Our second model is designed to decompose the exponential search space of all possible permutations. The prediction task is broken into two separate steps. In the first step, for each child word a binary classifier decides whether it appears before or after its parent in the target language. In the second step, we predict the best order of the words on each side of the parent. We show that the second approach is never w"
D13-1049,J93-2004,0,0.0449807,"rge number of experiments and language pairs we divide the experiments into groups and discuss each in turn. We only include the results 518 en: English1 cs: Czech2 de: German3 es: Spanish4 fr: French5 hu: Hungarian2 nl: Dutch3 pt: Portuguese4 UAS 92.28 84.66 89.30 86.24 88.57 87.66 86.09 90.22 LAS 90.28 72.01 86.98 82.32 86.40 82.51 82.31 87.26 POS 97.05 98.97 97.69 96.62 97.48 94.47 97.38 98.10 Table 1: Parsing accuracies on the retokenized treebanks. UAS is unlabeled attachment score, LAS is labeled attachment score, and POS is part-of-speech tagging accuracy. The treebank sources are (1): Marcus et al. (1993) + Judge et al. (2006) + Petrov and McDonald (2012), (2): Nivre et al. (2007), (3): Buchholz and Marsi (2006), (4): McDonald et al. (2013), (5): Abeill´e et al. (2003). from the forest-to-string system when they are better than the phrase-based results. We use * to denote results from the forest-to-string system. 4.1 WMT Experiments Table 2 presents detailed results on the WMT setup. Lexical reordering (Zens and Ney, 2006) never hurts and is thus included in all systems. Overall, our results are a little better than the best results of the WMT 2010 shared task for two language pairs and within"
D13-1049,P13-2017,1,0.102032,"Missing"
D13-1049,D12-1077,0,0.499858,"eprocess the input in such a way that the words on the source side appear closer to their final positions on the target side. Because preordering is performed prior to word alignment, it can improve the alignment process and can then be combined with any subsequent translation model. Most preordering models use a source-side syntactic parser and perform a series of tree transformations. Approaches that do not use a parser exist as well and typically induce a hierarchical representation that also allows them to perform longdistance changes (Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain impr"
D13-1049,P05-1013,0,0.0347065,"fe et al., 2006). The remaining treebanks are all available in dependency format. In all cases, we apply a set of heuristics to the treebank data to make the tokenization as similar as possible to the one of the bitext. Our heuristics can split treebank tokens but do not merge treebank tokens. We found that adjusting the treebank tokenization is crucial for obtaining good results. However, this makes the reported parsing accuracies not comparable to other numbers in the literature. When necessary, we projectivize the treebanks by raising arcs until the tree becomes projective, as described in Nivre and Nilsson (2005); we do not reconstruct non-projective arcs at parsing time, since our subsequent systems expect projective trees. Our part-of-speech tagger is a conditional random field model (Lafferty et al., 2001) with simple wordidentity and affix features. The parsing model is a shift-reduce dependency parser, using the higherorder features from Zhang and Nivre (2011). Additionally, we include 256 word-cluster features (Koo et al., 2008) trained on a large amount of unlabeled monolingual text (Uszkoreit and Brants, 2008). 4 Experiments Due to the large number of experiments and language pairs we divide t"
D13-1049,J04-4002,0,0.515116,"into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations. 1 Introduction Generating the appropriate word order for the target language has been one of the fundamental problems in machine translation since the ground setting work of Brown et al. (1990). Lexical reordering approaches (Tillmann, 2004; Zens and Ney, 2006) add a reordering component to standard phrase-based translation systems (Och and Ney, 2004). Because the reordering model is trained discriminatively, it can use a rich set of lexical features. However, it only has access to the local context which often times is insufficient to make the long-distance reordering decisions that are necessary for language pairs with significantly different word order. Models that use a source-side parser differ on two main dimensions: the way tree transformations are expressed, and whether they are built manually or learned from data. One common type of tree transformation are rewrite rules. These typically involve some condition under which the trans"
D13-1049,P02-1040,0,0.101695,"re or after its parent in the target language. In the second step, we predict the best order of the words on each side of the parent. We show that the second approach is never worse than the first one and sometimes significantly better. We present experiments on 22 language pairs from different language families using our preordering approach in a phrase-based system (Och and Ney, 2004), as well as a forest-to-string system (Zhang et al., 2011). In a first set of experiments, we use the WMT 2010 shared task data (CallisonBurch et al., 2010) and show significant improvements of up to 1.4 BLEU (Papineni et al., 2002) on three out of eight language pairs. In a second set of experiments, we use automatically mined parallel data from the web and build translation systems for languages from various language families. We obtain especially big improvements in translation quality (2-7 BLEU) when the language pairs have divergent word order (for example English to Indonesian, Japanese, Korean or Malay). In our experiments on English to and from Hungarian, Dutch, and Portuguese translation, we find that we can ob514 tain consistent improvements in both translation directions. To additionally verify our improvement"
D13-1049,petrov-etal-2012-universal,1,0.0318922,"ether each child is before, immediately before, immediately after, or after the head. • For every child, if there is a gap between it and the head, then the first and last word of that gap. • For every pair of consecutive children, if there is a gap between them, then the first and last word of that gap. • The head’s immediate sibling to the left/right or an indication that none exists. When extracting the features, every word can be represented by its word identity, its fine-grained POS tag from the treebank, and a coarse-grained POS category, similar to the universal categories described in Petrov et al. (2012). We also include pairs of these features, resulting in potentially bilexical features. 2.2 Training Data The training data for the classifiers is generated from the word aligned parallel text. Since parallel data is plentiful, we can afford to be selective. We first construct the intersection of high-confidence sourceto-target and target-to-source alignments. For every family in the source dependency tree we generate a training instance if and only if the intersection defines a full order on the source words: • Every source word must be aligned to at least one target word. 515 ROOT p pobj det"
D13-1049,W11-2102,0,0.04952,"s section, we analyze an example whose translation is significantly improved by our preordering approach, demonstrating the usefulness of our lexicalized features. Consider the English sentence: It was a real whirlwind. en-ar en-iw en-ja* no reordering fuzzy exact BLEU 63.2 19.8 11.4 67.9 22.2 18.8 44.1 0.0 14.9 fuzzy 83.5 89.8 80.9 manual exact 47.6 62.4 41.5 BLEU 12.4 20.3 18.4 fuzzy 79.0 89.2 78.5 automatic exact BLEU 38.9 12.6 61.2 20.2 36.8 18.6 Table 5: Preordering accuracy for the 2-step classifiers using manual alignments vs. automatic alignments. Fuzzy refers to the metric defined in Talbot et al. (2011) and exact is the percentage of sentences with a perfect preordering. taken from the WMT test set. The dependency parse tree is shown in Figure 2. In our experiments the rule-based approach of (Genzel, 2010) reordered the source sentence into: It was a whirlwind real. and produced the translation: Es un torbellino real. In comparison, our 2-step system kept the English sentence unchanged and produced the translation: Fue un aut´entico torbellino. The second translation is better than the first because of the correct tense (which is not related directly to the preordering) and because the noun"
D13-1049,N04-4026,0,0.294298,"res, including lexical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations. 1 Introduction Generating the appropriate word order for the target language has been one of the fundamental problems in machine translation since the ground setting work of Brown et al. (1990). Lexical reordering approaches (Tillmann, 2004; Zens and Ney, 2006) add a reordering component to standard phrase-based translation systems (Och and Ney, 2004). Because the reordering model is trained discriminatively, it can use a rich set of lexical features. However, it only has access to the local context which often times is insufficient to make the long-distance reordering decisions that are necessary for language pairs with significantly different word order. Models that use a source-side parser differ on two main dimensions: the way tree transformations are expressed, and whether they are built manually or learned from data. One c"
D13-1049,D09-1105,0,0.220343,"oaches (Xia and McCord, 2004; Collins et al., 2005) preprocess the input in such a way that the words on the source side appear closer to their final positions on the target side. Because preordering is performed prior to word alignment, it can improve the alignment process and can then be combined with any subsequent translation model. Most preordering models use a source-side syntactic parser and perform a series of tree transformations. Approaches that do not use a parser exist as well and typically induce a hierarchical representation that also allows them to perform longdistance changes (Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We present a simple and novel classifier-based preordering approach. Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order. Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features. We present extensive experiments on 22 language pairs, including preordering i"
D13-1049,P08-1086,0,0.0278128,"tivize the treebanks by raising arcs until the tree becomes projective, as described in Nivre and Nilsson (2005); we do not reconstruct non-projective arcs at parsing time, since our subsequent systems expect projective trees. Our part-of-speech tagger is a conditional random field model (Lafferty et al., 2001) with simple wordidentity and affix features. The parsing model is a shift-reduce dependency parser, using the higherorder features from Zhang and Nivre (2011). Additionally, we include 256 word-cluster features (Koo et al., 2008) trained on a large amount of unlabeled monolingual text (Uszkoreit and Brants, 2008). 4 Experiments Due to the large number of experiments and language pairs we divide the experiments into groups and discuss each in turn. We only include the results 518 en: English1 cs: Czech2 de: German3 es: Spanish4 fr: French5 hu: Hungarian2 nl: Dutch3 pt: Portuguese4 UAS 92.28 84.66 89.30 86.24 88.57 87.66 86.09 90.22 LAS 90.28 72.01 86.98 82.32 86.40 82.51 82.31 87.26 POS 97.05 98.97 97.69 96.62 97.48 94.47 97.38 98.10 Table 1: Parsing accuracies on the retokenized treebanks. UAS is unlabeled attachment score, LAS is labeled attachment score, and POS is part-of-speech tagging accuracy. T"
D13-1049,C96-2141,0,0.126104,"nature in which the rules are learned and applied. 3.1 WMT Setup In our first set of experiments, we use the data provided for the WMT 2010 shared task (CallisonBurch et al., 2010). We build systems for all language pairs: English to and from Czech, French, German, and Spanish. Since this is a publicly available dataset, it is easy to compare our results to other submissions to the shared task. During word alignment, we filter out sentences exceeding 60 words in the parallel texts and perform 6 iterations of IBM Model-1 training (Brown et al., 1993), followed by 6 iterations of HMM training (Vogel et al., 1996). We do not use Model-4 because it is slow and did not add much value to our systems in a pilot study. Standard phrase extraction heuristics (Koehn et al., 2003) are applied to extract phrase pairs with a length limit of 6 from alignments symmetrized with the “union” heuristic. Maximum jump width is set to 8. Rule extraction for the forest517 to-string system is limited to 16 rules per tree node. There are no length-based reordering constraints in the forest-to-string system. We train two 5-gram language models with Kneser-Ney smoothing for each of the target languages. One is trained on the t"
D13-1049,D07-1077,0,0.149226,"rdering decisions that are necessary for language pairs with significantly different word order. Models that use a source-side parser differ on two main dimensions: the way tree transformations are expressed, and whether they are built manually or learned from data. One common type of tree transformation are rewrite rules. These typically involve some condition under which the transformation can be applied (e.g., a noun and an adjective found in the same clause) and the transformation itself (e.g., move the adjective after the noun). These rules can be designed manually (Collins et al., 2005; Wang et al., 2007) or learned from data (Xia and McCord, 2004; Habash, 2007; Genzel, 2010; Wu et al., 2011). Another type of tree transformations uses ranking functions to implement precedence-based reordering. Here, a function assigns a numerical value to every word in a clause, intended to express the precedence of the word in the target language. The reordering operation is then to sort the words according to their assigned values. The ranking function 513 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 513–523, c Seattle, Washington, USA, 18-21 October 2013. 201"
D13-1049,I11-1004,0,0.397238,"rder. Models that use a source-side parser differ on two main dimensions: the way tree transformations are expressed, and whether they are built manually or learned from data. One common type of tree transformation are rewrite rules. These typically involve some condition under which the transformation can be applied (e.g., a noun and an adjective found in the same clause) and the transformation itself (e.g., move the adjective after the noun). These rules can be designed manually (Collins et al., 2005; Wang et al., 2007) or learned from data (Xia and McCord, 2004; Habash, 2007; Genzel, 2010; Wu et al., 2011). Another type of tree transformations uses ranking functions to implement precedence-based reordering. Here, a function assigns a numerical value to every word in a clause, intended to express the precedence of the word in the target language. The reordering operation is then to sort the words according to their assigned values. The ranking function 513 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 513–523, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics can be designed manually (Xu et al., 2009) or"
D13-1049,C04-1073,0,0.735637,"language pairs with significantly different word order. Models that use a source-side parser differ on two main dimensions: the way tree transformations are expressed, and whether they are built manually or learned from data. One common type of tree transformation are rewrite rules. These typically involve some condition under which the transformation can be applied (e.g., a noun and an adjective found in the same clause) and the transformation itself (e.g., move the adjective after the noun). These rules can be designed manually (Collins et al., 2005; Wang et al., 2007) or learned from data (Xia and McCord, 2004; Habash, 2007; Genzel, 2010; Wu et al., 2011). Another type of tree transformations uses ranking functions to implement precedence-based reordering. Here, a function assigns a numerical value to every word in a clause, intended to express the precedence of the word in the target language. The reordering operation is then to sort the words according to their assigned values. The ranking function 513 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 513–523, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics"
D13-1049,N09-1028,0,0.633481,"0; Wu et al., 2011). Another type of tree transformations uses ranking functions to implement precedence-based reordering. Here, a function assigns a numerical value to every word in a clause, intended to express the precedence of the word in the target language. The reordering operation is then to sort the words according to their assigned values. The ranking function 513 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 513–523, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics can be designed manually (Xu et al., 2009) or trained from data (Yang et al., 2012). This approach is particularly effective for Subject-Object-Verb (SOV) languages. In this work we present a simple classifier-based preordering model. Our model operates over dependency parse trees and is therefore able to perform long-distance reordering decisions, as is typical for preordering models. But instead of deterministic rules or ranking functions, we use discriminative classifiers to directly predict the final word order, using rich (bi-)lexical and syntactic features. We present two models. The first model uses a classifier to directly pre"
D13-1049,P01-1067,0,0.0203922,"We obtain strong results on the WMT 2010 shared task data, observing gains of up to 1.4 BLEU over a state-of-the-art system. We also show gains of up to 0.5 BLEU over a strong directly comparable preordering system that is based on learning unlexicalized reordering rules. We obtain improvements of more than 2 BLEU in experiments on additional languages. The gains are especially large for languages where the sentence structure is very different from English. These positive results are confirmed in human side-by-side evaluations. When comparing our approach to syntax-based translation systems (Yamada and Knight, 2001; Galley et al., 2004; Huang et al., 2006; Dyer and Resnik, 2010) we note that both approaches use syntactic information for reordering decisions. Our preordering approach has several advantages. First, beFeature ROOT p attr det nsubj amod It was a real whirlwind NN VBD DT JJ NN NOUN VERB DET ADJ NOUN Weight PrevChild:tag=JJ,PrevSibling:a 0.448 PrevChild:cat=ADJ,PrevSibling:a 0.292 PrevChild:cat=ADJ,NoNextSibling 0.212 ... . . . Figure 2: An example where lexical information is necessary for choosing the correct word order. cause preordering is performed before learning word alignments, it has"
D13-1049,P12-1096,0,0.315515,"ee transformations uses ranking functions to implement precedence-based reordering. Here, a function assigns a numerical value to every word in a clause, intended to express the precedence of the word in the target language. The reordering operation is then to sort the words according to their assigned values. The ranking function 513 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 513–523, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics can be designed manually (Xu et al., 2009) or trained from data (Yang et al., 2012). This approach is particularly effective for Subject-Object-Verb (SOV) languages. In this work we present a simple classifier-based preordering model. Our model operates over dependency parse trees and is therefore able to perform long-distance reordering decisions, as is typical for preordering models. But instead of deterministic rules or ranking functions, we use discriminative classifiers to directly predict the final word order, using rich (bi-)lexical and syntactic features. We present two models. The first model uses a classifier to directly predict the permutation order in which a fam"
D13-1049,W06-3108,0,0.0345542,"exical features. We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages. We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task. For languages from different families the improvements often exceed 2 BLEU. Many of these gains are also significant in human evaluations. 1 Introduction Generating the appropriate word order for the target language has been one of the fundamental problems in machine translation since the ground setting work of Brown et al. (1990). Lexical reordering approaches (Tillmann, 2004; Zens and Ney, 2006) add a reordering component to standard phrase-based translation systems (Och and Ney, 2004). Because the reordering model is trained discriminatively, it can use a rich set of lexical features. However, it only has access to the local context which often times is insufficient to make the long-distance reordering decisions that are necessary for language pairs with significantly different word order. Models that use a source-side parser differ on two main dimensions: the way tree transformations are expressed, and whether they are built manually or learned from data. One common type of tree tr"
D13-1049,P11-2033,0,0.00936711,"btaining good results. However, this makes the reported parsing accuracies not comparable to other numbers in the literature. When necessary, we projectivize the treebanks by raising arcs until the tree becomes projective, as described in Nivre and Nilsson (2005); we do not reconstruct non-projective arcs at parsing time, since our subsequent systems expect projective trees. Our part-of-speech tagger is a conditional random field model (Lafferty et al., 2001) with simple wordidentity and affix features. The parsing model is a shift-reduce dependency parser, using the higherorder features from Zhang and Nivre (2011). Additionally, we include 256 word-cluster features (Koo et al., 2008) trained on a large amount of unlabeled monolingual text (Uszkoreit and Brants, 2008). 4 Experiments Due to the large number of experiments and language pairs we divide the experiments into groups and discuss each in turn. We only include the results 518 en: English1 cs: Czech2 de: German3 es: Spanish4 fr: French5 hu: Hungarian2 nl: Dutch3 pt: Portuguese4 UAS 92.28 84.66 89.30 86.24 88.57 87.66 86.09 90.22 LAS 90.28 72.01 86.98 82.32 86.40 82.51 82.31 87.26 POS 97.05 98.97 97.69 96.62 97.48 94.47 97.38 98.10 Table 1: Parsin"
D13-1049,P11-1084,0,0.0136038,"ll possible permutations. The prediction task is broken into two separate steps. In the first step, for each child word a binary classifier decides whether it appears before or after its parent in the target language. In the second step, we predict the best order of the words on each side of the parent. We show that the second approach is never worse than the first one and sometimes significantly better. We present experiments on 22 language pairs from different language families using our preordering approach in a phrase-based system (Och and Ney, 2004), as well as a forest-to-string system (Zhang et al., 2011). In a first set of experiments, we use the WMT 2010 shared task data (CallisonBurch et al., 2010) and show significant improvements of up to 1.4 BLEU (Papineni et al., 2002) on three out of eight language pairs. In a second set of experiments, we use automatically mined parallel data from the web and build translation systems for languages from various language families. We obtain especially big improvements in translation quality (2-7 BLEU) when the language pairs have divergent word order (for example English to Indonesian, Japanese, Korean or Malay). In our experiments on English to and fr"
D13-1049,D07-1096,0,\N,Missing
D14-1134,D11-1039,1,0.834478,"et al. (2011), Goldwasser and Roth (2011), Liang 6 Results still vary slightly due to multi-threading. et al. (2011), Chen and Mooney (2011), and Chen (2012). In all these approaches the grammar structure is fixed prior to parameter estimation. Zettlemoyer and Collins (2005) proposed the learning regime most related to ours. Their learner alternates between batch lexical induction and online parameter estimation. Our learning algorithm design combines aspects of previously studied approaches into a batch method, including gradient updates (Kwiatkowski et al., 2010) and using weak supervision (Artzi and Zettlemoyer, 2011). In contrast, Artzi and Zettlemoyer (2013b) use online perceptron-style updates to optimize a margin-based loss. Our work also focuses on CCG lexicon induction but differs in the use of corpuslevel statistics through voting and pruning for explicitly controlling the size of the lexicon. Our approach is also related to the grammar induction algorithm introduced by Carroll and Char1281 niak (1992). Similar to our method, they process the data using two batch steps: the first proposes grammar rules, analogous to our step that generates lexical entries, and the second estimates parsing parameters"
D14-1134,Q13-1005,1,0.449487,"owers 26 0 N λx.intersection(x) N : λx.hall(x) String Table 3: Example entries from a learned ORACLE corpus lexicon using batch learning. For each string we report the number of lexical entries without voting (C ONSENSUS VOTE) and pruning and with, and provide a few examples. Struck entries were successfully avoided when using voting and pruning. decreases from 16.77 for online training to 8.11. Finally, the total computational cost of our approach is roughly equivalent to online approaches. In both approaches, each pass over the data makes the same number of inference calls, and in practice, Artzi and Zettlemoyer (2013b) used 6-8 iterations for online learning while we used 10. A benefit of the batch method is its insensitivity to data ordering, as expressed by the lower standard deviation between randomized runs in Table 2.6 7 Related Work There has been significant work on learning for semantic parsing. The majority of approaches treat grammar induction and parameter estimation separately, e.g. Wong and Mooney (2006), Kate and Mooney (2006), Clarke et al. (2010), Goldwasser et al. (2011), Goldwasser and Roth (2011), Liang 6 Results still vary slightly due to multi-threading. et al. (2011), Chen and Mooney"
D14-1134,S13-1045,0,0.107865,"(Steedman, 1996, 2000, CCG, henceforth) is a commonly used formalism for semantic parsing – the task of mapping natural language sentences to formal meaning representations (Zelle and Mooney, 1996). Recently, CCG semantic parsers have been used for numerous language understanding tasks, including querying databases (Zettlemoyer and Collins, 2005), referring to physical objects (Matuszek et al., 2012), information extraction (Krishnamurthy and Mitchell, 2012), executing instructions (Artzi and Zettlemoyer, 2013b), generating regular expressions (Kushman and Barzilay, 2013), question-answering (Cai and Yates, 2013) and textual entailment (Lewis and Steedman, 2013). In CCG, a lexicon is used to map words to formal representations of their meaning, which are then combined using bottom-up operations. In this paper we present learning techniques ∗ This research was carried out at Google. to explicitly control the size of the CCG lexicon, and show that this results in improved task performance and more compact models. In most approaches for inducing CCGs for semantic parsing, lexicon learning and parameter estimation are performed jointly in an online algorithm, as introduced by Zettlemoyer and Collins (2007"
D14-1134,P12-1045,0,0.611188,"82.89 83.69 83.00 84.10 Sequence R 58.95 61.23 60.13 64.86 63.45 65.97 66.40 66.15 P 68.35 66.83 72.64 72.79 72.99 75.15 72.91 75.65 Lexicon size 5383 3104 6323 2588 2446 2791 2186 2101 F1 63.26 63.88 65.76 68.55 67.84 70.19 69.47 70.55 Table 1: Ablation study using cross-validation on the ORACLE corpus training data. We report mean precision (P), recall (R) and harmonic mean (F1) of execution accuracy on single sentences and sequences of instructions and mean lexicon sizes. Bold numbers represent the best performing method on a given metric. Single sentence R F1 Chen and Mooney (2011) 54.40 Chen (2012) 57.28 + additional data 57.62 SAIL Kim and Mooney (2012) 57.22 Kim and Mooney (2013) 62.81 Artzi and Zettlemoyer (2013b) 67.60 65.28 66.42 Our Approach 66.67 64.36 65.49 Artzi and Zettlemoyer (2013b) 81.17 (0.68) 78.63 (0.84) 79.88 (0.76) ORACLE Our Approach 79.86 (0.50) 77.87 (0.41) 78.85 (0.45) Final results P P 38.06 41.30 68.07 (2.72) 76.05 (1.79) Sequence R 16.18 19.18 20.64 20.17 26.57 31.93 35.44 58.05 (3.12) 68.53 (1.76) F1 Lexicon size 34.72 38.14 62.65 (2.91) 72.10 (1.77) 10051 2873 6213 (217) 2365 (57) Table 2: Our final results compared to previous work on the SAIL and ORACLE corp"
D14-1134,J07-4004,0,0.0157977,"you, ι(λx.chair(x)∧ intersect(x, ι(λy.intersection(y))))))∧ move(a) ∧ len(a, 2) hFORWARD, FORWARDi turn left λa.turn(a) ∧ direction(a, left) hLEFTi go to the end of the hall λx.move(a) ∧ to(a, ι(λx.end(x, ι(λy.hall(y))))) hFORWARD, FORWARDi Figure 3: Fragment of a map and instructions for the navigation domain. The fragment includes two intersecting hallways (red and blue), two chairs and an agent facing left (green pentagon), which follows instructions such as these listed below. Each instruction is paired with a logical form representing its meaning and its execution in the map. inspired by Clark and Curran (2007). Let G EN(x, s; Λ) ⊂ Y be the set of all possible CCG parses given the sentence x, the current state s and the lexicon Λ. In G EN(x, s; Λ), multiple parse trees may have the same logical form; let Y(z) ⊂ G EN(x, s; Λ) be the subset of such parses with the logical form z at the root. Also, let θ ∈ Rd be a d-dimensional parameter vector. We define the probability of the logical form z as: X p(z|x, s; θ, Λ) = p(y|x, s; θ, Λ) (1) y∈Y(z) Above, we marginalize out the probabilities of all parse trees with the same logical form z at the root. The probability of a parse tree y is defined as: eθ·φ(x,s"
D14-1134,W10-2903,0,0.0454206,"ly equivalent to online approaches. In both approaches, each pass over the data makes the same number of inference calls, and in practice, Artzi and Zettlemoyer (2013b) used 6-8 iterations for online learning while we used 10. A benefit of the batch method is its insensitivity to data ordering, as expressed by the lower standard deviation between randomized runs in Table 2.6 7 Related Work There has been significant work on learning for semantic parsing. The majority of approaches treat grammar induction and parameter estimation separately, e.g. Wong and Mooney (2006), Kate and Mooney (2006), Clarke et al. (2010), Goldwasser et al. (2011), Goldwasser and Roth (2011), Liang 6 Results still vary slightly due to multi-threading. et al. (2011), Chen and Mooney (2011), and Chen (2012). In all these approaches the grammar structure is fixed prior to parameter estimation. Zettlemoyer and Collins (2005) proposed the learning regime most related to ours. Their learner alternates between batch lexical induction and online parameter estimation. Our learning algorithm design combines aspects of previously studied approaches into a batch method, including gradient updates (Kwiatkowski et al., 2010) and using weak"
D14-1134,P11-1149,0,0.0730235,"e approaches. In both approaches, each pass over the data makes the same number of inference calls, and in practice, Artzi and Zettlemoyer (2013b) used 6-8 iterations for online learning while we used 10. A benefit of the batch method is its insensitivity to data ordering, as expressed by the lower standard deviation between randomized runs in Table 2.6 7 Related Work There has been significant work on learning for semantic parsing. The majority of approaches treat grammar induction and parameter estimation separately, e.g. Wong and Mooney (2006), Kate and Mooney (2006), Clarke et al. (2010), Goldwasser et al. (2011), Goldwasser and Roth (2011), Liang 6 Results still vary slightly due to multi-threading. et al. (2011), Chen and Mooney (2011), and Chen (2012). In all these approaches the grammar structure is fixed prior to parameter estimation. Zettlemoyer and Collins (2005) proposed the learning regime most related to ours. Their learner alternates between batch lexical induction and online parameter estimation. Our learning algorithm design combines aspects of previously studied approaches into a batch method, including gradient updates (Kwiatkowski et al., 2010) and using weak supervision (Artzi and Zet"
D14-1134,P11-1060,0,0.247859,"Missing"
D14-1134,P06-1115,0,0.0177015,"of our approach is roughly equivalent to online approaches. In both approaches, each pass over the data makes the same number of inference calls, and in practice, Artzi and Zettlemoyer (2013b) used 6-8 iterations for online learning while we used 10. A benefit of the batch method is its insensitivity to data ordering, as expressed by the lower standard deviation between randomized runs in Table 2.6 7 Related Work There has been significant work on learning for semantic parsing. The majority of approaches treat grammar induction and parameter estimation separately, e.g. Wong and Mooney (2006), Kate and Mooney (2006), Clarke et al. (2010), Goldwasser et al. (2011), Goldwasser and Roth (2011), Liang 6 Results still vary slightly due to multi-threading. et al. (2011), Chen and Mooney (2011), and Chen (2012). In all these approaches the grammar structure is fixed prior to parameter estimation. Zettlemoyer and Collins (2005) proposed the learning regime most related to ours. Their learner alternates between batch lexical induction and online parameter estimation. Our learning algorithm design combines aspects of previously studied approaches into a batch method, including gradient updates (Kwiatkowski et al.,"
D14-1134,D12-1040,0,0.301446,"Missing"
D14-1134,P13-1022,0,0.159261,"Missing"
D14-1134,D12-1069,0,0.080607,"ord chair as learned with no corpus-level statistics. Our approach is able to correctly learn only the top two bolded entries. Introduction Combinatory Categorial Grammar (Steedman, 1996, 2000, CCG, henceforth) is a commonly used formalism for semantic parsing – the task of mapping natural language sentences to formal meaning representations (Zelle and Mooney, 1996). Recently, CCG semantic parsers have been used for numerous language understanding tasks, including querying databases (Zettlemoyer and Collins, 2005), referring to physical objects (Matuszek et al., 2012), information extraction (Krishnamurthy and Mitchell, 2012), executing instructions (Artzi and Zettlemoyer, 2013b), generating regular expressions (Kushman and Barzilay, 2013), question-answering (Cai and Yates, 2013) and textual entailment (Lewis and Steedman, 2013). In CCG, a lexicon is used to map words to formal representations of their meaning, which are then combined using bottom-up operations. In this paper we present learning techniques ∗ This research was carried out at Google. to explicitly control the size of the CCG lexicon, and show that this results in improved task performance and more compact models. In most approaches for inducing CCG"
D14-1134,N13-1103,0,0.038827,"ies. Introduction Combinatory Categorial Grammar (Steedman, 1996, 2000, CCG, henceforth) is a commonly used formalism for semantic parsing – the task of mapping natural language sentences to formal meaning representations (Zelle and Mooney, 1996). Recently, CCG semantic parsers have been used for numerous language understanding tasks, including querying databases (Zettlemoyer and Collins, 2005), referring to physical objects (Matuszek et al., 2012), information extraction (Krishnamurthy and Mitchell, 2012), executing instructions (Artzi and Zettlemoyer, 2013b), generating regular expressions (Kushman and Barzilay, 2013), question-answering (Cai and Yates, 2013) and textual entailment (Lewis and Steedman, 2013). In CCG, a lexicon is used to map words to formal representations of their meaning, which are then combined using bottom-up operations. In this paper we present learning techniques ∗ This research was carried out at Google. to explicitly control the size of the CCG lexicon, and show that this results in improved task performance and more compact models. In most approaches for inducing CCGs for semantic parsing, lexicon learning and parameter estimation are performed jointly in an online algorithm, as i"
D14-1134,D10-1119,0,0.0362849,"e and Mooney (2006), Clarke et al. (2010), Goldwasser et al. (2011), Goldwasser and Roth (2011), Liang 6 Results still vary slightly due to multi-threading. et al. (2011), Chen and Mooney (2011), and Chen (2012). In all these approaches the grammar structure is fixed prior to parameter estimation. Zettlemoyer and Collins (2005) proposed the learning regime most related to ours. Their learner alternates between batch lexical induction and online parameter estimation. Our learning algorithm design combines aspects of previously studied approaches into a batch method, including gradient updates (Kwiatkowski et al., 2010) and using weak supervision (Artzi and Zettlemoyer, 2011). In contrast, Artzi and Zettlemoyer (2013b) use online perceptron-style updates to optimize a margin-based loss. Our work also focuses on CCG lexicon induction but differs in the use of corpuslevel statistics through voting and pruning for explicitly controlling the size of the lexicon. Our approach is also related to the grammar induction algorithm introduced by Carroll and Char1281 niak (1992). Similar to our method, they process the data using two batch steps: the first proposes grammar rules, analogous to our step that generates lex"
D14-1134,D11-1140,0,0.155997,"tching categories to strings in the sentence using the lexicon. For example, the lexical entry walk ` S/N P : λx.λa.move(a) ∧ direction(a, x) pairs the string walk with the example category above. Each intermediate parse node is constructed by applying 1274 one of a small set of binary CCG combinators or unary operators. For example, in Figure 2 the category of the span walk forward is combined with the category of twice using backward application (<). Parsing concludes with a logical form that captures the meaning of the complete sentence. We adopt a factored representation for CCG lexicons (Kwiatkowski et al., 2011), where entries are dynamically generated by combining lexemes and templates. A lexeme is a pair that consists of a natural language string and a set of logical constants, while the template contains the syntactic and semantic components of a CCG category, abstracting over logical constants. For example, consider the lexical entry walk ` S/N P : λx.λa.move(a) ∧ direction(a, x). Under the factored representation, this entry can be constructed by combining the lexeme hwalk, {move, direction}i and the template λv1 .λv2 .[S/N P : λx.λa.v1 (a) ∧ v2 (a, x)]. This representation allows for better gen"
D14-1134,Q13-1015,0,0.0213264,"commonly used formalism for semantic parsing – the task of mapping natural language sentences to formal meaning representations (Zelle and Mooney, 1996). Recently, CCG semantic parsers have been used for numerous language understanding tasks, including querying databases (Zettlemoyer and Collins, 2005), referring to physical objects (Matuszek et al., 2012), information extraction (Krishnamurthy and Mitchell, 2012), executing instructions (Artzi and Zettlemoyer, 2013b), generating regular expressions (Kushman and Barzilay, 2013), question-answering (Cai and Yates, 2013) and textual entailment (Lewis and Steedman, 2013). In CCG, a lexicon is used to map words to formal representations of their meaning, which are then combined using bottom-up operations. In this paper we present learning techniques ∗ This research was carried out at Google. to explicitly control the size of the CCG lexicon, and show that this results in improved task performance and more compact models. In most approaches for inducing CCGs for semantic parsing, lexicon learning and parameter estimation are performed jointly in an online algorithm, as introduced by Zettlemoyer and Collins (2007). To induce the lexicon, words extracted from the"
D14-1134,N06-1056,0,0.0579694,"otal computational cost of our approach is roughly equivalent to online approaches. In both approaches, each pass over the data makes the same number of inference calls, and in practice, Artzi and Zettlemoyer (2013b) used 6-8 iterations for online learning while we used 10. A benefit of the batch method is its insensitivity to data ordering, as expressed by the lower standard deviation between randomized runs in Table 2.6 7 Related Work There has been significant work on learning for semantic parsing. The majority of approaches treat grammar induction and parameter estimation separately, e.g. Wong and Mooney (2006), Kate and Mooney (2006), Clarke et al. (2010), Goldwasser et al. (2011), Goldwasser and Roth (2011), Liang 6 Results still vary slightly due to multi-threading. et al. (2011), Chen and Mooney (2011), and Chen (2012). In all these approaches the grammar structure is fixed prior to parameter estimation. Zettlemoyer and Collins (2005) proposed the learning regime most related to ours. Their learner alternates between batch lexical induction and online parameter estimation. Our learning algorithm design combines aspects of previously studied approaches into a batch method, including gradient upda"
D14-1134,D07-1071,0,0.0217413,"swering (Cai and Yates, 2013) and textual entailment (Lewis and Steedman, 2013). In CCG, a lexicon is used to map words to formal representations of their meaning, which are then combined using bottom-up operations. In this paper we present learning techniques ∗ This research was carried out at Google. to explicitly control the size of the CCG lexicon, and show that this results in improved task performance and more compact models. In most approaches for inducing CCGs for semantic parsing, lexicon learning and parameter estimation are performed jointly in an online algorithm, as introduced by Zettlemoyer and Collins (2007). To induce the lexicon, words extracted from the training data are paired with CCG categories one sample at a time (for an overview of CCG, see §2). Joint approaches have the potential advantage that only entries participating in successful parses are added to the lexicon. However, new entries are added greedily and these decisions are never revisited at later stages. In practice, this often results in a large and noisy lexicon. Figure 1 lists a sample of CCG lexical entries learned for the word chair with a greedy joint algorithm (Artzi and Zettlemoyer, 2013b). In the studied navigation doma"
D15-1159,W09-1207,0,0.0536797,"Missing"
D15-1159,D12-1133,0,0.697074,"-of-words style) feature for 1 There is of course a much longer tradition of neural network dependency parsing models, going back at least to Titov and Henderson (2007). each word’s morphological attributes, and (2) a weighted set-valued feature for each word’s k-best POS tags. These features can be integrated naturally as atomic inputs to the embedding layer of the network and the model can learn arbitrary conjunctions with all other features through the hidden layers. In contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking. For example, Bohnet and Nivre (2012) had to carefully discretize the real-valued POS tag score in order to combine it with the other discrete binary features in their system. Additionally, we also experiment with different transition systems, most notably the integrated parsing and part-of-speech (POS) tagging system of Bohnet and Nivre (2012) and also the swap system of Nivre (2009). We evaluate our parser on the CoNLL ’09 shared task dependency treebanks, as well as on two English setups, achieving the best published numbers in many cases. 2 Model In this section, we review the baseline model, and then introduce the features ("
D15-1159,Q13-1034,0,0.0357995,"Missing"
D15-1159,W09-1210,0,0.092806,"Missing"
D15-1159,C10-1011,0,0.0368526,"Missing"
D15-1159,de-marneffe-etal-2006-generating,0,0.121505,"Missing"
D15-1159,P15-1033,0,0.246235,"Missing"
D15-1159,W09-1205,0,0.101624,"Missing"
D15-1159,N06-2015,0,0.0457254,"Missing"
D15-1159,P06-1063,0,0.0252688,"Missing"
D15-1159,P14-1130,0,0.087608,"Missing"
D15-1159,J93-2004,0,0.0503999,"Missing"
D15-1159,P13-2109,0,0.0946857,"Missing"
D15-1159,J08-4003,0,0.0491355,"gical properties and part-ofspeech confusion sets of the words being parsed. We also investigate the use of joint parsing and partof-speech tagging in the neural paradigm. Finally, we conduct a multi-lingual evaluation that demonstrates the robustness of the overall structured neural approach, as well as the benefits of the extensions proposed in this work. Our research further demonstrates the breadth of the applicability of neural network methods to dependency parsing, as well as the ease with which new features can be added to neural parsing models. 1 Introduction Transition-based parsers (Nivre, 2008) are extremely popular because of their high accuracy and speed. Inspired by the greedy neural network transition-based parser of Chen and Manning (2014), Weiss et al. (2015) and Zhou et al. (2015) concurrently developed structured neural network parsers that use beam search and achieve state-of-the-art accuracies for English dependency parsing.1 While very successful, these parsers have made use only of a small fraction of the rich options provided inside the transition-based framework: for example, all of these parsers use virtually identical atomic features and the arcstandard transition sy"
D15-1159,P09-1040,0,0.129181,"of the network and the model can learn arbitrary conjunctions with all other features through the hidden layers. In contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking. For example, Bohnet and Nivre (2012) had to carefully discretize the real-valued POS tag score in order to combine it with the other discrete binary features in their system. Additionally, we also experiment with different transition systems, most notably the integrated parsing and part-of-speech (POS) tagging system of Bohnet and Nivre (2012) and also the swap system of Nivre (2009). We evaluate our parser on the CoNLL ’09 shared task dependency treebanks, as well as on two English setups, achieving the best published numbers in many cases. 2 Model In this section, we review the baseline model, and then introduce the features (which are novel) and the transition systems (taken from existing work) that we propose as extensions. We measure the impact of each proposed change on the development sets of the multi-lingual CoNLL ’09 shared task treebanks (Hajiˇc et al., 2009). For details on our experimental setup, see Section 3. 2.1 Baseline Model Our baseline model is the str"
D15-1159,W09-1215,0,0.167979,"Missing"
D15-1159,P07-1080,0,0.00776618,"accuracies for English dependency parsing.1 While very successful, these parsers have made use only of a small fraction of the rich options provided inside the transition-based framework: for example, all of these parsers use virtually identical atomic features and the arcstandard transition system. In this paper we extend this line of work and introduce two new types of features that significantly improve parsing performance: (1) a set-valued (i.e., bag-of-words style) feature for 1 There is of course a much longer tradition of neural network dependency parsing models, going back at least to Titov and Henderson (2007). each word’s morphological attributes, and (2) a weighted set-valued feature for each word’s k-best POS tags. These features can be integrated naturally as atomic inputs to the embedding layer of the network and the model can learn arbitrary conjunctions with all other features through the hidden layers. In contrast, integrating such features into a model with discrete features requires nontrivial manual tweaking. For example, Bohnet and Nivre (2012) had to carefully discretize the real-valued POS tag score in order to combine it with the other discrete binary features in their system. Additi"
D15-1159,N03-1033,0,0.49419,"Missing"
D15-1159,P15-1032,1,0.64973,"igm. Finally, we conduct a multi-lingual evaluation that demonstrates the robustness of the overall structured neural approach, as well as the benefits of the extensions proposed in this work. Our research further demonstrates the breadth of the applicability of neural network methods to dependency parsing, as well as the ease with which new features can be added to neural parsing models. 1 Introduction Transition-based parsers (Nivre, 2008) are extremely popular because of their high accuracy and speed. Inspired by the greedy neural network transition-based parser of Chen and Manning (2014), Weiss et al. (2015) and Zhou et al. (2015) concurrently developed structured neural network parsers that use beam search and achieve state-of-the-art accuracies for English dependency parsing.1 While very successful, these parsers have made use only of a small fraction of the rich options provided inside the transition-based framework: for example, all of these parsers use virtually identical atomic features and the arcstandard transition system. In this paper we extend this line of work and introduce two new types of features that significantly improve parsing performance: (1) a set-valued (i.e., bag-of-words s"
D15-1159,P14-2107,0,0.226566,"Missing"
D15-1159,P11-2033,0,0.0460733,"Missing"
D15-1159,P15-1117,0,0.075381,"a multi-lingual evaluation that demonstrates the robustness of the overall structured neural approach, as well as the benefits of the extensions proposed in this work. Our research further demonstrates the breadth of the applicability of neural network methods to dependency parsing, as well as the ease with which new features can be added to neural parsing models. 1 Introduction Transition-based parsers (Nivre, 2008) are extremely popular because of their high accuracy and speed. Inspired by the greedy neural network transition-based parser of Chen and Manning (2014), Weiss et al. (2015) and Zhou et al. (2015) concurrently developed structured neural network parsers that use beam search and achieve state-of-the-art accuracies for English dependency parsing.1 While very successful, these parsers have made use only of a small fraction of the rich options provided inside the transition-based framework: for example, all of these parsers use virtually identical atomic features and the arcstandard transition system. In this paper we extend this line of work and introduce two new types of features that significantly improve parsing performance: (1) a set-valued (i.e., bag-of-words style) feature for 1 The"
D15-1159,E12-1009,0,\N,Missing
D15-1159,W09-1201,0,\N,Missing
D15-1159,D14-1082,0,\N,Missing
D17-1009,E17-2039,0,0.0131179,"Missing"
D17-1009,P15-1039,0,0.0304249,"ttempts at extracting semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded repres"
D17-1009,P14-1133,0,0.21499,"Missing"
D17-1009,Q15-1039,0,0.154417,"Missing"
D17-1009,W13-3520,0,0.0208057,"ty linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input to G RAPH PARSER, leaving the final disambiguation to the semantic parsing problem. Table 2 shows the 1-best and 10-best entity disambiguation F1 -scores for each language and dataset. Features We use features similar to Reddy et al. (2016): basic features of words and Freebase relations, and graph features crossing ungrounded events wi"
D17-1009,W09-1206,0,0.0126084,"e English-specific system of Reddy et al. (2016). We attribute the 0.8 point difference in F1 score to their use of the more fine-grained PTB tag set and Stanford Dependencies. CCGG RAPH This is the CCG-based semantic representation of Reddy et al. (2014). Note that this baseline exists only for English. UD EP L AMBDA SRL This is similar to UD EP L AMBDA except that instead of assuming nsubj, dobj and nsubjpass correspond to arg1 , arg2 and arg2 , we employ semantic role labeling to identify the correct interpretation. We used the systems of Roth and Woodsend (2014) for English and German and Bjrkelund et al. (2009) for Spanish trained on the CoNLL-2009 dataset (Haji et al., 2009).8 4.5 GraphQ. WebQ. Results Table 3 shows the performance of G RAPH PARSER with these different representations. Here and in what follows, we use average F1 -score of predicted answers (Berant et al., 2013) as the evaluation metric. We first observe that UD EP L AMBDA consistently outperforms the S INGLE E VENT and D EP T REE representations in all languages. For English, performance is on par with CCGG RAPH, which suggests that UD EP L AMBDA does not sacrifice too much specificity for universality. With both datasets, results"
D17-1009,P02-1041,0,0.341625,"Missing"
D17-1009,C04-1180,1,0.821524,"Missing"
D17-1009,W13-2322,0,0.0191232,", some and most, negation markers like no and not, and intentional verbs like believe and said. UD does not have special labels to indicate these. We discuss how to handle quantifiers in this framework in the supplementary material. Although in the current setup UD EP L AMBDA rules are hand-coded, the number of rules are only proportional to the number of UD labels, making rule-writing manageable.5 Moreover, we view UD EP L AMBDA as a first step towards learning rules for converting UD to richer semantic representations such as PropBank, AMR, or the Parallel Meaning Bank (Palmer et al., 2005; Banarescu et al., 2013; Abzianidze et al., 2017).. 4 Semantic Parsing as Graph Matching Cross-lingual Semantic Parsing To study the multilingual nature of UD EP L AMBDA, we conduct an empirical evaluation on question answering against Freebase in three different languages: English, Spanish, and German. Before discussing the details of this experiment, we briefly outline the semantic parsing framework employed. 4.2 Datasets We evaluate our approach on two public benchmarks of question answering against Freebase: WebQuestions (Berant et al., 2013), a widely used benchmark consisting of English questions and their ans"
D17-1009,W02-1001,0,0.0458946,"problem with the goal of finding the Freebase graphs that are structurally isomorphic to an ungrounded graph and rank them according to a model. To account for structural mismatches, G RAPH PARSER uses two graph transformations: CONTRACT and EXPAND . In Figure 3(a) there are two edges between x and Ghana. CONTRACT collapses one of these edges to create a graph isomorphic to Freebase. EXPAND, in contrast, adds edges to connect the graph in the case of disconnected components. The search space is explored by beam search and model parameters are estimated with the averaged structured perceptron (Collins, 2002) from training data consisting of question-answer pairs, using answer F1 -score as the objective. Other constructions that require lexical information are quantifiers like every, some and most, negation markers like no and not, and intentional verbs like believe and said. UD does not have special labels to indicate these. We discuss how to handle quantifiers in this framework in the supplementary material. Although in the current setup UD EP L AMBDA rules are hand-coded, the number of rules are only proportional to the number of UD labels, making rule-writing manageable.5 Moreover, we view UD"
D17-1009,W15-0128,0,0.0171085,"2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in"
D17-1009,D15-1127,0,0.0138165,"Missing"
D17-1009,C16-1056,0,0.096081,"ave mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurth"
D17-1009,jakob-etal-2010-mapping,0,0.0578418,"Missing"
D17-1009,D16-1086,0,0.0201594,"emantic types coupled with strong type constraints, which can be very informative but unavoidably language specific. Instead, UD EP L AMBDA relies on generic unlexicalized information present in dependency treebanks and uses a simple type system (one type for dependency labels, and one for words) along with a combinatory mechanism, which avoids type collisions. Earlier attempts at extracting semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) l"
D17-1009,C14-1122,0,0.0632483,"Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this framework can benefit ap"
D17-1009,P14-1134,0,0.024364,"for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to li"
D17-1009,P12-1051,0,0.0794212,"and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this frame"
D17-1009,E03-1030,0,0.105881,"Missing"
D17-1009,P15-1143,0,0.0127531,"tly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common"
D17-1009,N16-1088,0,0.0302269,"motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched"
D17-1009,Q16-1023,0,0.0120801,"stions de es 82.8 91.2 86.7 94.0 GraphQuestions en de es 47.2 56.9 39.9 48.4 39.5 51.6 Here we provide details on the syntactic analyzers employed, our entity resolution algorithm, and the features used by the grounding model. Table 2: Structured perceptron k-best entity linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input to G RAPH PARSER, leaving the final disambiguation to the semantic parsing prob"
D17-1009,Q15-1019,0,0.0329059,"d Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012"
D17-1009,D10-1119,1,0.865489,"al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to ex"
D17-1009,D13-1161,0,0.0259018,"nde et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less)"
D17-1009,levy-andrew-2006-tregex,0,0.0175027,"the in English have different semantics, despite being both determiners) and are not encoded in the UD schema. Furthermore, some cross-linguistic phenomena, such as long-distance dependencies, are not part of the core UD representation. To circumvent this limitation, a simple enhancement step enriches the original UD representation before binarization takes place (Section 3.1). This step adds to the dependency tree missing syntactic information and long-distance dependencies, thereby creating a graph. Whereas D EP L AMBDA is not able to handle graph-structured input, UD EP - 2 We use Tregex (Levy and Andrew, 2006) for substitution mappings and Cornell SPF (Artzi, 2013) as the lambdacalculus implementation. For example, in running horse, the tregex /label:amod/=target < /postag:verb/ matches amod to its INVERT expression λ f gx. ∃y. f (x) ∧ g(y) ∧ amodi (ye , xa ). 3 In what follows, all references to UD are to UD v1.3. 91 L AMBDA is designed to work with dependency graphs as well (Section 3.2). Finally, several constructions differ in structure between UD and SD, which requires different handling in the semantic interface (Section 3.3). 3.1 xcomp nsubj Anna wants to dobj marry Kristoff nsubj (a) With l"
D17-1009,N15-1114,0,0.00825081,"P L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this framework can benefit applications such as summarization (Liu et al., 2015) and machine reading (Sachan and Xing, 2016). Acknowledgements This work greatly benefited from discussions with Michael Collins, Dipanjan Das, Federico Fancellu, Julia Hockenmaier, Tom Kwiatkowski, Adam Lopez, Valeria de Paiva, Martha Palmer, Fernando Pereira, Emily Pitler, Vijay Saraswat, Nathan Schneider, Bonnie Webber, Luke Zettlemoyer, and the members of ILCC Edinburgh University, the Microsoft Research Redmond NLP group, the Stanford NLP group, and the UW NLP and Linguistics group. We thank Reviewer 2 for useful feedback. The authors would also like to thank the Universal Dependencies co"
D17-1009,Q14-1030,1,0.830946,"ikipedia, or SimpleQuestions (Bordes et al., 2015) are shown in parentheses. On GraphQuestions, we achieve a new state-of-the-art result with a gain of 4.8 F1 points over the previously reported best result. On WebQuestions we are 2.1 points below the best model using comparable resources, and 3.8 points below the state of the art. Most related to our work is the English-specific system of Reddy et al. (2016). We attribute the 0.8 point difference in F1 score to their use of the more fine-grained PTB tag set and Stanford Dependencies. CCGG RAPH This is the CCG-based semantic representation of Reddy et al. (2014). Note that this baseline exists only for English. UD EP L AMBDA SRL This is similar to UD EP L AMBDA except that instead of assuming nsubj, dobj and nsubjpass correspond to arg1 , arg2 and arg2 , we employ semantic role labeling to identify the correct interpretation. We used the systems of Roth and Woodsend (2014) for English and German and Bjrkelund et al. (2009) for Spanish trained on the CoNLL-2009 dataset (Haji et al., 2009).8 4.5 GraphQ. WebQ. Results Table 3 shows the performance of G RAPH PARSER with these different representations. Here and in what follows, we use average F1 -score o"
D17-1009,J93-2004,0,0.0647121,"ng-distance dependencies in relative clauses and control constructions. We follow Schuster and Manning (2016) and find these using the labels acl (relative) and xcomp (control). Figure 2(a) shows the long-distance dependency in the sentence Anna wants to marry Kristoff. Here, marry is provided with its missing nsubj (dashed arc). Second, UD conflates all coordinating constructions to a single dependency label, conj. To obtain the correct coordination scope, we refine conj to conj:verb, conj:vp, conj:sentence, conj:np, and conj:adj, similar to Reddy et al. (2016). Finally, unlike the PTB tags (Marcus et al., 1993) used by SD, the UD part-of-speech tags do not distinguish question words. Since these are crucial to question-answering, we use a small lexicon to refine the tags for determiners (DET), adverbs (ADV) and pronouns (PRON) to DET: WH, ADV: WH and PRON : WH, respectively. Specifically, we use a list of 12 (English), 14 (Spanish) and 35 (German) words, respectively. This is the only part of UD EP L AMBDA that relies on language-specific information. We hope that, as the coverage of morphological features in UD improves, this refinement can be replaced by relying on morphological features, such as"
D17-1009,D14-1045,0,0.0131176,"the state of the art. Most related to our work is the English-specific system of Reddy et al. (2016). We attribute the 0.8 point difference in F1 score to their use of the more fine-grained PTB tag set and Stanford Dependencies. CCGG RAPH This is the CCG-based semantic representation of Reddy et al. (2014). Note that this baseline exists only for English. UD EP L AMBDA SRL This is similar to UD EP L AMBDA except that instead of assuming nsubj, dobj and nsubjpass correspond to arg1 , arg2 and arg2 , we employ semantic role labeling to identify the correct interpretation. We used the systems of Roth and Woodsend (2014) for English and German and Bjrkelund et al. (2009) for Spanish trained on the CoNLL-2009 dataset (Haji et al., 2009).8 4.5 GraphQ. WebQ. Results Table 3 shows the performance of G RAPH PARSER with these different representations. Here and in what follows, we use average F1 -score of predicted answers (Berant et al., 2013) as the evaluation metric. We first observe that UD EP L AMBDA consistently outperforms the S INGLE E VENT and D EP T REE representations in all languages. For English, performance is on par with CCGG RAPH, which suggests that UD EP L AMBDA does not sacrifice too much specifi"
D17-1009,P16-2079,0,0.0223299,"versal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this framework can benefit applications such as summarization (Liu et al., 2015) and machine reading (Sachan and Xing, 2016). Acknowledgements This work greatly benefited from discussions with Michael Collins, Dipanjan Das, Federico Fancellu, Julia Hockenmaier, Tom Kwiatkowski, Adam Lopez, Valeria de Paiva, Martha Palmer, Fernando Pereira, Emily Pitler, Vijay Saraswat, Nathan Schneider, Bonnie Webber, Luke Zettlemoyer, and the members of ILCC Edinburgh University, the Microsoft Research Redmond NLP group, the Stanford NLP group, and the UW NLP and Linguistics group. We thank Reviewer 2 for useful feedback. The authors would also like to thank the Universal Dependencies community for the treebanks and documentation."
D17-1009,L16-1376,0,0.089339,"(Artzi, 2013) as the lambdacalculus implementation. For example, in running horse, the tregex /label:amod/=target < /postag:verb/ matches amod to its INVERT expression λ f gx. ∃y. f (x) ∧ g(y) ∧ amodi (ye , xa ). 3 In what follows, all references to UD are to UD v1.3. 91 L AMBDA is designed to work with dependency graphs as well (Section 3.2). Finally, several constructions differ in structure between UD and SD, which requires different handling in the semantic interface (Section 3.3). 3.1 xcomp nsubj Anna wants to dobj marry Kristoff nsubj (a) With long-distance dependency. Enhancement Both Schuster and Manning (2016) and Nivre et al. (2016) note the necessity of an enhanced UD representation to enable semantic applications. However, such enhancements are currently only available for a subset of languages in UD. Instead, we rely on a small number of enhancements for our main application—semantic parsing for questionanswering—with the hope that this step can be replaced by an enhanced UD representation in the future. Specifically, we define three kinds of enhancements: (1) long-distance dependencies; (2) types of coordination; and (3) refined question word tags. These correspond to line 2 in Algorithm 1. Fi"
D17-1009,J05-1004,0,0.0829545,"uantifiers like every, some and most, negation markers like no and not, and intentional verbs like believe and said. UD does not have special labels to indicate these. We discuss how to handle quantifiers in this framework in the supplementary material. Although in the current setup UD EP L AMBDA rules are hand-coded, the number of rules are only proportional to the number of UD labels, making rule-writing manageable.5 Moreover, we view UD EP L AMBDA as a first step towards learning rules for converting UD to richer semantic representations such as PropBank, AMR, or the Parallel Meaning Bank (Palmer et al., 2005; Banarescu et al., 2013; Abzianidze et al., 2017).. 4 Semantic Parsing as Graph Matching Cross-lingual Semantic Parsing To study the multilingual nature of UD EP L AMBDA, we conduct an empirical evaluation on question answering against Freebase in three different languages: English, Spanish, and German. Before discussing the details of this experiment, we briefly outline the semantic parsing framework employed. 4.2 Datasets We evaluate our approach on two public benchmarks of question answering against Freebase: WebQuestions (Berant et al., 2013), a widely used benchmark consisting of English"
D17-1009,P15-1142,0,0.0101447,"in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multi"
D17-1009,D14-1162,0,0.116047,"tic analyzers employed, our entity resolution algorithm, and the features used by the grounding model. Table 2: Structured perceptron k-best entity linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input to G RAPH PARSER, leaving the final disambiguation to the semantic parsing problem. Table 2 shows the 1-best and 10-best entity disambiguation F1 -scores for each language and dataset. Features We"
D17-1009,P16-2067,0,0.0225596,"ir translations. k en 1 10 Implementation Details 89.6 95.7 WebQuestions de es 82.8 91.2 86.7 94.0 GraphQuestions en de es 47.2 56.9 39.9 48.4 39.5 51.6 Here we provide details on the syntactic analyzers employed, our entity resolution algorithm, and the features used by the grounding model. Table 2: Structured perceptron k-best entity linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input t"
D17-1009,R11-1065,0,0.0249782,"ntic interfaces is the reliance on rich typed feature structures or semantic types coupled with strong type constraints, which can be very informative but unavoidably language specific. Instead, UD EP L AMBDA relies on generic unlexicalized information present in dependency treebanks and uses a simple type system (one type for dependency labels, and one for words) along with a combinatory mechanism, which avoids type collisions. Earlier attempts at extracting semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base"
D17-1009,D16-1054,0,0.127651,"uctions such as control. The different treatments of various linguistic constructions in UD compared to SD also require different handling in UD EP L AMBDA (Section 3.3). Our experiments focus on Freebase semantic parsing as a testbed for evaluating the framework’s multilingual appeal. We convert natural language to logical forms which in turn are converted to machine interpretable formal meaning representations for retrieving answers to questions from Freebase. To facilitate multilingual evaluation, we provide translations of the English WebQuestions (Berant et al., 2013) and GraphQuestions (Su et al., 2016) datasets to German and Spanish. We demonstrate that UD EP L AMBDA can be used to derive logical forms for these languages using a minimal amount of language-specific knowledge. Aside from developing the first multilingual semantic parsing tool for Freebase, we also experimentally show that UD EP L AMBDA outperforms strong baselines across Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. Howe"
D17-1009,N15-3006,0,0.0206547,"g semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et a"
D17-1009,D16-1177,0,0.0268016,"Missing"
D17-1009,P16-1220,1,0.172318,"Missing"
D17-1009,N15-3014,0,0.0204028,"Missing"
D17-1009,P14-1090,0,0.131516,"Missing"
D17-1009,D16-1015,0,0.0546486,"Missing"
D17-1009,P15-1128,0,0.0788276,"Missing"
D17-1309,P16-1039,0,0.0399199,"Missing"
D17-1309,D14-1082,0,0.251491,"ults at or near the state-of-the-art on a variety of natural language processing tasks, with an order of magnitude speedup over an LSTM-based approach. We begin by introducing the network model structure and the character-based representations we use throughout all tasks (§2). The four tasks that we address are: language identification (LangID), part-of-speech (POS) tagging, word segmentation, and preordering for translation. In order to use feed-forward networks for structured prediction tasks, we use transition systems (Titov and Henderson, 2007, 2010) with feature embeddings as proposed by Chen and Manning (2014), and introduce two novel transition systems for the last two tasks. We focus on budgeted models and ablate four techniques (one on each task) for improving accuracy for a given memory budget: 1. Quantization: Using more dimensions and less precision (Lang-ID: §3.1). 2. Word clusters: Reducing the network size to allow for word clusters and derived features (POS tagging: §3.2). 3. Selected features: Adding explicit feature conjunctions (segmentation: §3.3). 4. Pipelines: Introducing another task in a pipeline and allocating parameters to the auxiliary task instead (preordering: §3.4). We achie"
D17-1309,P05-1066,0,0.0288102,"Missing"
D17-1309,W08-0804,0,0.125718,"s applications of this network structure used (pretrained) word embeddings to represent words (Chen and Manning, 2014; Weiss et al., 2015). However, for word embeddings to be effective, they usually need to cover large vocabularies (100,000+) and dimensions (50+). Inspired by the success of character-based representations (Ling et al., 2015), we use features defined over character n-grams instead of relying on word embeddings, and learn their embeddings from scratch. We use a distinct feature group g for each ngram length N , and control the size Vg directly by applying random feature mixing (Ganchev and Dredze, 2008). That is, we define the feature value v for an n-gram string x as v = H(x) mod Vg , where H is a well-behaved hash function. Typical values for Vg are in the 100-5000 range, which is far smaller than the exponential number of unique raw n-grams. A consequence of these small feature vocabularies is that we can also use small feature embeddings, typically Dg =16. ⨁ Memory needs P are dominated by the embedding matrix sizes ( g Vg Dg , where Vg and Dg are the vocabulary sizes and dimensions respectively for each feature group g), while runtime is strongly influenced by the hidden layer dimension"
D17-1309,N16-1155,0,0.124158,"translation models have been able to attain impressive accuracies, with models that use hundreds of millions (Bahdanau et al., 2014; Wu et al., 2016) or billions (Shazeer et al., 2017) of parameters. These models, however, may not be feasible in all computational settings. In particular, models running on mobile devices are often constrained in terms of memory and computation. Long Short-Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997) have achieved good results with small memory footprints by using character-based input representations: e.g., the part-of-speech tagging models of Gillick et al. (2016) have only roughly 900,000 parameters. Latency, however, can still be an issue with LSTMs, due to the large number of matrix multiplications they require (eight per LSTM cell): Kim and Rush (2016) report speeds of only 8.8 words/second when running a two-layer LSTM translation system on an Android phone. Feed-forward neural networks have the potential to be much faster. In this paper, we show that small feed-forward networks can achieve results at or near the state-of-the-art on a variety of natural language processing tasks, with an order of magnitude speedup over an LSTM-based approach. We b"
D17-1309,N15-1105,0,0.123253,"Missing"
D17-1309,D16-1139,0,0.0228959,"s. These models, however, may not be feasible in all computational settings. In particular, models running on mobile devices are often constrained in terms of memory and computation. Long Short-Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997) have achieved good results with small memory footprints by using character-based input representations: e.g., the part-of-speech tagging models of Gillick et al. (2016) have only roughly 900,000 parameters. Latency, however, can still be an issue with LSTMs, due to the large number of matrix multiplications they require (eight per LSTM cell): Kim and Rush (2016) report speeds of only 8.8 words/second when running a two-layer LSTM translation system on an Android phone. Feed-forward neural networks have the potential to be much faster. In this paper, we show that small feed-forward networks can achieve results at or near the state-of-the-art on a variety of natural language processing tasks, with an order of magnitude speedup over an LSTM-based approach. We begin by introducing the network model structure and the character-based representations we use throughout all tasks (§2). The four tasks that we address are: language identification (LangID), part"
D17-1309,P08-1068,0,0.150947,"Missing"
D17-1309,D15-1176,0,0.100219,"Missing"
D17-1309,W16-4801,0,0.0687806,"Missing"
D17-1309,W16-5805,0,0.0415107,"rk given an embedding vector h0 . This cost is dominated by the matrix multiplications to compute (unscaled) activation unit values, hence our metric excludes the non-linearities and softmax normal2880 Model Baldwin and Lui (2010): NN Baldwin and Lui (2010): NP Small FF, 6 dim Small FF, 16 dim Small FF, 16 dim, quantized ization, but still accounts for the final layer logits. To ground this metric, we also provide indicative absolute speeds for each task, as measured on a modern workstation CPU (3.50GHz Intel Xeon E5-1650 v3). 3.1 Language Identification Recent shared tasks on code-switching (Molina et al., 2016) and dialects (Malmasi et al., 2016) have generated renewed interest in language identification. We restrict our focus to single language identification across diverse languages, and compare to the work of Baldwin and Lui (2010) on predicting the language of Wikipedia text in 66 languages. For this task, we obtain the input h0 by separately averaging the embeddings for each ngram length (N = [1, 4]), as summation did not produce good results. Table 1 shows that we outperform the lowmemory nearest-prototype model of Baldwin and Lui (2010). Their nearest neighbor model is the most accurate but i"
D17-1309,P15-1021,0,0.048914,"al. (2016)-combo’ in Table 4). However, such embeddings could easily lead to a model size explosion and thus are not considered in this work. The results in Table 4 show that spending our memory budget on small bigram embeddings is more effective than on larger character embeddings, in terms of both accuracy and model size. Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second. 3.4 Preordering Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015). We propose a novel transition system for this task (Table 5), so that we can repeatedly apply a small network to produce these permutations. Inspired by a non-projective parsing transition system (Nivre, 2009), the system uses a SWAP action to permute spans. The system is sound for permutations: any derivation will end with all of the input words in a permuted order, and complete: all permutations are reachable (use SHIFT and SWAP operations to perform a bubble sort, then APPEND n − 1 times to form a single span). For training and evaluation, we use the English-Japa"
D17-1309,P09-1040,0,0.0229685,"fective than on larger character embeddings, in terms of both accuracy and model size. Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second. 3.4 Preordering Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015). We propose a novel transition system for this task (Table 5), so that we can repeatedly apply a small network to produce these permutations. Inspired by a non-projective parsing transition system (Nivre, 2009), the system uses a SWAP action to permute spans. The system is sound for permutations: any derivation will end with all of the input words in a permuted order, and complete: all permutations are reachable (use SHIFT and SWAP operations to perform a bubble sort, then APPEND n − 1 times to form a single span). For training and evaluation, we use the English-Japanese manual word alignments from Nakagawa (2015). 2882 Model Nakagawa (2015) Small FF Small FF + POS tags Small FF + Tagger input fts. FRS 81.6 75.2 81.3 76.6 Size 0.5MB 1.3MB 3.7MB References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua"
D17-1309,P14-1028,0,0.025929,"pplementary material. Accuracy 95.01 95.95 94.24 94.16 95.18 Size − − 846KB 3.2MB 2.0MB Table 4: Segmentation results. Explicit bigrams are useful. Transition Precondition A PPEND ([σ|i|j], [β]) → ([σ|[ij]], [β]) S HIFT ([σ], [i|β]) → ([σ|i], [β]) S WAP ([σ|i|j], [β]) → [σ|j], [i|β]); i &lt; j Table 5: Preordering Transition system. Initially all words are part of singleton spans on the buffer: ([], [[w1 ][w2 ]...[wn ]]). In the final state the buffer is empty and the stack contains a single span. shown that explicitly using character bigram features leads to better accuracy (Zhang et al., 2016; Pei et al., 2014). Zhang et al. (2016) suggests that embedding manually specified feature conjunctions further improves accuracy (‘Zhang et al. (2016)-combo’ in Table 4). However, such embeddings could easily lead to a model size explosion and thus are not considered in this work. The results in Table 4 show that spending our memory budget on small bigram embeddings is more effective than on larger character embeddings, in terms of both accuracy and model size. Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second. 3.4 Preordering Preordering source-side words into the target-side"
D17-1309,W11-2102,0,0.0300358,"Missing"
D17-1309,D07-1099,0,0.13293,"In this paper, we show that small feed-forward networks can achieve results at or near the state-of-the-art on a variety of natural language processing tasks, with an order of magnitude speedup over an LSTM-based approach. We begin by introducing the network model structure and the character-based representations we use throughout all tasks (§2). The four tasks that we address are: language identification (LangID), part-of-speech (POS) tagging, word segmentation, and preordering for translation. In order to use feed-forward networks for structured prediction tasks, we use transition systems (Titov and Henderson, 2007, 2010) with feature embeddings as proposed by Chen and Manning (2014), and introduce two novel transition systems for the last two tasks. We focus on budgeted models and ablate four techniques (one on each task) for improving accuracy for a given memory budget: 1. Quantization: Using more dimensions and less precision (Lang-ID: §3.1). 2. Word clusters: Reducing the network size to allow for word clusters and derived features (POS tagging: §3.2). 3. Selected features: Adding explicit feature conjunctions (segmentation: §3.3). 4. Pipelines: Introducing another task in a pipeline and allocating"
D17-1309,P10-1040,0,0.188532,"Missing"
D17-1309,P15-1032,1,0.867077,"rams ), with one embedding matrix Eg ∈ RVg ×Dg per group. h1 h0 ⨁ Quantization A commonly used strategy for compressing neural networks is quantization, using less precision to store parameters (Han et al., 2015). We compress the embedding weights (the vast majority of the parameters for these shallow models) by storing scale factors for each embedding (details in the supplementary material). In §3.1, we contrast devoting model size to higher ⨁ Hashed Character n-grams Previous applications of this network structure used (pretrained) word embeddings to represent words (Chen and Manning, 2014; Weiss et al., 2015). However, for word embeddings to be effective, they usually need to cover large vocabularies (100,000+) and dimensions (50+). Inspired by the success of character-based representations (Ling et al., 2015), we use features defined over character n-grams instead of relying on word embeddings, and learn their embeddings from scratch. We use a distinct feature group g for each ngram length N , and control the size Vg directly by applying random feature mixing (Ganchev and Dredze, 2008). That is, we define the feature value v for an n-gram string x as v = H(x) mod Vg , where H is a well-behaved ha"
D17-1309,C04-1073,0,0.0181722,"ctions further improves accuracy (‘Zhang et al. (2016)-combo’ in Table 4). However, such embeddings could easily lead to a model size explosion and thus are not considered in this work. The results in Table 4 show that spending our memory budget on small bigram embeddings is more effective than on larger character embeddings, in terms of both accuracy and model size. Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second. 3.4 Preordering Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015). We propose a novel transition system for this task (Table 5), so that we can repeatedly apply a small network to produce these permutations. Inspired by a non-projective parsing transition system (Nivre, 2009), the system uses a SWAP action to permute spans. The system is sound for permutations: any derivation will end with all of the input words in a permuted order, and complete: all permutations are reachable (use SHIFT and SWAP operations to perform a bubble sort, then APPEND n − 1 times to form a single span). For training a"
D17-1309,P17-1078,0,0.0172196,"emory perspective, one multilingual BTS model will take less space than separate FF models. However, from a runtime perspective, a pipeline of our models doing language identification, word segmentation, and then POS tagging would still be faster than a single instance of the deep LSTM BTS model, by about 12x in our FLOPs estimate.4 3.3 Segmentation Word segmentation is critical for processing Asian languages where words are not explicitly separated by spaces. Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015). We use a structured model based on the transition system in Table 3, and similar to the one proposed by Zhang and Clark (2007). We conduct the segmentation experiments on the Chinese Treebank 6.0 with the recommended data splits. No external resources or pretrained embeddings are used. Hashing was detrimental to quality in our preliminary experiments, hence we do not use it for this task. To learn an embedding for unknown characters, we cast characters occurring only once in the training set to a special symbol. Selected Features Because we are not using hashing here, we"
D17-1309,P16-1040,0,0.0148556,"s detailed in the supplementary material. Accuracy 95.01 95.95 94.24 94.16 95.18 Size − − 846KB 3.2MB 2.0MB Table 4: Segmentation results. Explicit bigrams are useful. Transition Precondition A PPEND ([σ|i|j], [β]) → ([σ|[ij]], [β]) S HIFT ([σ], [i|β]) → ([σ|i], [β]) S WAP ([σ|i|j], [β]) → [σ|j], [i|β]); i &lt; j Table 5: Preordering Transition system. Initially all words are part of singleton spans on the buffer: ([], [[w1 ][w2 ]...[wn ]]). In the final state the buffer is empty and the stack contains a single span. shown that explicitly using character bigram features leads to better accuracy (Zhang et al., 2016; Pei et al., 2014). Zhang et al. (2016) suggests that embedding manually specified feature conjunctions further improves accuracy (‘Zhang et al. (2016)-combo’ in Table 4). However, such embeddings could easily lead to a model size explosion and thus are not considered in this work. The results in Table 4 show that spending our memory budget on small bigram embeddings is more effective than on larger character embeddings, in terms of both accuracy and model size. Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second. 3.4 Preordering Preordering source-side words in"
D17-1309,P07-1106,0,0.0152845,"our models doing language identification, word segmentation, and then POS tagging would still be faster than a single instance of the deep LSTM BTS model, by about 12x in our FLOPs estimate.4 3.3 Segmentation Word segmentation is critical for processing Asian languages where words are not explicitly separated by spaces. Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015). We use a structured model based on the transition system in Table 3, and similar to the one proposed by Zhang and Clark (2007). We conduct the segmentation experiments on the Chinese Treebank 6.0 with the recommended data splits. No external resources or pretrained embeddings are used. Hashing was detrimental to quality in our preliminary experiments, hence we do not use it for this task. To learn an embedding for unknown characters, we cast characters occurring only once in the training set to a special symbol. Selected Features Because we are not using hashing here, we need to be careful about the size of the input vocabulary. The neural network with its non-linearity is in theory able to learn bigrams by conjoinin"
D17-1309,N12-1052,1,\N,Missing
D17-1309,W14-3907,0,\N,Missing
D17-1309,W07-2218,0,\N,Missing
K17-3001,K17-3023,0,0.0375672,"Missing"
K17-3001,P16-1231,1,0.301678,"M Table 1: The supporting data overview: the number of words (M = million; K = thousand) for each language. http://commoncrawl.org/ Except for Ancient Greek, which was gathered from the Perseus Digital Library. 3 http://github.com/CLD2Owners/cld2 4 http://unicode.org/reports/tr15/ 3 verted to Unicode character NO-BREAK SPACE (U+00A0).5 The dimensionality of the word embeddings was chosen to be 100 after thorough discussion – more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that Andor et al. (2016) achieved state-of-the-art results with 64 dimensions. The word embeddings were precomputed using word2vec (Mikolov et al., 2013) with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line (Ginter et al., 2017). 2.3 this shared task, i.e., not included in any previous UD release. The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia;7 usually only a fe"
K17-3001,W06-2920,0,0.0145655,"categorization of the different approaches of the participating systems. Introduction Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to1 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked"
K17-3001,K17-3017,0,0.147208,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3005,0,0.0752704,"Missing"
K17-3001,K17-3026,0,0.0310687,"E 90.88 82.31 82.46 LyS-FASTPARSE 90.88 82.31 79.14 NAIST SATO 90.88 82.31 82.46 Orange – Deski˜n 90.88 38.81 15.38 UALING 90.88 82.31 82.46 UParse 90.88 82.31 82.46 naistCL 90.88 82.31 82.46 Table 5: Universal POS tags, features and lemmas (ordered by UPOS F1 scores). duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team (de Lhoneux et al., 2017) the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 ("
K17-3001,K17-3021,0,0.0954088,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3022,1,0.891655,"Missing"
K17-3001,K17-3025,0,0.0327614,"Missing"
K17-3001,K17-3024,0,0.050508,"Missing"
K17-3001,K17-3027,0,0.0537913,"Missing"
K17-3001,K17-3014,0,0.0756362,"Missing"
K17-3001,K17-3015,0,0.0745209,"Missing"
K17-3001,K17-3007,0,0.0511894,"Missing"
K17-3001,L16-1262,1,0.869327,"Missing"
K17-3001,W14-6111,0,0.0253686,"Missing"
K17-3001,W17-0411,1,0.831758,"ossible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants. Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison (Nivre and Fang, 2017). It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation. As CLAS is still experimental, we have designated full LAS as our main"
K17-3001,K17-3003,0,0.0845341,"Missing"
K17-3001,W17-0412,1,0.869806,"Missing"
K17-3001,L16-1680,1,0.0475333,"Missing"
K17-3001,K17-3009,1,0.104147,"Missing"
K17-3001,tiedemann-2012-parallel,0,0.0126153,"oses (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. 2.2 Supporting Data To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora. 1 Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects—for example SANCL (Petrov and McDonald, 2012) or SPMRL (S"
K17-3001,K17-3016,0,0.0605417,"Missing"
K17-3001,K17-3020,0,0.0375614,"Missing"
K17-3001,K17-3013,0,0.0456211,"Missing"
K17-3001,D07-1096,1,\N,Missing
K17-3001,K17-3002,1,\N,Missing
K17-3001,K17-3019,0,\N,Missing
K17-3001,K17-3012,1,\N,Missing
K17-3001,K17-3006,0,\N,Missing
K17-3001,K17-3010,0,\N,Missing
K17-3001,K17-3018,0,\N,Missing
K17-3001,K17-3028,1,\N,Missing
K17-3001,K17-3011,0,\N,Missing
K18-2001,K18-2015,0,0.053009,"Missing"
K18-2001,Q17-1010,0,0.211935,"Missing"
K18-2001,K18-2010,0,0.0386566,"Missing"
K18-2001,K18-2017,0,0.075361,"Missing"
K18-2001,W06-2920,0,0.453112,"Missing"
K18-2001,K18-2025,0,0.0365994,"Missing"
K18-2001,K18-2005,0,0.120251,"Missing"
K18-2001,K18-2013,1,0.806044,"Missing"
K18-2001,K18-2026,0,0.0321915,"Missing"
K18-2001,K18-2012,0,0.0235436,"above are all intrinsic measures: they evaluate the grammatical analysis task per se, with the hope that better scores correspond to output that is more useful for downstream NLP applications. Nevertheless, such correlations are not automatically granted. We thus seek to complement our task with an extrinsic evaluation, where the output of parsing systems is exploited by applications like biological event extraction, opinion analysis and negation scope resolution. This optional track involves English only. It is organized in collaboration with the EPE initiative;7 for details see Fares et al. (2018). Syntactic Word Alignment The higher segmentation level is based on the notion of syntactic word. Some languages contain multi-word tokens (MWT) that are regarded as contractions of multiple syntactic words. For example, the German token zum is a contraction of the preposition zu “to” and the article dem “the”. Syntactic words constitute independent nodes in dependency trees. As shown by the example, it is not required that the MWT is a pure concatenation of the participating words; the simple token alignment thus does not work when MWTs 4 TIRA: The System Submission Platform Similarly to our"
K18-2001,K18-2003,0,0.040574,"Missing"
K18-2001,K18-2006,0,0.0774162,"Missing"
K18-2001,K18-2014,0,0.0664725,"Missing"
K18-2001,K18-2008,0,0.0697052,"Missing"
K18-2001,L16-1262,1,0.910778,"Missing"
K18-2001,W17-0411,1,0.849881,"and in the system output before comparing them. In the end-to-end evaluation of our task, LAS is re-defined as the harmonic mean (F1 ) of precision P and recall R, where P = #correctRelations #systemNodes (1) R= #correctRelations #goldNodes (2) LAS = 2P R P +R (3) Note that attachment of all nodes including punctuation is evaluated. LAS is computed separately for each of the 82 test files and a macro-average of all these scores is used to rank the systems. 3.2 MLAS: Morphology-Aware Labeled Attachment Score MLAS aims at cross-linguistic comparability of the scores. It is an extension of CLAS (Nivre and Fang, 2017), which was tested experimentally in the 2017 task. CLAS focuses on dependencies between content words and disregards attachment of function words; in MLAS, function words are not ignored, but they are treated as features of content words. In addition, part-of-speech tags and morphological features are evaluated, too. 3.3 BLEX: Bilexical Dependency Score BLEX is similar to MLAS in that it focuses on relations between content words. Instead of morphological features, it incorporates lemmatization in the evaluation. It is thus closer to semantic content and evaluates two aspects of UD annota5 ar"
K18-2001,K18-2022,0,0.0296323,"Missing"
K18-2001,K18-2011,1,0.844373,"Missing"
K18-2001,W17-0412,1,0.901947,"Missing"
K18-2001,L16-1680,1,0.90044,"Missing"
K18-2001,K17-3009,1,0.858784,"Missing"
K18-2001,tiedemann-2012-parallel,0,0.0674866,"at follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers provided with large amounts of freely available data. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. We provided dependency-annotated training and test data, and also large quantities of crawled raw texts. Other language resources are available from third-party servers and we only referred to the respective download sites. 2.1 Training Data: UD 2.2 Training and development data came from the Universal Dependencies (UD) 2.2 collection (Nivre et al., 2018). This year, the official UD release immediately followed the test phase of the shared task. The training and development data were available to the"
K18-2001,K18-2016,0,0.0988933,"Missing"
K18-2001,K18-2019,0,0.110064,"Missing"
K18-2001,K18-2007,0,0.0602044,"Missing"
K18-2001,K18-2004,0,0.103154,"Missing"
L16-1262,W13-2308,0,0.0114723,"g diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Span"
L16-1262,W06-2920,0,0.443151,"clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies in the enhanced representation and miscellaneous information. The format is illustrated in Figure 3, with the French sentence from Figure 2. To support work on treebanks in this format,"
L16-1262,W09-2307,1,0.329071,"standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Ja"
L16-1262,P11-1061,1,0.573621,"UNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapt"
L16-1262,W08-1301,1,0.58058,"Missing"
L16-1262,de-marneffe-etal-2006-generating,1,0.213978,"Missing"
L16-1262,de-marneffe-etal-2014-universal,1,0.831309,"Missing"
L16-1262,E14-4028,0,0.0377832,"Missing"
L16-1262,N15-3011,1,0.696846,"Missing"
L16-1262,D07-1013,1,0.230402,"hocolat . le fille adorer le dessert a` le chocolat . DET NOUN VERB DET NOUN ADP DET NOUN PUNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged"
L16-1262,P13-2017,1,0.877648,"Missing"
L16-1262,W15-2127,0,0.0113361,"n English, we also obtain parallel representations between prepositional phrases and subordinate clauses, which are in practice often introduced by a preposition, as in (5). nmod case nsubj (5) a. Sue 1662 det left after the rehearsal advcl nsubj b. Sue Language mark nsubj left after we did The choice to make content words the backbone of the syntactic representations may seem to be at odds with the strong tendency in modern syntactic theory to give priority to functional heads, a tendency that is found in both constituency-based and dependency-based approaches to syntax (Brug´e et al., 2012; Osborne and Maxwell, 2015). We believe, however, that this conflict is more apparent than real. The UD view is that we need to recognize both lexical and functional heads, but in order to maximize parallelism across languages, only lexical heads are inferable from the topology of our tree structures. Functional heads are instead represented as specifying features of content words, using dedicated relation labels, features which can alternatively be specified through morphological processes. In the dependency grammar tradition, this is very close to the view of Tesni`ere (1959), according to whom dependencies hold betwe"
L16-1262,petrov-etal-2012-universal,1,0.717175,"exist to build consistent resources for many languages, and the UD project is a merger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we p"
L16-1262,rosa-etal-2014-hamledt,1,0.832586,"Missing"
L16-1262,L16-1376,1,0.208211,"fferent languages. For instance, while the universal UD scheme has a single relation acl for adnominal clauses, several languages make use of the subtype acl:relcl to distinguish relative clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies i"
L16-1262,E12-2021,1,0.581828,"Missing"
L16-1262,stepanek-pajas-2010-querying,0,0.067769,"Missing"
L16-1262,P13-2103,1,0.625022,"hese resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Spanish and Swedish). The first proposal for incorporating morphology was made by Tsarfaty (2013). The second version of HamleDT (Rosa et al., 2014) provided Stanford/Google annotation for 30 languages by automatically harmonizing treebanks with different native annotations. These efforts were followed by the development of the universal Stanford dependencies (USD), revising Stanford Dependencies for cross-linguistic annotations in light of the Google scheme (de Marneffe et al., 2014). UD is the result of merging all these initiatives into a single coherent framework, based on the universal Stanford dependencies, an extended version of the Google universal tag set, a revised subset of the"
L16-1262,I08-3008,1,0.20904,"the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-"
L16-1262,zeman-etal-2012-hamledt,1,0.729155,"Missing"
L16-1262,zeman-2008-reusable,1,0.897534,"erger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we present version 1 of the universal guidelines and explain the underlying d"
N07-1051,P05-1022,0,0.727191,"uage-specific tuning. 1 Introduction Treebank parsing comprises two problems: learning, in which we must select a model given a treebank, and inference, in which we must select a parse for a sentence given the learned model. Previous work has shown that high-quality unlexicalized PCFGs can be learned from a treebank, either by manual annotation (Klein and Manning, 2003) or automatic state splitting (Matsuzaki et al., 2005; Petrov et al., 2006). In particular, we demonstrated in Petrov et al. (2006) that a hierarchically split PCFG could exceed the accuracy of lexicalized PCFGs (Collins, 1999; Charniak and Johnson, 2005). However, many questions about inference with such split PCFGs remain open. In this work, we present 1. an effective method for pruning in split PCFGs 2. a comparison of objective functions for inference in split PCFGs, 3. experiments on automatic splitting for languages other than English. In Sec. 3, we present a novel coarse-to-fine processing scheme for hierarchically split PCFGs. Our In Sec. 4, we consider the well-known issue of inference objectives in split PCFGs. As in many model families (Steedman, 2000; Vijay-Shanker and Joshi, 1985), split PCFGs have a derivation / parse distinction"
N07-1051,N06-1022,0,0.577631,"Missing"
N07-1051,J99-1004,0,0.010554,"ections Recall that our final state-split grammars G come, by their construction process, with an ontogeny of grammars Gi where each grammar is a (partial) splitting of the preceding one. This gives us a natural chain of projections πi→j which projects backwards along this ontogeny of grammars (see Fig. 1). Of course, training also gives us parameters for the grammars, but only the chain of projections is needed. Note that the projected estimates need not 1 Whether or not the system has solutions depends on the parameters of the grammar. In particular, G may be improper, though the results of Chi (1999) imply that G will be proper if it is the maximum-likelihood estimate of a finite treebank. 407 (and in general will not) recover the original parameters exactly, nor would we want them to. Instead they take into account any smoothing, substate drift, and so on which occurred by the final grammar. Starting from the base grammar, we run the projection process for each stage in the sequence, calculating πi (chained incremental projections would also be possible). For the remainder of the paper, except where noted otherwise, all coarser grammars’ estimates are these reconstructions, rather than t"
N07-1051,N06-1043,0,0.0211945,"ees may be far from that of the treebank. Third, the meanings of the split states can and do drift between splitting stages. Fourth, and most importantly, we may wish to project grammars for which treebank estimation is problematic, for example, grammars which are more refined than the observed treebank grammars. Our method effectively avoids all of these problems by rebuilding and refitting the pruning grammars on the fly from the final grammar. 3.2.1 Estimating Projected Grammars Fortunately, there is a well worked-out notion of estimating a grammar from an infinite distribution over trees (Corazza and Satta, 2006). In particular, we can estimate parameters for a projected grammar π(G) from the tree distribution induced by G (which can itself be estimated in any manner). The earliest work that we are aware of on estimating models from models in this way is that of Nederhof (2005), who considers the case of learning language mod406 els from other language models. Corazza and Satta (2006) extend these methods to the case of PCFGs and tree distributions. The generalization of maximum likelihood estimation is to find the estimates for π(G) with minimum KL divergence from the tree distribution induced by G."
N07-1051,H05-1100,0,0.0553983,"osed form. very similar results (see Fig. 2), the M AX -RULE P RODUCT algorithm consistently outperformed the other two. Overall, the closed-form options were superior to the reranking ones, except on exact match, where the gains from correctly calculating the risk outweigh the losses from the truncation of the candidate set. 5 Multilingual Parsing Most research on parsing has focused on English and parsing performance on other languages is generally significantly lower.3 Recently, there have been some attempts to adapt parsers developed for English to other languages (Levy and Manning, 2003; Cowan and Collins, 2005). Adapting lexicalized parsers to other languages in not a trivial task as it requires at least the specification of head rules, and has had limited success. Adapting unlexicalized parsers appears to be equally difficult: Levy and Manning (2003) adapt the unlexicalized parser of Klein and Manning (2003) to Chinese, but even after significant efforts on choosing category splits, only modest performance gains are reported. In contrast, automatically learned grammars like the one of Matsuzaki et al. (2005) and Petrov et al. (2006) require a treebank for training but no additional human input. One"
N07-1051,W06-1638,0,0.02718,"This procedure gives an ontogeny of grammars Gi , where G = Gn is the final grammar. Empirically, the gains on the English Penn treebank level off after 6 rounds. In Petrov et al. (2006), some simple smoothing is also shown to be effective. It is interesting to note that these grammars capture many of the “structural zeros” described by Mohri and Roark (2006) and pruning rules with probability below e−10 reduces the grammar size drastically without influencing parsing performance. Some of our methods and conclusions are relevant to all state-split grammars, such as Klein and Manning (2003) or Dreyer and Eisner (2006), while others apply most directly to the hierarchical case. 3 Search When working with large grammars, it is standard to prune the search space in some way. In the case of lexicalized grammars, the unpruned chart often will not even fit in memory for long sentences. Several proven techniques exist. Collins (1999) combines a punctuation rule which eliminates many spans entirely, and then uses span-synchronous beams to prune in a bottom-up fashion. Charniak et al. (1998) 405 introduces best-first parsing, in which a figure-ofmerit prioritizes agenda processing. Most relevant to our work is Char"
N07-1051,P05-1039,0,0.0132973,"Missing"
N07-1051,W06-1673,0,0.0205353,",0,n) TG = argmaxT TG = argmaxT TG = argmaxT Q e∈T P Q q(e) e∈T q(e) e∈T q(e) Figure 3: Different objectives for parsing with posteriors, yielding comparable results. A, B, C are nonterminal symbols, x, y, z are latent annotations and i, j, k are between-word indices. Hence (Ax , i, j) denotes a constituent labeled with Ax spanning from i to j. Furthermore, we write e = (A → B C, i, j, k) for brevity. form. Exact match (likelihood) has this property. In general, however, we can approximate the expectation with samples from P (T |w, G). The method for sampling derivations of a PCFG is given in Finkel et al. (2006) and Johnson et al. (2007). It requires a single inside-outside computation per sentence and is then efficient per sample. Note that for split grammars, a posterior parse sample can be drawn by sampling a derivation and projecting away the substates. Fig. 2 shows the results of the following experiment. We constructed 10-best lists from the full grammar G in Sec. 2 using the parser of Petrov et al. (2006). We then took the same grammar and extracted 500-sample lists using the method of Finkel et al. (2006). The minimum risk parse candidate was selected for various loss functions. As can be see"
N07-1051,W01-0521,0,0.0825259,"y of features which cannot be captured by a generative model. Space does not permit a thorough exposition of our analysis, but as in the case of English (Petrov et al., 2006), the learned subcategories exhibit interesting linguistic interpretations. In German, for example, the model learns subcategories for different cases and genders. 5.2 Corpus Variation Related to cross language generalization is the generalization across domains for the same language. It is well known that a model trained on the Wall Street Journal loses significantly in performance when evaluated on the Brown Corpus (see Gildea (2001) for more details and the exact setup of their experiment, which we duplicated here). Recently McClosky et al. (2006) came to the conclusion that this performance drop is not due to overfitting the WSJ data. Fig. 4 shows the performance on the Brown corpus during hierarchical training. While the F1 score on the WSJ is rising we observe a drop in performance after the 5th iteration, suggesting that some overfitting is occurring. ≤ 40 words all LP LR LP LR ENGLISH Charniak et al. (2005) 90.1 90.1 89.5 89.6 Petrov et al. (2006) 90.3 90.0 89.8 89.6 This Paper 90.7 90.5 90.2 89.9 Parser ENGLISH (re"
N07-1051,P96-1024,0,0.21727,"arse-to-fine processing scheme for hierarchically split PCFGs. Our In Sec. 4, we consider the well-known issue of inference objectives in split PCFGs. As in many model families (Steedman, 2000; Vijay-Shanker and Joshi, 1985), split PCFGs have a derivation / parse distinction. The split PCFG directly describes a generative model over derivations, but evaluation is sensitive only to the coarser treebank symbols. While the most probable parse problem is NP-complete (Sima’an, 1992), several approximate methods exist, including n-best reranking by parse likelihood, the labeled bracket algorithm of Goodman (1996), and a variational approximation introduced in Matsuzaki et al. (2005). We present experiments which explicitly minimize various evaluation risks over a candidate set using samples from the split PCFG, and relate those conditions to the existing non-sampling algorithms. We demonstrate that n-best reranking according to likelihood is superior for exact match, and that the non-reranking methods are superior for maximizing F1 . A specific contribution is to discuss the role of unary productions, which previous work has glossed over, but which is important in understanding why the various methods"
N07-1051,N07-1018,0,0.27516,"rgmaxT TG = argmaxT Q e∈T P Q q(e) e∈T q(e) e∈T q(e) Figure 3: Different objectives for parsing with posteriors, yielding comparable results. A, B, C are nonterminal symbols, x, y, z are latent annotations and i, j, k are between-word indices. Hence (Ax , i, j) denotes a constituent labeled with Ax spanning from i to j. Furthermore, we write e = (A → B C, i, j, k) for brevity. form. Exact match (likelihood) has this property. In general, however, we can approximate the expectation with samples from P (T |w, G). The method for sampling derivations of a PCFG is given in Finkel et al. (2006) and Johnson et al. (2007). It requires a single inside-outside computation per sentence and is then efficient per sample. Note that for split grammars, a posterior parse sample can be drawn by sampling a derivation and projecting away the substates. Fig. 2 shows the results of the following experiment. We constructed 10-best lists from the full grammar G in Sec. 2 using the parser of Petrov et al. (2006). We then took the same grammar and extracted 500-sample lists using the method of Finkel et al. (2006). The minimum risk parse candidate was selected for various loss functions. As can be seen, in most cases, risk min"
N07-1051,P03-1054,1,0.366151,"minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning. 1 Introduction Treebank parsing comprises two problems: learning, in which we must select a model given a treebank, and inference, in which we must select a parse for a sentence given the learned model. Previous work has shown that high-quality unlexicalized PCFGs can be learned from a treebank, either by manual annotation (Klein and Manning, 2003) or automatic state splitting (Matsuzaki et al., 2005; Petrov et al., 2006). In particular, we demonstrated in Petrov et al. (2006) that a hierarchically split PCFG could exceed the accuracy of lexicalized PCFGs (Collins, 1999; Charniak and Johnson, 2005). However, many questions about inference with such split PCFGs remain open. In this work, we present 1. an effective method for pruning in split PCFGs 2. a comparison of objective functions for inference in split PCFGs, 3. experiments on automatic splitting for languages other than English. In Sec. 3, we present a novel coarse-to-fine process"
N07-1051,P03-1056,0,0.292247,"cting assumptions, in closed form. very similar results (see Fig. 2), the M AX -RULE P RODUCT algorithm consistently outperformed the other two. Overall, the closed-form options were superior to the reranking ones, except on exact match, where the gains from correctly calculating the risk outweigh the losses from the truncation of the candidate set. 5 Multilingual Parsing Most research on parsing has focused on English and parsing performance on other languages is generally significantly lower.3 Recently, there have been some attempts to adapt parsers developed for English to other languages (Levy and Manning, 2003; Cowan and Collins, 2005). Adapting lexicalized parsers to other languages in not a trivial task as it requires at least the specification of head rules, and has had limited success. Adapting unlexicalized parsers appears to be equally difficult: Levy and Manning (2003) adapt the unlexicalized parser of Klein and Manning (2003) to Chinese, but even after significant efforts on choosing category splits, only modest performance gains are reported. In contrast, automatically learned grammars like the one of Matsuzaki et al. (2005) and Petrov et al. (2006) require a treebank for training but no a"
N07-1051,J93-2004,0,0.0589627,"Missing"
N07-1051,P05-1010,0,0.949072,"tical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning. 1 Introduction Treebank parsing comprises two problems: learning, in which we must select a model given a treebank, and inference, in which we must select a parse for a sentence given the learned model. Previous work has shown that high-quality unlexicalized PCFGs can be learned from a treebank, either by manual annotation (Klein and Manning, 2003) or automatic state splitting (Matsuzaki et al., 2005; Petrov et al., 2006). In particular, we demonstrated in Petrov et al. (2006) that a hierarchically split PCFG could exceed the accuracy of lexicalized PCFGs (Collins, 1999; Charniak and Johnson, 2005). However, many questions about inference with such split PCFGs remain open. In this work, we present 1. an effective method for pruning in split PCFGs 2. a comparison of objective functions for inference in split PCFGs, 3. experiments on automatic splitting for languages other than English. In Sec. 3, we present a novel coarse-to-fine processing scheme for hierarchically split PCFGs. Our In Sec"
N07-1051,P06-1043,0,0.257607,"our analysis, but as in the case of English (Petrov et al., 2006), the learned subcategories exhibit interesting linguistic interpretations. In German, for example, the model learns subcategories for different cases and genders. 5.2 Corpus Variation Related to cross language generalization is the generalization across domains for the same language. It is well known that a model trained on the Wall Street Journal loses significantly in performance when evaluated on the Brown Corpus (see Gildea (2001) for more details and the exact setup of their experiment, which we duplicated here). Recently McClosky et al. (2006) came to the conclusion that this performance drop is not due to overfitting the WSJ data. Fig. 4 shows the performance on the Brown corpus during hierarchical training. While the F1 score on the WSJ is rising we observe a drop in performance after the 5th iteration, suggesting that some overfitting is occurring. ≤ 40 words all LP LR LP LR ENGLISH Charniak et al. (2005) 90.1 90.1 89.5 89.6 Petrov et al. (2006) 90.3 90.0 89.8 89.6 This Paper 90.7 90.5 90.2 89.9 Parser ENGLISH (reranked) Charniak et al. (2005)4 92.4 91.6 91.8 91.0 GERMAN Dubey (2005) F1 76.3 This Paper 80.8 80.7 80.1 80.1 CHINES"
N07-1051,N06-1040,0,0.0174503,", for example DT might become DT-1 and DT-2. The refined grammar is estimated using a variant of the forward-backward algorithm (Matsuzaki et al., 2005). After a splitting stage, many splits are rolled back based on (an approximation to) their likelihood gain. This procedure gives an ontogeny of grammars Gi , where G = Gn is the final grammar. Empirically, the gains on the English Penn treebank level off after 6 rounds. In Petrov et al. (2006), some simple smoothing is also shown to be effective. It is interesting to note that these grammars capture many of the “structural zeros” described by Mohri and Roark (2006) and pruning rules with probability below e−10 reduces the grammar size drastically without influencing parsing performance. Some of our methods and conclusions are relevant to all state-split grammars, such as Klein and Manning (2003) or Dreyer and Eisner (2006), while others apply most directly to the hierarchical case. 3 Search When working with large grammars, it is standard to prune the search space in some way. In the case of lexicalized grammars, the unpruned chart often will not even fit in memory for long sentences. Several proven techniques exist. Collins (1999) combines a punctuatio"
N07-1051,J05-2002,0,0.0109786,"han the observed treebank grammars. Our method effectively avoids all of these problems by rebuilding and refitting the pruning grammars on the fly from the final grammar. 3.2.1 Estimating Projected Grammars Fortunately, there is a well worked-out notion of estimating a grammar from an infinite distribution over trees (Corazza and Satta, 2006). In particular, we can estimate parameters for a projected grammar π(G) from the tree distribution induced by G (which can itself be estimated in any manner). The earliest work that we are aware of on estimating models from models in this way is that of Nederhof (2005), who considers the case of learning language mod406 els from other language models. Corazza and Satta (2006) extend these methods to the case of PCFGs and tree distributions. The generalization of maximum likelihood estimation is to find the estimates for π(G) with minimum KL divergence from the tree distribution induced by G. Since π(G) is a grammar over coarser symbols, we fit π(G) to the distribution G induces over π-projected trees: P (π(T )|G). The proofs of the general case are given in Corazza and Satta (2006), but the resulting procedure is quite intuitive. Given a (fully observed) tr"
N07-1051,P06-1055,1,0.770953,", we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning. 1 Introduction Treebank parsing comprises two problems: learning, in which we must select a model given a treebank, and inference, in which we must select a parse for a sentence given the learned model. Previous work has shown that high-quality unlexicalized PCFGs can be learned from a treebank, either by manual annotation (Klein and Manning, 2003) or automatic state splitting (Matsuzaki et al., 2005; Petrov et al., 2006). In particular, we demonstrated in Petrov et al. (2006) that a hierarchically split PCFG could exceed the accuracy of lexicalized PCFGs (Collins, 1999; Charniak and Johnson, 2005). However, many questions about inference with such split PCFGs remain open. In this work, we present 1. an effective method for pruning in split PCFGs 2. a comparison of objective functions for inference in split PCFGs, 3. experiments on automatic splitting for languages other than English. In Sec. 3, we present a novel coarse-to-fine processing scheme for hierarchically split PCFGs. Our In Sec. 4, we consider the w"
N07-1051,A97-1014,0,0.0456117,"Missing"
N07-1051,N04-1032,0,0.00799659,"Missing"
N07-1051,W06-1666,0,0.0158259,"re X is an evaluation symbol (such as NP) and k is some indicator of a subcategory, such as a parent annotation. G induces a derivation distribution P (T |G) over trees T labeled with split symbols. This distribution in turn induces a parse distribution P (T ′ |G) = P (π(T )|G) over (projected) trees with P unsplit evaluation symbols, where P (T ′ |G) = T :T ′ =π(T ) P (T |G). We now have several choices of how to select a tree given these posterior distributions over trees. In this section, we present experiments with the various options and explicitly relate them to parse risk minimization (Titov and Henderson, 2006). 408 G0 Nonterminals 98 Rules 3,700 No pruning 52 min X-bar pruning 8 min C-to-F (no loss) 6 min F1 for above 64.8 C-to-F (lossy) 6 min F1 for above 64.3 G2 G4 G6 219 498 1140 19,600 126,100 531,200 99 min 288 min 1612 min 14 min 30 min 111 min 12 min 16 min 20 min 85.2 89.7 91.2 8 min 9 min 11 min 84.7 89.4 91.1 Table 1: Grammar sizes, parsing times and accuracies for hierarchically split PCFGs with and without hierarchical coarse-tofine parsing on our development set (1578 sentences with 40 or less words from section 22 of the Penn Treebank). For comparison the parser of Charniak and Johnso"
N07-1051,P85-1011,0,0.243768,"eed the accuracy of lexicalized PCFGs (Collins, 1999; Charniak and Johnson, 2005). However, many questions about inference with such split PCFGs remain open. In this work, we present 1. an effective method for pruning in split PCFGs 2. a comparison of objective functions for inference in split PCFGs, 3. experiments on automatic splitting for languages other than English. In Sec. 3, we present a novel coarse-to-fine processing scheme for hierarchically split PCFGs. Our In Sec. 4, we consider the well-known issue of inference objectives in split PCFGs. As in many model families (Steedman, 2000; Vijay-Shanker and Joshi, 1985), split PCFGs have a derivation / parse distinction. The split PCFG directly describes a generative model over derivations, but evaluation is sensitive only to the coarser treebank symbols. While the most probable parse problem is NP-complete (Sima’an, 1992), several approximate methods exist, including n-best reranking by parse likelihood, the labeled bracket algorithm of Goodman (1996), and a variational approximation introduced in Matsuzaki et al. (2005). We present experiments which explicitly minimize various evaluation risks over a candidate set using samples from the split PCFG, and rel"
N07-1051,C02-1145,0,0.0573703,"Missing"
N07-1051,H86-1020,0,\N,Missing
N07-1051,C02-1126,0,\N,Missing
N07-1051,J03-4003,0,\N,Missing
N10-1003,W08-2102,0,0.149769,"Missing"
N10-1003,P05-1022,0,0.150809,"rence), and compares them to related work. There is a large body of work that has reported parsing accuracies for English, and we have grouped the different methods into categories for better overview. Our results on the English in-domain test set are higher than those obtained by any single component parser (SINGLE). The other methods quoted in Table 2 operate over the output of one or more single component parsers and are therefore largely orthogonal to our line of work. It is nonetheless exciting to see that our product model is competitive with the discriminative rescoring methods (RE) of Charniak and Johnson (2005) and Huang (2008), achieving higher F1 scores but lower exact match. These two methods work on top of the Charniak (2000) parser, and it would be possible to exchange that parser with our product model. We did not attempt this experiment, but we expect that those methods would stack well with our model, because they use primarily non-local features that are not available in a context-free grammar. 5 Conclusions We presented a simple product model that significantly improves parsing accuracies on different domains and languages. Our model leverages multiple automatically learned latent variable"
N10-1003,A00-2018,0,0.972276,"grammar (Charniak, 1996) is too permissive, making unrealistic context-freedom assumptions. For example, it postulates that there is only one type of noun phrase (NP), which can appear in all positions (subject, object, etc.), regardless of case, number or gender. As a result, the grammar can generate millions of (incorrect) parse trees for a given sentence, and has a flat posterior distribution. High accuracy grammars therefore add soft constraints on the way categories can be combined, and enrich the label set with additional information. These constraints can be lexicalized (Collins, 1999; Charniak, 2000), unlexicalized (Johnson, 1998; Klein and Manning, 2003b) or automatically learned (Matsuzaki et al., 2005; Petrov et al., 2006). The constraints serve the purpose of weakening the independence assumptions, and reduce the number of possible (but incorrect) parses. Here, we focus on the latent variable approach of Petrov et al. (2006), where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars. Each round of refinement introduces new constraints on how constituents can be combined, which in turn leads to a higher parsing accuracy. Howeve"
N10-1003,N09-2064,0,0.518354,"efinements. We use these variations to our advantage, and treat grammars learned from different random seeds as independent and equipotent experts. We use a product distribution for joint prediction, which gives more peaked posteriors than a sum, and enforces all constraints of the individual grammars, without the need to tune mixing weights. It should be noted here that our focus is on improving parsing performance using a single underlying grammar class, which is somewhat orthogonal to the issue of parser combination, that has been studied elsewhere in the literature (Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009). In contrast to that line of work, we also do not restrict ourselves to working with kbest output, but work directly with a packed forest representation of the posteriors, much in the spirit of Huang (2008), except that we work with several forests rather than rescoring a single one. 19 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 19–27, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics In our experimental section we give empirical answers to some of the remaining theoretical que"
N10-1003,W01-0521,0,0.0134497,"ey obtain an F1 score of 91.0 (which they further improve to 91.4 using a voting scheme). We replicated their experiment, but used an unweighted product of the two model scores. Using T REE L EVEL inference, we obtained an F1 score of 91.6, suggesting that weighting is not so important in the product case, as long as the classifiers are of comparable quality.6 This is in line with previous work on product models, where weighting has been important when combining heterogenous classifiers (Heskes, 1998), and less important when the classifiers are of similar accuracy (Smith et al., 2005). 5 See Gildea (2001) for the exact setup. The unweighted sum model, however, underperforms the individual models with an F1 score of only 90.3. 6 24 4 8 16 Number of grammars in product model Figure 6: Adding more grammars to the product model improves parsing accuracy, while C ONSTITUENT-L EVEL inference gives consistently better results. 4.2 Tree-Level vs. Constituent-Level Inference Figure 6 shows that accuracy increases when more grammars are added to the product model, but levels off after eight grammars. The plot also compares our two inference approximations, and shows that C ONSTITUENT-L EVEL inference re"
N10-1003,P96-1024,0,0.303946,"Missing"
N10-1003,W99-0623,0,0.105339,"al26 Type COMBO SELF RE SINGLE RE SING SING Techniques like self-training (SELF) and system combinations (COMBO) can further improve parsing accuracies, but are also orthogonal to our work. In particular the COMBO methods seem related to our work, but are very different in their nature. While we use multiple grammars in our work, all grammars are from the same model class for us. In contrast, those methods rely on a diverse set of individual parsers, each of which requires a significant effort to build. Furthermore, those techniques have largely relied on different voting schemes in the past (Henderson and Brill, 1999; Sagae and Lavie, 2006), and only more recently have started using actual posteriors from the underlying models (Fossum and Knight, 2009; Zhang et al., 2009). Even then, those methods operate only over k-best lists, and we are the first to work directly with parse forests from multiple grammars. It is also interesting to note that the best results in Zhang et al. (2009) are achieved by combining kbest lists from a latent variable grammar of Petrov et al. (2006) with the self-trained reranking parser of McClosky et al. (2006). Clearly, replacing the single latent variable grammar with a produc"
N10-1003,A00-2005,0,0.0204555,"t gives combined models their strength. One way of increasing diversity is by modifying the feature sets of the individual models (Baldridge and Osborne, 2008; Smith and Osborne, 2007). This approach has the disadvantage that it reduces the performance of the individual models, and is not directly applicable for latent variable grammars because the features are automatically learned. Alternatively, one can introduce diversity by changing the training distribution. Bagging (Breiman, 1996) and Boosting (Freund and Shapire, 1996) fall into this category, but have had limited success for parsing (Henderson and Brill, 2000). Furthermore boosting is impractical here, because it requires training dozens of grammars in sequence. Since training a single grammar takes roughly one day, we opted for a different, parallelizable way of changing the training distribution. In a first experiment, we divided the training set into two disjoint sets, and trained separate grammars on each half. These truly disjoint grammars had low F1 scores of 89.4 and 89.6 respectively (because they were trained on less data). Their combination unfortunately also achieves only an accuracy of 90.9, which is lower than what we get when training"
N10-1003,W05-1506,0,0.0106218,"model does significantly better on all. the search space (Goodman, 1996; Titov and Henderson, 2006; Petrov and Klein, 2007). The simplest approach is to stick to likelihood as the objective function, but to limit the search space to a set of high quality candidates T : T ∗ = argmax P(T |w) (3) T ∈T Because the likelihood of a given parse tree can be computed exactly for our product model (Eq. 2), the quality of this approximation is only limited by the quality of the candidate list. To generate the candidate list, we produce k-best lists of Viterbi derivations with the efficient algorithm of Huang and Chiang (2005), and erase the subcategory information to obtain parse trees over unsplit categories. We refer to this approximation as T REE -L EVEL inference, because it considers a list of complete trees from the underlying grammars, and selects the tree that has the highest likelihood under the product model. While the k-best lists are of very high quality, this is a fairly crude and unsatisfactory way of approximating the posterior distribution of the product model, as it does not allow the synthesis of new trees based on tree fragments from different grammars. An alternative is to use a tractable objec"
N10-1003,D09-1087,0,0.265244,"Missing"
N10-1003,P08-1067,0,0.283133,"a sum, and enforces all constraints of the individual grammars, without the need to tune mixing weights. It should be noted here that our focus is on improving parsing performance using a single underlying grammar class, which is somewhat orthogonal to the issue of parser combination, that has been studied elsewhere in the literature (Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009). In contrast to that line of work, we also do not restrict ourselves to working with kbest output, but work directly with a packed forest representation of the posteriors, much in the spirit of Huang (2008), except that we work with several forests rather than rescoring a single one. 19 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 19–27, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics In our experimental section we give empirical answers to some of the remaining theoretical questions. We address the question of averaging versus multiplying classifier predictions, we investigate different ways of introducing more diversity into the underlying grammars, and also compare combining partial (constituent-le"
N10-1003,J98-4004,0,0.357239,"permissive, making unrealistic context-freedom assumptions. For example, it postulates that there is only one type of noun phrase (NP), which can appear in all positions (subject, object, etc.), regardless of case, number or gender. As a result, the grammar can generate millions of (incorrect) parse trees for a given sentence, and has a flat posterior distribution. High accuracy grammars therefore add soft constraints on the way categories can be combined, and enrich the label set with additional information. These constraints can be lexicalized (Collins, 1999; Charniak, 2000), unlexicalized (Johnson, 1998; Klein and Manning, 2003b) or automatically learned (Matsuzaki et al., 2005; Petrov et al., 2006). The constraints serve the purpose of weakening the independence assumptions, and reduce the number of possible (but incorrect) parses. Here, we focus on the latent variable approach of Petrov et al. (2006), where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars. Each round of refinement introduces new constraints on how constituents can be combined, which in turn leads to a higher parsing accuracy. However, EM is a local method, and t"
N10-1003,N03-1016,0,0.0180537,"ing unrealistic context-freedom assumptions. For example, it postulates that there is only one type of noun phrase (NP), which can appear in all positions (subject, object, etc.), regardless of case, number or gender. As a result, the grammar can generate millions of (incorrect) parse trees for a given sentence, and has a flat posterior distribution. High accuracy grammars therefore add soft constraints on the way categories can be combined, and enrich the label set with additional information. These constraints can be lexicalized (Collins, 1999; Charniak, 2000), unlexicalized (Johnson, 1998; Klein and Manning, 2003b) or automatically learned (Matsuzaki et al., 2005; Petrov et al., 2006). The constraints serve the purpose of weakening the independence assumptions, and reduce the number of possible (but incorrect) parses. Here, we focus on the latent variable approach of Petrov et al. (2006), where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars. Each round of refinement introduces new constraints on how constituents can be combined, which in turn leads to a higher parsing accuracy. However, EM is a local method, and there are no guarantees th"
N10-1003,P03-1054,0,0.0177276,"ing unrealistic context-freedom assumptions. For example, it postulates that there is only one type of noun phrase (NP), which can appear in all positions (subject, object, etc.), regardless of case, number or gender. As a result, the grammar can generate millions of (incorrect) parse trees for a given sentence, and has a flat posterior distribution. High accuracy grammars therefore add soft constraints on the way categories can be combined, and enrich the label set with additional information. These constraints can be lexicalized (Collins, 1999; Charniak, 2000), unlexicalized (Johnson, 1998; Klein and Manning, 2003b) or automatically learned (Matsuzaki et al., 2005; Petrov et al., 2006). The constraints serve the purpose of weakening the independence assumptions, and reduce the number of possible (but incorrect) parses. Here, we focus on the latent variable approach of Petrov et al. (2006), where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars. Each round of refinement introduces new constraints on how constituents can be combined, which in turn leads to a higher parsing accuracy. However, EM is a local method, and there are no guarantees th"
N10-1003,J93-2004,0,0.0524473,"Missing"
N10-1003,P05-1010,0,0.710462,"mple, it postulates that there is only one type of noun phrase (NP), which can appear in all positions (subject, object, etc.), regardless of case, number or gender. As a result, the grammar can generate millions of (incorrect) parse trees for a given sentence, and has a flat posterior distribution. High accuracy grammars therefore add soft constraints on the way categories can be combined, and enrich the label set with additional information. These constraints can be lexicalized (Collins, 1999; Charniak, 2000), unlexicalized (Johnson, 1998; Klein and Manning, 2003b) or automatically learned (Matsuzaki et al., 2005; Petrov et al., 2006). The constraints serve the purpose of weakening the independence assumptions, and reduce the number of possible (but incorrect) parses. Here, we focus on the latent variable approach of Petrov et al. (2006), where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars. Each round of refinement introduces new constraints on how constituents can be combined, which in turn leads to a higher parsing accuracy. However, EM is a local method, and there are no guarantees that it will find the same grammars when initialized"
N10-1003,N06-1020,0,0.0336267,"ques have largely relied on different voting schemes in the past (Henderson and Brill, 1999; Sagae and Lavie, 2006), and only more recently have started using actual posteriors from the underlying models (Fossum and Knight, 2009; Zhang et al., 2009). Even then, those methods operate only over k-best lists, and we are the first to work directly with parse forests from multiple grammars. It is also interesting to note that the best results in Zhang et al. (2009) are achieved by combining kbest lists from a latent variable grammar of Petrov et al. (2006) with the self-trained reranking parser of McClosky et al. (2006). Clearly, replacing the single latent variable grammar with a product of latent variable grammars ought to improve performance. The results on the other two corpora are similar. A product of latent variable grammars very significantly outperforms a single latent variable grammar and sets new standards for the state-of-the-art. We also analyzed the errors of the product models. In addition to the illustrative example in Figure 5, we computed detailed error metrics for different phrasal categories. Figure 4 shows that a product of four random grammars is always better than even the best underly"
N10-1003,N07-1051,1,0.477835,"xpensive ADJP }| -12.9 -11.5 ADJP z -11.7 -12.4 than direct Treasury borrowing ADJP ? } ? Legend: log G1 -score log G2 -score | , { ADJP (D.Calif.) , unauthorized and expensive Such agency ‘self-help’ borrowing NP { said ? the bill’s chief sponsor ” VP } S “ Figure 5: Grammar G1 has a preference for flat structures, while grammar G2 prefers deeper hierarchical structures. Both grammars therefore make one mistake each on their own. However, the correct parse tree (which uses a flat ADJP in the first slot and a hierarchical NP in the second) scores highest under the product model. search space. Petrov and Klein (2007) present such an objective function, which maximizes the product of expected correct productions r: Y E(r|w) (4) T ∗ = argmax T r∈T These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al. (2005). We extend the algorithm to work over posterior distributions from multiple grammars, by aggregating their expectations into a product. In practice, we use a packed forest representation to approximate the posterior distribution, as in Huang (2008). We refer"
N10-1003,D08-1091,1,0.896326,"Missing"
N10-1003,P06-1055,1,0.557021,"there is only one type of noun phrase (NP), which can appear in all positions (subject, object, etc.), regardless of case, number or gender. As a result, the grammar can generate millions of (incorrect) parse trees for a given sentence, and has a flat posterior distribution. High accuracy grammars therefore add soft constraints on the way categories can be combined, and enrich the label set with additional information. These constraints can be lexicalized (Collins, 1999; Charniak, 2000), unlexicalized (Johnson, 1998; Klein and Manning, 2003b) or automatically learned (Matsuzaki et al., 2005; Petrov et al., 2006). The constraints serve the purpose of weakening the independence assumptions, and reduce the number of possible (but incorrect) parses. Here, we focus on the latent variable approach of Petrov et al. (2006), where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars. Each round of refinement introduces new constraints on how constituents can be combined, which in turn leads to a higher parsing accuracy. However, EM is a local method, and there are no guarantees that it will find the same grammars when initialized from different startin"
N10-1003,N06-2033,0,0.463664,"ations in the learned refinements. We use these variations to our advantage, and treat grammars learned from different random seeds as independent and equipotent experts. We use a product distribution for joint prediction, which gives more peaked posteriors than a sum, and enforces all constraints of the individual grammars, without the need to tune mixing weights. It should be noted here that our focus is on improving parsing performance using a single underlying grammar class, which is somewhat orthogonal to the issue of parser combination, that has been studied elsewhere in the literature (Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009). In contrast to that line of work, we also do not restrict ourselves to working with kbest output, but work directly with a packed forest representation of the posteriors, much in the spirit of Huang (2008), except that we work with several forests rather than rescoring a single one. 19 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 19–27, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics In our experimental section we give empirical answers to some of the"
N10-1003,A97-1014,0,0.0296909,"Missing"
N10-1003,P05-1003,0,0.192033,"nd Petrov et al. (2006), they obtain an F1 score of 91.0 (which they further improve to 91.4 using a voting scheme). We replicated their experiment, but used an unweighted product of the two model scores. Using T REE L EVEL inference, we obtained an F1 score of 91.6, suggesting that weighting is not so important in the product case, as long as the classifiers are of comparable quality.6 This is in line with previous work on product models, where weighting has been important when combining heterogenous classifiers (Heskes, 1998), and less important when the classifiers are of similar accuracy (Smith et al., 2005). 5 See Gildea (2001) for the exact setup. The unweighted sum model, however, underperforms the individual models with an F1 score of only 90.3. 6 24 4 8 16 Number of grammars in product model Figure 6: Adding more grammars to the product model improves parsing accuracy, while C ONSTITUENT-L EVEL inference gives consistently better results. 4.2 Tree-Level vs. Constituent-Level Inference Figure 6 shows that accuracy increases when more grammars are added to the product model, but levels off after eight grammars. The plot also compares our two inference approximations, and shows that C ONSTITUEN"
N10-1003,E09-1088,0,0.0174958,"Missing"
N10-1003,W06-1666,0,0.0357866,"Missing"
N10-1003,D09-1161,0,0.713283,"variations to our advantage, and treat grammars learned from different random seeds as independent and equipotent experts. We use a product distribution for joint prediction, which gives more peaked posteriors than a sum, and enforces all constraints of the individual grammars, without the need to tune mixing weights. It should be noted here that our focus is on improving parsing performance using a single underlying grammar class, which is somewhat orthogonal to the issue of parser combination, that has been studied elsewhere in the literature (Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009). In contrast to that line of work, we also do not restrict ourselves to working with kbest output, but work directly with a packed forest representation of the posteriors, much in the spirit of Huang (2008), except that we work with several forests rather than rescoring a single one. 19 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 19–27, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics In our experimental section we give empirical answers to some of the remaining theoretical questions. We address th"
N10-1003,W04-3242,0,\N,Missing
N10-1003,J03-4003,0,\N,Missing
N12-1054,C10-1007,0,0.491007,"ting the relationship to greedy transition-based dependency parsers that are also linear-time (Nivre et al., 2004) or quadratic-time (Yamada and Matsumoto, 2003). It is their success that motivates building explicitly trained, linear-time pruning models. However, while a greedy solution for arc-standard transition-based parsers can be computed in linear-time, Kuhlmann et al. (2011) recently showed that computing exact solutions or (max-)marginals has time complexity O(n4 ), making these models inappropriate for coarse-to-fine style pruning. As an alternative, Roark and Hollingshead (2008) and Bergsma and Cherry (2010) present approaches where individual classifiers are used to prune chart cells. Such approaches have the drawback that pruning decisions are made locally and therefore can rule out all valid structures, despite explicitly evaluating O(n2 ) chart cells. In contrast, we make pruning decisions based on global parse max-marginals using a vine pruning pass, which is linear in the sentence length, but nonetheless guarantees to preserve a valid parse structure. 0.4 0.3 0.2 0.1 1 2 3 4 5 6 7 8 9 modifier index (a) 0.0 1 2 3 4 5 6 dependency length (b) Figure 1: (a) Heat map indicating how likely a par"
N12-1054,W06-2920,0,0.155287,"ing cascade with a wide range of common pruning methods on the Penn WSJ Treebank (PTB) (Marcus et al., 1993). We then also show that vine pruning is effective across a variety of different languages. For English, we convert the PTB constituency trees to dependencies using the Stanford dependency framework (De Marneffe et al., 2006). We then train on the standard PTB split with sections 2-21 as training, section 22 as validation, and section 23 as test. Results are similar using the Yamada and Matsumoto (2003) conversion. We additionally selected six languages from the CoNLL-X shared task 504 (Buchholz and Marsi, 2006) that cover a number of different language families: Bulgarian, Chinese, Japanese, German, Portuguese, and Swedish. We use the standard CoNLL-X training/test split and tune parameters with cross-validation. All experiments use unlabeled dependencies for training and test. Accuracy is reported as unlabeled attachment score (UAS), the percentage of tokens with the correct head word. For English, UAS ignores punctuation tokens and the test set uses predicted POS tags. For the other languages we follow the CoNLL-X setup and include punctuation in UAS and use gold POS tags on the set set. Speedups"
N12-1054,W08-2102,0,0.0452562,"work, we present a multipass coarse-to-fine architecture for graph-based dependency parsing. We start with a linear-time vine pruning pass and build up to higher-order models, achieving speed-ups of two orders of magnitude while maintaining state-of-the-art accuracies. In constituency parsing, exhaustive inference for all but the simplest grammars tends to be prohibitively slow. Consequently, most high-accuracy constituency parsers routinely employ a coarse grammar to prune dynamic programming chart cells ∗ Research conducted at Google. of the final grammar of interest (Charniak et al., 2006; Carreras et al., 2008; Petrov, 2009). While there are no strong theoretical guarantees for these approaches,1 in practice one can obtain significant speed improvements with minimal loss in accuracy. This benefit comes primarily from reducing the large grammar constant |G |that can dominate the runtime of the cubic-time CKY inference algorithm. Dependency parsers on the other hand do not have a multiplicative grammar factor |G|, and until recently were considered efficient enough for exhaustive inference. However, the increased model complexity of a third-order parser forced Koo and Collins (2010) to prune with a f"
N12-1054,D07-1101,0,0.227812,"unary head or modifier token, as well as features for the POS tag bordering the cutoff and the direction of the arc. 5.2 5.3 Z HANG N IVRE an unlabeled reimplementation of the linear-time, k-best, transition-based parser of Zhang and Nivre (2011). This parser uses composite features up to third-order with a greedy decoding algorithm. The reimplementation is about twice as fast as their reported speed, but scores slightly lower. Features For the non-pruning models, we use a standard set of features proposed in the discriminative graphbased dependency parsing literature (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). 3 For the first-order parser, we found it beneficial to employ a reduced feature first-order pruner before the final model, i.e. the cascade has four rounds: dictionary, vine, first-order pruning, and first-order 1-best. 505 Results A comparison between the pruning methods is shown in Table 1. The table gives relative speedups, compared to the unpruned first-order baseline, as well as accuracy, pruning efficiency, and oracle scores. Note particularly that the third-order cascade is twice as fast as an unpruned first-order model and >200 times faster than the unpruned"
N12-1054,N06-1022,0,0.0323638,"omplex models. In this work, we present a multipass coarse-to-fine architecture for graph-based dependency parsing. We start with a linear-time vine pruning pass and build up to higher-order models, achieving speed-ups of two orders of magnitude while maintaining state-of-the-art accuracies. In constituency parsing, exhaustive inference for all but the simplest grammars tends to be prohibitively slow. Consequently, most high-accuracy constituency parsers routinely employ a coarse grammar to prune dynamic programming chart cells ∗ Research conducted at Google. of the final grammar of interest (Charniak et al., 2006; Carreras et al., 2008; Petrov, 2009). While there are no strong theoretical guarantees for these approaches,1 in practice one can obtain significant speed improvements with minimal loss in accuracy. This benefit comes primarily from reducing the large grammar constant |G |that can dominate the runtime of the cubic-time CKY inference algorithm. Dependency parsers on the other hand do not have a multiplicative grammar factor |G|, and until recently were considered efficient enough for exhaustive inference. However, the increased model complexity of a third-order parser forced Koo and Collins ("
N12-1054,W02-1001,0,0.0384352,"UAS 93.3 93.3 93.1 93.1 93.1 93.1 93.1 93.1 92.7 Table 1: Results comparing pruning methods on PTB Section 22. Oracle is the max achievable UAS after pruning. Pruning efficiency (PE) is the percentage of non-gold first-order dependency arcs pruned. Speed is parsing time relative to the unpruned first-order model (around 2000 tokens/sec). UAS is the unlabeled attachment score of the final parses. 4.3 1-Best Training For the final pass, we want to train the model for 1best output. Several different learning methods are available for structured prediction models including structured perceptron (Collins, 2002), max-margin models (Taskar et al., 2003), and log-linear models (Lafferty et al., 2001). In this work, we use the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al., 2006) with a hamming-loss margin. MIRA is an online algorithm with similar benefits as structured perceptron in terms of simplicity and fast training time. In practice, we found that MIRA with hamming-loss margin gives a performance improvement over structured perceptron and structured SVM. 5 Parsing Experiments To empirically demonstrate the effectiveness of our approach, we compare our vine prunin"
N12-1054,de-marneffe-etal-2006-generating,0,0.00809156,"Missing"
N12-1054,W05-1504,0,0.681294,"r forced Koo and Collins (2010) to prune with a first-order model in order to make inference practical. While fairly effective, all these approaches are limited by the fact that inference in the coarse model remains cubic in the sentence length. The desire to parse vast amounts of text necessitates more efficient dependency parsing algorithms. We thus propose a multi-pass coarse-to-fine approach where the initial pass is a linear-time sweep, which tries to resolve local ambiguities, but leaves arcs beyond a fixed length b unspecified (Section 3). The dynamic program is a form of vine parsing (Eisner and Smith, 2005), which we use to compute parse max-marginals, rather than for finding the 1best parse tree. To reduce pruning errors, the parameters of the vine parser (and all subsequent pruning models) are trained using the structured prediction cascades of Weiss and Taskar (2010) to optimize for pruning efficiency, and not for 1-best prediction (Section 4). Despite a limited scope of b = 3, the 1 This is in contrast to optimality preserving methods such as A* search, which typically do not provide sufficient speed-ups (Pauls and Klein, 2009). 498 2012 Conference of the North American Chapter of the Associ"
N12-1054,P10-1110,0,0.0481801,"Missing"
N12-1054,P10-1001,0,0.784373,"niak et al., 2006; Carreras et al., 2008; Petrov, 2009). While there are no strong theoretical guarantees for these approaches,1 in practice one can obtain significant speed improvements with minimal loss in accuracy. This benefit comes primarily from reducing the large grammar constant |G |that can dominate the runtime of the cubic-time CKY inference algorithm. Dependency parsers on the other hand do not have a multiplicative grammar factor |G|, and until recently were considered efficient enough for exhaustive inference. However, the increased model complexity of a third-order parser forced Koo and Collins (2010) to prune with a first-order model in order to make inference practical. While fairly effective, all these approaches are limited by the fact that inference in the coarse model remains cubic in the sentence length. The desire to parse vast amounts of text necessitates more efficient dependency parsing algorithms. We thus propose a multi-pass coarse-to-fine approach where the initial pass is a linear-time sweep, which tries to resolve local ambiguities, but leaves arcs beyond a fixed length b unspecified (Section 3). The dynamic program is a form of vine parsing (Eisner and Smith, 2005), which"
N12-1054,D10-1125,1,0.534942,"Missing"
N12-1054,P11-1068,0,0.019053,"Missing"
N12-1054,J93-2004,0,0.042341,"1). In this work, we use the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al., 2006) with a hamming-loss margin. MIRA is an online algorithm with similar benefits as structured perceptron in terms of simplicity and fast training time. In practice, we found that MIRA with hamming-loss margin gives a performance improvement over structured perceptron and structured SVM. 5 Parsing Experiments To empirically demonstrate the effectiveness of our approach, we compare our vine pruning cascade with a wide range of common pruning methods on the Penn WSJ Treebank (PTB) (Marcus et al., 1993). We then also show that vine pruning is effective across a variety of different languages. For English, we convert the PTB constituency trees to dependencies using the Stanford dependency framework (De Marneffe et al., 2006). We then train on the standard PTB split with sections 2-21 as training, section 22 as validation, and section 23 as test. Results are similar using the Yamada and Matsumoto (2003) conversion. We additionally selected six languages from the CoNLL-X shared task 504 (Buchholz and Marsi, 2006) that cover a number of different language families: Bulgarian, Chinese, Japanese,"
N12-1054,E06-1011,0,0.34821,"]l+1 0 , s ∈ [n] where k + 1 is the sibling order, l + 1 is the parent order, and k + l + 1 is the model order. The canonical second-order model uses I1,0 , which has a cardinality of O(n3 ). Although there are several possibilities for higher-order models, we use I1,1 as our third-order model. Generally, the parsing index set has cardinality |Ik,l |= O(n2+k+l ). Inference in higher-order models uses variants of the dynamic program for first-order parsing, and we refer to previous work for the full set of rules. For second-order models with index set I1,0 , parsing can be done in O(n3 ) time (McDonald and Pereira, 2006) and for third-order models in O(n4 ) time (Koo and Collins, 2010). Even though second-order parsing has the same asymptotic time complexity as first-order parsing, inference is significantly slower due to the cost of scoring the larger index set. We aim to prune the index set, by mapping each higher-order index down to a set of small set indices that can be pruned using a coarse pruning model. For example, to use a first-order model for pruning, we would map the higher-order index to the individual indices for its arc, grandparents, and siblings: * pk,l→1 (g, s) = {(g1 , sj ) : j ∈ [k + 1]} M"
N12-1054,P05-1012,0,0.251695,"a first-order model, since these two models tend to often agree. Included are lexical features, part-of-speech features, features on in-between tokens, as well as feature conjunctions, surrounding part-of-speech tags, and back-off features. In addition, we replicate each part-of-speech (POS) feature with an additional feature using coarse POS representations (Petrov et al., 2012). Our baseline parsing models replicate and, for some experiments, surpass previous best results. The first- and second-order pruning models have the same structure, but for efficiency use only the basic features from McDonald et al. (2005). As feature computation is quite costly, future work may investigate whether this set can be reduced further. V INE P RUNE and L OCAL S HORT use the same feature sets for short arcs. Outer arcs have features of the unary head or modifier token, as well as features for the POS tag bordering the cutoff and the direction of the arc. 5.2 5.3 Z HANG N IVRE an unlabeled reimplementation of the linear-time, k-best, transition-based parser of Zhang and Nivre (2011). This parser uses composite features up to third-order with a greedy decoding algorithm. The reimplementation is about twice as fast as t"
N12-1054,W04-2407,0,0.395442,"i-th order passes introduce larger scope features, while further constraining the search space. In Section 5 we present experiments in multiple languages. Our coarse-to-fine first-, second-, and third-order parsers preserve the accuracy of the unpruned models, but are faster by up to two orders of magnitude. Our pruned third-order model is faster than an unpruned first-order model, and compares favorably in speed to the state-of-the-art transitionbased parser of Zhang and Nivre (2011). It is worth noting the relationship to greedy transition-based dependency parsers that are also linear-time (Nivre et al., 2004) or quadratic-time (Yamada and Matsumoto, 2003). It is their success that motivates building explicitly trained, linear-time pruning models. However, while a greedy solution for arc-standard transition-based parsers can be computed in linear-time, Kuhlmann et al. (2011) recently showed that computing exact solutions or (max-)marginals has time complexity O(n4 ), making these models inappropriate for coarse-to-fine style pruning. As an alternative, Roark and Hollingshead (2008) and Bergsma and Cherry (2010) present approaches where individual classifiers are used to prune chart cells. Such appr"
N12-1054,N09-1063,0,0.0334304,"ied (Section 3). The dynamic program is a form of vine parsing (Eisner and Smith, 2005), which we use to compute parse max-marginals, rather than for finding the 1best parse tree. To reduce pruning errors, the parameters of the vine parser (and all subsequent pruning models) are trained using the structured prediction cascades of Weiss and Taskar (2010) to optimize for pruning efficiency, and not for 1-best prediction (Section 4). Despite a limited scope of b = 3, the 1 This is in contrast to optimality preserving methods such as A* search, which typically do not provide sufficient speed-ups (Pauls and Klein, 2009). 498 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 498–507, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics 2 Motivation & Overview The goal of this work is fast, high-order, graphbased dependency parsing. Previous work on constituency parsing demonstrates that performing several passes with increasingly more complex models results in faster inference (Charniak et al., 2006; Petrov and Klein, 2007). The same technique applies to dependency parsing with a cascade of models o"
N12-1054,N07-1051,1,0.120141,"Missing"
N12-1054,petrov-etal-2012-universal,1,0.647629,"and second-order pruning, and a final third-order 1best pass.3 We tune the pruning thresholds for each round and each cascade separately. This is because we might be willing to do a more aggressive vine pruning pass if the final model is a first-order model, since these two models tend to often agree. Included are lexical features, part-of-speech features, features on in-between tokens, as well as feature conjunctions, surrounding part-of-speech tags, and back-off features. In addition, we replicate each part-of-speech (POS) feature with an additional feature using coarse POS representations (Petrov et al., 2012). Our baseline parsing models replicate and, for some experiments, surpass previous best results. The first- and second-order pruning models have the same structure, but for efficiency use only the basic features from McDonald et al. (2005). As feature computation is quite costly, future work may investigate whether this set can be reduced further. V INE P RUNE and L OCAL S HORT use the same feature sets for short arcs. Outer arcs have features of the unary head or modifier token, as well as features for the POS tag bordering the cutoff and the direction of the arc. 5.2 5.3 Z HANG N IVRE an un"
N12-1054,C08-1094,0,0.22505,"Missing"
N12-1054,W03-3023,0,0.801784,"e features, while further constraining the search space. In Section 5 we present experiments in multiple languages. Our coarse-to-fine first-, second-, and third-order parsers preserve the accuracy of the unpruned models, but are faster by up to two orders of magnitude. Our pruned third-order model is faster than an unpruned first-order model, and compares favorably in speed to the state-of-the-art transitionbased parser of Zhang and Nivre (2011). It is worth noting the relationship to greedy transition-based dependency parsers that are also linear-time (Nivre et al., 2004) or quadratic-time (Yamada and Matsumoto, 2003). It is their success that motivates building explicitly trained, linear-time pruning models. However, while a greedy solution for arc-standard transition-based parsers can be computed in linear-time, Kuhlmann et al. (2011) recently showed that computing exact solutions or (max-)marginals has time complexity O(n4 ), making these models inappropriate for coarse-to-fine style pruning. As an alternative, Roark and Hollingshead (2008) and Bergsma and Cherry (2010) present approaches where individual classifiers are used to prune chart cells. Such approaches have the drawback that pruning decisions"
N12-1054,P11-2033,0,0.0369688,"ous best results. The first- and second-order pruning models have the same structure, but for efficiency use only the basic features from McDonald et al. (2005). As feature computation is quite costly, future work may investigate whether this set can be reduced further. V INE P RUNE and L OCAL S HORT use the same feature sets for short arcs. Outer arcs have features of the unary head or modifier token, as well as features for the POS tag bordering the cutoff and the direction of the arc. 5.2 5.3 Z HANG N IVRE an unlabeled reimplementation of the linear-time, k-best, transition-based parser of Zhang and Nivre (2011). This parser uses composite features up to third-order with a greedy decoding algorithm. The reimplementation is about twice as fast as their reported speed, but scores slightly lower. Features For the non-pruning models, we use a standard set of features proposed in the discriminative graphbased dependency parsing literature (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). 3 For the first-order parser, we found it beneficial to employ a reduced feature first-order pruner before the final model, i.e. the cascade has four rounds: dictionary, vine, first-order pruning, and first-"
P06-1055,P05-1022,0,0.484383,"pages 433–440, c Sydney, July 2006. 2006 Association for Computational Linguistics (a) RB NP cursively: (b) FRAG ROOT FRAG . FRAG Not DT NN . this year RB NP PIN (r, t, Ax ) = . X β(Ax → By Cz ) ×PIN (r, s, By )PIN (s, t, Cz ) y,z . Not DT NN POUT (r, s, By ) = X β(Ax → By Cz ) ×POUT (r, t, Ax )PIN (s, t, Cz ) POUT (s, t, Cz ) = X β(Ax → By Cz ) ×POUT (r, t, Ax )PIN (r, s, By ) this year x,z Figure 1: (a) The original tree. (b) The X-bar tree. x,y mar in Matsuzaki et al. (2005). Our grammar’s accuracy was higher than fully lexicalized systems, including the maximum-entropy inspired parser of Charniak and Johnson (2005). 1.1 Experimental Setup We ran our experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank using the standard setup: we trained on sections 2 to 21, and we used section 1 as a validation set for tuning model hyperparameters. Section 22 was used as development set for intermediate results. All of section 23 was reserved for the final test. We used the EVALB parseval reference implementation, available from Sekine and Collins (1997), for scoring. All reported development set results are averages over four runs. For the final test we selected the grammar that performed best on"
P06-1055,A00-2018,0,0.0879868,"Missing"
P06-1055,C02-1126,0,0.0109507,"Missing"
P06-1055,P96-1024,0,0.152353,"Missing"
P06-1055,P04-1013,0,0.0165333,"e likelihood of the training trees, despite the fact that the original trees lack the latent annotations. The Expectation-Maximization (EM) algorithm allows us to do exactly that.2 Given a sentence w and its unannotated tree T , consider a nonterminal A spanning (r, t) and its children B and C spanning (r, s) and (s, t). Let Ax be a subsymbol of A, By of B, and Cz of C. Then the inside and def outside probabilities PIN (r, t, Ax ) = P (wr:t |Ax ) and def POUT (r, t, Ax ) = P (w1:r Ax wt:n ) can be computed reencourages sparsity) suggest a large reduction. 2 Other techniques are also possible; Henderson (2004) uses neural networks to induce latent left-corner parser states. 434 Although we show only the binary component here, of course there are both binary and unary productions that are included. In the Expectation step, one computes the posterior probability of each annotated rule and position in each training set tree T : P ((r, s, t, Ax → By Cz )|w, T ) ∝ POUT (r, t, Ax ) ×β(Ax → By Cz )PIN (r, s, By )PIN (s, t, Cz ) (1) In the Maximization step, one uses the above probabilities as weighted observations to update the rule probabilities: β(Ax → By Cz ) := P #{Ax → By Cz } #{Ax → By′ Cz′ } y ′ ,z"
P06-1055,J98-4004,0,0.0704583,"Missing"
P06-1055,P03-1054,1,0.417007,"with a Markov grammar and manually splitting symbols in response to observed linguistic trends in the data. For example, the symbol NP might be split into the subsymbol NPˆS in subject position and the subsymbol NPˆVP in object position. Recently, Matsuzaki et al. (2005) and also Prescher (2005) exhibited an automatic approach in which each symbol is split into a fixed number of subsymbols. For example, NP would be split into NP-1 through NP-8. Their exciting result was that, while grammars quickly grew too large to be managed, a 16-subsymbol induced grammar reached the parsing performance of Klein and Manning (2003)’s manual grammar. Other work has also investigated aspects of automatic grammar refinement; for example, Chiang and Bikel (2002) learn annotations such as head rules in a constrained declarative language for tree-adjoining grammars. We present a method that combines the strengths of both manual and automatic approaches while addressing some of their common shortcomings. Like Matsuzaki et al. (2005) and Prescher (2005), we induce splits in a fully automatic fashion. However, we use a more sophisticated split-and-merge approach that allocates subsymbols adaptively where they are most effective,"
P06-1055,P05-1010,0,0.430483,"ization and intricate smoothing (Collins, 1999; Charniak, 2000). In this paper, we investigate the learning of a grammar consistent with a treebank at the level of evaluation symbols (such as NP, VP, etc.) but split based on the likelihood of the training trees. Klein and Manning (2003) addressed this question from a linguistic perspective, starting with a Markov grammar and manually splitting symbols in response to observed linguistic trends in the data. For example, the symbol NP might be split into the subsymbol NPˆS in subject position and the subsymbol NPˆVP in object position. Recently, Matsuzaki et al. (2005) and also Prescher (2005) exhibited an automatic approach in which each symbol is split into a fixed number of subsymbols. For example, NP would be split into NP-1 through NP-8. Their exciting result was that, while grammars quickly grew too large to be managed, a 16-subsymbol induced grammar reached the parsing performance of Klein and Manning (2003)’s manual grammar. Other work has also investigated aspects of automatic grammar refinement; for example, Chiang and Bikel (2002) learn annotations such as head rules in a constrained declarative language for tree-adjoining grammars. We present a"
P06-1055,P92-1017,0,0.169459,"Missing"
P06-1055,J98-1004,0,0.116152,"ons). For example, plural common nouns (NNS) divide into the maximum number of categories (16). One category consists primarily of dates, whose typical parent is an NP subsymbol whose typical parent is a root S, essentially modeling the temporal noun annotation discussed in Klein and Manning (2003). Another category specializes in capitalized words, preferring as a parent an NP with an S parent (i.e. subject position). A third category specializes in monetary units, and so on. These kinds of syntactico-semantic categories are typical, and, given distributional clustering results like those of Schuetze (1998), unsurprising. The singular nouns are broadly similar, if slightly more homogenous, being dominated by categories for stocks and trading. The proper noun category (NNP, shown) also splits into the maximum 16 categories, including months, countries, variants of Co. and Inc., first names, last names, initials, and so on. Verbal categories are also heavily split. Verbal subcategories sometimes reflect syntactic selectional preferences, sometimes reflect semantic selectional preferences, and sometimes reflect other aspects of verbal syntax. For example, the present tense third person verb subsymb"
P06-1055,J98-2004,0,\N,Missing
P06-1055,J03-4003,0,\N,Missing
P11-1061,N10-1083,0,0.250923,"Missing"
P11-1061,A00-1031,0,0.143858,"n into account yet. • With LP: Our full model uses both stages of label propagation (Eq. 2) before extracting the constraint features. As a result, we are able to extract the constraint feature for all foreign word types and furthermore expect the projected tag distributions to be smoother and more stable. Our oracles took advantage of the labeled treebanks: • TB Dictionary: We extracted tagging dictionaries from the treebanks and and used them as constraint features in the feature-based HMM. Evaluation was done using the prespecified mappings. • Supervised: We trained the supervised model of Brants (2000) on the original treebanks and mapped the language-specific tags to the universal tags for evaluation. 6.4 Experimental Setup While we tried to minimize the number of free parameters in our model, there are a few hyperparameters that need to be set. Fortunately, performance was stable across various values, and we were able to use the same hyperparameters for all languages. We used C = 1.0 as the L2 regularization constant in (Eq. 10) and trained both EM and L-BFGS for 1000 iterations. When extracting the vector Model EM-HMM baselines Feature-HMM Projection No LP our approach With LP TB Dictio"
P11-1061,J93-2003,0,0.0267443,"Missing"
P11-1061,W06-2920,0,0.320179,"e presenting our results, we describe the datasets that we used, as well as two baselines. 6.1 z −CkΘk22 Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. However, we do not explore this possibility in the current work. Datasets We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side. The availability of these resources guided our selection of foreign languages. For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). The parallel data came from the Europarl corpus (Koehn, 2005) and the ODS United Nations dataset (UN, 2006). Taking the intersection of languages in these resources, and selecting languages with large amounts of parallel data, yields the following set of eight Indo-European languages: Danish, Dutch, German, Greek, Italian, Portuguese, Spanish and Swedish. Of course, we are primarily interested in applying our techniques to languages for which no labeled resources are available. However, we needed to restrict ourselves to these languages in order to be able to evaluate th"
P11-1061,D10-1056,0,0.0918227,"anguages. Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus. As discussed in more detail in §3, we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types, while the vertices on the English side are individual word types. Graph construction does not require any labeled data, but makes use of two similarity functions. The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function, designed to indicate how syntactically 2 See Christodoulopoulos et al. (2010) for a discussion of metrics for evaluating unsupervised POS induction systems. 601 Algorithm 1 Bilingual POS Induction Require: Parallel English and foreign language data De and Df , unlabeled foreign training data Γf ; English tagger. Ensure: Θf , a set of parameters learned using a constrained unsupervised model (§5). 1: D e↔f ← word-align-bitext(D e , D f ) ce ← pos-tag-supervised(De ) 2: D ce ) 3: A ← extract-alignments(D e↔f , D f f 4: G ← construct-graph(Γ , D , A) ˜ ← graph-propagate(G) 5: G ˜ 6: ∆ ← extract-word-constraints(G) f 7: Θ ← pos-induce-constrained(Γf , ∆) 8: Return Θf simil"
P11-1061,P09-1042,0,0.572213,"Missing"
P11-1061,D07-1031,0,0.0799468,"or our experiments; however, one could as well have fixed the set of HMM states to be a constant across languages, and created one mapping to the universal POS tagset. 6.3 Various Models To provide a thorough analysis, we evaluated three baselines and two oracles in addition to two variants of our graph-based approach. We were intentionally lenient with our baselines: • EM-HMM: A traditional HMM baseline, with multinomial emission and transition distributions estimated by the Expectation Maximization algorithm. We evaluated POS tagging accuracy using the lenient many-to-1 evaluation approach (Johnson, 2007). • Feature-HMM: The vanilla feature-HMM of Berg-Kirkpatrick et al. (2010) (i.e. no additional constraint feature) served as a second baseline. Model parameters were estimated with L-BFGS and evaluation again used a greedy many-to-1 mapping. • Projection: Our third baseline incorporates bilingual information by projecting POS tags directly across alignments in the parallel data. For unaligned words, we set the tag to the most frequent tag in the corresponding treebank. For 606 each language, we took the same number of sentences from the bitext as there are in its treebank, and trained a superv"
P11-1061,2005.mtsummit-papers.11,0,0.0472091,"−CkΘk22 Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. However, we do not explore this possibility in the current work. Datasets We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side. The availability of these resources guided our selection of foreign languages. For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). The parallel data came from the Europarl corpus (Koehn, 2005) and the ODS United Nations dataset (UN, 2006). Taking the intersection of languages in these resources, and selecting languages with large amounts of parallel data, yields the following set of eight Indo-European languages: Danish, Dutch, German, Greek, Italian, Portuguese, Spanish and Swedish. Of course, we are primarily interested in applying our techniques to languages for which no labeled resources are available. However, we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach. We paid particular attention to minimize the number o"
P11-1061,J93-2004,0,0.0522945,"Missing"
P11-1061,D10-1120,0,0.173472,"Missing"
P11-1061,petrov-etal-2012-universal,1,0.831691,"Missing"
P11-1061,P09-1057,0,0.179296,"Missing"
P11-1061,P07-1096,0,0.045915,"Missing"
P11-1061,P09-1009,0,0.228365,"est. To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language. This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al., 2000; Xi and Hwa, 2005; Ganchev et al., 2009). Naseem et al. (2009) and Snyder et al. (2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available. Our work is closest to that of Yarowsky and Ngai (2001), but differs in two important ways. First, we use a novel graph-based framework for projecting syntactic information across language boundaries. To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4). Second, we treat the project"
P11-1061,D10-1017,1,0.393286,"l (Zhu et al., 2003). Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand, using individual words as the vertices throws away the context 3 The word alignment methods do not use POS information. necessary for disambiguation; on the other hand, it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences. Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning. More recently, Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model. They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types, and edge weights based on distributional similarity, to improve a supervised conditional random field tagger. 3.1 Graph Vertices We extend Subramanya et al.’s intuitions to our bilingual setup. Because the information flow in our graph is asymmetric (from English to the foreign language), we use different types of vertices for each language. The foreign language vertices (denoted by Vf ) correspond to fore"
P11-1061,C96-2141,0,0.211995,"Missing"
P11-1061,H05-1107,0,0.173919,"Missing"
P11-1061,N01-1026,0,0.91404,"ource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language. This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al., 2000; Xi and Hwa, 2005; Ganchev et al., 2009). Naseem et al. (2009) and Snyder et al. (2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available. Our work is closest to that of Yarowsky and Ngai (2001), but differs in two important ways. First, we use a novel graph-based framework for projecting syntactic information across language boundaries. To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4). Second, we treat the projected labels as features in an unsuper1 For simplicity of exposition we refer to the resource-poor language as the “foreign language.” Similarly, we use English as the resource-rich language, but an"
P11-1061,D07-1096,0,\N,Missing
P12-2047,D08-1107,0,0.0259032,"Missing"
P12-2047,W02-1001,0,0.0946452,"Missing"
P12-2047,P06-1063,0,0.061908,"Missing"
P12-2047,P09-1097,0,0.376704,"Missing"
P12-2047,J93-2004,0,0.0413361,"h of two other taggings. Note that snippet 1 and the query get different taggings primarily due to orthographic variations. It would then add “budget/NNP rent/NNP a/NNP car/NNP” to its training set. The interpolation approach of Bendersky et al. (2010) would tag the query as “budget/NNP rent/VB a/DET car/NN”. To see why this is the case, consider the probability for rent/VB vs rent/NNP. For rent/VB we have 0.2 + 0.8 × 23 , while for rent/NNP we have 0 + 0.8 × 31 assuming that π(VB|rent) = 1. 3 Experimental Setup We assume that we have access to labeled English sentences from the PennTreebank (Marcus et al., 1993) and the QuestionBank (Judge et al., 2006), as well as large amounts of unlabeled search queries. Each query is paired with a set of relevant results represented by snippets (sentence fragments containing the search terms), as well as information about the order in which the results were shown to the user and possibly the result the user clicked on. Note that different sets of results are possible for the same query, because of personalization and ranking changes over time. 3.1 Method D IRECT- CLICK D IRECT- ALL D IRECT- TOP -1 Evaluation Data We use two data sets for evaluation. The first is"
P12-2047,petrov-etal-2012-universal,1,0.604169,"Missing"
P12-2047,P11-1097,0,0.0559009,"Missing"
P12-2047,P08-1086,0,0.0889423,"Missing"
P12-2047,P10-1136,0,\N,Missing
P12-3029,de-marneffe-etal-2006-generating,0,0.124502,"Missing"
P12-3029,P06-1063,0,0.00936653,"Missing"
P12-3029,J93-2004,0,0.0435453,"ecific syntactic configuration, but ignoring the number of words that appear in between. 5 POS Tags base adapted 97.9 97.9 96.8 97.5 94.2 97.5 91.6 93.3 Domain Adaptation The results on the treebank evaluation sets need to be taken with caution, since performance often suffers when generalized to other domains. To get a better estimate of the POS tagging and parsing accuracies we conducted a detailed study for English. We chose English since it is the largest language in our corpus and because labeled treebank data for multiple domains is available. In addition to the WSJ (newswire) treebank (Marcus et al., 1993), we use: the Brown corpus (Francis and Kucera, 1979), which provides a balanced sample of text from the early 1960s; the QuestionBank (Judge et 173 al., 2006), which consists entirely of questions; and the PPCMBE corpus (Kroch et al., 2010), which contains modern British English from 1700 to 1914 and is perhaps most close to our application domain. Since the English treebanks are in constituency format, we used the StanfordConverter (de Marneffe et al., 2006) to convert the parse trees to dependencies and ignored the arc labels. The dependency conversion was unfortunately not possible for the"
P12-3029,P05-1013,0,0.00574035,"el (Nivre, 2008) with an arc-eager transition strategy. A linear kernel SVM with the following features is used for prediction: the partof-speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available). All non-lexical feature conjunctions are 172 included. For treebanks with non-projective trees we use the pseudo-projective parsing technique to transform the treebank into projective structures (Nivre and Nilsson, 2005). To standardize and simplify the dependency relations across languages we use unlabeled directed dependency arcs. Table 3 shows unlabeled attachment scores on the treebank evaluation sets with automatically predicted POS tags. 4.3 Syntactic Ngrams As described above, we extract raw ngrams (n ≤ 5) from the book text. Additionally, we provide ngrams annotated with POS tags and dependency relations. The syntactic ngrams comprise words (e.g., burnt), POS-annotated words (e.g. burnt VERB), and POS tags (e.g., VERB ). All of these forms can be mixed freely in 1-, 2- and 3-grams (e.g., the ADJ toast"
P12-3029,J08-4003,0,0.00741823,"ncy syntax representation, since it is intuitive to work with and can be predicted effectively. Additionally, dependency parse tree corpora exist for several languages, making the representation desirable from a practical standpoint. Dependency parse trees specify pairwise relationships between words in the same sentence. Directed arcs specify which words modify a given word (if any), or alternatively, which head word governs a given word (there can only be one). For example, in Figure 2, hair is the head of the modifier short. We use a deterministic transition-based dependency parsing model (Nivre, 2008) with an arc-eager transition strategy. A linear kernel SVM with the following features is used for prediction: the partof-speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available). All non-lexical feature conjunctions are 172 included. For treebanks with non-projective trees we use the pseudo-projective parsing technique to transform the treebank into projective structures (Nivre and Nilsso"
P12-3029,petrov-etal-2012-universal,1,0.104095,"adapt our models to handle noisy and historical text. We perform POS tagging with a state-of-the-art2 Conditional Random Field (CRF) based tagger (Lafferty et al., 2001) trained on manually annotated treebank data. We use the following fairly standard features in our tagger: current word, suffixes and prefixes of length 1, 2 and 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word. To provide a language-independent interface, we use the universal POS tagset described in detail in Petrov et al. (2012). This universal POS tagset defines the following twelve POS tags, which exist in similar form in most languages: N OUN (nouns), V ERB (verbs), A DJ (adjectives), A DV (adverbs), P RON (pronouns), D ET (determiners and articles), A DP (prepositions and postpositions), N UM (numerals), C ONJ (conjunctions), P RT (particles), ‘.’ (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words). Table 2 shows the two most common words for 2 On a standard benchmark (training on sections 1-18 of the Penn Treebank (Marcus et al., 1993) and testing on sections 2224)"
P12-3029,P08-1086,0,0.00490517,"ter-annotator agreement (Manning, 2011). As we demonstrate in the next section, these numbers are misleading since they are computed on test data that is very close to the training domain. We therefore need to specifically adapt our models to handle noisy and historical text. We perform POS tagging with a state-of-the-art2 Conditional Random Field (CRF) based tagger (Lafferty et al., 2001) trained on manually annotated treebank data. We use the following fairly standard features in our tagger: current word, suffixes and prefixes of length 1, 2 and 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word. To provide a language-independent interface, we use the universal POS tagset described in detail in Petrov et al. (2012). This universal POS tagset defines the following twelve POS tags, which exist in similar form in most languages: N OUN (nouns), V ERB (verbs), A DJ (adjectives), A DV (adverbs), P RON (pronouns), D ET (determiners and articles), A DP (prepositions and postpositions), N UM (numerals), C ONJ (conjunctions), P RT (particles), ‘.’ (punctuation marks) and X (a catch-all for other categ"
P13-2017,W06-2920,0,0.808379,"ebank is made freely available in order to facilitate research on multilingual dependency parsing.1 1 Introduction In recent years, syntactic representations based on head-modifier dependency relations between words have attracted a lot of interest (K¨ubler et al., 2009). Research in dependency parsing – computational methods to predict such representations – has increased dramatically, due in large part to the availability of dependency treebanks in a number of languages. In particular, the CoNLL shared tasks on dependency parsing have provided over twenty data sets in a standardized format (Buchholz and Marsi, 2006; Nivre et al., 2007). While these data sets are standardized in terms of their formal representation, they are still heterogeneous treebanks. That is to say, despite them all being dependency treebanks, which annotate each sentence with a dependency tree, they subscribe to different annotation schemes. This can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/. 92 Proceedings of the 51st Annual Mee"
P13-2017,W02-1503,0,0.0535563,"Missing"
P13-2017,W09-2307,0,0.0712427,"Missing"
P13-2017,P11-1061,1,0.243183,"aking fine-grained label distinctions was discouraged. Once these guidelines were fixed, annotators selected roughly an equal amount of sentences to be annotated from each domain in the unlabeled data. As the sentences were already randomly selected from a larger corpus, annotators were told to view the sentences in order and to discard a sentence only if it was 1) fragmented because of a sentence splitting error; 2) not from the language of interest; 3) incomprehensible to a native speaker; or 4) shorter than three words. The selected sentences were pre-processed using cross-lingual taggers (Das and Petrov, 2011) and parsers (McDonald et al., 2011). The annotators modified the pre-parsed trees using the TrEd2 tool. At the beginning of the annotation process, double-blind annotation, followed by manual arbitration and consensus, was used iteratively for small batches of data until the guidelines were finalized. Most of the data was annotated using single-annotation and full review: one annotator annotating the data and another reviewing it, making changes in close collaboration with the original annotator. As a final step, all annotated data was semi-automatically checked for annotation consistency. 2."
P13-2017,W08-1301,0,0.173029,"Missing"
P13-2017,D11-1006,1,0.855635,"icient if one’s goal is to build monolingual parsers and evaluate their quality without reference to other languages, as in the original CoNLL shared tasks, but there are many cases where heterogenous treebanks are less than adequate. First, a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to cr"
P13-2017,P07-1122,1,0.763455,"as head in copula constructions. For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In addition, a small number of constructions required structural conversion, notably coordination, which in the Swedish Treebank is given a Prague style analysis (Nilsson et al., 2007). For both English and Swedish, we mapped the language-specific partof-speech tags to universal tags using the mappings of Petrov et al. (2012). Towards A Universal Treebank The Stanford typed dependencies for English (De Marneffe et al., 2006; de Marneffe and Manning, 2008) serve as the point of departure for our ‘universal’ dependency representation, together with the tag set of Petrov et al. (2012) as the underlying part-of-speech representation. The Stanford scheme, partly inspired by the LFG framework, has emerged as a de facto standard for dependency annotation in English and has recentl"
P13-2017,de-marneffe-etal-2006-generating,0,0.333356,"Missing"
P13-2017,P09-1042,1,0.250489,"arsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to create homogenous syntactic dependency annotation in multiple languages. In terms of automatic construction, Zeman et al. (2012) attempt to harmonize a large number of dependency treebanks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common synWe present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages: German, English,"
P13-2017,W12-1909,0,0.0247816,"Missing"
P13-2017,petrov-etal-2012-universal,1,0.702758,"nal Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do the same for syntactic dependencies and present cross-lingual parsing experiments to highlight some of the benefits of cross-lingually consistent annotation. First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of McDonald et al. (2011). Second, the evaluation scores in general are significantly higher than previous cross-lingual studies, su"
P13-2017,D09-1086,0,0.0317978,"ross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to create homogenous syntactic dependency annotation in multiple languages. In terms of automatic construction, Zeman et al. (2012) attempt to harmonize a large number of dependency treebanks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common synWe present a new collection of treebanks with homogeneous syntactic dependency annotation for six lang"
P13-2017,Q13-1001,1,0.0665003,"Missing"
P13-2017,W04-2709,0,0.0429789,"annotation schemes. This can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/. 92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 201"
P13-2017,P13-2103,0,0.129897,"Missing"
P13-2017,N06-2015,0,0.0347787,"s can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/. 92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do th"
P13-2017,zeman-etal-2012-hamledt,0,0.060315,"Missing"
P13-2017,P03-1054,0,0.0144203,"nch data set is shown in Figure 1. We take two approaches to generating data. The first is traditional manual annotation, as previously used by Helmreich et al. (2004) for multilingual syntactic treebank construction. The second, used only for English and Swedish, is to automatically convert existing treebanks, as in Zeman et al. (2012). 2.1 Automatic Conversion Since the Stanford dependencies for English are taken as the starting point for our universal annotation scheme, we begin by describing the data sets produced by automatic conversion. For English, we used the Stanford parser (v1.6.8) (Klein and Manning, 2003) to convert the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) to basic dependency trees, including punctuation and with the copula verb as head in copula constructions. For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In"
P13-2017,P11-2033,1,0.192695,"Missing"
P13-2017,P04-1061,0,0.0673447,"word expressions (Nilsson et al., 2007; K¨ubler et al., 2009; Zeman et al., 2012). These data sets can be sufficient if one’s goal is to build monolingual parsers and evaluate their quality without reference to other languages, as in the original CoNLL shared tasks, but there are many cases where heterogenous treebanks are less than adequate. First, a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2"
P13-2017,D12-1125,0,0.0145332,"for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do the same for syntactic dependencies and present cross-lingual parsing experiments to highlight some of the benefits of cross-lingually consistent annotation. First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of McDonald et al. (2011). Second, the evaluation scores in general are significantly higher than previous cross-lingual studies, suggesting that most of these studies underestimate true accuracy. Finally, unlike all previous cross-ling"
P13-2017,J93-2004,0,\N,Missing
P13-2017,W08-1300,0,\N,Missing
P13-2017,D07-1096,1,\N,Missing
P14-5020,N13-1138,0,0.0154841,"across languages, but also for the same language), it can be useful to examine and potentially aggregate the frequencies of all inflected forms. This can be accomplished by manually deriving all inflected forms and then using arithmetic operations to aggregate their counts. Our new inflected form search accomplishes this automatically. By appending the keyword INF to a word, a set of ngrams with all inflected forms of the word will be retrieved. To generate the inflected forms we make use of Wiktionary3 and supplement it with automatically generated inflection tables based on the approach of Durrett and DeNero (2013). Because there are at most a few dozen inflected forms for any given word, we can afford to substitute and retrieve all inflections of the marked word, even the ones that are not grammatical in a given ngram context. This has the advantage that we only need to store inflected forms for individual words rather than entire ngrams. If a generated ngram has no support in the corpus, we simply omit it from the final set of results. We do not perform any additional filtering; as a result, an inflection search can produce many results, especially for morphologically rich languages like Russian. We h"
P14-5020,P12-3029,1,0.911178,"and retrieve a collection of ngrams, to facilitate the discovery of patterns in the underlying data. First, users can replace one query term with a placeholder symbol ‘*’ (wildcard, henceforth), which will return the ten most frequent expansions of the wildcard in the corpus for the specified year range. Second, by adding a specific marker to any word in a query (‘ INF’), ngrams with all Introduction The Google Books Ngram project facilitates the analysis of cultural, social and linguistic trends through five centuries of written text in eight languages. The Ngram Corpus (Michel et al., 2011; Lin et al., 2012) consists of words and phrases (i.e., ngrams) and their usage frequency over time.1 The interactive Ngram Viewer2 allows users to retrieve and plot the frequency of multiple ngrams on a simple webpage. The Viewer is widely popular and can be used to efficiently explore and visualize patterns in the underlying ngram data. For example, the ngram data has been used to detect emotion trends in 20th century books (Acerbi et al., 2013), to analyze text focusing on market capitalism throughout the past century (Schulz and Robinson, 2013), detect social and cultural impact of historical personalities"
P14-5020,petrov-etal-2012-universal,1,0.750823,"d is described in Lin et al. 116 tences to enable the distinction of sentence medial ngrams from those near sentence boundaries. They also ensure that sentences that span across page boundaries are included. Due to these differences, as well as the availability of additional book data, improvements to the optical character recognition algorithms and metadata extraction for dating the books, the ngrams counts from the two editions are not the same. The edition from Lin et al. (2012) additionally includes syntactic ngrams. The corpus is tagged using the universal part-of-speech (POS) tag set of Petrov et al. (2012): NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners and articles), ADP (prepositions and postpositions), CONJ (conjunctions). Words can be disambiguated by their POS tag by simply appending the tag to the word with an underscore (e.g. book NOUN) and can also be replaced by POS tags in the ngrams, see Lin et al. (2012) for details. The corpus is parsed with a dependency parser and head-modifier syntactic relations between words in the same sentence are extracted. Dependency relations are represented as ‘=>’ in the corpus. Our new enhanced search feat"
P15-1032,D09-1087,0,0.0207389,"Missing"
P15-1032,E12-1009,0,0.0813111,"Missing"
P15-1032,P06-1063,0,0.191628,"Missing"
P15-1032,C10-1011,0,0.0209909,"Missing"
P15-1032,P08-1068,1,0.672703,"quires a feature-vector definition φ that maps a sentence x together with a configuration c to a feature vector φ(x, c) ∈ Rd . There is a one-to-one mapping between configurations c and decision sequences 3.3 Incorporating Unlabeled Data Given the high capacity, non-linear nature of the deep network we hypothesize that our model can 1 If the gold parse tree stays within the beam until the end of the sentence, conventional perceptron updates are used. 326 4 be significantly improved by incorporating more data. One way to use unlabeled data is through unsupervised methods such as word clusters (Koo et al., 2008); we follow Chen and Manning (2014) and use pretrained word embeddings to initialize our model. The word embeddings capture similar distributional information as word clusters and give consistent improvements by providing a good initialization and information about words not seen in the treebank data. Experiments In this section we present our experimental setup and the main results of our work. 4.1 Experimental Setup We conduct our experiments on two English language benchmarks: (1) the standard Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) and (2) a more comprehen"
P15-1032,W06-2920,0,0.0478166,"Missing"
P15-1032,P14-1043,0,0.205475,"Missing"
P15-1032,J93-2004,0,0.0558856,"Missing"
P15-1032,D14-1082,0,0.790216,"l Network Transition-Based Parsing David Weiss Chris Alberti Michael Collins Slav Petrov Google Inc New York, NY {djweiss,chrisalberti,mjcollins,slav}@google.com Abstract perceptron training algorithm can greatly improve accuracy. Nonetheless, significant manual feature engineering was required before transitionbased systems provided competitive accuracy with graph-based parsers (Zhang and Nivre, 2011), and only by incorporating graph-based scoring functions were Bohnet and Kuhn (2012) able to exceed the accuracy of graph-based approaches. In contrast to these carefully hand-tuned approaches, Chen and Manning (2014) recently presented a neural network version of a greedy transition-based parser. In their model, a feedforward neural network with a hidden layer is used to make the transition decisions. The hidden layer has the power to learn arbitrary combinations of the atomic inputs, thereby eliminating the need for hand-engineered features. Furthermore, because the neural network uses a distributed representation, it is able to model lexical, part-of-speech (POS) tag, and arc label similarities in a continuous space. However, although their model outperforms its greedy hand-engineered counterparts, it i"
P15-1032,P13-2109,0,0.0322801,"Missing"
P15-1032,D13-1129,0,0.0762893,"Missing"
P15-1032,N06-1020,0,0.256666,"Missing"
P15-1032,P04-1015,1,0.226241,"eriments. 3.2 v(y j ) · φ(x, y1 . . . y j−1 ). j=1 Thus each decision y j receives a score: where λ is a regularization hyper-parameter over the hidden layer parameters (we use λ = 10−4 in all experiments) and j sums over all decisions and configurations {y j , c j } extracted from gold parse trees in the dataset. The specific update rule we apply at iteration t is as follows: gt = µgt−1 − ∆L(Θt ), m X Structured Perceptron Training Given the hidden representations, we now describe how the perceptron can be trained to utilize these representations. The perceptron algorithm with early updates (Collins and Roark, 2004) requires a feature-vector definition φ that maps a sentence x together with a configuration c to a feature vector φ(x, c) ∈ Rd . There is a one-to-one mapping between configurations c and decision sequences 3.3 Incorporating Unlabeled Data Given the high capacity, non-linear nature of the deep network we hypothesize that our model can 1 If the gold parse tree stays within the beam until the end of the sentence, conventional perceptron updates are used. 326 4 be significantly improved by incorporating more data. One way to use unlabeled data is through unsupervised methods such as word cluster"
P15-1032,de-marneffe-etal-2006-generating,0,0.0608748,"Missing"
P15-1032,P15-1033,0,0.237047,"course easier to parse, having an average length of 15 words, compared to 24 words for the tune set overall. However, because we only use these sentences to extract individual transition decisions, the shorter length does not seem to hurt their utility. We generate 107 tokens worth of new parses and use this data in the backpropagation stage of training. 327 Method UAS LAS Beam Graph-based Bohnet (2010) 92.88 90.71 Martins et al. (2013) 92.89 90.55 Zhang and McDonald (2014) 93.22 91.02 n/a n/a n/a Transition-based ? Zhang and Nivre (2011) Bohnet and Kuhn (2012) Chen and Manning (2014) S-LSTM (Dyer et al., 2015) Our Greedy Our Perceptron 93.00 93.27 91.80 93.20 93.19 93.99 90.95 91.19 89.60 90.90 91.18 92.05 32 40 1 1 1 8 Tri-training ? Zhang and Nivre (2011) Our Greedy Our Perceptron 92.92 90.88 93.46 91.49 94.26 92.41 32 1 8 Method Transition-based ? Zhang and Nivre (2011) Bohnet and Kuhn (2012) Our Greedy Our Perceptron (B=16) 91.15 91.69 91.21 92.25 Tri-training ? Zhang and Nivre (2011) Our Greedy Our Perceptron (B=16) 91.46 85.51 91.36 91.82 86.37 90.58 92.62 87.00 93.05 85.24 85.33 85.41 86.44 92.46 92.21 90.61 92.06 Table 2: Final Treebank Union test set results. We report LAS only for brevity"
P15-1032,P04-1013,0,0.163874,"the architecture and modeling choices we introduce in this paper. Neural Network Model In this section, we describe the architecture of our model, which is summarized in Figure 1. Note that we separate the embedding processing to a distinct “embedding layer” for clarity of presentation. Our model is based upon that of Chen and Manning (2014) and we discuss the differences between our model and theirs in detail at the end of this section. We use the arc-standard (Nivre, 2004) transition system. Finally, we also note that neural network representations have a long history in syntactic parsing (Henderson, 2004; Titov and Henderson, 2007; Titov and Henderson, 2010); however, like Chen and Manning (2014), our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients. Our work is also not the first to apply structured training to neural networks (see e.g. Peng et al. (2009) and Do and Artires (2010) for Conditional Random Field (CRF) training of neural networks). Our paper ex2.1 Input layer Given a parse configuration c (consisting of a stack s and a buffer b), we extract a rich set of discrete features which w"
P15-1032,W04-0308,0,0.397798,"and other retrospective experiments. One of the goals of this work is to provide guidance for future refinements and improvements on the architecture and modeling choices we introduce in this paper. Neural Network Model In this section, we describe the architecture of our model, which is summarized in Figure 1. Note that we separate the embedding processing to a distinct “embedding layer” for clarity of presentation. Our model is based upon that of Chen and Manning (2014) and we discuss the differences between our model and theirs in detail at the end of this section. We use the arc-standard (Nivre, 2004) transition system. Finally, we also note that neural network representations have a long history in syntactic parsing (Henderson, 2004; Titov and Henderson, 2007; Titov and Henderson, 2010); however, like Chen and Manning (2014), our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients. Our work is also not the first to apply structured training to neural networks (see e.g. Peng et al. (2009) and Do and Artires (2010) for Conditional Random Field (CRF) training of neural networks). Our paper ex2.1"
P15-1032,N06-2015,0,0.0159683,"Missing"
P15-1032,J08-4003,0,0.235391,"knowledge is the best accuracy on Stanford Dependencies to date. We also provide indepth ablative analysis to determine which aspects of our model provide the largest gains in accuracy. 1 Introduction Syntactic analysis is a central problem in language understanding that has received a tremendous amount of attention. Lately, dependency parsing has emerged as a popular approach to this problem due to the availability of dependency treebanks in many languages (Buchholz and Marsi, 2006; Nivre et al., 2007; McDonald et al., 2013) and the efficiency of dependency parsers. Transition-based parsers (Nivre, 2008) have been shown to provide a good balance between efficiency and accuracy. In transition-based parsing, sentences are processed in a linear left to right pass; at each position, the parser needs to choose from a set of possible actions defined by the transition strategy. In greedy models, a classifier is used to independently decide which transition to take based on local features of the current parse configuration. This classifier typically uses hand-engineered features and is trained on individual transitions extracted from the gold transition sequence. While extremely fast, these greedy mo"
P15-1032,N03-1033,0,0.211668,"Missing"
P15-1032,P06-1055,1,0.177457,", but with a more diverse dataset for training and evaluation. Following Vinyals et al. (2015), we use (in addition to the WSJ), the OntoNotes corpus version 5 (Hovy et al., 2006), the English Web Treebank (Petrov and McDonald, 2012), and the updated and corrected Question Treebank (Judge et al., 2006). We train on the union of each corpora’s training set and test on each domain separately. We refer to this setup as the “Treebank Union” setup. In our semi-supervised experiments, we use the corpus from Chelba et al. (2013) as our source of unlabeled data. We process it with the BerkeleyParser (Petrov et al., 2006), a latent variable constituency parser, and a reimplementation of ZPar (Zhang and Nivre, 2011), a transition-based parser with beam search. Both parsers are included as baselines in our evaluation. We select the first 107 tokens for which the two parsers agree as additional training data. For our tri-training experiments, we re-train the POS tagger using the POS tags assigned on the unlabeled data from the Berkeley constituency parser. This increases POS However, obtaining more training data is even more important than a good initialization. One potential way to obtain additional training dat"
P15-1032,W03-3023,0,0.871614,"Missing"
P15-1032,D10-1069,1,0.495674,"Missing"
P15-1032,W97-0301,0,0.0405813,"Missing"
P15-1032,D09-1058,1,0.921961,"Missing"
P15-1032,D08-1059,0,0.780514,"Missing"
P15-1032,P14-2107,0,0.0889251,"Missing"
P15-1032,P11-2112,0,0.0631238,"Missing"
P15-1032,P11-2033,0,0.723082,"he activations from all layers as features in the structured perceptron. Using the probability estimates directly is very similar to Ratnaparkhi (1997), where a maximum-entropy model was used to model the distribution over possible actions at each parser state, and beam search was used to search for the highest probability parse. A known problem with beam search in this setting is the label-bias problem. Table 5 shows the impact of using structured perceptron training over using the softmax function during beam search as a function of the beam size used. For reference, our reimplementation of Zhang and Nivre (2011) is trained equivalently for each setting. We also show the impact on beam size when tri-training is used. Although the beam does marginally improve accuracy for the softmax model, much greater gains are achieved when perceptron training is used. Increasing hidden units yields large gains. For these experiments, we fixed the embedding sizes Dword = 64, Dtag = Dlabel = 32 and tried increasing and decreasing the dimensionality of the hidden layers on a logarthmic scale. Improvements in accuracy did not appear to saturate even with increasing the number of hidden units by an order of magnitude, t"
P15-1032,D07-1099,0,0.0367885,"and modeling choices we introduce in this paper. Neural Network Model In this section, we describe the architecture of our model, which is summarized in Figure 1. Note that we separate the embedding processing to a distinct “embedding layer” for clarity of presentation. Our model is based upon that of Chen and Manning (2014) and we discuss the differences between our model and theirs in detail at the end of this section. We use the arc-standard (Nivre, 2004) transition system. Finally, we also note that neural network representations have a long history in syntactic parsing (Henderson, 2004; Titov and Henderson, 2007; Titov and Henderson, 2010); however, like Chen and Manning (2014), our network avoids any recurrent structure so as to keep inference fast and efficient and to allow the use of simple backpropagation to compute gradients. Our work is also not the first to apply structured training to neural networks (see e.g. Peng et al. (2009) and Do and Artires (2010) for Conditional Random Field (CRF) training of neural networks). Our paper ex2.1 Input layer Given a parse configuration c (consisting of a stack s and a buffer b), we extract a rich set of discrete features which we feed into the neural netw"
P15-1032,P15-1117,0,0.197062,"input source: words, POS tags, and arc labels. The full set of features is given in Table 2. The features extracted for each group are represented as a sparse F ⇥ V matrix X, where V is the size of the vocabulary of the feature group and F is the number of features: the value of element Xf v is 1 if the f ’th feature takes on value v. We produce three input matrices: Xword for words features, Xtag for POS tag features, and Xlabel for arc labels. For all feature groups, we add additional special tends this line of work to the setting of inexact search with beam decoding for dependency parsing; Zhou et al. (2015) concurrently explored a similar approach using a structured probabilistic 2 Neural Network Model ranking objective. Dyer et al. (2015) concurrently In this section, we describe the architecture of our model, which is summarized in figure 2. NoteShort-Term that developed the Stack Long Memory we separate the embedding processing to a distinct (S-LSTM) architecture, which “embedding layer” for clarity of presentation. Our does incorporate model is based upon that of Chen and Manning recurrent architecture and look-ahead, and which yields comparable accuracy on the Penn Treebank to our greedy mo"
P15-1032,D07-1096,0,\N,Missing
P15-1032,W07-2218,0,\N,Missing
P16-1231,P99-1070,0,0.206492,"G that are not in PL . The proof that PG * PL gives a clear illustration of the label bias problem. Proof that PL ⊆ PG : We need to show that for any locally normalized distribution pL , we can construct a globally normalized model pG such 6 Smith and Johnson (2007) cite Michael Collins as the source of the example underlying the proof. Note that the theorem refers to conditional models of the form p(d1:n |x1:n ) with global or local normalization. Equivalence (or non-equivalence) results for joint models of the form p(d1:n , x1:n ) are quite different: for example results from Chi (1999) and Abney et al. (1999) imply that weighted context-free grammars (a globally normalized joint model) and probabilistic context-free grammars (a locally normalized joint model) are equally expressive. (7) Note that the input x2 = b is ambiguous: it can take tags B or D. This ambiguity is resolved when the next input symbol, c or e, is observed. Now consider a globally normalized model, where the scores ρ(d1:i−1 , di , x1:i ) are defined as follows. Define T as the set {(A, B), (B, C), (A, D), (D, E)} of bigram tag transitions seen in the data. Similarly, define E as the set {(a, A), (b, B), (c, C), (b, D), (e, E)} o"
P16-1231,D15-1159,1,0.642693,"Missing"
P16-1231,D15-1041,0,0.15485,"Missing"
P16-1231,D12-1133,0,0.21927,"Missing"
P16-1231,D15-1042,0,0.0431093,"Missing"
P16-1231,Q13-1033,0,0.121645,"Missing"
P16-1231,D14-1082,0,0.651136,"oduced impressive results on some of the classic NLP tasks such as part-ofspeech tagging (Ling et al., 2015), syntactic parsing (Vinyals et al., 2015) and semantic role labeling (Zhou and Xu, 2015). One might speculate that it is the recurrent nature of these models that enables these results. In this work we demonstrate that simple feedforward networks without any recurrence can achieve comparable or better accuracies than LSTMs, as long as they are globally normalized. Our model, described in detail in Section 2, uses a transition system (Nivre, 2006) and feature embeddings as introduced by Chen and Manning (2014). We do not use any recurrence, but perform beam search for maintaining multiple hy∗ On leave from Columbia University. potheses and introduce global normalization with a conditional random field (CRF) objective (Bottou et al., 1997; Le Cun et al., 1998; Lafferty et al., 2001; Collobert et al., 2011) to overcome the label bias problem that locally normalized models suffer from. Since we use beam inference, we approximate the partition function by summing over the elements in the beam, and use early updates (Collins and Roark, 2004; Zhou et al., 2015). We compute gradients based on this approxi"
P16-1231,J99-1004,0,0.253294,"tributions in PG that are not in PL . The proof that PG * PL gives a clear illustration of the label bias problem. Proof that PL ⊆ PG : We need to show that for any locally normalized distribution pL , we can construct a globally normalized model pG such 6 Smith and Johnson (2007) cite Michael Collins as the source of the example underlying the proof. Note that the theorem refers to conditional models of the form p(d1:n |x1:n ) with global or local normalization. Equivalence (or non-equivalence) results for joint models of the form p(d1:n , x1:n ) are quite different: for example results from Chi (1999) and Abney et al. (1999) imply that weighted context-free grammars (a globally normalized joint model) and probabilistic context-free grammars (a locally normalized joint model) are equally expressive. (7) Note that the input x2 = b is ambiguous: it can take tags B or D. This ambiguity is resolved when the next input symbol, c or e, is observed. Now consider a globally normalized model, where the scores ρ(d1:i−1 , di , x1:i ) are defined as follows. Define T as the set {(A, B), (B, C), (A, D), (D, E)} of bigram tag transitions seen in the data. Similarly, define E as the set {(a, A), (b, B), ("
P16-1231,P04-1015,1,0.818985,"on system (Nivre, 2006) and feature embeddings as introduced by Chen and Manning (2014). We do not use any recurrence, but perform beam search for maintaining multiple hy∗ On leave from Columbia University. potheses and introduce global normalization with a conditional random field (CRF) objective (Bottou et al., 1997; Le Cun et al., 1998; Lafferty et al., 2001; Collobert et al., 2011) to overcome the label bias problem that locally normalized models suffer from. Since we use beam inference, we approximate the partition function by summing over the elements in the beam, and use early updates (Collins and Roark, 2004; Zhou et al., 2015). We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss. In Section 3 we revisit the label bias problem and the implication that globally normalized models are strictly more expressive than locally normalized models. Lookahead features can partially mitigate this discrepancy, but cannot fully compensate for it—a point to which we return later. To empirically demonstrate the effectiveness of global normalization, we evaluate our model on part-of-speech tagging, synt"
P16-1231,de-marneffe-etal-2006-generating,0,0.00968088,"Missing"
P16-1231,P15-1030,0,0.068148,"ks. We apply our approach to POS tagging, syntactic dependency parsing, and sentence compression. While directly optimizing the global model defined by Eq. (5) works well, we found that training the model in two steps achieves the same precision much faster: we first pretrain the network using the local objective given in Eq. (4), and then perform additional training steps using the global objective given in Eq. (6). We pretrain all layers except the softmax layer in this way. We purposefully abstain from complicated hand engineering of input features, which might improve performance further (Durrett and Klein, 2015). We use the training recipe from Weiss et al. (2015) for each training stage of our model. Specifically, we use averaged stochastic gradient descent with momentum, and we tune the learning rate, learning rate schedule, momentum, and early stopping time using a separate held-out corpus for each task. We tune again with a different set of hyperparameters for training with the global objective. 4.1 Part of Speech Tagging Part of speech (POS) tagging is a classic NLP task, where modeling the structure of the output is important for achieving state-of-the-art performance. 2446 Data & Evaluation. W"
P16-1231,P15-1033,0,0.18042,"Missing"
P16-1231,P04-1013,0,0.655478,"Missing"
P16-1231,N06-2015,0,0.0782403,"Missing"
P16-1231,P06-1063,0,0.0228459,"Missing"
P16-1231,P14-1130,0,0.0317446,"Missing"
P16-1231,D15-1176,0,0.0214013,"Missing"
P16-1231,J93-2004,0,0.0799101,"Missing"
P16-1231,P13-2109,0,0.15041,"Missing"
P16-1231,P09-1040,0,0.0569419,"arameters θ(d) . We next describe how softmax-style normalization can be performed at the local or global level. 2.2 Global vs. Local Normalization In the Chen and Manning (2014) style of greedy neural network parsing, the conditional probability distribution over decisions dj given context d1:j−1 is defined as p(dj |d1:j−1 ; θ) = exp ρ(d1:j−1 , dj ; θ) , (1) ZL (d1:j−1 ; θ) where ZL (d1:j−1 ; θ) = X exp ρ(d1:j−1 , d0 ; θ). d0 ∈A(d1:j−1 ) 1 http://github.com/tensorflow/models/tree/master/syntaxnet http://www.tensorflow.org 3 Note that this is not true for the swap transition system defined in Nivre (2009). 2 4 It is straightforward to extend the approach to make use of dynamic programming in the case where the same state can be reached by multiple decision sequences. 2443 Each ZL (d1:j−1 ; θ) is a local normalization term. The probability of a sequence of decisions d1:n is pL (d1:n ) = n Y p(dj |d1:j−1 ; θ) j=1 P exp nj=1 ρ(d1:j−1 , dj ; θ) Qn . = j=1 ZL (d1:j−1 ; θ) (2) Beam search can be used to attempt to find the maximum of Eq. (2) with respect to d1:n . The additive scores used in beam search are the logsoftmax of each decision, ln p(dj |d1:j−1 ; θ), not the raw scores ρ(d1:j−1 , dj ; θ)."
P16-1231,J07-4003,0,0.381027,"the beam throughout decoding, a gradient step is performed using Bn , the beam at the end of decoding. 3 The Label Bias Problem Intuitively, we would like the model to be able to revise an earlier decision made during search, when later evidence becomes available that rules out the earlier decision as incorrect. At first glance, it might appear that a locally normalized model used in conjunction with beam search or exact search is able to revise earlier decisions. However the label bias problem (see Bottou (1991), Collins (1999) pages 222-226, Lafferty et al. (2001), Bottou and LeCun (2005), Smith and Johnson (2007)) means that locally normalized models often have a very weak ability to revise earlier decisions. This section gives a formal perspective on the label bias problem, through a proof that globally normalized models are strictly more expressive than locally normalized models. The theorem was originally proved5 by Smith and Johnson (2007). 5 More precisely Smith and Johnson (2007) prove the theorem for models with potential functions of the form ρ(di−1 , di , xi ); the generalization to potential functions of the form ρ(d1:i−1 , di , x1:i ) is straightforward. 2444 The example underlying the proo"
P16-1231,Q16-1014,0,0.0921649,"Missing"
P16-1231,P15-1113,0,0.262839,"Missing"
P16-1231,P15-1032,1,0.843195,"ce d1 . . . dj . We assume that there is a one-to-one mapping between decision sequences d1:j−1 and states sj : that is, we essentially assume that a state encodes the entire history of decisions. Thus, each state can be reached by a unique decision sequence from s† .4 We will use decision sequences d1:j−1 and states interchangeably: in a slight abuse of notation, we define ρ(d1:j−1 , d; θ) to be equal to ρ(s, d; θ) where s is the state reached by the decision sequence d1:j−1 . The scoring function ρ(s, d; θ) can be defined in a number of ways. In this work, following Chen and Manning (2014), Weiss et al. (2015), and Zhou et al. (2015), we define it via a feedforward neural network as ρ(s, d; θ) = φ(s; θ(l) ) · θ(d) . Transition System Given an input x, most often a sentence, we define: • A set of states S(x). • A special start state s† ∈ S(x). • A set of allowed decisions A(s, x) for all s ∈ S(x). • A transition function t(s, d, x) returning a new state s0 for any decision d ∈ A(s, x). We will use a function ρ(s, d, x; θ) to compute the score of decision d in state s for input x. The vector θ contains the model parameters and we assume that ρ(s, d, x; θ) is differentiable with respect to θ. In this"
P16-1231,K15-1015,0,0.059669,"Missing"
P16-1231,P14-2107,0,0.07694,"Missing"
P16-1231,P15-1109,0,0.0181463,"Missing"
P16-1231,P15-1117,0,0.518116,"nd feature embeddings as introduced by Chen and Manning (2014). We do not use any recurrence, but perform beam search for maintaining multiple hy∗ On leave from Columbia University. potheses and introduce global normalization with a conditional random field (CRF) objective (Bottou et al., 1997; Le Cun et al., 1998; Lafferty et al., 2001; Collobert et al., 2011) to overcome the label bias problem that locally normalized models suffer from. Since we use beam inference, we approximate the partition function by summing over the elements in the beam, and use early updates (Collins and Roark, 2004; Zhou et al., 2015). We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss. In Section 3 we revisit the label bias problem and the implication that globally normalized models are strictly more expressive than locally normalized models. Lookahead features can partially mitigate this discrepancy, but cannot fully compensate for it—a point to which we return later. To empirically demonstrate the effectiveness of global normalization, we evaluate our model on part-of-speech tagging, syntactic dependency par"
P16-1231,J03-4003,1,\N,Missing
P16-1231,N03-1014,0,\N,Missing
petrov-etal-2012-universal,D07-1013,1,\N,Missing
petrov-etal-2012-universal,zeman-2008-reusable,0,\N,Missing
petrov-etal-2012-universal,dickinson-jochim-2008-simple,0,\N,Missing
petrov-etal-2012-universal,nivre-etal-2006-talbanken05,0,\N,Missing
petrov-etal-2012-universal,J93-2004,0,\N,Missing
petrov-etal-2012-universal,D11-1018,0,\N,Missing
petrov-etal-2012-universal,boguslavsky-etal-2002-development,0,\N,Missing
petrov-etal-2012-universal,D10-1120,0,\N,Missing
petrov-etal-2012-universal,A97-1014,0,\N,Missing
petrov-etal-2012-universal,N03-1033,0,\N,Missing
petrov-etal-2012-universal,H05-1107,0,\N,Missing
petrov-etal-2012-universal,W06-2920,0,\N,Missing
petrov-etal-2012-universal,N01-1026,0,\N,Missing
petrov-etal-2012-universal,D10-1056,0,\N,Missing
petrov-etal-2012-universal,P10-1131,0,\N,Missing
petrov-etal-2012-universal,N09-1010,0,\N,Missing
petrov-etal-2012-universal,A00-1031,0,\N,Missing
petrov-etal-2012-universal,P11-1061,1,\N,Missing
petrov-etal-2012-universal,I08-3008,0,\N,Missing
petrov-etal-2012-universal,P04-1061,0,\N,Missing
petrov-etal-2012-universal,P11-2008,1,\N,Missing
petrov-etal-2012-universal,P95-1039,0,\N,Missing
petrov-etal-2012-universal,D11-1005,1,\N,Missing
petrov-etal-2012-universal,W00-1906,0,\N,Missing
petrov-etal-2012-universal,dzeroski-etal-2006-towards,0,\N,Missing
petrov-etal-2012-universal,simov-etal-2002-building,0,\N,Missing
petrov-etal-2012-universal,D07-1096,1,\N,Missing
petrov-etal-2012-universal,rambow-etal-2006-parallel,0,\N,Missing
petrov-etal-2012-universal,erjavec-2004-multext,0,\N,Missing
petrov-etal-2012-universal,afonso-etal-2002-floresta,0,\N,Missing
petrov-etal-2012-universal,P05-1044,0,\N,Missing
petrov-etal-2012-universal,P07-1096,0,\N,Missing
Q13-1001,I05-1075,0,\N,Missing
Q13-1001,D12-1127,0,\N,Missing
Q13-1001,J93-2004,0,\N,Missing
Q13-1001,N12-1052,1,\N,Missing
Q13-1001,N10-1083,0,\N,Missing
Q13-1001,H05-1107,0,\N,Missing
Q13-1001,W06-2920,0,\N,Missing
Q13-1001,N01-1026,0,\N,Missing
Q13-1001,C10-1124,0,\N,Missing
Q13-1001,D10-1056,0,\N,Missing
Q13-1001,P08-1085,0,\N,Missing
Q13-1001,D12-1075,0,\N,Missing
Q13-1001,P08-1086,0,\N,Missing
Q13-1001,P09-1057,0,\N,Missing
Q13-1001,P02-1035,0,\N,Missing
Q13-1001,petrov-etal-2012-universal,1,\N,Missing
Q13-1001,P10-1040,0,\N,Missing
Q13-1001,2005.mtsummit-papers.11,0,\N,Missing
Q13-1001,D07-1096,1,\N,Missing
Q13-1001,P05-1044,0,\N,Missing
Q19-1026,D15-1075,0,0.0508562,"ent set also appear in the training set, we implement two ‘‘copying’’ baselines. The first of these simply selects the most frequent annotation applied to a given page in the training set. The second selects the annotation given to the training set question closest to the eval set question according to TFIDF weighted word overlap. These three baselines are reported as First paragraph, Most frequent, and Closest question in Table 3, respectively. 6.3 Custom Pipeline (DecAtt + DocReader) One view of the long answer selection task is that it is more closely related to natural language inference (Bowman et al., 2015; Williams et al., 2018) than short answer extraction. A valid long answer must contain all of the information required to infer the answer. Short answers do not need to contain this information—they need to be surrounded by it. Motivated by this intuition, we implement a pipelined approach that uses a model drawn from the natural language interference literature to select long answers. Then short answers are selected from these using a model drawn from the short answer extraction literature. 6.2 Document-QA We adapt the reference implementation12 of Document-QA (Clark and Gardner, 2018) for t"
Q19-1026,P17-1171,0,0.275889,"Missing"
Q19-1026,D18-1241,0,0.0611339,"choice of supporting facts is somewhat subjective. They set high human upper bounds by selecting, for each example, the score maximizing partition of four annotations into one prediction and three references. The reference labels chosen by this maximization are not representative of the reference labels in HotpotQA’s evaluation set, and it is not clear that the upper bounds are achievable. A more robust approach is to keep the evaluation distribution fixed, and calculate an acheivable upper bound by approximating the expectation over annotations—as we have done for NQ in Section 5. The QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018) data sets contain dialogues between a questioner, who is trying to learn about a text, and an answerer. QuAC also prevents the questioner from seeing the evidence text. Conversational QA is an exciting new area, but it is significantly different from the single turn QA task in NQ. In both QuAC and CoQA, conversations tend to explore evidence texts incrementally, progressing from the start to the end of the text. 3 Task Definition and Data Collection Natural Questions contains (question, wikipedia page, long answer, short answer) quadruples where: the question see"
Q19-1026,P18-1078,0,0.217148,"(P), recall (R), and the harmonic mean of these (F1) of all baselines, a single annotator, and the super-annotator upper bound. The human performances marked with † are evaluated on a sample of five annotations from the 25-way annotated data introduced in Section 5. To address (ii), we tried adding special NULL passages to represent the lack of answer. However, we achieved better performance by training on the subset of questions with answers and then only predicting those answers whose scores exceed a threshold. With these two modifications, we are able to apply Document-QA to NQ. We follow Clark and Gardner (2018) in pruning documents down to the set of passages that have highest TFIDF similarity with the question. Under this approach, we consider the top 16 passages as long answers. We consider short answers containing up to 17 words. We train Document-QA for 30 epochs with batches containing 15 examples. The post hoc score threshold is set to 3.0. All of these values were chosen on the basis of development set performance. 6.1 Untrained Baselines NQ’s long answer selection task admits several untrained baselines. The first paragraph of a Wikipedia page commonly acts as a summary of the most important"
Q19-1026,D17-1082,0,0.109935,"annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should n"
Q19-1026,W18-2605,0,0.0359237,"ions. MS Marco contains 100,000 questions with freeform answers. For each question, the annotator is presented with 10 passages returned by the search engine, and is asked to generate an answer to the query, or to say that the answer is not contained within the passages. Free-form text answers allow more flexibility in providing abstractive answers, but lead to difficulties in evaluation (BLEU score [Papineni et al., 2002] is used). MS Marco’s authors do not discuss issues of variability or report quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and ans"
Q19-1026,D18-1260,0,0.0463566,"variability or report quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the ins"
Q19-1026,C92-2082,0,0.0914034,"elines and tooling divide the annotation task into three conceptual stages, where all three stages are completed by a single annotator in succession. The decision flow through these is illustrated in Figure 2 and the instructions given to annotators are summarized below. Question Identification: Contributors determine whether the given question is good or bad. A good question is a fact-seeking question that can be answered with an entity or explanation. A bad question is ambigous, incomprehensible, 3 We pre-define the set of categorical noun phrases used in 4 and 5 by running Hearst patterns (Hearst, 1992) to find a broad set of hypernyms. Part of speech tags and entities are identified using Google’s Cloud NLP API: https://cloud. google.com/natural-language. 456 2) k -way annotations (with k = 25) on a subset of the data. Post hoc evaluation of non-null answers leads directly to a measure of annotation precision. As is common in information-retrieval style problems such as long-answer identification, measuring recall is more challenging. However, we describe how 25-way annotated data provide useful insights into recall, particularly when combined with expert judgments. dependent on clear false"
Q19-1026,D16-1241,0,0.0190295,"nts to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should not change the answer, and SQuAD 2.0 introduces questions that are designed to be unanswerable. However, we argue that questions written to be unanswerable can be identified as such with little reasoning, in contrast to NQ’s task of deciding whether a pa"
Q19-1026,P16-1144,0,0.031038,"to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should not change the answer, and SQuAD 2.0 introduces questions that are designed to be unanswerable. However, we argue that questions written to be unanswerable can be identified as such with little reasoning, in contrast to NQ’s task of"
Q19-1026,D17-1215,0,0.0458559,"questions from the query stream. Thus the questions are ‘‘natural’’ in that they represent real queries from people seeking information. 2 Related Work The SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Kocisky et al., 2018), and HotpotQA (Yang et al., 2018) data sets contain questions and answers written by annotators who have first read a short text containing the answer. The SQuAD data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as test data. We justify the use of 5-way annotation for evaluation in Section 5. 454 This contrasts with NQ, where individual questions often require reasoning over large bodies of text. The WikiQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) data sets contain queries sampled from the Bing search engine. WikiQA contains only 3,047 questions. MS Marco contains 100,000 questions with free"
Q19-1026,P17-1147,0,0.26169,"ssues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should not change the answer, and SQuAD 2.0 introduc"
Q19-1026,P02-1040,0,0.108627,"asoning over large bodies of text. The WikiQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) data sets contain queries sampled from the Bing search engine. WikiQA contains only 3,047 questions. MS Marco contains 100,000 questions with freeform answers. For each question, the annotator is presented with 10 passages returned by the search engine, and is asked to generate an answer to the query, or to say that the answer is not contained within the passages. Free-form text answers allow more flexibility in providing abstractive answers, but lead to difficulties in evaluation (BLEU score [Papineni et al., 2002] is used). MS Marco’s authors do not discuss issues of variability or report quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading co"
Q19-1026,N18-1101,0,0.0325746,"n the training set, we implement two ‘‘copying’’ baselines. The first of these simply selects the most frequent annotation applied to a given page in the training set. The second selects the annotation given to the training set question closest to the eval set question according to TFIDF weighted word overlap. These three baselines are reported as First paragraph, Most frequent, and Closest question in Table 3, respectively. 6.3 Custom Pipeline (DecAtt + DocReader) One view of the long answer selection task is that it is more closely related to natural language inference (Bowman et al., 2015; Williams et al., 2018) than short answer extraction. A valid long answer must contain all of the information required to infer the answer. Short answers do not need to contain this information—they need to be surrounded by it. Motivated by this intuition, we implement a pipelined approach that uses a model drawn from the natural language interference literature to select long answers. Then short answers are selected from these using a model drawn from the short answer extraction literature. 6.2 Document-QA We adapt the reference implementation12 of Document-QA (Clark and Gardner, 2018) for the NQ task. This system"
Q19-1026,P18-2124,0,0.322524,"answer (s) can be a span or set of spans (typically entities) within l that answer the question, a boolean yes or no answer, or NULL. If l = NULL then s = NULL, necessarily. Figure 1 shows examples. Natural Questions has the following properties: Source of questions The questions consist of real anonymized, aggregated queries issued to the Google search engine. Simple heuristics are used to filter questions from the query stream. Thus the questions are ‘‘natural’’ in that they represent real queries from people seeking information. 2 Related Work The SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Kocisky et al., 2018), and HotpotQA (Yang et al., 2018) data sets contain questions and answers written by annotators who have first read a short text containing the answer. The SQuAD data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as"
Q19-1026,D16-1264,0,0.822388,"chine translation, speech recognition, and image recognition. One major factor in these successes has been the development of neural methods that far exceed the performance of previous approaches. A second major factor has been the existence of large quantities of training data for these systems. Open-domain question answering (QA) is a benchmark task in natural language understanding (NLU), which has significant utility to users, and in addition is potentially a challenge task that can drive the development of methods for NLU. Several pieces of recent work have introduced QA data sets (e.g., Rajpurkar et al., 2016; Reddy et al., 2018). However, in contrast to tasks where it is relatively easy to gather naturally occurring examples,1 the definition of a suitable QA task, and the development of a methodology for annotation and evaluation, is challenging. Key issues include the methods and sources used to obtain questions; the methods used to annotate and collect answers; the methods used to measure and ensure annotation quality; and the metrics used for evaluation. For more discussion of the limitations of previous work with respect to these issues, see Section 2 of this paper. This paper introduces Natu"
Q19-1026,D15-1237,0,0.10531,"D data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as test data. We justify the use of 5-way annotation for evaluation in Section 5. 454 This contrasts with NQ, where individual questions often require reasoning over large bodies of text. The WikiQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) data sets contain queries sampled from the Bing search engine. WikiQA contains only 3,047 questions. MS Marco contains 100,000 questions with freeform answers. For each question, the annotator is presented with 10 passages returned by the search engine, and is asked to generate an answer to the query, or to say that the answer is not contained within the passages. Free-form text answers allow more flexibility in providing abstractive answers, but lead to difficulties in evaluation (BLEU score [Papineni et al., 2002] is used). MS Marco’s authors do not discus"
Q19-1026,D18-1259,0,0.167869,"answer the question, a boolean yes or no answer, or NULL. If l = NULL then s = NULL, necessarily. Figure 1 shows examples. Natural Questions has the following properties: Source of questions The questions consist of real anonymized, aggregated queries issued to the Google search engine. Simple heuristics are used to filter questions from the query stream. Thus the questions are ‘‘natural’’ in that they represent real queries from people seeking information. 2 Related Work The SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Kocisky et al., 2018), and HotpotQA (Yang et al., 2018) data sets contain questions and answers written by annotators who have first read a short text containing the answer. The SQuAD data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as test data. We justify the use of 5-way annotation for evaluation in Se"
Q19-1026,D13-1020,0,0.136104,"quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sent"
Q19-1026,D16-1244,1,\N,Missing
Q19-1026,Q18-1023,0,\N,Missing
W06-2903,A00-2018,0,0.0276932,"nt comes from the learning of specialized grammars that capture non-local correlations. 1 Introduction The probabilistic context-free grammar (PCFG) formalism is the basis of most modern statistical parsers. The symbols in a PCFG encode contextfreedom assumptions about statistical dependencies in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce scores on trees. Compared to a basic treebank grammar (Charniak, 1996), the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical (Charniak, 2000; Collins, 1999) or nonlexical (Klein and Manning, 2003; Matsuzaki et al., 2005) conditioning information. While such splitting, or conditioning, can cause problems for statistical estimation, it can dramatically improve the accuracy of a parser. However, the configurations exploited in PCFG parsers are quite local: rules’ probabilities may depend on parents or head words, but do not depend on arbitrarily distant tree configurations. For example, it is generally not modeled that if one quantifier phrase (QP in the Penn Treebank) appears in a sentence, the likelihood of finding another QP in th"
W06-2903,W01-0521,0,0.122497,"PRN → -LRB- NP -RRBADJP → QP PP → IN NP ADVP NP → NP PRN VP → VBN PP PP PP ADVP → NP RBR Score 131.6 77.1 33.7 28.4 17.3 13.3 12.3 12.3 11.6 10.1 Figure 1: Self-triggering: QP → # CD CD. If one British financial occurs in the sentence, the probability of seeing a second one in the same sentence is highly inreased. There is also a similar, but weaker, correlation for the American financial ($). On the right hand side we show the ten rules whose likelihoods are most increased in a sentence containing this rule. strong and that weakening them results in better models of language (Johnson, 1998; Gildea, 2001; Klein and Manning, 2003). In particular, certain grammar productions often cooccur with other productions, which may be either near or distant in the parse tree. In general, there exist three types of correlations: (i) local (e.g. parent-child), (ii) non-local, and (iii) self correlations (which may be local or non-local). In order to quantify the strength of a correlation, we use a likelihood ratio (LR). For two rules X → α and Y → β, we compute LR(X → α, Y → β) = P(α, β|X, Y ) P(α|X, Y )P(β|X, Y ) This measures how much more often the rules occur together than they would in the case of ind"
W06-2903,J98-4004,0,0.0343584,"→ VBD NP NP PP PRN → -LRB- NP -RRBADJP → QP PP → IN NP ADVP NP → NP PRN VP → VBN PP PP PP ADVP → NP RBR Score 131.6 77.1 33.7 28.4 17.3 13.3 12.3 12.3 11.6 10.1 Figure 1: Self-triggering: QP → # CD CD. If one British financial occurs in the sentence, the probability of seeing a second one in the same sentence is highly inreased. There is also a similar, but weaker, correlation for the American financial ($). On the right hand side we show the ten rules whose likelihoods are most increased in a sentence containing this rule. strong and that weakening them results in better models of language (Johnson, 1998; Gildea, 2001; Klein and Manning, 2003). In particular, certain grammar productions often cooccur with other productions, which may be either near or distant in the parse tree. In general, there exist three types of correlations: (i) local (e.g. parent-child), (ii) non-local, and (iii) self correlations (which may be local or non-local). In order to quantify the strength of a correlation, we use a likelihood ratio (LR). For two rules X → α and Y → β, we compute LR(X → α, Y → β) = P(α, β|X, Y ) P(α|X, Y )P(β|X, Y ) This measures how much more often the rules occur together than they would in t"
W06-2903,P03-1054,1,0.397849,"mars that capture non-local correlations. 1 Introduction The probabilistic context-free grammar (PCFG) formalism is the basis of most modern statistical parsers. The symbols in a PCFG encode contextfreedom assumptions about statistical dependencies in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce scores on trees. Compared to a basic treebank grammar (Charniak, 1996), the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical (Charniak, 2000; Collins, 1999) or nonlexical (Klein and Manning, 2003; Matsuzaki et al., 2005) conditioning information. While such splitting, or conditioning, can cause problems for statistical estimation, it can dramatically improve the accuracy of a parser. However, the configurations exploited in PCFG parsers are quite local: rules’ probabilities may depend on parents or head words, but do not depend on arbitrarily distant tree configurations. For example, it is generally not modeled that if one quantifier phrase (QP in the Penn Treebank) appears in a sentence, the likelihood of finding another QP in that same sentence is greatly increased. This kind of eff"
W06-2903,P05-1010,0,0.0587569,"al correlations. 1 Introduction The probabilistic context-free grammar (PCFG) formalism is the basis of most modern statistical parsers. The symbols in a PCFG encode contextfreedom assumptions about statistical dependencies in the derivations of sentences, and the relative conditional probabilities of the grammar rules induce scores on trees. Compared to a basic treebank grammar (Charniak, 1996), the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical (Charniak, 2000; Collins, 1999) or nonlexical (Klein and Manning, 2003; Matsuzaki et al., 2005) conditioning information. While such splitting, or conditioning, can cause problems for statistical estimation, it can dramatically improve the accuracy of a parser. However, the configurations exploited in PCFG parsers are quite local: rules’ probabilities may depend on parents or head words, but do not depend on arbitrarily distant tree configurations. For example, it is generally not modeled that if one quantifier phrase (QP in the Penn Treebank) appears in a sentence, the likelihood of finding another QP in that same sentence is greatly increased. This kind of effect is neither surprising"
W06-2903,P85-1011,0,0.375937,"Missing"
W06-2903,H86-1020,0,\N,Missing
W06-2903,J03-4003,0,\N,Missing
W06-2903,C92-3126,0,\N,Missing
W08-1005,P05-1022,0,0.747949,"nitial grammar is hierarchically refined using an adaptive split-and-merge EM procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005). However, as demonstrated in Charniak (1996) and Klein and Manning (2003), a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well. This naive grammar is a poor one because its contextfreedom assumptions are too strong in some ways (e.g. it assumes that subject and object NPs share the same distribution) and too weak in others (e.g. it assumes that long rewrites do not decompose into smaller steps). Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symb"
W08-1005,A00-2018,0,0.0517578,"hod, a minimal initial grammar is hierarchically refined using an adaptive split-and-merge EM procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005). However, as demonstrated in Charniak (1996) and Klein and Manning (2003), a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well. This naive grammar is a poor one because its contextfreedom assumptions are too strong in some ways (e.g. it assumes that subject and object NPs share the same distribution) and too weak in others (e.g. it assumes that long rewrites do not decompose into smaller steps). Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from si"
W08-1005,C02-1126,0,0.0501696,"Missing"
W08-1005,P05-1039,0,0.0663587,"Missing"
W08-1005,P96-1024,0,0.0581366,"nguistic like a parent annotation context, but which is formally just an integer. G therefore induces a derivation distribution over trees labeled with split symbols. This distribution in turn induces a parse distribution over (projected) trees with unsplit evaluation symbols. We have several choices of how to select a tree given these posterior distributions over trees. Since computing the most likely parse tree is NP-complete (Sima’an, 1992), we settle for an approximation that allows us to (partially) sum out the latent annotation. In Petrov and Klein (2007) we relate this approximation to Goodman (1996)’s labeled brackets algorithm applied to rules and to Matsuzaki et al. (2005)’s sentence specific variational approximation. This procedure is substantially superior to simply erasing the latent annotations from the the Viterbi derivation. 2.3 Results In Petrov and Klein (2007) we trained models for English, Chinese and German using the standard corpora and setups. We applied our latent variable model directly to each of the treebanks, without any 35 Table 1: Our split-and-merge latent variable approach produces the best published parsing performance on many languages. language dependent modif"
W08-1005,J98-4004,0,0.125561,"demonstrated in Charniak (1996) and Klein and Manning (2003), a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well. This naive grammar is a poor one because its contextfreedom assumptions are too strong in some ways (e.g. it assumes that subject and object NPs share the same distribution) and too weak in others (e.g. it assumes that long rewrites do not decompose into smaller steps). Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symbol splitting (Johnson, 1998; Klein and Manning, 2003) to full lexicalization and intricate smoothing (Collins, 1999; Charniak, 2000). We view treebank parsing as the search for an optimally refined grammar consistent with a coarse training treebank. As a result, we begin with the provided evaluation symbols (such as NP, VP, etc.) but split them based on the statistical patterns in the training trees. A manual approach might take the symbol NP and subdivide it into one subsymbol NPˆS for subjects and another subsymbol NPˆVP for objects. However, rather than devising linguistically motivated features or splits, we take a"
W08-1005,P03-1054,1,0.132934,"procedure, giving compact, accurate grammars. The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features. Nonetheless, the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005). However, as demonstrated in Charniak (1996) and Klein and Manning (2003), a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well. This naive grammar is a poor one because its contextfreedom assumptions are too strong in some ways (e.g. it assumes that subject and object NPs share the same distribution) and too weak in others (e.g. it assumes that long rewrites do not decompose into smaller steps). Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symbol splitting (Johnson, 1998; Klein and Manning, 2003) to full lexicalizati"
W08-1005,P05-1010,0,0.244311,"atent variable approach recovers linguistically interpretable phenomena. In our analysis, we pay particular attention to similarities and differences between 33 Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 33–39, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics FRAG ROOT NP FRAG-x RB Not . DT this NN year (a) . FRAG-x .-x RB-x NP-x Not DT-x NN-x this . year (b) Figure 1: (a) The original tree. (b) The binarized tree with latent variables. grammars learned from the two treebanks. 2 Latent Variable Parsing In latent variable parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006), we learn rule probabilities on latent annotations that, when marginalized out, maximize the likelihood of the unannotated training trees. We use an automatic approach in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of the training treebank. In this section we briefly review the main ideas in latent variable parsing. This work has been previously published and we therefore provide only a short overview. For a more detailed exposition of the learning algorithm the reader is referred to Petrov et al. (2006). Th"
W08-1005,N07-1051,1,0.948011,"ities on latent annotations that, when marginalized out, maximize the likelihood of the unannotated training trees. We use an automatic approach in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of the training treebank. In this section we briefly review the main ideas in latent variable parsing. This work has been previously published and we therefore provide only a short overview. For a more detailed exposition of the learning algorithm the reader is referred to Petrov et al. (2006). The corresponding inference procedure is described in detail in Petrov and Klein (2007). The parser, code, and trained models are available for download at http://nlp.cs.berkeley.edu. 2.1 Learning Starting with a simple X-bar grammar, we use the Expectation-Maximization (EM) algorithm to learn a new grammar whose nonterminals are subsymbols of the original evaluation nonterminals. The X-bar grammar is created by binarizing the treebank trees; for each local tree rooted at an evaluation nonterminal X, we introduce a cascade of new nodes labeled X so that each node has at most two children, see Figure 1. This initialization is the absolute minimum starting grammar that distinguish"
W08-1005,P06-1055,1,0.589636,"tically interpretable phenomena. In our analysis, we pay particular attention to similarities and differences between 33 Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 33–39, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics FRAG ROOT NP FRAG-x RB Not . DT this NN year (a) . FRAG-x .-x RB-x NP-x Not DT-x NN-x this . year (b) Figure 1: (a) The original tree. (b) The binarized tree with latent variables. grammars learned from the two treebanks. 2 Latent Variable Parsing In latent variable parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006), we learn rule probabilities on latent annotations that, when marginalized out, maximize the likelihood of the unannotated training trees. We use an automatic approach in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of the training treebank. In this section we briefly review the main ideas in latent variable parsing. This work has been previously published and we therefore provide only a short overview. For a more detailed exposition of the learning algorithm the reader is referred to Petrov et al. (2006). The corresponding inference procedure is"
W08-1005,J03-4003,0,\N,Missing
W10-2906,P07-2045,0,0.00225424,"or retraining, we used the same data, but weighted it to match the sizes of the original monolingual treebanks. We tested on the standard Chinese treebank development set, which also includes English translations. 9.1 Machine Translation Experiments Although we don’t have hand-labeled data for our largest Chinese-English parallel corpora, we can still evaluate our parsing results via our performance on a downstream machine translation (MT) task. Our experimental setup is as follows: first, we used the first 100,000 sentences of the EnglishChinese bitext from Wang et al. (2007) to train Moses (Koehn et al., 2007), a phrase-based MT system that we use as a baseline. We then used the same sentences to extract tree-to-string transducer rules from target-side (English) trees (Galley et al., 2004). We compare the single-reference BLEU scores of syntactic MT systems that result from using different parsers to generate these trees. Table 4 gives results for syntactic parsing. For comparison, we also show results for the supervised bilingual model of Burkett and Klein (2008). This model uses the same features at prediction time as the multiview trained “Bilingual w/ Full” model, but it is trained on hand-anno"
W10-2906,J93-2004,0,0.0453072,"st time. The texts used for retraining overlapped with the bitexts used for training the bilingual model, but both sets were disjoint from the test sets. 8 NER Experiments We demonstrate the utility of multiview learning for named entity recognition (NER) on English/German sentence pairs. We built both our full and weakened monolingual English and German models from the CoNLL 2003 shared task 9 Parsing Experiments Our next set of experiments are on syntactic parsing of English and Chinese. We trained both our full and weakened monolingual English models on the Penn Wall Street Journal corpus (Marcus et al., 1993), as described in Section 4. Our full and weakened Chinese models were trained on 2 Of course, unannotated monolingual data is even more plentiful, but as we will show, with the same amount of data, our method is more effective than simple monolingual selftraining. 51 Eng Parliament Prec Rec F1 Eng Newswire Ger Parliament Prec Rec F1 Prec Rec F1 Monolingual Models (Baseline) 58.5 67.7 83.0 74.6 71.3 36.4 48.2 68.4 80.1 88.7 84.2 69.8 44.0 54.0 Multiview Trained Bilingual Models 62.7 71.4 86.2 78.1 70.1 66.3 68.2 68.7 80.6 88.7 84.4 70.1 70.1 70.1 Retrained Monolingual Models 72.9 79.9 87.4 83."
W10-2906,E03-1035,0,0.158748,"Missing"
W10-2906,N07-1051,1,0.365502,"Figure 1 contains sample values for each of these features. Another natural setting where bilingual constraints can be exploited is syntactic parsing. Figure 2 shows an example English prepositional phrase attachment ambiguity that can be resolved bilingually by exploiting Chinese. The English monolingual parse mistakenly attaches to to the verb increased. In Chinese, however, this ambiguity does not exist. Instead, the word 对, which aligns to to, has strong selectional preference for attaching to a noun on the left. In our parsing experiments, we use the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), a split-merge latent variable parser, for our monolingual models. Our full model is the result of training the parser with five split-merge phases. Our weakened model uses only two. For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). Table 2 gives some examples, but does not exhaustively enumerate those features. 5 Our training algorithm is summarized in Figure 3. For each unlabeled point x = (x1 , x2 ), let yˆM be the joint label which has the highest score from the independent monolingual models (line ˆ λ ˆ1, λ ˆ2 1). We then find bilingual parameter"
W10-2906,D08-1092,1,0.701008,"odel consists of two views, which we will refer to as monolingual and bilingual. The monolingual view estimates the joint probability as the product of independent marginal distributions over each language, pM (y|x) = p1 (y1 |x1 )p2 (y2 |x2 ). In our applications, these marginal distributions will be computed by stateof-the-art statistical taggers and parsers trained on large monolingual corpora. This work focuses on learning parameters for the bilingual view of the data. We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008). In this work, we use the term node to indicate a particular component of a label, such as a single (multi-word) named entity or a node in a parse tree. In Figure 2(a), for example, the nodes labeled NP1 in both the Chinese and English trees are matched. Since we don’t know a priori how the components relate to one another, we treat these matchings as hidden. For each matching a and pair of labels y, we define a feature vector φ(y1 , a, y2 ) which factors on edges in the matching. Our model is a conditional exponential family distribution over matchings and labels: h i pθ (y, a|x) = exp θ > φ"
W10-2906,P06-1055,1,0.1235,"has the same label. Figure 1 contains sample values for each of these features. Another natural setting where bilingual constraints can be exploited is syntactic parsing. Figure 2 shows an example English prepositional phrase attachment ambiguity that can be resolved bilingually by exploiting Chinese. The English monolingual parse mistakenly attaches to to the verb increased. In Chinese, however, this ambiguity does not exist. Instead, the word 对, which aligns to to, has strong selectional preference for attaching to a noun on the left. In our parsing experiments, we use the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), a split-merge latent variable parser, for our monolingual models. Our full model is the result of training the parser with five split-merge phases. Our weakened model uses only two. For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). Table 2 gives some examples, but does not exhaustively enumerate those features. 5 Our training algorithm is summarized in Figure 3. For each unlabeled point x = (x1 , x2 ), let yˆM be the joint label which has the highest score from the independent monolingual models (line ˆ λ ˆ1, λ ˆ2 1). We then"
W10-2906,P05-1022,0,0.00423527,"able 1: Sample features used for named entity recognition for the ORG entity in Figure 1. Berichte des Europäischen Rechnungshofes ORG1 rate predictions, but ensure that bilingual features will be required to optimize the training objective. W W W Let `W 1 = log p1 (y1 |x1 ), `2 = log p2 (y2 |x2 ) be the log-probability scores from the weakened models. Our final approximation to the marginal distribution over labels y is: h Figure 1: An example where English NER can be used to disambiguate German NER. We further simplify inference in our model by working in a reranking setting (Collins, 2000; Charniak and Johnson, 2005), where we only consider the top k outputs from monolingual models in both languages, for a total of k 2 labels y. In practice, k 2 ≤ 10, 000 for our largest problem. 3.1 Examples I NSIDE B OTH=3 I N E N O NLY=0 L BL M ATCH=true B IAS=true def W qλ1 ,λ2 ,θ (y|x) = max exp λ1 `W 1 + λ2 `2 + a i ˜ 1 , λ2 , θ; x) . θ > φ(y1 , a, y2 ) − A(λ (1) Where ˜ 1 , λ2 , θ; x) = A(λ h i X W > log max exp λ1 `W 1 + λ2 `2 + θ φ(y1 , a, y2 ) Including Weakened Models Now that we have defined our bilingual model, we could train it to agree with the output of the monolingual model (Collins and Singer, 1999; Ganc"
W10-2906,W99-0613,0,0.739657,"0; Charniak and Johnson, 2005), where we only consider the top k outputs from monolingual models in both languages, for a total of k 2 labels y. In practice, k 2 ≤ 10, 000 for our largest problem. 3.1 Examples I NSIDE B OTH=3 I N E N O NLY=0 L BL M ATCH=true B IAS=true def W qλ1 ,λ2 ,θ (y|x) = max exp λ1 `W 1 + λ2 `2 + a i ˜ 1 , λ2 , θ; x) . θ > φ(y1 , a, y2 ) − A(λ (1) Where ˜ 1 , λ2 , θ; x) = A(λ h i X W > log max exp λ1 `W 1 + λ2 `2 + θ φ(y1 , a, y2 ) Including Weakened Models Now that we have defined our bilingual model, we could train it to agree with the output of the monolingual model (Collins and Singer, 1999; Ganchev et al., 2008). As we will see in Section 4, however, the feature functions φ(y1 , a, y2 ) make no reference to the input sentences x, other than through a fixed word alignment. With such limited monolingual information, it is impossible for the bilingual model to adequately capture all of the information necessary for NER or parsing. As a simple example, a bilingual NER model will be perfectly happy to label two aligned person names as ORG instead of PER: both labelings agree equally well. We briefly illustrate how poorly such a basic bilingual model performs in Section 10. One way t"
W10-2906,W04-3207,0,0.293071,"llel text (after machine translation) (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of possible annotations for the other. Our work falls into the final category: We wish to use bilingual data to improve monolingual models which are already trained on large amounts of data and effective on their own (Huang and Vogel, 2002; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Procedurally, our work is most closely related to that of Burkett and Klein (2008). They used an annotated bitext to learn parse reranking models for English and Chinese, exploiting features that examine pieces of parse trees in both languages. Our method can be thought of as the semi-supervised counterpart to their supervised model. Indeed, we achieve nearly the same results, but without annotated bitexts. Smith and Smith (2004) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they con"
W10-2906,P05-1045,0,0.00791798,"Missing"
W10-2906,P09-1009,0,0.0824724,"r each matching a and pair of labels y, we define a feature vector φ(y1 , a, y2 ) which factors on edges in the matching. Our model is a conditional exponential family distribution over matchings and labels: h i pθ (y, a|x) = exp θ > φ(y1 , a, y2 ) − A(θ; x) , Prior Work on Learning from Bilingual Text Prior work in learning monolingual models from bitexts falls roughly into three categories: Unsupervised induction, cross-lingual projection, and bilingual constraints for supervised monolingual models. Two recent, successful unsupervised induction methods are those of Blunsom et al. (2009) and Snyder et al. (2009). Both of them estimate hierarchical Bayesian models and employ bilingual data to constrain the types of models that can be derived. Projection methods, on the other hand, were among the first applications of parallel text (after machine translation) (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of possible annotations for"
W10-2906,N04-1035,0,0.00650347,"includes English translations. 9.1 Machine Translation Experiments Although we don’t have hand-labeled data for our largest Chinese-English parallel corpora, we can still evaluate our parsing results via our performance on a downstream machine translation (MT) task. Our experimental setup is as follows: first, we used the first 100,000 sentences of the EnglishChinese bitext from Wang et al. (2007) to train Moses (Koehn et al., 2007), a phrase-based MT system that we use as a baseline. We then used the same sentences to extract tree-to-string transducer rules from target-side (English) trees (Galley et al., 2004). We compare the single-reference BLEU scores of syntactic MT systems that result from using different parsers to generate these trees. Table 4 gives results for syntactic parsing. For comparison, we also show results for the supervised bilingual model of Burkett and Klein (2008). This model uses the same features at prediction time as the multiview trained “Bilingual w/ Full” model, but it is trained on hand-annotated parses. We first examine the first four rows of Table 4. The “Bilingual w/ Full” model significantly improves performance in both English and Chinese relative to the monolingual"
W10-2906,C02-1145,0,0.0106436,"the Penn Treebank, are larger than those in Chinese, which is in domain. We also emphasize that, unlike our NER data, this bitext was fairly small relative to the annotated monolingual data. Therefore, while we still learn good bilingual model parameters which give a sizable agreement-based boost when doing bilingual prediction, we don’t expect retraining to result in a coverage-based boost in monolingual performance. Table 4: Parsing results. Rows are grouped by data condition. We bold entries that are best in their group and beat the the Full Monolingual baseline. the Penn Chinese treebank (Xue et al., 2002) (articles 400-1151), excluding the bilingual portion. The bilingual data consists of the parallel part of the Chinese treebank (articles 1-270), which also includes manually parsed English translations of each Chinese sentence (Bies et al., 2007). Only the Chinese sentences and their English translations were used to train the bilingual models – the gold trees were ignored. For retraining, we used the same data, but weighted it to match the sizes of the original monolingual treebanks. We tested on the standard Chinese treebank development set, which also includes English translations. 9.1 Mac"
W10-2906,P09-1042,0,0.431985,"rsed text. We achieve nearly identical improvements using a purely unlabeled bitext. These results carry over to machine translation, where we can achieve slightly better BLEU improvements than the supervised model of Burkett and Klein (2008) since we are able to train our model directly on the parallel data where we perform rule extraction. Introduction Natural language analysis in one language can be improved by exploiting translations in another language. This observation has formed the basis for important work on syntax projection across languages (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009) and unsupervised syntax induction in multiple languages (Snyder et al., 2009), as well as other tasks, such as cross-lingual named entity recognition (Huang and Vogel, 2002; Moore, 2003) and information retrieval (Si and Callan, 2005). In all of these cases, multilingual models yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. In the present work, we consider a setting where we already possess supervised monolingual models, and wish to improve these models using unannotated bilingua"
W10-2906,N01-1026,0,0.756885,"ual Text Prior work in learning monolingual models from bitexts falls roughly into three categories: Unsupervised induction, cross-lingual projection, and bilingual constraints for supervised monolingual models. Two recent, successful unsupervised induction methods are those of Blunsom et al. (2009) and Snyder et al. (2009). Both of them estimate hierarchical Bayesian models and employ bilingual data to constrain the types of models that can be derived. Projection methods, on the other hand, were among the first applications of parallel text (after machine translation) (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Hwa et al., 2005; Ganchev et al., 2009). They assume the existence of a good, monolingual model for one language but little or no information about the second language. Given a parallel sentence pair, they use the annotations for one language to heavily constrain the set of possible annotations for the other. Our work falls into the final category: We wish to use bilingual data to improve monolingual models which are already trained on large amounts of data and effective on their own (Huang and Vogel, 2002; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Procedura"
W10-2906,H01-1035,0,0.415326,"nolingual parsers using parallel, hand-parsed text. We achieve nearly identical improvements using a purely unlabeled bitext. These results carry over to machine translation, where we can achieve slightly better BLEU improvements than the supervised model of Burkett and Klein (2008) since we are able to train our model directly on the parallel data where we perform rule extraction. Introduction Natural language analysis in one language can be improved by exploiting translations in another language. This observation has formed the basis for important work on syntax projection across languages (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009) and unsupervised syntax induction in multiple languages (Snyder et al., 2009), as well as other tasks, such as cross-lingual named entity recognition (Huang and Vogel, 2002; Moore, 2003) and information retrieval (Si and Callan, 2005). In all of these cases, multilingual models yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. In the present work, we consider a setting where we already possess supervised monolingual models, and wish to improv"
W11-2921,N03-1016,0,0.0705654,"oaches often rely on coarse approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b). These coarse models are used to constrain and prune the search space of possible parse trees before applying the final model of interest. As such, these approaches can lead to great speed-ups, but introduce search errors. Our approach in contrast preserves optimality and could in principle be combined with such multi-pass approaches to yield additional speed improvements. There are also some optimality preserving approaches based on A∗ -search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al., 2011) that aim to speed up CKY inference. We suspect that most of the ideas therein are orthogonal to our approach, and therefore leave their integration into our GPU-based parser for future work. cessors to parallelize upon. We exploit the massive fine-grained parallelism inherent in natural language parsing and achieve a speedup of more than an order of magnitude. Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998;"
W11-2921,W08-2102,0,0.0408834,"Missing"
W11-2921,J93-2004,0,0.036638,"Missing"
W11-2921,P05-1022,0,0.0349881,"work are implemented on multicore systems, where the limited parallelization possibilities provided by the systems restrict the speedups that can be achieved. For example, van Lohuizen (1999) reports a 1.8× speedup, while Manousopoulou et al. (1997) claims a 7-8× speedup. In contrast, our parallel parser is implemented on a manycore system with an abundant number of threads and pro183 It should be noted that there are a also number of orthogonal approaches for accelerating natural language parsers. Those approaches often rely on coarse approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b). These coarse models are used to constrain and prune the search space of possible parse trees before applying the final model of interest. As such, these approaches can lead to great speed-ups, but introduce search errors. Our approach in contrast preserves optimality and could in principle be combined with such multi-pass approaches to yield additional speed improvements. There are also some optimality preserving approaches based on A∗ -search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al., 2011) that aim to speed u"
W11-2921,A00-2018,0,0.405451,"Missing"
W11-2921,W11-2920,0,0.0590766,"in the shared memory. tex:rule stands for loading the rule information from texture memory and tex:scores for loading the scores array from texture memory. tex:both means both the tex:rule and tex:scores are applied. Core i7 2.80 GHz 2GB GTX285 GTX480 648 MHz 1400 MHz 1GB 1.5GB 30 15 8 32 16KB up to 64KB N/A up to 64KB Table 1: Experimental platforms specifications. The exhaustive sequential CKY parser was written in C and is reasonably optimized, taking 5.5 seconds per sentence (or 5,505 seconds for the 1000 benchmark sentences). This is comparable to the better implementations presented in Dunlop et al. (2011). As can be seen in Figure 11, the fastest configuration on the GTX285 is Block+PR+SS+tex:scores, which shows a 17.4× speedup against the sequential parser. On the GTX480, Block+PR is the fastest, showing a 25.8× speedup. Their runtimes were 0.32 seconds/sentence and 0.21 seconds/sentence, respectively. It is noteworthy that the fastest configuration differs for the two devices. We provide an explanation later in this section. ever, as the array is updated at every iteration of binary relaxation, we need to update it also in texture memory by calling a binding API (between line 4 and 5 in Figu"
W11-2921,N09-1063,0,0.0129556,"se approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b). These coarse models are used to constrain and prune the search space of possible parse trees before applying the final model of interest. As such, these approaches can lead to great speed-ups, but introduce search errors. Our approach in contrast preserves optimality and could in principle be combined with such multi-pass approaches to yield additional speed improvements. There are also some optimality preserving approaches based on A∗ -search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al., 2011) that aim to speed up CKY inference. We suspect that most of the ideas therein are orthogonal to our approach, and therefore leave their integration into our GPU-based parser for future work. cessors to parallelize upon. We exploit the massive fine-grained parallelism inherent in natural language parsing and achieve a speedup of more than an order of magnitude. Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1"
W11-2921,P08-1109,0,0.0203867,"national Conference on Parsing Technologies, pages 175–185, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. (a) (b) ROOT S . @S NP I VP . VBP NP love PRP I love you . Lexicon: (P1) Pr(IPRP) = -0.23 (P2) Pr(..) = -0.01 … Grammar: (G1) Pr(NPPRP) = -1.35 (G2) Pr(VPVBP NP) = -2.45 … I love (1,3) (0,2) love I (1,2) (0,1) 0 you 1 Execution order you . (2,4) you . (2,3) 2 (3,4) 3 4 Figure 2: The chart that visualizes the bottom-up process of CKY parsing for the sentence “I love you .” Figure 1: An example of a parse tree for the sentence “I love you .” Finkel et al. (2008).1 The grammars in our experiments have on the order of thousands of nonterminals and millions of productions. Figure 1(a) shows a constituency parse tree. Leaf nodes in the parse tree, also called terminal nodes, correspond to words in the language. Preterminals correspond to part-of-speech tags, while the other nonterminals correspond to phrasal categories. For ease of exposition, we will say that terminal productions are part of a lexicon. For example, (L1) in Figure 1(b) is a lexical rule providing a score (of −0.23) for mapping the word ”I” to the symbol “PRP.” We assume that the grammar"
W11-2921,N07-1051,1,0.937324,"racNatural Language Parsing While we assume a basic familiarity with probabilistic CKY parsing, in this section we briefly review the CKY dynamic programming algorithm and the Viterbi algorithm for extracting the highest scoring path through the dynamic program. 2.1 Context-Free Grammars In this work we focus our attention on constituency parsing and assume that a weighted CFG is available to us. In our experiments we will use a probabilistic latent variable CFG (Petrov et al., 2006). However, our algorithms can be used with any weighted CFG, including discriminative ones, such as the ones in Petrov and Klein (2007a) and 1 For feature-rich discriminative models a trivially parallelizable pass can be used to pre-compute the rule-potentials. 2 This observation is due to Dan Klein, p.c. 176 Algorithm: binaryRelax(scores, nW ords, length, gr) Input: scores /* the 3-dimensional scores */ nW ords /* the number of total words */ length /* the current span */ gr /* the grammar */ Output: None Algorithm: parse(sen, lex, gr) Input: sen /* the input sentence */ lex /* the lexicon */ gr /* the grammar */ Output: tree /* the most probable parse tree */ 1 2 3 4 5 6 7 8 scores[][][] = initScores(); nW ords = readSente"
W11-2921,P06-1055,1,0.887923,"eanwhile, we have entered a manycore computing era, where the number of processing cores in computer systems doubles every second year, while the clock frequency has converged somewhere around 3 GHz (Asanovic et al., 2006). This opens up new opportunities for increasing the speed of parsers. We present a general approach for parallelizing the CKY algorithm that can handle arbitrary context-free grammars (Section 2). We make no assumptions about the size of the grammar and we demonstrate the efficacy of our approach by implementing a decoder for the state-of-the-art latent variable grammars of Petrov et al. (2006) (a.k.a. Berkeley Parser) on a Graphics Processor Unit (GPU). We first present an overview of the general architecture of GPUs and the efficient synchronization provided by the Compute Unified Device Architecture (CUDA (Nickolls et al., 2008)) programming model (Section 3). We then discuss how the hundreds of cores available on a GPU can enable a fine-grained parallel execution of the CKY algorithm. We explore the design space with different thread mappings onto the GPU and discuss how the various synchronization methods might be applied in this context (Section 4). Key to our approach is the"
W11-2921,W97-0302,0,0.0804533,"arsers in past work are implemented on multicore systems, where the limited parallelization possibilities provided by the systems restrict the speedups that can be achieved. For example, van Lohuizen (1999) reports a 1.8× speedup, while Manousopoulou et al. (1997) claims a 7-8× speedup. In contrast, our parallel parser is implemented on a manycore system with an abundant number of threads and pro183 It should be noted that there are a also number of orthogonal approaches for accelerating natural language parsers. Those approaches often rely on coarse approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b). These coarse models are used to constrain and prune the search space of possible parse trees before applying the final model of interest. As such, these approaches can lead to great speed-ups, but introduce search errors. Our approach in contrast preserves optimality and could in principle be combined with such multi-pass approaches to yield additional speed improvements. There are also some optimality preserving approaches based on A∗ -search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al"
W11-2921,W05-1506,0,0.0195481,"used: scores[start][end][symbol] (see also Figure 2). After initializing the preterminal level of the chart with the part-of-speech scores from the lexicon, the algorithm continues by repeatedly applying all binary and unary rules in order to build up larger spans (pseudocode is given in Figure 3). To reconstruct the highest scoring parse tree we perform a top-down search. We found this to be more efficient than keeping backpointers.2 One should also note that many real-world applications benefit from, or even expect n-best lists of possible parse trees. Using the lazy evaluation algorithm of Huang and Chiang (2005) the extracNatural Language Parsing While we assume a basic familiarity with probabilistic CKY parsing, in this section we briefly review the CKY dynamic programming algorithm and the Viterbi algorithm for extracting the highest scoring path through the dynamic program. 2.1 Context-Free Grammars In this work we focus our attention on constituency parsing and assume that a weighted CFG is available to us. In our experiments we will use a probabilistic latent variable CFG (Petrov et al., 2006). However, our algorithms can be used with any weighted CFG, including discriminative ones, such as the"
W11-2921,J03-4003,0,\N,Missing
W14-2517,P12-2051,0,0.113044,"Missing"
W14-2517,N13-1090,0,0.010539,"dels (wherein words are represented as indices in a vocabulary set). Thus, words that are semantically close to one another would have word vectors that are likewise ‘close’ (as measured by a distance metric) in the vector space. In fact, Mikolov et al. (2013a) report that word vectors obtained through NLMs capture much deeper level of semantic information than had been previously thought. For example, if xw is the word vector for word w, they note that xapple − xapples ≈ xcar − xcars ≈ xf amily − xf amilies . That is, the concept of pluralization is learned by the vector representations (see Mikolov et al. (2013a) for more examples). NLMs are but one of many methods to obtain word vectors—other techniques include Latent Semantic Analysis (LSA) (Deerwester et al., 1990), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and variations thereof. And even within NLMs there exist various architectures for learning word vectors (Bengio et al. (2003); Mikolov et al. (2010); Collobert et al. (2011); Yih et al. (2011)). We utilize an architecture introduced by Mikolov et al. (2013b), called the Skip-gram, which allows for efficient estimation of word vectors from large corpora. In a Skip-gram model, each"
W14-2517,I13-1040,0,0.0392927,"t al. (2009) use a variation of Latent Semantic Analysis to identify semantic change of specific words from early to modern English. Wijaya and Yeniterzi (2011) utilize a Topics-overTime model and K-means clustering to identify periods during which selected words move from one topic/cluster to another. They correlate their findings with the underlying historical events during that time. Gulordava and Baroni (2011) use co-occurrence counts of words from 1960s and 1990s to detect semantic change. They find that the words identified by the model are consistent with evaluations from human raters. Popescu and Strapparava (2013) employ statistical tests on frequencies of political, social, and emotional words to identify and characterize epochs. Our work contributes to the domain in sev1 http://www.yoon.io 61 Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 61–65, c Baltimore, Maryland, USA, June 26, 2014. 2014 Association for Computational Linguistics eral ways. Whereas previous work has generally involved researchers manually identifying words that have changed (with the exception of Gulordava and Baroni (2011)), we are able to automatically identify them. We are"
W14-2517,W09-0214,0,0.315907,"ll as NLP researchers working with diachronic corpora. Methods employed in previous work have been varied, from analyses of word frequencies to more involved techniques (Guolordava et al. (2011); Mihalcea and Nataste (2012)). In our framework, we train a Neural Language Model (NLM) on yearly corpora to obtain word vectors for each year Related Work Previously, researchers have computationally investigated diachronic language change in various ways. Mihalcea and Nastase (2012) take a supervised learning approach and predict the time period to which a word belongs given its surrounding context. Sagi et al. (2009) use a variation of Latent Semantic Analysis to identify semantic change of specific words from early to modern English. Wijaya and Yeniterzi (2011) utilize a Topics-overTime model and K-means clustering to identify periods during which selected words move from one topic/cluster to another. They correlate their findings with the underlying historical events during that time. Gulordava and Baroni (2011) use co-occurrence counts of words from 1960s and 1990s to detect semantic change. They find that the words identified by the model are consistent with evaluations from human raters. Popescu and"
W14-2517,P12-3029,1,0.551997,"Missing"
W14-2517,W11-0329,0,0.0187404,"Missing"
W14-2517,W11-2508,0,\N,Missing
