2020.acl-main.391,D19-1435,0,0.297242,"Missing"
2020.acl-main.391,W19-4410,0,0.0426625,"orpora, however, it may not be adapted to either the GEC task or the task-specific distribution of inputs. The reason is that in the GEC model, unlike the data used for training BERT, the input can be an erroneous sentence. To fill the gap between corpora used to train GEC and BERT, we additionally train BERT on GEC corpora (BERT-fuse mask) or finetune BERT as a GED model (BERT-fuse GED) and use it for BERT-fuse. GED is a sequence labeling task that detects grammatically incorrect words in input sentences (Rei and Yannakoudakis, 2016; Kaneko et al., 2017). Since BERT is also effective in GED (Bell et al., 2019; Kaneko and Komachi, 2019), it is considered to be suitable for fine-tuning to take into account grammatical errors. Transformer (big) 30 4096 Adam (β1 = 0.9, β2 = 0.98,  = 1 × 10−8 ) 3 × 10−5 1 × 10−6 label smoothed cross-entropy (ls = 0.1) (Szegedy et al., 2016) 0.3 0.1 5 Models Hyperparameter values for the GEC model is listed in Table 1. For the BERT initialized GEC model, we provided experiments based on the open-source code2 . For the BERT-fuse GEC model, we use the code provided by Zhu et al. (2020)3 . While the training the GEC model, the model was evaluated on the development set a"
2020.acl-main.391,P17-1074,0,0.0862394,"p1 https://www.cl.cam.ac.uk/research/nl/ bea2019st/ BERT-Base (cased) 3 32 128 Adam (β1 = 0.9, β2 = 0.999,  = 1 × 10−8 ) 4e − 5 0.1 Table 1: Hyperparameters values of GEC model and Fine-tuned BERT. we use W&I-train, NUCLE, and FCE-train as training, and W&I-dev was used as development data. 4.2 Evaluating GEC Performance In GEC, it is important to evaluate the model with multiple datasets (Mita et al., 2019). Therefore, we used GEC evaluation data such as W&I-test, CoNLL-2014 (Ng et al., 2014), FCE-test and JFLEG (Napoles et al., 2017). We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 (Mizumoto et al., 2011) datasets. We use W&I-dev as a development set. Note tha"
2020.acl-main.391,P19-1042,0,0.214777,"d from the GEC training data. Our experiments show that using the output of the fine-tuned BERT model as additional features in the GEC model (method (c)) is the most effective way of using BERT in most of the GEC corpora that we used in the experiments. We also show that the performance of GEC improves further by combining the BERT-fuse mask and BERTfuse GED methods. The best-performing model achieves state-of-the-art results on the BEA-2019 and CoNLL-2014 benchmarks. 2 Related Work Studies have reported that a MLM can improve the performance of GEC when it is employed either as a re-ranker (Chollampatt et al., 2019; Kaneko et al., 2019) or as a filtering tool (Asano et al., 2019; Kiyono et al., 2019). EncDec-based GEC models combined with MLMs can also be used in combination with these pipeline methods. Asano et al. (2019) proposed sequence labeling models based on correction methods. Our method can utilize the existing EncDec GEC knowledge, but these methods cannot be utilized due to the different architecture of the model. Besides, to the best of our knowledge, no research has yet been conducted that incorporates information of MLMs for effectively training the EncDec GEC model. MLMs are generally use"
2020.acl-main.391,N12-1067,0,0.545172,"019st/ BERT-Base (cased) 3 32 128 Adam (β1 = 0.9, β2 = 0.999,  = 1 × 10−8 ) 4e − 5 0.1 Table 1: Hyperparameters values of GEC model and Fine-tuned BERT. we use W&I-train, NUCLE, and FCE-train as training, and W&I-dev was used as development data. 4.2 Evaluating GEC Performance In GEC, it is important to evaluate the model with multiple datasets (Mita et al., 2019). Therefore, we used GEC evaluation data such as W&I-test, CoNLL-2014 (Ng et al., 2014), FCE-test and JFLEG (Napoles et al., 2017). We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 (Mizumoto et al., 2011) datasets. We use W&I-dev as a development set. Note that we excluded sentence pairs that were not corre"
2020.acl-main.391,W13-1703,0,0.279925,"est, CoNLL-2014 (Ng et al., 2014), FCE-test and JFLEG (Napoles et al., 2017). We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 (Mizumoto et al., 2011) datasets. We use W&I-dev as a development set. Note that we excluded sentence pairs that were not corrected from the training data. To train BERT for BERT-fuse mask and GED, 1 Model Architecture Number of epochs Batch size Max sentence length Optimizer BERT-fuse Mask and GED Experimental Setup 4.1 GED model Learning rate Dropout The advantage of the BERT-fuse is that it can preserve pre-trained information from raw corpora, however, it may not be adapted to either the GEC task or the task-specific distribution of input"
2020.acl-main.391,N19-1423,0,0.230041,"hiro/bert-gec. 1 Introduction Grammatical Error Correction (GEC) is a sequenceto-sequence task where a model corrects an ungrammatical sentence to a grammatical sentence. Numerous studies on GEC have successfully used encoder-decoder (EncDec) based models, and in fact, most current state-of-the-art neural GEC models employ this architecture (Zhao et al., 2019; Grundkiewicz et al., 2019; Kiyono et al., 2019). In light of this trend, one natural, intriguing question is whether neural EndDec GEC models can benefit from the recent advances of masked language models (MLMs) since MLMs such as BERT (Devlin et al., 2019) have been shown to yield substantial improvements in a variety of NLP Kentaro Inui3,2 tasks (Qiu et al., 2020). BERT, for example, builds on the Transformer architecture (Vaswani et al., 2017) and is trained on large raw corpora to learn general representations of linguistic components (e.g., words and sentences) in context, which have been shown useful for various tasks. In recent years, MLMs have been used not only for classification and sequence labeling tasks but also for language generation, where combining MLMs with EncDec models of a downstream task makes a noticeable improvement (Lamp"
2020.acl-main.391,C16-1079,0,0.36437,"elopment Sets workshop1 https://www.cl.cam.ac.uk/research/nl/ bea2019st/ BERT-Base (cased) 3 32 128 Adam (β1 = 0.9, β2 = 0.999,  = 1 × 10−8 ) 4e − 5 0.1 Table 1: Hyperparameters values of GEC model and Fine-tuned BERT. we use W&I-train, NUCLE, and FCE-train as training, and W&I-dev was used as development data. 4.2 Evaluating GEC Performance In GEC, it is important to evaluate the model with multiple datasets (Mita et al., 2019). Therefore, we used GEC evaluation data such as W&I-test, CoNLL-2014 (Ng et al., 2014), FCE-test and JFLEG (Napoles et al., 2017). We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 (Mizumoto et al., 2011) datasets. We use W&I-dev as a dev"
2020.acl-main.391,W19-4427,0,0.49639,"nal features in the GEC model, maximizes the benefit of the MLM. The best-performing model achieves state-ofthe-art performances on the BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at: https://github.com/ kanekomasahiro/bert-gec. 1 Introduction Grammatical Error Correction (GEC) is a sequenceto-sequence task where a model corrects an ungrammatical sentence to a grammatical sentence. Numerous studies on GEC have successfully used encoder-decoder (EncDec) based models, and in fact, most current state-of-the-art neural GEC models employ this architecture (Zhao et al., 2019; Grundkiewicz et al., 2019; Kiyono et al., 2019). In light of this trend, one natural, intriguing question is whether neural EndDec GEC models can benefit from the recent advances of masked language models (MLMs) since MLMs such as BERT (Devlin et al., 2019) have been shown to yield substantial improvements in a variety of NLP Kentaro Inui3,2 tasks (Qiu et al., 2020). BERT, for example, builds on the Transformer architecture (Vaswani et al., 2017) and is trained on large raw corpora to learn general representations of linguistic components (e.g., words and sentences) in context, which have been shown useful for various"
2020.acl-main.391,I17-1005,1,0.888118,"is that it can preserve pre-trained information from raw corpora, however, it may not be adapted to either the GEC task or the task-specific distribution of inputs. The reason is that in the GEC model, unlike the data used for training BERT, the input can be an erroneous sentence. To fill the gap between corpora used to train GEC and BERT, we additionally train BERT on GEC corpora (BERT-fuse mask) or finetune BERT as a GED model (BERT-fuse GED) and use it for BERT-fuse. GED is a sequence labeling task that detects grammatically incorrect words in input sentences (Rei and Yannakoudakis, 2016; Kaneko et al., 2017). Since BERT is also effective in GED (Bell et al., 2019; Kaneko and Komachi, 2019), it is considered to be suitable for fine-tuning to take into account grammatical errors. Transformer (big) 30 4096 Adam (β1 = 0.9, β2 = 0.98,  = 1 × 10−8 ) 3 × 10−5 1 × 10−6 label smoothed cross-entropy (ls = 0.1) (Szegedy et al., 2016) 0.3 0.1 5 Models Hyperparameter values for the GEC model is listed in Table 1. For the BERT initialized GEC model, we provided experiments based on the open-source code2 . For the BERT-fuse GEC model, we use the code provided by Zhu et al. (2020)3 . While the training the GEC"
2020.acl-main.391,D19-1119,1,0.899179,"el, maximizes the benefit of the MLM. The best-performing model achieves state-ofthe-art performances on the BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at: https://github.com/ kanekomasahiro/bert-gec. 1 Introduction Grammatical Error Correction (GEC) is a sequenceto-sequence task where a model corrects an ungrammatical sentence to a grammatical sentence. Numerous studies on GEC have successfully used encoder-decoder (EncDec) based models, and in fact, most current state-of-the-art neural GEC models employ this architecture (Zhao et al., 2019; Grundkiewicz et al., 2019; Kiyono et al., 2019). In light of this trend, one natural, intriguing question is whether neural EndDec GEC models can benefit from the recent advances of masked language models (MLMs) since MLMs such as BERT (Devlin et al., 2019) have been shown to yield substantial improvements in a variety of NLP Kentaro Inui3,2 tasks (Qiu et al., 2020). BERT, for example, builds on the Transformer architecture (Vaswani et al., 2017) and is trained on large raw corpora to learn general representations of linguistic components (e.g., words and sentences) in context, which have been shown useful for various tasks. In recent year"
2020.acl-main.391,N19-1333,0,0.192243,"tting. Hyperparameter values for the GED model is listed in Table 1. We used the BERT-Base cased model, for consistency across experiments5 . The model was evaluated on the development set. 4.4 Pseudo-data We also performed experiments utilizing BERTfuse, BERT-fuse mask, and BERT-fuse GED outputs as additional features to the pre-trained on the pseudo-data GEC model. The pre-trained model using pseudo-data was initialized with the P RETL ARGE +SSE model used in the Kiyono et al. (2019)6 experiments. This pseudo-data is generated by probabilistically injecting character errors into the output (Lichtarge et al., 2019) of a back4 https://github.com/huggingface/ transformers 5 https://github.com/google-research/ bert 6 https://github.com/butsugiri/ gec-pseudodata translation (Xie et al., 2018) model that generates grammatically incorrect sentences from grammatically correct sentences (Kiyono et al., 2019). 4.5 Right-to-left (R2L) Re-ranking for Ensemble We describe the R2L re-ranking technique incorporated in our experiments proposed by Sennrich et al. (2016), which proved to be efficient for the GEC task (Grundkiewicz et al., 2019; Kiyono et al., 2019). Standard left-to-right (L2R) models generate the n-bes"
2020.acl-main.391,N19-1132,1,0.882324,"Missing"
2020.acl-main.391,I11-1017,0,0.532637,". We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 (Mizumoto et al., 2011) datasets. We use W&I-dev as a development set. Note that we excluded sentence pairs that were not corrected from the training data. To train BERT for BERT-fuse mask and GED, 1 Model Architecture Number of epochs Batch size Max sentence length Optimizer BERT-fuse Mask and GED Experimental Setup 4.1 GED model Learning rate Dropout The advantage of the BERT-fuse is that it can preserve pre-trained information from raw corpora, however, it may not be adapted to either the GEC task or the task-specific distribution of inputs. The reason is that in the GEC model, unlike the data used for training B"
2020.acl-main.391,P15-2097,0,0.235789,"× 10−8 ) 4e − 5 0.1 Table 1: Hyperparameters values of GEC model and Fine-tuned BERT. we use W&I-train, NUCLE, and FCE-train as training, and W&I-dev was used as development data. 4.2 Evaluating GEC Performance In GEC, it is important to evaluate the model with multiple datasets (Mita et al., 2019). Therefore, we used GEC evaluation data such as W&I-test, CoNLL-2014 (Ng et al., 2014), FCE-test and JFLEG (Napoles et al., 2017). We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 (Mizumoto et al., 2011) datasets. We use W&I-dev as a development set. Note that we excluded sentence pairs that were not corrected from the training data. To train BERT for BERT-fuse mask and"
2020.acl-main.391,E17-2037,0,0.211708,"BERT and the encoder of the GEC model. 3.3 4 Train and Development Sets workshop1 https://www.cl.cam.ac.uk/research/nl/ bea2019st/ BERT-Base (cased) 3 32 128 Adam (β1 = 0.9, β2 = 0.999,  = 1 × 10−8 ) 4e − 5 0.1 Table 1: Hyperparameters values of GEC model and Fine-tuned BERT. we use W&I-train, NUCLE, and FCE-train as training, and W&I-dev was used as development data. 4.2 Evaluating GEC Performance In GEC, it is important to evaluate the model with multiple datasets (Mita et al., 2019). Therefore, we used GEC evaluation data such as W&I-test, CoNLL-2014 (Ng et al., 2014), FCE-test and JFLEG (Napoles et al., 2017). We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 ("
2020.acl-main.391,N18-1057,0,0.15554,"Missing"
2020.acl-main.391,P11-1019,0,0.514397,"CE-test and JFLEG (Napoles et al., 2017). We used ERRANT evaluation metrics (Felice et al., 2016; Bryant et al., 2017) for W&I-test, M2 score (Dahlmeier and Ng, 2012) for CoNLL-2014 and FCE-test sets, and GLEU (Napoles et al., 2015) for JFLEG. All our results (except ensemble) are the average of four distinct trials using four different random seeds. 4.3 We use the BEA-2019 (Bryant et al., 2019) official shared task data as training and development sets. Specifically, to train a GEC model, we use W&I-train (Granger, 1998; Yannakoudakis et al., 2018), NUCLE (Dahlmeier et al., 2013), FCE-train (Yannakoudakis et al., 2011) and Lang-8 (Mizumoto et al., 2011) datasets. We use W&I-dev as a development set. Note that we excluded sentence pairs that were not corrected from the training data. To train BERT for BERT-fuse mask and GED, 1 Model Architecture Number of epochs Batch size Max sentence length Optimizer BERT-fuse Mask and GED Experimental Setup 4.1 GED model Learning rate Dropout The advantage of the BERT-fuse is that it can preserve pre-trained information from raw corpora, however, it may not be adapted to either the GEC task or the task-specific distribution of inputs. The reason is that in the GEC model,"
2020.acl-main.391,P16-1112,0,0.0371852,"he advantage of the BERT-fuse is that it can preserve pre-trained information from raw corpora, however, it may not be adapted to either the GEC task or the task-specific distribution of inputs. The reason is that in the GEC model, unlike the data used for training BERT, the input can be an erroneous sentence. To fill the gap between corpora used to train GEC and BERT, we additionally train BERT on GEC corpora (BERT-fuse mask) or finetune BERT as a GED model (BERT-fuse GED) and use it for BERT-fuse. GED is a sequence labeling task that detects grammatically incorrect words in input sentences (Rei and Yannakoudakis, 2016; Kaneko et al., 2017). Since BERT is also effective in GED (Bell et al., 2019; Kaneko and Komachi, 2019), it is considered to be suitable for fine-tuning to take into account grammatical errors. Transformer (big) 30 4096 Adam (β1 = 0.9, β2 = 0.98,  = 1 × 10−8 ) 3 × 10−5 1 × 10−6 label smoothed cross-entropy (ls = 0.1) (Szegedy et al., 2016) 0.3 0.1 5 Models Hyperparameter values for the GEC model is listed in Table 1. For the BERT initialized GEC model, we provided experiments based on the open-source code2 . For the BERT-fuse GEC model, we use the code provided by Zhu et al. (2020)3 . Whil"
2020.acl-main.391,N19-1014,0,0.217076,"uned MLM as additional features in the GEC model, maximizes the benefit of the MLM. The best-performing model achieves state-ofthe-art performances on the BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at: https://github.com/ kanekomasahiro/bert-gec. 1 Introduction Grammatical Error Correction (GEC) is a sequenceto-sequence task where a model corrects an ungrammatical sentence to a grammatical sentence. Numerous studies on GEC have successfully used encoder-decoder (EncDec) based models, and in fact, most current state-of-the-art neural GEC models employ this architecture (Zhao et al., 2019; Grundkiewicz et al., 2019; Kiyono et al., 2019). In light of this trend, one natural, intriguing question is whether neural EndDec GEC models can benefit from the recent advances of masked language models (MLMs) since MLMs such as BERT (Devlin et al., 2019) have been shown to yield substantial improvements in a variety of NLP Kentaro Inui3,2 tasks (Qiu et al., 2020). BERT, for example, builds on the Transformer architecture (Vaswani et al., 2017) and is trained on large raw corpora to learn general representations of linguistic components (e.g., words and sentences) in context, which have be"
2020.acl-main.391,W14-1701,0,\N,Missing
2020.acl-main.391,K19-1074,0,\N,Missing
2020.acl-main.391,W19-4406,0,\N,Missing
2020.acl-main.391,W19-4418,1,\N,Missing
2020.acl-main.391,W19-4422,1,\N,Missing
2020.acl-main.47,P16-1162,0,0.0235598,"rst study where evidence is collected on the validity of using LMs for word order analysis and encourages further research on collecting such evidence and examining under what conditions this validity is guaranteed. 3.5 LMs settings We used auto-regressive, unidirectional LMs with Transformer (Vaswani et al., 2017). We used two variants of LMs, a character-based LM (CLM) and a subword-based LM (SLM). In training SLM, the input sentences are once divided into morphemes by MeCab (Kudo, 2006) with a UniDic dictionary,4 and then these morphemes are split into subword units by byte-pair-encoding. (Sennrich et al., 2016)5 . 160M sentences6 randomly selected from 3B web pages were used to train the LMs. Hyperparameters are shown in Appendix A. Given a sentence s, we calculate its generation − − − probability p(s) = → p (s) · ← p (s), where → p (·) and ← − p (·) are generation probabilities calculated by a left-to-right LM and a right-to-left LM, respectively. Depending on the hypothesis, we compare the generation probabilities of various variants of s with different word orders. We assume that the word order with the highest generation probability follows their canonical word order. 4 Experiment1: comparing hu"
2020.acl-main.47,D11-1045,0,0.0393984,"nical word order Every language is assumed to have a canonical word order, even those with flexible word order (Comrie, 1989). There has been a significant linguistic effort to reveal the factors determining the canonical word order (Bresnan et al., 2007; Hoji, 1985). The motivations for revealing the canonical word order range from linguistic interests to those involved in various other fields—it relates to language acquisition and production in psycholinguistics (Slobin and Bever, 1982; Akhtar, 1999), second language education (Alonso Belmonte et al., 2000), and natural language generation (Visweswariah et al., 2011) or error correction (Cheng et al., 2014) in NLP. In Japanese, there are also many studies on its canonical word order (Hoji, 1985; Saeki, 1998; Koizumi and Tamaoka, 2004; Sasano and Okumura, 2016). Japanese canonical word order The word order of Japanese is basically subject-object-verb (SOV) order, but there is no strict rule except placing the verb at the end of the sentence (Tsujimura, 2013). For example, the following three sentences have the same denotational meaning (“A teacher gave a student a book.”): あげた. (2) a. 先生が 生徒に 本を ::::: .............. teacher-NOM student-DAT book-ACC gave. b"
2020.acl-main.47,W06-3122,0,\N,Missing
2020.acl-main.47,C14-1028,0,\N,Missing
2020.acl-main.47,P16-1211,0,\N,Missing
2020.acl-main.47,W16-4120,0,\N,Missing
2020.acl-main.47,W18-2805,0,\N,Missing
2020.acl-main.47,K18-1031,0,\N,Missing
2020.acl-main.47,W17-0706,0,\N,Missing
2020.acl-main.543,D15-1244,1,0.878591,"in a downward-entailing context with a more speciﬁc phrase (replacing cats in P with small cats as in H) yields a sentence inferable from the original sentence. Such primitive inference patterns combine recursively as in (3). This manner of monotonicity and its productivity produces a potentially inﬁnite number of inferential patterns. Therefore, NLI models must be capable of systematically interpreting such primitive patterns and reasoning over unseen combinations of patterns. Although many studies have addressed this issue by modeling logical reasoning in formal semantics (Abzianidze, 2015; Mineshima et al., 2015; Hu et al., 2019) and testing DNN-based models on monotonicity inference (Yanaka et al., 2019a,b; Richardson et al., 6105 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6105–6117 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Systematicity Productivity Train 1 : Fix a quantiﬁer and feed Train 2 : Fix a predicate replacement various predicate replacements. and feed various quantiﬁers. Some puppies ran Some wild dogs ran No dog ran Several puppies ran LEX ADJ Some dogs ran LEX LEX PREP Some dogs in the park ran Several dogs ra"
2020.acl-main.543,D14-1162,0,0.0876244,"archical syntactic structures. Each premise and hypothesis is processed as a tree structure by bottomup combinations of constituent nodes using the same shared compositional function, input word information, and between-word relational information. We parse all premise–hypothesis pairs with the dependency parser using the spaCy library3 and obtain tree structures. For each experimental setting, we randomly sample 100 tree structures and check their correctness. In LSTM and TreeLSTM, the dimension of hidden units is 200, and we initialize the word embeddings with 300-dimensional GloVe vectors (Pennington et al., 2014). Both models are optimized with Adam (Kingma and Ba, 2015), and no dropout is applied. The third architecture is a Bidirectional Encoder Representations from Transformers (BERT) model (Devlin et al., 2019). We used the baseuncased model pre-trained on Wikipedia and BookCorpus from the pytorch-pretrained-bert library4 , ﬁne-tuned for the NLI task using our dataset. In ﬁne-tuning BERT, no dropout is applied, and we choose hyperparameters that are commonly used for MultiNLI. We train all models over 25 epochs or until convergence, and select the best-performing model based on its performance on"
2020.acl-main.543,S18-2023,0,0.0872287,"Missing"
2020.acl-main.543,D19-1228,0,0.0733835,"Geiger et al. (2019) obtained negative results by testing models under fair conditions of natural logic. Our study suggests that these conﬂicting results come from an absence of perspective on combinations of semantic properties. Regarding assessment of the behavior of modern language models, Linzen et al. (2016), Tran et al. (2018), and Goldberg (2019) investigated their syntactic capabilities by testing such models on subject–verb agreement tasks. Many studies of NLI tasks (Liu et al., 2019; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; McCoy et al., 2019; Rozen et al., 2019; Ross and Pavlick, 2019) have provided evaluation methodologies and found that current NLI models often fail on particular inference types, or that they learn undesired heuristics from the training set. In particular, recent works (Yanaka et al., 2019a,b; Richardson et al., 2020) have evaluated models on monotonicity, but did not focus on the ability to generalize to unseen combinations of patterns. Monotonicity covers various systematic inferential patterns, and thus is an adequate semantic phenomenon for assessing inferential systematicity in natural language. Another beneﬁt of focusing on monotonicity is that it p"
2020.acl-main.543,K19-1019,0,0.117841,"engths. In contrast, Geiger et al. (2019) obtained negative results by testing models under fair conditions of natural logic. Our study suggests that these conﬂicting results come from an absence of perspective on combinations of semantic properties. Regarding assessment of the behavior of modern language models, Linzen et al. (2016), Tran et al. (2018), and Goldberg (2019) investigated their syntactic capabilities by testing such models on subject–verb agreement tasks. Many studies of NLI tasks (Liu et al., 2019; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; McCoy et al., 2019; Rozen et al., 2019; Ross and Pavlick, 2019) have provided evaluation methodologies and found that current NLI models often fail on particular inference types, or that they learn undesired heuristics from the training set. In particular, recent works (Yanaka et al., 2019a,b; Richardson et al., 2020) have evaluated models on monotonicity, but did not focus on the ability to generalize to unseen combinations of patterns. Monotonicity covers various systematic inferential patterns, and thus is an adequate semantic phenomenon for assessing inferential systematicity in natural language. Another beneﬁt of focusing on"
2020.acl-main.543,D18-1503,0,0.0471875,"Missing"
2020.acl-main.543,S18-2032,0,0.0188615,"pairs, the output of the prover matched with the entailment relation automatically determined by monotonicity calculus. 3.2 Models We consider three DNN-based NLI models. The ﬁrst architecture employs long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997). We set the number of layers to three with no attention. Each premise and hypothesis is processed as a sequence of words using a recurrent neural network with LSTM cells, and the ﬁnal hidden state of each serves as its representation. The second architecture employs multiplicative tree-structured LSTM (TreeLSTM) networks (Tran and Cheng, 2018), which are expected to be more sensitive to hierarchical syntactic structures. Each premise and hypothesis is processed as a tree structure by bottomup combinations of constituent nodes using the same shared compositional function, input word information, and between-word relational information. We parse all premise–hypothesis pairs with the dependency parser using the spaCy library3 and obtain tree structures. For each experimental setting, we randomly sample 100 tree structures and check their correctness. In LSTM and TreeLSTM, the dimension of hidden units is 200, and we initialize the wor"
2020.acl-main.543,L18-1239,0,0.0543074,"Missing"
2020.acl-main.543,W18-5446,0,0.0818843,"Missing"
2020.acl-main.543,N18-1101,0,0.311412,"es are nearly the same as those in the training set. 1 (1) P : Some [puppies ↑] ran. H: Some dogs ran. (2) P : No [cats ↓] ran. H: No small cats ran. (3) P : Some [puppies which chased no [cats ↓]] ran. H: Some dogs which chased no small cats ran. Introduction Natural language inference (NLI), a task whereby a system judges whether given a set of premises P semantically entails a hypothesis H (Dagan et al., 2013; Bowman et al., 2015), is a fundamental task for natural language understanding. As with other NLP tasks, recent studies have shown a remarkable impact of deep neural networks in NLI (Williams et al., 2018; Wang et al., 2019; Devlin et al., 2019). However, it remains unclear to what extent DNN-based models are capable of learning the compositional generalization underlying NLI from given labeled training instances. As in (1), for example, quantiﬁers such as some exhibit upward monotone (shown as [... ↑]), and replacing a phrase in an upward-entailing context in a sentence with a more general phrase (replacing puppies in P with dogs as in H) yields a sentence inferable from the original sentence. In contrast, as in (2), quantiﬁers such as no exhibit downward monotone (shown as [... ↓]), and repl"
2020.acl-main.543,W19-4804,1,0.948143,"s in H) yields a sentence inferable from the original sentence. Such primitive inference patterns combine recursively as in (3). This manner of monotonicity and its productivity produces a potentially inﬁnite number of inferential patterns. Therefore, NLI models must be capable of systematically interpreting such primitive patterns and reasoning over unseen combinations of patterns. Although many studies have addressed this issue by modeling logical reasoning in formal semantics (Abzianidze, 2015; Mineshima et al., 2015; Hu et al., 2019) and testing DNN-based models on monotonicity inference (Yanaka et al., 2019a,b; Richardson et al., 6105 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6105–6117 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Systematicity Productivity Train 1 : Fix a quantiﬁer and feed Train 2 : Fix a predicate replacement various predicate replacements. and feed various quantiﬁers. Some puppies ran Some wild dogs ran No dog ran Several puppies ran LEX ADJ Some dogs ran LEX LEX PREP Some dogs in the park ran Several dogs ran Several dogs ran PREP Several dogs in the park ran which chased some puppies ran {LEX,ADJ,PR"
2020.acl-main.543,S19-1027,1,0.949189,"s in H) yields a sentence inferable from the original sentence. Such primitive inference patterns combine recursively as in (3). This manner of monotonicity and its productivity produces a potentially inﬁnite number of inferential patterns. Therefore, NLI models must be capable of systematically interpreting such primitive patterns and reasoning over unseen combinations of patterns. Although many studies have addressed this issue by modeling logical reasoning in formal semantics (Abzianidze, 2015; Mineshima et al., 2015; Hu et al., 2019) and testing DNN-based models on monotonicity inference (Yanaka et al., 2019a,b; Richardson et al., 6105 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6105–6117 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Systematicity Productivity Train 1 : Fix a quantiﬁer and feed Train 2 : Fix a predicate replacement various predicate replacements. and feed various quantiﬁers. Some puppies ran Some wild dogs ran No dog ran Several puppies ran LEX ADJ Some dogs ran LEX LEX PREP Some dogs in the park ran Several dogs ran Several dogs ran PREP Several dogs in the park ran which chased some puppies ran {LEX,ADJ,PR"
2020.acl-main.543,2014.lilt-9.7,0,\N,Missing
2020.acl-main.543,D15-1296,0,\N,Missing
2020.acl-main.543,Q16-1037,0,\N,Missing
2020.acl-main.543,P18-2103,0,\N,Missing
2020.acl-main.543,2018.lilt-16.1,0,\N,Missing
2020.acl-main.543,W18-5413,0,\N,Missing
2020.acl-main.543,N19-1423,0,\N,Missing
2020.acl-main.543,D19-1456,0,\N,Missing
2020.acl-main.543,W15-4002,0,\N,Missing
2020.acl-main.55,P17-1046,0,0.0634245,"on arises from a nature of dialogue, that is, there are many acceptable responses to an input context, known as the one-to-many problem (Zhao et al., 2017). To tackle this problematic issue, we focus on evaluating response generation systems via response selection. In this task, systems select an appropriate response for a given context from a set of response candidates. Each candidate has the label that indicates whether the candidate is appropriate response for the given context. Traditionally, response selection has been used to evaluate retrieval-based dialogue systems (Lowe et al., 2015; Wu et al., 2017). We consider applying this task to driving the research for dialogue generation systems. Specifically, we consider using response selection to pick out promising systems that should be evaluated more precisely by humans among a lot of candidate systems. We assume that response selection is a valid option for such a preliminary evaluation on the basis of the following assumption: systems that can generate appropriate responses can also select appropriate responses. One advantage of evaluating generation systems via response selection is that it can remedy the one-to-many problem, because we do"
2020.acl-main.55,P02-1040,0,\N,Missing
2020.acl-main.55,W05-0909,0,\N,Missing
2020.acl-main.55,W04-1013,0,\N,Missing
2020.acl-main.55,D16-1230,0,\N,Missing
2020.acl-main.55,W15-4640,0,\N,Missing
2020.acl-main.55,L18-1275,0,\N,Missing
2020.acl-main.55,W19-4107,0,\N,Missing
2020.acl-main.575,D18-1019,0,0.0147201,"One drawback of this approach is the difficulty dealing with nested entities.3 By contrast, the span classification approach, adopted in this study, can straightforwardly solve nested NER (Finkel and Manning, 2009; Sohrab and Miwa, 2018; Xia et al., 2019).4 2 Very recently, a hybrid model of these two approaches has been proposed by Liu et al. (2019). 3 Some studies have sophisticated sequence labeling models for nested NER (Ju et al., 2018; Zheng et al., 2019). 4 There is an approach specialized for nested NER using hypergraphs (Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018). 3.1 Instance-Based Span Classification NER as span classification NER can be solved as multi-class classification, where each of possible spans in a sentence is assigned a class label. As we mentioned in Section 2, this approach can naturally avoid inconsistent label prediction and straightforwardly deal with nested entities. Because of these advantages over tokenwise classification, span classification has been gaining a considerable attention (Sohrab and Miwa, 2018; Xia et al., 2019). Formally, given an input sentence of T words X = (w1 , w2 , . . . , wT ), we first enumerate possible span"
2020.acl-main.575,W04-2407,0,0.0902054,"rty. The rationales underlying the model predictions are opaque for humans to understand. Many recent studies have tried to look into classifier-based neural models (Ribeiro et al., 2016; Lundberg and Lee, 2017; Koh and Liang, 2017). In this paper, instead of looking into the black-box, we build interpretable models based on instance-based learning. Before the current neural era, instance-based learning, sometimes called memory-based learning (Daelemans and Van den Bosch, 2005), was widely used for various NLP tasks, such as part-of-speech tagging (Daelemans et al., 1996), dependency parsing (Nivre et al., 2004) and machine translation (Nagao, 1984). For NER, some instance-based models have been proposed (Tjong Kim Sang, 2002; De Meulder and Daelemans, 2003; Hendrickx and van den Bosch, 2003). Recently, despite its high interpretability, this direction has not been explored. One exception is Wiseman and Stratos (2019), which used instance-based learning of token representations. Due to BIO tagging, it faces one technical challenge: inconsistent label prediction. For example, an entity candidate “World Health Organization” can be assigned inconsistent labels such as “B-LOC I-ORG I-ORG,” whereas the gr"
2020.acl-main.575,P16-1218,0,0.0699399,"Missing"
2020.acl-main.575,D18-1191,1,0.672389,"and been underexplored. This study presents and investigates an instancebased learning method for span representations. A span is a unit that consists of one or more linguistically linked words. Why do we focus on spans instead of tokens? One reason is relevant to performance. Recent neural networks can induce good span feature representations and achieve high performance in structured prediction tasks, such as named entity recognition (NER) (Sohrab and Miwa, 2018; Xia et al., 2019), constituency parsing (Stern et al., 2017; Kitaev et al., 2019), semantic role labeling (SRL) (He et al., 2018; Ouchi et al., 2018) and coreference resolution (Lee et al., 2017). Another reason is relevant to interpretability. The tasks above require recognition of linguistic structure that consists of spans. Thus, directly classifying each span based on its representation is more interpretable than token-wise classification such as BIO tagging, which reconstructs each span label from the predicted token-wise BIO tags. Our method builds a feature space where spans with the same class label are close to each other. At inference time, each span is assigned a class label based on its neighbor spans in the feature space. We c"
2020.acl-main.575,D14-1162,0,0.0920737,"en multiply hlstm with a weight matrix W and obtain the span s representation: hs = W hlstm . For the scoring s function in Equation 1 in the instance-based span model, we use the inner product between a pair of span representations: score(si , sj ) = hsi · hsj . Model configuration We train instance-based models by using K = 50 training sentences randomly retrieved for each mini-batch. At test time, we use K = 50 nearest training sentences for each sentence based on the cosine similarities between their sentence vectors8 . For the word embeddings, we use the GloVe 100-dimensional embeddings (Pennington et al., 2014) and the BERT embeddings (Devlin et al., 2019).9 6 We use the same one pre-processed by Zheng et al. (2019) at https://github.com/thecharm/ boundary-aware-nested-ner 7 We use the different span representation from the one used for flat NER because concatenating the addition features, − → − → ← − ← − h a + h b and h a + h b , to the subtraction features improves performance in our preliminary experiments. 8 For each sentence X = (w1 , w2 , . . . , wT ), its sentence vector is defined as the vector averagedP over the word embeddings (GloVe) within the sentence: T1 t wemb . t 9 Details on the exp"
2020.acl-main.575,P19-1533,0,0.0192399,"table models based on instance-based learning. Before the current neural era, instance-based learning, sometimes called memory-based learning (Daelemans and Van den Bosch, 2005), was widely used for various NLP tasks, such as part-of-speech tagging (Daelemans et al., 1996), dependency parsing (Nivre et al., 2004) and machine translation (Nagao, 1984). For NER, some instance-based models have been proposed (Tjong Kim Sang, 2002; De Meulder and Daelemans, 2003; Hendrickx and van den Bosch, 2003). Recently, despite its high interpretability, this direction has not been explored. One exception is Wiseman and Stratos (2019), which used instance-based learning of token representations. Due to BIO tagging, it faces one technical challenge: inconsistent label prediction. For example, an entity candidate “World Health Organization” can be assigned inconsistent labels such as “B-LOC I-ORG I-ORG,” whereas the groundtruth labels are “B-ORG I-ORG I-ORG.” To remedy this issue, they presented a heuristic technique for encouraging contiguous token alignment. In contrast to such token-wise prediction, we adopt span-wise prediction, which can naturally avoid this issue because each span is assigned one label. NER is generall"
2020.acl-main.575,D19-1034,0,0.0704427,"d by using neural networks and fed into a classifier, such as conditional random fields (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). One drawback of this approach is the difficulty dealing with nested entities.3 By contrast, the span classification approach, adopted in this study, can straightforwardly solve nested NER (Finkel and Manning, 2009; Sohrab and Miwa, 2018; Xia et al., 2019).4 2 Very recently, a hybrid model of these two approaches has been proposed by Liu et al. (2019). 3 Some studies have sophisticated sequence labeling models for nested NER (Ju et al., 2018; Zheng et al., 2019). 4 There is an approach specialized for nested NER using hypergraphs (Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018). 3.1 Instance-Based Span Classification NER as span classification NER can be solved as multi-class classification, where each of possible spans in a sentence is assigned a class label. As we mentioned in Section 2, this approach can naturally avoid inconsistent label prediction and straightforwardly deal with nested entities. Because of these advantages over tokenwise classification, span classification has been gaining a considerable attent"
2020.acl-main.575,N16-3020,0,0.226569,"w much each training instance contributes to the predictions. Through empirical analysis on named entity recognition, we demonstrate that our method enables to build models that have high interpretability without sacrificing performance. 1 Introduction Neural networks have contributed to performance improvements in structured prediction. Instead, the rationales underlying the model predictions are difficult for humans to understand (Lei et al., 2016). In practical applications, interpretable rationales play a critical role for driving human’s decisions and promoting human-machine cooperation (Ribeiro et al., 2016). With this motivation, we aim to build models that have high interpretability without sacrificing performance. As an approach to this challenge, we focus on instance-based learning. Instance-based learning (Aha et al., 1991) is a machine learning method that learns similarities between instances. At inference time, the class labels of the most similar training instances are assigned to the new instances. This transparent inference process provides an answer to the following question: Which points in the training set most closely resemble a test point or influenced the prediction? This is cate"
2020.acl-main.575,D18-1309,0,0.0276638,"Missing"
2020.acl-main.575,P17-1076,0,0.027973,"al., 2010). Recently, despite its preferable property, it has received little attention and been underexplored. This study presents and investigates an instancebased learning method for span representations. A span is a unit that consists of one or more linguistically linked words. Why do we focus on spans instead of tokens? One reason is relevant to performance. Recent neural networks can induce good span feature representations and achieve high performance in structured prediction tasks, such as named entity recognition (NER) (Sohrab and Miwa, 2018; Xia et al., 2019), constituency parsing (Stern et al., 2017; Kitaev et al., 2019), semantic role labeling (SRL) (He et al., 2018; Ouchi et al., 2018) and coreference resolution (Lee et al., 2017). Another reason is relevant to interpretability. The tasks above require recognition of linguistic structure that consists of spans. Thus, directly classifying each span based on its representation is more interpretable than token-wise classification such as BIO tagging, which reconstructs each span label from the predicted token-wise BIO tags. Our method builds a feature space where spans with the same class label are close to each other. At inference time,"
2020.acl-main.575,W02-2025,0,0.0739357,"ook into classifier-based neural models (Ribeiro et al., 2016; Lundberg and Lee, 2017; Koh and Liang, 2017). In this paper, instead of looking into the black-box, we build interpretable models based on instance-based learning. Before the current neural era, instance-based learning, sometimes called memory-based learning (Daelemans and Van den Bosch, 2005), was widely used for various NLP tasks, such as part-of-speech tagging (Daelemans et al., 1996), dependency parsing (Nivre et al., 2004) and machine translation (Nagao, 1984). For NER, some instance-based models have been proposed (Tjong Kim Sang, 2002; De Meulder and Daelemans, 2003; Hendrickx and van den Bosch, 2003). Recently, despite its high interpretability, this direction has not been explored. One exception is Wiseman and Stratos (2019), which used instance-based learning of token representations. Due to BIO tagging, it faces one technical challenge: inconsistent label prediction. For example, an entity candidate “World Health Organization” can be assigned inconsistent labels such as “B-LOC I-ORG I-ORG,” whereas the groundtruth labels are “B-ORG I-ORG I-ORG.” To remedy this issue, they presented a heuristic technique for encouraging"
2020.acl-main.575,W00-0726,0,0.146652,"Missing"
2020.acl-main.575,W96-0102,0,\N,Missing
2020.acl-main.575,D09-1015,0,\N,Missing
2020.acl-main.575,W03-0435,0,\N,Missing
2020.acl-main.575,W03-0427,0,\N,Missing
2020.acl-main.575,W03-0419,0,\N,Missing
2020.acl-main.575,D15-1102,0,\N,Missing
2020.acl-main.575,N16-1030,0,\N,Missing
2020.acl-main.575,D17-1276,0,\N,Missing
2020.acl-main.575,N18-1079,0,\N,Missing
2020.acl-main.575,N18-1131,0,\N,Missing
2020.acl-main.575,P19-1138,0,\N,Missing
2020.acl-main.575,P19-1524,0,\N,Missing
2020.acl-main.575,N19-1423,0,\N,Missing
2020.acl-main.575,Q16-1026,0,\N,Missing
2020.acl-main.575,P19-1340,0,\N,Missing
2020.acl-main.602,P15-1034,0,0.0366732,"Missing"
2020.acl-main.602,N19-1405,0,0.0494746,"o address this issue, the community has introduced increasingly more difficult Question Answering (QA) problems, for example, so that answerFigure 1: R4 C, a new RC task extending upon the standard RC setting, requiring systems to provide not only an answer, but also a derivation. The example is taken from HotpotQA (Yang et al., 2018), where sentences [1-2, 4, 6-7] are supporting facts, and [3,5] are not. related information is scattered across several articles (Welbl et al., 2018; Yang et al., 2018) (i.e. multi-hop QA). However, recent studies show that such multi-hop QA also has weaknesses (Chen and Durrett, 2019; Min et al., 2019; Jiang et al., 2019), e.g. combining multiple sources of information is not always necessary to find answers. Another direction, which we follow, includes evaluating a systems’ reasoning (Jansen, 2018; Yang et al., 2018; Thorne and Vlachos, 2018; Camburu et al., 2018; Fan et al., 2019; Rajani et al., 2019). In 6740 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6740–6750 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the context of RC, Yang et al. (2018) propose HotpotQA, which requires systems not only to"
2020.acl-main.602,P19-1346,0,0.014447,"tpotQA (Yang et al., 2018), where sentences [1-2, 4, 6-7] are supporting facts, and [3,5] are not. related information is scattered across several articles (Welbl et al., 2018; Yang et al., 2018) (i.e. multi-hop QA). However, recent studies show that such multi-hop QA also has weaknesses (Chen and Durrett, 2019; Min et al., 2019; Jiang et al., 2019), e.g. combining multiple sources of information is not always necessary to find answers. Another direction, which we follow, includes evaluating a systems’ reasoning (Jansen, 2018; Yang et al., 2018; Thorne and Vlachos, 2018; Camburu et al., 2018; Fan et al., 2019; Rajani et al., 2019). In 6740 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6740–6750 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the context of RC, Yang et al. (2018) propose HotpotQA, which requires systems not only to give an answer but also to identify supporting facts (SFs), sentences containing information that supports the answer. SFs are defined as sentences containing information that supports the answer (see “Supporting facts” in Fig. 1 for an example). As shown in SFs [1] , [2] , and [7] , however, only a sub"
2020.acl-main.602,D16-1264,0,0.167371,"Missing"
2020.acl-main.602,C18-1283,0,0.0154463,"also a derivation. The example is taken from HotpotQA (Yang et al., 2018), where sentences [1-2, 4, 6-7] are supporting facts, and [3,5] are not. related information is scattered across several articles (Welbl et al., 2018; Yang et al., 2018) (i.e. multi-hop QA). However, recent studies show that such multi-hop QA also has weaknesses (Chen and Durrett, 2019; Min et al., 2019; Jiang et al., 2019), e.g. combining multiple sources of information is not always necessary to find answers. Another direction, which we follow, includes evaluating a systems’ reasoning (Jansen, 2018; Yang et al., 2018; Thorne and Vlachos, 2018; Camburu et al., 2018; Fan et al., 2019; Rajani et al., 2019). In 6740 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6740–6750 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the context of RC, Yang et al. (2018) propose HotpotQA, which requires systems not only to give an answer but also to identify supporting facts (SFs), sentences containing information that supports the answer. SFs are defined as sentences containing information that supports the answer (see “Supporting facts” in Fig. 1 for an example). As shown in SFs ["
2020.acl-main.602,Q18-1021,1,0.940297,"[7] Wood died only days before the scheduled release of the band’s debut album, “Apple”, thus ending the… Explanation Answer Supporting facts (SFs): Malfunkshun Output [1], [2], [4], [6], [7] R4C: Derivation [Malfunkshun] is [a rock band] [Andrew Wood] is lead singer of [Malfunkshun] [Andrew Wood] is a member of [Mother Love Bone] [Malfunkshun] is former of [Mother Love Bone] [Andrew Wood] died just before the release of [Apple] Introduction Reading comprehension (RC) has become a key benchmark for natural language understanding (NLU) systems, and a large number of datasets are now available (Welbl et al., 2018; Koˇcisk`y et al., 2018; Yang et al., 2018, i.a.). However, it has been established that these datasets suffer from annotation artifacts and other biases, which may allow systems to “cheat”: Instead of learning to read and comprehend texts in their entirety, systems learn to exploit these biases and find answers via simple heuristics, such as looking for an entity with a particular semantic type (Sugawara et al., 2018; Mudrakarta et al., 2018) (e.g. given a question starting with Who, a system finds a person entity found in a document). To address this issue, the community has introduced incr"
2020.acl-main.602,D18-1259,0,0.358136,"release of the band’s debut album, “Apple”, thus ending the… Explanation Answer Supporting facts (SFs): Malfunkshun Output [1], [2], [4], [6], [7] R4C: Derivation [Malfunkshun] is [a rock band] [Andrew Wood] is lead singer of [Malfunkshun] [Andrew Wood] is a member of [Mother Love Bone] [Malfunkshun] is former of [Mother Love Bone] [Andrew Wood] died just before the release of [Apple] Introduction Reading comprehension (RC) has become a key benchmark for natural language understanding (NLU) systems, and a large number of datasets are now available (Welbl et al., 2018; Koˇcisk`y et al., 2018; Yang et al., 2018, i.a.). However, it has been established that these datasets suffer from annotation artifacts and other biases, which may allow systems to “cheat”: Instead of learning to read and comprehend texts in their entirety, systems learn to exploit these biases and find answers via simple heuristics, such as looking for an entity with a particular semantic type (Sugawara et al., 2018; Mudrakarta et al., 2018) (e.g. given a question starting with Who, a system finds a person entity found in a document). To address this issue, the community has introduced increasingly more difficult Question Answering"
2020.acl-main.602,W18-1703,0,0.0229255,"o provide not only an answer, but also a derivation. The example is taken from HotpotQA (Yang et al., 2018), where sentences [1-2, 4, 6-7] are supporting facts, and [3,5] are not. related information is scattered across several articles (Welbl et al., 2018; Yang et al., 2018) (i.e. multi-hop QA). However, recent studies show that such multi-hop QA also has weaknesses (Chen and Durrett, 2019; Min et al., 2019; Jiang et al., 2019), e.g. combining multiple sources of information is not always necessary to find answers. Another direction, which we follow, includes evaluating a systems’ reasoning (Jansen, 2018; Yang et al., 2018; Thorne and Vlachos, 2018; Camburu et al., 2018; Fan et al., 2019; Rajani et al., 2019). In 6740 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6740–6750 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the context of RC, Yang et al. (2018) propose HotpotQA, which requires systems not only to give an answer but also to identify supporting facts (SFs), sentences containing information that supports the answer. SFs are defined as sentences containing information that supports the answer (see “Supporting facts”"
2020.acl-main.602,L18-1433,0,0.116796,"Missing"
2020.acl-main.602,P19-1261,0,0.0191287,"troduced increasingly more difficult Question Answering (QA) problems, for example, so that answerFigure 1: R4 C, a new RC task extending upon the standard RC setting, requiring systems to provide not only an answer, but also a derivation. The example is taken from HotpotQA (Yang et al., 2018), where sentences [1-2, 4, 6-7] are supporting facts, and [3,5] are not. related information is scattered across several articles (Welbl et al., 2018; Yang et al., 2018) (i.e. multi-hop QA). However, recent studies show that such multi-hop QA also has weaknesses (Chen and Durrett, 2019; Min et al., 2019; Jiang et al., 2019), e.g. combining multiple sources of information is not always necessary to find answers. Another direction, which we follow, includes evaluating a systems’ reasoning (Jansen, 2018; Yang et al., 2018; Thorne and Vlachos, 2018; Camburu et al., 2018; Fan et al., 2019; Rajani et al., 2019). In 6740 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6740–6750 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the context of RC, Yang et al. (2018) propose HotpotQA, which requires systems not only to give an answer but also to identify sup"
2020.acl-main.602,Q18-1023,0,0.0608472,"Missing"
2020.acl-main.602,P19-1416,0,0.0313211,"e community has introduced increasingly more difficult Question Answering (QA) problems, for example, so that answerFigure 1: R4 C, a new RC task extending upon the standard RC setting, requiring systems to provide not only an answer, but also a derivation. The example is taken from HotpotQA (Yang et al., 2018), where sentences [1-2, 4, 6-7] are supporting facts, and [3,5] are not. related information is scattered across several articles (Welbl et al., 2018; Yang et al., 2018) (i.e. multi-hop QA). However, recent studies show that such multi-hop QA also has weaknesses (Chen and Durrett, 2019; Min et al., 2019; Jiang et al., 2019), e.g. combining multiple sources of information is not always necessary to find answers. Another direction, which we follow, includes evaluating a systems’ reasoning (Jansen, 2018; Yang et al., 2018; Thorne and Vlachos, 2018; Camburu et al., 2018; Fan et al., 2019; Rajani et al., 2019). In 6740 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6740–6750 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the context of RC, Yang et al. (2018) propose HotpotQA, which requires systems not only to give an answer but"
2020.acl-main.602,P19-1487,0,0.0634591,"., 2018), where sentences [1-2, 4, 6-7] are supporting facts, and [3,5] are not. related information is scattered across several articles (Welbl et al., 2018; Yang et al., 2018) (i.e. multi-hop QA). However, recent studies show that such multi-hop QA also has weaknesses (Chen and Durrett, 2019; Min et al., 2019; Jiang et al., 2019), e.g. combining multiple sources of information is not always necessary to find answers. Another direction, which we follow, includes evaluating a systems’ reasoning (Jansen, 2018; Yang et al., 2018; Thorne and Vlachos, 2018; Camburu et al., 2018; Fan et al., 2019; Rajani et al., 2019). In 6740 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6740–6750 c July 5 - 10, 2020. 2020 Association for Computational Linguistics the context of RC, Yang et al. (2018) propose HotpotQA, which requires systems not only to give an answer but also to identify supporting facts (SFs), sentences containing information that supports the answer. SFs are defined as sentences containing information that supports the answer (see “Supporting facts” in Fig. 1 for an example). As shown in SFs [1] , [2] , and [7] , however, only a subset of SFs may contrib"
2020.acl-srw.30,Q16-1026,0,0.252432,". Introduction Sequence labeling is a problem in which a label is assigned to each word in an input sentence. In many label sets, each label consists of different types of elements. For example, IOB-format entity labels (Ramshaw and Marcus, 1995), such as B-Person and I-Location, can be decomposed into span (e.g., B, I and O) and type information (e.g., Person and Location). Also, morphological feature tags (More et al., 2018), such as Gender=Masc|Number=Sing, can be decomposed into gender, number and other information. General sequence labeling models (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016), however, do not consider such components. Specifically, the probability that each word is assigned a label is computed on the basis of the inner product between word representation and label embedding (see Equation 2 in Section 2.1). Here, the label embedding is associated with each label and independently trained without considering its components. This means that labels are treated as mutually exclusive. In fact, labels often share some To investigate the effectiveness of our method, we take the task of fine-grained Named Entity Recognition (NER) as a case study. Typically, in this task, a"
2020.acl-srw.30,N19-1423,0,0.0150743,"a gap between the frequencies, i.e., how many times each label appears in the training set. We categorize each label into three classes on the basis of its frequency, shown in Table 2. For example, if a label appears 0–100 times in the training set, it is categorized into the “Low” class. Moreover, we denote how many times entities with the labels belonging to each frequency class appear in the development or test set. To better understand the model behavior, we investigate the performance of each frequency class. Model setup As the encoder f (x, X) in Equation 2 in Section 2.1, we use BERT5 (Devlin et al., 2019), which is a state-of-the-art language model.6 As the baseline model, we use the general label embedding matrix without considering label components, i.e., each label embedding W[y] in Equation 2 is randomly initialized and independently learned. In contrast, our proposed model calculates the label embedding matrix from label components (Equations 3 and 4). The only difference between these models is the label embedding matrix, so if a performance gap between them is observed, it stems from this point. Experiments Settings 224 Hyperparameters The overall settings of hyperparameters are the sam"
2020.acl-srw.30,W95-0107,0,0.538029,"s of label components (see details in Section 2.2). Specifically, we first decompose each label into its components. We then assign an embedding to each component and summarize the embeddings of all the components into one as a label embedding used in a model. This component-level operation enables the model to share information on the common components across label embeddings. Introduction Sequence labeling is a problem in which a label is assigned to each word in an input sentence. In many label sets, each label consists of different types of elements. For example, IOB-format entity labels (Ramshaw and Marcus, 1995), such as B-Person and I-Location, can be decomposed into span (e.g., B, I and O) and type information (e.g., Person and Location). Also, morphological feature tags (More et al., 2018), such as Gender=Masc|Number=Sing, can be decomposed into gender, number and other information. General sequence labeling models (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016), however, do not consider such components. Specifically, the probability that each word is assigned a label is computed on the basis of the inner product between word representation and label embedding (see Equation 2 in S"
2020.acl-srw.30,D17-1206,0,0.0224735,"them in detail, we obtained two findings. First, in the embedding space learned by the proposed model, we found that two distinct clusters were formed corresponding to the two span labels (i.e. B and I). Second, the labels that have the same top layer label (represented in the same color) also formed some smaller clusters within the B and I-label clusters. For example, Figure 3c shows the Product cluster whose members are the labels sharing the top layer label Product. Related work Sequence labeling has been widely studied and applied to many tasks, such as Chunking (Ramshaw and Marcus, 1995; Hashimoto et al., 2017), NER (Ma and Hovy, 2016; Chiu and Nichols, 2016) and Semantic Role Labeling (SRL) (Zhou and Xu, 2015; He et al., 2017). In English fine-grained entity recognition, Ling and Weld (2012) created a standard fine-grained entity typing dataset with multi-class, multi-label annotations. Ringland et al. (2019) developed a dataset for nested NER dataset. These datasets independently handle each label without considering label components. In Japanese NER, Misawa et al. (2017) combined word and character information to improve performance. Mai et al. (2018) reported that dictionary information improves"
2020.acl-srw.30,P19-1510,0,0.0170803,"smaller clusters within the B and I-label clusters. For example, Figure 3c shows the Product cluster whose members are the labels sharing the top layer label Product. Related work Sequence labeling has been widely studied and applied to many tasks, such as Chunking (Ramshaw and Marcus, 1995; Hashimoto et al., 2017), NER (Ma and Hovy, 2016; Chiu and Nichols, 2016) and Semantic Role Labeling (SRL) (Zhou and Xu, 2015; He et al., 2017). In English fine-grained entity recognition, Ling and Weld (2012) created a standard fine-grained entity typing dataset with multi-class, multi-label annotations. Ringland et al. (2019) developed a dataset for nested NER dataset. These datasets independently handle each label without considering label components. In Japanese NER, Misawa et al. (2017) combined word and character information to improve performance. Mai et al. (2018) reported that dictionary information improves the performance of finegrained NER. Their methods do not consider label components and are orthogonal to our method. Some existing studies take shared components (or information) across labels into account. In Entity Typing, Ma et al. (2016) and Shimaoka et al. (2017) proposed to calculate entity label"
2020.acl-srw.30,P17-1044,0,0.0162298,"nct clusters were formed corresponding to the two span labels (i.e. B and I). Second, the labels that have the same top layer label (represented in the same color) also formed some smaller clusters within the B and I-label clusters. For example, Figure 3c shows the Product cluster whose members are the labels sharing the top layer label Product. Related work Sequence labeling has been widely studied and applied to many tasks, such as Chunking (Ramshaw and Marcus, 1995; Hashimoto et al., 2017), NER (Ma and Hovy, 2016; Chiu and Nichols, 2016) and Semantic Role Labeling (SRL) (Zhou and Xu, 2015; He et al., 2017). In English fine-grained entity recognition, Ling and Weld (2012) created a standard fine-grained entity typing dataset with multi-class, multi-label annotations. Ringland et al. (2019) developed a dataset for nested NER dataset. These datasets independently handle each label without considering label components. In Japanese NER, Misawa et al. (2017) combined word and character information to improve performance. Mai et al. (2018) reported that dictionary information improves the performance of finegrained NER. Their methods do not consider label components and are orthogonal to our method. S"
2020.acl-srw.30,sekine-etal-2002-extended,0,0.721438,"y-type labels are predefined in a hierarchical structure, and intermediate type labels can be used as label components, as well as leaf type labels and B/I-labels. In this sense, the fine-grained NER can be seen as a good example of the potential applications of the proposed method. Furthermore, some entity labels occur more frequently than others. An interesting question is whether our method of label component sharing exhibits an improvement in recognizing entities of infrequent labels. In our experiments, we use the English and Japanese NER corpora with the Extended Named Entity Hierarchy (Sekine et al., 2002) including 200 entity tags. To sum up, our main contributions are as follows: (i) we propose a method that shares and learns label component embeddings, and (ii) through experiments on English and Japanese fine-grained NER, we demonstrate that the proposed method achieves better performance than a standard sequence labeling model, especially for instances with low-frequency labels. 222 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 222–229 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics label em"
2020.acl-srw.30,N16-1030,0,0.0112779,"ross label embeddings. Introduction Sequence labeling is a problem in which a label is assigned to each word in an input sentence. In many label sets, each label consists of different types of elements. For example, IOB-format entity labels (Ramshaw and Marcus, 1995), such as B-Person and I-Location, can be decomposed into span (e.g., B, I and O) and type information (e.g., Person and Location). Also, morphological feature tags (More et al., 2018), such as Gender=Masc|Number=Sing, can be decomposed into gender, number and other information. General sequence labeling models (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016), however, do not consider such components. Specifically, the probability that each word is assigned a label is computed on the basis of the inner product between word representation and label embedding (see Equation 2 in Section 2.1). Here, the label embedding is associated with each label and independently trained without considering its components. This means that labels are treated as mutually exclusive. In fact, labels often share some To investigate the effectiveness of our method, we take the task of fine-grained Named Entity Recognition (NER) as a case study. T"
2020.acl-srw.30,E17-1119,1,0.927102,"mbedding. Compared with the summation, one disadvantage of the concatenation is memory efficiency: the number of dimensions of the label embeddings increases according to the number of label components K. Our label embedding calculation enables models to share the embeddings of label components commonly shared across labels. For example, the embeddings of both B-Facility/GOE/Park and B-Facility/GOE/School are calculated by adding the embeddings of the shared components (i.e., B, Facility and GOE). Equations 3 and 4 can be regarded as a general form of the hierarchical label matrix proposed by Shimaoka et al. (2017) because our method can treat not only hierarchical structures but also any type of type–value set, such as morphological feature labels (e.g. Gender=Masc|Number=Sing). 3 3.1 Dataset We use the Extended Named Entity Corpus for English2 and Japanese.3 fine-grained NER (Mai et al., 2018) In this dataset, each NE is assigned one of 200 entity labels defined in the Extended Named Entity Hierarchy (Sekine et al., 2002). For the English dataset, we follow the training/development/test split defined by Mai et al. (2018). For the Japanese dataset, we follow the training/development/test split of Unive"
2020.acl-srw.30,P16-1101,0,0.241916,"ommon components across label embeddings. Introduction Sequence labeling is a problem in which a label is assigned to each word in an input sentence. In many label sets, each label consists of different types of elements. For example, IOB-format entity labels (Ramshaw and Marcus, 1995), such as B-Person and I-Location, can be decomposed into span (e.g., B, I and O) and type information (e.g., Person and Location). Also, morphological feature tags (More et al., 2018), such as Gender=Masc|Number=Sing, can be decomposed into gender, number and other information. General sequence labeling models (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016), however, do not consider such components. Specifically, the probability that each word is assigned a label is computed on the basis of the inner product between word representation and label embedding (see Equation 2 in Section 2.1). Here, the label embedding is associated with each label and independently trained without considering its components. This means that labels are treated as mutually exclusive. In fact, labels often share some To investigate the effectiveness of our method, we take the task of fine-grained Named Entity Recognition (NE"
2020.acl-srw.30,C16-1017,0,0.0197358,"yping dataset with multi-class, multi-label annotations. Ringland et al. (2019) developed a dataset for nested NER dataset. These datasets independently handle each label without considering label components. In Japanese NER, Misawa et al. (2017) combined word and character information to improve performance. Mai et al. (2018) reported that dictionary information improves the performance of finegrained NER. Their methods do not consider label components and are orthogonal to our method. Some existing studies take shared components (or information) across labels into account. In Entity Typing, Ma et al. (2016) and Shimaoka et al. (2017) proposed to calculate entity label embeddings by considering a label hierarchical structure. While their method is limited to only a hierarchical structure, our method can be applied to any set of components and can be regarded as a general form of their method. In multi-label classification, Zhong et al. (2018) assumed that the labels cooccurring in many instances are correlated with each other and share some common features, and proposed a method that learns a feature (label em226 I-labels B-labels (a) BASELINE (b) P ROPOSED :S UM (c) Enlarged view of a cluster in"
2020.acl-srw.30,P09-1003,0,0.0452038,"ication, Zhong et al. (2018) assumed that the labels cooccurring in many instances are correlated with each other and share some common features, and proposed a method that learns a feature (label em226 I-labels B-labels (a) BASELINE (b) P ROPOSED :S UM (c) Enlarged view of a cluster in (b). The embeddings of the labels sharing the top layer label Product form this cluster. Figure 3: Visualization of the label embedding space. The same color represents the labels that have the same hierarchical top layer label. bedding) space where such co-occurring labels are close to each other. The work of Matsubayashi et al. (2009) is the closest to ours in terms of decomposing the features of labels. They regard an original label comprising a mixture of components as a set of multiple labels and made models that are able to exploit the multiple components to effectively learn in the SRL task. 5 Conclusion We proposed a method that shares and learns the embeddings of label components. Through experiments on English and Japanese fine-grained NER, we demonstrated that our proposed method improves the performance, especially for instances with low-frequency labels. For future work, we envision to apply our method to other"
2020.acl-srw.30,W17-4114,0,0.0139155,"elated work Sequence labeling has been widely studied and applied to many tasks, such as Chunking (Ramshaw and Marcus, 1995; Hashimoto et al., 2017), NER (Ma and Hovy, 2016; Chiu and Nichols, 2016) and Semantic Role Labeling (SRL) (Zhou and Xu, 2015; He et al., 2017). In English fine-grained entity recognition, Ling and Weld (2012) created a standard fine-grained entity typing dataset with multi-class, multi-label annotations. Ringland et al. (2019) developed a dataset for nested NER dataset. These datasets independently handle each label without considering label components. In Japanese NER, Misawa et al. (2017) combined word and character information to improve performance. Mai et al. (2018) reported that dictionary information improves the performance of finegrained NER. Their methods do not consider label components and are orthogonal to our method. Some existing studies take shared components (or information) across labels into account. In Entity Typing, Ma et al. (2016) and Shimaoka et al. (2017) proposed to calculate entity label embeddings by considering a label hierarchical structure. While their method is limited to only a hierarchical structure, our method can be applied to any set of compo"
2020.acl-srw.32,Q13-1032,0,0.0344833,"of the corresponding rubric. In the experiments, we use the sum of these analytic scores as a ground truth score of each response.2 Table 1 shows the statistics of the dataset. In the dataset, the randomly sampled 100 responses per prompt are annotated by two human raters. Therefore, we can calculate QWKs and their Kappa values (Cohen, 1960) between the two human raters to confirm the degree of human agreement. The Kappa values on this dataset are comparable to or higher than those on other datasets for the SAS task (Leacock and Chodorow, 2003; Mohler and Mihalcea, 2009; Mohler et al., 2011; Basu et al., 2013). As additional statistics, we calculated the number of CSEs and CSRate in various settings of λ in Settings We split the dataset into training data (1, 600), validation data (200), and test data (200). We used pretrained BERT (Devlin et al., 2019) as the embedding layer of the model.3 We adopted the same optimization algorithm, learning rate, batch size, and output dimension of the recurrent layer as in Taghipour and Ng (2016). We trained the SAS models for 50 epochs and selected the parameters in the epoch in which the best QWK was achieved for the development set. We trained five models wit"
2020.acl-srw.32,N19-1423,0,0.0186021,"Missing"
2020.acl-srw.32,W19-4433,1,0.929237,"given prompt on the basis of whether the answer satisfies the rubrics prepared by a human in advance. SAS systems have mainly been developed to markedly reduce the scoring cost of human raters. Moreover, the SAS systems play a central role in providing stable and sustainable scoring in a repeated and large-scale examination and (online) self-study learning support system (Attali and Burstein, 2006; Shermis et al., 2010; Leacock and Chodorow, 2003; Burrows et al., 2015). The development of the SAS systems has a long history (Page, 1994; Foltz et al., 1999). Many recent previous studies, e.g., (Mizumoto et al., 2019; Taghipour and Ng, 2016; Riordan et al., 2017; Wang et al., 2019), utilize Quadratic Weighted Kappa (QWK) (Cohen, 1968) as a measure for the achievement and for the comparison of the performances of the SAS systems. QWK is indeed useful for measuring and comparing the overall performance of each system and the daily developments of their scoring models. In our experiments, however, we reveal that the SAS systems with high QWK potentially incur serious scoring errors (see experiments in Section 5.3). Such serious scoring errors are rarely incurred by trained human raters, therefore, we need to"
2020.acl-srw.32,P11-1076,0,0.0265483,"vidually on the basis of the corresponding rubric. In the experiments, we use the sum of these analytic scores as a ground truth score of each response.2 Table 1 shows the statistics of the dataset. In the dataset, the randomly sampled 100 responses per prompt are annotated by two human raters. Therefore, we can calculate QWKs and their Kappa values (Cohen, 1960) between the two human raters to confirm the degree of human agreement. The Kappa values on this dataset are comparable to or higher than those on other datasets for the SAS task (Leacock and Chodorow, 2003; Mohler and Mihalcea, 2009; Mohler et al., 2011; Basu et al., 2013). As additional statistics, we calculated the number of CSEs and CSRate in various settings of λ in Settings We split the dataset into training data (1, 600), validation data (200), and test data (200). We used pretrained BERT (Devlin et al., 2019) as the embedding layer of the model.3 We adopted the same optimization algorithm, learning rate, batch size, and output dimension of the recurrent layer as in Taghipour and Ng (2016). We trained the SAS models for 50 epochs and selected the parameters in the epoch in which the best QWK was achieved for the development set. We tra"
2020.acl-srw.32,E09-1065,0,0.0201369,"ch criterion was rated individually on the basis of the corresponding rubric. In the experiments, we use the sum of these analytic scores as a ground truth score of each response.2 Table 1 shows the statistics of the dataset. In the dataset, the randomly sampled 100 responses per prompt are annotated by two human raters. Therefore, we can calculate QWKs and their Kappa values (Cohen, 1960) between the two human raters to confirm the degree of human agreement. The Kappa values on this dataset are comparable to or higher than those on other datasets for the SAS task (Leacock and Chodorow, 2003; Mohler and Mihalcea, 2009; Mohler et al., 2011; Basu et al., 2013). As additional statistics, we calculated the number of CSEs and CSRate in various settings of λ in Settings We split the dataset into training data (1, 600), validation data (200), and test data (200). We used pretrained BERT (Devlin et al., 2019) as the embedding layer of the model.3 We adopted the same optimization algorithm, learning rate, batch size, and output dimension of the recurrent layer as in Taghipour and Ng (2016). We trained the SAS models for 50 epochs and selected the parameters in the epoch in which the best QWK was achieved for the de"
2020.acl-srw.32,D15-1182,0,0.0207878,"Missing"
2020.acl-srw.32,W17-5017,0,0.0153614,"satisfies the rubrics prepared by a human in advance. SAS systems have mainly been developed to markedly reduce the scoring cost of human raters. Moreover, the SAS systems play a central role in providing stable and sustainable scoring in a repeated and large-scale examination and (online) self-study learning support system (Attali and Burstein, 2006; Shermis et al., 2010; Leacock and Chodorow, 2003; Burrows et al., 2015). The development of the SAS systems has a long history (Page, 1994; Foltz et al., 1999). Many recent previous studies, e.g., (Mizumoto et al., 2019; Taghipour and Ng, 2016; Riordan et al., 2017; Wang et al., 2019), utilize Quadratic Weighted Kappa (QWK) (Cohen, 1968) as a measure for the achievement and for the comparison of the performances of the SAS systems. QWK is indeed useful for measuring and comparing the overall performance of each system and the daily developments of their scoring models. In our experiments, however, we reveal that the SAS systems with high QWK potentially incur serious scoring errors (see experiments in Section 5.3). Such serious scoring errors are rarely incurred by trained human raters, therefore, we need to avoid containing this type of errors to ensur"
2020.acl-srw.32,D16-1193,0,0.0184529,"is of whether the answer satisfies the rubrics prepared by a human in advance. SAS systems have mainly been developed to markedly reduce the scoring cost of human raters. Moreover, the SAS systems play a central role in providing stable and sustainable scoring in a repeated and large-scale examination and (online) self-study learning support system (Attali and Burstein, 2006; Shermis et al., 2010; Leacock and Chodorow, 2003; Burrows et al., 2015). The development of the SAS systems has a long history (Page, 1994; Foltz et al., 1999). Many recent previous studies, e.g., (Mizumoto et al., 2019; Taghipour and Ng, 2016; Riordan et al., 2017; Wang et al., 2019), utilize Quadratic Weighted Kappa (QWK) (Cohen, 1968) as a measure for the achievement and for the comparison of the performances of the SAS systems. QWK is indeed useful for measuring and comparing the overall performance of each system and the daily developments of their scoring models. In our experiments, however, we reveal that the SAS systems with high QWK potentially incur serious scoring errors (see experiments in Section 5.3). Such serious scoring errors are rarely incurred by trained human raters, therefore, we need to avoid containing this t"
2020.acl-srw.32,D19-6119,1,0.835486,"prepared by a human in advance. SAS systems have mainly been developed to markedly reduce the scoring cost of human raters. Moreover, the SAS systems play a central role in providing stable and sustainable scoring in a repeated and large-scale examination and (online) self-study learning support system (Attali and Burstein, 2006; Shermis et al., 2010; Leacock and Chodorow, 2003; Burrows et al., 2015). The development of the SAS systems has a long history (Page, 1994; Foltz et al., 1999). Many recent previous studies, e.g., (Mizumoto et al., 2019; Taghipour and Ng, 2016; Riordan et al., 2017; Wang et al., 2019), utilize Quadratic Weighted Kappa (QWK) (Cohen, 1968) as a measure for the achievement and for the comparison of the performances of the SAS systems. QWK is indeed useful for measuring and comparing the overall performance of each system and the daily developments of their scoring models. In our experiments, however, we reveal that the SAS systems with high QWK potentially incur serious scoring errors (see experiments in Section 5.3). Such serious scoring errors are rarely incurred by trained human raters, therefore, we need to avoid containing this type of errors to ensure the sufficient sco"
2020.coling-main.160,1993.eamt-1.1,0,0.755864,"s study aims to estimate event salience in an unsupervised manner. Manually annotating event salience is prohibitively costly because it requires annotators to deeply understand the notion of event salience in narratology (Finlayson, 2015). In fact, despite a long history of research, very few narrative corpora are annotated with event salience. Thus, it is crucial to develop a method for estimating event salience that does not rely on event salience-annotated corpora. In order to estimate event salience without annotated data, we adopt the definition of cardinal functions (CFs) introduced by Barthes (1966; 1975), the successor of Proppian function 1 (Propp, 1928), as follows: cardinal functions are logically essential to the narrative action and cannot be eliminated without destroying its causal-chronological coherence. (Prince, 2003) This definition suggests a simple test for identifying event salience: an event is highly salient if removing it greatly reduces the story’s coherence. We adopt this idea for two reasons. First, CFs are commonly used in narrative analysis (Abbott, 2008). Second, the idea of CFs can be directly operationalized without any annotated data. Computing event salience b"
2020.coling-main.160,J08-1001,0,0.0613243,"definition suggests a simple test for identifying event salience: an event is highly salient if removing it greatly reduces the story’s coherence. We adopt this idea for two reasons. First, CFs are commonly used in narrative analysis (Abbott, 2008). Second, the idea of CFs can be directly operationalized without any annotated data. Computing event salience based on the idea of CFs requires measuring narrative texts’ coherence, but recent advances in discourse coherence models can provide a solution for this difficulty. To date, a wide variety of discourse coherence models have been proposed (Barzilay and Lapata, 2008; Li and Jurafsky, 2017). See et al. (2019) have reported that GPT-2 (Radford et al., 2019), a powerful left-to-right language model (LM), could accurately estimate narrative texts’ coherence, importantly, without any annotated data. Note that, in folkloristics and narratology, another well-known concept of ∗ Present affiliation: Stony Brook University. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1784 Proceedings of the 28th International Conference on Computational Linguistics, pages 1784–"
2020.coling-main.160,C18-1326,0,0.0212571,"hree functions r. 1. Sentence Deletion (SD): Removing the entire sentence 2. Verb Anonymization (VA): Replacing all verbs in the sentence with common verbs (e.g., “do”, “does”, “did”) based on the POS tags of each verb 2 3. Predicate and Arguments Anonymization (PAA): Replacing all verbs with common verbs (as in VA) and their main arguments with an indefinite pronoun (e.g., “someone”, “something”) 3 We employ VA and PAA because predicates and their arguments are main components of commonly used event representations (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Martin et al., 2018; Niklaus et al., 2018). Computing narratives’ coherence: c Following See et al. (2019), we compute the generation probability of a narrative using a pre-trained language model and regard it as the narrative’s coherence score. Importantly, pre-trained LMs allow us to evaluate narrative’s coherence without any annotated data. Here, the narrative’s generation probability is the product of word probabilities, which is influenced by the number of words in the narrative. Thus, following Li and Jurafsky (2017), we estimate the coherence score by the average log-likelihood of all tokens. Moreover, we consider only sentence"
2020.coling-main.160,W03-1204,0,0.109173,"nks a sentence based on its salience score. We used mean average precision (MAP) as an evaluation metric (Manning et al., 2008). We calculated the average precision for each story and reported their macro average score. 4.2 Experimental results Table 2 shows the experimental results. The results show all proposed methods consistently outperform the random baseline method, and the proposed method (SD, ProppLearner) yields the best performance. 4 5 We used transformers (Wolf et al., 2019) pre-trained model (12-layer, 768-hidden, 12-heads, 117M parameters). We used the score referred to as T3 in Nobata et al. (2003) 1787 Event removal methods We found SD performed comparably to or relatively better than VA and PAA 6 . We employed VA and PAA, aiming to remove event information from the sentence more elaborately than SD. However, experimental results show that these methods do not improve the proposed method. We suspect unnatural sentences produced by the operations in VA and PAA might negatively affect inference of the language model, indicating some room for improvement in how to remove events from a sentence. Effect of fine-tuning GPT-2 Fine-tuning GPT-2 on the BookCorpus slightly but consistently impro"
2020.coling-main.160,D19-1379,1,0.802917,"uage model (fine-tuning) We used GPT2 as a pre-trained language model for computing coherence scores4 . Note that See et al. (2019) reported that GPT-2 outperforms state-of-the-art story generation models in coherence evaluation. In Appendix A, we provide further evidence that GPT-2 can accurately evaluate the coherence of the narrative texts used in our experiments. Moreover, we consider three fine-tuning settings. 1. No fine-tuning 2. Fine-tuning GPT-2 on BookCorpus (Zhu et al., 2015) as domain adaptation. 3. Fine-tuning GPT-2 on ProppLearner as transductive domain adaptation (Vapnik, 1998; Ouchi et al., 2019). Baselines We compared the proposed methods with the following baseline methods: • Random baseline: This method assigns a random score in the range [0, 1) to each sentence. • Sentence position baseline (ascending): This method assigns a score based on the position of each sentence. Here, we assumed that a sentence closer to the story’s end has higher salience (Friedland and Allan, 2008). • Sentence position baseline (descending): This method assigns a score in the opposite way to sentence position baseline (ascending). • TF-IDF baseline: This method assigns the sum of the TF-IDF values5 of th"
2020.coling-main.160,D15-1257,0,0.0737266,"xts is a key factor in improving the proposed methods. 1 Introduction Narratives (e.g., folktales, literary short stories) are the representations of a series of events (Abbott, 2008). Events, the essential components of narratives, differ in salience: some are more important to the story than others. Taking Cinderella as an example, The prince falls in love with Cinderella is a salient event; however, Cinderella draws water from a well is not. Estimating event salience is a fundamental task in analyzing and processing narratives, ranging from narrative analysis to automatic story generation (Ouyang and McKeown, 2015; Choubey et al., 2018; Papalampidi et al., 2019). This study aims to estimate event salience in an unsupervised manner. Manually annotating event salience is prohibitively costly because it requires annotators to deeply understand the notion of event salience in narratology (Finlayson, 2015). In fact, despite a long history of research, very few narrative corpora are annotated with event salience. Thus, it is crucial to develop a method for estimating event salience that does not rely on event salience-annotated corpora. In order to estimate event salience without annotated data, we adopt the"
2020.coling-main.160,D19-1180,0,0.143314,"thods. 1 Introduction Narratives (e.g., folktales, literary short stories) are the representations of a series of events (Abbott, 2008). Events, the essential components of narratives, differ in salience: some are more important to the story than others. Taking Cinderella as an example, The prince falls in love with Cinderella is a salient event; however, Cinderella draws water from a well is not. Estimating event salience is a fundamental task in analyzing and processing narratives, ranging from narrative analysis to automatic story generation (Ouyang and McKeown, 2015; Choubey et al., 2018; Papalampidi et al., 2019). This study aims to estimate event salience in an unsupervised manner. Manually annotating event salience is prohibitively costly because it requires annotators to deeply understand the notion of event salience in narratology (Finlayson, 2015). In fact, despite a long history of research, very few narrative corpora are annotated with event salience. Thus, it is crucial to develop a method for estimating event salience that does not rely on event salience-annotated corpora. In order to estimate event salience without annotated data, we adopt the definition of cardinal functions (CFs) introduce"
2020.coling-main.160,K19-1079,0,0.241518,"event salience: an event is highly salient if removing it greatly reduces the story’s coherence. We adopt this idea for two reasons. First, CFs are commonly used in narrative analysis (Abbott, 2008). Second, the idea of CFs can be directly operationalized without any annotated data. Computing event salience based on the idea of CFs requires measuring narrative texts’ coherence, but recent advances in discourse coherence models can provide a solution for this difficulty. To date, a wide variety of discourse coherence models have been proposed (Barzilay and Lapata, 2008; Li and Jurafsky, 2017). See et al. (2019) have reported that GPT-2 (Radford et al., 2019), a powerful left-to-right language model (LM), could accurately estimate narrative texts’ coherence, importantly, without any annotated data. Note that, in folkloristics and narratology, another well-known concept of ∗ Present affiliation: Stony Brook University. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1784 Proceedings of the 28th International Conference on Computational Linguistics, pages 1784–1794 Barcelona, Spain (Online), December 8-"
2020.coling-main.160,2020.acl-main.161,0,0.0142095,"he task of our work. Despite various existing approaches for extractive summarization (Mani, 2001; Gambhir and Gupta, 2017), it is the open problem whether these methods can be directly applied to narrative texts. Extractive summarization conventionally focuses on domains with rigid structures, such as news articles or scientific papers, while narrative texts do not have such rigid structures (Kazantseva and Szpakowicz, 2010). In the context of narrative processing in NLP, several methods have been proposed to identify some kinds of salient events: suspenseful events in entertainment stories (Wilmot and Keller, 2020), turning points in a movie script (Papalampidi et al., 2019), and reportable events in personal narratives (Ouyang and McKeown, 2015). In contrast to studies focusing on a specific type of narrative (e.g., movie scripts, personal narratives), our method is potentially applicable to any type of narrative because Barthes’ CFs is not a concept specific to those particular kinds of narratives and because our methods require only a pre-trained language model. 3 3.1 Estimating event salience Task setup Original narrative: ?{?:?} &quot;{?:?} Event-removed narrative: ? Cinderella draws water from a well."
2020.coling-main.435,P15-2053,0,0.074614,"n understanding the predicate-argument structure of a sentence. This task is referred to as zero anaphora resolution (ZAR). Figure 1a depicts an example of the task. In the sentence, the nominative argument of the predicate was acquitted is omitted. Such an omitted argument is referred to as a zero pronoun often represented as φ. In Figure 1a, φ refers to the man. One critical issue of ZAR is the scarcity of labeled data. To compensate for the scarcity, several studies have explored the direction of exploiting unlabeled data (Sasano et al., 2008; Sasano and Kurohashi, 2011; Chen and Ng, 2014; Chen and Ng, 2015; Liu et al., 2017; Yamashiro et al., 2018; Kurita et al., 2018). Another promising direction is to automatically augment labeled data (data augmentation). One state-of-the-art data augmentation method is contextual data augmentation (CDA), which augments labeled data by replacing an arbitrary token(s) with another token(s) predicted to be plausible in the surrounding context by a pretrained language model (LM). CDA works well in several natural language processing (NLP) tasks, such as text classification (Kobayashi, 2018; Wu et al., 2019) and machine translation (Gao et al., 2019). However, u"
2020.coling-main.435,P16-1074,0,0.013654,"robabilities. We deepen our understanding of the change of antecedents through a detailed analysis on actual augmented data. In summary, our main contributions are as follows: • Two technical modifications to a standard CDA method; • Extensive empirical results indicating which type of tokens should (or should not) be the target for replacement; and • In-depth analysis on the change of antecedents of zero pronouns, suggesting a direction for future improvements. 2 Related Work ZAR has been studied in the Asian languages, such as Chinese (Converse, 2005; Zhao and Ng, 2007; Kong and Zhou, 2010; Chen and Ng, 2016; Yin et al., 2018), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2003; Iida et al., 2006; Iida et al., 2007; Iida et al., 2015; Iida et al., 2016), and Korean (Han, 2006), and Romance languages, such as Italian (Rodr´ıguez et al., 2010; Iida and Poesio, 2011) and Spanish (Ferr´andez and Peral, 2000; Palomar et al., 2001). While many recent studies 4957 adopted supervised learning methods,1 the amount of labeled data is not sufficient to teach a model about knowledge for accurately resolving anaphoric relations. Some studies tried extracting and exploiting such knowledge"
2020.coling-main.435,I05-3016,0,0.0118431,"nd replacement of each token type with various masking probabilities. We deepen our understanding of the change of antecedents through a detailed analysis on actual augmented data. In summary, our main contributions are as follows: • Two technical modifications to a standard CDA method; • Extensive empirical results indicating which type of tokens should (or should not) be the target for replacement; and • In-depth analysis on the change of antecedents of zero pronouns, suggesting a direction for future improvements. 2 Related Work ZAR has been studied in the Asian languages, such as Chinese (Converse, 2005; Zhao and Ng, 2007; Kong and Zhou, 2010; Chen and Ng, 2016; Yin et al., 2018), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2003; Iida et al., 2006; Iida et al., 2007; Iida et al., 2015; Iida et al., 2016), and Korean (Han, 2006), and Romance languages, such as Italian (Rodr´ıguez et al., 2010; Iida and Poesio, 2011) and Spanish (Ferr´andez and Peral, 2000; Palomar et al., 2001). While many recent studies 4957 adopted supervised learning methods,1 the amount of labeled data is not sufficient to teach a model about knowledge for accurately resolving anaphoric relations. S"
2020.coling-main.435,W17-3203,0,0.0455832,"Missing"
2020.coling-main.435,N19-1423,0,0.496159,"data generation (Liu et al., 2017), and semi-supervised adversarial training (Kurita et al., 2018)). Although findings on using unlabeled data for ZAR have been accumulated, the automatic augmentation of labeled data has been underinvestigated. One reason for this is the difficulty in producing high-quality pseudo data from its original labeled data. Some existing methods of text data augmentation take advantage of hand-crafted resources (Zhang et al., 2015; Wang and Yang, 2015) and rules (F¨urstenau and Lapata, 2009; Kafle et al., 2017). With the recent advances of LMs (Peters et al., 2018; Devlin et al., 2019), CDA, which is a new method using LMs, has been proposed and reported to effectively increase labeled data in several NLP tasks (Kobayashi, 2018; Wu et al., 2019; Gao et al., 2019). This method offers a variety of good substitutes of original tokens without any hand-crafted resources. In ZAR, a key to performance improvement is training a model on anaphoric relations in various contexts. However, the original labeled data over a limited portion of contextual diversity. Here, CDA is a promising tool for producing good contextual variations of each original sentence. Considering this advantage,"
2020.coling-main.435,P00-1022,0,0.138897,"Missing"
2020.coling-main.435,E09-1026,0,0.0603316,"Missing"
2020.coling-main.435,P19-1555,0,0.178272,".jp y.m@tohoku.ac.jp {shun.kiyono, hiroki.ouchi}@riken.jp Abstract One critical issue of zero anaphora resolution (ZAR) is the scarcity of labeled data. This study explores how effectively this problem can be alleviated by data augmentation. We adopt a state-ofthe-art data augmentation method, called the contextual data augmentation (CDA), that generates labeled training instances using a pretrained language model. The CDA has been reported to work well for several other natural language processing tasks, including text classification and machine translation (Kobayashi, 2018; Wu et al., 2019; Gao et al., 2019). This study addresses two underexplored issues on CDA, that is, how to reduce the computational cost of data augmentation and how to ensure the quality of the generated data. We also propose two methods to adapt CDA to ZAR: [MASK]-based augmentation and linguistically-controlled masking. Consequently, the experimental results on Japanese ZAR show that our methods contribute to both the accuracy gain and the computation cost reduction. Our closer analysis reveals that the proposed method can improve the quality of the augmented training data when compared to the conventional CDA. 1 Introductio"
2020.coling-main.435,P11-1081,0,0.0125441,"h type of tokens should (or should not) be the target for replacement; and • In-depth analysis on the change of antecedents of zero pronouns, suggesting a direction for future improvements. 2 Related Work ZAR has been studied in the Asian languages, such as Chinese (Converse, 2005; Zhao and Ng, 2007; Kong and Zhou, 2010; Chen and Ng, 2016; Yin et al., 2018), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2003; Iida et al., 2006; Iida et al., 2007; Iida et al., 2015; Iida et al., 2016), and Korean (Han, 2006), and Romance languages, such as Italian (Rodr´ıguez et al., 2010; Iida and Poesio, 2011) and Spanish (Ferr´andez and Peral, 2000; Palomar et al., 2001). While many recent studies 4957 adopted supervised learning methods,1 the amount of labeled data is not sufficient to teach a model about knowledge for accurately resolving anaphoric relations. Some studies tried extracting and exploiting such knowledge from large-scale unlabeled data (e.g., case frame construction (Sasano et al., 2008; Sasano and Kurohashi, 2011; Yamashiro et al., 2018), pseudo training data generation (Liu et al., 2017), and semi-supervised adversarial training (Kurita et al., 2018)). Although findings on using"
2020.coling-main.435,W03-2604,1,0.620827,"alysis on actual augmented data. In summary, our main contributions are as follows: • Two technical modifications to a standard CDA method; • Extensive empirical results indicating which type of tokens should (or should not) be the target for replacement; and • In-depth analysis on the change of antecedents of zero pronouns, suggesting a direction for future improvements. 2 Related Work ZAR has been studied in the Asian languages, such as Chinese (Converse, 2005; Zhao and Ng, 2007; Kong and Zhou, 2010; Chen and Ng, 2016; Yin et al., 2018), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2003; Iida et al., 2006; Iida et al., 2007; Iida et al., 2015; Iida et al., 2016), and Korean (Han, 2006), and Romance languages, such as Italian (Rodr´ıguez et al., 2010; Iida and Poesio, 2011) and Spanish (Ferr´andez and Peral, 2000; Palomar et al., 2001). While many recent studies 4957 adopted supervised learning methods,1 the amount of labeled data is not sufficient to teach a model about knowledge for accurately resolving anaphoric relations. Some studies tried extracting and exploiting such knowledge from large-scale unlabeled data (e.g., case frame construction (Sasano et al., 2008; Sasano"
2020.coling-main.435,P06-1079,1,0.643088,"gmented data. In summary, our main contributions are as follows: • Two technical modifications to a standard CDA method; • Extensive empirical results indicating which type of tokens should (or should not) be the target for replacement; and • In-depth analysis on the change of antecedents of zero pronouns, suggesting a direction for future improvements. 2 Related Work ZAR has been studied in the Asian languages, such as Chinese (Converse, 2005; Zhao and Ng, 2007; Kong and Zhou, 2010; Chen and Ng, 2016; Yin et al., 2018), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2003; Iida et al., 2006; Iida et al., 2007; Iida et al., 2015; Iida et al., 2016), and Korean (Han, 2006), and Romance languages, such as Italian (Rodr´ıguez et al., 2010; Iida and Poesio, 2011) and Spanish (Ferr´andez and Peral, 2000; Palomar et al., 2001). While many recent studies 4957 adopted supervised learning methods,1 the amount of labeled data is not sufficient to teach a model about knowledge for accurately resolving anaphoric relations. Some studies tried extracting and exploiting such knowledge from large-scale unlabeled data (e.g., case frame construction (Sasano et al., 2008; Sasano and Kurohashi, 2011"
2020.coling-main.435,D15-1260,0,0.0456267,"tributions are as follows: • Two technical modifications to a standard CDA method; • Extensive empirical results indicating which type of tokens should (or should not) be the target for replacement; and • In-depth analysis on the change of antecedents of zero pronouns, suggesting a direction for future improvements. 2 Related Work ZAR has been studied in the Asian languages, such as Chinese (Converse, 2005; Zhao and Ng, 2007; Kong and Zhou, 2010; Chen and Ng, 2016; Yin et al., 2018), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2003; Iida et al., 2006; Iida et al., 2007; Iida et al., 2015; Iida et al., 2016), and Korean (Han, 2006), and Romance languages, such as Italian (Rodr´ıguez et al., 2010; Iida and Poesio, 2011) and Spanish (Ferr´andez and Peral, 2000; Palomar et al., 2001). While many recent studies 4957 adopted supervised learning methods,1 the amount of labeled data is not sufficient to teach a model about knowledge for accurately resolving anaphoric relations. Some studies tried extracting and exploiting such knowledge from large-scale unlabeled data (e.g., case frame construction (Sasano et al., 2008; Sasano and Kurohashi, 2011; Yamashiro et al., 2018), pseudo trai"
2020.coling-main.435,D16-1132,0,0.0130488,"ollows: • Two technical modifications to a standard CDA method; • Extensive empirical results indicating which type of tokens should (or should not) be the target for replacement; and • In-depth analysis on the change of antecedents of zero pronouns, suggesting a direction for future improvements. 2 Related Work ZAR has been studied in the Asian languages, such as Chinese (Converse, 2005; Zhao and Ng, 2007; Kong and Zhou, 2010; Chen and Ng, 2016; Yin et al., 2018), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2003; Iida et al., 2006; Iida et al., 2007; Iida et al., 2015; Iida et al., 2016), and Korean (Han, 2006), and Romance languages, such as Italian (Rodr´ıguez et al., 2010; Iida and Poesio, 2011) and Spanish (Ferr´andez and Peral, 2000; Palomar et al., 2001). While many recent studies 4957 adopted supervised learning methods,1 the amount of labeled data is not sufficient to teach a model about knowledge for accurately resolving anaphoric relations. Some studies tried extracting and exploiting such knowledge from large-scale unlabeled data (e.g., case frame construction (Sasano et al., 2008; Sasano and Kurohashi, 2011; Yamashiro et al., 2018), pseudo training data generation"
2020.coling-main.435,W03-1024,0,0.130254,"nts through a detailed analysis on actual augmented data. In summary, our main contributions are as follows: • Two technical modifications to a standard CDA method; • Extensive empirical results indicating which type of tokens should (or should not) be the target for replacement; and • In-depth analysis on the change of antecedents of zero pronouns, suggesting a direction for future improvements. 2 Related Work ZAR has been studied in the Asian languages, such as Chinese (Converse, 2005; Zhao and Ng, 2007; Kong and Zhou, 2010; Chen and Ng, 2016; Yin et al., 2018), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2003; Iida et al., 2006; Iida et al., 2007; Iida et al., 2015; Iida et al., 2016), and Korean (Han, 2006), and Romance languages, such as Italian (Rodr´ıguez et al., 2010; Iida and Poesio, 2011) and Spanish (Ferr´andez and Peral, 2000; Palomar et al., 2001). While many recent studies 4957 adopted supervised learning methods,1 the amount of labeled data is not sufficient to teach a model about knowledge for accurately resolving anaphoric relations. Some studies tried extracting and exploiting such knowledge from large-scale unlabeled data (e.g., case frame construction (Sasano et"
2020.coling-main.435,W17-3529,0,0.0131256,"2008; Sasano and Kurohashi, 2011; Yamashiro et al., 2018), pseudo training data generation (Liu et al., 2017), and semi-supervised adversarial training (Kurita et al., 2018)). Although findings on using unlabeled data for ZAR have been accumulated, the automatic augmentation of labeled data has been underinvestigated. One reason for this is the difficulty in producing high-quality pseudo data from its original labeled data. Some existing methods of text data augmentation take advantage of hand-crafted resources (Zhang et al., 2015; Wang and Yang, 2015) and rules (F¨urstenau and Lapata, 2009; Kafle et al., 2017). With the recent advances of LMs (Peters et al., 2018; Devlin et al., 2019), CDA, which is a new method using LMs, has been proposed and reported to effectively increase labeled data in several NLP tasks (Kobayashi, 2018; Wu et al., 2019; Gao et al., 2019). This method offers a variety of good substitutes of original tokens without any hand-crafted resources. In ZAR, a key to performance improvement is training a model on anaphoric relations in various contexts. However, the original labeled data over a limited portion of contextual diversity. Here, CDA is a promising tool for producing good"
2020.coling-main.435,N18-2072,0,0.310827,"ryuto, ryo.t, inui}@ecei.tohoku.ac.jp y.m@tohoku.ac.jp {shun.kiyono, hiroki.ouchi}@riken.jp Abstract One critical issue of zero anaphora resolution (ZAR) is the scarcity of labeled data. This study explores how effectively this problem can be alleviated by data augmentation. We adopt a state-ofthe-art data augmentation method, called the contextual data augmentation (CDA), that generates labeled training instances using a pretrained language model. The CDA has been reported to work well for several other natural language processing tasks, including text classification and machine translation (Kobayashi, 2018; Wu et al., 2019; Gao et al., 2019). This study addresses two underexplored issues on CDA, that is, how to reduce the computational cost of data augmentation and how to ensure the quality of the generated data. We also propose two methods to adapt CDA to ZAR: [MASK]-based augmentation and linguistically-controlled masking. Consequently, the experimental results on Japanese ZAR show that our methods contribute to both the accuracy gain and the computation cost reduction. Our closer analysis reveals that the proposed method can improve the quality of the augmented training data when compared to"
2020.coling-main.435,D10-1086,0,0.0191121,"ith various masking probabilities. We deepen our understanding of the change of antecedents through a detailed analysis on actual augmented data. In summary, our main contributions are as follows: • Two technical modifications to a standard CDA method; • Extensive empirical results indicating which type of tokens should (or should not) be the target for replacement; and • In-depth analysis on the change of antecedents of zero pronouns, suggesting a direction for future improvements. 2 Related Work ZAR has been studied in the Asian languages, such as Chinese (Converse, 2005; Zhao and Ng, 2007; Kong and Zhou, 2010; Chen and Ng, 2016; Yin et al., 2018), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2003; Iida et al., 2006; Iida et al., 2007; Iida et al., 2015; Iida et al., 2016), and Korean (Han, 2006), and Romance languages, such as Italian (Rodr´ıguez et al., 2010; Iida and Poesio, 2011) and Spanish (Ferr´andez and Peral, 2000; Palomar et al., 2001). While many recent studies 4957 adopted supervised learning methods,1 the amount of labeled data is not sufficient to teach a model about knowledge for accurately resolving anaphoric relations. Some studies tried extracting and exploit"
2020.coling-main.435,P18-1044,0,0.0843007,"e. This task is referred to as zero anaphora resolution (ZAR). Figure 1a depicts an example of the task. In the sentence, the nominative argument of the predicate was acquitted is omitted. Such an omitted argument is referred to as a zero pronoun often represented as φ. In Figure 1a, φ refers to the man. One critical issue of ZAR is the scarcity of labeled data. To compensate for the scarcity, several studies have explored the direction of exploiting unlabeled data (Sasano et al., 2008; Sasano and Kurohashi, 2011; Chen and Ng, 2014; Chen and Ng, 2015; Liu et al., 2017; Yamashiro et al., 2018; Kurita et al., 2018). Another promising direction is to automatically augment labeled data (data augmentation). One state-of-the-art data augmentation method is contextual data augmentation (CDA), which augments labeled data by replacing an arbitrary token(s) with another token(s) predicted to be plausible in the surrounding context by a pretrained language model (LM). CDA works well in several natural language processing (NLP) tasks, such as text classification (Kobayashi, 2018; Wu et al., 2019) and machine translation (Gao et al., 2019). However, unlike the direction of exploiting unlabeled data, the potential"
2020.coling-main.435,P17-1010,0,0.0653346,"predicate-argument structure of a sentence. This task is referred to as zero anaphora resolution (ZAR). Figure 1a depicts an example of the task. In the sentence, the nominative argument of the predicate was acquitted is omitted. Such an omitted argument is referred to as a zero pronoun often represented as φ. In Figure 1a, φ refers to the man. One critical issue of ZAR is the scarcity of labeled data. To compensate for the scarcity, several studies have explored the direction of exploiting unlabeled data (Sasano et al., 2008; Sasano and Kurohashi, 2011; Chen and Ng, 2014; Chen and Ng, 2015; Liu et al., 2017; Yamashiro et al., 2018; Kurita et al., 2018). Another promising direction is to automatically augment labeled data (data augmentation). One state-of-the-art data augmentation method is contextual data augmentation (CDA), which augments labeled data by replacing an arbitrary token(s) with another token(s) predicted to be plausible in the surrounding context by a pretrained language model (LM). CDA works well in several natural language processing (NLP) tasks, such as text classification (Kobayashi, 2018; Wu et al., 2019) and machine translation (Gao et al., 2019). However, unlike the directio"
2020.coling-main.435,I17-2022,1,0.851764,"ure exploration of this direction. 3 Japanese Zero Anaphora Resolution 3.1 Problem Formulation and Notation Japanese ZAR is often formulated as a part of the predicate-argument structure (PAS) analysis, which includes the task of detecting syntactically depending arguments (DEPs), in addition to ZAR. Given a sentence X and a list of predicates p, the Japanese PAS analysis task aims to identify the head words of nominative (NOM), accusative (ACC), and dative (DAT) arguments for each predicate. Our task definition strictly follows that of previous studies (Iida et al., 2015; Ouchi et al., 2017; Matsubayashi and Inui, 2017; Matsubayashi and Inui, 2018), making our results comparable with those published previously. In the following sections we consider that the input sentence X = (x1 , . . . , xI ) consists of a sequence of one-hot vectors xi ∈ {0, 1}|V |, each of which represents a word in the sentence. V denotes a vocabulary set. Each sentence comprises J predicates p = (p1 , . . . , pJ ), where pj ∈ N is a natural number indicating a predicate position. 3.2 Baseline Model The baseline model employed in our experiments (hereinafter referred to as MP-masked language model (MP-MLM)) is based on the multi-predic"
2020.coling-main.435,C18-1009,1,0.89101,"tion. 3 Japanese Zero Anaphora Resolution 3.1 Problem Formulation and Notation Japanese ZAR is often formulated as a part of the predicate-argument structure (PAS) analysis, which includes the task of detecting syntactically depending arguments (DEPs), in addition to ZAR. Given a sentence X and a list of predicates p, the Japanese PAS analysis task aims to identify the head words of nominative (NOM), accusative (ACC), and dative (DAT) arguments for each predicate. Our task definition strictly follows that of previous studies (Iida et al., 2015; Ouchi et al., 2017; Matsubayashi and Inui, 2017; Matsubayashi and Inui, 2018), making our results comparable with those published previously. In the following sections we consider that the input sentence X = (x1 , . . . , xI ) consists of a sequence of one-hot vectors xi ∈ {0, 1}|V |, each of which represents a word in the sentence. V denotes a vocabulary set. Each sentence comprises J predicates p = (p1 , . . . , pJ ), where pj ∈ N is a natural number indicating a predicate position. 3.2 Baseline Model The baseline model employed in our experiments (hereinafter referred to as MP-masked language model (MP-MLM)) is based on the multi-predicate (MP) model proposed by Mat"
2020.coling-main.435,N19-1344,0,0.0108544,"ly 4961 BASELINE BASELINE (2 X ) CDA M ASKING ZAR F1 DEP F1 64.08 63.55 64.30 64.89 92.82 92.75 92.79 92.94 ZAR F1 Method 65.00 64.75 64.50 64.25 64.00 63.75 63.50 63.25 ALL F1 87.43 ±0.14 87.24 ±0.17 87.42 ±0.18 87.64 ±0.09 Baseline allverb allsymbol 0.2 Table 4: F1 scores on the NTC 1.5 validation set. Bold values indicate best results in the same column. 0.4 allparticle all
oun 0.6 all 0.8 1.0 Rate of masked tokens to all tokens in training data Figure 3: The effect of changing masking probability α on ZAR F1 adopted by previous studies (Ouchi et al., 2017; Matsubayashi and Inui, 2018; Omori and Komachi, 2019). Table 1 presents the number of instances in NTC 1.5, and Table 2 presents the number of tokens whose POS tag appears in the NTC 1.5 training set. We used both the modified sentence X 0 and the original sentence X in a 1:1 ratio for training. Each model was trained using 10 different random seeds. The POS tags from the NTC gold standard tags were used in the experiments. The arg max function was employed as F in Equation (11), which denotes the function for selecting the most probable word from the probability distribution. The average F1 -scores were reported for both ZAR and DEP.3 The BERT"
2020.coling-main.435,P17-1146,1,0.922205,"to encourage the future exploration of this direction. 3 Japanese Zero Anaphora Resolution 3.1 Problem Formulation and Notation Japanese ZAR is often formulated as a part of the predicate-argument structure (PAS) analysis, which includes the task of detecting syntactically depending arguments (DEPs), in addition to ZAR. Given a sentence X and a list of predicates p, the Japanese PAS analysis task aims to identify the head words of nominative (NOM), accusative (ACC), and dative (DAT) arguments for each predicate. Our task definition strictly follows that of previous studies (Iida et al., 2015; Ouchi et al., 2017; Matsubayashi and Inui, 2017; Matsubayashi and Inui, 2018), making our results comparable with those published previously. In the following sections we consider that the input sentence X = (x1 , . . . , xI ) consists of a sequence of one-hot vectors xi ∈ {0, 1}|V |, each of which represents a word in the sentence. V denotes a vocabulary set. Each sentence comprises J predicates p = (p1 , . . . , pJ ), where pj ∈ N is a natural number indicating a predicate position. 3.2 Baseline Model The baseline model employed in our experiments (hereinafter referred to as MP-masked language model (MP-MLM))"
2020.coling-main.435,J01-4005,0,0.0495747,"Missing"
2020.coling-main.435,N18-1202,0,0.0181123,"018), pseudo training data generation (Liu et al., 2017), and semi-supervised adversarial training (Kurita et al., 2018)). Although findings on using unlabeled data for ZAR have been accumulated, the automatic augmentation of labeled data has been underinvestigated. One reason for this is the difficulty in producing high-quality pseudo data from its original labeled data. Some existing methods of text data augmentation take advantage of hand-crafted resources (Zhang et al., 2015; Wang and Yang, 2015) and rules (F¨urstenau and Lapata, 2009; Kafle et al., 2017). With the recent advances of LMs (Peters et al., 2018; Devlin et al., 2019), CDA, which is a new method using LMs, has been proposed and reported to effectively increase labeled data in several NLP tasks (Kobayashi, 2018; Wu et al., 2019; Gao et al., 2019). This method offers a variety of good substitutes of original tokens without any hand-crafted resources. In ZAR, a key to performance improvement is training a model on anaphoric relations in various contexts. However, the original labeled data over a limited portion of contextual diversity. Here, CDA is a promising tool for producing good contextual variations of each original sentence. Consi"
2020.coling-main.435,I11-1085,0,0.119926,"uch omitted arguments plays an important role in understanding the predicate-argument structure of a sentence. This task is referred to as zero anaphora resolution (ZAR). Figure 1a depicts an example of the task. In the sentence, the nominative argument of the predicate was acquitted is omitted. Such an omitted argument is referred to as a zero pronoun often represented as φ. In Figure 1a, φ refers to the man. One critical issue of ZAR is the scarcity of labeled data. To compensate for the scarcity, several studies have explored the direction of exploiting unlabeled data (Sasano et al., 2008; Sasano and Kurohashi, 2011; Chen and Ng, 2014; Chen and Ng, 2015; Liu et al., 2017; Yamashiro et al., 2018; Kurita et al., 2018). Another promising direction is to automatically augment labeled data (data augmentation). One state-of-the-art data augmentation method is contextual data augmentation (CDA), which augments labeled data by replacing an arbitrary token(s) with another token(s) predicted to be plausible in the surrounding context by a pretrained language model (LM). CDA works well in several natural language processing (NLP) tasks, such as text classification (Kobayashi, 2018; Wu et al., 2019) and machine tran"
2020.coling-main.435,C08-1097,0,0.130918,"atic recognition of such omitted arguments plays an important role in understanding the predicate-argument structure of a sentence. This task is referred to as zero anaphora resolution (ZAR). Figure 1a depicts an example of the task. In the sentence, the nominative argument of the predicate was acquitted is omitted. Such an omitted argument is referred to as a zero pronoun often represented as φ. In Figure 1a, φ refers to the man. One critical issue of ZAR is the scarcity of labeled data. To compensate for the scarcity, several studies have explored the direction of exploiting unlabeled data (Sasano et al., 2008; Sasano and Kurohashi, 2011; Chen and Ng, 2014; Chen and Ng, 2015; Liu et al., 2017; Yamashiro et al., 2018; Kurita et al., 2018). Another promising direction is to automatically augment labeled data (data augmentation). One state-of-the-art data augmentation method is contextual data augmentation (CDA), which augments labeled data by replacing an arbitrary token(s) with another token(s) predicted to be plausible in the surrounding context by a pretrained language model (LM). CDA works well in several natural language processing (NLP) tasks, such as text classification (Kobayashi, 2018; Wu et"
2020.coling-main.435,C02-1078,0,0.0658891,"change of antecedents through a detailed analysis on actual augmented data. In summary, our main contributions are as follows: • Two technical modifications to a standard CDA method; • Extensive empirical results indicating which type of tokens should (or should not) be the target for replacement; and • In-depth analysis on the change of antecedents of zero pronouns, suggesting a direction for future improvements. 2 Related Work ZAR has been studied in the Asian languages, such as Chinese (Converse, 2005; Zhao and Ng, 2007; Kong and Zhou, 2010; Chen and Ng, 2016; Yin et al., 2018), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2003; Iida et al., 2006; Iida et al., 2007; Iida et al., 2015; Iida et al., 2016), and Korean (Han, 2006), and Romance languages, such as Italian (Rodr´ıguez et al., 2010; Iida and Poesio, 2011) and Spanish (Ferr´andez and Peral, 2000; Palomar et al., 2001). While many recent studies 4957 adopted supervised learning methods,1 the amount of labeled data is not sufficient to teach a model about knowledge for accurately resolving anaphoric relations. Some studies tried extracting and exploiting such knowledge from large-scale unlabeled data (e.g., case fram"
2020.coling-main.435,P18-2097,0,0.0245657,"Missing"
2020.coling-main.435,D08-1055,0,0.0374468,"Missing"
2020.coling-main.435,W19-2304,0,0.0607581,"Missing"
2020.coling-main.435,D15-1306,0,0.0257145,"unlabeled data (e.g., case frame construction (Sasano et al., 2008; Sasano and Kurohashi, 2011; Yamashiro et al., 2018), pseudo training data generation (Liu et al., 2017), and semi-supervised adversarial training (Kurita et al., 2018)). Although findings on using unlabeled data for ZAR have been accumulated, the automatic augmentation of labeled data has been underinvestigated. One reason for this is the difficulty in producing high-quality pseudo data from its original labeled data. Some existing methods of text data augmentation take advantage of hand-crafted resources (Zhang et al., 2015; Wang and Yang, 2015) and rules (F¨urstenau and Lapata, 2009; Kafle et al., 2017). With the recent advances of LMs (Peters et al., 2018; Devlin et al., 2019), CDA, which is a new method using LMs, has been proposed and reported to effectively increase labeled data in several NLP tasks (Kobayashi, 2018; Wu et al., 2019; Gao et al., 2019). This method offers a variety of good substitutes of original tokens without any hand-crafted resources. In ZAR, a key to performance improvement is training a model on anaphoric relations in various contexts. However, the original labeled data over a limited portion of contextual"
2020.coling-main.435,Y18-1089,0,0.0554788,"t structure of a sentence. This task is referred to as zero anaphora resolution (ZAR). Figure 1a depicts an example of the task. In the sentence, the nominative argument of the predicate was acquitted is omitted. Such an omitted argument is referred to as a zero pronoun often represented as φ. In Figure 1a, φ refers to the man. One critical issue of ZAR is the scarcity of labeled data. To compensate for the scarcity, several studies have explored the direction of exploiting unlabeled data (Sasano et al., 2008; Sasano and Kurohashi, 2011; Chen and Ng, 2014; Chen and Ng, 2015; Liu et al., 2017; Yamashiro et al., 2018; Kurita et al., 2018). Another promising direction is to automatically augment labeled data (data augmentation). One state-of-the-art data augmentation method is contextual data augmentation (CDA), which augments labeled data by replacing an arbitrary token(s) with another token(s) predicted to be plausible in the surrounding context by a pretrained language model (LM). CDA works well in several natural language processing (NLP) tasks, such as text classification (Kobayashi, 2018; Wu et al., 2019) and machine translation (Gao et al., 2019). However, unlike the direction of exploiting unlabele"
2020.coling-main.435,C18-1002,0,0.0164597,"epen our understanding of the change of antecedents through a detailed analysis on actual augmented data. In summary, our main contributions are as follows: • Two technical modifications to a standard CDA method; • Extensive empirical results indicating which type of tokens should (or should not) be the target for replacement; and • In-depth analysis on the change of antecedents of zero pronouns, suggesting a direction for future improvements. 2 Related Work ZAR has been studied in the Asian languages, such as Chinese (Converse, 2005; Zhao and Ng, 2007; Kong and Zhou, 2010; Chen and Ng, 2016; Yin et al., 2018), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2003; Iida et al., 2006; Iida et al., 2007; Iida et al., 2015; Iida et al., 2016), and Korean (Han, 2006), and Romance languages, such as Italian (Rodr´ıguez et al., 2010; Iida and Poesio, 2011) and Spanish (Ferr´andez and Peral, 2000; Palomar et al., 2001). While many recent studies 4957 adopted supervised learning methods,1 the amount of labeled data is not sufficient to teach a model about knowledge for accurately resolving anaphoric relations. Some studies tried extracting and exploiting such knowledge from large-scale un"
2020.coling-main.435,D07-1057,0,0.0201904,"f each token type with various masking probabilities. We deepen our understanding of the change of antecedents through a detailed analysis on actual augmented data. In summary, our main contributions are as follows: • Two technical modifications to a standard CDA method; • Extensive empirical results indicating which type of tokens should (or should not) be the target for replacement; and • In-depth analysis on the change of antecedents of zero pronouns, suggesting a direction for future improvements. 2 Related Work ZAR has been studied in the Asian languages, such as Chinese (Converse, 2005; Zhao and Ng, 2007; Kong and Zhou, 2010; Chen and Ng, 2016; Yin et al., 2018), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2003; Iida et al., 2006; Iida et al., 2007; Iida et al., 2015; Iida et al., 2016), and Korean (Han, 2006), and Romance languages, such as Italian (Rodr´ıguez et al., 2010; Iida and Poesio, 2011) and Spanish (Ferr´andez and Peral, 2000; Palomar et al., 2001). While many recent studies 4957 adopted supervised learning methods,1 the amount of labeled data is not sufficient to teach a model about knowledge for accurately resolving anaphoric relations. Some studies tried e"
2020.coling-main.435,P15-1109,0,0.0195255,", where btarget represents whether or not the word is the target predicate, and bothers represents i i i whether or not the word is the predicate. This concatenation operation is presented as follows: h0i,j = ei ⊕ btarget ⊕ bothers , i i btarget i ( 1 (if i = pj ) = 0 (otherwise), bothers i (3) (2) ( 1 (if i ∈ p) = 0 (otherwise), (4) where ⊕ denotes the concatenation operation, and h0i,j ∈ RD+2 denotes the output of this concatenation operation. The obtained vectors are then fed into the k-layer bi-directional RNN (BiRNN) with residual connections (He et al., 2016) and alternating directions (Zhou and Xu, 2015) as follows: h1i,j =RNN1 (h0i,j , h1i−1,j ), hki,j ( k−1 k hi,j + RNNk (hk−1 i,j , hi−1,j ) (k is odd) = (k ≥ 2), k−1 k hi,j + RNNk (hk−1 i,j , hi+1,j ) (k is even) (5) (6) where hki,j ∈ RM denotes the output of the kth RNN layer, and RNNk denotes the function representing the kth RNN layer. Gated recurrent units (Cho et al., 2014) are used for RNN cells. A four-dimensional vector representing a probability distribution P (yi,j |X, p, i, j) ∈ R4 is then obtained as follows by M applying a softmax layer to the output hK i,j ∈ R : P (yi,j |X, p, i, j) = softmax(W hK i,j ), (7) where W ∈ R4×M is"
2020.coling-main.521,N19-1311,0,0.0177356,"en if the model ranked two references correctly. Similar but different way of contrastive evaluation is performed on a clean input and its noisy counterpart. Heigold et al. (2018) introduced rule-based character replacement noise to imitate misspellings found in a variety of real-world applications. Following work by Karpukhin et al. (2019) and Belinkov and Bisk (2018) extended its scope to natural noise by using edit histories from online websites. However, instead of giving translations to raw noisy sentences, they relied on a noisy version of input artificially created from the clean text. Anastasopoulos et al. (2019) is similar to our work in that they explored the effect of errors naturally created by humans. They focused on the effect of grammatical errors against NMT by adding translations to the JFLEG corpus (Napoles et al., 2017), one of the common benchmarks for grammatical error correction. Their results demonstrated that even a very small perturbation could significantly drop the performance of MT systems while exposure to similar noise during training time alleviates the problem. However, these aspects are only a small subset of possible reasons to explain why current models are still not good at"
2020.coling-main.521,N18-1118,0,0.0712839,"airs generated during the process of creating corpora, not at handling noisy input. The way of current evaluation definitely prevents us from developing truly robust systems, and motivated us to create a new dataset for focused evaluation. A range of studies have aimed to elucidate the cause of mistranslations from the viewpoint of linguistic phenomena, such as typographical errors (Heigold et al., 2018; Belinkov and Bisk, 2018; Karpukhin et al., 2019; Niu et al., 2020), grammaticality (Sennrich, 2017), presence of named entities (Ugawa et al., 2018), and identification of anaphoric pronouns (Bawden et al., 2018; M¨uller et al., 2018). One of the pioneering works to analyze the behavior of NMT is the challenge set approach proposed by Isabelle et al. (2017). They defined various subcategories of structural differences between English and French to evaluate how well models can handle them in detail. Though the approach has the potential of 1 2 https://www.reddit.com http://www.statmt.org/wmt19/robustness.html 5930 Step1: Annotating phenomena labels Ja En ��� Ja Phe.1 En Phe.2 Step2: Extracting targeted expressions MTNT corpus Ja: ������ (apude, update) �� En: That’s a plain update though Orig. Norm. E"
2020.coling-main.521,D19-5617,0,0.0200286,"re greatly disturbed by the presence of certain phenomena. 1 Introduction The advancement of Neural Machine Translation (NMT) has brought great improvement in translation quality when translating clean input, such as text from the news domain (Luong et al., 2015; Vaswani et al., 2017), and it was recently claimed that NMT has even achieved human parity in certain language pairs (Hassan et al., 2018; Barrault et al., 2019). Despite its remarkable advancements, the applicability of NMT over User-Generated Contents (UGC), such as social media text, still remains limited (Michel and Neubig, 2018; Berard et al., 2019a). Since UGC are prevailing in our real-life communication, it is undoubtedly one of the challenges we need to overcome to make MT systems invaluable for promoting cross-cultural communication. Recently, with the increasing interest in handling UGC, a shared task was organized to measure how well MT systems adapt to those texts (Li et al., 2019). However, the way in which they evaluate systems is just giving an overall score to a dataset, which is the same as traditional MT evaluation (Figure 1a). The overall score does not provide precise information for understanding what leads to the huge"
2020.coling-main.521,W19-5361,0,0.0167496,"re greatly disturbed by the presence of certain phenomena. 1 Introduction The advancement of Neural Machine Translation (NMT) has brought great improvement in translation quality when translating clean input, such as text from the news domain (Luong et al., 2015; Vaswani et al., 2017), and it was recently claimed that NMT has even achieved human parity in certain language pairs (Hassan et al., 2018; Barrault et al., 2019). Despite its remarkable advancements, the applicability of NMT over User-Generated Contents (UGC), such as social media text, still remains limited (Michel and Neubig, 2018; Berard et al., 2019a). Since UGC are prevailing in our real-life communication, it is undoubtedly one of the challenges we need to overcome to make MT systems invaluable for promoting cross-cultural communication. Recently, with the increasing interest in handling UGC, a shared task was organized to measure how well MT systems adapt to those texts (Li et al., 2019). However, the way in which they evaluate systems is just giving an overall score to a dataset, which is the same as traditional MT evaluation (Figure 1a). The overall score does not provide precise information for understanding what leads to the huge"
2020.coling-main.521,N19-1154,0,0.0145464,"ita et al., 2020), one of the largest parallel corpora available in Japanese-English. The larger model (L ARGE), is only different in the size of training data from the S MALL. We applied Byte-Pair-Encoding (BPE) models (Sennrich et al., 2016) with a joint vocabulary of 32,000 for these models using the sentencepiece toolkit (Kudo and Richardson, 2018). The character-based model (C HAR) is different from the two models in the way of segmentation. The model translates a sequence of characters in the source language into another sequence of characters in the target language (Wang et al., 2015). Durrani et al. (2019) pointed out that character-based models are more robust to noisy text than BPE-based models. We revisit the issue of segmentation to see if the model is also good at handling UGC. We shared the vocabulary between two languages in this setting as well to expect the model to capture copying behavior. For the pronunciation-based model (P RON), we applied a unique preprocessing method to the source (Japanese) sentence. More specifically, we first applied the MeCab toolkit (Kudo et al., 2004), a Japanese morphological analyzer, with naist-jdic for the dictionary to obtain the pronunciation of each"
2020.coling-main.521,W18-1807,0,0.0972352,"Missing"
2020.coling-main.521,W16-3918,0,0.0390284,"Missing"
2020.coling-main.521,D17-1263,0,0.0143517,"eloping truly robust systems, and motivated us to create a new dataset for focused evaluation. A range of studies have aimed to elucidate the cause of mistranslations from the viewpoint of linguistic phenomena, such as typographical errors (Heigold et al., 2018; Belinkov and Bisk, 2018; Karpukhin et al., 2019; Niu et al., 2020), grammaticality (Sennrich, 2017), presence of named entities (Ugawa et al., 2018), and identification of anaphoric pronouns (Bawden et al., 2018; M¨uller et al., 2018). One of the pioneering works to analyze the behavior of NMT is the challenge set approach proposed by Isabelle et al. (2017). They defined various subcategories of structural differences between English and French to evaluate how well models can handle them in detail. Though the approach has the potential of 1 2 https://www.reddit.com http://www.statmt.org/wmt19/robustness.html 5930 Step1: Annotating phenomena labels Ja En ��� Ja Phe.1 En Phe.2 Step2: Extracting targeted expressions MTNT corpus Ja: ������ (apude, update) �� En: That’s a plain update though Orig. Norm. En Phenomenon2 Orig. Step3: Normalizing the expressions Orig.: ������ (apude, update) �� Norm.: ��������� (update) �� Norm. En ��� Prefiltering with"
2020.coling-main.521,W18-6478,0,0.0185446,"we actually need to develop any UGC-specific techniques or not, we do not even know with such a many-sided dataset that how much the improvement in some metrics, such as BLEU score (Papineni et al., 2002), actually contributes to improve robustness on various noise. In fact, Berald et al. (2019b), the winning team in the shared task, reported that none of the techniques specifically designed for UGC was more effective in improving BLEU score than corpus filtering. Though there is no doubt that corpus filtering is one of the essential techniques for data-driven MT systems (Koehn et al., 2018; Junczys-Dowmunt, 2018), this is rather aimed at removing inappropriate sentence pairs generated during the process of creating corpora, not at handling noisy input. The way of current evaluation definitely prevents us from developing truly robust systems, and motivated us to create a new dataset for focused evaluation. A range of studies have aimed to elucidate the cause of mistranslations from the viewpoint of linguistic phenomena, such as typographical errors (Heigold et al., 2018; Belinkov and Bisk, 2018; Karpukhin et al., 2019; Niu et al., 2020), grammaticality (Sennrich, 2017), presence of named entities (Ugaw"
2020.coling-main.521,D19-5506,0,0.124651,"ng is one of the essential techniques for data-driven MT systems (Koehn et al., 2018; Junczys-Dowmunt, 2018), this is rather aimed at removing inappropriate sentence pairs generated during the process of creating corpora, not at handling noisy input. The way of current evaluation definitely prevents us from developing truly robust systems, and motivated us to create a new dataset for focused evaluation. A range of studies have aimed to elucidate the cause of mistranslations from the viewpoint of linguistic phenomena, such as typographical errors (Heigold et al., 2018; Belinkov and Bisk, 2018; Karpukhin et al., 2019; Niu et al., 2020), grammaticality (Sennrich, 2017), presence of named entities (Ugawa et al., 2018), and identification of anaphoric pronouns (Bawden et al., 2018; M¨uller et al., 2018). One of the pioneering works to analyze the behavior of NMT is the challenge set approach proposed by Isabelle et al. (2017). They defined various subcategories of structural differences between English and French to evaluate how well models can handle them in detail. Though the approach has the potential of 1 2 https://www.reddit.com http://www.statmt.org/wmt19/robustness.html 5930 Step1: Annotating phenomen"
2020.coling-main.521,D18-2012,0,0.0146356,". In addition, we replaced possible usernames with regular expressions. We offered this model to see whether or not the phenomena would become less problematic with increasing amount of training data. For the other four models, we additionally used JParacrawl v2.0 (Morishita et al., 2020), one of the largest parallel corpora available in Japanese-English. The larger model (L ARGE), is only different in the size of training data from the S MALL. We applied Byte-Pair-Encoding (BPE) models (Sennrich et al., 2016) with a joint vocabulary of 32,000 for these models using the sentencepiece toolkit (Kudo and Richardson, 2018). The character-based model (C HAR) is different from the two models in the way of segmentation. The model translates a sequence of characters in the source language into another sequence of characters in the target language (Wang et al., 2015). Durrani et al. (2019) pointed out that character-based models are more robust to noisy text than BPE-based models. We revisit the issue of segmentation to see if the model is also good at handling UGC. We shared the vocabulary between two languages in this setting as well to expect the model to capture copying behavior. For the pronunciation-based mode"
2020.coling-main.521,W04-3230,0,0.0299887,"in the source language into another sequence of characters in the target language (Wang et al., 2015). Durrani et al. (2019) pointed out that character-based models are more robust to noisy text than BPE-based models. We revisit the issue of segmentation to see if the model is also good at handling UGC. We shared the vocabulary between two languages in this setting as well to expect the model to capture copying behavior. For the pronunciation-based model (P RON), we applied a unique preprocessing method to the source (Japanese) sentence. More specifically, we first applied the MeCab toolkit (Kudo et al., 2004), a Japanese morphological analyzer, with naist-jdic for the dictionary to obtain the pronunciation of each morpheme in the sentences. We can transliterate any words in Japanese by using phonetic symbols such as hiragana and katakana characters. Since the MeCab toolkit outputs the pronunciation in katakana characters by default, we simply concatenated them to create a fully pronunciation-based corpus. We prepared this model with the expectation to improve the robustness against Variant expressions. More specifically, we aimed at absorbing the orthographic variations caused by hiragana-katakana"
2020.coling-main.521,W19-5303,0,0.060677,"ity in certain language pairs (Hassan et al., 2018; Barrault et al., 2019). Despite its remarkable advancements, the applicability of NMT over User-Generated Contents (UGC), such as social media text, still remains limited (Michel and Neubig, 2018; Berard et al., 2019a). Since UGC are prevailing in our real-life communication, it is undoubtedly one of the challenges we need to overcome to make MT systems invaluable for promoting cross-cultural communication. Recently, with the increasing interest in handling UGC, a shared task was organized to measure how well MT systems adapt to those texts (Li et al., 2019). However, the way in which they evaluate systems is just giving an overall score to a dataset, which is the same as traditional MT evaluation (Figure 1a). The overall score does not provide precise information for understanding what leads to the huge performance gap between the translation of clean input and that of UGC. To find a clue for improving the performance of MT systems on UGC, we need a solid basis for more detailed error analysis. As a first step towards a more refined evaluation of MT systems on UGC, we present a new dataset, PheMT: Phenomenon-wise Dataset for Machine Translation"
2020.coling-main.521,P19-1291,0,0.0185382,"transliterate any words in Japanese by using phonetic symbols such as hiragana and katakana characters. Since the MeCab toolkit outputs the pronunciation in katakana characters by default, we simply concatenated them to create a fully pronunciation-based corpus. We prepared this model with the expectation to improve the robustness against Variant expressions. More specifically, we aimed at absorbing the orthographic variations caused by hiragana-katakana confusion, which is a part of Variant. Also, previous study suggests that phonetic information is highly useful to resolve homophone noise (Liu et al., 2019). Finally, we prepared the concatenated model (C AT), trained on a joined corpus for the L ARGE and the P RON.5 In this setting, we converted the transliterated part into hiragana characters and applied the same BPE model as used in the L ARGE to the whole corpus. We expect the model to learn robust representations by forcing it to produce the same translation from the original source sentence and its transliterated counterpart. We used transformer-base architecture (Vaswani et al., 2017) implemented in the fairseq toolkit (Ott et al., 2019) and hyperparameters proposed by Murakami et al. (201"
2020.coling-main.521,D15-1166,0,0.0433624,"between the translation of clean input and that of UGC. To answer the question, we present a new dataset, PheMT, for evaluating the robustness of MT systems against specific linguistic phenomena in Japanese-English translation. Our experiments with the created dataset revealed that not only our in-house models but even widely used off-the-shelf systems are greatly disturbed by the presence of certain phenomena. 1 Introduction The advancement of Neural Machine Translation (NMT) has brought great improvement in translation quality when translating clean input, such as text from the news domain (Luong et al., 2015; Vaswani et al., 2017), and it was recently claimed that NMT has even achieved human parity in certain language pairs (Hassan et al., 2018; Barrault et al., 2019). Despite its remarkable advancements, the applicability of NMT over User-Generated Contents (UGC), such as social media text, still remains limited (Michel and Neubig, 2018; Berard et al., 2019a). Since UGC are prevailing in our real-life communication, it is undoubtedly one of the challenges we need to overcome to make MT systems invaluable for promoting cross-cultural communication. Recently, with the increasing interest in handli"
2020.coling-main.521,D18-1050,0,0.402372,"d off-the-shelf systems are greatly disturbed by the presence of certain phenomena. 1 Introduction The advancement of Neural Machine Translation (NMT) has brought great improvement in translation quality when translating clean input, such as text from the news domain (Luong et al., 2015; Vaswani et al., 2017), and it was recently claimed that NMT has even achieved human parity in certain language pairs (Hassan et al., 2018; Barrault et al., 2019). Despite its remarkable advancements, the applicability of NMT over User-Generated Contents (UGC), such as social media text, still remains limited (Michel and Neubig, 2018; Berard et al., 2019a). Since UGC are prevailing in our real-life communication, it is undoubtedly one of the challenges we need to overcome to make MT systems invaluable for promoting cross-cultural communication. Recently, with the increasing interest in handling UGC, a shared task was organized to measure how well MT systems adapt to those texts (Li et al., 2019). However, the way in which they evaluate systems is just giving an overall score to a dataset, which is the same as traditional MT evaluation (Figure 1a). The overall score does not provide precise information for understanding wh"
2020.coling-main.521,2020.lrec-1.443,1,0.710945,"n robustness, namely TED talks, KFTT (Kyoto Free Translation Task), and JESC (Japanese-English Subtitle Corpus). The MTNT dataset was also available in the task, but we didn’t include any of the sentence pairs to train our models. We replaced emojis and emoticons with placeholders following a previous study by Murakami et al. (2019). In addition, we replaced possible usernames with regular expressions. We offered this model to see whether or not the phenomena would become less problematic with increasing amount of training data. For the other four models, we additionally used JParacrawl v2.0 (Morishita et al., 2020), one of the largest parallel corpora available in Japanese-English. The larger model (L ARGE), is only different in the size of training data from the S MALL. We applied Byte-Pair-Encoding (BPE) models (Sennrich et al., 2016) with a joint vocabulary of 32,000 for these models using the sentencepiece toolkit (Kudo and Richardson, 2018). The character-based model (C HAR) is different from the two models in the way of segmentation. The model translates a sequence of characters in the source language into another sequence of characters in the target language (Wang et al., 2015). Durrani et al. (2"
2020.coling-main.521,W18-6307,0,0.0489643,"Missing"
2020.coling-main.521,W19-5365,1,0.910515,"ing methods for our experiments. The smaller model (S MALL) was trained on the data offered in the WMT 2019 shared 4 Note that a sentence could be given more than one label. These sentences are treated differently according to the label which we focus on. 5933 task for machine translation robustness, namely TED talks, KFTT (Kyoto Free Translation Task), and JESC (Japanese-English Subtitle Corpus). The MTNT dataset was also available in the task, but we didn’t include any of the sentence pairs to train our models. We replaced emojis and emoticons with placeholders following a previous study by Murakami et al. (2019). In addition, we replaced possible usernames with regular expressions. We offered this model to see whether or not the phenomena would become less problematic with increasing amount of training data. For the other four models, we additionally used JParacrawl v2.0 (Morishita et al., 2020), one of the largest parallel corpora available in Japanese-English. The larger model (L ARGE), is only different in the size of training data from the S MALL. We applied Byte-Pair-Encoding (BPE) models (Sennrich et al., 2016) with a joint vocabulary of 32,000 for these models using the sentencepiece toolkit ("
2020.coling-main.521,E17-2037,0,0.0264309,"to imitate misspellings found in a variety of real-world applications. Following work by Karpukhin et al. (2019) and Belinkov and Bisk (2018) extended its scope to natural noise by using edit histories from online websites. However, instead of giving translations to raw noisy sentences, they relied on a noisy version of input artificially created from the clean text. Anastasopoulos et al. (2019) is similar to our work in that they explored the effect of errors naturally created by humans. They focused on the effect of grammatical errors against NMT by adding translations to the JFLEG corpus (Napoles et al., 2017), one of the common benchmarks for grammatical error correction. Their results demonstrated that even a very small perturbation could significantly drop the performance of MT systems while exposure to similar noise during training time alleviates the problem. However, these aspects are only a small subset of possible reasons to explain why current models are still not good at handling UGC. To the best of our knowledge, there is no previous work aimed at investigating the effect of UGC-specific challenges in a fine-grained manner. Also, behavioral analysis of NMT in dissimilar language pairs su"
2020.coling-main.521,2020.acl-main.755,0,0.0115546,"al techniques for data-driven MT systems (Koehn et al., 2018; Junczys-Dowmunt, 2018), this is rather aimed at removing inappropriate sentence pairs generated during the process of creating corpora, not at handling noisy input. The way of current evaluation definitely prevents us from developing truly robust systems, and motivated us to create a new dataset for focused evaluation. A range of studies have aimed to elucidate the cause of mistranslations from the viewpoint of linguistic phenomena, such as typographical errors (Heigold et al., 2018; Belinkov and Bisk, 2018; Karpukhin et al., 2019; Niu et al., 2020), grammaticality (Sennrich, 2017), presence of named entities (Ugawa et al., 2018), and identification of anaphoric pronouns (Bawden et al., 2018; M¨uller et al., 2018). One of the pioneering works to analyze the behavior of NMT is the challenge set approach proposed by Isabelle et al. (2017). They defined various subcategories of structural differences between English and French to evaluate how well models can handle them in detail. Though the approach has the potential of 1 2 https://www.reddit.com http://www.statmt.org/wmt19/robustness.html 5930 Step1: Annotating phenomena labels Ja En ���"
2020.coling-main.521,N19-4009,0,0.0139419,"rmation is highly useful to resolve homophone noise (Liu et al., 2019). Finally, we prepared the concatenated model (C AT), trained on a joined corpus for the L ARGE and the P RON.5 In this setting, we converted the transliterated part into hiragana characters and applied the same BPE model as used in the L ARGE to the whole corpus. We expect the model to learn robust representations by forcing it to produce the same translation from the original source sentence and its transliterated counterpart. We used transformer-base architecture (Vaswani et al., 2017) implemented in the fairseq toolkit (Ott et al., 2019) and hyperparameters proposed by Murakami et al. (2019) for all models. The size of the training data was 3.9 M for the S MALL, 14.0 M for the L ARGE, C HAR and P RON, and 28.0 M for the C AT. In addition to the in-house models, we also investigated the impact of the phenomena on two widely used MT systems, namely, Google Translate6 and DeepL Translator. 7,8 These systems are expected to be more robust against UGC because they are by nature exposed to user input. By conducting experiments on such systems, we reveal the presence of phenomena with impending needs for improvement, and also confir"
2020.coling-main.521,P02-1040,0,0.107587,"for MT systems. Their results with the baseline systems demonstrated the difficulty of properly translating UGC. The dataset was also used as in-domain data for the first shared task on machine translation robustness held at WMT 2019.2 However, the dataset is still miscellaneous in the degree of politeness, domain of the conversations, and even in the quality of translations. Though it is still a question whether we actually need to develop any UGC-specific techniques or not, we do not even know with such a many-sided dataset that how much the improvement in some metrics, such as BLEU score (Papineni et al., 2002), actually contributes to improve robustness on various noise. In fact, Berald et al. (2019b), the winning team in the shared task, reported that none of the techniques specifically designed for UGC was more effective in improving BLEU score than corpus filtering. Though there is no doubt that corpus filtering is one of the essential techniques for data-driven MT systems (Koehn et al., 2018; Junczys-Dowmunt, 2018), this is rather aimed at removing inappropriate sentence pairs generated during the process of creating corpora, not at handling noisy input. The way of current evaluation definitely"
2020.coling-main.521,C14-1167,0,0.0262309,"Missing"
2020.coling-main.521,I13-1019,0,0.0293741,"Missing"
2020.coling-main.521,P16-1162,0,0.0320939,"ls. We replaced emojis and emoticons with placeholders following a previous study by Murakami et al. (2019). In addition, we replaced possible usernames with regular expressions. We offered this model to see whether or not the phenomena would become less problematic with increasing amount of training data. For the other four models, we additionally used JParacrawl v2.0 (Morishita et al., 2020), one of the largest parallel corpora available in Japanese-English. The larger model (L ARGE), is only different in the size of training data from the S MALL. We applied Byte-Pair-Encoding (BPE) models (Sennrich et al., 2016) with a joint vocabulary of 32,000 for these models using the sentencepiece toolkit (Kudo and Richardson, 2018). The character-based model (C HAR) is different from the two models in the way of segmentation. The model translates a sequence of characters in the source language into another sequence of characters in the target language (Wang et al., 2015). Durrani et al. (2019) pointed out that character-based models are more robust to noisy text than BPE-based models. We revisit the issue of segmentation to see if the model is also good at handling UGC. We shared the vocabulary between two lang"
2020.coling-main.521,E17-2060,0,0.0220698,"stems (Koehn et al., 2018; Junczys-Dowmunt, 2018), this is rather aimed at removing inappropriate sentence pairs generated during the process of creating corpora, not at handling noisy input. The way of current evaluation definitely prevents us from developing truly robust systems, and motivated us to create a new dataset for focused evaluation. A range of studies have aimed to elucidate the cause of mistranslations from the viewpoint of linguistic phenomena, such as typographical errors (Heigold et al., 2018; Belinkov and Bisk, 2018; Karpukhin et al., 2019; Niu et al., 2020), grammaticality (Sennrich, 2017), presence of named entities (Ugawa et al., 2018), and identification of anaphoric pronouns (Bawden et al., 2018; M¨uller et al., 2018). One of the pioneering works to analyze the behavior of NMT is the challenge set approach proposed by Isabelle et al. (2017). They defined various subcategories of structural differences between English and French to evaluate how well models can handle them in detail. Though the approach has the potential of 1 2 https://www.reddit.com http://www.statmt.org/wmt19/robustness.html 5930 Step1: Annotating phenomena labels Ja En ��� Ja Phe.1 En Phe.2 Step2: Extracti"
2020.coling-main.521,W16-2710,0,0.0247029,"Missing"
2020.coling-main.521,C18-1274,0,0.0158821,"018), this is rather aimed at removing inappropriate sentence pairs generated during the process of creating corpora, not at handling noisy input. The way of current evaluation definitely prevents us from developing truly robust systems, and motivated us to create a new dataset for focused evaluation. A range of studies have aimed to elucidate the cause of mistranslations from the viewpoint of linguistic phenomena, such as typographical errors (Heigold et al., 2018; Belinkov and Bisk, 2018; Karpukhin et al., 2019; Niu et al., 2020), grammaticality (Sennrich, 2017), presence of named entities (Ugawa et al., 2018), and identification of anaphoric pronouns (Bawden et al., 2018; M¨uller et al., 2018). One of the pioneering works to analyze the behavior of NMT is the challenge set approach proposed by Isabelle et al. (2017). They defined various subcategories of structural differences between English and French to evaluate how well models can handle them in detail. Though the approach has the potential of 1 2 https://www.reddit.com http://www.statmt.org/wmt19/robustness.html 5930 Step1: Annotating phenomena labels Ja En ��� Ja Phe.1 En Phe.2 Step2: Extracting targeted expressions MTNT corpus Ja: ������ (a"
2020.coling-main.521,1994.amta-1.25,0,0.460306,"Missing"
2020.emnlp-demos.28,N19-4009,0,0.0146983,"e, in contrast to correcting surface-level errors (Napoles et al., 2017). The diversity of the output revisions is encouraged using diverse beam search (Vijayakumar et al., 2018). In addition, these revisions are ordered by a language model that is fine-tuned for NLP papers. That is, revisions with lower perplexity are listed in the upper part of the suggestion box. Furthermore, the revisions are highlighted in colors, which makes it easier to distinguish the characteristics of each revision. Implementation. We trained a revision model using LightConv (Wu et al., 2019) implemented in Fairseq (Ott et al., 2019). The revision model generates a sentence based on a given input sentence. The model was trained on a slightly modified version of the synthetic training data used in Ito et al. (2019). As an example of these modifications, synthetic edit marks were added for a subset of the training data. These marks were attached to a part 10 The system performs sentence-level revisions. Hence the users are instructed to select the non-sentence-crossing area. 11 We allow the system to correct the parts outside the selected span because sometimes the revision for a specific part requires another adjustment fo"
2020.emnlp-demos.28,2020.acl-main.704,0,0.0263968,"pants who chose the option. Half of the participants first revised a draft with the H UMAN - ONLY setting, and then revised another draft with the H UMAN &M ACHINE setting; the other half performed the same task in the opposite order. Ultimately, we collected two H UMAN &M ACHINE revisions and two H UMAN O NLY revisions for each first draft. Comparison and results. We compared the quality of the three versions of the revised drafts: M ACHINE -O NLY revision, H UMAN -O NLY revision, and H UMAN &M ACHINE revision. We compared the revised drafts with their corresponding final draft using BLEURT (Sellam et al., 2020), the state-of-the-art automatic evaluation metric for natural language generation tasks. Details of the evaluation procedure is shown in Appendix D. Note that the score is not in the range [0, 1], and a higher score means that the revision is closer to the final draft. Table 1 shows that H UMAN &M ACHINE revisions were significantly better20 than M ACHINE ONLY and H UMAN - ONLY revisions. The results suggest the effectiveness of human–machine interaction achieved in Langsmith. Since this experiment was relatively small in scale and only used an automatic evaluation metric, we will conduct a l"
2020.emnlp-demos.28,2020.acl-demos.17,0,0.0430432,"to convey their ideas accurately. Ito et al. (2019) introduced a dataset and models for revising early-stage drafts, and the 1-to-N nature of the revisions was pointed out. We tackled this difficulty by designing an overall demonstration system, including a user interface. 2.2 Writing assistance tools Error checkers. Grammar/spelling checkers are typical writing assistance tools. Some highlight errors (e.g., Write&Improve4 ), while others suggest 3 4 This paper was also written using Langsmith. writeandimprove.com corrections (e.g., Grammarly5 , LanguageTool6 , Ginger7 , and LinggleWrite; see Tsai et al. (2020)) for writers. Langsmith has a revision feature (Ito et al., 2019), as well as a grammar/spelling checker. The revision feature suggests better versions of poor written phrases or sentences in terms of fluency and style, whereas error checkers are typically designed to correct apparent errors only. In addition, Langsmith is specialized for the NLP domain and enables domain-specific revisions, such as correcting technical terms. Text completion. Completing a text is another typical feature in writing assistance applications (WriteAhead8 , Write With Transformer9 , and Smart Compose; see Chen et"
2020.emnlp-demos.28,P10-2021,0,0.0411196,"in green are added to the original sentence, and the red points indicate tracked deletions. model. Furthermore, the communication between the server and the web frontend is achieved via a protocol specialized in writing software called the Text Editing Assistance Smartness Protocol for Natural Language (TEASPN) (Hagiwara et al., 2019). We hope that our system will help the NLP community and researchers, especially those lacking a native command of English.3 2 Related work 2.1 Natural language processing for academic writing Academic writing assistance has gained considerable attention in NLP (Wu et al., 2010; Yimam et al., 2020; Lee and Webster, 2012), and several shared tasks have been organized (Dale and Kilgarriff, 2011; Daudaraviˇcius, 2015). These tasks focus on polishing texts in already published articles or documents near completion. In contrast, this study focuses on revising texts in the earlier stages of writing (e.g., first drafts), where inexperienced, non-native authors might even struggle to convey their ideas accurately. Ito et al. (2019) introduced a dataset and models for revising early-stage drafts, and the 1-to-N nature of the revisions was pointed out. We tackled this difficu"
2020.emnlp-demos.28,2020.lrec-1.722,0,0.0243564,"d to the original sentence, and the red points indicate tracked deletions. model. Furthermore, the communication between the server and the web frontend is achieved via a protocol specialized in writing software called the Text Editing Assistance Smartness Protocol for Natural Language (TEASPN) (Hagiwara et al., 2019). We hope that our system will help the NLP community and researchers, especially those lacking a native command of English.3 2 Related work 2.1 Natural language processing for academic writing Academic writing assistance has gained considerable attention in NLP (Wu et al., 2010; Yimam et al., 2020; Lee and Webster, 2012), and several shared tasks have been organized (Dale and Kilgarriff, 2011; Daudaraviˇcius, 2015). These tasks focus on polishing texts in already published articles or documents near completion. In contrast, this study focuses on revising texts in the earlier stages of writing (e.g., first drafts), where inexperienced, non-native authors might even struggle to convey their ideas accurately. Ito et al. (2019) introduced a dataset and models for revising early-stage drafts, and the 1-to-N nature of the revisions was pointed out. We tackled this difficulty by designing an"
2020.emnlp-main.236,S14-2039,0,0.076462,"Missing"
2020.emnlp-main.236,S15-2027,0,0.0580653,"Missing"
2020.emnlp-main.236,N18-1049,0,0.457536,"antages, EMD-based methods have underperformed sentence-vector-based methods on STS tasks. The goal of this study is to identify and resolve the obstacles faced by EMD-based methods (Section 5). Sentence-vector Approach. Another popular approach is to employ general-purpose sentence vectors of given texts and to compute the cosine similarity between such vectors. A variety of methods to compute sentence vectors have been proposed, ranging from utilizing deep sentence encoders (Kiros et al., 2015; Conneau et al., 2017; Cer et al., 2018), learning and using word vectors optimized for summation (Pagliardini et al., 2018; Wieting and Gimpel, 2018), and estimating latent sentence vectors from pre-trained word vectors (Arora et al., 2017; Ethayarajh, 2018; Liu et al., 2019b). This paper demonstrates that some recently proposed sentence vectors can be reformulated as a sum of the converted word vectors. By utilizing the converted word vectors, our method can achieve similar or better performance compared to sentence-vector approaches (Section 6). 4 4.1 Word Mover’s Distance and its Issues Earth Mover’s Distance Intuitively, earth mover’s distance (EMD)2 (Villani, 2009; Santambrogio, 2015; Peyr´e and Cuturi, 2019"
2020.emnlp-main.574,P16-1162,0,0.0838261,"Missing"
2020.emnlp-main.574,J03-1002,0,\N,Missing
2020.emnlp-main.574,P17-1106,0,\N,Missing
2020.emnlp-main.574,W18-6304,0,\N,Missing
2020.emnlp-main.574,W18-5431,0,\N,Missing
2020.emnlp-main.574,N19-1419,0,\N,Missing
2020.emnlp-main.574,P19-1356,0,\N,Missing
2020.emnlp-main.574,P19-1124,0,\N,Missing
2020.emnlp-main.574,N19-1423,0,\N,Missing
2020.emnlp-main.574,N16-1082,0,\N,Missing
2020.emnlp-main.574,D19-1445,0,\N,Missing
2020.emnlp-main.574,2020.tacl-1.54,0,\N,Missing
2020.emnlp-main.574,P00-1056,0,\N,Missing
2020.emnlp-main.574,N13-1073,0,\N,Missing
2020.emnlp-main.68,D18-1431,0,0.129604,"e phrases from the table. As a result, the phrase table included 68,891 phrase pairs, which were used as the key phrase set P as described in Section 5.1. To compute the content relatedness SR , we created a sentence vector from pre-trained fastText word embeddings (Bojanowski et al., 2017; Mikolov et al., 2018) following Arora et al. (2017)’s method, i.e., using SIF weighting and common component removal. Their method is reported to be useful for computing the relatedness of two given sentences and used in many studies (Marelli et al., 2014b,a; Conneau et al., 2017; Subramanian et al., 2018; Baheti et al., 2018). We learned common components using 30K sentences randomly selected from the training costs appropriately. We then removed the first common component for all sentence vectors. Baselines. For comparison, we prepared the following: • Cs´aky et al. (2019): Entropy-based filtering to remove generic utterances from the training data for promoting less-boring response generation. SRC/TRG indicates that using the entropy of source/target utterances. • Junczys-Dowmunt (2018): Filtering for NMT based on the dual conditional cross-entropy computed by a neural encoder-decoder model. It achieved the best"
2020.emnlp-main.68,2020.acl-main.9,0,0.0901188,"Missing"
2020.emnlp-main.68,Q17-1010,0,0.0121349,"rrence frequency (here, less than 200 times) or composed 5 See Appendix B for details on our data such as the preparation procedure and statistics. (a) Cs´aky et al. (2019) TRG (b) Junczys-Dowmunt (2018) (c) Ours SC+R Figure 2: Distributions between human scores and automatically computed scores by each method (English). of the same phrases from the table. As a result, the phrase table included 68,891 phrase pairs, which were used as the key phrase set P as described in Section 5.1. To compute the content relatedness SR , we created a sentence vector from pre-trained fastText word embeddings (Bojanowski et al., 2017; Mikolov et al., 2018) following Arora et al. (2017)’s method, i.e., using SIF weighting and common component removal. Their method is reported to be useful for computing the relatedness of two given sentences and used in many studies (Marelli et al., 2014b,a; Conneau et al., 2017; Subramanian et al., 2018; Baheti et al., 2018). We learned common components using 30K sentences randomly selected from the training costs appropriately. We then removed the first common component for all sentence vectors. Baselines. For comparison, we prepared the following: • Cs´aky et al. (2019): Entropy-based f"
2020.emnlp-main.68,2020.acl-main.218,0,0.0281163,"et utterances. • Junczys-Dowmunt (2018): Filtering for NMT based on the dual conditional cross-entropy computed by a neural encoder-decoder model. It achieved the best performance on the Parallel Corpus Filtering Task at WMT 2018.6 Human evaluation. To validate the ability of the proposed method to estimate the quality of utterance pairs, we measured the correlation between its scores and those assigned by humans through crowdsourcing. We used Amazon Mechanical Turk.7 We randomly extracted 2008 scored 6 http://www.statmt.org/wmt18/ https://www.mturk.com/ 8 Same size as Sedoc et al. (2019) and Cho and May (2020). 7 Scoring method Spearman’s ρ Cs´aky et al. (2019) SRC Cs´aky et al. (2019) TRG Junczys-Dowmunt (2018) Ours SC+R Ours SC (ablation study) Ours SR (ablation study) p-value −0.1173 0.0462 0.2973 0.3751 9.8 × 10−2 5.2 × 10−1 1.9 × 10−5 4.4 × 10−8 0.2044 0.3007 3.7 × 10−3 1.5 × 10−5 Table 2: Correlation coefficient between human scores and automatically computed scores (English). utterance pairs and asked native English-speaking crowdworkers to answer the following question for each pair: Is the sequence of the two utterances acceptable as a dialogue? Workers were instructed to provide an answer"
2020.emnlp-main.68,D17-1070,0,0.0999009,", that is, content relatedness as follows:  SR (x, y) := max cos(v(x), v(y)), 0 . (3) where α, β ∈ R≥0 are hyperparameters that weigh the two viewpoints. For our experiments, we fix α and β as follows: • If a phrase pair (f, e) has a high co-occurrence, the association strength of (x, y) including (f, e) might also be high. • If a phrase f or e occupies almost the entire sentence x or y, (f, e) is a strong indicator of the association of (x, y). 5.2 Summary (2) Cosine similarity between certain kinds of sentence vectors is known to be a good proxy of the topical relatedness of two sentences (Conneau et al., 2017; Subramanian et al., 2018; Xu et al., 2018a). For the same reasons as Equation 1, we ignore the negative cos scores by the max(·, 0) operation. 944 α= 6 1 1 X , β= SC (x, y) 1 |D| (x,y)∈D X 1 SR (x, y) |D| (x,y)∈D . (4) Experiments: Data Scoring In this section, we describe our experiments that validate the effectiveness of the proposed scoring method. 6.1 Experimental Setup Dataset. We conducted our experiments on a noisy English dialogue corpus from OpenSubtitles (Lison et al., 2018) containing roughly 441M lines. As explained in Section 1, this corpus includes many unacceptable utterance p"
2020.emnlp-main.68,N13-1073,0,0.0120095,"isy English dialogue corpus from OpenSubtitles (Lison et al., 2018) containing roughly 441M lines. As explained in Section 1, this corpus includes many unacceptable utterance pairs (Section 1). We first applied several rule-based filtering as rudimentary preprocesses, which are typically used in the related literature. Then, we obtained 79,445,453 utterance pairs as our training data, which excludes our test and validation data.5 Proposed method: detailed setup. To compute the connectivity SC , we obtained a phrase table on our training data by using Moses (Koehn et al., 2007) with fastAlign (Dyer et al., 2013). We then removed phrase pairs with a low co-occurrence frequency (here, less than 200 times) or composed 5 See Appendix B for details on our data such as the preparation procedure and statistics. (a) Cs´aky et al. (2019) TRG (b) Junczys-Dowmunt (2018) (c) Ours SC+R Figure 2: Distributions between human scores and automatically computed scores by each method (English). of the same phrases from the table. As a result, the phrase table included 68,891 phrase pairs, which were used as the key phrase set P as described in Section 5.1. To compute the content relatedness SR , we created a sentence v"
2020.emnlp-main.68,N19-4011,0,0.0123439,"e entropy of source/target utterances. • Junczys-Dowmunt (2018): Filtering for NMT based on the dual conditional cross-entropy computed by a neural encoder-decoder model. It achieved the best performance on the Parallel Corpus Filtering Task at WMT 2018.6 Human evaluation. To validate the ability of the proposed method to estimate the quality of utterance pairs, we measured the correlation between its scores and those assigned by humans through crowdsourcing. We used Amazon Mechanical Turk.7 We randomly extracted 2008 scored 6 http://www.statmt.org/wmt18/ https://www.mturk.com/ 8 Same size as Sedoc et al. (2019) and Cho and May (2020). 7 Scoring method Spearman’s ρ Cs´aky et al. (2019) SRC Cs´aky et al. (2019) TRG Junczys-Dowmunt (2018) Ours SC+R Ours SC (ablation study) Ours SR (ablation study) p-value −0.1173 0.0462 0.2973 0.3751 9.8 × 10−2 5.2 × 10−1 1.9 × 10−5 4.4 × 10−8 0.2044 0.3007 3.7 × 10−3 1.5 × 10−5 Table 2: Correlation coefficient between human scores and automatically computed scores (English). utterance pairs and asked native English-speaking crowdworkers to answer the following question for each pair: Is the sequence of the two utterances acceptable as a dialogue? Workers were instruct"
2020.emnlp-main.68,P16-1162,0,0.0349849,"Missing"
2020.emnlp-main.68,P19-1021,0,0.0183406,"ility scores are given by humans on the English OpenSubtitles corpus. Introduction Some million-scale datasets such as movie scripts and social media posts have become available in recent years for building neural dialogue agents (Lison and Tiedemann, 2016; Henderson et al., 2019). Such large-scale datasets can be expected to improve the performance of dialogue response generation models based on deep neural networks (DNNs) since the combination of DNNs and large-scale training datasets has led to considerable performance improvement in many sentence generation tasks (Koehn and Knowles, 2017; Sennrich and Zhang, 2019; Adiwardana et al., 2020). In contrast to the quantity of the data, the quality of the data has often been problematic. For example, OpenSubtitles (Lison and Tiedemann, 2016; Lison et al., 2018), the most widely used largescale English dialogue corpus, was constructed by 1 The code is publicly available at https://github. com/jqk09a/CoRe-dialogue-filtering. collecting two consecutive lines of movie subtitles under the simplified assumption that one line of a movie subtitle is one utterance and the next line is the next utterance follow it. Inevitably, this corpus includes unacceptable utteran"
2020.emnlp-main.68,P15-1152,0,0.0851264,"Missing"
2020.emnlp-main.68,2020.acl-main.220,0,0.0214665,"s, these methods cannot straightforwardly be considered as alternatives to the proposed method, which aims at filtering. 10 Relationship with Evaluation Metric The proposed method SC+R maps an utterance pair to a score (scalar value) in terms of the quality of dialogue. That is, formally, our method is similar to the reference-free automatic evaluation metrics for dialogue agents; both of them evaluate the response given an input utterance and also map into a score. Recently, the novel reference-free metrics for evaluating generated responses such as USR (Mehri and Eskenazi, 2020) or M AU DE (Sinha et al., 2020) ware developed. While it is possible to use them as a scoring method for filtering noisy data, in theory, there are some concerns with applying them in practice. One is the difference of the data of interest; since evaluation metrics are intended for responses generated as dialogue, i.e., somewhat valid dialogue data, it is unclear whether they also work for apparently noisy data. Another one is the difference of desired Conclusion In light of the success of noisy corpus filtering in neural machine translation, we attempted to filter out unacceptable utterance pairs from large dialogue corpor"
2020.findings-emnlp.26,W19-4418,1,0.829117,"SR +PRET+R2L+SED 55.8 60.4 64.2 65.0 63.1 68.8 69.8 72.9 73.9 59.9 63.3 61.2 61.4 63.7 69.5 70.2 67.8 Table 7: Comparison with existing top models: a bold value denotes the best result within the column. Both SR and BEA indicate SR BEA+EF+L8 and BEA-test, respectively. ensemble of four left-to-right (L2R) models and then re-scored the hypotheses using these models. We then re-ranked the n-best hypotheses based on the sum of the both two scores. Sentence-level error detection (SED) SED is used to identify whether a given sentence contains any grammatical errors. Following the work presented by Asano et al. (2019), we employed a strategy based on reducing the number of false positives by only considering sentences that contained grammatical errors in the GEC model, using an SED model. We implemented the same model employed for SED filtering. We evaluated the performance of the proposed best denoised model incorporated with the taskspecific techniques on the three existing benchmarks: CoNLL-2014, JFLEG, and BEA-test, and then compared the scores with existing bestperforming models. Table 7 shows the results for both the single and the ensemble models after applying PRET, SED13 , and R2L to SR14 . Since"
2020.findings-emnlp.26,D19-1435,0,0.0267376,"Missing"
2020.findings-emnlp.26,W18-6404,0,0.120723,"illed annotators are corrected by an existing GEC model. This approach relies on the consistence of the GEC model’s predictions (Section 4). We evaluated the effectiveness of our method over several GEC datasets, and found that it considerably outperformed baseline methods, includ1 https://corpus.mml.cam.ac.uk/ efcamdat2/public_html/ 267 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 267–280 c November 16 - 20, 2020. 2020 Association for Computational Linguistics ing three strong denoising baselines based on a filtering approach, which is a common approach in MT (Bei et al., 2018; Junczys-Dowmunt, 2018; Rossenbach et al., 2018). We further improved the performance by applying task-specific techniques and achieved state-of-the-art performance on the CoNLL-2014, JFLEG, and BEA-2019 benchmarks. Finally, through our analysis, we found unexpected benefits to our approach: (i) the approach benefits from the advantage of self-training in neural sequence generation due to its structural similarity (Section 6.3), (ii) resulted in significant increase in recall while maintaining equal precision, indicating improved coverage of correction (Section 6.4), and (iii) there seems to"
2020.findings-emnlp.26,P06-1032,0,0.0692196,"oposed denoising method, and found that our approach leads to improved coverage of corrections and facilitated fluency edits which are reflected in higher recall and overall performance. 1 2 : Errors are left uncorrected Source: We discuss about our sales target. Target: We discuss about our sales target. Table 1: Example of an inappropriately corrected error and an unchanged error in EFCamDat. We consider these types of errors to be dataset noise that might hinder GEC model performance. Introduction Grammatical error correction (GEC) is often considered a variant of machine translation (MT) (Brockett et al., 2006; Junczys-Dowmunt et al., 2018) due to their structural similarity–“translating” from source ungrammatical text to target grammatical text. At present, several neural encoderdecoder (EncDec) approaches have been introduced for this task and have achieved remarkable results (Chollampatt and Ng, 2018; Zhao et al., 2019; Kiyono et al., 2019). EncDec models tend to further improve in performance with increasing data size (Koehn and Knowles, 2017; Sennrich and Zhang, 2019), however, this is not necessarily true in GEC. For example, Lo et al. (2018) reported that an EncDec-based GEC model trained on"
2020.findings-emnlp.26,W19-4406,0,0.0268239,"Missing"
2020.findings-emnlp.26,N12-1067,0,0.0314065,"the effectiveness of the proposed method, we followed the work by Mita et al. (2019) and evaluated the performance of the GEC models across various GEC datasets in terms of the same evaluation metrics. We report 5 https://spacy.io/ https://github.com/rsennrich/ subword-nmt 6 Dataset #sent (pairs) Split BEA-train EFCamDAT Lang-8 BEA-valid CoNLL-2014 JFLEG BEA-test 561,100 train 2,269,595 train 5,689,213 train 2,377 valid Scorer - 1,312 test M2 scorer & GLEU 747 test M2 scorer & GLEU 4,477 test ERRANT Table 4: Summary of datasets used in our experiments. the results measured by both M2 scorer (Dahlmeier and Ng, 2012)7 and GLEU metric (Napoles et al., 2015, 2016)8 on both the CoNLL-2014 test set and the JFLEG test set (Napoles et al., 2017). All reported results (except those corresponding to the ensemble models) are the average of three distinct trials using three different random seeds. Let us emphasize that our focus is on denoising the training data, and denoising the test data is out of the scope of this study. The commonly used test data, such as CoNLL-2014 and JFLEG, have multiple references which can lower the noise factor. In addition to having multiple references, both JFLEG and CoNLL-2014 have b"
2020.findings-emnlp.26,W13-1703,0,0.0762998,"Missing"
2020.findings-emnlp.26,N19-1423,0,0.04159,"Missing"
2020.findings-emnlp.26,W19-4427,0,0.0294103,"Missing"
2020.findings-emnlp.26,P15-2097,0,0.0237037,"we followed the work by Mita et al. (2019) and evaluated the performance of the GEC models across various GEC datasets in terms of the same evaluation metrics. We report 5 https://spacy.io/ https://github.com/rsennrich/ subword-nmt 6 Dataset #sent (pairs) Split BEA-train EFCamDAT Lang-8 BEA-valid CoNLL-2014 JFLEG BEA-test 561,100 train 2,269,595 train 5,689,213 train 2,377 valid Scorer - 1,312 test M2 scorer & GLEU 747 test M2 scorer & GLEU 4,477 test ERRANT Table 4: Summary of datasets used in our experiments. the results measured by both M2 scorer (Dahlmeier and Ng, 2012)7 and GLEU metric (Napoles et al., 2015, 2016)8 on both the CoNLL-2014 test set and the JFLEG test set (Napoles et al., 2017). All reported results (except those corresponding to the ensemble models) are the average of three distinct trials using three different random seeds. Let us emphasize that our focus is on denoising the training data, and denoising the test data is out of the scope of this study. The commonly used test data, such as CoNLL-2014 and JFLEG, have multiple references which can lower the noise factor. In addition to having multiple references, both JFLEG and CoNLL-2014 have been specifically constructed for GEC ev"
2020.findings-emnlp.26,E17-2037,0,0.0757371,"models across various GEC datasets in terms of the same evaluation metrics. We report 5 https://spacy.io/ https://github.com/rsennrich/ subword-nmt 6 Dataset #sent (pairs) Split BEA-train EFCamDAT Lang-8 BEA-valid CoNLL-2014 JFLEG BEA-test 561,100 train 2,269,595 train 5,689,213 train 2,377 valid Scorer - 1,312 test M2 scorer & GLEU 747 test M2 scorer & GLEU 4,477 test ERRANT Table 4: Summary of datasets used in our experiments. the results measured by both M2 scorer (Dahlmeier and Ng, 2012)7 and GLEU metric (Napoles et al., 2015, 2016)8 on both the CoNLL-2014 test set and the JFLEG test set (Napoles et al., 2017). All reported results (except those corresponding to the ensemble models) are the average of three distinct trials using three different random seeds. Let us emphasize that our focus is on denoising the training data, and denoising the test data is out of the scope of this study. The commonly used test data, such as CoNLL-2014 and JFLEG, have multiple references which can lower the noise factor. In addition to having multiple references, both JFLEG and CoNLL-2014 have been specifically constructed for GEC evaluation, while the training data (Lang-8 and EFCamDat) are more of an organic collect"
2020.findings-emnlp.26,P19-1256,0,0.0278192,"generate new target sentences Yˆi from the original target sentences Yi and pair them with their original source sentences Xi (line 4 in Algorithm 1). The consistency of the base model predictions ensures that the ˆ = {(Xi , Yˆi )}n contain resulting parallel data D i=1 noise at a less extent. It is worth noting that SR can be regarded as a variant of self-training due to its structural similarity, except that it takes the target sentences rather than the source sentences as input to the model. The algorithm itself is the key difference from existing methods based on selftraining (Wang, 2019; Nie et al., 2019; Xie et al., 2020). One challenge of this approach is that the base model may consistently make inaccurate corrections. We thus incorporate a fail-safe mechanism Experiments We evaluate the proposed method in two ways. First, we exclusively focus on investigating the effectiveness of the proposed denoising method (Section 5.3). Then, we compare our strongest model trained with denoised data (henceforth, denoised model), with current best-performing ones to investigate whether the proposed method has a complementary effect on existing task-specific techniques (Section 5.4). 5.1 Configurations"
2020.findings-emnlp.26,2020.bea-1.16,0,0.0622744,"Missing"
2020.findings-emnlp.26,N19-4009,0,0.0124714,"the scope of this study. The commonly used test data, such as CoNLL-2014 and JFLEG, have multiple references which can lower the noise factor. In addition to having multiple references, both JFLEG and CoNLL-2014 have been specifically constructed for GEC evaluation, while the training data (Lang-8 and EFCamDat) are more of an organic collection of learner and editor interactions. Naturally, we believe it is reasonable to assume that the test data are considerably cleaner. Model We employed the “Transformer (big)” settings Vaswani et al. (2017) using the implementation in the fairseq toolkit (Ott et al., 2019). Details on the hyper-parameters are listed in Appendix B. As a language model for the fail-safe mechanism, we used the PyTorch implementation of GPT-2 (Radford et al., 2019)9 . Note that to avoid a preference for shorter phrases, we normalized the perplexity by sentence length. 5.2 Baselines As argued in Section 4, we hypothesized that the filtering-based denoising approaches are not wellsuited for GEC. To verify this hypothesis, we employed the following three filtering-based denoising baseline methods in addition to a base model trained in noisy parallel data D (henceforth, no denoising)."
2020.findings-emnlp.26,N19-1132,1,0.898095,"datasets are summarized in Table 4. For preprocessing, we tokenized the training data using the spaCy tokenizer5 . Then, we removed sentence pairs where both sentences where identical or both longer than 80 tokens. Finally, we acquired subwords from the target sentence via the byte-pair-encoding (BPE) (Sennrich et al., 2016b) algorithm. We used the subword-nmt implementation6 and then applied BPE to splitting both source and target texts. The number of merge operations was set to 8,000. 270 Evaluation To investigate the effectiveness of the proposed method, we followed the work by Mita et al. (2019) and evaluated the performance of the GEC models across various GEC datasets in terms of the same evaluation metrics. We report 5 https://spacy.io/ https://github.com/rsennrich/ subword-nmt 6 Dataset #sent (pairs) Split BEA-train EFCamDAT Lang-8 BEA-valid CoNLL-2014 JFLEG BEA-test 561,100 train 2,269,595 train 5,689,213 train 2,377 valid Scorer - 1,312 test M2 scorer & GLEU 747 test M2 scorer & GLEU 4,477 test ERRANT Table 4: Summary of datasets used in our experiments. the results measured by both M2 scorer (Dahlmeier and Ng, 2012)7 and GLEU metric (Napoles et al., 2015, 2016)8 on both the Co"
2020.findings-emnlp.26,W18-6487,0,0.0574237,"Missing"
2020.findings-emnlp.26,I11-1017,0,0.0370849,"Missing"
2020.findings-emnlp.26,W17-4739,0,0.0455444,"Missing"
2020.findings-emnlp.26,W16-2323,0,0.0200525,"5.1 Configurations Dataset For the training dataset, we used the same datasets as mentioned in Section 3: BEAtrain, EFCamDat, and Lang-8. In addition, we used the BEA official validation set (henceforth, BEA-valid) provided in the BEA-2019 workshop as validation data. The characteristics of the datasets are summarized in Table 4. For preprocessing, we tokenized the training data using the spaCy tokenizer5 . Then, we removed sentence pairs where both sentences where identical or both longer than 80 tokens. Finally, we acquired subwords from the target sentence via the byte-pair-encoding (BPE) (Sennrich et al., 2016b) algorithm. We used the subword-nmt implementation6 and then applied BPE to splitting both source and target texts. The number of merge operations was set to 8,000. 270 Evaluation To investigate the effectiveness of the proposed method, we followed the work by Mita et al. (2019) and evaluated the performance of the GEC models across various GEC datasets in terms of the same evaluation metrics. We report 5 https://spacy.io/ https://github.com/rsennrich/ subword-nmt 6 Dataset #sent (pairs) Split BEA-train EFCamDAT Lang-8 BEA-valid CoNLL-2014 JFLEG BEA-test 561,100 train 2,269,595 train 5,689,2"
2020.findings-emnlp.26,P11-1019,0,0.072917,"Missing"
2020.findings-emnlp.26,P16-1162,0,0.0110038,"5.1 Configurations Dataset For the training dataset, we used the same datasets as mentioned in Section 3: BEAtrain, EFCamDat, and Lang-8. In addition, we used the BEA official validation set (henceforth, BEA-valid) provided in the BEA-2019 workshop as validation data. The characteristics of the datasets are summarized in Table 4. For preprocessing, we tokenized the training data using the spaCy tokenizer5 . Then, we removed sentence pairs where both sentences where identical or both longer than 80 tokens. Finally, we acquired subwords from the target sentence via the byte-pair-encoding (BPE) (Sennrich et al., 2016b) algorithm. We used the subword-nmt implementation6 and then applied BPE to splitting both source and target texts. The number of merge operations was set to 8,000. 270 Evaluation To investigate the effectiveness of the proposed method, we followed the work by Mita et al. (2019) and evaluated the performance of the GEC models across various GEC datasets in terms of the same evaluation metrics. We report 5 https://spacy.io/ https://github.com/rsennrich/ subword-nmt 6 Dataset #sent (pairs) Split BEA-train EFCamDAT Lang-8 BEA-valid CoNLL-2014 JFLEG BEA-test 561,100 train 2,269,595 train 5,689,2"
2020.findings-emnlp.26,N19-1014,0,0.0156961,"e of an inappropriately corrected error and an unchanged error in EFCamDat. We consider these types of errors to be dataset noise that might hinder GEC model performance. Introduction Grammatical error correction (GEC) is often considered a variant of machine translation (MT) (Brockett et al., 2006; Junczys-Dowmunt et al., 2018) due to their structural similarity–“translating” from source ungrammatical text to target grammatical text. At present, several neural encoderdecoder (EncDec) approaches have been introduced for this task and have achieved remarkable results (Chollampatt and Ng, 2018; Zhao et al., 2019; Kiyono et al., 2019). EncDec models tend to further improve in performance with increasing data size (Koehn and Knowles, 2017; Sennrich and Zhang, 2019), however, this is not necessarily true in GEC. For example, Lo et al. (2018) reported that an EncDec-based GEC model trained on EFCamDat (Geertzen et al., 2013)1 , the largest publicly available learner corpus as of today (two million sentence pairs), was outperformed by a model trained on a smaller dataset (e.g., 720K pairs). They hypothesized that this may be due to the noisiness of EFCamDat, i.e., the presence of sentence pairs whose corr"
2020.findings-emnlp.26,P19-1021,0,0.0228999,"r GEC model performance. Introduction Grammatical error correction (GEC) is often considered a variant of machine translation (MT) (Brockett et al., 2006; Junczys-Dowmunt et al., 2018) due to their structural similarity–“translating” from source ungrammatical text to target grammatical text. At present, several neural encoderdecoder (EncDec) approaches have been introduced for this task and have achieved remarkable results (Chollampatt and Ng, 2018; Zhao et al., 2019; Kiyono et al., 2019). EncDec models tend to further improve in performance with increasing data size (Koehn and Knowles, 2017; Sennrich and Zhang, 2019), however, this is not necessarily true in GEC. For example, Lo et al. (2018) reported that an EncDec-based GEC model trained on EFCamDat (Geertzen et al., 2013)1 , the largest publicly available learner corpus as of today (two million sentence pairs), was outperformed by a model trained on a smaller dataset (e.g., 720K pairs). They hypothesized that this may be due to the noisiness of EFCamDat, i.e., the presence of sentence pairs whose correction still contained grammatical errors due to inappropriate edits or being left uncorrected. For example, in Table 1, “discuss about” should most likel"
2020.findings-emnlp.26,P12-2039,0,0.0281947,"Missing"
2020.findings-emnlp.26,W19-8639,0,0.0280445,")}ni=1 , we generate new target sentences Yˆi from the original target sentences Yi and pair them with their original source sentences Xi (line 4 in Algorithm 1). The consistency of the base model predictions ensures that the ˆ = {(Xi , Yˆi )}n contain resulting parallel data D i=1 noise at a less extent. It is worth noting that SR can be regarded as a variant of self-training due to its structural similarity, except that it takes the target sentences rather than the source sentences as input to the model. The algorithm itself is the key difference from existing methods based on selftraining (Wang, 2019; Nie et al., 2019; Xie et al., 2020). One challenge of this approach is that the base model may consistently make inaccurate corrections. We thus incorporate a fail-safe mechanism Experiments We evaluate the proposed method in two ways. First, we exclusively focus on investigating the effectiveness of the proposed denoising method (Section 5.3). Then, we compare our strongest model trained with denoised data (henceforth, denoised model), with current best-performing ones to investigate whether the proposed method has a complementary effect on existing task-specific techniques (Section 5.4). 5"
2020.lantern-1.3,D19-1243,0,0.0468409,"Missing"
2020.lantern-1.3,Q18-1023,0,0.0531252,"Missing"
2020.lantern-1.3,N16-1023,0,0.02877,"tennis ball Figure 1: Example of three selected and one removed commonsense questions from two MCScript2.0 instances. questions about common properties and locations of objects that it previously answered incorrectly. The ultimate goal of our work is to discover an alternative to the expensive (in terms of time) and limited (in terms of coverage) crowdsourced-commonsense acquisition approach. 2 Related work Knowledge extraction. Previous works have already recognized the rich content of computer vision datasets and investigated its benefits for commonsense knowledge extraction. For instance, Yatskar et al. (2016) and Mukuze et al. (2018) derived 16K commonsense relations and 2,000 verb/location pairs (e.g., holds(dining-table, cutlery), eat/restaurant) from the annotations included in the Microsoft Common Objects in Context dataset (Lin et al., 2014) (MS-COCO). However, they only focused on physical commonsense. A more recent trend is to query LMs for commonsense facts. While a robust LM like BERT has shown a strong performance retrieving commonsense knowledge at a similar level to factual knowledge (Petroni et al., 2019), this seems to happen only when that knowledge is explicitly written down (Forbe"
2020.lrec-1.42,P16-1070,0,0.0141724,"ecome available to the public. Now, a wide variety of them are available. Examples are corpora annotated with grammatical errors and/or spelling errors as in a part of NICT JLE, CLC FCE (Yannakoudakis et al., 2011), KJ (Nagata et al., 2011), and the CoNLL shared-task datasets (Ng et al., 2013; Ng et al., 2014). Related to these are parallel corpora where the original and its corrected sentences are paired as in the Lang-8 learner corpus (Lang-8) (Mizumoto et al., 2012) and JFLEG (Napoles et al., 2017). Another direction is syntactic annotation as in the work by Nagata and Sakaguchi (2016) and Berzak et al. (2016). In contrast, there have been almost no publicly available corpora annotated with feedback comments despite the fact that they are essential for research in feedback comment generation. An exception is the dataset reported in the work (Nagata, 2019). The reason why datasets for feedback comment generation are rare is that it is highly costly lines at ICNALE Learner Essays with Feedback Comments (https://www.gsk.or.jp/en/catalog/gsk2019-b) and Konan-JIEM Learner Corpus Sixth Edition (https://www.gsk.or.jp/en/catalog/gsk2019-a) 2 The details of the shared task are available at https://fcg.share"
2020.lrec-1.42,de-marneffe-etal-2006-generating,0,0.23502,"Missing"
2020.lrec-1.42,P00-1068,0,0.303632,"decide what sort of feedback comment to give. In English language teaching research, there is a great amount of work on how to correct grammatical errors as in the work by (Robb et al., 1986; Ferris and Roberts, 2001) but little work on what information we should provide as feedback comments and how; as far as we know, Bitchener et al. (2005) show that error correction can signiﬁcantly improve learners’ writing accuracy levels when combined with feedback messages3 . Because of this data limitation, there has been only a small amount of work on feedback comment generation. Some researchers as Kakegawa et al. (2000) and McCoy et al. (1996) made an attempt to develop handcrafted rules to diagnose errors. Nagata et al. (2014) proposed to use automatically extracted case frames to correct preposition errors with explanations. These approaches encounter the difﬁculty of covering a wide variety of errors. More recently, Lai and Chang (2019) proposed a method that uses grammatical error correction and templates to generate detailed comments. Nagata (2019) reports on performance of a neural retrieval-based method on their dataset. 3. Corpus Design and Guidelines 3.1. Target Corpus As already mentioned in Sect."
2020.lrec-1.42,D19-3040,0,0.248771,"ner et al. (2005) show that error correction can signiﬁcantly improve learners’ writing accuracy levels when combined with feedback messages3 . Because of this data limitation, there has been only a small amount of work on feedback comment generation. Some researchers as Kakegawa et al. (2000) and McCoy et al. (1996) made an attempt to develop handcrafted rules to diagnose errors. Nagata et al. (2014) proposed to use automatically extracted case frames to correct preposition errors with explanations. These approaches encounter the difﬁculty of covering a wide variety of errors. More recently, Lai and Chang (2019) proposed a method that uses grammatical error correction and templates to generate detailed comments. Nagata (2019) reports on performance of a neural retrieval-based method on their dataset. 3. Corpus Design and Guidelines 3.1. Target Corpus As already mentioned in Sect. 1., ICNALE and KJ are our base corpora. ICNALE has several suitable properties for this task. In particular, its essay topics are well-controlled, which enables us to simulate a situation common to language teaching and learning where all learners write on the same topic as in writing exercises in class and writing tasks in"
2020.lrec-1.42,C12-2084,0,0.0155435,"2004), and ICNALE, to name a few. They are crucial sources of information about learner language. Since around 2000, annotated learner corpora have become available to the public. Now, a wide variety of them are available. Examples are corpora annotated with grammatical errors and/or spelling errors as in a part of NICT JLE, CLC FCE (Yannakoudakis et al., 2011), KJ (Nagata et al., 2011), and the CoNLL shared-task datasets (Ng et al., 2013; Ng et al., 2014). Related to these are parallel corpora where the original and its corrected sentences are paired as in the Lang-8 learner corpus (Lang-8) (Mizumoto et al., 2012) and JFLEG (Napoles et al., 2017). Another direction is syntactic annotation as in the work by Nagata and Sakaguchi (2016) and Berzak et al. (2016). In contrast, there have been almost no publicly available corpora annotated with feedback comments despite the fact that they are essential for research in feedback comment generation. An exception is the dataset reported in the work (Nagata, 2019). The reason why datasets for feedback comment generation are rare is that it is highly costly lines at ICNALE Learner Essays with Feedback Comments (https://www.gsk.or.jp/en/catalog/gsk2019-b) and Konan"
2020.lrec-1.42,P16-1173,1,0.842807,"annotated learner corpora have become available to the public. Now, a wide variety of them are available. Examples are corpora annotated with grammatical errors and/or spelling errors as in a part of NICT JLE, CLC FCE (Yannakoudakis et al., 2011), KJ (Nagata et al., 2011), and the CoNLL shared-task datasets (Ng et al., 2013; Ng et al., 2014). Related to these are parallel corpora where the original and its corrected sentences are paired as in the Lang-8 learner corpus (Lang-8) (Mizumoto et al., 2012) and JFLEG (Napoles et al., 2017). Another direction is syntactic annotation as in the work by Nagata and Sakaguchi (2016) and Berzak et al. (2016). In contrast, there have been almost no publicly available corpora annotated with feedback comments despite the fact that they are essential for research in feedback comment generation. An exception is the dataset reported in the work (Nagata, 2019). The reason why datasets for feedback comment generation are rare is that it is highly costly lines at ICNALE Learner Essays with Feedback Comments (https://www.gsk.or.jp/en/catalog/gsk2019-b) and Konan-JIEM Learner Corpus Sixth Edition (https://www.gsk.or.jp/en/catalog/gsk2019-a) 2 The details of the shared task are avail"
2020.lrec-1.42,P11-1121,1,0.885575,"escribe. This is largely due to the fact that there had been no publicly available corpus annotated with feedback comments. Corpora annotated with feedback comments will likely facilitate the research in this domain just as the common datasets in grammatical error correction such as the CoNLL shared-task datasets (Ng et al., 2013; Ng et al., 2014) did. Recently, Nagata (2019) has released a dataset consisting of learner corpora manually annotated with feedback comments on preposition use; their target corpora are the two existing corpora — the written essays in ICNALE (Ishikawa, 2013) and KJ (Nagata et al., 2011). In this work, we extended the dataset by manually annotating the two learner corpora with feedback comments in general and those on preposition use (hereafter, general and preposition feedback comments, respectively). We also extended guidelines for this annotation. We annotated approximately 3,000 and 2,400 essays with 50,000 general and 6,700 preposition feedback comments, respectively. We released a part of these annotated corpora to the public on the web1 . We are now planning to organize a feedback 1 The corpus data are available together with the guidecomment generation shared task in"
2020.lrec-1.42,P14-1071,1,0.750827,"work on how to correct grammatical errors as in the work by (Robb et al., 1986; Ferris and Roberts, 2001) but little work on what information we should provide as feedback comments and how; as far as we know, Bitchener et al. (2005) show that error correction can signiﬁcantly improve learners’ writing accuracy levels when combined with feedback messages3 . Because of this data limitation, there has been only a small amount of work on feedback comment generation. Some researchers as Kakegawa et al. (2000) and McCoy et al. (1996) made an attempt to develop handcrafted rules to diagnose errors. Nagata et al. (2014) proposed to use automatically extracted case frames to correct preposition errors with explanations. These approaches encounter the difﬁculty of covering a wide variety of errors. More recently, Lai and Chang (2019) proposed a method that uses grammatical error correction and templates to generate detailed comments. Nagata (2019) reports on performance of a neural retrieval-based method on their dataset. 3. Corpus Design and Guidelines 3.1. Target Corpus As already mentioned in Sect. 1., ICNALE and KJ are our base corpora. ICNALE has several suitable properties for this task. In particular, i"
2020.lrec-1.42,D19-1316,1,0.6794,"if one can automatically generate natural and effective feedback comments with grammatical error correction results. Unfortunately, however, there has been a very limited amount of work on feedback comment generation as Sect. 2. will describe. This is largely due to the fact that there had been no publicly available corpus annotated with feedback comments. Corpora annotated with feedback comments will likely facilitate the research in this domain just as the common datasets in grammatical error correction such as the CoNLL shared-task datasets (Ng et al., 2013; Ng et al., 2014) did. Recently, Nagata (2019) has released a dataset consisting of learner corpora manually annotated with feedback comments on preposition use; their target corpora are the two existing corpora — the written essays in ICNALE (Ishikawa, 2013) and KJ (Nagata et al., 2011). In this work, we extended the dataset by manually annotating the two learner corpora with feedback comments in general and those on preposition use (hereafter, general and preposition feedback comments, respectively). We also extended guidelines for this annotation. We annotated approximately 3,000 and 2,400 essays with 50,000 general and 6,700 prepositi"
2020.lrec-1.42,E17-2037,0,0.0122386,"They are crucial sources of information about learner language. Since around 2000, annotated learner corpora have become available to the public. Now, a wide variety of them are available. Examples are corpora annotated with grammatical errors and/or spelling errors as in a part of NICT JLE, CLC FCE (Yannakoudakis et al., 2011), KJ (Nagata et al., 2011), and the CoNLL shared-task datasets (Ng et al., 2013; Ng et al., 2014). Related to these are parallel corpora where the original and its corrected sentences are paired as in the Lang-8 learner corpus (Lang-8) (Mizumoto et al., 2012) and JFLEG (Napoles et al., 2017). Another direction is syntactic annotation as in the work by Nagata and Sakaguchi (2016) and Berzak et al. (2016). In contrast, there have been almost no publicly available corpora annotated with feedback comments despite the fact that they are essential for research in feedback comment generation. An exception is the dataset reported in the work (Nagata, 2019). The reason why datasets for feedback comment generation are rare is that it is highly costly lines at ICNALE Learner Essays with Feedback Comments (https://www.gsk.or.jp/en/catalog/gsk2019-b) and Konan-JIEM Learner Corpus Sixth Editio"
2020.lrec-1.42,W13-3601,0,0.142335,"error correction as language learning assistance if one can automatically generate natural and effective feedback comments with grammatical error correction results. Unfortunately, however, there has been a very limited amount of work on feedback comment generation as Sect. 2. will describe. This is largely due to the fact that there had been no publicly available corpus annotated with feedback comments. Corpora annotated with feedback comments will likely facilitate the research in this domain just as the common datasets in grammatical error correction such as the CoNLL shared-task datasets (Ng et al., 2013; Ng et al., 2014) did. Recently, Nagata (2019) has released a dataset consisting of learner corpora manually annotated with feedback comments on preposition use; their target corpora are the two existing corpora — the written essays in ICNALE (Ishikawa, 2013) and KJ (Nagata et al., 2011). In this work, we extended the dataset by manually annotating the two learner corpora with feedback comments in general and those on preposition use (hereafter, general and preposition feedback comments, respectively). We also extended guidelines for this annotation. We annotated approximately 3,000 and 2,400"
2020.lrec-1.42,W14-1701,0,0.0807344,"as language learning assistance if one can automatically generate natural and effective feedback comments with grammatical error correction results. Unfortunately, however, there has been a very limited amount of work on feedback comment generation as Sect. 2. will describe. This is largely due to the fact that there had been no publicly available corpus annotated with feedback comments. Corpora annotated with feedback comments will likely facilitate the research in this domain just as the common datasets in grammatical error correction such as the CoNLL shared-task datasets (Ng et al., 2013; Ng et al., 2014) did. Recently, Nagata (2019) has released a dataset consisting of learner corpora manually annotated with feedback comments on preposition use; their target corpora are the two existing corpora — the written essays in ICNALE (Ishikawa, 2013) and KJ (Nagata et al., 2011). In this work, we extended the dataset by manually annotating the two learner corpora with feedback comments in general and those on preposition use (hereafter, general and preposition feedback comments, respectively). We also extended guidelines for this annotation. We annotated approximately 3,000 and 2,400 essays with 50,00"
2020.lrec-1.42,P11-1019,0,0.0368903,"uage Processing (NLP) techniques related to learner language including grammatical error correction and automated essay scoring; In the beginning, raw learner corpora made their appearance to the public. Examples are: ICLE (Granger, 1993), NICT JLE (Izumi et al., 2004), and ICNALE, to name a few. They are crucial sources of information about learner language. Since around 2000, annotated learner corpora have become available to the public. Now, a wide variety of them are available. Examples are corpora annotated with grammatical errors and/or spelling errors as in a part of NICT JLE, CLC FCE (Yannakoudakis et al., 2011), KJ (Nagata et al., 2011), and the CoNLL shared-task datasets (Ng et al., 2013; Ng et al., 2014). Related to these are parallel corpora where the original and its corrected sentences are paired as in the Lang-8 learner corpus (Lang-8) (Mizumoto et al., 2012) and JFLEG (Napoles et al., 2017). Another direction is syntactic annotation as in the work by Nagata and Sakaguchi (2016) and Berzak et al. (2016). In contrast, there have been almost no publicly available corpora annotated with feedback comments despite the fact that they are essential for research in feedback comment generation. An exce"
2020.sustainlp-1.6,P07-1056,0,0.335774,"Missing"
2020.sustainlp-1.6,N19-1423,0,0.0110327,"Networks, Inc.2 RIKEN3 sosk@preferred.jp {yokoi,jun.suzuki,inui}@ecei.tohoku.ac.jp Abstract method, which (i) is computationally more efficient while (ii) useful for applications (iii) without significant sacrifice of model performance. We propose a trick for enabling a neural network without restrictions to estimate the influence, which we refer to as turn-over dropout. This method is computationally efficient as it requires only running two forward computations after training a single model on the entire training dataset. In addition to the efficiency, we demonstrated that it enabled BERT (Devlin et al., 2019) and VGGNet (Simonyan and Zisserman, 2015) to analyze the influences of training through various experiments, including example-based interpretation of error predictions and data cleansing to improve the accuracy on a test set with a distributional shift. Understanding the influence of a training instance on a neural network model leads to improving interpretability. However, it is difficult and inefficient to evaluate the influence, which shows how a model’s prediction would be changed if a training instance were not used. In this paper, we propose an efficient method for estimating the influ"
2020.sustainlp-1.6,P16-1162,0,0.0176054,"etwork models are notorious for their black-box prediction, which harms the trust and usability (Ribeiro et al., 2016). The influence estimation can mitigate this problem by suggesting possible reasons for a wrong model prediction by identifying influential training instances. To verify this benefit, we collected the misclassified instances of the validation or test set and searched for the training instances that most influenced the wrong predictions. Figure 3 indicates a text example from the results. Rare words of named entities were divided into many subwords (Schuster and Nakajima, 2012; Sennrich et al., 2016; Wu et al., 2016) and requiring more complex processing. A guess is that BERT might fail to understand the input due to the cluttered subwords, and predict a wrong label, which depended on a training instance similarly with many subwords. Additionally, we conducted the same experiment on Yahoo Answers 10-label question classification dataset (Zhang et al., 2015)8 , which is more complex than sentiment analysis. Figure 4 shows the results on Yahoo Answers. The misclassified text shares the phrase “ch ##rist” with the two influenSanity Check: Learning Curves We first observed an interesting pro"
2020.sustainlp-1.6,2020.acl-main.492,0,0.0206173,"t influenced by z. Our estimation uses the difference between the two sub-networks. prediction on another instance ztarget . The instance of interest ztarget is typically an instance in a test or validation dataset. 2.2 3 Related Methods 3.1 Computing the influence in Equation (1) by retraining two models for each instance is computationally expensive, and several estimation methods are proposed. Koh and Liang (2017) proposed an estimation method that assumed a strongly convex loss function and a global optimal solution1 . While the method is used even with neural models (Koh and Liang, 2017; Han et al., 2020), which do not satisfy the assumption, it still requires high computational cost. Hara et al. (2019) proposed a method without these restrictions; however, it consumes large disk storage and computation time that depend on the number of optimization steps. Our proposed method is much more efficient, as shown in Table 1. For example, in a case where Koh and Liang (2017)’s method took 10 minutes to estimate the influences of 10,000 training instances on another instance with BERT (Han et al., 2020), our method only required 35 seconds2 . This efficiency will expand the scope of applications of c"
2020.sustainlp-1.6,D13-1170,0,0.00361024,"D{zi } 4 . From this analogy, the influence of a training instance can be evaluated by considering these two sub-networks. The influence I(ztarget , zi ; D) = L(fD{zi } , ztarget ) L(fD , ztarget ) is estimated as ˆ target , zi ; D) I(z f m(zi ) := L(fD , ztarget ) m(zi ) L(fD The computational efficiency of our method is discussed in Section 2. Moreover, we answer a question: even if it is efficient, does it work well on applications? To demonstrate the applicability, we conducted experiments using different models and datasets. Setup First, we used the Stanford Sentiment TreeBank (SST-2) (Socher et al., 2013) binary sentiment classification task. Five thousand instances were sampled from the training set, and 872 instances in the development set were used. We trained BERT-base classifiers (Wolf et al., 2019) with the adapter modules (Houlsby et al., 2019), which froze the pre-trained BERT parameters but newly trained branch networks in addition to the output layers. We applied the turn-over dropout on the adapter modules and output layers. In addition, we used the CIFAR-10 (Krizhevsky, 2009) 10-class image classification task, with the 50,000 training instances and 10,000 validation instances. We"
2020.sustainlp-1.6,N18-1101,0,0.0942795,"Missing"
2020.sustainlp-1.6,2020.acl-demos.14,0,0.0208975,"Missing"
2020.sustainlp-1.6,N16-3020,0,0.0880143,"bout the decrease of model performances when using turn-over dropout. While we experimented with the successful architectures only, exploring the side effect in various architectures and its remedy is important future work. 4.2 Figure 4: A misclassified text in the test set and the texts with the highest influence with the error label in the training set for BERT on Yahoo Answers. ing the flipped mask does not learn each training instance ztrain . 4.3 Interpretation of Error of Predictions Neural network models are notorious for their black-box prediction, which harms the trust and usability (Ribeiro et al., 2016). The influence estimation can mitigate this problem by suggesting possible reasons for a wrong model prediction by identifying influential training instances. To verify this benefit, we collected the misclassified instances of the validation or test set and searched for the training instances that most influenced the wrong predictions. Figure 3 indicates a text example from the results. Rare words of named entities were divided into many subwords (Schuster and Nakajima, 2012; Sennrich et al., 2016; Wu et al., 2016) and requiring more complex processing. A guess is that BERT might fail to unde"
2021.acl-long.405,P18-1254,0,0.0490997,"Missing"
2021.acl-long.405,W12-1706,0,0.376094,"uniform information density. Overall, our results suggest that a crosslingual evaluation will be necessary to construct human-like computational models. 1 Introduction It is well known that the probability of a word in context (i.e., surprisal) impacts its processing difficulty in incremental human language comprehension (Hale, 2001; Demberg and Keller, 2008; Levy, 2008; Smith and Levy, 2013). Building on this basis, researchers have compared a variety of language models (LMs) in terms of how well their surprisal correlates with human reading behavior (Roark et al., 2009; Frank and Bod, 2011; Fossum and Levy, 2012; Hale et al., 2018; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Merkx and Frank, 2020; Wilcox et al., 2020). Such investigations could provide insights into the development of a general computational model of human language processing. For example, recent studies reported that LMs with better performance for next-word prediction could also better predict the human reading behavior (i.e. more humanlike) (Fossum and Levy, 2012; Goodkind and Bicknell, 2018; Wilcox et al., 2020). In this paper, we re-examine whether the recent findings on human-like computational models can be genera"
2021.acl-long.405,D09-1034,0,0.251632,"r explored from the perspective of (non-)uniform information density. Overall, our results suggest that a crosslingual evaluation will be necessary to construct human-like computational models. 1 Introduction It is well known that the probability of a word in context (i.e., surprisal) impacts its processing difficulty in incremental human language comprehension (Hale, 2001; Demberg and Keller, 2008; Levy, 2008; Smith and Levy, 2013). Building on this basis, researchers have compared a variety of language models (LMs) in terms of how well their surprisal correlates with human reading behavior (Roark et al., 2009; Frank and Bod, 2011; Fossum and Levy, 2012; Hale et al., 2018; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Merkx and Frank, 2020; Wilcox et al., 2020). Such investigations could provide insights into the development of a general computational model of human language processing. For example, recent studies reported that LMs with better performance for next-word prediction could also better predict the human reading behavior (i.e. more humanlike) (Fossum and Levy, 2012; Goodkind and Bicknell, 2018; Wilcox et al., 2020). In this paper, we re-examine whether the recent findings on h"
2021.acl-long.405,P16-1162,0,0.00660288,"sahara et al., 2016) and was extensively annotated with various linguistic properties (Asahara and Kato, 2017; Asahara, 2017, 2018). 3 Methods This section describes the settings of LMs, eye movement data, and evaluation metrics. 3.1 Language models A variety of sentence-level, left-to-right sequential LMs was used. Training data of English LMs: We used the WikiText-103 dataset to train the English LMs. Based on the reports that subword-level English LMs exhibits superior psychometric predictive power (Wilcox et al., 2020), input texts were divided into subwords by a byte-pair encoding (BPE) (Sennrich et al., 2016).2 The training data consist of approximately 4M sentences (114M subwords units). Training data size: For each neural LM architecture (T RANS - LG, T RANS - SM, and LSTM), three variants were trained using different training data sizes: LG (full training data), MD (1/10 training data), and SM (1/100 training data). The N-gram LMs were trained on LG datasets. Number of updates: The parameters of each neural LM were saved at four different points during training: 100, 1K, 10K, and 100K parameter updates. To summarize, 39 LM training settings were attained for each language (3 architectures × 3 d"
2021.argmining-1.6,W16-2815,0,0.053599,"Missing"
2021.argmining-1.6,P11-1099,0,0.028453,"keywords are derived from the original argument (i.e., claim and premise). Our assumption follows the formal definition of warrants in the sense that warrants act as a inferential link between the contents of claim and its premise (Toulmin, 1958; Freeman, 1992). further research in warrant explication, we publish our crowdsourcing guidelines and the preliminary corpus of around 1700 warrants that are annotated via UKW methodology, covering over 600 arguments 1 . 2 Related Work Explication of warrants in arguments has already been approached in many previous researches. In an initial attempt, Feng and Hirst (2011) proposed leveraging argumentation schemes (Walton et al., 2008) as a means to automatically reconstruct warrant, but did not approach the task due to the absence of training datasets. To overcome the unavailability of dataset, Boltuži´c and Šnajder (2016) leverage crowdsourcing and ask non-expert workers to annotate all possible variations of warrants for a given claim-premise pair. However, they concluded that the annotation of warrants varied both in number annotated per argument and in content due to no restrictions imposed on in the annotation process. In order to overcome the prior diffi"
2021.argmining-1.6,S18-1121,0,0.0278032,"To overcome the unavailability of dataset, Boltuži´c and Šnajder (2016) leverage crowdsourcing and ask non-expert workers to annotate all possible variations of warrants for a given claim-premise pair. However, they concluded that the annotation of warrants varied both in number annotated per argument and in content due to no restrictions imposed on in the annotation process. In order to overcome the prior difficulties, recent approaches leverage crowdsourcing to restrict the number of warrants collected per argument and either employ a step by step filtering process to weed out bad warrants (Habernal et al., 2018) or hire exMany previous work demonstrated different strategies of collecting high quality warrants (Becker et al., 2017; Habernal et al., 2018; Becker et al., 2020), but did not apply any restriction on the structure of warrants to handle the variety of reasoning patterns in which warrants can be explicated. In contrast, our annotation methodologies are designed to restrict the reasoning pattern and ensure that warrant explicates the reasoning link between claim and premise. In order to evaluate the warrants annotated via our proposed UKW and PKW methodologies, we devise specific guidelines t"
2021.eacl-main.153,P19-1279,0,0.0243558,"nce some types of graphs are easier to memorize for a LM than others (See Appx. G). While we use language like “train a LM to memorize statements” for simplicity throughout this work, what we do in case of pretrained LMs is more akin to adaptive pretraining (Gururangan et al., 2020). It is possible that integrating entity supervision directly into LM pretraining (F´evry et al., 2020) allows more efficient fact storage. Our analysis was focused on entity representations and ignored the question of how to represent relation predicates or entire relation triples. Here, relation learning (Baldini Soares et al., 2019) and LM pretraining on fact-aligned corpora (Elsahar et al., 2018) are avenues for future work. Finally, we formulated the LM-as-KB paradigm in terms of storing and retrieving relation triples. While structured KBs such as Wikidata consist of such triples and hence our experiments showing storage and retrieval of triples LMs are sufficient as a proof-of-concept in principle, structured KBs allow more complex queries than the ones considered here, such as 1-to-n relations, multihop inference, queries involving numerical ranges, or facts qualified by time and location (Hoffart et al., 2013). Con"
2021.eacl-main.153,2020.acl-main.463,0,0.0182682,"s. Knowledge bases, such as Wikidata, typically contain thousands of predicates, millions of entities, and billions of relations. 2 World Knowledge in Language Models Large pretrained LMs have been the driver of recent progress in natural language processing (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2019; Devlin et al., 2019). While the trend towards larger LMs is likely to continue (Raffel et al., 2019; Kaplan et al., 2020; Brown et al., 2020), it has limitations: (i) A model trained only on text lacks grounding in perception and experience and hence cannot learn meaning (Bender and Koller, 2020). (ii) Reporting bias leads to certain knowledge rarely or never being expressed in text. For example, a LM will easily learn to associate the phrase “Barack Obama” with the phrase “U.S. President”, but might less likely learn that he is a “human being”, since the latter fact is rarely stated explicitly in text. In contrast, this type of knowledge is readily available in KBs. (iii) A large number of rare entities (Hoffart et al., 2014; Derczynski et al., 2017; Ilievski et al., 2018) are, by definition, rarely mentioned, making it difficult for 1 Code available at: https://github.com/bheinzerli"
2021.eacl-main.153,D13-1160,0,0.161565,"Missing"
2021.eacl-main.153,P17-1171,0,0.060545,"Missing"
2021.eacl-main.153,W17-4418,0,0.0458047,"Missing"
2021.eacl-main.153,N19-1423,0,0.523642,"ving a large number of entities and (ii) the ability to query stored facts. We explore three entity representations that allow LMs to handle millions of entities and present a detailed case study on paraphrased querying of facts stored in LMs, thereby providing a proof-of-concept that language models can indeed serve as knowledge bases. 1 Figure 1: The LM-as-KB paradigm, introduced by Petroni et al. (2019). A LM memorizes factual statements, which can be queried in natural language. Introduction Language models (LMs) appear to memorize world knowledge facts during training. For example, BERT (Devlin et al., 2019) correctly answers the query “Paris is the capital of [MASK]” with “France”. This observation prompted Petroni et al. (2019) to ask if LMs can serve as an alternative or complement to structured knowledge bases (KBs), thereby introducing the idea of treating LMs as KBs: During training, the LM encounters world knowledge facts expressed in its training data, some of which are stored in some form in the LM’s parameters. After training, some of the stored facts can be recovered from the LM’s parameters by means of a suitable natural language query (Fig. 1). A LM with such a “built-in” KB is usefu"
2021.eacl-main.153,L18-1544,0,0.0282184,"ers (See Appx. G). While we use language like “train a LM to memorize statements” for simplicity throughout this work, what we do in case of pretrained LMs is more akin to adaptive pretraining (Gururangan et al., 2020). It is possible that integrating entity supervision directly into LM pretraining (F´evry et al., 2020) allows more efficient fact storage. Our analysis was focused on entity representations and ignored the question of how to represent relation predicates or entire relation triples. Here, relation learning (Baldini Soares et al., 2019) and LM pretraining on fact-aligned corpora (Elsahar et al., 2018) are avenues for future work. Finally, we formulated the LM-as-KB paradigm in terms of storing and retrieving relation triples. While structured KBs such as Wikidata consist of such triples and hence our experiments showing storage and retrieval of triples LMs are sufficient as a proof-of-concept in principle, structured KBs allow more complex queries than the ones considered here, such as 1-to-n relations, multihop inference, queries involving numerical ranges, or facts qualified by time and location (Hoffart et al., 2013). Conclusions. We gave a positive answer to Petroni et al. (2019)‘s que"
2021.eacl-main.153,2020.emnlp-main.400,0,0.222158,"Missing"
2021.eacl-main.153,N16-1150,0,0.0673693,"Missing"
2021.eacl-main.153,K19-1049,0,0.041084,"Missing"
2021.eacl-main.153,C18-1056,0,0.0277019,"(i) A model trained only on text lacks grounding in perception and experience and hence cannot learn meaning (Bender and Koller, 2020). (ii) Reporting bias leads to certain knowledge rarely or never being expressed in text. For example, a LM will easily learn to associate the phrase “Barack Obama” with the phrase “U.S. President”, but might less likely learn that he is a “human being”, since the latter fact is rarely stated explicitly in text. In contrast, this type of knowledge is readily available in KBs. (iii) A large number of rare entities (Hoffart et al., 2014; Derczynski et al., 2017; Ilievski et al., 2018) are, by definition, rarely mentioned, making it difficult for 1 Code available at: https://github.com/bheinzerling/lm-as-kb 1773 3 Entity Representations How can millions of entities be represented in a LM? To answer our first question, we compare three types of entity representations: symbolic, surface form, and continuous. Experimental setup. We evaluate entity representations by measuring how well they allow a LM to store and retrieve world knowledge facts. For example, if the LM’s training data contains the statement “Bert is a character on Sesame Street”, the model should memorize this s"
2021.eacl-main.153,2020.tacl-1.28,0,0.315618,"Missing"
2021.eacl-main.153,K18-1050,0,0.24393,"Missing"
2021.eacl-main.153,2021.ccl-1.108,0,0.0755972,"Missing"
2021.eacl-main.153,P19-1598,0,0.0241,"retrieving a surface form which may or may not correspond to an entity. 1.0 Memorization accuracy LMs to acquire knowledge about this long tail of entities from text alone. These limitations have motivated efforts to explicitly2 equip LMs with world knowledge. Table 2 (Appx. A) situates these efforts on a spectrum from purely text-based LMs to representations of structured KBs. Models based on text generation (Raffel et al., 2019; Roberts et al., 2020) and retrieval (Guu et al., 2020) have proven most successful in knowledge-intensive tasks. However, we argue that models which reify entities (Logan et al., 2019), i.e., models in which entities are “first-class citizens” that can be directly predicted3 , are a promising research direction, since the direct links into a KB can be seen as a form of grounding. This is one of our main motivations for considering symbolic and continuous entity representations. 0.8 0.6 0.4 LSTM 1024 LSTM 256 RoBERTa-base RoBERTa-base without pretraining 0.2 0.0 0 1M 2M 3M Number of statements 4M 5M Figure 2: Accuracy of statement memorization with symbolic representation of 1 million entities. from the sets defined above for which this relation holds.4 To make queries for a"
2021.eacl-main.153,W19-4319,1,0.876677,"Missing"
2021.eacl-main.153,P17-1079,0,0.0371746,"Missing"
2021.eacl-main.153,N19-4009,0,0.0263153,"achieved with continuous representation. Like surface representation, continuous representation scales to 6 million entities, and we see the same relative order of models, but with overall lower accuracies. RoBERTa without pretraining has the highest capacity for storing world knowledge statements, memorizing 67 percent of 10 million statements, while the small LSTM 256 model has the lowest capacity, memorizing 42 percent. Although far from fully understood, sequence-to-sequence architectures are relatively mature, with highly-optimized toolkits and hyperparameter settings publicly available (Ott et al., 2019). In contrast, prediction of continuous representations is still in an early stage of research (Kumar and Tsvetkov, 2019). We therefore see these results as lower bounds for LM capacity with continuous representations. By design, memorization with continuous representations does not rely on entity names, and hence, in contrast to surface form representation, does not lead to difficulties in handling entities with long names. However, as with surface form representation, infrequent entities are more difficult to memorize than frequent ones. Most of the memorization errors (Fig. 6, blue, left) i"
2021.eacl-main.153,N18-1202,0,0.0525705,"natural language statement, such as the English sentence Barack Obama was Born in Hawaii, or with a relation triple, such as hBarack Obama, wasBornIn, Hawaiii. A relation triple, or relation for short, consists of a subject entity (Barack Obama), a predicate (wasBornIn), and an object entity (Hawaii). A knowledge base is a set of relations. Knowledge bases, such as Wikidata, typically contain thousands of predicates, millions of entities, and billions of relations. 2 World Knowledge in Language Models Large pretrained LMs have been the driver of recent progress in natural language processing (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2019; Devlin et al., 2019). While the trend towards larger LMs is likely to continue (Raffel et al., 2019; Kaplan et al., 2020; Brown et al., 2020), it has limitations: (i) A model trained only on text lacks grounding in perception and experience and hence cannot learn meaning (Bender and Koller, 2020). (ii) Reporting bias leads to certain knowledge rarely or never being expressed in text. For example, a LM will easily learn to associate the phrase “Barack Obama” with the phrase “U.S. President”, but might less likely learn that he is a “human being”,"
2021.eacl-main.153,D19-1005,0,0.0638891,"Missing"
2021.eacl-main.153,D19-1250,0,0.0560348,"Missing"
2021.eacl-main.153,D18-1359,0,0.0238628,"solution is to make all embeddings the same, e.g., all-zero vectors. To prevent this, it is common practice to use negative samples (Bordes et al., 2013). When using fixed, pretrained embeddings as supervision signal, negative sampling is not necessary, since the target embeddings are not updated and therefore cannot become degenerate. Wikidata embeddings. We train embeddings for 6 million Wikidata entities using feature-specific autoencoders to encode entity features such as names, aliases, description, entity types, and numeric attributes, following prior work on multi-modal KB embeddings (Pezeshkpour et al., 2018) and KB embeddings with autoencoders (Takahashi et al., 2018). Embedding training is detailed in Appx. E. Results. Fig. 5 shows memorization accuracies achieved with continuous representation. Like surface representation, continuous representation scales to 6 million entities, and we see the same relative order of models, but with overall lower accuracies. RoBERTa without pretraining has the highest capacity for storing world knowledge statements, memorizing 67 percent of 10 million statements, while the small LSTM 256 model has the lowest capacity, memorizing 42 percent. Although far from ful"
2021.eacl-main.153,K16-1025,0,0.0621431,"Missing"
2021.eacl-main.153,D17-1197,0,0.059256,"Missing"
2021.eacl-main.153,P19-1139,0,0.0993545,"Missing"
2021.eacl-main.321,D18-1289,0,0.0224668,"Missing"
2021.eacl-main.321,E17-1013,0,0.118463,"textual edges, one can expect far more chances to find informative path evidences between any given target entity pairs, because the number of such textual edges is likely to be much larger than the number of KG edges (Note that one can collect as many textual edges as needed from a raw text corpus with an entity linker). Extending a KG to a UG, therefore, may allow a DS-RE model to learn richer distant supervision signals. The idea of using multi-hop paths over a UG is not necessarily new on its own. For example, Toutanova et al. (2015) propose to use a UG for knowledge graph completion, and Das et al. (2017b) propose a model trained to reason over a UG for question answering. However, there is no prior study that has explored the effective way to use a UG for the task of DS-RE from text. In fact, finding an effective way of using a UG for DS-RE is not as simple as it may seem. As we report in this paper, a straightforward extension of the Dai et al. (2019) model to the UG setting may result in performance degradation. Motivated by this, in this paper, we address how one can make effective use of UG for DS-RE. We 3674 first report our observation that a straightforward extension of the Dai et al."
2021.eacl-main.321,P17-2057,0,0.123399,"textual edges, one can expect far more chances to find informative path evidences between any given target entity pairs, because the number of such textual edges is likely to be much larger than the number of KG edges (Note that one can collect as many textual edges as needed from a raw text corpus with an entity linker). Extending a KG to a UG, therefore, may allow a DS-RE model to learn richer distant supervision signals. The idea of using multi-hop paths over a UG is not necessarily new on its own. For example, Toutanova et al. (2015) propose to use a UG for knowledge graph completion, and Das et al. (2017b) propose a model trained to reason over a UG for question answering. However, there is no prior study that has explored the effective way to use a UG for the task of DS-RE from text. In fact, finding an effective way of using a UG for DS-RE is not as simple as it may seem. As we report in this paper, a straightforward extension of the Dai et al. (2019) model to the UG setting may result in performance degradation. Motivated by this, in this paper, we address how one can make effective use of UG for DS-RE. We 3674 first report our observation that a straightforward extension of the Dai et al."
2021.eacl-main.321,D18-1245,0,0.13509,"is a relation and e2 is a tail entity, such as (aspirin, may treat, pain) and (Guy Maddin, place lived, Winnpeg). RE can be formulated as a classification task to predict a predefined relation r from entity pair (e1 , e2 ) annotated evidences. One obstacle that is encountered when building a RE system is the generation of a large amount of manually annotated training instances, which is exRecently, neural network models with attention mechanism have been proposed to alleviate the wrong labeling problem and attend informative sentence evidences such as (1a) (Lin et al., 2016; Ji et al., 2017; Du et al., 2018; Jat et al., 2018; Han et al., 2018a,b). However, there can be a large portion of entity pairs that lack such informative sentence evidences that explicitly express their relation. This makes Distantly Supervised Relation Extraction (DS-RE) further challenging (Sun et al., 2019). For compensating the lack of informative sentence evidences, Dai et al. (2019) utilize multihop paths connecting a target entity pair (hereafter, path) over a KG as extra evidences for DS-RE. An example of such multi-hop KG path can be seen in Figure 1, where p1 depicts a multi-hop KG path 3673 Proceedings of the 16t"
2021.eacl-main.321,D18-1247,0,0.101982,"ity, such as (aspirin, may treat, pain) and (Guy Maddin, place lived, Winnpeg). RE can be formulated as a classification task to predict a predefined relation r from entity pair (e1 , e2 ) annotated evidences. One obstacle that is encountered when building a RE system is the generation of a large amount of manually annotated training instances, which is exRecently, neural network models with attention mechanism have been proposed to alleviate the wrong labeling problem and attend informative sentence evidences such as (1a) (Lin et al., 2016; Ji et al., 2017; Du et al., 2018; Jat et al., 2018; Han et al., 2018a,b). However, there can be a large portion of entity pairs that lack such informative sentence evidences that explicitly express their relation. This makes Distantly Supervised Relation Extraction (DS-RE) further challenging (Sun et al., 2019). For compensating the lack of informative sentence evidences, Dai et al. (2019) utilize multihop paths connecting a target entity pair (hereafter, path) over a KG as extra evidences for DS-RE. An example of such multi-hop KG path can be seen in Figure 1, where p1 depicts a multi-hop KG path 3673 Proceedings of the 16th Conference of the European Chapter"
2021.eacl-main.321,P11-1055,0,0.0703995,"R Mintz 0.7 0.6 0.5 0.4 0.3 0.0 0.1 0.2 0.3 0.4 Recall 0.5 0.6 0.7 Figure 7: PR curves of previous state-of-the-art methods and our proposed model on NYT10 dataset. two datasets from different domains further proves the validity of our proposed methods. AUC and P@N Evaluation. Table 2 further presents the results in terms of AUC and P@N. From them, we have similar observation to the Comparison with State-of-the-art Baselines on NYT10. To demonstrate the effectiveness of our proposed model, we also compare it against the following baselines on NYT10 dataset: Mintz (Mintz et al., 2009), MultiR (Hoffmann et al., 2011), MIMLRE (Surdeanu et al., 2012), PCNN (Zeng et al., 2015), PCNN+ATT (Lin et al., 2016), BGWA (Jat et al., 2018), PCNN+HATT (Han et al., 2018b), RESIDE (Vashishth et al., 2018), DISTRE (Alt et al., 2019) and Sent+KG (Dai et al., 2019). The results shown in Figure 7 and Table 3 indicate that: (1) our selected base model, “Sent+KG”, is a strong baseline because it signif3680 System Mintz† PCNN+ATT† RESIDE† PCNN+HATT‡ DISTRE† Sent+KG Our Model AUC 10.7 34.1 41.5 42.0 42.2 50.2 55.0 P@0.1k 52.3 73.0 81.8 81.0 68.0 80.0 86.0 P@0.2k 50.2 68.0 75.4 79.5 67.0 82.0 84.0 P@0.3k 45.0 67.3 74.3 75.7 65.3"
2021.eacl-main.321,P19-1135,0,0.0355209,"Missing"
2021.eacl-main.321,P17-1004,0,0.0135511,"DS-RE. Han et al. (2018b) propose a relation hierarchy based attention mechanism. Han et al. (2018a) propose a joint model that adopts a KG embeddings based attention mechanism. Jia et al. (2019) propose an attention regularization framework to select informative sentence evidences for DS-RE. However, these models rely only on noisy sentence evidences from DS, neglecting the rich UG paths for DS-RE. Besides the sentence evidences from DS, researchers also leverage external evidences for DSRE. Ji et al. (2017) apply entity descriptions generated from Freebase and Wikipedia as extra evidences, Lin et al. (2017) utilize multilingual text as extra evidences and Vashishth et al. (2018) use multiple extra evidences including entity types, dependency and relation alias information for DS-RE. Alt et al. (2019) utilize pretrained language model 3 3.1 Base Model KG Encoder Suppose we have a KG containing a set of fact triplets O = {(e1 , r, e2 ), ...}, where each fact triplet consists of two entities e1 , e2 ∈ E and their relation r ∈ R. Here E and R stand for the set of entities and relations respectively. The KG Encoder then encodes e1 , e2 ∈ E and their relation r ∈ R into low-dimensional vectors h, t ∈"
2021.eacl-main.321,P16-1200,0,0.397826,"plets, where e1 is a head entity, r is a relation and e2 is a tail entity, such as (aspirin, may treat, pain) and (Guy Maddin, place lived, Winnpeg). RE can be formulated as a classification task to predict a predefined relation r from entity pair (e1 , e2 ) annotated evidences. One obstacle that is encountered when building a RE system is the generation of a large amount of manually annotated training instances, which is exRecently, neural network models with attention mechanism have been proposed to alleviate the wrong labeling problem and attend informative sentence evidences such as (1a) (Lin et al., 2016; Ji et al., 2017; Du et al., 2018; Jat et al., 2018; Han et al., 2018a,b). However, there can be a large portion of entity pairs that lack such informative sentence evidences that explicitly express their relation. This makes Distantly Supervised Relation Extraction (DS-RE) further challenging (Sun et al., 2019). For compensating the lack of informative sentence evidences, Dai et al. (2019) utilize multihop paths connecting a target entity pair (hereafter, path) over a KG as extra evidences for DS-RE. An example of such multi-hop KG path can be seen in Figure 1, where p1 depicts a multi-hop K"
2021.eacl-main.321,N13-1095,0,0.0281297,"e intermediate entities, each UG path consists of multiple hops and each hop represents a KG relation (such as “Colestipol may treat hyperglyceridemia”) or Text (or Textual) relation (such as TR1 and TR2), which is the sentence containing two (target or intermediate) entities. of the form of e1 component of e3 may treat e2 . −−−−−−−−−→ −−−−−−→ The model of Dai et al. (2019) uses such multi-hop paths as additional features for predicting the relation between a given target entity pair (e1 , e2 ), which is reported effective for performance improvement. However, KGs are often highly incomplete (Min et al., 2013) and may be too sparse to provide enough informative paths in practice, which may hamper the effectiveness of multi-hop paths. Given this background, in this study, we take one step further, aiming for inducing maximal signals of distant supervision from both a KG and a large text collection (hereafter, Text). For this purpose, we consider using multi-hop paths over a Universal Graph (UG) as extra features for DS-RE. Here, we define a UG as a joint graph representation of both KG and Text, where each node represents an entity from KG or Text, and each edge indicates a KG relation or Textual re"
2021.eacl-main.321,P09-1113,0,0.0644178,"WA PCNN+ATT PCNN MIMLRE MultiR Mintz 0.7 0.6 0.5 0.4 0.3 0.0 0.1 0.2 0.3 0.4 Recall 0.5 0.6 0.7 Figure 7: PR curves of previous state-of-the-art methods and our proposed model on NYT10 dataset. two datasets from different domains further proves the validity of our proposed methods. AUC and P@N Evaluation. Table 2 further presents the results in terms of AUC and P@N. From them, we have similar observation to the Comparison with State-of-the-art Baselines on NYT10. To demonstrate the effectiveness of our proposed model, we also compare it against the following baselines on NYT10 dataset: Mintz (Mintz et al., 2009), MultiR (Hoffmann et al., 2011), MIMLRE (Surdeanu et al., 2012), PCNN (Zeng et al., 2015), PCNN+ATT (Lin et al., 2016), BGWA (Jat et al., 2018), PCNN+HATT (Han et al., 2018b), RESIDE (Vashishth et al., 2018), DISTRE (Alt et al., 2019) and Sent+KG (Dai et al., 2019). The results shown in Figure 7 and Table 3 indicate that: (1) our selected base model, “Sent+KG”, is a strong baseline because it signif3680 System Mintz† PCNN+ATT† RESIDE† PCNN+HATT‡ DISTRE† Sent+KG Our Model AUC 10.7 34.1 41.5 42.0 42.2 50.2 55.0 P@0.1k 52.3 73.0 81.8 81.0 68.0 80.0 86.0 P@0.2k 50.2 68.0 75.4 79.5 67.0 82.0 84.0"
2021.eacl-main.321,P15-1016,0,0.0251454,"dataset. as background information for DS-RE. Sun et al. (2019) apply relational table extracted from Web as supplementary evidences for DS-RE. To apply DS-RE beyond sentence boundary, Quirk and Poon (2017) utilize syntactic information to extract relation from neighboring sentences. Zeng et al. (2017) apply two-hop KG paths identified from two-hop textual paths as extra evidences for DS-RE. Different from this work, we directly use the rich UG paths as extra evidences. Dai et al. (2019) extend the framework of Han et al. (2018a) by introducing multiple KG paths as extra evidences for DS-RE. Neelakantan et al. (2015); Das et al. (2017a) use multiple reasoning paths over Text and KG for relation prediction in the paradigm of Knowledge Graph Completion. Our work differs from the ones mentioned above in two ways: (i) We utilize the UG paths as extra evidences for the task of DS-RE from text, (ii) We take into account the factor of attention bias while encoding UG paths and propose two effective debiasing methods to exploit the potential of UG paths for DS-RE. 2 We select the DS-RE model proposed by Dai et al. (2019) as our base model and extend it into our UG setting. Given a target entity pair (e1 , e2 ), a"
2021.eacl-main.321,W19-5034,0,0.0286584,"il both datasets as follows. Biomedical Dataset. This datatset is created by linking biomedical KG with biomedical Text. We choose UMLS2 and Medline corpus as the biomedical KG and Text respectively. UMLS is a frequently used biomedical knowledge base, while Medline corpus is a large collection of biomedical abstracts, both are developed and maintained by the U.S. National Library of Medicine3 . For identifying UMLS entity mentions in the Medline corpus, we use a state-of-the-art UMLS Named En2 https://www.nlm.nih.gov/research/ umls/ 3 https://www.nlm.nih.gov/ tity Recognizer (NER), ScispaCy (Neumann et al., 2019). The NER identifies UMLS concepts and annotates them by their corresponding UMLS Concept Unique Identifier (CUI) and entity types. From the UMLS KG and the entity linked Medline corpus, we extract fact triplets (i.e., (e1 , r, e2 )) and corresponding sentence evidences containing (e1 , e2 ) under the restriction that: (1) each entity pair should be connected by a RO (RO stands for “has Relationship Other than synonymous, narrower, or broader”) relationship; (2) each entity should belong to the following entity types: Protein, Gene, Disease or Syndrome, Enzyme, Chemical, Sign or Symptom and Ph"
2021.eacl-main.321,E17-1110,0,0.0202995,"T10 (Riedel et al., 2010) datasets prove that: (1) UG paths have the potential to bring performance gain for DS-RE as compared with KG paths; (2) the proposed training methods are effective to fully exploit the potential of UG paths for DS-RE because the proposed methods significantly and consistently outperform several baselines on both datasets and especially achieve a new state-of-the-art result on the NYT10 dataset. as background information for DS-RE. Sun et al. (2019) apply relational table extracted from Web as supplementary evidences for DS-RE. To apply DS-RE beyond sentence boundary, Quirk and Poon (2017) utilize syntactic information to extract relation from neighboring sentences. Zeng et al. (2017) apply two-hop KG paths identified from two-hop textual paths as extra evidences for DS-RE. Different from this work, we directly use the rich UG paths as extra evidences. Dai et al. (2019) extend the framework of Han et al. (2018a) by introducing multiple KG paths as extra evidences for DS-RE. Neelakantan et al. (2015); Das et al. (2017a) use multiple reasoning paths over Text and KG for relation prediction in the paradigm of Knowledge Graph Completion. Our work differs from the ones mentioned abo"
2021.eacl-main.321,C14-1220,0,0.0801168,"Missing"
2021.eacl-main.321,D17-1186,0,0.0168232,"gain for DS-RE as compared with KG paths; (2) the proposed training methods are effective to fully exploit the potential of UG paths for DS-RE because the proposed methods significantly and consistently outperform several baselines on both datasets and especially achieve a new state-of-the-art result on the NYT10 dataset. as background information for DS-RE. Sun et al. (2019) apply relational table extracted from Web as supplementary evidences for DS-RE. To apply DS-RE beyond sentence boundary, Quirk and Poon (2017) utilize syntactic information to extract relation from neighboring sentences. Zeng et al. (2017) apply two-hop KG paths identified from two-hop textual paths as extra evidences for DS-RE. Different from this work, we directly use the rich UG paths as extra evidences. Dai et al. (2019) extend the framework of Han et al. (2018a) by introducing multiple KG paths as extra evidences for DS-RE. Neelakantan et al. (2015); Das et al. (2017a) use multiple reasoning paths over Text and KG for relation prediction in the paradigm of Knowledge Graph Completion. Our work differs from the ones mentioned above in two ways: (i) We utilize the UG paths as extra evidences for the task of DS-RE from text, ("
2021.eacl-main.321,D19-1039,0,0.0309205,"Missing"
2021.eacl-main.321,D12-1042,0,0.0500841,"0.1 0.2 0.3 0.4 Recall 0.5 0.6 0.7 Figure 7: PR curves of previous state-of-the-art methods and our proposed model on NYT10 dataset. two datasets from different domains further proves the validity of our proposed methods. AUC and P@N Evaluation. Table 2 further presents the results in terms of AUC and P@N. From them, we have similar observation to the Comparison with State-of-the-art Baselines on NYT10. To demonstrate the effectiveness of our proposed model, we also compare it against the following baselines on NYT10 dataset: Mintz (Mintz et al., 2009), MultiR (Hoffmann et al., 2011), MIMLRE (Surdeanu et al., 2012), PCNN (Zeng et al., 2015), PCNN+ATT (Lin et al., 2016), BGWA (Jat et al., 2018), PCNN+HATT (Han et al., 2018b), RESIDE (Vashishth et al., 2018), DISTRE (Alt et al., 2019) and Sent+KG (Dai et al., 2019). The results shown in Figure 7 and Table 3 indicate that: (1) our selected base model, “Sent+KG”, is a strong baseline because it signif3680 System Mintz† PCNN+ATT† RESIDE† PCNN+HATT‡ DISTRE† Sent+KG Our Model AUC 10.7 34.1 41.5 42.0 42.2 50.2 55.0 P@0.1k 52.3 73.0 81.8 81.0 68.0 80.0 86.0 P@0.2k 50.2 68.0 75.4 79.5 67.0 82.0 84.0 P@0.3k 45.0 67.3 74.3 75.7 65.3 81.3 83.3 P@0.5k 39.7 63.6 69.7"
2021.eacl-main.321,D15-1174,0,0.0169378,"dge may treat, and another textual edge TR2. By augmenting the original KG with textual edges, one can expect far more chances to find informative path evidences between any given target entity pairs, because the number of such textual edges is likely to be much larger than the number of KG edges (Note that one can collect as many textual edges as needed from a raw text corpus with an entity linker). Extending a KG to a UG, therefore, may allow a DS-RE model to learn richer distant supervision signals. The idea of using multi-hop paths over a UG is not necessarily new on its own. For example, Toutanova et al. (2015) propose to use a UG for knowledge graph completion, and Das et al. (2017b) propose a model trained to reason over a UG for question answering. However, there is no prior study that has explored the effective way to use a UG for the task of DS-RE from text. In fact, finding an effective way of using a UG for DS-RE is not as simple as it may seem. As we report in this paper, a straightforward extension of the Dai et al. (2019) model to the UG setting may result in performance degradation. Motivated by this, in this paper, we address how one can make effective use of UG for DS-RE. We 3674 first"
2021.eacl-main.321,D18-1157,0,0.0288913,"Missing"
2021.eacl-main.321,N19-1288,0,0.0594667,"e model aims to measure the probability of (e1 , e2 ) having a predefined relation r (including the empty relation NA). The base model consists of four main modules: KG Encoder, Sentence Evidence Encoder, Path Evidence Encoder and Relation Classification Layer, as shown in Figure 2. Related Work To improve the performance of a DS-RE model, recently, researchers introduce various attention mechanisms. Lin et al. (2016) propose a relation vector based attention mechanism. Jat et al. (2018); Du et al. (2018) propose multi-level (e.g., wordlevel and sentence-level) structured attention mechanism. Ye and Ling (2019) apply both intra-bag and inter-bag attention for DS-RE. Han et al. (2018b) propose a relation hierarchy based attention mechanism. Han et al. (2018a) propose a joint model that adopts a KG embeddings based attention mechanism. Jia et al. (2019) propose an attention regularization framework to select informative sentence evidences for DS-RE. However, these models rely only on noisy sentence evidences from DS, neglecting the rich UG paths for DS-RE. Besides the sentence evidences from DS, researchers also leverage external evidences for DSRE. Ji et al. (2017) apply entity descriptions generated"
2021.eacl-main.321,D15-1203,0,0.027989,"0.7 Figure 7: PR curves of previous state-of-the-art methods and our proposed model on NYT10 dataset. two datasets from different domains further proves the validity of our proposed methods. AUC and P@N Evaluation. Table 2 further presents the results in terms of AUC and P@N. From them, we have similar observation to the Comparison with State-of-the-art Baselines on NYT10. To demonstrate the effectiveness of our proposed model, we also compare it against the following baselines on NYT10 dataset: Mintz (Mintz et al., 2009), MultiR (Hoffmann et al., 2011), MIMLRE (Surdeanu et al., 2012), PCNN (Zeng et al., 2015), PCNN+ATT (Lin et al., 2016), BGWA (Jat et al., 2018), PCNN+HATT (Han et al., 2018b), RESIDE (Vashishth et al., 2018), DISTRE (Alt et al., 2019) and Sent+KG (Dai et al., 2019). The results shown in Figure 7 and Table 3 indicate that: (1) our selected base model, “Sent+KG”, is a strong baseline because it signif3680 System Mintz† PCNN+ATT† RESIDE† PCNN+HATT‡ DISTRE† Sent+KG Our Model AUC 10.7 34.1 41.5 42.0 42.2 50.2 55.0 P@0.1k 52.3 73.0 81.8 81.0 68.0 80.0 86.0 P@0.2k 50.2 68.0 75.4 79.5 67.0 82.0 84.0 P@0.3k 45.0 67.3 74.3 75.7 65.3 81.3 83.3 P@0.5k 39.7 63.6 69.7 68.0 65.0 77.2 80.4 P@1k 3"
2021.eacl-main.78,D15-1296,0,0.0220803,"e 1 shows, if a model has the ability to perform both Boolean inference and veridical inference, it is desirable to have the ability to combine both types to make a chained inference. tions, regardless of the existence of basic inference patterns in the training set. These results suggest there is much room for improving the generalization capacities of DNN models for combining basic inferential abilities. 2 Related Work Transitivity The transitivity of entailment relations, which derives A → C from A → B and B → C, is incorporated into logic-based NLI systems using automated theorem proving (Abzianidze, 2015; Mineshima et al., 2015). This is a basic property of formal logic, also known as syllogism in traditional logic or the cut rule in proof theory (Troelstra and Schwichtenberg, 2000; van Dalen, 2013). Transitivity inference in its various forms has also been widely studied as a fundamental property of human reasoning in cognitive psychology (Johnson-Laird and Byrne, 1991; Khemlani and Johnson-Laird, 2012). In the context of NLP, previous works have proposed a method for training models with transitivity constraints in multi-hop reasoning tasks (Asai and Hajishirzi, 2020) and temporal relation"
2021.eacl-main.78,2020.acl-main.499,0,0.0284969,"using automated theorem proving (Abzianidze, 2015; Mineshima et al., 2015). This is a basic property of formal logic, also known as syllogism in traditional logic or the cut rule in proof theory (Troelstra and Schwichtenberg, 2000; van Dalen, 2013). Transitivity inference in its various forms has also been widely studied as a fundamental property of human reasoning in cognitive psychology (Johnson-Laird and Byrne, 1991; Khemlani and Johnson-Laird, 2012). In the context of NLP, previous works have proposed a method for training models with transitivity constraints in multi-hop reasoning tasks (Asai and Hajishirzi, 2020) and temporal relation extraction tasks (Ning et al., 2017). Clark et al. (2020) investigated a transformer’s ability to perform a chain of reasoning where reasoning rules are explicitly given. In this work, we study model ability to learn transitivity of entailment relations from training examples, rather than explicitly providing rules. Such transitivity inferences are by no means trivial. For instance, if the premise is changed to Jo knows that Ann or Bob left, it does not follow that Bob left, even though the veridical verb know appears. Models relying on shallow heuristics such as lexical"
2021.eacl-main.78,2020.acl-main.212,0,0.0558622,"guistically challenging (adversarial) inferences (Rozen et al., 2019; Nie et al., 2019; Yanaka et al., 2019a; Richardson et al., 2020), learning undesired biases (Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Liu et al., 2019), and heuristics (McCoy et al., 2019). Our approach also provides adversarial test sets against such heuristics by considering combinations of veridical inferences and diverse (lexical, structural, and logical) types of inferences. One way to learn challenging inferences is data augmentation, and prior studies (Yanaka et al., 2019b; Richardson et al., 2020; Min et al., 2020) have shown that data augmentation with synthesized datasets improves performance with challenging linguistic phenomena. However, it remains unclear whether data augmentation can help models learn composite inferences mixing several inference types from training instances. We address this question in Section 4.3. 3 Dataset 3.1 Overview To investigate whether models can capture transitivity, we consider two basic inference patterns and their combinations. The ﬁrst basic pattern, I1 , is veridical inference. We write f (s1 ) → s1 to denote a schematic veridical inference, where f is a clause-emb"
2021.eacl-main.78,D15-1244,1,0.817838,"del has the ability to perform both Boolean inference and veridical inference, it is desirable to have the ability to combine both types to make a chained inference. tions, regardless of the existence of basic inference patterns in the training set. These results suggest there is much room for improving the generalization capacities of DNN models for combining basic inferential abilities. 2 Related Work Transitivity The transitivity of entailment relations, which derives A → C from A → B and B → C, is incorporated into logic-based NLI systems using automated theorem proving (Abzianidze, 2015; Mineshima et al., 2015). This is a basic property of formal logic, also known as syllogism in traditional logic or the cut rule in proof theory (Troelstra and Schwichtenberg, 2000; van Dalen, 2013). Transitivity inference in its various forms has also been widely studied as a fundamental property of human reasoning in cognitive psychology (Johnson-Laird and Byrne, 1991; Khemlani and Johnson-Laird, 2012). In the context of NLP, previous works have proposed a method for training models with transitivity constraints in multi-hop reasoning tasks (Asai and Hajishirzi, 2020) and temporal relation extraction tasks (Ning et"
2021.eacl-main.78,D17-1108,0,0.0267735,", 2015). This is a basic property of formal logic, also known as syllogism in traditional logic or the cut rule in proof theory (Troelstra and Schwichtenberg, 2000; van Dalen, 2013). Transitivity inference in its various forms has also been widely studied as a fundamental property of human reasoning in cognitive psychology (Johnson-Laird and Byrne, 1991; Khemlani and Johnson-Laird, 2012). In the context of NLP, previous works have proposed a method for training models with transitivity constraints in multi-hop reasoning tasks (Asai and Hajishirzi, 2020) and temporal relation extraction tasks (Ning et al., 2017). Clark et al. (2020) investigated a transformer’s ability to perform a chain of reasoning where reasoning rules are explicitly given. In this work, we study model ability to learn transitivity of entailment relations from training examples, rather than explicitly providing rules. Such transitivity inferences are by no means trivial. For instance, if the premise is changed to Jo knows that Ann or Bob left, it does not follow that Bob left, even though the veridical verb know appears. Models relying on shallow heuristics such as lexical overlap can wrongly predict entailment in this case. To co"
2021.eacl-main.78,D14-1162,0,0.0873065,"0 ± 0.0 22.3 ± 13.6 100.0 ± 0.0 100.0 ± 0.0 80.6 ± 3.4 97.1 ± 0.3 91.0 ± 0.0 Table 6: Accuracies for the naturalistic transitivity test set and the validation set. LSTM We use an LSTM (Hochreiter and Schmidhuber, 1997) model, where each premise and hypothesis is processed as a sequence of words using RNN with LSTM cells, and the ﬁnal hidden state of each serves as its representation. The model concatenates the premise and hypothesis representations and passes the result to three hidden layers followed by a two-way softmax classiﬁer. The model is initialized with 300-dimensional GloVe vectors (Pennington et al., 2014) and optimized using Adam (Kingma and Ba, 2015). We search dropout probabilities of [0, 0.1, 0.2] on the output. BERT We use the base-uncased pretrained BERT (Devlin et al., 2019) model1 , ﬁne-tuned for the NLI classiﬁcation task on training data in the standard way. When ﬁne-tuning BERT, we search dropout probabilities of [0, 0.1, 0.2] on the output, and hyperparameters are the same as those commonly used for MultiNLI. 4.2 Testing transitivity We ﬁrst evaluate whether the models trained with basic inferences f (s1 ) → s1 and s1 → s2 can consistently make judgements on the composite inferences"
2021.eacl-main.78,D18-1501,0,0.0354422,"Missing"
2021.eacl-main.78,N18-1101,0,0.127681,"Missing"
2021.eacl-main.78,2020.acl-main.543,1,0.838381,"t the data augmentation improves model performance on similar combinaSystematicity There has been extensive discussion of whether neural networks (aka Connectionist models) can exhibit systematicity of cognitive capacities (Fodor and Pylyshyn, 1988; Marcus, 2003). Recent works have explored whether modern neural networks can learn systematicity in semantic parsing tasks (Lake and Baroni, 2017; Baroni, 2020; Kim and Linzen, 2020) and question answering tasks (Sinha et al., 2019), whereas our focus is the systematicity in NLI. In works related to the systematicity in NLI, Goodwin et al. (2020), Yanaka et al. (2020), and Geiger et al. (2020) used a manually constructed NLI dataset of monotonicity inferences with and without negation (e.g., The child is not holding plants → The child is not holding ﬂowers) to examine DNN models’ generalization capacities. While these approaches concentrate on monotonicity inferences involving quantiﬁers and negative expressions, our method using veridical inference is general in that it can be applied to any entailment 921 relation that combines basic inference patterns; we generate composite inferences by embedding various types of sentences into clause-embedding verbs."
2021.eacl-main.78,W19-4804,1,0.785654,"ws that Ann or Bob left, it does not follow that Bob left, even though the veridical verb know appears. Models relying on shallow heuristics such as lexical overlap can wrongly predict entailment in this case. To correctly handle such composite inferences, models must capture structural relations between veridical inferences and various kinds of embedded inference. Previous studies on the generalization capacities of NLI models have addressed how models could learn inferences with various challenging linguistic phenomena (Bowman et al., 2015b; Dasgupta et al., 2018; Geiger et al., 2019, 2020; Yanaka et al., 2019a,b; Richardson et al., 2020). However, these studies have focused on the linguistic phenomena in isolation, and thus do not address how a model could learn the interactions between them. Our aim is to ﬁll this gap by presenting a method for probing generalization capacity of DNN models performing transitivity inferences. This study provides three main contributions. First, we create and publicly release two types of NLI datasets for testing model ability to perform transitivity inferences: a fully synthetic dataset that combines veridical inferences and Boolean inferences, and a naturalistic"
2021.eacl-main.78,S19-1027,1,0.86909,"ws that Ann or Bob left, it does not follow that Bob left, even though the veridical verb know appears. Models relying on shallow heuristics such as lexical overlap can wrongly predict entailment in this case. To correctly handle such composite inferences, models must capture structural relations between veridical inferences and various kinds of embedded inference. Previous studies on the generalization capacities of NLI models have addressed how models could learn inferences with various challenging linguistic phenomena (Bowman et al., 2015b; Dasgupta et al., 2018; Geiger et al., 2019, 2020; Yanaka et al., 2019a,b; Richardson et al., 2020). However, these studies have focused on the linguistic phenomena in isolation, and thus do not address how a model could learn the interactions between them. Our aim is to ﬁll this gap by presenting a method for probing generalization capacity of DNN models performing transitivity inferences. This study provides three main contributions. First, we create and publicly release two types of NLI datasets for testing model ability to perform transitivity inferences: a fully synthetic dataset that combines veridical inferences and Boolean inferences, and a naturalistic"
2021.emnlp-main.266,P19-1285,0,0.1743,"Evaluation We evaluate the performance with sacreBLEU (Post, 2018). Throughout the experiment, we apply the moses detokenizer to the system output and then compute the detokenized BLEU5 . Models We adopt transformer-base (Vaswani et al., 2017) with APE, SHAPE, or RPE, respectively. Our implementations are based on OpenNMT-py (Klein et al., 2017). Unless otherwise stated, we use a fixed value (K = 500) for the maximum shift of SHAPE to demonstrate that SHAPE is robust against the choice of K. We set the relative distance limit in RPE to 16 following Shaw et al. (2018) and Neishi and Yoshinaga (2019)6 . Dataset We used the WMT 2016 EnglishGerman dataset for training and followed Ott et al. (2018) for tokenization and subword segmentation (Sennrich et al., 2016). We used newstest20102013 and newstest2014-2016 as the validation and test sets, respectively. Our experiments consist of the following three distinct dataset settings: (i) VANILLA: Identical to previous studies (Vaswani et al., 2017; Ott et al., 2018). (ii) E XTRAPOLATE: Shift-invariant models are typically evaluated in terms of extrapolation abil- 3.2 Experiment 1: Shift Invariance ity (Wang et al., 2021; Newman et al., 2020). We"
2021.emnlp-main.266,W17-3203,0,0.067057,"Missing"
2021.emnlp-main.266,K19-1031,0,0.387142,"coder models (Transformers) (Vaswani et al., 2017), enabling the self-attention to recognize the order of input sequences. Position representations have two categories (Dufter et al., 2021): absolute position embedding (APE) (Gehring et al., 2017; Vaswani et al., 2017) and relative position embedding (RPE) (Shaw et al., 2018). With APE, each position is represented by a unique embedding, which is added to inputs. RPE represents the position based on the relative distance between two tokens in the self-attention mechanism. RPE outperforms APE on sequence-to-sequence tasks (Narang et al., 2021; Neishi and Yoshinaga, 2019) due to extrapolation, i.e., the ability to generalize to sequences that are longer than those observed during training (Newman et al., 2020). Wang et al. (2021) reported that one of the key properties contributing to RPE’s superior performance is shift invariance2 , the property of a function to not change its output even if its input is shifted. However, unlike APE, RPE’s formulation Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3309–3321 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Position Representations Figure 1 g"
2021.emnlp-main.266,2020.blackboxnlp-1.26,0,0.22932,"have two categories (Dufter et al., 2021): absolute position embedding (APE) (Gehring et al., 2017; Vaswani et al., 2017) and relative position embedding (RPE) (Shaw et al., 2018). With APE, each position is represented by a unique embedding, which is added to inputs. RPE represents the position based on the relative distance between two tokens in the self-attention mechanism. RPE outperforms APE on sequence-to-sequence tasks (Narang et al., 2021; Neishi and Yoshinaga, 2019) due to extrapolation, i.e., the ability to generalize to sequences that are longer than those observed during training (Newman et al., 2020). Wang et al. (2021) reported that one of the key properties contributing to RPE’s superior performance is shift invariance2 , the property of a function to not change its output even if its input is shifted. However, unlike APE, RPE’s formulation Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3309–3321 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Position Representations Figure 1 gives an overview of the position representations compared in this paper. We denote a source sequence X as a sequence of I tokens, namely, X ="
2021.emnlp-main.266,W19-5333,0,0.0621577,"Missing"
2021.emnlp-main.266,2020.coling-main.319,0,0.0903647,"Missing"
2021.emnlp-main.266,2020.acl-main.89,0,0.0281093,"Missing"
2021.emnlp-main.266,2020.findings-emnlp.298,0,0.0314762,"2021). However, the self-attention mechanism of RPE involves more computation than that of APE4 . In addition, more importantly, RPE requires the modification of the architecture, while APE does not. Specifically, RPE strongly depends on the self-attention mechanism; thus, it is not necessarily compatible with studies that attempt to replace the self-attention with a more lightweight alternative (Kitaev et al., 2020; Choromanski et al., 2021; Tay et al., 2020). RPE, which was originally proposed by Shaw et al. (2018), has many variants in the literature (Dai et al., 2019; Raffel et al., 2020; Huang et al., 2020; Wang et al., 2021; Wu et al., 2021). They aim to improve the empirical performance or the computational speed compared with the original RPE. However, the original RPE is still a strong method in terms of the performance. Narang et al. (2021) conducted a thorough comparison on multiple sequence-to-sequence tasks and reported that the performance of the original RPE is comparable to or sometimes better than its variants. Thus, we exclusively use the original RPE in our experiments. 2.3 Shifted Absolute Position Embedding (SHAPE) Given the drawbacks of RPE, we investigate SHAPE (Figure 1c) as"
2021.emnlp-main.308,2020.lrec-1.11,0,0.0266601,"els (MLMs) (Devlin et al., 2019), which is effecIn pronoun-dropping languages such as Japanese tive in implicitly capturing anaphoric relational and Chinese, the semantic arguments of predicates knowledge. In fact, recent studies used pretrained can be omitted from sentences. As shown in Fig- MLMs and achieved drastic performance improveure 1, the semantic subject of the predicate used is ments in the tasks that require anaphoric relational omitted and represented by φ, which is called zero knowledge, including Japanese ZAR (Joshi et al., pronoun. This pronoun refers to the criminal in 2019; Aloraini and Poesio, 2020; Song et al., 2020; the first sentence. This way, the task of recogniz- Konno et al., 2020). ing the antecedents of zero pronouns is called zero To get more out of MLMs, in this paper, we anaphora resolution (ZAR). This study focuses propose a new training framework that pretrains on Japanese ZAR. and finetunes MLMs specialized for ZAR. The idea ZAR is a challenging task because it requires is two-fold. reasoning with commonsense knowledge about the First, we design a new pretraining task that trains semantic associations between zero pronouns and MLMs with explicit supervision on anaphoric r"
2021.emnlp-main.308,P09-1068,0,0.0321386,"11; Yamashiro et al., 2018) and selectional preference probability (Shibata et al., 2016). Furthermore, semi-supervised learning approaches, such as pseudo data generation (Liu et al., 2017) and adversarial training (Kurita et al., 2018), have been proposed. However, the use of pretrained MLM has been the most successful approach (Konno et al., 2020), and we sought to improve the pretraining task to better acquire anaphoric relational knowledge. Anaphoric Relational Knowledge Our proposed pretraining task for acquiring anaphoric relational knowledge is related to script knowledge acquisition (Chambers and Jurafsky, 2009). Script knowledge models chains of typical events (predicates and their arguments). Between events, some Pseudo Zero Pronoun Resolution arguments are shared and represented as variables, (PZ ERO) Several studies have created training such as purchase X → acquire X, which can instances in a similar way as in PZ ERO. For be regarded as a type of anaphoric relational knowl- example, Liu et al. (2017) casted the ZAR problem 3797 as a reading comprehension problem, such that the model chose an appropriate word for the [MASK] from the vocabulary set. The difference is that, unlike their work, we fi"
2021.emnlp-main.308,N19-1423,0,0.530331,"nges. 被害者の 部屋 から 犯人の 凶器が 見つかった。 victim-GEN room from criminal-GEN weapon-NOM was found. refer to 使用した 模様。 !-NOM ハンマーを !-NOM hammer-ACC used seem. The criminal’s weapon was found in the victim’s room. It seems that ! used a hammer. Figure 1: Example of argument omission in Japanese. pairs from large-scale raw corpora and then, use them as features (Sasano et al., 2008; Sasano and Kurohashi, 2011; Yamashiro et al., 2018), or using selectional preference probability (Shibata and Kurohashi, 2018) in machine learning models. A modern approach is to use masked language mod1 Introduction els (MLMs) (Devlin et al., 2019), which is effecIn pronoun-dropping languages such as Japanese tive in implicitly capturing anaphoric relational and Chinese, the semantic arguments of predicates knowledge. In fact, recent studies used pretrained can be omitted from sentences. As shown in Fig- MLMs and achieved drastic performance improveure 1, the semantic subject of the predicate used is ments in the tasks that require anaphoric relational omitted and represented by φ, which is called zero knowledge, including Japanese ZAR (Joshi et al., pronoun. This pronoun refers to the criminal in 2019; Aloraini and Poesio, 2020; Song e"
2021.emnlp-main.308,2021.acl-long.295,0,0.0292089,"to mitigate such discrepancies. Therefore, inspired by these studies, we designed a finetuning model (AS-PZ ERO) that is suitable for a model pretrained on PZ ERO and demonstrated its effectiveness. Prompt-based Learning Our use of query chunk in AS-PZ ERO can be seen as a prompt-based learning approach (Radford et al., 2019; Brown et al., 2020), which has been actively studied (Liu et al., 2021). In a typical prompt-based learning with a pretrained MLM, a model is trained to replace the masked token with a token from a predefined vocabulary (Schick et al., 2020; Schick and Schütze, 2021a,b; Gao et al., 2021). Our model is pretrained on PZ ERO, which is a task to select a pseudo antecedent from the preceding context. Thus, we designed AS-PZ ERO as a model to select the argument from the input sentences using a prompt-based approach to avoid the pretrainfinetune discrepancy. 8 Conclusion are still challenging. Acknowledgements We thank anonymous reviewers for their insightful comments. We thank Jun Suzuki, Ana Brassard, Tatsuki Kuribayashi, Takumi Ito, Shiki Sato, and Yosuke Kishinami for their valuable comments. This work was supported by JSPS KAKENHI Grant Numbers JP19K12112, JP19K20351, and JP19"
2021.emnlp-main.308,N19-1340,0,0.0239606,"tion instances of the intra-zero and DEP categories are the same for all four models. The differences are that the models (f) and (g) have broader contexts (inter-sentential contexts), i.e., multiple preceding sentences as inputs, and extra training signals from both the inter-zero and exophoric instances. A comparison of these four models shows that (f) and (g) have better performance than (d) and (e) in intra-zero and DEP. This result indicates that inter-sentential contexts are important clues even for identifying intra-sentential argument relations. This result is consistent with those of Guan et al. (2019) and Shibata and Kurohashi (2018), which discussed a method for utilizing inter-sentential contexts as clues for resolving semantic relations in target sentences. We finetuned AS and AS-PZ ERO from a pretrained MLM. The results in Table 1 show that both the AS and AS-PZ ERO models already out- (II) Does pretraining on PZero improve the perperformed the previous state-of-the-art models in formance of AS? As shown in Table 2, the comintra-zero and DEP (Konno et al., 2020) with large parison between the models pretrained on PZ ERO margins. This improvement is due to the differ- (j) and Cloze (h)"
2021.emnlp-main.308,D13-1095,0,0.0240844,": the arguments (entities) that exist outside the text. These are categorized into one of three types: author, reader, and general.3 The identification of inter-sentential and exophoric arguments is an especially difficult task (Shibata and Kurohashi, 2018). For inter-sentential arguments, a model has to search the whole document. For exophoric arguments, a model has to deal with entities outside the document. Because of this difficulty, many previous studies have exclusively focused on the intra-sentential task. In this paper, not 3 The definitions of Author and reader correspond to those in Hangyo et al. (2013). General refers to the rest of exophoric. 3791 only the intra-sentential task, we also treat intersentential and exophoric tasks as the same task formulations, as in previous studies. sentence with a single mask token and collect this sequence as a training instance. 3 Our model for PZ ERO closely resembles that of the transformer-based MLM (Devlin et al., 2019). Given an input sequence X, each token xt ∈ {0, 1}|V |is mapped to an embedding vector of size D, namely, et ∈ RD as follows: 3.1 Pseudo Zero Pronoun Resolution Motivation and Task Formulation The proposed PZ ERO is a pretraining task"
2021.emnlp-main.308,P17-1044,0,0.0264373,"of an argument mention. If there are multiple correct answers in the coreference relations in the context, we assign gold labels to all these mentions. We prepare a probability distribution y ∈ RT of gold labels over the input token in a manner similar to that in Section 3.3. The models are then trained to assign high probabilities to gold arguments. where etoken and eposition are the same as in Equat t predicate tion 1. Moreover, et is an embedding vector computed from E predicate , pstart , and pend . This vector represents whether the token in position t is a part of the predicate or not (He et al., 2017). 4.2 Argument Selection as Pseudo Zero Second, we apply a pretrained transformer to Pronoun Resolution: AS-PZ ERO encode each embedding et into the final hidden state ht ∈ RD . The probability distribution of One potential disadvantage of the AS model is that assigning the label l over the input tokens ol = it may suffer from pretrain-finetune discrepancy. 3793 That is, AS does not use the pretrained parameters, such as W1 , W2 , b1 , and b2 in Equation 2, but is instead finetuned with randomly-initialized new parameters, such as wl and bl in Equation 4. To make efficient use of the anaphoric"
2021.emnlp-main.308,P11-1081,0,0.0405707,"model performance. 7 Related Work edge. While script knowledge only deals with shared arguments as anaphoric (coreferring) phenomena, anaphoric relational knowledge is not limited to them. In the sentence of Figure 1, the word criminal is not an argument of the predicate and is ignored in script knowledge, whereas it is within the scope of this work. Thus, it can be said that this work deals with broader anaphoric phenomena. Zero Anaphora Resolution (ZAR) ZAR has been studied in multiple languages, such as Chinese (Yin et al., 2018), Japanese (Iida et al., 2016), Korean (Han, 2006), Italian (Iida and Poesio, 2011), and Spanish (Palomar et al., 2001). ZAR faces a lack of labeled data, which is a major challenge, and the traditional approach to overcome this is to use large-scale raw corpora. Several studies have employed these corpora as a source of knowledge for ZAR, e.g., case-frame construction (Sasano et al., 2008; Sasano and Kurohashi, 2011; Yamashiro et al., 2018) and selectional preference probability (Shibata et al., 2016). Furthermore, semi-supervised learning approaches, such as pseudo data generation (Liu et al., 2017) and adversarial training (Kurita et al., 2018), have been proposed. Howeve"
2021.emnlp-main.308,D16-1132,0,0.0225755,"no anaphoric relationships, and its effect on the model performance. 7 Related Work edge. While script knowledge only deals with shared arguments as anaphoric (coreferring) phenomena, anaphoric relational knowledge is not limited to them. In the sentence of Figure 1, the word criminal is not an argument of the predicate and is ignored in script knowledge, whereas it is within the scope of this work. Thus, it can be said that this work deals with broader anaphoric phenomena. Zero Anaphora Resolution (ZAR) ZAR has been studied in multiple languages, such as Chinese (Yin et al., 2018), Japanese (Iida et al., 2016), Korean (Han, 2006), Italian (Iida and Poesio, 2011), and Spanish (Palomar et al., 2001). ZAR faces a lack of labeled data, which is a major challenge, and the traditional approach to overcome this is to use large-scale raw corpora. Several studies have employed these corpora as a source of knowledge for ZAR, e.g., case-frame construction (Sasano et al., 2008; Sasano and Kurohashi, 2011; Yamashiro et al., 2018) and selectional preference probability (Shibata et al., 2016). Furthermore, semi-supervised learning approaches, such as pseudo data generation (Liu et al., 2017) and adversarial train"
2021.emnlp-main.308,D19-1588,0,0.0375319,"Missing"
2021.emnlp-main.308,W02-2016,0,0.425648,"predicate in Equation 3. We encode the embeddings with the transformer layer, and then use Equation 2 for the remaining computation of AS-PZ ERO to fill out the [MASK] token with the argument of the target predicate. If the score of the dummy token (x1 ) is highest, the model computes exophoric scores as described in Section 4.1 using Equation 5. 5 Experimental Settings PZ ERO Dataset Japanese Wikipedia corpus (Wikipedia) is the origin of the training data of PZ ERO.5 All the NPs in the corpus are PZ ERO targets. To detect NPs, we parsed Wikipedia using the Japanese dependency parser Cabocha (Kudo and Matsumoto, 2002) and applied a heuristic rule based on part-of-speech tags. We used n = 4 consecutive sentences to develop the input sequence X. From 17.4M sentences in Wikipedia, we obtained 17.3M instances as training data for PZ ERO. ZAR Dataset For the ZAR task, we used NAIST Text Corpus 1.5 (NTC) (Iida et al., 2010, 2017), which is a standard benchmark dataset of this task (Ouchi et al., 2017; Matsubayashi and Inui, 2018; Omori and Komachi, 2019; Konno et al., 2020). We used the training, development, and test splits proposed by Taira et al. (2008). The numbers of intra-sentential, inter-sentential, and"
2021.emnlp-main.308,P18-1044,0,0.0814696,"n Embs + + + [CLS] …確かめ ##る … [SEP] … … + + … … + + + [CLS] … 確かめ ##る … [SEP] [MASK] が con ##firm con ##firm AS-PZ ERO Model AS Model + + + + 確かめ ##る NOM con ##firm query chunk Figure 3: Input layer of AS and AS-PZ ERO. Their differences are that (1) a query chunk exists for AS-PZ ERO, and (2) the position of the target predicate is informed via different embedding types: E predicate and E position . 4.1 (ol,1 , ..., ol,T ) ∈ RT is then obtained by the softmax layer: Argument Selection with Label Probability: AS The argument selection model, hereinafter AS, is a model inspired by the model of Kurita et al. (2018). From the recent standard practice of the pretrainfinetune paradigm (Devlin et al., 2019), we add a classification layer on top of the pretrained model. The model takes an input sequence X, which is created in a similar manner to that described in Section 3.1. X consists of multiple sentences and is pruned to contain Tmax tokens at maximum. The target predicate is in the last sentence, and the [CLS] and [SEP] tokens are included. Also, the model takes two natural numbers pstart and pend as inputs, where 1 ≤ pstart ≤ pend ≤ T . These represent the position of the target predicate. The model se"
2021.emnlp-main.308,P17-1010,0,0.0220999,"l., 2018), Japanese (Iida et al., 2016), Korean (Han, 2006), Italian (Iida and Poesio, 2011), and Spanish (Palomar et al., 2001). ZAR faces a lack of labeled data, which is a major challenge, and the traditional approach to overcome this is to use large-scale raw corpora. Several studies have employed these corpora as a source of knowledge for ZAR, e.g., case-frame construction (Sasano et al., 2008; Sasano and Kurohashi, 2011; Yamashiro et al., 2018) and selectional preference probability (Shibata et al., 2016). Furthermore, semi-supervised learning approaches, such as pseudo data generation (Liu et al., 2017) and adversarial training (Kurita et al., 2018), have been proposed. However, the use of pretrained MLM has been the most successful approach (Konno et al., 2020), and we sought to improve the pretraining task to better acquire anaphoric relational knowledge. Anaphoric Relational Knowledge Our proposed pretraining task for acquiring anaphoric relational knowledge is related to script knowledge acquisition (Chambers and Jurafsky, 2009). Script knowledge models chains of typical events (predicates and their arguments). Between events, some Pseudo Zero Pronoun Resolution arguments are shared and"
2021.emnlp-main.308,C18-1009,1,0.928713,"us (Wikipedia) is the origin of the training data of PZ ERO.5 All the NPs in the corpus are PZ ERO targets. To detect NPs, we parsed Wikipedia using the Japanese dependency parser Cabocha (Kudo and Matsumoto, 2002) and applied a heuristic rule based on part-of-speech tags. We used n = 4 consecutive sentences to develop the input sequence X. From 17.4M sentences in Wikipedia, we obtained 17.3M instances as training data for PZ ERO. ZAR Dataset For the ZAR task, we used NAIST Text Corpus 1.5 (NTC) (Iida et al., 2010, 2017), which is a standard benchmark dataset of this task (Ouchi et al., 2017; Matsubayashi and Inui, 2018; Omori and Komachi, 2019; Konno et al., 2020). We used the training, development, and test splits proposed by Taira et al. (2008). The numbers of intra-sentential, inter-sentential, and exophoric for the training/test instances were 18068/6159, 11175/4081, and 13676/3826, respectively. The NTC details are shown in Appendix A. The evaluation script corresponds to that of Matsubayashi and Inui (2018). Model Our implementation is based on the Transformers library (Wolf et al., 2020). We used the pretrained parameters of the bert-base-japanese model as the initial parameters of our pretraining mo"
2021.emnlp-main.308,N19-1344,0,0.0788355,"of the training data of PZ ERO.5 All the NPs in the corpus are PZ ERO targets. To detect NPs, we parsed Wikipedia using the Japanese dependency parser Cabocha (Kudo and Matsumoto, 2002) and applied a heuristic rule based on part-of-speech tags. We used n = 4 consecutive sentences to develop the input sequence X. From 17.4M sentences in Wikipedia, we obtained 17.3M instances as training data for PZ ERO. ZAR Dataset For the ZAR task, we used NAIST Text Corpus 1.5 (NTC) (Iida et al., 2010, 2017), which is a standard benchmark dataset of this task (Ouchi et al., 2017; Matsubayashi and Inui, 2018; Omori and Komachi, 2019; Konno et al., 2020). We used the training, development, and test splits proposed by Taira et al. (2008). The numbers of intra-sentential, inter-sentential, and exophoric for the training/test instances were 18068/6159, 11175/4081, and 13676/3826, respectively. The NTC details are shown in Appendix A. The evaluation script corresponds to that of Matsubayashi and Inui (2018). Model Our implementation is based on the Transformers library (Wolf et al., 2020). We used the pretrained parameters of the bert-base-japanese model as the initial parameters of our pretraining models. We trained our mode"
2021.emnlp-main.308,P17-1146,1,0.851878,"anese Wikipedia corpus (Wikipedia) is the origin of the training data of PZ ERO.5 All the NPs in the corpus are PZ ERO targets. To detect NPs, we parsed Wikipedia using the Japanese dependency parser Cabocha (Kudo and Matsumoto, 2002) and applied a heuristic rule based on part-of-speech tags. We used n = 4 consecutive sentences to develop the input sequence X. From 17.4M sentences in Wikipedia, we obtained 17.3M instances as training data for PZ ERO. ZAR Dataset For the ZAR task, we used NAIST Text Corpus 1.5 (NTC) (Iida et al., 2010, 2017), which is a standard benchmark dataset of this task (Ouchi et al., 2017; Matsubayashi and Inui, 2018; Omori and Komachi, 2019; Konno et al., 2020). We used the training, development, and test splits proposed by Taira et al. (2008). The numbers of intra-sentential, inter-sentential, and exophoric for the training/test instances were 18068/6159, 11175/4081, and 13676/3826, respectively. The NTC details are shown in Appendix A. The evaluation script corresponds to that of Matsubayashi and Inui (2018). Model Our implementation is based on the Transformers library (Wolf et al., 2020). We used the pretrained parameters of the bert-base-japanese model as the initial par"
2021.emnlp-main.308,J01-4005,0,0.215004,"ge. While script knowledge only deals with shared arguments as anaphoric (coreferring) phenomena, anaphoric relational knowledge is not limited to them. In the sentence of Figure 1, the word criminal is not an argument of the predicate and is ignored in script knowledge, whereas it is within the scope of this work. Thus, it can be said that this work deals with broader anaphoric phenomena. Zero Anaphora Resolution (ZAR) ZAR has been studied in multiple languages, such as Chinese (Yin et al., 2018), Japanese (Iida et al., 2016), Korean (Han, 2006), Italian (Iida and Poesio, 2011), and Spanish (Palomar et al., 2001). ZAR faces a lack of labeled data, which is a major challenge, and the traditional approach to overcome this is to use large-scale raw corpora. Several studies have employed these corpora as a source of knowledge for ZAR, e.g., case-frame construction (Sasano et al., 2008; Sasano and Kurohashi, 2011; Yamashiro et al., 2018) and selectional preference probability (Shibata et al., 2016). Furthermore, semi-supervised learning approaches, such as pseudo data generation (Liu et al., 2017) and adversarial training (Kurita et al., 2018), have been proposed. However, the use of pretrained MLM has bee"
2021.emnlp-main.308,D08-1055,0,0.0233328,"edia using the Japanese dependency parser Cabocha (Kudo and Matsumoto, 2002) and applied a heuristic rule based on part-of-speech tags. We used n = 4 consecutive sentences to develop the input sequence X. From 17.4M sentences in Wikipedia, we obtained 17.3M instances as training data for PZ ERO. ZAR Dataset For the ZAR task, we used NAIST Text Corpus 1.5 (NTC) (Iida et al., 2010, 2017), which is a standard benchmark dataset of this task (Ouchi et al., 2017; Matsubayashi and Inui, 2018; Omori and Komachi, 2019; Konno et al., 2020). We used the training, development, and test splits proposed by Taira et al. (2008). The numbers of intra-sentential, inter-sentential, and exophoric for the training/test instances were 18068/6159, 11175/4081, and 13676/3826, respectively. The NTC details are shown in Appendix A. The evaluation script corresponds to that of Matsubayashi and Inui (2018). Model Our implementation is based on the Transformers library (Wolf et al., 2020). We used the pretrained parameters of the bert-base-japanese model as the initial parameters of our pretraining models. We trained our model using an Adam optimizer (Kingma and Ba, 2015) with warm-up steps. As a loss function, we used cross-ent"
2021.emnlp-main.308,Y18-1089,0,0.0709578,"finetune discrepancy. Our experiments on Japanese ZAR demonstrated that our two proposals boost the state-of-the-art performance, and our detailed analysis provides new insights on the remaining challenges. 被害者の 部屋 から 犯人の 凶器が 見つかった。 victim-GEN room from criminal-GEN weapon-NOM was found. refer to 使用した 模様。 !-NOM ハンマーを !-NOM hammer-ACC used seem. The criminal’s weapon was found in the victim’s room. It seems that ! used a hammer. Figure 1: Example of argument omission in Japanese. pairs from large-scale raw corpora and then, use them as features (Sasano et al., 2008; Sasano and Kurohashi, 2011; Yamashiro et al., 2018), or using selectional preference probability (Shibata and Kurohashi, 2018) in machine learning models. A modern approach is to use masked language mod1 Introduction els (MLMs) (Devlin et al., 2019), which is effecIn pronoun-dropping languages such as Japanese tive in implicitly capturing anaphoric relational and Chinese, the semantic arguments of predicates knowledge. In fact, recent studies used pretrained can be omitted from sentences. As shown in Fig- MLMs and achieved drastic performance improveure 1, the semantic subject of the predicate used is ments in the tasks that require anaphoric"
2021.emnlp-main.308,C18-1002,0,0.0164586,"Ps with the same surface but no anaphoric relationships, and its effect on the model performance. 7 Related Work edge. While script knowledge only deals with shared arguments as anaphoric (coreferring) phenomena, anaphoric relational knowledge is not limited to them. In the sentence of Figure 1, the word criminal is not an argument of the predicate and is ignored in script knowledge, whereas it is within the scope of this work. Thus, it can be said that this work deals with broader anaphoric phenomena. Zero Anaphora Resolution (ZAR) ZAR has been studied in multiple languages, such as Chinese (Yin et al., 2018), Japanese (Iida et al., 2016), Korean (Han, 2006), Italian (Iida and Poesio, 2011), and Spanish (Palomar et al., 2001). ZAR faces a lack of labeled data, which is a major challenge, and the traditional approach to overcome this is to use large-scale raw corpora. Several studies have employed these corpora as a source of knowledge for ZAR, e.g., case-frame construction (Sasano et al., 2008; Sasano and Kurohashi, 2011; Yamashiro et al., 2018) and selectional preference probability (Shibata et al., 2016). Furthermore, semi-supervised learning approaches, such as pseudo data generation (Liu et al"
2021.emnlp-main.335,D17-1098,0,0.0515645,"Missing"
2021.emnlp-main.335,N16-1012,0,0.0238994,"roWe also show that our generation strategy perduced the Generative Adversarial Network (GAN) forms better than previous strategies. to the model of Mou et al. (2015) to resolve the exposure bias problem (Bengio et al., 2015) caused 1 Introduction by generating sequences individually, and used the Following the initial work of Rush et al. (2015), attention mechanism (Bahdanau et al., 2015) to abstractive headline generation using the encoder- improve the consistency between both sequences. However, their model does not support the seq2seq decoder model has been studied extensively framework. (Chopra et al., 2016; Nallapati et al., 2016; Paulus Recently, He et al. (2020) used a Transformeret al., 2018). In the automatic headline generation based model (Vaswani et al., 2017), which is refor advertising articles, there are requests to include ported to achieve high performance, to generate a a given phrase such as a company or product name headline containing a given phrase. They proposed in the headline. providing an encoder with additional information Generating a headline that includes a given phrase has been considered one of the lexically con- related to the given phrase. However, their method stra"
2021.emnlp-main.335,W19-8641,1,0.842409,"specify the areas where input tokens disallow the attention in the backward and forward directions, depending on each generation strategy (see Figure 2). 3 Experiment We conducted the experiment to verify the performance of our methods in the headline generation task. The objective of our experiment is to compare our method with previous Transformer-based methods that generate tokens from left to right. We also compare Seq-B/F, the generation orders proposed by Mou et al. (2016), with Tok-B/F, our new generation orders. 3.1 Setting We used the 2019 version of the Japanese News Corpus (JNC)1 (Hitomi et al., 2019) as the dataset. The JNC contains 1,932,399 article-headline pairs, and we split them randomly at a ratio of 98:1:1 for use as training, validation, and test sets, respectively.2 We utilized MeCab (Kudo et al., 2004) with the IPAdic3 and then applied the Byte Pair Encoding (BPE) algorithm4 (Gage, 1994) for tokenization. We trained BPE with 10,000 merge operations and obtained the most frequent 32,000 1 https://cl.asahi.com/api_data/ jnc-jamul-en.html 2 We applied the preprocessing script https://github.com/asahi-research/ script-for-transformer-based-seq2bf the original JNC to obtain the split"
2021.emnlp-main.335,P17-1141,0,0.0210451,"Okazaki Kentaro Inui Koichi Takeda1 1 2 3 Nagoya University Insight Edge, Inc. The Asahi Shimbun Company 4 5 6 Tokyo Institute of Technology Tohoku University RIKEN AIP yamada.kosuke@c.mbox.nagoya-u.ac.jp, yuta.hitomi@insightedge.jp, tamori-h@asahi.com, {sasano,takedasu}@i.nagoya-u.ac.jp, okazaki@c.titech.ac.jp, inui@ecei.tohoku.ac.jp Abstract there are two major approaches. One approach is to select a plausible sentence including the given This paper explores a variant of automatic phrase from several candidate sentences generated headline generation methods, where a generfrom left to right (Hokamp and Liu, 2017; Anderated headline is required to include a given son et al., 2017; Post and Vilar, 2018). Although phrase such as a company or a product name. these methods can include multiple phrases in a Previous methods using Transformer-based generated sentence, they are computationally exmodels generate a headline including a given pensive due to the large search space of the decodphrase by providing the encoder with additional information corresponding to the given ing process. In addition, since they try to force phrase. However, these methods cannot algiven phrases into sentences at every step of"
2021.emnlp-main.335,W04-3230,0,0.0630665,"Missing"
2021.emnlp-main.335,2020.acl-main.703,0,0.0296085,"2.32 tokens. We evaluated our methods using precision, recall, and F-score of ROUGE-1/2/L (Lin, 2004) and success rate (SR), which is the percentage of the headline that includes the given phrase. We also calculated the Average Length Difference (ALD) to analyze the length of the generated headlines, as n 1X ALD = li − leni , n (2) i=1 where n, li , and leni are the number of samples, the length of the generated headline, and the length of the reference headline, respectively. As a comparison method, we adopted the method proposed by He et al. (2020) with vanilla Transformer instead of BART (Lewis et al., 2020). This method controls the output by inserting the given phrase and the special token ‘|’ in front of the input articles and randomly drops the given phrase from the input articles during training to improve the performance. The hyperparameters of both the comparison and our models are determined as described in Vaswani et al. (2017). The training was terminated when the perplexity computed on the validation set did not update three times in a row, and we used the model with the minimum perplexity on the validation set. The beam size during the inference was set to three. 5 6 4087 https://gith"
2021.emnlp-main.335,D15-1044,0,0.0453908,"ou et al. (2015), generating the backward seguaranteed to include the phrase in the generquence from the phrase and then generating the ated headline, achieve ROUGE scores comparable to previous Transformer-based methods. remaining forward sequence. Liu et al. (2019) introWe also show that our generation strategy perduced the Generative Adversarial Network (GAN) forms better than previous strategies. to the model of Mou et al. (2015) to resolve the exposure bias problem (Bengio et al., 2015) caused 1 Introduction by generating sequences individually, and used the Following the initial work of Rush et al. (2015), attention mechanism (Bahdanau et al., 2015) to abstractive headline generation using the encoder- improve the consistency between both sequences. However, their model does not support the seq2seq decoder model has been studied extensively framework. (Chopra et al., 2016; Nallapati et al., 2016; Paulus Recently, He et al. (2020) used a Transformeret al., 2018). In the automatic headline generation based model (Vaswani et al., 2017), which is refor advertising articles, there are requests to include ported to achieve high performance, to generate a a given phrase such as a company or product n"
2021.emnlp-main.335,W04-1013,0,0.0196194,"e preprocessing script https://github.com/asahi-research/ script-for-transformer-based-seq2bf the original JNC to obtain the split dataset. 3 https://taku910.github.io/mecab/ 4 https://github.com/rsennrich/ subword-nmt at to tokens from the articles and the headlines, respectively. We used context word sequences extracted from the reference headlines by GiNZA5 as the ‘given’ phrase.6 An average of 4.99 phrases was extracted from the reference headlines, and the ‘given’ phrases consisted of an average of 2.32 tokens. We evaluated our methods using precision, recall, and F-score of ROUGE-1/2/L (Lin, 2004) and success rate (SR), which is the percentage of the headline that includes the given phrase. We also calculated the Average Length Difference (ALD) to analyze the length of the generated headlines, as n 1X ALD = li − leni , n (2) i=1 where n, li , and leni are the number of samples, the length of the generated headline, and the length of the reference headline, respectively. As a comparison method, we adopted the method proposed by He et al. (2020) with vanilla Transformer instead of BART (Lewis et al., 2020). This method controls the output by inserting the given phrase and the special tok"
2021.emnlp-main.335,C16-1316,0,0.138499,"ir method strained sentence generation tasks. For these tasks, may not always include the given phrases in the generated headline. ∗ Work done during an internship at The Asahi Shimbun In this study, we work on generating lexically Company † Work done at The Asahi Shimbun Company constrained headlines using Transformer-based 4085 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4085–4090 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Proposed Method We propose a Transformer-based Seq2BF model that applies Seq2BF proposed by Mou et al. (2016) to the Transformer model to generate headlines including a given phrase. The Seq2BF takes W (= w1 , ..., wL ; w1:L ) as the given phrase consisting of L tokens and generates the headline y−M :−1 of M tokens backward from W , and the headline y1:N of N tokens forward from W . The Transformerbased Seq2BF is the Transformer model with two generation components, consisting of a linear and a softmax layer (see Figure 1). In Transformer-based Seq2BF unlike Transformer generating tokens from left to right, the token position changes relatively depending on already generated tokens. We determine the"
2021.emnlp-main.335,K16-1028,0,0.0193208,"ur generation strategy perduced the Generative Adversarial Network (GAN) forms better than previous strategies. to the model of Mou et al. (2015) to resolve the exposure bias problem (Bengio et al., 2015) caused 1 Introduction by generating sequences individually, and used the Following the initial work of Rush et al. (2015), attention mechanism (Bahdanau et al., 2015) to abstractive headline generation using the encoder- improve the consistency between both sequences. However, their model does not support the seq2seq decoder model has been studied extensively framework. (Chopra et al., 2016; Nallapati et al., 2016; Paulus Recently, He et al. (2020) used a Transformeret al., 2018). In the automatic headline generation based model (Vaswani et al., 2017), which is refor advertising articles, there are requests to include ported to achieve high performance, to generate a a given phrase such as a company or product name headline containing a given phrase. They proposed in the headline. providing an encoder with additional information Generating a headline that includes a given phrase has been considered one of the lexically con- related to the given phrase. However, their method strained sentence generation"
2021.emnlp-main.335,N18-1119,0,0.0158763,"himbun Company 4 5 6 Tokyo Institute of Technology Tohoku University RIKEN AIP yamada.kosuke@c.mbox.nagoya-u.ac.jp, yuta.hitomi@insightedge.jp, tamori-h@asahi.com, {sasano,takedasu}@i.nagoya-u.ac.jp, okazaki@c.titech.ac.jp, inui@ecei.tohoku.ac.jp Abstract there are two major approaches. One approach is to select a plausible sentence including the given This paper explores a variant of automatic phrase from several candidate sentences generated headline generation methods, where a generfrom left to right (Hokamp and Liu, 2017; Anderated headline is required to include a given son et al., 2017; Post and Vilar, 2018). Although phrase such as a company or a product name. these methods can include multiple phrases in a Previous methods using Transformer-based generated sentence, they are computationally exmodels generate a headline including a given pensive due to the large search space of the decodphrase by providing the encoder with additional information corresponding to the given ing process. In addition, since they try to force phrase. However, these methods cannot algiven phrases into sentences at every step of the ways include the phrase in the generated generation process, these methods may harm the"
2021.emnlp-main.373,2020.coling-main.320,0,0.527786,"urthermore, attention weight αi,j boosts the shrinking effect. Kobayashi et al. (2020) reported the negative correlation between kf h (xj )k and h αi,j on frequent tokens. That is, ATTN wastes a h lot of attention weights αi,j by assigning them to small vectors kf h (xj )k. To summarize, ATTN’s shrinking effect is probably achieved by (i) the shrinking in f alone and (ii) further shrinking through the cancellation of α and kf (x)k. By these mechanisms, ATTN can contribute to decreasing the mixing ratio. has been investigated in terms of stability and speed of training (Parisotto et al., 2020; Liu et al., 2020), the effects of affine transformation have rarely been explored. By comparing the mixing ratios obtained from ATTB R ES - N and ATTN R ES L N - N (Table 2), we discovered that LN reduced the context-mixing ratio. This suggests that the scaling (by γ) of the affine transformation shrinks the vector from ATTN and emphasizes RES over ATTN. 5 Detailed analysis We further analyzed the mixing ratio of the masked language models in detail from the perspectives of both the layer and word attributes. In this section, we inherit the experimental setup (Section 4.1) from the previous section and demonst"
2021.emnlp-main.373,2021.ccl-1.108,0,0.0371413,"Missing"
2021.emnlp-main.373,D19-1410,0,0.0150472,"th low-frequency ones.8 Discussion: Discounting high-frequency words is a common practice for making the semantic representation of a sentence or a text from word representations; examples are Luhn’s heuristic in classical text summarization (Luhn, 1958) and the smooth inverse frequency (SIF) weighting in sentence vector generation (Arora et al., 2017). Our frequency-based results reveal that attention blocks in BERT achieve this desirable property. Our observation may also explain the phenomenon that adding up BERT’s internal or output representations does not produce a good sentence vector (Reimers and Gurevych, 2019). In contrast, in static word embeddings (e.g., word2vec (Mikolov et al., 2013)), the norm encodes the word importance derived from its frequency (Schakel and Wilson, 2015); we can generate a good sentence vector by simply adding these static word vectors (Yokoi et al., 2020). Our finding suggests that BERT encodes the token’s importance through the context-mixing ratio rather than the norm.9 In this sense, it is plausible that additive composition using BERT’s internal or output representations does not perform well. Generalizability: Contrary to the other experimental results, only the relat"
2021.emnlp-main.373,P19-1580,0,0.0293602,"(i.e., behave as an auto-encoder). From this point of view, the superiority of the preserving effect is the intuitive behaviors of the masked language models. Low impact of discarding learned attention patterns: Several studies have reported the low impact of discarding the learned attention patterns in Transformers. Michel et al. (2019) and Kovaleva et al. (2019) reported that the attention patterns of many attention heads in Transformers can be removed or overwritten into the uniform patterns with almost no change in their performance, and this even brought about improvements in some cases. Voita et al. (2019) also reported the same phenomenon using a pruning method with additional training. In addition, Toneva and Wehbe (2019) reported that using uniform attention in early layers of BERT instead of the learned attention patterns leads to a better ability to simulate human brain activity. Our analysis shows that most of the attention signal is reduced by the immediately following modules, RES and LN. This fact may explain the above observations that discarding the learned attention patterns of many attention heads does not cause a severe difference. 4.4 Mechanism How is the mixing effect conducted"
2021.emnlp-main.373,N18-1101,0,0.0250719,"Missing"
2021.emnlp-main.490,2020.emnlp-main.751,0,0.0400145,"013) to aggregate all the judgements (Suf ). Due to the cost,5 we evaluate 100 gold explanations and 200 generated explanations for each configuration. We obtained Krippendorff’s α of 0.298 on average, indicating a fair agreement. See §D in Appendix for further details of crowdsourced judgement. In some experiments, we report the similarity between generated explanations and humanannotated explanations as a proxy for sufficiency, due to the cost of human evaluation. We employ ROUGE-2 (Lin, 2004) (RG2), which is proven a high correlation between human ratings on several summarization datasets (Bhandari et al., 2020). QA performance We report F1, one of the official evaluation measures of HotpotQA. Given that our ultimate goal is to create an explainable RC system, we also introduce XF1, new evaluation measure: XF1 = N 1 X suf(i) · F1(i), N (8) i where N is the number of instances in the dataset, suf(i) is a crowdsourced sufficiency label (yes=1, no=0), and F1(i) is a F1 score of i-th instance. This captures how well the system generates sufficient explanations and predicts the correct answer. 5.5 Results and discussion Conciseness To assess the compactness of gen- Abstractive explanations are more concis"
2021.emnlp-main.490,2020.emnlp-main.711,0,0.0976984,"ave greatly improved its explainability, models ability to explain their own answers (Thayaparan et al., 2020). Some adopt a pipelined architecture, where they generate an explanation first and then use it to answer the question. This “faithful-by-construction” approach is aimed at ensuring that generated explanations are closer to the systems’ internal reasoning (i.e. faithfulness). The explanation generation step is typically formulated as a sentence selection task over the input text — selecting a set of sentences which provide support for the answer output by the model (Yang et al., 2018; Groeneveld et al., 2020, etc.). However, the main problem with these approaches is that the explanations obtained from the sentence selection tasks are not always minimal, sufficient, and comprehensible. The extractive explanations can include extraneous or superfluous texts which express information that is not necessary for answering questions. For example, as shown in Fig. 1 (a), the fragments such as 2007 British-American fantasy adventure and Young Tommy in “Never Let Me Go” are not needed to explain the answer Northern Lights. Secondly, the extractive explanations may also not be sufficient: the interpretation"
2021.emnlp-main.490,N13-1132,0,0.0222202,"n in Table 2 (see QAM w/o AX). 5.4 Evaluation measures ers are asked to judge if generated explanations alone provide sufficient information for answering the question in a 3-point Likert scale (yes, likely, no) plus “unsure”. To reliably estimate the quality of explanations, we additionally ask them answers that they inferred from the given explanations. To aggregate each annotator’s judgement, we first replace crowdworker’s submission with ‘no’ when (i) the answer is different from the gold standard answer, or (ii) the judgement is unsure, and replace ‘likely’ with ‘yes’. We then used MACE (Hovy et al., 2013) to aggregate all the judgements (Suf ). Due to the cost,5 we evaluate 100 gold explanations and 200 generated explanations for each configuration. We obtained Krippendorff’s α of 0.298 on average, indicating a fair agreement. See §D in Appendix for further details of crowdsourced judgement. In some experiments, we report the similarity between generated explanations and humanannotated explanations as a proxy for sufficiency, due to the cost of human evaluation. We employ ROUGE-2 (Lin, 2004) (RG2), which is proven a high correlation between human ratings on several summarization datasets (Bhan"
2021.emnlp-main.490,2020.acl-main.602,1,0.923821,"ervision for explanations. 2 Related work Explainable NLP Three aspects of explainability have been explored (Jacovi and Goldberg, 2020): (i) comprehensibility to humans (Camburu However, developing such an abstractive exet al., 2018; Rajani et al., 2019), (ii) faithfulness, plainer imposes a significant challenge because of correlation with systems’ internal decision (Kumar the limited amount of human-annotated abstractive and Talukdar, 2020; Glockner et al., 2020), (iii) explanations available and prohibitively high costs conciseness, namely minimality, comprehensibility in extending these (Inoue et al., 2020). Given this and sufficiency for solving an end task (Paranjape limited supervision, how can we ensure that genet al., 2020). erated explanations are sufficient while promoting Earlier approaches to explainable NLP focus on compression? comprehensibility (Camburu et al., 2018; Rajani Our solution is to teach an abstractive explainer et al., 2019), and then the community moves tothrough trial and error maximizing a conciseness- wards ensuring faithfulness by a system’s archipromoting reward function in a reinforcement learn- tecture (faithful by construction), ranging from ing (RL) framework. T"
2021.emnlp-main.490,2020.acl-main.386,0,0.0218105,"sy adventure, and to generate context-independent sentences such as Charlie Rowe plays Billy Costa in The Golden Compass, instead of His film roles includes.... • Even small amounts of human-annotated explanation supervision significantly improve the conciseness of generated explanations. For example, incorporating even 298 instances of annotated explanations makes the compression rate ×1.3 higher and improves human-judged sufficiency by +11.0 points compared to the setting with no supervision for explanations. 2 Related work Explainable NLP Three aspects of explainability have been explored (Jacovi and Goldberg, 2020): (i) comprehensibility to humans (Camburu However, developing such an abstractive exet al., 2018; Rajani et al., 2019), (ii) faithfulness, plainer imposes a significant challenge because of correlation with systems’ internal decision (Kumar the limited amount of human-annotated abstractive and Talukdar, 2020; Glockner et al., 2020), (iii) explanations available and prohibitively high costs conciseness, namely minimality, comprehensibility in extending these (Inoue et al., 2020). Given this and sufficiency for solving an end task (Paranjape limited supervision, how can we ensure that genet al."
2021.emnlp-main.490,L18-1433,0,0.0608874,"Missing"
2021.emnlp-main.490,W06-0707,0,0.0759635,"Missing"
2021.emnlp-main.490,2020.findings-emnlp.171,0,0.0198042,"additional RL training. SuQA-NoRL resembles fully-supervised, generation-based explain-then-predict models by Camburu et al. (2018); Rajani et al. (2019). AX We initialize the AX with DistilBART finetuned on CNN/Daily Mail, one of large, standard datasets of summarization (Shleifer and Rush, 2020). During training, we feed supporting paragraphs as an input to the model. At test time, we use predicted relevant paragraphs from §5.2 as an input. For hyperparameter tuning, we reserve 500 training instances as a validation dataset. See §A in Appendix for further details. QAM We use UnifiedQA-base (Khashabi et al., 2020) as the QAM and freezed it during training. Ideally, the AX should learn from a “perfect” QA model that does not perform disconnected reasoning (Trivedi et al., 2020). However, such a QA model is not available at the moment. We thus simulate it by using UnifiedQA (Khashabi et al., 2020), a T5 (Raffel et al., 2020)-based QA model finetuned on a diverse set of QA datasets (e.g. SQuAD, NarrativeQA, RACE) excluding HotpotQA. We expect this to discourage the QAM from giving correct answers for insufficient explanations by disconnected reasoning, which improves the quality of reward function of RL."
2021.emnlp-main.490,2020.acl-main.435,0,0.0282163,"Missing"
2021.emnlp-main.490,2020.findings-emnlp.97,0,0.0342714,"annotated explanations makes the compression rate ×1.3 higher and improves human-judged sufficiency by +11.0 points compared to the setting with no supervision for explanations. 2 Related work Explainable NLP Three aspects of explainability have been explored (Jacovi and Goldberg, 2020): (i) comprehensibility to humans (Camburu However, developing such an abstractive exet al., 2018; Rajani et al., 2019), (ii) faithfulness, plainer imposes a significant challenge because of correlation with systems’ internal decision (Kumar the limited amount of human-annotated abstractive and Talukdar, 2020; Glockner et al., 2020), (iii) explanations available and prohibitively high costs conciseness, namely minimality, comprehensibility in extending these (Inoue et al., 2020). Given this and sufficiency for solving an end task (Paranjape limited supervision, how can we ensure that genet al., 2020). erated explanations are sufficient while promoting Earlier approaches to explainable NLP focus on compression? comprehensibility (Camburu et al., 2018; Rajani Our solution is to teach an abstractive explainer et al., 2019), and then the community moves tothrough trial and error maximizing a conciseness- wards ensuring faith"
2021.emnlp-main.490,2020.acl-main.771,0,0.0410993,"Missing"
2021.emnlp-main.490,2020.acl-main.703,0,0.0409814,"latent variables, similar to retrieval-augmented language models (Guu et al., 2020; Lewis etPal., 2020b). Specifically, we have p(a|q, p) = e pφ (a|q, e)pπ (e|q, p), assuming pφ (a|q, e, p) = pφ (a|q, e). Replacing the sum with arg max yields Equation 2. The main challenge is that pπ (e|q, p) is not a retriever but a text generator. Abstractive explainer (AX) It takes a paragraph p and a question q as an input, and outputs an explanation e. We implement the AX using a sequenceto-sequence generation model as follows: pπ (e|q, p) = n Y pπ (et |e<t , q, p) (3) t In our experiments, we use BART (Lewis et al., 2020a). We simply concatenate q and p into one Fig. 1 summarizes the overall architecture. To text with a separator token to generate a questionensure the faithfulness of explanations, we use a focused summary of the paragraph. 6066 Question Charlie Rowe plays Billy Costa in a film based on what novel? ⇒ Pretrained with large summarization dataset ⇒ QA model’s F1 score ⇒ Compression ratio ⇒ Abstractiveness ⇒ … ⇒ Supervised with few abstractive explanations Context [P1] The Golden Compass is a 2007 British-American fantasy adventure film based on &quot;Northern Lights&quot;, … Abstractive Explainer (AX) Feed"
2021.emnlp-main.490,W04-1013,0,0.0484184,"rd answer, or (ii) the judgement is unsure, and replace ‘likely’ with ‘yes’. We then used MACE (Hovy et al., 2013) to aggregate all the judgements (Suf ). Due to the cost,5 we evaluate 100 gold explanations and 200 generated explanations for each configuration. We obtained Krippendorff’s α of 0.298 on average, indicating a fair agreement. See §D in Appendix for further details of crowdsourced judgement. In some experiments, we report the similarity between generated explanations and humanannotated explanations as a proxy for sufficiency, due to the cost of human evaluation. We employ ROUGE-2 (Lin, 2004) (RG2), which is proven a high correlation between human ratings on several summarization datasets (Bhandari et al., 2020). QA performance We report F1, one of the official evaluation measures of HotpotQA. Given that our ultimate goal is to create an explainable RC system, we also introduce XF1, new evaluation measure: XF1 = N 1 X suf(i) · F1(i), N (8) i where N is the number of instances in the dataset, suf(i) is a crowdsourced sufficiency label (yes=1, no=0), and F1(i) is a F1 score of i-th instance. This captures how well the system generates sufficient explanations and predicts the correct"
2021.emnlp-main.490,2021.ccl-1.108,0,0.0609081,"Missing"
2021.emnlp-main.490,P17-1098,0,0.0487882,"Missing"
2021.emnlp-main.490,2020.emnlp-main.153,0,0.0503167,"Missing"
2021.emnlp-main.490,P19-1487,0,0.144125,"tead of His film roles includes.... • Even small amounts of human-annotated explanation supervision significantly improve the conciseness of generated explanations. For example, incorporating even 298 instances of annotated explanations makes the compression rate ×1.3 higher and improves human-judged sufficiency by +11.0 points compared to the setting with no supervision for explanations. 2 Related work Explainable NLP Three aspects of explainability have been explored (Jacovi and Goldberg, 2020): (i) comprehensibility to humans (Camburu However, developing such an abstractive exet al., 2018; Rajani et al., 2019), (ii) faithfulness, plainer imposes a significant challenge because of correlation with systems’ internal decision (Kumar the limited amount of human-annotated abstractive and Talukdar, 2020; Glockner et al., 2020), (iii) explanations available and prohibitively high costs conciseness, namely minimality, comprehensibility in extending these (Inoue et al., 2020). Given this and sufficiency for solving an end task (Paranjape limited supervision, how can we ensure that genet al., 2020). erated explanations are sufficient while promoting Earlier approaches to explainable NLP focus on compression?"
2021.emnlp-main.490,2020.emnlp-main.712,1,0.756304,"lize the AX with DistilBART finetuned on CNN/Daily Mail, one of large, standard datasets of summarization (Shleifer and Rush, 2020). During training, we feed supporting paragraphs as an input to the model. At test time, we use predicted relevant paragraphs from §5.2 as an input. For hyperparameter tuning, we reserve 500 training instances as a validation dataset. See §A in Appendix for further details. QAM We use UnifiedQA-base (Khashabi et al., 2020) as the QAM and freezed it during training. Ideally, the AX should learn from a “perfect” QA model that does not perform disconnected reasoning (Trivedi et al., 2020). However, such a QA model is not available at the moment. We thus simulate it by using UnifiedQA (Khashabi et al., 2020), a T5 (Raffel et al., 2020)-based QA model finetuned on a diverse set of QA datasets (e.g. SQuAD, NarrativeQA, RACE) excluding HotpotQA. We expect this to discourage the QAM from giving correct answers for insufficient explanations by disconnected reasoning, which improves the quality of reward function of RL. At test time, we use UnifiedQA finetuned on HotpotQA, whose performance is shown in Table 2 (see QAM w/o AX). 5.4 Evaluation measures ers are asked to judge if genera"
2021.emnlp-main.490,2020.acl-main.414,0,0.0220894,"en the community moves tothrough trial and error maximizing a conciseness- wards ensuring faithfulness by a system’s archipromoting reward function in a reinforcement learn- tecture (faithful by construction), ranging from ing (RL) framework. The reward function assesses Natural Language Inference (Kumar and Talukdar, generated explanations against various criteria re- 2020), Fact Verification (Glockner et al., 2020) to lated to conciseness, such as linguistic acceptabil- Question Answering (Latcinnik and Berant, 2020; ity, abstractiveness, and the accuracy of RC mod- Groeneveld et al., 2020; Yadav et al., 2020). ule’s prediction on the generated explanations. By Conciseness, in contrast, has been relatively undoing so, the model gradually learns to extract and explored. One exception is Paranjape et al. (2020), summarize information from input texts so that they who propose to learn to extract a minimal set of help the RC module arrive at the correct answers. input sentences that are useful for solving downAlso, because the explainer aims to produce ab- stream tasks by imposing information bottleneck stractive summaries, we can initialize the explainer on the NLP framework. Although our work shares"
2021.emnlp-main.490,D18-1259,0,0.123157,"omprehension (RC) have greatly improved its explainability, models ability to explain their own answers (Thayaparan et al., 2020). Some adopt a pipelined architecture, where they generate an explanation first and then use it to answer the question. This “faithful-by-construction” approach is aimed at ensuring that generated explanations are closer to the systems’ internal reasoning (i.e. faithfulness). The explanation generation step is typically formulated as a sentence selection task over the input text — selecting a set of sentences which provide support for the answer output by the model (Yang et al., 2018; Groeneveld et al., 2020, etc.). However, the main problem with these approaches is that the explanations obtained from the sentence selection tasks are not always minimal, sufficient, and comprehensible. The extractive explanations can include extraneous or superfluous texts which express information that is not necessary for answering questions. For example, as shown in Fig. 1 (a), the fragments such as 2007 British-American fantasy adventure and Young Tommy in “Never Let Me Go” are not needed to explain the answer Northern Lights. Secondly, the extractive explanations may also not be suffi"
2021.emnlp-main.766,N18-1055,0,0.0414508,"Missing"
2021.emnlp-main.766,2020.lrec-1.41,0,0.374919,"ting Learning 1 Kazuaki Hanawa1,2 , Ryo Nagata3,4 , Kentaro Inui2,1 RIKEN Center for Advanced Intelligence Project, 2 Tohoku University 3 Konan University, 4 JST, PRESTO kazuaki.hanawa@riken.jp nagata-emnlp2021 @ ml.hyogo-u.ac.jp. inui@ecei.tohoku.ac.jp Abstract This type of feedback can assist the writer in determining why their writing was wrong and how to The task of generating explanatory notes for fix it. language learners is known as feedback comment generation. Although various generation While datasets (Nagata, 2019; Nagata et al., techniques are available, little is known about 2020; Pilan et al., 2020) for this task have bewhich methods are appropriate for this task. come available, very little is known about the feedNagata (2019) demonstrates the effectiveness back comment generation methods. Nagata (2019) of neural-retrieval-based methods in generatdemonstrates deep neural network (DNN)-based reing feedback comments for preposition use. trieval methods are effective in generating feedback Retrieval-based methods have limitations in comments for preposition use, which is almost the that they can only output feedback comments existing in a given training data. Furthermore, only knowledge av"
2021.emnlp-main.766,D15-1044,0,0.0303729,"ataset that included feedback comments on linkadvantage of flexible generation. ing word usage with error tags and learners’ revi• SIMPLE GENERATION outperforms others in sions. terms of preposition feedback comment generAlthough datasets have become available, we ation but generates more mixed feedback comknow little about DNN-based methods for feedments than the other two in terms of general back comment generation, which have been proven feedback comment generation, for reasons we to be effective in various natural language generwill discuss in Section 6.2. ation tasks (Shang et al., 2015; Rush et al., 2015; • For these reasons, RETRIEVAL - BASED perBahdanau et al., 2016; Yuan and Briscoe, 2016). forms the best in general feedback comments, Nagata (2019) demonstrates that a neural-retrievalwhich implies that significant progress is rebased method performs well although they can only quired to improve in general feedback comgenerate feedback comments existing in the trainments by DNN-based generation methods. ing data. This implies that retrieve-and-edit-based • SIMPLE GENERATION and RETRIEVE - AND methods, such as those proposed by Hashimoto EDIT generate more feedback comments et al. (2018), wh"
2021.emnlp-main.766,P17-1099,0,0.671102,"L - BASED, hereafter) is naturally choFeedback comment generation is the task of gen- sen as a baseline method (Nagata, 2019). However, RETRIEVAL - BASED is inflexible in that it can only erating explanatory notes for writing learning. An output feedback comments existing in a given trainexample of a feedback comment would be: ing data. DNN-based generation (i.e., sequence(1) Target sentence: *We discussed about it. to-sequence) methods demonstrate more flexible Feedback comment: Since discuss is a tran- generation. We choose the Pointer Generator Netsitive verb, about is not required.2 work (See et al., 2017) as a simple DNN-based 1 Our source code is available at https://github.c generation representative, which will be referred om/k-hanawa/fcg_emnlp2021 to as SIMPLE GENERATION, hereafter. It is an 2 Note that feedback comments are actually written in encoder-decoder neural network with attention and Japanese but we show English translation for clarity in this paper. copy mechanisms. It is preferable to have a copy 9719 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9719–9730 c November 7–11, 2021. 2021 Association for Computational Linguistics mecha"
2021.emnlp-main.766,P00-1068,0,0.379029,"owever, our experiments show completely different performance orders in preposition and general feedback comments: RETRIEVE - AND - EDIT &lt; RETRIEVAL - BASED &lt; SIMPLE GENERATION for the former whereas SIMPLE GENERATION &lt; RETRIEVE - AND - EDIT &lt; RETRIEVAL - BASED for the latter. We investigate the generation results to reveal the following four findings: • It will be necessary to develop a system that makes it easier for users to select results, such as confidence estimation. 2 Related Work There has been little study on feedback comment generation. Some researchers (Mccoy and Pennington, 1996; Kakegawa et al., 2000; Nagata et al., 2014) attempted to develop rule-based methods for diagnosing errors in line with grammatical error detection/correction. Typically, rule-based methods parse input sentences and then apply rules to the resulting parse to diagnose errors. However, they encounter enormous difficulties in dealing with numerous errors and maintaining a broad set of rules. Lai and Chang (2019) proposed a template-based method that identifies error types and generates comments based on them. Recently, datasets for research in feedback comment generation have become available. Nagata (2019) proposed a"
2021.emnlp-main.766,P15-1152,0,0.0393049,"that outweighs the dataset that included feedback comments on linkadvantage of flexible generation. ing word usage with error tags and learners’ revi• SIMPLE GENERATION outperforms others in sions. terms of preposition feedback comment generAlthough datasets have become available, we ation but generates more mixed feedback comknow little about DNN-based methods for feedments than the other two in terms of general back comment generation, which have been proven feedback comment generation, for reasons we to be effective in various natural language generwill discuss in Section 6.2. ation tasks (Shang et al., 2015; Rush et al., 2015; • For these reasons, RETRIEVAL - BASED perBahdanau et al., 2016; Yuan and Briscoe, 2016). forms the best in general feedback comments, Nagata (2019) demonstrates that a neural-retrievalwhich implies that significant progress is rebased method performs well although they can only quired to improve in general feedback comgenerate feedback comments existing in the trainments by DNN-based generation methods. ing data. This implies that retrieve-and-edit-based • SIMPLE GENERATION and RETRIEVE - AND methods, such as those proposed by Hashimoto EDIT generate more feedback comment"
2021.emnlp-main.766,D19-3040,0,0.0276289,"system that makes it easier for users to select results, such as confidence estimation. 2 Related Work There has been little study on feedback comment generation. Some researchers (Mccoy and Pennington, 1996; Kakegawa et al., 2000; Nagata et al., 2014) attempted to develop rule-based methods for diagnosing errors in line with grammatical error detection/correction. Typically, rule-based methods parse input sentences and then apply rules to the resulting parse to diagnose errors. However, they encounter enormous difficulties in dealing with numerous errors and maintaining a broad set of rules. Lai and Chang (2019) proposed a template-based method that identifies error types and generates comments based on them. Recently, datasets for research in feedback comment generation have become available. Nagata (2019) proposed a feedback comment generation • RETRIEVE - AND - EDIT frequently makes untask with a dataset. Nagata et al. (2020) extended necessary edits (we will call this phenomenon the dataset to feedback comments in preposition over-editing). The over-editing problem leads use and in general. Pilan et al. (2020) published a to performance degradation that outweighs the dataset that included feedbac"
2021.emnlp-main.766,D19-1316,1,0.893626,"Konan University, 4 JST, PRESTO kazuaki.hanawa@riken.jp nagata-emnlp2021 @ ml.hyogo-u.ac.jp. inui@ecei.tohoku.ac.jp Abstract This type of feedback can assist the writer in determining why their writing was wrong and how to The task of generating explanatory notes for fix it. language learners is known as feedback comment generation. Although various generation While datasets (Nagata, 2019; Nagata et al., techniques are available, little is known about 2020; Pilan et al., 2020) for this task have bewhich methods are appropriate for this task. come available, very little is known about the feedNagata (2019) demonstrates the effectiveness back comment generation methods. Nagata (2019) of neural-retrieval-based methods in generatdemonstrates deep neural network (DNN)-based reing feedback comments for preposition use. trieval methods are effective in generating feedback Retrieval-based methods have limitations in comments for preposition use, which is almost the that they can only output feedback comments existing in a given training data. Furthermore, only knowledge available. Recently, a wide variety feedback comments can be made on other of DNN-based generation methods are available grammatical"
2021.emnlp-main.766,2020.lrec-1.42,1,0.926017,"er of DNN-based generation methods are available grammatical and writing items than preposiand they can improve feedback comment generation use, which is still unaddressed. To shed tion performance significantly. Furthermore, Nalight on these points, we investigate a wider gata (2019) focused on feedback comments for range of methods for generating many feedpreposition use. DNN-based retrieval methods back comments in this study. Our close analmight not perform well on more general feedback ysis of the type of task leads us to investigate three different architectures for comment gencomments (Nagata et al., 2020; Pilan et al., 2020) eration: (i) a neural-retrieval-based method as because they involve a wide range of writing items a baseline, (ii) a pointer-generator-based generand rules. More sophisticated methods will likely ation method as a neural seq2seq method, (iii) perform better in such cases. a retrieve-and-edit method, a hybrid of (i) and Given these contexts, we investigate feedback (ii). Intuitively, the pointer-generator should comment generation methods for preposition and outperform neural-retrieval, and retrieve-andedit should perform best. However, in our exgeneral feedback comments"
2021.emnlp-main.766,P14-1071,1,0.820217,"show completely different performance orders in preposition and general feedback comments: RETRIEVE - AND - EDIT &lt; RETRIEVAL - BASED &lt; SIMPLE GENERATION for the former whereas SIMPLE GENERATION &lt; RETRIEVE - AND - EDIT &lt; RETRIEVAL - BASED for the latter. We investigate the generation results to reveal the following four findings: • It will be necessary to develop a system that makes it easier for users to select results, such as confidence estimation. 2 Related Work There has been little study on feedback comment generation. Some researchers (Mccoy and Pennington, 1996; Kakegawa et al., 2000; Nagata et al., 2014) attempted to develop rule-based methods for diagnosing errors in line with grammatical error detection/correction. Typically, rule-based methods parse input sentences and then apply rules to the resulting parse to diagnose errors. However, they encounter enormous difficulties in dealing with numerous errors and maintaining a broad set of rules. Lai and Chang (2019) proposed a template-based method that identifies error types and generates comments based on them. Recently, datasets for research in feedback comment generation have become available. Nagata (2019) proposed a feedback comment gene"
2021.emnlp-main.766,W18-5713,0,0.0282355,"ral feedback comgenerate feedback comments existing in the trainments by DNN-based generation methods. ing data. This implies that retrieve-and-edit-based • SIMPLE GENERATION and RETRIEVE - AND methods, such as those proposed by Hashimoto EDIT generate more feedback comments et al. (2018), which are a natural extension of that may mislead learners than RETRIEVAL retrieval-based methods, are likely to be effective BASED because of their flexibility. in this task. Based on these analyses, we reach the following Several variations on the retrieve-and-edit apconclusion. proach have been proposed. Weston et al. (2018); • SIMPLE GENERATION performs best in a set- Guu et al. (2018); Cao et al. (2018); Hossain et al. ting with few variations of feedback com- (2020) retrieve a similar instance using a superments such as preposition feedback com- ficial similarity of the entire input. In the feedments. back comment generation task, we must retrieve • RETRIEVE - AND - EDIT is considered to be an instance of similar errors, and it is inappropripromising for general feedback comment gen- ate to use superficial similarity because sentences eration, but the over-editing problem prevents with the same error are often"
2021.emnlp-main.766,N16-1042,0,0.0268835,"word usage with error tags and learners’ revi• SIMPLE GENERATION outperforms others in sions. terms of preposition feedback comment generAlthough datasets have become available, we ation but generates more mixed feedback comknow little about DNN-based methods for feedments than the other two in terms of general back comment generation, which have been proven feedback comment generation, for reasons we to be effective in various natural language generwill discuss in Section 6.2. ation tasks (Shang et al., 2015; Rush et al., 2015; • For these reasons, RETRIEVAL - BASED perBahdanau et al., 2016; Yuan and Briscoe, 2016). forms the best in general feedback comments, Nagata (2019) demonstrates that a neural-retrievalwhich implies that significant progress is rebased method performs well although they can only quired to improve in general feedback comgenerate feedback comments existing in the trainments by DNN-based generation methods. ing data. This implies that retrieve-and-edit-based • SIMPLE GENERATION and RETRIEVE - AND methods, such as those proposed by Hashimoto EDIT generate more feedback comments et al. (2018), which are a natural extension of that may mislead learners than RETRIEVAL retrieval-based me"
2021.emnlp-main.766,W17-5039,0,0.0254068,"Missing"
2021.findings-acl.10,W13-2322,0,0.0446875,"ce versa, using an off-the-shelf FOL theorem prover1 . To see the logical relationship between G and P , we measure the accuracy for unidirectional and bidirectional Using these multiple forms enables us to analyze whether the difﬁculty in semantic generalization 1 We use a state-of-the-art FOL prover Vampire available at https://github.com/vprover/vampire 106 One dog↑ ran↑ : All dogs↓ ran↑ : All dogs↓ did not run↓ : ∃x.(dog↑ (x) ∧ run↑ (x)) F-score over matching clause, which is similar to S MATCH (Cai and Knight, 2013), an evaluation metric designed for Abstract Meaning Representation (AMR; Banarescu et al., 2013). C OUNTER alleviates the process of variable renaming and correctly evaluates the cases where the order of clauses is different from that of gold answers. ∀x.(dog↓ (x) → run↑ (x)) ∀x.(dog↓ (x) → ¬run↓ (x)) Table 3: Examples of monotonicity-based polarity assignments for FOL formulas. entailment: G ⇒ P , G ⇐ P , and G ⇔ P . Second, the polarity of each content word appearing in a sentence can be extracted from the FOL formula using its monotonicity property (van Benthem, 1986; MacCartney and Manning, 2007). This enables us to analyze whether models can correctly capture entailment relations tr"
2021.findings-acl.10,bos-2008-lets,0,0.06366,", recent analyses (Talmor and Berant, 2019; Liu et al., 2019; McCoy et al., 2019) have pointed out that the standard paradigm for evaluation, where a test set is drawn from the same distribution as the training set, does not always indicate that the model has obtained the intended generalization ability for language understanding. Second, the NLI task of predicting the relationship between a premise sentence and an associated hypothesis without asking their semantic interpretation tends to be black-box, in that it is often difﬁcult to isolate the reasons why models make incorrect predictions (Bos, 2008). To address these issues, we propose SyGNS (pronounced as signs), a Systematic Generalization testbed based on Natural language Semantics. The goal is to map English sentences to various meaning representations, so it can be taken as a sequence-to-sequence semantic parsing task. 103 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 103–119 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1 illustrates our evaluation protocol using SyGNS. To address the ﬁrst issue above, we probe the generalization capability of DNN models on two out-of-d"
2021.findings-acl.10,W15-4002,0,0.0257511,"usal form: b2 wild x1 b2 dog x1 b3 run x1 Figure 1: Illustration of our evaluation protocol using SyGNS. The goal is to map English sentences to meaning representations. The generalization test evaluates novel combinations of operations (modiﬁer, quantiﬁer, negation) in the training set. We use multiple meaning representations and evaluation methods. Introduction Deep neural networks (DNNs) have shown impressive performance in various language understanding tasks (Wang et al., 2019a,b, i.a.), including semantically challenging tasks such as Natural Language Inference (NLI; Dagan et al., 2013; Bowman et al., 2015). However, a number of studies to probe DNN models with various NLI datasets (Naik et al., 2018; Dasgupta et al., 2018; Yanaka et al., 2019; Kim et al., 2019; Richardson et al., 2020; Saha et al., 2020; Geiger et al., 2020) have reported that current DNN models have some limitations to generalize to diverse semantic phenomena, and it is still not clear whether DNN models obtain the ability to capture compositional aspects of meaning in natural language. There are two issues to consider here. First, recent analyses (Talmor and Berant, 2019; Liu et al., 2019; McCoy et al., 2019) have pointed out"
2021.findings-acl.10,P13-2131,0,0.0777977,"Missing"
2021.findings-acl.10,D14-1179,0,0.0506541,"Missing"
2021.findings-acl.10,2020.blackboxnlp-1.16,0,0.148648,"of operations (modiﬁer, quantiﬁer, negation) in the training set. We use multiple meaning representations and evaluation methods. Introduction Deep neural networks (DNNs) have shown impressive performance in various language understanding tasks (Wang et al., 2019a,b, i.a.), including semantically challenging tasks such as Natural Language Inference (NLI; Dagan et al., 2013; Bowman et al., 2015). However, a number of studies to probe DNN models with various NLI datasets (Naik et al., 2018; Dasgupta et al., 2018; Yanaka et al., 2019; Kim et al., 2019; Richardson et al., 2020; Saha et al., 2020; Geiger et al., 2020) have reported that current DNN models have some limitations to generalize to diverse semantic phenomena, and it is still not clear whether DNN models obtain the ability to capture compositional aspects of meaning in natural language. There are two issues to consider here. First, recent analyses (Talmor and Berant, 2019; Liu et al., 2019; McCoy et al., 2019) have pointed out that the standard paradigm for evaluation, where a test set is drawn from the same distribution as the training set, does not always indicate that the model has obtained the intended generalization ability for language und"
2021.findings-acl.10,D19-1107,0,0.0535318,"Missing"
2021.findings-acl.10,P18-2103,0,0.0296841,"ty is limited to the combinations whose forms are similar to those of the training instances. In addition, the models struggle with parsing sentences involving nested clauses. We also show that the extent of generalization depends on the choice of primitive patterns and representation forms. 2 Related Work The question of whether neural networks obtain the systematic generalization capacity has long been discussed (Fodor and Pylyshyn, 1988; Marcus, 2003; Baroni, 2020). Recently, empirical studies using NLI tasks have revisited this question, showing that current models learn undesired biases (Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Geva et al., 2019; Liu et al., 2019) and heuristics (McCoy et al., 2019), and fail to consistently learn various inference types (Rozen et al., 2019; Nie et al., 2019; Yanaka et al., 2019; Richardson et al., 2020; Joshi et al., 2020). In particular, previous works (Goodwin et al., 2020; Yanaka et al., 2020; Geiger et al., 2020; Yanaka et al., 2021) have examined whether models learn the systematicity of NLI on monotonicity and veridicality. While this line of work has shown certain limitations of model generalization capacity, it is often difﬁcult to ﬁgur"
2021.findings-acl.10,2020.acl-main.177,0,0.0421389,"Missing"
2021.findings-acl.10,N18-1108,0,0.0177427,"s (e.g., walk twice → WALK WALK) generalize to new combinations of primitives (e.g., jump twice → JUMP JUMP). However, these datasets deal with artiﬁcial languages, where the variation of linguistic expressions is limited, so it 104 is not clear to what extent the models systematically interpret various semantic phenomena in natural language, such as quantiﬁcation and negation. Regarding the generalization capacity of DNN models in natural language, previous studies have focused on syntactic and morphological generalization capacities such as subject-verb agreement tasks (Linzen et al., 2016; Gulordava et al., 2018; Marvin and Linzen, 2018, i.a.). Perhaps closest to our work is the COGS task (Kim and Linzen, 2020) for probing the generalization capacity of semantic parsing in a synthetic natural language fragment. For instance, the task is to see whether models trained to parse sentences where some lexical items only appear in subject position (e.g., John ate the meat) can generalize to structurally related sentences where these items appear in object position (e.g., The kid liked John). In contrast to this work, our focus is more on semantic parsing of sentences with logical and semantic phenomena that"
2021.findings-acl.10,2020.conll-1.4,0,0.0431365,"resentation forms. 2 Related Work The question of whether neural networks obtain the systematic generalization capacity has long been discussed (Fodor and Pylyshyn, 1988; Marcus, 2003; Baroni, 2020). Recently, empirical studies using NLI tasks have revisited this question, showing that current models learn undesired biases (Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Geva et al., 2019; Liu et al., 2019) and heuristics (McCoy et al., 2019), and fail to consistently learn various inference types (Rozen et al., 2019; Nie et al., 2019; Yanaka et al., 2019; Richardson et al., 2020; Joshi et al., 2020). In particular, previous works (Goodwin et al., 2020; Yanaka et al., 2020; Geiger et al., 2020; Yanaka et al., 2021) have examined whether models learn the systematicity of NLI on monotonicity and veridicality. While this line of work has shown certain limitations of model generalization capacity, it is often difﬁcult to ﬁgure out why the NLI model fails and how to improve it, partly because NLI tasks depend on multiple factors, including semantic interpretation of target phenomena and acquisition of background knowledge. By focusing on semantic parsing rather than NLI, one can probe to what"
2021.findings-acl.10,N19-1225,0,0.0391183,"Missing"
2021.findings-acl.10,W07-1431,0,0.0923388,"Cai and Knight, 2013), an evaluation metric designed for Abstract Meaning Representation (AMR; Banarescu et al., 2013). C OUNTER alleviates the process of variable renaming and correctly evaluates the cases where the order of clauses is different from that of gold answers. ∀x.(dog↓ (x) → run↑ (x)) ∀x.(dog↓ (x) → ¬run↓ (x)) Table 3: Examples of monotonicity-based polarity assignments for FOL formulas. entailment: G ⇒ P , G ⇐ P , and G ⇔ P . Second, the polarity of each content word appearing in a sentence can be extracted from the FOL formula using its monotonicity property (van Benthem, 1986; MacCartney and Manning, 2007). This enables us to analyze whether models can correctly capture entailment relations triggered by quantiﬁer and negation scopes. Table 3 shows some examples of monotonicity-based polarity assignments. For example, existential quantiﬁers such as one are upward monotone (shown as ↑) with respect to the subject NP and the VP, because they can be substituted with their hypernyms (e.g., One dog ran ⇒ One animal moved). These polarities can be extracted from the FOL formula because ∃ and ∧ are upward monotone operators in FOL. Universal quantiﬁers such as all are downward monotone (shown as ↓) wit"
2021.findings-acl.10,2020.emnlp-main.731,0,0.505862,"JUMP). However, these datasets deal with artiﬁcial languages, where the variation of linguistic expressions is limited, so it 104 is not clear to what extent the models systematically interpret various semantic phenomena in natural language, such as quantiﬁcation and negation. Regarding the generalization capacity of DNN models in natural language, previous studies have focused on syntactic and morphological generalization capacities such as subject-verb agreement tasks (Linzen et al., 2016; Gulordava et al., 2018; Marvin and Linzen, 2018, i.a.). Perhaps closest to our work is the COGS task (Kim and Linzen, 2020) for probing the generalization capacity of semantic parsing in a synthetic natural language fragment. For instance, the task is to see whether models trained to parse sentences where some lexical items only appear in subject position (e.g., John ate the meat) can generalize to structurally related sentences where these items appear in object position (e.g., The kid liked John). In contrast to this work, our focus is more on semantic parsing of sentences with logical and semantic phenomena that require scoped meaning representations. Our study also improves previous work on the compositional g"
2021.findings-acl.10,S19-1026,0,0.0497369,"Missing"
2021.findings-acl.10,Q16-1037,0,0.0235165,"p → JUMP) and modiﬁers (e.g., walk twice → WALK WALK) generalize to new combinations of primitives (e.g., jump twice → JUMP JUMP). However, these datasets deal with artiﬁcial languages, where the variation of linguistic expressions is limited, so it 104 is not clear to what extent the models systematically interpret various semantic phenomena in natural language, such as quantiﬁcation and negation. Regarding the generalization capacity of DNN models in natural language, previous studies have focused on syntactic and morphological generalization capacities such as subject-verb agreement tasks (Linzen et al., 2016; Gulordava et al., 2018; Marvin and Linzen, 2018, i.a.). Perhaps closest to our work is the COGS task (Kim and Linzen, 2020) for probing the generalization capacity of semantic parsing in a synthetic natural language fragment. For instance, the task is to see whether models trained to parse sentences where some lexical items only appear in subject position (e.g., John ate the meat) can generalize to structurally related sentences where these items appear in object position (e.g., The kid liked John). In contrast to this work, our focus is more on semantic parsing of sentences with logical and"
2021.findings-acl.10,P18-1040,0,0.0206243,"3 is ﬂipped to downward monotone.2 For evaluation based on monotonicity, we extract a polarity for each content word in a gold formula and a prediction and calculate the F-score for each monotonicity direction (upward and downward). DRS A DRS is a form of scoped meaning representations proposed in Discourse Representation Theory, a well-studied formalism in formal semantics (Kamp and Reyle, 1993; Asher, 1993; Muskens, 1996). By translating a box notation as in (3a) to the clausal form as in (3b), one can evaluate DRSs by C OUNTER3 , which is a standard tool for evaluating neural DRS parsers (Liu et al., 2018; van Noord et al., 2018b). C OUNTER searches for the best variable mapping between predicted DRS clauses and gold DRS clauses and calculates an 2 We follow the surface order of NPs and take it that the subject NP always take scope over the VP. 3 https://github.com/RikVN/DRS_parsing VF formula FOL formulas in our fragment have logically equivalent forms in a variablefree format, which does not contain parentheses nor variables as in the example (4). Our format is similar to a variable-free form in Description Logic (Baader et al., 2003) and Natural Logic (Prat-Hartmann and Moss, 2009). VF form"
2021.findings-acl.10,C18-1198,0,0.0268726,"SyGNS. The goal is to map English sentences to meaning representations. The generalization test evaluates novel combinations of operations (modiﬁer, quantiﬁer, negation) in the training set. We use multiple meaning representations and evaluation methods. Introduction Deep neural networks (DNNs) have shown impressive performance in various language understanding tasks (Wang et al., 2019a,b, i.a.), including semantically challenging tasks such as Natural Language Inference (NLI; Dagan et al., 2013; Bowman et al., 2015). However, a number of studies to probe DNN models with various NLI datasets (Naik et al., 2018; Dasgupta et al., 2018; Yanaka et al., 2019; Kim et al., 2019; Richardson et al., 2020; Saha et al., 2020; Geiger et al., 2020) have reported that current DNN models have some limitations to generalize to diverse semantic phenomena, and it is still not clear whether DNN models obtain the ability to capture compositional aspects of meaning in natural language. There are two issues to consider here. First, recent analyses (Talmor and Berant, 2019; Liu et al., 2019; McCoy et al., 2019) have pointed out that the standard paradigm for evaluation, where a test set is drawn from the same distributio"
2021.findings-acl.10,L18-1267,0,0.0501462,"Missing"
2021.findings-acl.10,P19-1485,0,0.0237725,"ch as Natural Language Inference (NLI; Dagan et al., 2013; Bowman et al., 2015). However, a number of studies to probe DNN models with various NLI datasets (Naik et al., 2018; Dasgupta et al., 2018; Yanaka et al., 2019; Kim et al., 2019; Richardson et al., 2020; Saha et al., 2020; Geiger et al., 2020) have reported that current DNN models have some limitations to generalize to diverse semantic phenomena, and it is still not clear whether DNN models obtain the ability to capture compositional aspects of meaning in natural language. There are two issues to consider here. First, recent analyses (Talmor and Berant, 2019; Liu et al., 2019; McCoy et al., 2019) have pointed out that the standard paradigm for evaluation, where a test set is drawn from the same distribution as the training set, does not always indicate that the model has obtained the intended generalization ability for language understanding. Second, the NLI task of predicting the relationship between a premise sentence and an associated hypothesis without asking their semantic interpretation tends to be black-box, in that it is often difﬁcult to isolate the reasons why models make incorrect predictions (Bos, 2008). To address these issues, we pr"
2021.findings-acl.10,Q18-1043,0,0.031035,"Missing"
2021.findings-acl.10,2020.acl-main.543,1,0.845773,"obtain the systematic generalization capacity has long been discussed (Fodor and Pylyshyn, 1988; Marcus, 2003; Baroni, 2020). Recently, empirical studies using NLI tasks have revisited this question, showing that current models learn undesired biases (Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Geva et al., 2019; Liu et al., 2019) and heuristics (McCoy et al., 2019), and fail to consistently learn various inference types (Rozen et al., 2019; Nie et al., 2019; Yanaka et al., 2019; Richardson et al., 2020; Joshi et al., 2020). In particular, previous works (Goodwin et al., 2020; Yanaka et al., 2020; Geiger et al., 2020; Yanaka et al., 2021) have examined whether models learn the systematicity of NLI on monotonicity and veridicality. While this line of work has shown certain limitations of model generalization capacity, it is often difﬁcult to ﬁgure out why the NLI model fails and how to improve it, partly because NLI tasks depend on multiple factors, including semantic interpretation of target phenomena and acquisition of background knowledge. By focusing on semantic parsing rather than NLI, one can probe to what extent models systematically interpret the meaning of sentences according"
2021.findings-acl.10,W19-4804,1,0.910912,"s to meaning representations. The generalization test evaluates novel combinations of operations (modiﬁer, quantiﬁer, negation) in the training set. We use multiple meaning representations and evaluation methods. Introduction Deep neural networks (DNNs) have shown impressive performance in various language understanding tasks (Wang et al., 2019a,b, i.a.), including semantically challenging tasks such as Natural Language Inference (NLI; Dagan et al., 2013; Bowman et al., 2015). However, a number of studies to probe DNN models with various NLI datasets (Naik et al., 2018; Dasgupta et al., 2018; Yanaka et al., 2019; Kim et al., 2019; Richardson et al., 2020; Saha et al., 2020; Geiger et al., 2020) have reported that current DNN models have some limitations to generalize to diverse semantic phenomena, and it is still not clear whether DNN models obtain the ability to capture compositional aspects of meaning in natural language. There are two issues to consider here. First, recent analyses (Talmor and Berant, 2019; Liu et al., 2019; McCoy et al., 2019) have pointed out that the standard paradigm for evaluation, where a test set is drawn from the same distribution as the training set, does not always indic"
2021.findings-acl.10,2021.eacl-main.78,1,0.758,"ity has long been discussed (Fodor and Pylyshyn, 1988; Marcus, 2003; Baroni, 2020). Recently, empirical studies using NLI tasks have revisited this question, showing that current models learn undesired biases (Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Geva et al., 2019; Liu et al., 2019) and heuristics (McCoy et al., 2019), and fail to consistently learn various inference types (Rozen et al., 2019; Nie et al., 2019; Yanaka et al., 2019; Richardson et al., 2020; Joshi et al., 2020). In particular, previous works (Goodwin et al., 2020; Yanaka et al., 2020; Geiger et al., 2020; Yanaka et al., 2021) have examined whether models learn the systematicity of NLI on monotonicity and veridicality. While this line of work has shown certain limitations of model generalization capacity, it is often difﬁcult to ﬁgure out why the NLI model fails and how to improve it, partly because NLI tasks depend on multiple factors, including semantic interpretation of target phenomena and acquisition of background knowledge. By focusing on semantic parsing rather than NLI, one can probe to what extent models systematically interpret the meaning of sentences according to their structures and the meanings of the"
2021.naacl-main.304,S19-1028,0,0.0556889,"Missing"
2021.naacl-main.304,P19-1393,0,0.041756,"Missing"
2021.naacl-main.304,D18-1009,0,0.0266148,"ues are characterized by a perels’ increased ability to “cheat” by relying on super- formance gap: they show high scores on easy inficial cues (Gururangan et al., 2018; Sugawara et al., stances, but much lower scores on hard ones. 2018; Niven and Kao, 2019). That is, even though Previous work has aimed at countering superfimodels may perform better in terms of benchmark cial cues. A direct, if drastic, method is to comscores, they often are right for the wrong reasons pletely remove easy instances from the training (McCoy et al., 2019) and exhibit worse perfor- data via adversarial filtering (Zellers et al., 2018), mance when prevented from exploiting superficial which leads to better performance on hard incues (Gururangan et al., 2018; Sugawara et al., stances, but, as Gururangan et al. (2018) point out, 2018; Niven and Kao, 2019). filtering easy instances may harm performance by 3890 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3890–3898 June 6–11, 2021. ©2021 Association for Computational Linguistics reducing the data diversity and size. Instead of completely removing easy instances, Schuster et"
C00-1051,P98-1029,0,0.268556,"Missing"
C00-1051,P96-1025,0,0.018103,"hat framework enables the coupling with probabilistic partial parsing. To demonstrate how it works, we report the results of our parsing experiments on a Japanese tree bank. 2 Probabilistic partial parsing 2.1 Dependency probability In this paper, we consider the task of deciding the dependency structure of a Japanese input sentence. Note that, while we restrict our discussion to analysis of Japanese sentences in this paper, what we present below should also be straightforwardly applicable to more wideranged tasks such as English dependency analysis just like the problem setting considered by Collins (1996). Given an input sentence s as a sequence of Bunsetsu -phrases (BPs)1, b1 b2 : : : bn , our task is to identify their inter-BP dependency structure R = fr(bi ; bj )ji = 1; : : : ; ng, where r(bi ; bj ) denotes that bi depends on (or modi es) bj . Let us consider a dependency probability (DP), P (r(bi ; bj )js), a probabilityPthat r(bi ; bj ) holds in a given sentence s: 8i: j P (r(bi ; bj )js) = 1. 2.2 Estimation of DPs Some of the state-of-the-art probabilistic language models such as the bottomup models P (Rjs) proposed by Collins (1996) and Fujio et al. (1998) directly estimate DPs for a gi"
C00-1051,P97-1003,0,0.0471743,"fy their inter-BP dependency structure R = fr(bi ; bj )ji = 1; : : : ; ng, where r(bi ; bj ) denotes that bi depends on (or modi es) bj . Let us consider a dependency probability (DP), P (r(bi ; bj )js), a probabilityPthat r(bi ; bj ) holds in a given sentence s: 8i: j P (r(bi ; bj )js) = 1. 2.2 Estimation of DPs Some of the state-of-the-art probabilistic language models such as the bottomup models P (Rjs) proposed by Collins (1996) and Fujio et al. (1998) directly estimate DPs for a given input, whereas other models such as PCFGbased topdown generation models P (R; s) do not (Charniak, 1997; Collins, 1997; Shirai et al., 1998). If the latter type of models were totally excluded from any committee, our committeebased framework would not work well in practice. Fortunately, however, even for such a model, one can still estimate DPs in the following way if the model provides the n-best depenA bunsetsu phrase (BP) is a chunk of words consisting of a content word (noun, verb, adjective, etc.) accompanied by some functional word(s) (particle, auxiliary, etc.). A Japanese sentence can be analyzed as a sequence of BPs, which constitutes an inter-BP dependency structure 1 dency structure candidates coup"
C00-1051,A94-1016,0,0.0680198,"ng to their coverage-accuracy (or recallprecision) curves. Taking the signi cance of partial parsing into account, therefore in this paper, we evaluate parsing performance according to coverage-accuracy curves. Committee-based decision making is to combine the outputs from several di erent systems (e.g. parsers) to make a better decision. Recently, there have been various attempts to apply committee-based techniques to NLP tasks such as POS tagging (Halteren et al., 1998; Brill et al., 1998), parsing (Henderson and Brill, 1999), word sense disambiguation (Pedersen, 2000), machine translation (Frederking and Nirenburg, 1994), and speech recognition (Fiscus, 1997). Those works empirically demonstrated that combining di erent systems often achieved signi cant improvements over the previous best system. In order to couple those committee-based schemes with probabilistic partial parsing, however, one would still need to make a further extension. Aiming at this coupling, in this paper, we consider a general framework of committeebased decision making that consists of a set of weighting functions and a combination function, and discuss how that framework enables the coupling with probabilistic partial parsing. To demon"
C00-1051,W99-0623,0,0.479234,"Missing"
C00-1051,J93-4006,0,0.021414,"r of attempts to use statistical techniques to improve parsing performance. While this goal has been achieved to a certain degree given the increasing availability of large tree banks, the remaining room for the improvement appears to be getting saturated as long as only statistical techniques are taken into account. This paper explores two directions for the next step beyond the state of the art of statistical parsing: probabilistic partial parsing and committee-based decision making. Probabilistic partial parsing is a probabilistic extension of the existing notion of partial parsing ( e.g. (Jensen et al., 1993)) where a parser selects as its output only a part of the parse tree that are probabilistically highly reliable. This decision-making scheme enables a ne-grained arbitrary choice on the trade-o between accuracy and coverage. Such trade-o is important since there are various applications that require reasonably high accuracy even sacri cing coverage. A typical example is the paraphrasing task embedded in summarization, sentence simpli cation (e.g. (Carroll et al., 1998)), etc. Enabling such trade-o choice will make stateof-the-art parsers of wider application. Partial parsing has also been prov"
C00-1051,A00-2009,0,0.124094,"evaluates statistical parsers according to their coverage-accuracy (or recallprecision) curves. Taking the signi cance of partial parsing into account, therefore in this paper, we evaluate parsing performance according to coverage-accuracy curves. Committee-based decision making is to combine the outputs from several di erent systems (e.g. parsers) to make a better decision. Recently, there have been various attempts to apply committee-based techniques to NLP tasks such as POS tagging (Halteren et al., 1998; Brill et al., 1998), parsing (Henderson and Brill, 1999), word sense disambiguation (Pedersen, 2000), machine translation (Frederking and Nirenburg, 1994), and speech recognition (Fiscus, 1997). Those works empirically demonstrated that combining di erent systems often achieved signi cant improvements over the previous best system. In order to couple those committee-based schemes with probabilistic partial parsing, however, one would still need to make a further extension. Aiming at this coupling, in this paper, we consider a general framework of committeebased decision making that consists of a set of weighting functions and a combination function, and discuss how that framework enables the"
C00-1051,W98-1510,1,0.921533,"BP dependency structure R = fr(bi ; bj )ji = 1; : : : ; ng, where r(bi ; bj ) denotes that bi depends on (or modi es) bj . Let us consider a dependency probability (DP), P (r(bi ; bj )js), a probabilityPthat r(bi ; bj ) holds in a given sentence s: 8i: j P (r(bi ; bj )js) = 1. 2.2 Estimation of DPs Some of the state-of-the-art probabilistic language models such as the bottomup models P (Rjs) proposed by Collins (1996) and Fujio et al. (1998) directly estimate DPs for a given input, whereas other models such as PCFGbased topdown generation models P (R; s) do not (Charniak, 1997; Collins, 1997; Shirai et al., 1998). If the latter type of models were totally excluded from any committee, our committeebased framework would not work well in practice. Fortunately, however, even for such a model, one can still estimate DPs in the following way if the model provides the n-best depenA bunsetsu phrase (BP) is a chunk of words consisting of a content word (noun, verb, adjective, etc.) accompanied by some functional word(s) (particle, auxiliary, etc.). A Japanese sentence can be analyzed as a sequence of BPs, which constitutes an inter-BP dependency structure 1 dency structure candidates coupled with probabilistic"
C00-1051,E99-1026,0,0.0426688,"KANA (Ehara, 1998): a bottom-up model based on maximum entropy estimation. Since dependency score matrices given by KANA have no probabilistic semantics, we normalized them for each row using a certain function manually tuned for this parser.  CHAGAKE (Fujio et al., 1998): an extension of the bottom-up model proposed by Collins (Collins, 1996).  Kanayama&apos;s parser (Kanayama et al., 1999): a bottom-up model coupled with an HPSG.  Shirai&apos;s parser (Shirai et al., 1998): a topdown model incorporating lexical collocation statistics. Equation (1) was used for estimating DPs.  Peach Pie Parser (Uchimoto et al., 1999): a bottom-up model based on maximum entropy estimation. Note that these models were developed fully independently of each other, and have signi cantly di erent characters (for a comparison of their performance, see Table 1). In what follows, these models are referred to anonymously. For the source of the training/test set, we used the Kyoto corpus (ver.2.0) (Kurohashi et al., 1997), which is a collection of Japanese newspaper articles annotated in terms of word boundaries, POS tags, BP boundaries, and inter-BP dependency relations. The corpus originally contained 19,956 sentences. To make the"
C00-1051,P98-1081,0,0.228778,"Missing"
C00-1051,C98-1078,0,\N,Missing
C00-1051,C98-1029,0,\N,Missing
C00-1051,W98-1511,0,\N,Missing
C08-1001,W02-2016,1,0.753818,"rn are shared with each other. We call such anchors type-based anchors because bread in (3a) and bread in (3b) do not refer to the same object but are identical just as type. Given a pair of predicates Pred1 and Pred2 , we search a corpus for sentences where Pred1 and Pred2 co-occur, and calculate the frequency counts of their argument ﬁllers appearing in those sentences: 4 Experiments 4.1 Settings For an empirical evaluation, we used a sample of approximately 500M sentences taken from the Web corpus collected by Kawahara and Kurohashi (2006). The sentences were dependencyparsed with CaboCha (Kudo and Matsumoto, 2002), and co-occurrence samples of event mentions were extracted. Event mentions with patterns whose frequency was less than 20 were discarded in order to reduce computational costs. In our experiments, we considered two of Inui et al. (2003)’s four types of causal relations: actioneffect relations (Effect in Inui et al.’s terminology) and action-means relations (Means). An actioneffect relation holds between events x and y if and only if non-volitional event y is likely to happen as either a direct or indirect effect of volitional action x. For example, the action X-ga undou-suru (X exercises) an"
C08-1001,P06-1015,0,0.0337029,"s boosts the accuracy as well as identifying shared arguments. (a) Anchor word Anc is the head of a noun phrase ﬁlling argument Arg1 of Pred1 appearing in a Web page. 3.2 Predicate pair acquisition (b) Anc also ﬁlls argument Arg2 of Pred2 appearing in the same Web page as above. For predicate pair acquisition, we can choose one from a range of state-of-the-art pattern-based methods. Among others, in our experiments, we adopted Abe et al. (2008)’s method because it had an advantage in that it was capable of learning patterns as well as relation instances. Abe et al. (2008)’s method is based on Pantel and Pennacchiotti (2006)’s Espresso algorithm, (c) Anc must not be any of those in the stop list. (d) pmi(Predi , Argi ) ≥ −1.0 for i ∈ {1, 2} For our experiments, we manually created the stop list, which contained 219 words including pronouns, numerals and highly generic nouns such as 3 Figure 1: Two-phased event relation acquisition 4 S1 = {Arg|Pred1 -Arg1 ; Pred2 ; Anc}, “ͱ (thing)”, “ͷ (thing)” and “ͱ (time)”. pmi(Predi , Argi ) in condition (d) is the point-wise mutual information between Predi and Argi . This condition is imposed for pruning wrong anchors misidentiﬁed due to parsing errors. While Pekar car"
C08-1001,I08-1065,1,0.212189,"o obtain and maintain a potentially huge collection of such event relation instances. This paper addresses the issue of how to automatically acquire such instances of relations between events (henceforth, event relation instances) from a large-scale text collection. Motivated by this issue, several research groups have reported their experiments on automatic acquisition of causal, temporal and entailment relations between event expressions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006; Abe et al., 2008, etc.). As we explain below, however, none of these studies fully achieves the goal we pursue in this paper. An important aspect to consider in event relation acquisition is that each event has arguments. For example, the causal relation between wash something and something is clean can be represented naturally as: Addressing the task of acquiring semantic relations between events from a large corpus, we ﬁrst argue the complementarity between the pattern-based relation-oriented approach and the anchor-based argumentoriented approach. We then propose a twophased approach, which ﬁrst uses lexic"
C08-1001,N06-1007,0,0.547598,"hing event. A crucial issue is how to obtain and maintain a potentially huge collection of such event relation instances. This paper addresses the issue of how to automatically acquire such instances of relations between events (henceforth, event relation instances) from a large-scale text collection. Motivated by this issue, several research groups have reported their experiments on automatic acquisition of causal, temporal and entailment relations between event expressions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006; Abe et al., 2008, etc.). As we explain below, however, none of these studies fully achieves the goal we pursue in this paper. An important aspect to consider in event relation acquisition is that each event has arguments. For example, the causal relation between wash something and something is clean can be represented naturally as: Addressing the task of acquiring semantic relations between events from a large corpus, we ﬁrst argue the complementarity between the pattern-based relation-oriented approach and the anchor-based argumentoriented approach. We then propose a"
C08-1001,I05-1069,0,0.103686,"this object is clean as a result of the washing event. A crucial issue is how to obtain and maintain a potentially huge collection of such event relation instances. This paper addresses the issue of how to automatically acquire such instances of relations between events (henceforth, event relation instances) from a large-scale text collection. Motivated by this issue, several research groups have reported their experiments on automatic acquisition of causal, temporal and entailment relations between event expressions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006; Abe et al., 2008, etc.). As we explain below, however, none of these studies fully achieves the goal we pursue in this paper. An important aspect to consider in event relation acquisition is that each event has arguments. For example, the causal relation between wash something and something is clean can be represented naturally as: Addressing the task of acquiring semantic relations between events from a large corpus, we ﬁrst argue the complementarity between the pattern-based relation-oriented approach and the anchor-based argumentoriented"
C08-1001,P06-1079,1,0.89187,"Missing"
C08-1001,N06-1008,0,0.20994,"esult of the washing event. A crucial issue is how to obtain and maintain a potentially huge collection of such event relation instances. This paper addresses the issue of how to automatically acquire such instances of relations between events (henceforth, event relation instances) from a large-scale text collection. Motivated by this issue, several research groups have reported their experiments on automatic acquisition of causal, temporal and entailment relations between event expressions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006; Abe et al., 2008, etc.). As we explain below, however, none of these studies fully achieves the goal we pursue in this paper. An important aspect to consider in event relation acquisition is that each event has arguments. For example, the causal relation between wash something and something is clean can be represented naturally as: Addressing the task of acquiring semantic relations between events from a large corpus, we ﬁrst argue the complementarity between the pattern-based relation-oriented approach and the anchor-based argumentoriented approach. We th"
C08-1001,P06-1107,0,0.0300652,"crucial issue is how to obtain and maintain a potentially huge collection of such event relation instances. This paper addresses the issue of how to automatically acquire such instances of relations between events (henceforth, event relation instances) from a large-scale text collection. Motivated by this issue, several research groups have reported their experiments on automatic acquisition of causal, temporal and entailment relations between event expressions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006; Abe et al., 2008, etc.). As we explain below, however, none of these studies fully achieves the goal we pursue in this paper. An important aspect to consider in event relation acquisition is that each event has arguments. For example, the causal relation between wash something and something is clean can be represented naturally as: Addressing the task of acquiring semantic relations between events from a large corpus, we ﬁrst argue the complementarity between the pattern-based relation-oriented approach and the anchor-based argumentoriented approach. We then propose a twophased approach, whi"
C08-1001,N06-1023,0,0.0463994,"un bread to be an anchor indicating that the object of bake and the subject of burn are shared with each other. We call such anchors type-based anchors because bread in (3a) and bread in (3b) do not refer to the same object but are identical just as type. Given a pair of predicates Pred1 and Pred2 , we search a corpus for sentences where Pred1 and Pred2 co-occur, and calculate the frequency counts of their argument ﬁllers appearing in those sentences: 4 Experiments 4.1 Settings For an empirical evaluation, we used a sample of approximately 500M sentences taken from the Web corpus collected by Kawahara and Kurohashi (2006). The sentences were dependencyparsed with CaboCha (Kudo and Matsumoto, 2002), and co-occurrence samples of event mentions were extracted. Event mentions with patterns whose frequency was less than 20 were discarded in order to reduce computational costs. In our experiments, we considered two of Inui et al. (2003)’s four types of causal relations: actioneffect relations (Effect in Inui et al.’s terminology) and action-means relations (Means). An actioneffect relation holds between events x and y if and only if non-volitional event y is likely to happen as either a direct or indirect effect of"
C08-1001,W04-3206,0,\N,Missing
C08-1111,S07-1094,0,0.019138,"limited to Uncertain and Non-Uncertain because the purpose of ITSPOKE is to recognize the user’s problem or discomfort in a tutoring dialog. Our goal, on the other hand, is to classify the user’s emotions into more fine-grained emotion classes. In a more general research context, while quite a few studies have been presented about opinion mining and sentiment analysis (Liu, 2006), research into fine-grained emotion classification has emerged only recently. There are two approaches commonly used in emotion classification: a rulebased approach and a statistical approach. Masum et al. (2007) and Chaumartin (2007) propose a rule-based approach to emotion classification. Chaumartin has developed a linguistic rulebased system, which classifies the emotions engendered by news headlines using the WordNet, SentiWordNet, and WordNet-Affect lexical resources. The system detects the sentiment polarity for each word in a news headline based on linguistic resources, and then attempts emotion classification by using rules based on its knowledge of sentence structures. The recall of this system is low, however, because of the limited coverage of the lexical resources. Regarding the statistical approach, Kozareva e"
C08-1111,P97-1023,0,0.0294959,"emotion classification. Chaumartin has developed a linguistic rulebased system, which classifies the emotions engendered by news headlines using the WordNet, SentiWordNet, and WordNet-Affect lexical resources. The system detects the sentiment polarity for each word in a news headline based on linguistic resources, and then attempts emotion classification by using rules based on its knowledge of sentence structures. The recall of this system is low, however, because of the limited coverage of the lexical resources. Regarding the statistical approach, Kozareva et al. (2007) apply the theory of (Hatzivassiloglou and McKeown, 1997) and (Turney, 2002) to emotion classification and propose a method based on the co-occurrence distribution over content words and six emotion words (e.g. joy, fear). For example, birthday appears more often with joy, while war appears more often with fear. However, the accuracy achieved by their method is not practical in applications assumed in this paper. As we demonstrate in Section 4, our method significantly outperforms Kozareva’s method. 3 Emotion Classification 3.1 The basic idea We consider the task of emotion classification as a classification problem where a given input sentence (a u"
C08-1111,kawahara-kurohashi-2006-case,0,0.0202172,"Missing"
C08-1111,S07-1072,0,0.581091,"tin (2007) propose a rule-based approach to emotion classification. Chaumartin has developed a linguistic rulebased system, which classifies the emotions engendered by news headlines using the WordNet, SentiWordNet, and WordNet-Affect lexical resources. The system detects the sentiment polarity for each word in a news headline based on linguistic resources, and then attempts emotion classification by using rules based on its knowledge of sentence structures. The recall of this system is low, however, because of the limited coverage of the lexical resources. Regarding the statistical approach, Kozareva et al. (2007) apply the theory of (Hatzivassiloglou and McKeown, 1997) and (Turney, 2002) to emotion classification and propose a method based on the co-occurrence distribution over content words and six emotion words (e.g. joy, fear). For example, birthday appears more often with joy, while war appears more often with fear. However, the accuracy achieved by their method is not practical in applications assumed in this paper. As we demonstrate in Section 4, our method significantly outperforms Kozareva’s method. 3 Emotion Classification 3.1 The basic idea We consider the task of emotion classification as a"
C08-1111,W04-3239,1,0.80589,"t3} 4. {event4} 5. {event5} increase emotion similarity &lt;disappointment> 0.75 &lt;unpleasantness> 0.70 &lt;loneliness> 0.70 &lt;loneliness> 0.67 &lt;loneliness> 0.63 Ranking of emotion rank emotion score 2.0 1. &lt;loneliness> 2. &lt;disappointment> 0.75 3.&lt;unpleasantness> 0.70 voting Figure 3: An example of a word-polarity lattice Figure 4: Emotion Classification by kNN (k=5) Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al., 2002) and dependency structure (Kudo and Matsumoto, 2004). Our sentiment polarity classification model is trained with SVMs (Vapnik, 1995), and the features are {1-gram, 2-gram, 3gram} of words and the sentiment polarity of the words themselves. Figure 3 illustrates how the sentence “子供の教育の負担が増える (The cost of educating my child increases)” is encoded to a feature vector. Here we assume the sentiment polarity of the “子供 (child)” and “教育 (education)” are positive, while the “負担 (cost)” is negative. These polarity values are represented in parallel with the corresponding words, as shown in Figure 3. By expanding {1-gram, 2-gram, 3-gram} in this lattice"
C08-1111,P04-1045,0,0.018237,"y have forgotten to lock my house should be You’re worried about that. In this paper, we address the above issue of emotion classification in the context of humancomputer dialog, and demonstrate that massive examples of emotion-provoking events can be extracted from the Web with a reasonable accuracy and those examples can be used to build a semantic content-based model for fine-grained emotion classification. 2 Related Work Recently, several studies have reported about dialog systems that are capable of classifying emotions in a human-computer dialog (Batliner et al., 2004; Ang et al., 2002; Litman and Forbes-Riley, 2004; Rotaru et al., 2005). ITSPOKE is a tutoring dialog system, that can recognize the user’s emotion using acoustic-prosodic features and lexical features. However, the emotion classes are limited to Uncertain and Non-Uncertain because the purpose of ITSPOKE is to recognize the user’s problem or discomfort in a tutoring dialog. Our goal, on the other hand, is to classify the user’s emotions into more fine-grained emotion classes. In a more general research context, while quite a few studies have been presented about opinion mining and sentiment analysis (Liu, 2006), research into fine-grained em"
C08-1111,W02-1011,0,0.019451,"s rank event 1. {event1} 2. {event2} 2. {event3} 4. {event4} 5. {event5} increase emotion similarity &lt;disappointment> 0.75 &lt;unpleasantness> 0.70 &lt;loneliness> 0.70 &lt;loneliness> 0.67 &lt;loneliness> 0.63 Ranking of emotion rank emotion score 2.0 1. &lt;loneliness> 2. &lt;disappointment> 0.75 3.&lt;unpleasantness> 0.70 voting Figure 3: An example of a word-polarity lattice Figure 4: Emotion Classification by kNN (k=5) Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al., 2002) and dependency structure (Kudo and Matsumoto, 2004). Our sentiment polarity classification model is trained with SVMs (Vapnik, 1995), and the features are {1-gram, 2-gram, 3gram} of words and the sentiment polarity of the words themselves. Figure 3 illustrates how the sentence “子供の教育の負担が増える (The cost of educating my child increases)” is encoded to a feature vector. Here we assume the sentiment polarity of the “子供 (child)” and “教育 (education)” are positive, while the “負担 (cost)” is negative. These polarity values are represented in parallel with the corresponding words, as shown in Figure 3. B"
C08-1111,W06-1323,1,0.802163,"nt polarity errors, which are considered fatal errors in real dialog applications. 1 Introduction Previous research into human-computer interaction has mostly focused on task-oriented dialogs, where the goal is considered to be to achieve a c 2008.  Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. given task as precisely and efficiently as possible by exchanging information required for the task through dialog (Allen et al., 1994, etc.). More recent research (Foster, 2007; Tokuhisa and Terashima, 2006, etc.), on the other hand, has been providing evidence for the importance of the affective or emotional aspect in a wider range of dialogic contexts, which has been largely neglected in the context of task-oriented dialogs. A dialog system may be expected to serve, for example, as an active listening 1 partner of an elderly user living alone who sometimes wishes to have a chat. In such a context, the dialog system is expected to understand the user’s emotions and sympathize with the user. For example, given an utterence I traveled far to get to the shop, but it was closed from the user, if th"
C08-1111,P02-1053,0,0.0128052,"veloped a linguistic rulebased system, which classifies the emotions engendered by news headlines using the WordNet, SentiWordNet, and WordNet-Affect lexical resources. The system detects the sentiment polarity for each word in a news headline based on linguistic resources, and then attempts emotion classification by using rules based on its knowledge of sentence structures. The recall of this system is low, however, because of the limited coverage of the lexical resources. Regarding the statistical approach, Kozareva et al. (2007) apply the theory of (Hatzivassiloglou and McKeown, 1997) and (Turney, 2002) to emotion classification and propose a method based on the co-occurrence distribution over content words and six emotion words (e.g. joy, fear). For example, birthday appears more often with joy, while war appears more often with fear. However, the accuracy achieved by their method is not practical in applications assumed in this paper. As we demonstrate in Section 4, our method significantly outperforms Kozareva’s method. 3 Emotion Classification 3.1 The basic idea We consider the task of emotion classification as a classification problem where a given input sentence (a user’s utterance) is"
C10-2061,P08-1118,0,0.145329,"Missing"
C10-2061,C08-1031,0,0.0321247,"Missing"
C10-2061,N09-2029,0,0.0304626,"zing contradictions. For example, Harabagiu et al. (2006) used negative expressions, antonyms and contrast discourse relations to recognize contradictions. These methods only detect relations between given sentences, and do not create a bird’s-eye view. To create a kind of bird’s-eye view, Kawahara et al. (2008), Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010) identified various relations between statements including contradictory relations, but do not handle contrastive relations, which are one of the important relations for taking a bird’s-eye view on a topic. Lerman and McDonald (2009) proposed a method for generating contrastive summaries about given two entities on the basis of KL-divergence. This study is related to ours in the aspect of extracting implicit contrasts, but contrastive summaries are different from contrastive relations between statements in our study. 3 Our Method We propose a method for grasping overall information on the Web on a given query (topic). This method extracts and presents statements that are relevant to a given topic, including direct contrastive statements and contradictory/contrastive relations between these statements. As a unit for statem"
C10-2061,P98-2127,0,0.0187212,"lowing patterns. They are extracted from the statement candidates. (patent system of America is different from φ of Japan · · ·) In this sentence, “nihon” (Japan) has a meaning of “nihon-no tokkyo seido” (patent system of Japan). That is to say, “tokkyo seido” (patent system), which is the attribute of comparison, is omitted. In this study, in addition to patterns of contrastive constructs, we use checking and filtering on the basis of similarity. The use of similarity is inspired by the semantic parallelism between contrasted keywords. As this similarity, we employ distributional similarity (Lin, 1998), which is calculated using automatic dependency parses of 100 million Japanese Web pages. By searching similar keywords from the above sentence, we successfully extract a contrastive keyword pair, “amerika” (America) and “nihon” (Japan), and the above sentence as a direct contrastive statement. Similarly, a target of comparison can be omitted as in the following sentence. (4) nedan-wa gosei senzai-yori takaidesu price-TOP synthetic detergent-ABL high (price of φ is higher than synthetic detergent) 7-Eleven-wa hokano konbini-to 7-Eleven-TOP other convenience store-ABL • X-wa Y-to {chigau |koto"
C10-2061,D08-1002,0,0.193277,"olarity. To aggregate statements and detect relations between them, one of important modules is recognition of synonymous, entailed, contradictory and contrastive statements. Studies on rhetorical structure theory (Mann and Thompson, 1988) and recognizing textual entailment (RTE) deal with these relations. In particular, evaluative workshops on RTE have been held and this kind of research has been actively studied (Bentivogli et al., 2009). The recent workshops of this series set up a task that recognizes contradictions. Harabagiu et al. (2006), de Marneffe et al. (2008), Voorhees (2008), and Ritter et al. (2008) focused on recognizing contradictions. For example, Harabagiu et al. (2006) used negative expressions, antonyms and contrast discourse relations to recognize contradictions. These methods only detect relations between given sentences, and do not create a bird’s-eye view. To create a kind of bird’s-eye view, Kawahara et al. (2008), Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010) identified various relations between statements including contradictory relations, but do not handle contrastive relations, which are one of the important relations for taking a bird’s-ey"
C10-2061,I08-2110,1,0.909525,"ly learned probabilities of polarity. To aggregate statements and detect relations between them, one of important modules is recognition of synonymous, entailed, contradictory and contrastive statements. Studies on rhetorical structure theory (Mann and Thompson, 1988) and recognizing textual entailment (RTE) deal with these relations. In particular, evaluative workshops on RTE have been held and this kind of research has been actively studied (Bentivogli et al., 2009). The recent workshops of this series set up a task that recognizes contradictions. Harabagiu et al. (2006), de Marneffe et al. (2008), Voorhees (2008), and Ritter et al. (2008) focused on recognizing contradictions. For example, Harabagiu et al. (2006) used negative expressions, antonyms and contrast discourse relations to recognize contradictions. These methods only detect relations between given sentences, and do not create a bird’s-eye view. To create a kind of bird’s-eye view, Kawahara et al. (2008), Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010) identified various relations between statements including contradictory relations, but do not handle contrastive relations, which are one of the"
C10-2061,I08-1025,1,0.833222,"s. Then, we explain our method of extracting direct contrastive statements with contrastive keyword pairs, and identifying contradictory and contrastive relations in detail. 536 3.1 Extraction and Aggregation of Predicate-argument Structures (1) A predicate-argument structure consists of a predicate and one or more arguments that have a dependency relation to the predicate. We extract predicate-argument structures from automatic parses of Web pages on a given topic by using the method of Kawahara et al. (2008). We apply the following procedure to Web pages that are retrieved from the TSUBAKI (Shinzato et al., 2008) open search engine infrastructure, by inputting the topic as a query. 1. Extract important sentences from each Web page. Important sentences are defined as sentences neighboring the topic word(s). 2. Obtain results of morphological analysis (JUMAN1 ) and dependency parsing (KNP2 ) of the important sentences, and extract predicate-argument structures from them. 3. Filter out functional and meaningless predicate-argument structures, which are not relevant to the topic. Pointwise mutual information between the entire Web and the target Web pages for a topic is used. Note that the analyses in ste"
C10-2061,P09-1026,0,0.258539,"omatically identify such pairs. Ganapathibhotla and Liu (2008) proposed a method for detecting which entities (“target” and “basis”) in a direct contrastive statement are preferred by its author. There is also related work that focuses on noncontrastive sentences. Ohshima et al. (2006) extracted coordinated terms, which are semantically broader than our contrastive keyword pairs, using hit counts from a search engine. They made use of syntactic parallelism among coordinated terms. Their task was to input one of coordinated terms as a query, which is different from ours. Somasundaran and Wiebe (2009) presented a method for recognizing a stance in online debates. They formulated this task as debate-side classification and solved it by using automatically learned probabilities of polarity. To aggregate statements and detect relations between them, one of important modules is recognition of synonymous, entailed, contradictory and contrastive statements. Studies on rhetorical structure theory (Mann and Thompson, 1988) and recognizing textual entailment (RTE) deal with these relations. In particular, evaluative workshops on RTE have been held and this kind of research has been actively studied"
C10-2061,P08-1008,0,0.150366,"ed probabilities of polarity. To aggregate statements and detect relations between them, one of important modules is recognition of synonymous, entailed, contradictory and contrastive statements. Studies on rhetorical structure theory (Mann and Thompson, 1988) and recognizing textual entailment (RTE) deal with these relations. In particular, evaluative workshops on RTE have been held and this kind of research has been actively studied (Bentivogli et al., 2009). The recent workshops of this series set up a task that recognizes contradictions. Harabagiu et al. (2006), de Marneffe et al. (2008), Voorhees (2008), and Ritter et al. (2008) focused on recognizing contradictions. For example, Harabagiu et al. (2006) used negative expressions, antonyms and contrast discourse relations to recognize contradictions. These methods only detect relations between given sentences, and do not create a bird’s-eye view. To create a kind of bird’s-eye view, Kawahara et al. (2008), Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010) identified various relations between statements including contradictory relations, but do not handle contrastive relations, which are one of the important relati"
C10-2061,P09-2039,0,0.0162946,"issue. Contrastive relations are the relations between statements in which two entities or issues are contrasted. In particular, we have the following two novel contributions. • We identify contrastive relations between statements, which consist of in-document and cross-document implicit relations. These relations complement direct contrastive statements, which are explicitly mentioned in a single sentence. • We precisely extract direct contrastive statements and contrastive keyword pairs in an unsupervised manner, whereas most previous studies used supervised methods (Jindal and Liu, 2006b; Yang and Ko, 2009). Our system focuses on the Japanese language. For example, Figure 1 shows examples of extracted statements on the topic “gosei senzai” (synthetic detergent). Rounded rectangles represent statements relevant to this topic. The first statement is a direct contrastive statement, which refers to a contrastive keyword pair, “gosei senzai” (synthetic detergent) and “sekken” (soap). The pairs of statements connected with a broad arrow have contradictory relations. The pairs of statements connected with a thin arrow have contrastive relations. Users not only can see what is written on this topic at a"
C10-2061,W07-1401,0,\N,Missing
C10-2061,C98-2122,0,\N,Missing
C10-2061,P09-4001,1,\N,Missing
C12-1079,W08-2222,0,0.0123274,"h that x = introduced: s x, y = 0. 3 Coreference Resolution in ILP-based Abductive Framework 3.1 Abduction for Discourse Processing Abductive reasoning can be used to recover implicit information from natural language texts. The implicit information includes semantic relations between discourse entities, anaphoric relations, character’s intentions, etc; see (Hobbs et al., 1993) for detailed examples. A logical form (LF) of a text represents observations, which need to be explained by background knowledge. In our discourse processing pipeline, a text is first input to the English parser Boxer (Bos, 2008). For each segment, the parse produced by Boxer is a first-order fragment of the DRS language used in Discourse Representation Theory (Kamp and Reyle, 1993). An add-on to Boxer converts the DRS into a logical form in the style of (Hobbs, 1985). The LF is a conjunction of propositions, which have generalized eventuality arguments that can be used for showing relationships among the propositions. According to (Hobbs, 1985), any predication in the logical notation has an extra argument, which refers to the “condition” of that predication being true. Thus, in the logical form J ohn(e1 , j) ∧ run(e"
C12-1079,W08-2208,0,0.0222857,"Missing"
C12-1079,P09-1068,0,0.0170892,"taset. In addition, we used a resource assigning possible lexical fillers disambiguated into WordNet synsets to FrameNet roles (Bryl et al., 2012). For example, the role THEME of the GIVING frame is mapped to synsets object#n#1 and thing#n#1. Given this information, the following axiom is generated. thing#n#1(s, x) → GIVING(e1 ) ∧ 3 THEME (e1 , x) http://www.wordfrequency.info/ 1300 Weights of these axioms are based on the scores provided by Bryl et al. (2012). We generated 24,571 axioms from the dataset. Narrative chains Similar to (Rahman and Ng, 2012), we employ narrative chains learned by Chambers and Jurafsky (2009), which were shown to have impact on resolving complex coreference; see (Rahman and Ng, 2012) for details. Narrative chains are partially ordered sets of events centered around a common protagonist that are likely to happen in a sequence. Knowledge about such sequences can facilitate coreference resolution. For example, given Max fell, because John pushed him we know that Max and him are coreferential, because we know that an object of the pushing event can be a subject of the falling event. For example, we generate the following axioms. Script#1(s, e1 , x 1 , u) → arrest(e1 , x 1 , x 2 , x 3"
C12-1079,W12-4502,0,0.0233481,"ally motivated constraints in order to prohibit incorrect unification in an abductive framework (Ovchinnikova et al., 2011; Ovchinnikova, 2012). However, the issue of overmerging was never systematically studied and the proposed solutions were never evaluated. In this paper, we investigate whether adding linguistically motivated features can help to block incorrect links in an inference-based framework. A lot of effort in NLP was put into coreference resolution systems ranging from rule-based (Lee et al., 2011, etc.) to machine learning-based resolvers (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012, etc.); see (Ng, 2010) for a detailed survey. Coreference resolution may require deep understanding of text, access to world knowledge, and inference ability. For example, (Levesque, 2011) considers twin sentences such as Ed shouted at Tim because he crashed the car and Ed shouted at Tim because he was angry. In order to resolve coreference in these sentences one requires world knowledge about people shouting when being angry and people shouting at someone who made a mistake, e.g., crashed a car. Surprisingly, most of the contemporary coreference resolution systems including the winners of th"
C12-1079,P08-2012,0,0.0200756,"cate that proposition P has cost c and cost(P) to represent the cost of P. w The background knowledge B is a set of first-order logic formulas of the form P1 1 ∧ ... ∧ Pnw n → Q 1 ∧ ... ∧ Q m . All variables occurring in the antecedent of such axioms are universally quantified with the widest possible scope. Other variables are existentially quantified within the scope of the universal quantifiers. Propositions in the antecedents are assigned positive real-valued weights. We use the notation P w to indicate that proposition P has weight w. 1 There has been work on applying ILP to coreference (Finkel and Manning, 2008; Denis and Baldridge, 2009), but with no relationship with logical inference. 1293 The two main inference operations in weighted abduction are backward chaining and unification. Backward chaining is the introduction of new assumptions given an observation and background knowledge. For example, given O = ∃x(q(x)$10 ) and B = {∀x(p(x)1.2 → q(x))}, there are two candidate hypotheses: H1 = ∃x(q(x)$10 ) and H2 = ∃x(p(x)$12 ). In weighted abduction, a cost function f is used to calculate assumption costs. The function takes two arguments: costs of the propositions backchained on and weight of the a"
C12-1079,P85-1008,1,0.646074,"ormation includes semantic relations between discourse entities, anaphoric relations, character’s intentions, etc; see (Hobbs et al., 1993) for detailed examples. A logical form (LF) of a text represents observations, which need to be explained by background knowledge. In our discourse processing pipeline, a text is first input to the English parser Boxer (Bos, 2008). For each segment, the parse produced by Boxer is a first-order fragment of the DRS language used in Discourse Representation Theory (Kamp and Reyle, 1993). An add-on to Boxer converts the DRS into a logical form in the style of (Hobbs, 1985). The LF is a conjunction of propositions, which have generalized eventuality arguments that can be used for showing relationships among the propositions. According to (Hobbs, 1985), any predication in the logical notation has an extra argument, which refers to the “condition” of that predication being true. Thus, in the logical form J ohn(e1 , j) ∧ run(e2 , j) for the sentence John runs, e2 is a running event by John and e1 is a condition of j being named “John”. In the context of discourse processing, we call a hypothesis explaining a logical form an interpretation of this LF. The interpreta"
C12-1079,N06-2015,0,0.0321093,"canonical entities like people or places, registered in a knowledge base like DBpedia (Bizer et al., 2009) or YAGO (Suchanek et al., 2008). For example, mentions A. Einstein and Einstein will be both mapped to the YAGO node Albert_Einstein. An add-on to our pipeline assigns the same variables to each two named entities disambiguated by AIDA into the same YAGO node. 4 Evaluation We evaluate coreference resolution in our weighted abduction framework using the CoNLL2011 shared task dataset (Pradhan et al., 2011). The CoNLL-2011 dataset was based on the English portion of the OntoNotes 4.0 data (Hovy et al., 2006). OntoNotes is a corpus of large scale annotation of multiple levels of the shallow semantic structure in text. The OntoNotes coreference annotation captures general anaphoric coreference. Note that OntoNotes captures explicit coreference links only, while our procedure also discovers implicit semantic overlap. The CoNLL-2011 shared task was to automatically identify mentions of entities and events in text and to link the corefering mentions together to form entity/event chains. In our experiment, we do not identify mentions, but only compute precision and recall of the inferred coreference li"
C12-1079,N12-1015,0,0.0717548,"Missing"
C12-1079,W11-1913,0,0.0113036,"n order to resolve coreference in these sentences one requires world knowledge about people shouting when being angry and people shouting at someone who made a mistake, e.g., crashed a car. Surprisingly, most of the contemporary coreference resolution systems including the winners of the CoNLL-2011 and CoNLL-2012 shared tasks (Lee et al., 2011; Fernandes et al., 2012) do not exploit any world knowledge. There exist attempts to resolve coreference based on world knowledge resources such as WordNet hierarchy, Wikipedia, semantic similarity, narravite chains (Ponzetto and Strube, 2006; Ng, 2007; Irwin et al., 2011; Rahman and Ng, 2012). Unfortunately, the corresponding resolvers were either not evaluated in large-scale challenges or did not show convincing performance in the challenges. Thus, the question remains open whether employing world knowledge can improve coreference resolution in large unfiltered corpora. In this paper, we investigate whether adding world knowledge for establishing more coreference links can 1292 improve coreference resolution. In the world knowledge employed, our work is most similar to the study on twin sentences presented in (Rahman and Ng, 2012). However, instead of using"
C12-1079,J94-4002,0,0.113926,"Missing"
C12-1079,W11-1902,0,0.0315203,"Missing"
C12-1079,D10-1123,0,0.0150906,"ually collected a set of 33 predicates indicating explicit nonidentity, e.g., similar to, different from. Presence of these predicates in a logical form indicates that their second and third arguments are unlikely to refer to the same entity. We create binary feature N I and compute its value as follows: If there is p(e, v1 , v2 ) and p is a predicate indicating explicit non-identity then N I(v1 , v2 ) = 1; otherwise N I(v1 , v2 ) = 0. Functional relations A binary relation r is functional if ∀x, y1 , y2 : r(x, y1 )∧ r(x, y2 ) → y1 = y2 . For example, a person can be a son of exactly one man. Lin et al. (2010) automatically learn functional relations from a corpus and assign a confidence score to each extracted relation. We use the set of functional relations generated by Lin et al. (2010) in order to generate feature F R. We extract 1,661 functional relations from the dataset. We create a binary feature F R and compute its value as follows: if (i) there are two predicates p(e1 , v1 , x 1 ), p(e2 , v2 , x 2 ), where p indicates a functional relation, (ii) x 1 6= x 2 , and (iii) v1 = v2 then F R(v1 , v2 ) = 1; otherwise F R(v1 , v2 ) = 0. Modality We assume that two predications having different mod"
C12-1079,H05-1004,0,0.0221705,"the CoNLL-2011 shared task, four metrics were used for evaluating coreference performance: MUC, B3 , CEAF, and BLANC. The evaluation metrics are described in (Pradhan et al., 2011). Each of the metric tries to address the shortcomings of the earlier metrics. MUC is the oldest metric; it has been criticized for not penalizing overmerging (Recasens and Hovy, 2010). Since one of the goals of this study is to reduce overmerging in our inference-based framework, this metric does not seem to be representative for us. The B3 and CEAF metrics were also considered to produce counterintuitive results (Luo, 2005; Recasens and Hovy, 2010). BLANC, as the most recent evaluation metric, overcomes the drawbacks of MUC, B3 , and CEAF. The definition formula of BLANC given in (Recasens and Hovy, 2010) is replicated in Table 2, where 1301 r c, wc, r n, wn indicate the number of right coreference links, wrong coreference links, right non-coreference links, and wrong non-coreference links correspondingly. Score Coreference rc P Pc = R Rc = F Fc = r c + wc rc r c + wn 2Pc R c Pc + R c Non-coreference rn Pn = r n + wn rn Rn = r n + wc 2Pn R n Fn = Pn + R n Metric Pc + Pn BLANC-P = BLANC-R = BLANC = 2 Rc + Rn 2 F"
C12-1079,N10-1069,0,0.0322037,"φ(H) are the sums of feature vectors for variable unification assumptions updates. φ(H) ˆ and H respectively. ∆(H, ˆ H) is a loss function that measures how different H ˆ and H are. in H ˆ and H are the larger an ensured margin is. In our experiments, we use The more different H ˆ H) = WO /TO , where TO is the total number of pairs of logical atomic the loss function ∆ P (H, terms in the observation and WO is the total number of variable unification assumptions for ˆ that disagrees with H. We implemented this training algorithm in a observed logical terms in H distributed learning framework (McDonald et al., 2010). 3.3 Features Each feature we use is defined for pairs of unifiable variables (v1 , v2 ). The features are summarized in Table 1. Incompatible properties If two entities have incompatible properties, they are unlikely to be identical. We use WordNet antonymy (black – white) and sibling relation (cat – dog) to derive incompatible properties. Moreover, we assume that two proper names not belonging to the same WordNet synset are unlikely to refer to the same entity. Correspondingly, we generate three binary features A, S, and P (see Table 1). 1297 Algorithm 1 Passive-Aggressive algorithm for par"
C12-1079,P10-1142,0,0.013933,"to prohibit incorrect unification in an abductive framework (Ovchinnikova et al., 2011; Ovchinnikova, 2012). However, the issue of overmerging was never systematically studied and the proposed solutions were never evaluated. In this paper, we investigate whether adding linguistically motivated features can help to block incorrect links in an inference-based framework. A lot of effort in NLP was put into coreference resolution systems ranging from rule-based (Lee et al., 2011, etc.) to machine learning-based resolvers (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012, etc.); see (Ng, 2010) for a detailed survey. Coreference resolution may require deep understanding of text, access to world knowledge, and inference ability. For example, (Levesque, 2011) considers twin sentences such as Ed shouted at Tim because he crashed the car and Ed shouted at Tim because he was angry. In order to resolve coreference in these sentences one requires world knowledge about people shouting when being angry and people shouting at someone who made a mistake, e.g., crashed a car. Surprisingly, most of the contemporary coreference resolution systems including the winners of the CoNLL-2011 and CoNLL-"
C12-1079,P02-1014,0,0.0772542,"pts to use linguistically motivated constraints in order to prohibit incorrect unification in an abductive framework (Ovchinnikova et al., 2011; Ovchinnikova, 2012). However, the issue of overmerging was never systematically studied and the proposed solutions were never evaluated. In this paper, we investigate whether adding linguistically motivated features can help to block incorrect links in an inference-based framework. A lot of effort in NLP was put into coreference resolution systems ranging from rule-based (Lee et al., 2011, etc.) to machine learning-based resolvers (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012, etc.); see (Ng, 2010) for a detailed survey. Coreference resolution may require deep understanding of text, access to world knowledge, and inference ability. For example, (Levesque, 2011) considers twin sentences such as Ed shouted at Tim because he crashed the car and Ed shouted at Tim because he was angry. In order to resolve coreference in these sentences one requires world knowledge about people shouting when being angry and people shouting at someone who made a mistake, e.g., crashed a car. Surprisingly, most of the contemporary coreference resolution systems inc"
C12-1079,W11-0124,1,0.904163,"age understanding the more general principle that we understand our environment by coming up with the best explanation for the observables in the environment. Hobbs et al. (1993) show that the lowest-cost abductive proof provides the solution to a whole range of natural language pragmatics problems, such as word sense disambiguation, anaphora and metonymy resolution, interpretation of noun compounds and prepositional phrases and detection of discourse relations. For examples of the application of weighted abduction to discourse processing see (Charniak and Goldman, 1991; Inoue and Inui, 2011; Ovchinnikova et al., 2011; Ovchinnikova, 2012). If weighted abduction is applied to discourse processing, coreference links naturally follow as a by-product of constructing best explanations. In weighted abduction, coreference resolution is equal to unification of predications; see Sec. 3.1. Similarly, if deductive model building is applied to discourse interpretation, coreference links result from the model minimality. Both inference approaches are based on the idea that predications having the same predicates describe the same situation and therefore their arguments can be assumed to be equal if no logical contradic"
C12-1079,N06-1025,0,0.0322134,"houted at Tim because he was angry. In order to resolve coreference in these sentences one requires world knowledge about people shouting when being angry and people shouting at someone who made a mistake, e.g., crashed a car. Surprisingly, most of the contemporary coreference resolution systems including the winners of the CoNLL-2011 and CoNLL-2012 shared tasks (Lee et al., 2011; Fernandes et al., 2012) do not exploit any world knowledge. There exist attempts to resolve coreference based on world knowledge resources such as WordNet hierarchy, Wikipedia, semantic similarity, narravite chains (Ponzetto and Strube, 2006; Ng, 2007; Irwin et al., 2011; Rahman and Ng, 2012). Unfortunately, the corresponding resolvers were either not evaluated in large-scale challenges or did not show convincing performance in the challenges. Thus, the question remains open whether employing world knowledge can improve coreference resolution in large unfiltered corpora. In this paper, we investigate whether adding world knowledge for establishing more coreference links can 1292 improve coreference resolution. In the world knowledge employed, our work is most similar to the study on twin sentences presented in (Rahman and Ng, 201"
C12-1079,D08-1068,0,0.0285617,"prove coreference resolution in large unfiltered corpora. In this paper, we investigate whether adding world knowledge for establishing more coreference links can 1292 improve coreference resolution. In the world knowledge employed, our work is most similar to the study on twin sentences presented in (Rahman and Ng, 2012). However, instead of using world knowledge for generating features in a machine learning framework, we explore inference-based discourse processing. Regarding inference, our method may seem related to the coreference resolution research based on Markov Logic Networks (MLNs) (Poon and Domingos, 2008; Song et al., 2012). However, previous MLN-based work on coreference resolution does not incorporate inference rules based on world knowledge. The key contributions of our work are the following. First, we propose a novel solution to the overmerging problem in an inference-based framework. We extend (Hobbs et al., 1993)’s weighted abduction in order to accommodate unification weights and show how to learn the weights by applying machine learning techniques. For making large-scale processing and parameter learning in an abductive logic framework feasible, we employ a new efficient implementati"
C12-1079,W11-1901,0,0.0304668,"Missing"
C12-1079,D12-1071,0,0.0772712,"oreference in these sentences one requires world knowledge about people shouting when being angry and people shouting at someone who made a mistake, e.g., crashed a car. Surprisingly, most of the contemporary coreference resolution systems including the winners of the CoNLL-2011 and CoNLL-2012 shared tasks (Lee et al., 2011; Fernandes et al., 2012) do not exploit any world knowledge. There exist attempts to resolve coreference based on world knowledge resources such as WordNet hierarchy, Wikipedia, semantic similarity, narravite chains (Ponzetto and Strube, 2006; Ng, 2007; Irwin et al., 2011; Rahman and Ng, 2012). Unfortunately, the corresponding resolvers were either not evaluated in large-scale challenges or did not show convincing performance in the challenges. Thus, the question remains open whether employing world knowledge can improve coreference resolution in large unfiltered corpora. In this paper, we investigate whether adding world knowledge for establishing more coreference links can 1292 improve coreference resolution. In the world knowledge employed, our work is most similar to the study on twin sentences presented in (Rahman and Ng, 2012). However, instead of using world knowledge for ge"
C12-1079,D07-1002,0,0.141474,"Missing"
C12-1079,D12-1114,0,0.0143829,"ion in large unfiltered corpora. In this paper, we investigate whether adding world knowledge for establishing more coreference links can 1292 improve coreference resolution. In the world knowledge employed, our work is most similar to the study on twin sentences presented in (Rahman and Ng, 2012). However, instead of using world knowledge for generating features in a machine learning framework, we explore inference-based discourse processing. Regarding inference, our method may seem related to the coreference resolution research based on Markov Logic Networks (MLNs) (Poon and Domingos, 2008; Song et al., 2012). However, previous MLN-based work on coreference resolution does not incorporate inference rules based on world knowledge. The key contributions of our work are the following. First, we propose a novel solution to the overmerging problem in an inference-based framework. We extend (Hobbs et al., 1993)’s weighted abduction in order to accommodate unification weights and show how to learn the weights by applying machine learning techniques. For making large-scale processing and parameter learning in an abductive logic framework feasible, we employ a new efficient implementation of weighted abduc"
C12-1079,J01-4004,0,0.0947431,"rt, 2011) and attempts to use linguistically motivated constraints in order to prohibit incorrect unification in an abductive framework (Ovchinnikova et al., 2011; Ovchinnikova, 2012). However, the issue of overmerging was never systematically studied and the proposed solutions were never evaluated. In this paper, we investigate whether adding linguistically motivated features can help to block incorrect links in an inference-based framework. A lot of effort in NLP was put into coreference resolution systems ranging from rule-based (Lee et al., 2011, etc.) to machine learning-based resolvers (Soon et al., 2001; Ng and Cardie, 2002; Fernandes et al., 2012, etc.); see (Ng, 2010) for a detailed survey. Coreference resolution may require deep understanding of text, access to world knowledge, and inference ability. For example, (Levesque, 2011) considers twin sentences such as Ed shouted at Tim because he crashed the car and Ed shouted at Tim because he was angry. In order to resolve coreference in these sentences one requires world knowledge about people shouting when being angry and people shouting at someone who made a mistake, e.g., crashed a car. Surprisingly, most of the contemporary coreference r"
C12-1171,bentivogli-etal-2010-building,0,0.0699236,"Missing"
C12-1171,W09-3401,0,0.0239442,"Missing"
C12-1171,N10-1066,0,0.143081,"Supervised on Forward-Entailment and outperforms all other methods on Alternation/Negation), it performs drastically worse on sentence-level semantic relation recognition, sometimes with an f-score that is more than 20 points lower than the best performing method. These results suggest that it is important to jointly model alignment prediction and sentence-level semantic relation recognition so that globally optimal alignments are promoted. 2817 5 Related Work There are a number of existing works which explore the use of latent variable or structure models for recognizing textual entailment. Chang et al. (2010) proposed a discriminative linear model where alignments are treated as hidden structures, and the sentence-level semantic relation is derived based on the best latent alignment structure. They formulated the problem of predicting the best hidden structure as an Integer Linear Programming problem, where domain knowledge is encoded as constraints. Wang and Manning (2010) proposed a latent variable model where the model provides a conditional distribution of a sequence of edits, which can be seen as a transformation-based approach. In the model, edits are treated as hidden variables that populat"
C12-1171,D09-1122,0,0.0292576,"nt and its lemmas are the same. 1 if e has an argument of some predicates and the other sentence also contains an argument and its cases are the same. 1 if the head word of the bunsetsu in e is also contained in the other sentence. POS sequence in e head POS of e edit type of e the number of bunsetsus in e the number of shared arguments if both t and h in e are predicates 1 if t and t have the same particle Set relation type if e matches an entry in Japanese WordNet (Bond et al., 2009). 1 if e matches an entry in (Sumida et al., 2008). 1 if an entry in the verb entailment relation dictionary (Hashimoto et al., 2009) matches e 1 if an entry in the verb relation dictionary (Matsuyoshi et al., 2008) matches e the number of shared arguments of the parent of t and h if t and h are arguments 1 if each bunsetsu in e is an argument of a predicate. 1 if each bunsetsu in e has the same case. 1 if the POSs of the heads in e are the same 1 if the POS sequences of chunks in e are the same 1 if t and h are the same return unigram cosine value if the cosine similarity of two chunks in e is greater than the pre-defined threshold SAME_CASE_IN_{T,H} {T,H}_CONTAINS_{H,T}_LEMMA ΨA POS_SEQ HEADPOS TYPE SIZE NUM_SHARED_ARGS S"
C12-1171,N10-1145,0,0.0286773,"way to capture the affects of diverse linguistic phenomena and their interactions, where a set of linguistic phenomena are decomposed into units. By doing so it becomes possible to consider their effects on entailment independently. A number of previous works explores transformation-based entailment relation recognition. The approach of Stern et al. (2011) recognizes a sentence-level semantic relation through a proof which represents a sequence of edits from T to H produced by applying various entailment rules and the operations such as insertion, deletion, moving subtrees, etc. In addition, Heilman and Smith (2010) proposed a tree edit model which selects a sequence of edits using Tree Kernels, and Wang and Manning (2010) proposed a latent variable model which consider possible alignments as hidden structures. However, these model do not sufficiently represent interactions between linguistic phenomena such as factuality reversals caused by negation and flipping of entailment direction under downward-monotone contexts. In order to realize precise entailment relation recognition, we need to appropriately deal with semantic relations resulting from the interaction between linguistic phenomena. One of the m"
C12-1171,P10-1026,0,0.0164499,"f the cosine similarity of two chunks in e is greater than the pre-defined threshold SAME_CASE_IN_{T,H} {T,H}_CONTAINS_{H,T}_LEMMA ΨA POS_SEQ HEADPOS TYPE SIZE NUM_SHARED_ARGS SUB PARTICLE_SAME JAPANESE_WORDNET WIKIPEDIA_HYPERNYM-HYPONYM VERB_ENTAILMENT_REL VERB_RELATION_REL PARENT_NUM_SHARED_ARGS BOTH_HAVE_A_ROLE BOTH_HAVE_THE_SAME_ROLE HEAD_POS_SAME POS_SEQ_SAME EXACT_MATCH UNIGRAM_COSINE SIZE HEAD_LEMMA HEAD_WORD_CLASS number of bunsetsus in e lemma of the head of bunsetsu in e Word class of the head of bunsetsu in e. The word class information is extracted from the dictionary provided by (Kazama et al., 2010) 1 if the bunsetsu contains a negation Pair of the number of bunsetsus in e POS pair of the heads of bunsetsus in e 1 if the lemmas of the heads in e are the same 1 if the POS sequences of chunks in e are the same same as in ΨA same as in ΨA same as in ΨA same as in ΨA ΨS DEL, INS ΨS SUB ΨP – MONOTONE_{UP/DOWN} the context of e is upward-monotone or downwardmonotone. ΨC – COMPOSITION_RULE 1 if the tuple of semantic relations is included in a set of defined compositional rules. NEGATION SIZE HEAD_POS_PAIR HEAD_LEMMA_SAME POS_SEQ_SAME JAPANESE_WORDNET WIKIPEDIA_HYPERNYM-HYPONYM VERB_ENTAILMENT_R"
C12-1171,W02-2016,0,0.0460378,"68.8 68.4 54.9 100.0 100.0 100.0 100.0 reachable examples only 45.6 65.5 53.8 41.7 48.4 66.7 56.1 42.6 72.4 73.1 72.7 59.2 74.2 75.0 74.6 61.9 Sem. Rel. Acc. 54.9 53.5 51.9 55.5 100.0 44.5 44.9 51.6 55.2 100.0 31.8 35.0 47.5 43.7 55.2 59.9 58.8 59.8 62.6 49.2 49.4 59.5 62.3 38.5 37.7 60.2 46.4 Table 4: Performance of alignment prediction and sentence-level semantic relation recognition. 4.4 Preprocessing For each sentence, we conducted various forms of linguistic analysis: morphological analysis using MeCab (Kudo et al., 2004), syntactic parsing using the Japanese dependency parser, CaboCha (Kudo and Matsumoto, 2002) and predicate-argument structure analysis (Watanabe et al., 2010) to provide a basis for alignment and semantic relation classification. 4.5 Results Table 4 shows the experimental results of 10-fold cross validation for alignment prediction and sentence level semantic relation recognition. We can see that while the proposed method is less successful at reproducing gold standard alignments, it greatly outperforms Supervised Learning for sentence-level semantic relation recognition8 . We expected Supervised Learning to perform best on reachable examples, which should have the most straightforwa"
C12-1171,W04-3230,0,0.00953329,"beled) Prec. Rec. F1 42.6 62.5 50.6 37.5 45.6 63.3 53.0 38.6 67.1 67.8 67.4 51.3 68.0 68.8 68.4 54.9 100.0 100.0 100.0 100.0 reachable examples only 45.6 65.5 53.8 41.7 48.4 66.7 56.1 42.6 72.4 73.1 72.7 59.2 74.2 75.0 74.6 61.9 Sem. Rel. Acc. 54.9 53.5 51.9 55.5 100.0 44.5 44.9 51.6 55.2 100.0 31.8 35.0 47.5 43.7 55.2 59.9 58.8 59.8 62.6 49.2 49.4 59.5 62.3 38.5 37.7 60.2 46.4 Table 4: Performance of alignment prediction and sentence-level semantic relation recognition. 4.4 Preprocessing For each sentence, we conducted various forms of linguistic analysis: morphological analysis using MeCab (Kudo et al., 2004), syntactic parsing using the Japanese dependency parser, CaboCha (Kudo and Matsumoto, 2002) and predicate-argument structure analysis (Watanabe et al., 2010) to provide a basis for alignment and semantic relation classification. 4.5 Results Table 4 shows the experimental results of 10-fold cross validation for alignment prediction and sentence level semantic relation recognition. We can see that while the proposed method is less successful at reproducing gold standard alignments, it greatly outperforms Supervised Learning for sentence-level semantic relation recognition8 . We expected Supervi"
C12-1171,D08-1084,0,0.443103,"ent and semantic relation 2806 recognition between sentences is needed that learns the alignments which will generate the correct semantic relation by considering the interaction between diverse linguistic phenomena. In this paper, we propose a novel latent discriminative model that jointly handles predicting alignment edits, classification of their semantic relations and entailment relation recognition by providing a joint distribution of variables including alignment edits, their local semantic relations and sentence-level semantic relations. Inspired by the Natural Logic-based approach of (MacCartney et al., 2008), we incorporate the set of semantic relations and their composition rules from Natural Logic into our proposed model. In addition, our model can be trained from only sentence-level semantic relations to predict alignments and semantic relations that are consistent with Natural Logic composition. To the best of our knowledge, our study is the first work to propose a latent model for training a Natural Logic-based semantic relation recognition system that does not require alignment annotations and that jointly predicts plausible alignments and semantic relations between sentences, modeling a va"
C12-1171,C08-1066,0,0.274111,"Tree Kernels, and Wang and Manning (2010) proposed a latent variable model which consider possible alignments as hidden structures. However, these model do not sufficiently represent interactions between linguistic phenomena such as factuality reversals caused by negation and flipping of entailment direction under downward-monotone contexts. In order to realize precise entailment relation recognition, we need to appropriately deal with semantic relations resulting from the interaction between linguistic phenomena. One of the most promising approaches to RTE is Natural Logic-based recognition (MacCartney and Manning, 2008; MacCartney, 2009). This approach represents transformations from T to H with a set of three types of alignment edits (substitution, insertion and deletion), and assigns one of a set-theoretically defined semantic relations to each alignment edit. This approach is based on the principle of compositionality, i.e. the sentence-level semantic relation is derived by combining semantic relations of edits using pre-defined composition rules. By doing so, this approach makes progress toward precise sentence-level entailment relation recognition that considers linguistic phenomena and their interacti"
C12-1171,P10-1122,0,0.0720479,"Missing"
C12-1171,sumida-etal-2008-boosting,0,0.0188101,"s a nominative argument and the other sentence also contains a nominative argument and its lemmas are the same. 1 if e has an argument of some predicates and the other sentence also contains an argument and its cases are the same. 1 if the head word of the bunsetsu in e is also contained in the other sentence. POS sequence in e head POS of e edit type of e the number of bunsetsus in e the number of shared arguments if both t and h in e are predicates 1 if t and t have the same particle Set relation type if e matches an entry in Japanese WordNet (Bond et al., 2009). 1 if e matches an entry in (Sumida et al., 2008). 1 if an entry in the verb entailment relation dictionary (Hashimoto et al., 2009) matches e 1 if an entry in the verb relation dictionary (Matsuyoshi et al., 2008) matches e the number of shared arguments of the parent of t and h if t and h are arguments 1 if each bunsetsu in e is an argument of a predicate. 1 if each bunsetsu in e has the same case. 1 if the POSs of the heads in e are the same 1 if the POS sequences of chunks in e are the same 1 if t and h are the same return unigram cosine value if the cosine similarity of two chunks in e is greater than the pre-defined threshold SAME_CASE"
C12-1171,C10-1131,0,0.405266,"phenomena are decomposed into units. By doing so it becomes possible to consider their effects on entailment independently. A number of previous works explores transformation-based entailment relation recognition. The approach of Stern et al. (2011) recognizes a sentence-level semantic relation through a proof which represents a sequence of edits from T to H produced by applying various entailment rules and the operations such as insertion, deletion, moving subtrees, etc. In addition, Heilman and Smith (2010) proposed a tree edit model which selects a sequence of edits using Tree Kernels, and Wang and Manning (2010) proposed a latent variable model which consider possible alignments as hidden structures. However, these model do not sufficiently represent interactions between linguistic phenomena such as factuality reversals caused by negation and flipping of entailment direction under downward-monotone contexts. In order to realize precise entailment relation recognition, we need to appropriately deal with semantic relations resulting from the interaction between linguistic phenomena. One of the most promising approaches to RTE is Natural Logic-based recognition (MacCartney and Manning, 2008; MacCartney,"
C12-1171,D09-1082,0,0.0220178,"Text T and Hypothesis H. RTE is useful for many information access tasks that depend on natural language processing technologies, and a breakthrough would lead to significant progress in information retrieval, document summarization, and question answering, among other tasks. The majority of approaches proposed in previous work recognize entailment relations between a pair of texts by capturing lexical or structural correspondences. Methods include simple word overlap-based measures (Jijkoun and de Rijke, 2005) as well as alignment of syntactic and semantic dependencies (Sammons et al., 2009; Wang and Zhang, 2009). However, sentencelevel semantic relations are affected by various linguistic phenomena: not only lexical semantic relations (synonyms, antonyms) but also monotonicity (e.g. downward-monotone caused by scope of negation), implicative/factive expressions, quantifiers, etc. Thus similarity measures are insufficient to capture these phenomena and their interactions. Transformation-based approaches are one way to capture the affects of diverse linguistic phenomena and their interactions, where a set of linguistic phenomena are decomposed into units. By doing so it becomes possible to consider the"
C12-1171,P10-2018,1,0.83968,"65.5 53.8 41.7 48.4 66.7 56.1 42.6 72.4 73.1 72.7 59.2 74.2 75.0 74.6 61.9 Sem. Rel. Acc. 54.9 53.5 51.9 55.5 100.0 44.5 44.9 51.6 55.2 100.0 31.8 35.0 47.5 43.7 55.2 59.9 58.8 59.8 62.6 49.2 49.4 59.5 62.3 38.5 37.7 60.2 46.4 Table 4: Performance of alignment prediction and sentence-level semantic relation recognition. 4.4 Preprocessing For each sentence, we conducted various forms of linguistic analysis: morphological analysis using MeCab (Kudo et al., 2004), syntactic parsing using the Japanese dependency parser, CaboCha (Kudo and Matsumoto, 2002) and predicate-argument structure analysis (Watanabe et al., 2010) to provide a basis for alignment and semantic relation classification. 4.5 Results Table 4 shows the experimental results of 10-fold cross validation for alignment prediction and sentence level semantic relation recognition. We can see that while the proposed method is less successful at reproducing gold standard alignments, it greatly outperforms Supervised Learning for sentence-level semantic relation recognition8 . We expected Supervised Learning to perform best on reachable examples, which should have the most straightforward connection between alignment semantic relation labels and sente"
C16-1184,A00-2004,0,0.0251382,"throughout this study, we use the term segment to refer to a discourse segment in lyrics. 2 Related work This section reviews related work into the discourse structure of lyrics, with particular focus on the segmentation of lyrics using repeated patterns. Text segmentation is a classic text retrieval problem, and there exists a rich body of research into text segmentation in natural language processing. Various linguistic cues have been suggested to identify text boundaries such as expressions that frequently appear at the end of segments (Beeferman et al., 1999), contextual/topical changes (Choi, 2000; Malioutov and Barzilay, 2006; Riedl and Biemann, 2012), 1960 and word/entity repetition (Kan et al., 1998; Reynar, 1999). Although we share the same motivation as these studies, these text segmentation methods do not consider repeated patterns of phrasal segments because this type of repetition is nearly always absent in prose text. On the other hand, segments in lyrics often have repetitions (Austin et al., 2010) as shown in Section 3.1. We aim to capture the segment structure of lyrics using repeated patterns. Previous computational work into lyrics segmentation has focused on identifying"
C16-1184,W98-1123,0,0.213989,"work This section reviews related work into the discourse structure of lyrics, with particular focus on the segmentation of lyrics using repeated patterns. Text segmentation is a classic text retrieval problem, and there exists a rich body of research into text segmentation in natural language processing. Various linguistic cues have been suggested to identify text boundaries such as expressions that frequently appear at the end of segments (Beeferman et al., 1999), contextual/topical changes (Choi, 2000; Malioutov and Barzilay, 2006; Riedl and Biemann, 2012), 1960 and word/entity repetition (Kan et al., 1998; Reynar, 1999). Although we share the same motivation as these studies, these text segmentation methods do not consider repeated patterns of phrasal segments because this type of repetition is nearly always absent in prose text. On the other hand, segments in lyrics often have repetitions (Austin et al., 2010) as shown in Section 3.1. We aim to capture the segment structure of lyrics using repeated patterns. Previous computational work into lyrics segmentation has focused on identifying the segment labels of lyrics that are already segmented. For example, the structure of lyrics can be repres"
C16-1184,P06-1004,0,0.0386521,"this study, we use the term segment to refer to a discourse segment in lyrics. 2 Related work This section reviews related work into the discourse structure of lyrics, with particular focus on the segmentation of lyrics using repeated patterns. Text segmentation is a classic text retrieval problem, and there exists a rich body of research into text segmentation in natural language processing. Various linguistic cues have been suggested to identify text boundaries such as expressions that frequently appear at the end of segments (Beeferman et al., 1999), contextual/topical changes (Choi, 2000; Malioutov and Barzilay, 2006; Riedl and Biemann, 2012), 1960 and word/entity repetition (Kan et al., 1998; Reynar, 1999). Although we share the same motivation as these studies, these text segmentation methods do not consider repeated patterns of phrasal segments because this type of repetition is nearly always absent in prose text. On the other hand, segments in lyrics often have repetitions (Austin et al., 2010) as shown in Section 3.1. We aim to capture the segment structure of lyrics using repeated patterns. Previous computational work into lyrics segmentation has focused on identifying the segment labels of lyrics t"
C16-1184,J02-1002,0,0.0380659,"ther. 5.1 Performance evaluation metrics We used two sets of metrics to evaluate the performance of each model for the task. One was standardly used in audio music segmentation, i.e., the precision, recall, and F-measure of identifying segment boundaries. Precision is the ratio of correctly predicted boundaries over all predicted boundaries, recall is the ratio of correctly predicted boundaries over all true boundaries, and F-measure is the harmonic mean of precision and recall. The other set was standardly used in text segmentation literature: Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002). Pk is the probability of segmentation error that evaluates whether two lines li and lj in lyrics fewer than k lines apart are incorrectly concatenated or divided by a segmentation model. Pk is considered a more suitable measure than F-measure in text segmentation because it assigns partial credit to nearly correct estimations. WD is a variant of Pk that resolves a problem of Pk by penalizing false positives. We set the window size k of Pk and WD to 1965 1 2 1: I think of you 2: in the spring when gentle rains turn to showers 3: I think of you 4: when the summer breezes dance through the flow"
C16-1184,P99-1046,0,0.120921,"reviews related work into the discourse structure of lyrics, with particular focus on the segmentation of lyrics using repeated patterns. Text segmentation is a classic text retrieval problem, and there exists a rich body of research into text segmentation in natural language processing. Various linguistic cues have been suggested to identify text boundaries such as expressions that frequently appear at the end of segments (Beeferman et al., 1999), contextual/topical changes (Choi, 2000; Malioutov and Barzilay, 2006; Riedl and Biemann, 2012), 1960 and word/entity repetition (Kan et al., 1998; Reynar, 1999). Although we share the same motivation as these studies, these text segmentation methods do not consider repeated patterns of phrasal segments because this type of repetition is nearly always absent in prose text. On the other hand, segments in lyrics often have repetitions (Austin et al., 2010) as shown in Section 3.1. We aim to capture the segment structure of lyrics using repeated patterns. Previous computational work into lyrics segmentation has focused on identifying the segment labels of lyrics that are already segmented. For example, the structure of lyrics can be represented Figure 2:"
C16-1184,W12-3307,0,0.0779148,"gment to refer to a discourse segment in lyrics. 2 Related work This section reviews related work into the discourse structure of lyrics, with particular focus on the segmentation of lyrics using repeated patterns. Text segmentation is a classic text retrieval problem, and there exists a rich body of research into text segmentation in natural language processing. Various linguistic cues have been suggested to identify text boundaries such as expressions that frequently appear at the end of segments (Beeferman et al., 1999), contextual/topical changes (Choi, 2000; Malioutov and Barzilay, 2006; Riedl and Biemann, 2012), 1960 and word/entity repetition (Kan et al., 1998; Reynar, 1999). Although we share the same motivation as these studies, these text segmentation methods do not consider repeated patterns of phrasal segments because this type of repetition is nearly always absent in prose text. On the other hand, segments in lyrics often have repetitions (Austin et al., 2010) as shown in Section 3.1. We aim to capture the segment structure of lyrics using repeated patterns. Previous computational work into lyrics segmentation has focused on identifying the segment labels of lyrics that are already segmented."
C16-1184,N03-1033,0,0.0409146,"Missing"
C16-1184,Y14-1049,1,0.770092,"of discourse segments in lyrics. 1 Introduction Lyrics are an important element of popular music. They provide an effective means to express the message and emotion of music. Similar to prose text, lyrics are a discourse: i.e., they are, typically, a sequence of related lines, rather than an unconnected stream of lines of arbitrary order. Thus, like texts, lyrics also have a discourse structure consisting discourse segments. Each discourse segment exhibits an individual topic in discourse, and the transition of topics over successive discourse segments constitutes a flow (Austin et al., 2010; Watanabe et al., 2014). Unlike prose text, lyrics have their own peculiar properties, such as frequent repetition of identical or similar phrases and extensive use of rhyme and refrain (Austin et al., 2010), as illustrated in Figure 1. Analogous to prose text, the lyrics in Figure 1 can be viewed as a sequence of discourse segments, wherein each segment is depicted by a colored box. 1 and  3 appear repeatedly (e.g., segments  4 and  8 are identical to segment ), 1 Segments  which is not typically observed in prose text. Our goal is to reveal the discourse structure of lyrics in popular music by quantitatively"
C16-1266,P09-1068,0,0.0361461,"ra et al., 2014) estimate the plausibility of a subject–verb–object (SVO) tuple. These studies model a type of CSP: a subject or an object can be regarded as an additional context to restrict a set of possible fillers of a query predicate. However, the context captured in our study is not a local context of a query predicate but that of a query argument, working as a validator of the narrative consistency between a query predicate and events in which a query argument participates (see Section 4). In addition, the modeling of a narrative consistency between events has been studied extensively (Chambers and Jurafsky, 2009; Modi and Titov, 2014; Granroth-wilding and Clark, 2016, etc.). Chambers and Jurafsky (2009) acquired sets of narratively related events sharing at least one entity (e.g., {X commit a crime, police arrest X, X convict, ...}) by collecting a set of verbal mentions sharing coreferring arguments in a large corpus. The relatedness between two events was then estimated statistically through pointwise mutual information (Church and Hanks, 1990). To address the data sparseness problem of Chambers and Jurafsky (2009), Granroth-wilding and Clark (2016) proposed an architecture based on distributed rep"
C16-1266,J81-4005,0,0.754299,"Missing"
C16-1266,P07-1028,0,0.244953,"fundamental approach to modeling SP is to count the co-occurrences of predicates and their arguments on a large corpus. As simply counting a predicate-argument pair causes data sparseness problem, previous SP models adopted methods for smoothing co-occurrence counts. Earlier efforts combined a manually crafted thesaurus with the acquired distribution (Resnik, 1996; Li and Abe, 1998). Another approach used a latent probabilistic model to obtain a semantically smoothed probability distribution (Rooth et al., 1999; S´eaghdha and Korhonen, 2014). Other directions include example-based approaches (Erk, 2007). However, these studies differ from ours in that they do not consider the context-sensitivity. Some previous studies (Ritter et al., 2010; Van de Cruys, 2014; Kawahara et al., 2014) estimate the plausibility of a subject–verb–object (SVO) tuple. These studies model a type of CSP: a subject or an object can be regarded as an additional context to restrict a set of possible fillers of a query predicate. However, the context captured in our study is not a local context of a query predicate but that of a query argument, working as a validator of the narrative consistency between a query predicate"
C16-1266,D15-1038,0,0.0148855,"s candidates for antecedents. As he(?) is a subject of the predicate made a statement, sc(⟨cs , v, o⟩) gives the preference of cs as an antecedent of the pronoun, where v = made and o = statement. In the example, cs can be a preceding noun with its context attached (if any) or without its context: ⟨Chen, declare, determination⟩subj , ⟨Chen, declare, determination⟩obj , speech, or people. We expect that an SP model prefers the correct antecedent ⟨Chen, declare, determination⟩subj over the others. We measure the ability of a model for selecting the correct cluster by using a mean quantile (MQ) (Guu et al., 2015). Let p be the target pronoun, Cp+ the correct cluster for the pronoun, and Np the set of negative (incorrect) clusters. MQ for the pronoun p is defined as: MQ(p) = |{C − ∈ Np |sp(C − , p) &lt; sp(Cp+ , p)}| . |Np | (10) Intuitively, MQ(p) represents the ratio where a correct cluster Cp+ is preferred to incorrect clusters C − ∈ Np by the model. In general, a cluster contains multiple mentions; thus, we simply consider the maximal of the scores in a cluster C: for example, sp(C, p) = maxm∈C sc(m, v, o). 7 To ensure that the two subsets are strictly disjoint, we prohibited a test instance from bein"
C16-1266,D14-1163,0,0.248412,"y not. The challenges in modeling a CSP are as follows: (i) data sparseness caused by the incorporation of context words, and (ii) an effective means of incorporating context-sensitivity into SP. To address these issues, we propose to extend the state-of-the-art SP model by using a distributed representation (Van de Cruys, 2014). The distributed framework alleviates the data sparseness problem and naturally injects the contextual information of a query argument into its word vector based on compositional distributional semantics (Socher et al., 2012; Socher et al., 2013; Muraoka et al., 2014; Hashimoto et al., 2014, etc.). We empirically evaluate the impacts of incorporating context-sensitivity into SP for two tasks: (i) context-sensitive pseudo-disambiguation, a novel benchmark tailored for evaluating CSP models, and (ii) coreference cluster ranking for pronominal anaphora resolution. The results demonstrate that our approach achieves considerable improvements. Moreover, the results suggest that CSP is a meaningful problem setting and that our model captures the context-sensitivity of SP. 2 Related Work A fundamental approach to modeling SP is to count the co-occurrences of predicates and their argumen"
C16-1266,N06-2015,0,0.23662,"uples; B hereafter) and 4,824,394 T YPE A positive instances (1,500,990 unique tuples; A hereafter). 6 Evaluation To check whether the CSP model can properly learn the conventional SP and narrative consistency, we first evaluated the CSP model against Van de Cruys’ model by using a pseudo-disambiguation test, a binary classification task of discriminating a positive SVO tuple from its pseudo-negative counterpart. We then evaluated the effectiveness of the CSP model in a realistic problem setting, in which the disambiguation test is created from coreference annotations of the OntoNotes corpus (Hovy et al., 2006). 6.1 Parameters We set the dimension of word embedding d = 50 and the dimension of hidden layer h = 50. The word embeddings are initialized with the publicly available word vectors trained through GloVe (Pennington et al., 2014)5 and updated through back propagation. We updated weights by using Adam (Kingma and Ba, 2014) with a mini-batch size of 1,000 and 30 epochs6 . To evaluate the effectiveness of the CSP model, we replicated the SVO model of Van de Cruys (2014) by training the CSP model only with T YPE A instances (henceforth, SP). 6.2 Pseudo-disambiguation test Inspired by the conventio"
C16-1266,C12-1079,1,0.824571,"ord vector, which is explained in the rest of this section. Similarly, for ⟨cs , v, o⟩ and ⟨cs , v, co ⟩, we extend Equation (3) as follows: gc′ s ,v,o = f (ϕc (cs ) ⊕ ϕ(v) ⊕ ϕ(o)), and gc′ s ,v,co = f (ϕc (cs ) ⊕ ϕ(v) ⊕ ϕc (co )). As a context of w, we can potentially consider various types of modifiers, such as predicates, adverbs, appositives, and genitives, that affects the preference score of a query argument. In this study, as a first step, we restrict the context information to PASs along the lines of the previous studies that utilize event-to-event relations in an anaphora resolution (Inoue et al., 2012; Peng et al., 2015). Specifically, we first assume a context of a query argument be a single PAS which takes the query argument as its argument (referred to as a context-PAS). Then we represent w with its context-PAS as cw = ⟨sw , pw , ow ⟩r , where sw and ow are respectively the subject and object of a predicate pw that syntactically governs the word w (i.e., either sw or ow is w). r indicates the grammatical role of the query argument w in the context-PAS, whose value is either subj (when w = sw ) or obj (when w = ow ). For example, for text (4), the context for the query object Bob, modifi"
C16-1266,P14-1097,0,0.0213887,"s data sparseness problem, previous SP models adopted methods for smoothing co-occurrence counts. Earlier efforts combined a manually crafted thesaurus with the acquired distribution (Resnik, 1996; Li and Abe, 1998). Another approach used a latent probabilistic model to obtain a semantically smoothed probability distribution (Rooth et al., 1999; S´eaghdha and Korhonen, 2014). Other directions include example-based approaches (Erk, 2007). However, these studies differ from ours in that they do not consider the context-sensitivity. Some previous studies (Ritter et al., 2010; Van de Cruys, 2014; Kawahara et al., 2014) estimate the plausibility of a subject–verb–object (SVO) tuple. These studies model a type of CSP: a subject or an object can be regarded as an additional context to restrict a set of possible fillers of a query predicate. However, the context captured in our study is not a local context of a query predicate but that of a query argument, working as a validator of the narrative consistency between a query predicate and events in which a query argument participates (see Section 4). In addition, the modeling of a narrative consistency between events has been studied extensively (Chambers and Jur"
C16-1266,J98-2002,0,0.0307096,"ution. The results demonstrate that our approach achieves considerable improvements. Moreover, the results suggest that CSP is a meaningful problem setting and that our model captures the context-sensitivity of SP. 2 Related Work A fundamental approach to modeling SP is to count the co-occurrences of predicates and their arguments on a large corpus. As simply counting a predicate-argument pair causes data sparseness problem, previous SP models adopted methods for smoothing co-occurrence counts. Earlier efforts combined a manually crafted thesaurus with the acquired distribution (Resnik, 1996; Li and Abe, 1998). Another approach used a latent probabilistic model to obtain a semantically smoothed probability distribution (Rooth et al., 1999; S´eaghdha and Korhonen, 2014). Other directions include example-based approaches (Erk, 2007). However, these studies differ from ours in that they do not consider the context-sensitivity. Some previous studies (Ritter et al., 2010; Van de Cruys, 2014; Kawahara et al., 2014) estimate the plausibility of a subject–verb–object (SVO) tuple. These studies model a type of CSP: a subject or an object can be regarded as an additional context to restrict a set of possible"
C16-1266,P14-5010,0,0.00333912,"secret, hurt, someone⟩ from B1 and B2 , respectively. To generate T YPE A negative instance, we follow the same procedure described in Section 3 but use probabilistic negative sampling instead. 2 Although Van de Cruys (2014) used random sampling to generate negative instances, we used probabilistic sampling because our preliminary experiment shows a better performance. 2833 5.3 Dataset We identified syntactic dependency relations and coreference relations in 4.5 billion sentences extracted from the ClueWeb12 corpus3 , that is, a large collection of Web documents, by applying Stanford CoreNLP (Manning et al., 2014). To reduce noises from the obtained coreference relations, we applied the following heuristics to skim only highly plausible coreference relations off from the pool: (i) the coreference relation must be intrasentential, (ii) the head words of the coreferent mentions must be identical and nonpronominal (e.g. John–John but not John–boy)4 . Furthermore, we discarded T YPE A and T YPE B instances containing low-frequency words so that all our training instances include only the top 50k frequent verbs, 50k frequent nouns, and 50k frequent adjectives. We replaced all the rare words (occurring less"
C16-1266,W14-1606,0,0.11654,"plausibility of a subject–verb–object (SVO) tuple. These studies model a type of CSP: a subject or an object can be regarded as an additional context to restrict a set of possible fillers of a query predicate. However, the context captured in our study is not a local context of a query predicate but that of a query argument, working as a validator of the narrative consistency between a query predicate and events in which a query argument participates (see Section 4). In addition, the modeling of a narrative consistency between events has been studied extensively (Chambers and Jurafsky, 2009; Modi and Titov, 2014; Granroth-wilding and Clark, 2016, etc.). Chambers and Jurafsky (2009) acquired sets of narratively related events sharing at least one entity (e.g., {X commit a crime, police arrest X, X convict, ...}) by collecting a set of verbal mentions sharing coreferring arguments in a large corpus. The relatedness between two events was then estimated statistically through pointwise mutual information (Church and Hanks, 1990). To address the data sparseness problem of Chambers and Jurafsky (2009), Granroth-wilding and Clark (2016) proposed an architecture based on distributed representation to judge t"
C16-1266,Y14-1010,1,0.929498,"ut other candidates may not. The challenges in modeling a CSP are as follows: (i) data sparseness caused by the incorporation of context words, and (ii) an effective means of incorporating context-sensitivity into SP. To address these issues, we propose to extend the state-of-the-art SP model by using a distributed representation (Van de Cruys, 2014). The distributed framework alleviates the data sparseness problem and naturally injects the contextual information of a query argument into its word vector based on compositional distributional semantics (Socher et al., 2012; Socher et al., 2013; Muraoka et al., 2014; Hashimoto et al., 2014, etc.). We empirically evaluate the impacts of incorporating context-sensitivity into SP for two tasks: (i) context-sensitive pseudo-disambiguation, a novel benchmark tailored for evaluating CSP models, and (ii) coreference cluster ranking for pronominal anaphora resolution. The results demonstrate that our approach achieves considerable improvements. Moreover, the results suggest that CSP is a meaningful problem setting and that our model captures the context-sensitivity of SP. 2 Related Work A fundamental approach to modeling SP is to count the co-occurrences of pred"
C16-1266,N15-1082,0,0.0834339,"explained in the rest of this section. Similarly, for ⟨cs , v, o⟩ and ⟨cs , v, co ⟩, we extend Equation (3) as follows: gc′ s ,v,o = f (ϕc (cs ) ⊕ ϕ(v) ⊕ ϕ(o)), and gc′ s ,v,co = f (ϕc (cs ) ⊕ ϕ(v) ⊕ ϕc (co )). As a context of w, we can potentially consider various types of modifiers, such as predicates, adverbs, appositives, and genitives, that affects the preference score of a query argument. In this study, as a first step, we restrict the context information to PASs along the lines of the previous studies that utilize event-to-event relations in an anaphora resolution (Inoue et al., 2012; Peng et al., 2015). Specifically, we first assume a context of a query argument be a single PAS which takes the query argument as its argument (referred to as a context-PAS). Then we represent w with its context-PAS as cw = ⟨sw , pw , ow ⟩r , where sw and ow are respectively the subject and object of a predicate pw that syntactically governs the word w (i.e., either sw or ow is w). r indicates the grammatical role of the query argument w in the context-PAS, whose value is either subj (when w = sw ) or obj (when w = ow ). For example, for text (4), the context for the query object Bob, modified by the PAS of the"
C16-1266,D14-1162,0,0.0798895,"luated the CSP model against Van de Cruys’ model by using a pseudo-disambiguation test, a binary classification task of discriminating a positive SVO tuple from its pseudo-negative counterpart. We then evaluated the effectiveness of the CSP model in a realistic problem setting, in which the disambiguation test is created from coreference annotations of the OntoNotes corpus (Hovy et al., 2006). 6.1 Parameters We set the dimension of word embedding d = 50 and the dimension of hidden layer h = 50. The word embeddings are initialized with the publicly available word vectors trained through GloVe (Pennington et al., 2014)5 and updated through back propagation. We updated weights by using Adam (Kingma and Ba, 2014) with a mini-batch size of 1,000 and 30 epochs6 . To evaluate the effectiveness of the CSP model, we replicated the SVO model of Van de Cruys (2014) by training the CSP model only with T YPE A instances (henceforth, SP). 6.2 Pseudo-disambiguation test Inspired by the conventional SP model (Erk, 2007; Van de Cruys, 2014, etc.), we set up three binary classification tasks. We performed hold-out validation on datasets A and B. 6.2.1 Tasks Pseudo-disambiguation (PD) discriminates a positive non-context-in"
C16-1266,P10-1044,0,0.131649,"Missing"
C16-1266,P99-1014,0,0.812898,"phrase as an argument. For example, the object slot of eat is generally filled by a noun phrase denoting food such as an apple; it is rarely filled by a phrase that is not food such as a watch. As the knowledge of SP has been recognized as key for many natural language processing tasks, including semantic role labeling and anaphora resolution, automatic acquisition of SP knowledge has persisted as a popular research topic. In literature, a variety of computational models for SP have been proposed, ranging from thesaurus-based approaches (Resnik, 1996), to probabilistic latent variable models (Rooth et al., 1999; S´eaghdha and Korhonen, 2014), and distributed approaches (Van de Cruys, 2014). Conventionally, SP is defined as the context-independent acceptability of a word as a filler of a predicate in the sense of a semantic type. Suppose that we must identify the referent of him(j) : (1) John(i) beat Bob(j) . Mary comforted him(j) . Henceforth, we call a predicate (e.g., comfort) and an argument (e.g., John and Bob) to be examined as a query predicate and query argument, respectively. Conventional SP models judge the appropriateness of John(i) and Bob(j) in terms of whether comfort can take each noun"
C16-1266,J14-3005,0,0.0513825,"Missing"
C16-1266,D12-1110,0,0.672745,"ntexts relevant to narrative consistency but other candidates may not. The challenges in modeling a CSP are as follows: (i) data sparseness caused by the incorporation of context words, and (ii) an effective means of incorporating context-sensitivity into SP. To address these issues, we propose to extend the state-of-the-art SP model by using a distributed representation (Van de Cruys, 2014). The distributed framework alleviates the data sparseness problem and naturally injects the contextual information of a query argument into its word vector based on compositional distributional semantics (Socher et al., 2012; Socher et al., 2013; Muraoka et al., 2014; Hashimoto et al., 2014, etc.). We empirically evaluate the impacts of incorporating context-sensitivity into SP for two tasks: (i) context-sensitive pseudo-disambiguation, a novel benchmark tailored for evaluating CSP models, and (ii) coreference cluster ranking for pronominal anaphora resolution. The results demonstrate that our approach achieves considerable improvements. Moreover, the results suggest that CSP is a meaningful problem setting and that our model captures the context-sensitivity of SP. 2 Related Work A fundamental approach to modelin"
C16-1266,P13-1045,0,0.254546,"rrative consistency but other candidates may not. The challenges in modeling a CSP are as follows: (i) data sparseness caused by the incorporation of context words, and (ii) an effective means of incorporating context-sensitivity into SP. To address these issues, we propose to extend the state-of-the-art SP model by using a distributed representation (Van de Cruys, 2014). The distributed framework alleviates the data sparseness problem and naturally injects the contextual information of a query argument into its word vector based on compositional distributional semantics (Socher et al., 2012; Socher et al., 2013; Muraoka et al., 2014; Hashimoto et al., 2014, etc.). We empirically evaluate the impacts of incorporating context-sensitivity into SP for two tasks: (i) context-sensitive pseudo-disambiguation, a novel benchmark tailored for evaluating CSP models, and (ii) coreference cluster ranking for pronominal anaphora resolution. The results demonstrate that our approach achieves considerable improvements. Moreover, the results suggest that CSP is a meaningful problem setting and that our model captures the context-sensitivity of SP. 2 Related Work A fundamental approach to modeling SP is to count the"
C16-1266,D14-1004,0,0.303167,"Missing"
C16-1266,N16-1114,0,0.0603717,"Missing"
C16-1266,J90-1003,0,\N,Missing
C18-1009,P16-1061,0,0.0450151,"Missing"
C18-1009,P17-1044,0,0.0506496,""","",$ ℎ& "",$ ℎ-"",),$ ?"",(,$ Attention & ℎ& "",$ ℎ(,( … … ℎ& (,+ (c) POOL-SELFATT ?"",$ Self-Attention ℎ′"",$ ℎ"",$ Max Pooling ℎ"",$ ?"",$ ?"",(,$ ?"","",$ ?"",),$ ℎ& "",$ ℎ& "",$ ℎ& "",$ ℎ& (,$ ℎ& "",$ ℎ& ),$ ?"",$ ?"",( … … ?"",+ Figure 3: Three variants of interaction layers. in the corpus, when an argument is omitted (i.e., zero-anaphora), the antecedent is identified with an appropriate semantic role, which is a prominent problem in Japanese semantic analysis and is the primary target of this study. 3 Base Model Our proposed models extend end-to-end style SRL systems using deep bi-RNN (Zhou and Xu, 2015; He et al., 2017; Ouchi et al., 2017) to combine mechanisms that consider multiple predicate interactions. Figure 2a shows the network of our base model. Formally, given a word sequence w = w1 , ..., wn and a target predicate position pi in p, the model outputs a label probability for each word position: p(ci,1 |i, p, w), ..., p(ci,n |i, p, w). Here, ci,t ∈ {NOM, ACC, DAT, NONE} represents the argument label of the word wt for the target predicate wpi . The input layer creates a vector h0i,t ∈ Rdw +1 for each pair of a predicate wpi and a word wt by concatenating a word embedding e(wt ) ∈ Rdw and a binary val"
C18-1009,D15-1260,0,0.183575,"ricted to the identification of intra-sentential predicate-argument relations (Matsubayashi and Inui, 2017). DAT DAT NOM 記者 [団] が 尋ねる と [reporters]-NOM ask then NOM [首相] が 答え た 。 [prime minister]-NOM answer-PAST ‘The prime minister answered when the reporters asked.’ Figure 1: Example of PAS analysis. The dashed lines represent the predicate-argument relations. “[reporters]-NOM ask then” constitutes a subordinate clause and “[prime minister]-NOM answerPAST ” constitutes a matrix clause. One promising approach for addressing this problem is to model argument sharing across multiple predicates (Iida et al., 2015; Ouchi et al., 2015; Ouchi et al., 2017). In Figure 1, for example, one can find very limited syntactic clues for predicting the long-distance dative relation between answer and reporters. However, the relation must be easy to identify for human readers who know that the person who asks a question is likely to be answered; namely, the nominative argument of ask is likely to be shared with the dative argument of answer. Capturing such inter-predicative dependencies has, therefore, been considered crucial of Japanese PAS analysis. This work is licensed under a Creative Commons Attribution 4.0 I"
C18-1009,D16-1132,0,0.538479,"odel that predicts arguments with the average score of the 10 MP-P OOL -ATT models further improves the overall F1 by 1.4 points from that of a single model, achieving state-of-the-art accuracy for NTC 1.5. Table 3 shows the F1 score for each case label. In a comparison of the single models, although our MP-P OOL -ATT model slightly degrades the scores of NOM and ACC on the Dep cases compared to the state-of-the-art model (M&I 2017), it greatly improves the scores for DAT and the Zero cases. Regarding the ensemble models, MP-P OOL -ATT improves the scores for all cases. Iida et al. (2015) and Iida et al. (2016) report Japanese subject anaphora resolution systems, designed to predict only Zero NOM arguments. It is not straightforward to directly compare their results with ours due to the differences in the experimental settings. However, our best performing model outperforms the 2 We discussed this negative result, including the implementation details, with one of the authors of Ouchi et al. (2017). However, we could not find a plausible reason for the results. 3 For the purpose of a strict comparison with Ouchi et al. (2017), we re-evaluate the model of Matsubayashi and Inui (2017) by excluding the"
C18-1009,P09-2022,0,0.267976,"Missing"
C18-1009,N16-1099,1,0.884231,"Missing"
C18-1009,N15-1121,0,0.0502567,"Missing"
C18-1009,Q16-1037,0,0.0184357,"7) used Grid RNN to incorporate intermediate representations of the prediction for one predicate generated by an RNN layer into the inputs of the RNN layer for another predicate. However, in this model, since the information of multiple predicates also propagates through the RNNs, the integration of the prediction information is influenced by word order and distance, which is not necessarily important for aspects of syntactic and semantic relations. Consequently, there might be information loss caused by the surface distances of words, as previous work had pointed out for RNN language models (Linzen et al., 2016). In this study, we propose new Japanese PAS analysis models that integrate the prediction information of arguments in multiple predicates. We extend a standard end-to-end style deep bi-RNN model (Figure 2a) and introduce components that consider the multiple predicate interactions into both the input and last layers (Figures 2b and 3). In contrast to Grid RNN, our extended models stack the extra layers using pooling and attention mechanisms on top of a deep bi-RNN so that they can directly associate the label prediction information for a target (predicate, word) pair with the predictions for"
C18-1009,D17-1159,0,0.0381514,"Missing"
C18-1009,I17-2022,1,0.89092,"art performance in the overall F1 on a standard benchmark corpus. 2 Task In this paper, we employ a task definition based on the NAIST Text Corpus (NTC) (Iida et al., 2010; Iida et al., 2017), a commonly used benchmark corpus annotated with nominative (NOM), accusative (ACC), and dative (DAT) arguments for predicates. Given a tokenized sentence w = w1 , ..., wn and its predicate positions p = p1 , ..., pq , our task is to identify at most one head of the filler tokens for each argument slot of each predicate. In this study, we follow the setting of Iida et al. (2015), Ouchi et al. (2017), and Matsubayashi and Inui (2017), and focus only on analyzing arguments in a target sentence. In addition, we exclude argument instances that are in the same bunsetsu, a base phrase unit in Japanese, as the target predicate, following Ouchi et al. (2017), which we will compare with the results in experiments. The semantic labels used in NTC may seem to be rather syntactic as they are named nominative, accusative, etc. However, this annotation task markedly differs from shallow syntactic parsing and is, in fact, more like a semantic role labeling (SRL) task including implicit argument prediction. First, the semantic labels in"
C18-1009,P15-1093,0,0.558682,"ification of intra-sentential predicate-argument relations (Matsubayashi and Inui, 2017). DAT DAT NOM 記者 [団] が 尋ねる と [reporters]-NOM ask then NOM [首相] が 答え た 。 [prime minister]-NOM answer-PAST ‘The prime minister answered when the reporters asked.’ Figure 1: Example of PAS analysis. The dashed lines represent the predicate-argument relations. “[reporters]-NOM ask then” constitutes a subordinate clause and “[prime minister]-NOM answerPAST ” constitutes a matrix clause. One promising approach for addressing this problem is to model argument sharing across multiple predicates (Iida et al., 2015; Ouchi et al., 2015; Ouchi et al., 2017). In Figure 1, for example, one can find very limited syntactic clues for predicting the long-distance dative relation between answer and reporters. However, the relation must be easy to identify for human readers who know that the person who asks a question is likely to be answered; namely, the nominative argument of ask is likely to be shared with the dative argument of answer. Capturing such inter-predicative dependencies has, therefore, been considered crucial of Japanese PAS analysis. This work is licensed under a Creative Commons Attribution 4.0 International License"
C18-1009,P17-1146,0,0.106962,"entential predicate-argument relations (Matsubayashi and Inui, 2017). DAT DAT NOM 記者 [団] が 尋ねる と [reporters]-NOM ask then NOM [首相] が 答え た 。 [prime minister]-NOM answer-PAST ‘The prime minister answered when the reporters asked.’ Figure 1: Example of PAS analysis. The dashed lines represent the predicate-argument relations. “[reporters]-NOM ask then” constitutes a subordinate clause and “[prime minister]-NOM answerPAST ” constitutes a matrix clause. One promising approach for addressing this problem is to model argument sharing across multiple predicates (Iida et al., 2015; Ouchi et al., 2015; Ouchi et al., 2017). In Figure 1, for example, one can find very limited syntactic clues for predicting the long-distance dative relation between answer and reporters. However, the relation must be easy to identify for human readers who know that the person who asks a question is likely to be answered; namely, the nominative argument of ask is likely to be shared with the dative argument of answer. Capturing such inter-predicative dependencies has, therefore, been considered crucial of Japanese PAS analysis. This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.o"
C18-1009,J05-1004,0,0.0788769,"Japanese, as the target predicate, following Ouchi et al. (2017), which we will compare with the results in experiments. The semantic labels used in NTC may seem to be rather syntactic as they are named nominative, accusative, etc. However, this annotation task markedly differs from shallow syntactic parsing and is, in fact, more like a semantic role labeling (SRL) task including implicit argument prediction. First, the semantic labels in NTC generalize case alteration caused by voice alteration and thus represent semantic roles analogous to ARG0, ARG1, etc. in the PropBank-style annotation (Palmer et al., 2005). Second, 95 (a) POOL (b) ATT-POOL ?"",(,$ Max Pooling ?"",(,$ ℎ& "",$ ℎ& (,$ ?"","",$ ℎ& "",$ ℎ& "",$ ?"",),$ ℎ& "",$ ℎ& ),$ ℎ& "",$ ℎ"",$ ℎ-"",(,$ Max Pooling ?"","",$ ?"",),$ ℎ& "",$ ℎ-"","",$ ℎ& "",$ ℎ-"",),$ ?"",(,$ Attention & ℎ& "",$ ℎ(,( … … ℎ& (,+ (c) POOL-SELFATT ?"",$ Self-Attention ℎ′"",$ ℎ"",$ Max Pooling ℎ"",$ ?"",$ ?"",(,$ ?"","",$ ?"",),$ ℎ& "",$ ℎ& "",$ ℎ& "",$ ℎ& (,$ ℎ& "",$ ℎ& ),$ ?"",$ ?"",( … … ?"",+ Figure 3: Three variants of interaction layers. in the corpus, when an argument is omitted (i.e., zero-anaphora), the antecedent is identified with an appropriate semantic role, which is a prominent problem in Jap"
C18-1009,I11-1085,0,0.449629,"Missing"
C18-1009,P16-1117,0,0.204085,"Missing"
C18-1009,D08-1055,0,0.65263,"ents We evaluated the impacts of our extensions and compared their performances to those of previous studies. Our main hypothesis is that the pooling and attention mechanisms are both useful for capturing different types of argument interactions as we explained in Section 4 and do work complementarily of each other to improve the prediction accuracy, especially for arguments in a long-distance dependency. 5.1 Settings 5.2 Dataset and Implementation Details The experiments were performed on NTC 1.5. We divided the corpus into the commonly used divisions of training, development, and test sets (Taira et al., 2008), each of which includes 24,283, 4,833, and 9,284 sentences, respectively. NTC represents each argument of a predicate by indicating a coreference cluster in a text. For each given predicate-argument slot, we count a system’s output as correct if the output token is included in the coreference cluster corresponding to the slot fillers. The evaluation is performed on the basis of the precision, recall, and F1 score. The hyperparameters were selected to obtain a maximum F1 on the development set. The details of the hyperparameter selection and preprocessing are described in the supplemental mate"
C18-1009,D14-1041,0,0.0553115,"Missing"
C18-1009,P15-1109,0,0.0829205,",$ ?"",),$ ℎ& "",$ ℎ-"","",$ ℎ& "",$ ℎ-"",),$ ?"",(,$ Attention & ℎ& "",$ ℎ(,( … … ℎ& (,+ (c) POOL-SELFATT ?"",$ Self-Attention ℎ′"",$ ℎ"",$ Max Pooling ℎ"",$ ?"",$ ?"",(,$ ?"","",$ ?"",),$ ℎ& "",$ ℎ& "",$ ℎ& "",$ ℎ& (,$ ℎ& "",$ ℎ& ),$ ?"",$ ?"",( … … ?"",+ Figure 3: Three variants of interaction layers. in the corpus, when an argument is omitted (i.e., zero-anaphora), the antecedent is identified with an appropriate semantic role, which is a prominent problem in Japanese semantic analysis and is the primary target of this study. 3 Base Model Our proposed models extend end-to-end style SRL systems using deep bi-RNN (Zhou and Xu, 2015; He et al., 2017; Ouchi et al., 2017) to combine mechanisms that consider multiple predicate interactions. Figure 2a shows the network of our base model. Formally, given a word sequence w = w1 , ..., wn and a target predicate position pi in p, the model outputs a label probability for each word position: p(ci,1 |i, p, w), ..., p(ci,n |i, p, w). Here, ci,t ∈ {NOM, ACC, DAT, NONE} represents the argument label of the word wt for the target predicate wpi . The input layer creates a vector h0i,t ∈ Rdw +1 for each pair of a predicate wpi and a word wt by concatenating a word embedding e(wt ) ∈ Rdw"
C18-1286,N16-1162,0,0.0132384,"y considering their posts. Therefore, our method is useful for analyzing opinions, including those of the silent majority. However, the performance of the stance detection in this work is not yet sufficient. To improve this performance, there are several points that need to be considered. First, even though features based on 3388 n-gram and dependencies are currently used as features of users’ posts, it may be possible to replace these with distributed representations. In recent years, multiple researchers have tried to build distributed representations of sentences (Logeswaran and Lee, 2018; Hill et al., 2016). These methods are known to demonstrate high performance in many tasks. By incorporating these methods into our proposed method, we will be able to express users’ posts more compactly, and this is expected to improve the accuracy of predicting users’ stances. In addition, FMs used in this work are advantageous because various types of features can be used simultaneously in an input matrix. Therefore, it is expected that social media specific features such as users’ follow-follower relationships, retweet relationships, or profile could be useful information for predicting stances. Moreover, it"
C18-1286,W04-3230,0,0.0266527,"of completing missing stances. Each cell presents the accuracy. “Numbers of stances stated” indicates the condition for evaluation. For example, as to ≥ 5, we evaluate the performance for completing missing stances for users who declared their stances toward no less than five topics. Note that, this treatment is applied only for the test portion of the cross validation. each topic, these patterns are strong clues and it is thought that a proper evaluation could not be performed if they were included. We perform preprocessing on target tweets using the Japanese dependency parsing tool CaboCha (Kudo et al., 2004)4 . We then extracted features about uni-grams, bi-grams and features based on dependencies from posts of each user. Table 1 shows examples of extracted features. Here, we choose only adnominal, adjective → noun phrase, noun phrase → adjective, and noun phrase → verb as features based on dependencies. Verb → noun phrase and triplet such as (subject, predicate, object) are not suitable as features because the number of users using each feature is very small and they become sparse. 4 4.1 Evaluation Completing missing stances The first experiment examines how well FMs complete missing stances of"
C18-1286,S16-1064,0,0.0309518,"Missing"
C18-1286,S16-1003,0,0.180372,"edia data. The experimental results demonstrate that users’ posts are useful to model topic preferences and therefore predict stances of silent users. 1 Introduction Web and social media provide platforms to express, discuss, and shape opinions about events and issues in the real world. However, these platforms suffer from new emerging problems such as filter bubble (Pariser, 2011), echo chamber (Jamieson and Cappella, 2008), and fake news (Lazer et al., 2018). An important step to analyze the argumentation structure of social media and to assist in healthy decisionmaking is stance detection (Mohammad et al., 2016): predicting whether a given text/user is in favor (agree), against (disagree), or neutral toward a target topic (e.g., Donald Trump). Unfortunately, users rarely make an explicit statement about a topic. For example, computers may easily detect a stance for the topic of the Trans-Pacific Partnership (TPP) from the sentence, “I totally disagree with TPP,” containing the topic word ‘TPP’ with the explicit linguistic pattern “I totally disagree with ...” However, many social media posts may not refer to a topic but only to its related topics, for example, “We should protect Japanese agriculture."
C18-1286,C10-2100,0,0.0268129,"election (Hong and Nadler, 2012). However, few studies have addressed the silent majority (who hardly expresses a stance), limiting the target to users who explicitly expressed their opinions (Gayo-Avello, 2012). In our work, we shed light on the silent majority as well as vocal users (who express stances explicitly). We aim to predict stances of the silent majority by modeling the topic preferences of users with explicit stance statements and ordinary posts. Stance detection has been extensively studied in recent years as a task to predict a stance of a user or text toward a specific topic (Murakami and Raymond, 2010; Mohammad et al., 2016; Persing and Ng, 2016). However, most studies have depended heavily on labeled training data (Tutek et al., 2016; Liu et al., 2016). Therefore, these methods had difficulty in predicting a stance toward an unseen topic. Accordingly, the SemEval-2016 task 6B released unlabeled data on a topic to be predicted and labeled data on other topics (Mohammad et al., 2016). However, the accuracy on the setting dropped drastically compared with the setting when labeled data for the topic are given. One reason for the relatively low accuracy is that, if no labeled data are availabl"
C18-1286,P16-1205,0,0.0239929,"dies have addressed the silent majority (who hardly expresses a stance), limiting the target to users who explicitly expressed their opinions (Gayo-Avello, 2012). In our work, we shed light on the silent majority as well as vocal users (who express stances explicitly). We aim to predict stances of the silent majority by modeling the topic preferences of users with explicit stance statements and ordinary posts. Stance detection has been extensively studied in recent years as a task to predict a stance of a user or text toward a specific topic (Murakami and Raymond, 2010; Mohammad et al., 2016; Persing and Ng, 2016). However, most studies have depended heavily on labeled training data (Tutek et al., 2016; Liu et al., 2016). Therefore, these methods had difficulty in predicting a stance toward an unseen topic. Accordingly, the SemEval-2016 task 6B released unlabeled data on a topic to be predicted and labeled data on other topics (Mohammad et al., 2016). However, the accuracy on the setting dropped drastically compared with the setting when labeled data for the topic are given. One reason for the relatively low accuracy is that, if no labeled data are available for a new topic, it is impossible to learn e"
C18-1286,P17-1037,1,0.929526,"unseen topic. Accordingly, the SemEval-2016 task 6B released unlabeled data on a topic to be predicted and labeled data on other topics (Mohammad et al., 2016). However, the accuracy on the setting dropped drastically compared with the setting when labeled data for the topic are given. One reason for the relatively low accuracy is that, if no labeled data are available for a new topic, it is impossible to learn expressions that may imply stances for that topic. One way to overcome this problem is to incorporate external knowledge about the topics. The most relevant work is our previous work (Sasaki et al., 2017). Main goal of this work was to acquire knowledge such as “those who agree with the TPP also agree with free trade.” We called such knowledge “inter-topic preferences.” We used texts on Twitter and modeled inter-topic preferences using matrix factorization. As a side effect of this modeling, we were able to predict the missing stances of users by considering users’ explicit stances toward other topics. However, the method of this previous work cannot be applied to the silent majority, who does not explicitly express any stances. Several studies have focused on the silent majority and lurkers."
C18-1286,S16-1075,0,0.0612242,"Missing"
C96-1012,P92-1032,0,0.411131,"Missing"
C96-1012,C94-1049,0,0.155908,"Missing"
C96-1012,P91-1034,0,0.395741,"Missing"
C96-1012,J94-4003,0,0.176682,"Missing"
C96-1012,P95-1026,0,0.135131,"Missing"
D07-1114,W03-2607,0,0.0319522,"the generality of the model. Aspect-of relations can be regarded as a sub-type of bridging reference (Clark, 1977), which is a common linguistic phenomenon where the referent of a definite noun phrase refers to a discourse-new entity implicitly related to some previously mentioned entity. For example, we can see a relation of bridging reference between “the door” and “the room” in “She entered the room. The door closed automatically.” A common approach is to use cooccurrence statistics between the referring expression (e.g. “the door” in the above example) and the related entity (“the room”) (Bunescu, 2003; Poesio et al., 2004). Our approach newly incorporates automatically induced syntactic patterns as contextual clues into such a co-occurrence model, producing significant improvements of accuracy. 3.2 Our approach Now we describe our approach to aspect-evaluation and aspect-of relation extraction. The key idea is to combine the following two kinds of information using a machine learning technique for both tasks. Contextual clues: Syntactic patterns such as hAspecti-ga hAspecti-NOM VP-te, hEvaluationi VP-CONJ hEvaluationi which matches such a sentence as hsekkyakui-ga kunrens-aretei-te hkimoch"
D07-1114,W06-1651,0,0.469248,"the task of extracting hSubject,Aspect,Evaluationi. However, none of those papers reports on such an extensive corpus study as what we report in this paper. In addition, in this paper, we consider not only aspect-evaluation relations but also hierarchical chains of subject-aspect and aspect-aspect relations, which has never been addressed in previous work. Open-domain opinion extraction is another trend of research on opinion extraction, which aims to extract a wider range of opinions from such texts as newspaper articles (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wiebe et al., 2005; Choi et al., 2006). To the best of our knowledge, one of the most extensive corpus studies in this field has been conducted in the MPQA project (Wiebe et al., 2005); while their concerns include the types of opinions we consider, they annotate newspaper articles, which presumably exhibit considerably different characteristics from customer-generated texts. Though we do not discuss the problem of determining semantic orientation, we assume availability of state-of-the-art methods that perform this task (Suzuki et al., 2006; Takamura et al., 2006, etc.). The problem of determining semantic orientation will be sol"
D07-1114,W03-2604,1,0.664242,"entification: Given a target evaluation (or aspect), select the most likely candidate aspect c∗ within the target sentence with the intra-sentential model described in 3.2.1. If the score of c∗ is positive, return c∗ ; otherwise, go to the inter-sentential relation extraction phase. 2) Inter-sentential relation identification: Search for the most likely candidate aspect in the sentences preceding the target evaluation (or aspect). This task can be regarded as a zero-anaphora resolution problem. For this purpose, we employ the supervised learning model for zero-anaphora resolution proposed by (Iida et al., 2003). 3.5 Opinion-hood determination Evaluation phrases do not always extract correct opinion units in a given domain. Consider an example from the digital camera domain, “The weather was good. so I went to the park to take some pictures”. “good” expresses the evaluation for “the weather”, but “the weather” is not an aspect of digital cameras. Therefore, hthe weather, goodi is not an opinion in the digital camera domain. We can consider a binary classification task of judging whether the obtained opinion unit is a real opinion or not in 1071 a given domain. In this paper, we conduct a preliminary"
D07-1114,C04-1071,0,0.019269,"Missing"
D07-1114,C04-1200,0,0.0348149,"aper (Kobayashi et al., 2005) addresses the task of extracting hSubject,Aspect,Evaluationi. However, none of those papers reports on such an extensive corpus study as what we report in this paper. In addition, in this paper, we consider not only aspect-evaluation relations but also hierarchical chains of subject-aspect and aspect-aspect relations, which has never been addressed in previous work. Open-domain opinion extraction is another trend of research on opinion extraction, which aims to extract a wider range of opinions from such texts as newspaper articles (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wiebe et al., 2005; Choi et al., 2006). To the best of our knowledge, one of the most extensive corpus studies in this field has been conducted in the MPQA project (Wiebe et al., 2005); while their concerns include the types of opinions we consider, they annotate newspaper articles, which presumably exhibit considerably different characteristics from customer-generated texts. Though we do not discuss the problem of determining semantic orientation, we assume availability of state-of-the-art methods that perform this task (Suzuki et al., 2006; Takamura et al., 2006, etc.). The problem of dete"
D07-1114,W06-0301,0,0.15085,"ually devised rules. An exception is the model reported by Kanayama et al.(2004), which uses a component of an existing MT system to identify the “aspect” argument of a given “evaluation” predicate. However, the MT component they use is not publicly available, and even if it were, it would be difficult to apply it to tasks in hand due of the opaqueness of its mechanism. Our approach aims to develop a more generally applicable model of aspect-evaluation extraction. In open-domain opinion extraction, some approaches use syntactic features obtained from parsed input sentences (Choi et al., 2006; Kim and Hovy, 2006), as is commonly done in semantic role labeling. Choi et al. (2006) address the task of extracting opinion entities and their relations, and incorporate syntactic features to their relation extraction 1069 model. Kim and Hovy (2006) proposed a method for extracting opinion holders, topics and opinion words, in which they use semantic role labeling as an intermediate step to label opinion holders and topics. However, these approaches do not address the task of extracting aspect-of relations and make use of syntactic features only for labeling opinion holders and topics. In contrast, as we descr"
D07-1114,I05-2030,1,0.510845,"heavily domain-dependent. In fact, according to our investigation on our opinion-annotated corpus, the number 1068 # !"" $# !""      There are several researches on customer opinion extraction. Hu and Liu (2004) considered the task of extracting hAspect, Sentence, Semantic-orientationi triples in our terminology, where Sentence is the one that includes the Aspect, and Semantic-orientation is either positive or negative. The notion of Evaluation in our term has also been introduced by previous work (Popescu and Etzioni, 2005; Tateishi et al., 2004; Suzuki et al., 2006; Kobayashi et al., 2005, etc.). For example, our previous paper (Kobayashi et al., 2005) addresses the task of extracting hSubject,Aspect,Evaluationi. However, none of those papers reports on such an extensive corpus study as what we report in this paper. In addition, in this paper, we consider not only aspect-evaluation relations but also hierarchical chains of subject-aspect and aspect-aspect relations, which has never been addressed in previous work. Open-domain opinion extraction is another trend of research on opinion extraction, which aims to extract a wider range of opinions from such texts as newspaper artic"
D07-1114,W04-3239,1,0.748648,"aluation t, we first represent all the sentences in the annotated corpus that has both an aspect and its evaluation, as shown in Figure 3. A sentence is analyzed by a dependency parser, then the dependency tree is converted so as to represent the relation between content words clearly and to attach other information (such as POS labels and other morphological features of content words and the functional words attached to the content words) as shown in the lower part of Figure 3. Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)’s algorithm, packaged as a free software named BACT. 1070 Given a set of training examples represented as ordered trees labeled either positive or negative class, this algorithm learns a list of weighted decision stumps as a discrimination function with the Boosting algorithm. Each decision stump is associated with tuple hs, l, wi, where s is a subtree appearing in the training set, l a label, and w a weight of this pattern. The strength of this algorithm is that it automatically acquires structured features and allows us to analyze the utility of features. Given a c-t pair in an annotated se"
D07-1114,W02-1011,0,0.0157809,"luation and Aspect-of Relations in Opinion Mining Nozomi Kobayashi ∗ Kentaro Inui, and Yuji Matsumoto Graduate School of Information Science, Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {nozomi-k,inui,matsu}@is.naist.jp Abstract reviews) can be classified into two approaches: Document classification and information extraction. The former is the task of classifying documents or passages according to their semantic orientation such as positive vs. negative. This direction has been forming the mainstream of research on opinion-sensitive text processing (Pang et al., 2002; Turney, 2002, etc.). The latter, on the other hand, focuses on the task of extracting opinions consisting of information about, for example, hwho feels how about which aspect of what producti from unstructured text data. In this paper, we refer to this information extractionoriented task as opinion extraction. In contrast to sentiment classification, opinion extraction aims at producing richer information and requires an indepth analysis of opinions, which has only recently been attempted by a growing but still relatively small research community (Yi et al., 2003; Hu and Liu, 2004; Popescu a"
D07-1114,P04-1019,0,0.0075112,"of the model. Aspect-of relations can be regarded as a sub-type of bridging reference (Clark, 1977), which is a common linguistic phenomenon where the referent of a definite noun phrase refers to a discourse-new entity implicitly related to some previously mentioned entity. For example, we can see a relation of bridging reference between “the door” and “the room” in “She entered the room. The door closed automatically.” A common approach is to use cooccurrence statistics between the referring expression (e.g. “the door” in the above example) and the related entity (“the room”) (Bunescu, 2003; Poesio et al., 2004). Our approach newly incorporates automatically induced syntactic patterns as contextual clues into such a co-occurrence model, producing significant improvements of accuracy. 3.2 Our approach Now we describe our approach to aspect-evaluation and aspect-of relation extraction. The key idea is to combine the following two kinds of information using a machine learning technique for both tasks. Contextual clues: Syntactic patterns such as hAspecti-ga hAspecti-NOM VP-te, hEvaluationi VP-CONJ hEvaluationi which matches such a sentence as hsekkyakui-ga kunrens-aretei-te hkimochiyoii hservicei-NOM be"
D07-1114,H05-1043,0,0.907688,"al., 2002; Turney, 2002, etc.). The latter, on the other hand, focuses on the task of extracting opinions consisting of information about, for example, hwho feels how about which aspect of what producti from unstructured text data. In this paper, we refer to this information extractionoriented task as opinion extraction. In contrast to sentiment classification, opinion extraction aims at producing richer information and requires an indepth analysis of opinions, which has only recently been attempted by a growing but still relatively small research community (Yi et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005, etc.). The technology of opinion extraction allows users to retrieve and analyze people’s opinions scattered over Web documents. We define an opinion unit as a quadruple consisting of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of the evaluation that expresses a positive or negative assessment. We use this definition as the basis for our opinion extraction task. We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluation relations, and (b) extracting aspect-of relations, and we appr"
D07-1114,E06-1026,0,0.0335255,"Missing"
D07-1114,P02-1053,0,0.00485714,"of Relations in Opinion Mining Nozomi Kobayashi ∗ Kentaro Inui, and Yuji Matsumoto Graduate School of Information Science, Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {nozomi-k,inui,matsu}@is.naist.jp Abstract reviews) can be classified into two approaches: Document classification and information extraction. The former is the task of classifying documents or passages according to their semantic orientation such as positive vs. negative. This direction has been forming the mainstream of research on opinion-sensitive text processing (Pang et al., 2002; Turney, 2002, etc.). The latter, on the other hand, focuses on the task of extracting opinions consisting of information about, for example, hwho feels how about which aspect of what producti from unstructured text data. In this paper, we refer to this information extractionoriented task as opinion extraction. In contrast to sentiment classification, opinion extraction aims at producing richer information and requires an indepth analysis of opinions, which has only recently been attempted by a growing but still relatively small research community (Yi et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 20"
D07-1114,W03-1017,0,0.0181728,".). For example, our previous paper (Kobayashi et al., 2005) addresses the task of extracting hSubject,Aspect,Evaluationi. However, none of those papers reports on such an extensive corpus study as what we report in this paper. In addition, in this paper, we consider not only aspect-evaluation relations but also hierarchical chains of subject-aspect and aspect-aspect relations, which has never been addressed in previous work. Open-domain opinion extraction is another trend of research on opinion extraction, which aims to extract a wider range of opinions from such texts as newspaper articles (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wiebe et al., 2005; Choi et al., 2006). To the best of our knowledge, one of the most extensive corpus studies in this field has been conducted in the MPQA project (Wiebe et al., 2005); while their concerns include the types of opinions we consider, they annotate newspaper articles, which presumably exhibit considerably different characteristics from customer-generated texts. Though we do not discuss the problem of determining semantic orientation, we assume availability of state-of-the-art methods that perform this task (Suzuki et al., 2006; Takamura et al., 2006, etc.)."
D07-1114,H05-2017,0,\N,Missing
D18-1203,Q17-1010,0,0.0317567,"on in the context of machine translation (Section 6.3). 6.1 PHSIC Settings To take advantage of recent developments in representation learning, we used several pre-trained models for encoding sentences into vectors and several kernels between these vectors for PHSIC. Encoders As sentence encorders, we used two pre-trained models without fine-tuning. First, the sum of the word vectors effectively represents a sentence (Mikolov et al., 2013a): P P x = w∈x vec(w), y = w∈y vec(w). (22) For vec(·), we used the pre-trained fastText model7 , which is a high-accuracy and popular word embedding model (Bojanowski et al., 2017); models in 157 languages are publicly distributed (Grave et al., 2018). Second, we also used a DNN-based sentence encoder, called the universal sentence encoder (Cer et al., 2018), which utilizes the deep averaging network (DAN) (Iyyer et al., 2015). The pre-trained model for English sentences we used is publicly available8 . Kernels As kernels between these vectors, we used cosine similarity (cos) k(x, x0 ) = cos(x, x0 ) (23) and the Gaussian kernel (also known as the radial basis function kernel; RBF kernel)   kx − x0 k22 0 k(x, x ) = exp − , (24) 2σ 2 and similarly for `(y, y 0 ). The ex"
D18-1203,D18-2029,0,0.0224654,"ding sentences into vectors and several kernels between these vectors for PHSIC. Encoders As sentence encorders, we used two pre-trained models without fine-tuning. First, the sum of the word vectors effectively represents a sentence (Mikolov et al., 2013a): P P x = w∈x vec(w), y = w∈y vec(w). (22) For vec(·), we used the pre-trained fastText model7 , which is a high-accuracy and popular word embedding model (Bojanowski et al., 2017); models in 157 languages are publicly distributed (Grave et al., 2018). Second, we also used a DNN-based sentence encoder, called the universal sentence encoder (Cer et al., 2018), which utilizes the deep averaging network (DAN) (Iyyer et al., 2015). The pre-trained model for English sentences we used is publicly available8 . Kernels As kernels between these vectors, we used cosine similarity (cos) k(x, x0 ) = cos(x, x0 ) (23) and the Gaussian kernel (also known as the radial basis function kernel; RBF kernel)   kx − x0 k22 0 k(x, x ) = exp − , (24) 2σ 2 and similarly for `(y, y 0 ). The experiments are ran with hyperparameter σ = 1.0 for the RBF kernel, and d = 100 for incomplete Cholesky decomposition (for more detail, see Section B). 6.2 Ranking: Dialogue Response"
D18-1203,P08-1090,0,0.0893409,"Missing"
D18-1203,P89-1010,0,0.602018,"we also demonstrate that PHSIC is beneficial as a criterion of a data selection task for machine translation owing to its ability to give high (low) scores to a consistent (inconsistent) pair with other pairs. 1 Kentaro Inui 1,2 PMI log P log n · c(x, y) P c(x, y 0 ) x0 c(x0, y) y0 X Eq. 1 b RNN (y|x) P b RNN (y) P Eq. 2 X PHSIC bXY (ψ(y)−ψ(y)) Sec. 5.1 (φ(x)−φ(x))> C X X bICD (b−b) (a−a)> C X X Sec. 5.2 Table 1: The proposed co-occurrence norm, PHSIC, eliminates the trade-off between robustness to data sparsity and learning time, which PMI has (Section 1). Pointwise mutual information (PMI) (Church and Hanks, 1989) is frequently used to model the co-occurrence strength of linguistic expression pairs. There are two typical types of PMI estimation (computation) method. One is a countingbased estimator using maximum likelihood estimation, sometimes with smoothing techniques, for example, Introduction Computing the co-occurrence strength between two linguistic expressions is a fundamental task in natural language processing (NLP). For example, in collocation extraction (Manning and Sch¨utze, 1999), word bigrams are collected from corpora and then strongly co-occurring bigrams (e.g., “New York”) are found. I"
D18-1203,N13-1073,0,0.0352961,"R; Recall@1,2. The best result in each column is in bold. The other notation is the same as in Table 4. Selection Criteria (all the training set) Random fast align PHSIC # of Selected Data K 0.5M 1M 3M - - 41.02 34.26 39.82 - 38.63 40.56 - Encoder Kernel fastText RBF 38.95 40.95 - Table 6: BLEU scores with the Transformer for each data selection criterion and each size of selected data K for the parallel corpus filtering task.“Random” represents the baseline method of selecting sentences at random. Baseline Measure As a baseline measure, we utilize a publicly available script17 of fast align (Dyer et al., 2013), which is one of the state-of-theart word aligner. We firstly used the fast align for the training set D = {(xi , yi )}i to obtain the word alignment between each sentence pair (xi , yi ), i.e., a set of aligned word pairs with its probabilities. We then computed the co-occurrence score of (xi , yi ) with sentence-length normalization, i.e., the average log probability of aligned word pairs. Experimental Results Table 6 shows the results of our data selection evaluation. It is common knowledge in NMT that more data gives better performance in general. However, we observed that PHSIC successfu"
D18-1203,L18-1550,0,0.013819,"o take advantage of recent developments in representation learning, we used several pre-trained models for encoding sentences into vectors and several kernels between these vectors for PHSIC. Encoders As sentence encorders, we used two pre-trained models without fine-tuning. First, the sum of the word vectors effectively represents a sentence (Mikolov et al., 2013a): P P x = w∈x vec(w), y = w∈y vec(w). (22) For vec(·), we used the pre-trained fastText model7 , which is a high-accuracy and popular word embedding model (Bojanowski et al., 2017); models in 157 languages are publicly distributed (Grave et al., 2018). Second, we also used a DNN-based sentence encoder, called the universal sentence encoder (Cer et al., 2018), which utilizes the deep averaging network (DAN) (Iyyer et al., 2015). The pre-trained model for English sentences we used is publicly available8 . Kernels As kernels between these vectors, we used cosine similarity (cos) k(x, x0 ) = cos(x, x0 ) (23) and the Gaussian kernel (also known as the radial basis function kernel; RBF kernel)   kx − x0 k22 0 k(x, x ) = exp − , (24) 2σ 2 and similarly for `(y, y 0 ). The experiments are ran with hyperparameter σ = 1.0 for the RBF kernel, and d"
D18-1203,N16-1162,0,0.0410635,"Missing"
D18-1203,P15-1162,0,0.0572835,"Missing"
D18-1203,W17-5715,0,0.0223865,"Missing"
D18-1203,N16-1014,0,0.395639,". In either case, a set of linguistic expression pairs D = {(xi , yi )}ni=1 is first collected and then the co-occurrence strength of a (new) pair (x, y) is computed. n · c(x, y) P , 0 0 y 0 c(x, y ) x0 c(x , y) d MLE (x, y; D) = log P PMI (1) where c(x, y) denotes the frequency of the pair (x, y) in given data D. This is easy to compute and is commonly used to measure co-occurrence between words, such as in collocation extraction1 ; however, when data D is sparse, i.e., when x or y is a phrase or sentence, this approach is unrealistic. The second method uses recurrent neural networks (RNNs). Li et al. (2016) proposed to em1 In collocation extraction, simple counting c(x, y) ∝ b P(x, y), rather than PMI, ranks undesirable function-word pairs (e.g., “of the”) higher (Manning and Sch¨utze, 1999). 1763 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1763–1775 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ploy PMI to suppress dull responses for utterance generation in dialogue systems2 . They estimated P(y) and P(y|x) using RNN language models and estimated PMI as follows: b d RNN (x, y; D) = log PRNN (y"
D18-1203,W15-4640,0,0.0969236,"e co-occurrence strength of linguistic expression pairs. There are two typical types of PMI estimation (computation) method. One is a countingbased estimator using maximum likelihood estimation, sometimes with smoothing techniques, for example, Introduction Computing the co-occurrence strength between two linguistic expressions is a fundamental task in natural language processing (NLP). For example, in collocation extraction (Manning and Sch¨utze, 1999), word bigrams are collected from corpora and then strongly co-occurring bigrams (e.g., “New York”) are found. In dialogue response selection (Lowe et al., 2015), pairs comprising a context and its response sentence are collected from dialogue corpora and the goal is to rank the candidate responses for each given context sentence. In either case, a set of linguistic expression pairs D = {(xi , yi )}ni=1 is first collected and then the co-occurrence strength of a (new) pair (x, y) is computed. n · c(x, y) P , 0 0 y 0 c(x, y ) x0 c(x , y) d MLE (x, y; D) = log P PMI (1) where c(x, y) denotes the frequency of the pair (x, y) in given data D. This is easy to compute and is commonly used to measure co-occurrence between words, such as in collocation extrac"
D18-1203,E06-1015,0,0.148985,"Missing"
D18-1203,Q16-1019,0,0.0607695,"Missing"
D18-1203,P02-1040,0,0.100679,"sed in the NMT community and known as one of the current state-of-the-art architectures. We utilized fairseq15 , a publicly available tool for neural sequence-to-sequence models, for building our models. Experimental Procedure We used the following procedure for this evaluation: (1) rank all parallel sentences in a given parallel corpus according to each criterion, (2) extract the top K ranked parallel sentences, (3) train the NMT model using the extracted parallel sentences, and (4) evaluate the translation quality of the test data using a typical MT automatic evaluation measure, i.e., BLEU (Papineni et al., 2002)16 . In our experiments we evaluated PHSIC with K = 0.5M and 1M. 13 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2017/ 15 https://github.com/pytorch/fairseq 16 We used multi-bleu.perl in the Moses tool (https:// github.com/moses-smt/mosesdecoder). 14 1769 Models Config Size of Training Set n 104 105 103 Chance Level 5 × 105 .50; .29; .10, .20 .50; .29; .10, .20 .50; .29; .10, .20 .50; .29; .10, .20 Dim. Init. b RNN (y) P 1200 fastText .50; .29; .10, .21 .50; .30; .11, .21 .50; .30; .10, .21 .50; .30; .13, .25 b RNN (y|x) P 1200 fastText .50; .29; .10, .21 .50;"
D18-1203,D14-1162,0,0.0846055,"Missing"
D18-1203,N15-1020,0,0.0131251,"experiments are ran with hyperparameter σ = 1.0 for the RBF kernel, and d = 100 for incomplete Cholesky decomposition (for more detail, see Section B). 6.2 Ranking: Dialogue Response Selection In the first experiment, we applied PHSIC as a ranking criterion of the task of dialogue response 7 https://fasttext.cc/docs/en/english-vectors. html, https://fasttext.cc/docs/en/crawl-vectors. html 8 https://www.tensorflow.org/hub/modules/google/ universal-sentence-encoder/1 1767 Experimental Settings Dataset For the training data, we gathered approximately 5 × 105 reply chains from Twitter, following Sordoni et al. (2015)9 . In addition, we randomly selected {103 , 104 , 105 } reply chains from that dataset. Using these small subsets, we confirmed the effect of the difference in the size of the training set (data sparseness) on the learning time and predictive performance. For validation and test data, we used a small (approximately 2000 pairs each) but highly reliable dataset created by Sordoni et al. (2015)10 , which consists only of conversations given high scores by human annotators. Therefore, this set was not expected to include dull responses. For each dataset, we converted each contextmessage-response"
D18-1453,D16-1203,0,0.087457,"Missing"
D18-1453,N18-1144,0,0.0185537,"atasets and the annotation results, are available at https://github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a multi-hop architecture with an"
D18-1453,P17-1168,0,0.139783,"whether this exposes any differences among the subsets. To investigate these two concerns, we manually annotate sample questions from each subset in terms of validity and required reasoning skills, such as word matching, knowledge inference, and multiple sentence reasoning. We examine 12 recently proposed MRC datasets (Table 1), which include answer extraction, description, and multiple-choice styles. We also observe differences based on these styles. For our baselines, we use two neural-based systems, namely, the Bidirectional Attention Flow (Seo et al., 2017) and the Gated-Attention Reader (Dhingra et al., 2017). In Section 5, we describe the advantages and disadvantages of different question styles with regard to evaluating NLU systems. We also interpret our heuristics for constructing realistic MRC datasets. Our contributions are as follows: • • This study is the first large-scale investigation across recent 12 MRC datasets with three question styles. We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. A"
D18-1453,N18-2017,0,0.0586381,"Missing"
D18-1453,P16-1145,0,0.02858,"nswer extraction, description, and 1 All scripts used in this study, along with the subsets of the datasets and the annotation results, are available at https://github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingr"
D18-1453,P99-1042,0,0.140082,"ned sentence (s1 ) by watching word overlaps. able for the detailed testing of NLU. Our motivation originates from studies that demonstrated unintended biases in the sourcing of other NLU tasks, in which questions contain simple patterns and systems can recognize these patterns to answer them (Gururangan et al., 2018; Mostafazadeh et al., 2017). Introduction Evaluating natural language understanding (NLU) systems is a long-established problem in AI (Levesque, 2014). One approach to doing so is the machine reading comprehension (MRC) task, in which a system answers questions about given texts (Hirschman et al., 1999). Although recent studies have made advances (Yu et al., 2018), it is still unclear to what precise extent questions require understanding of texts (Jia and Liang, 2017). In this study, we examine MRC datasets and discuss what is needed to create datasets suitWe conjecture that a situation similar to this occurs in MRC datasets. Consider the question shown in Figure 1, for example. Although the question, starting with when, requires an answer that is expressed as a moment in time, there is only one such expression (i.e., November 2014) in the given text (we refer to the text as the context). I"
D18-1453,D17-1215,0,0.405835,"ng of other NLU tasks, in which questions contain simple patterns and systems can recognize these patterns to answer them (Gururangan et al., 2018; Mostafazadeh et al., 2017). Introduction Evaluating natural language understanding (NLU) systems is a long-established problem in AI (Levesque, 2014). One approach to doing so is the machine reading comprehension (MRC) task, in which a system answers questions about given texts (Hirschman et al., 1999). Although recent studies have made advances (Yu et al., 2018), it is still unclear to what precise extent questions require understanding of texts (Jia and Liang, 2017). In this study, we examine MRC datasets and discuss what is needed to create datasets suitWe conjecture that a situation similar to this occurs in MRC datasets. Consider the question shown in Figure 1, for example. Although the question, starting with when, requires an answer that is expressed as a moment in time, there is only one such expression (i.e., November 2014) in the given text (we refer to the text as the context). In other words, the question has only a single candidate answer. The system can solve it merely by recognizing the entity type required by when. In addition to this, even"
D18-1453,P17-1147,0,0.0383334,"h regard to evaluating NLU systems. We also interpret our heuristics for constructing realistic MRC datasets. Our contributions are as follows: • • This study is the first large-scale investigation across recent 12 MRC datasets with three question styles. We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. AddSent (Jia and Liang, 2017) 3. NewsQA (Trischler et al., 2017) 4. TriviaQA (Wikipedia set) (Joshi et al., 2017) 5. QAngaroo (WikiHop) (Welbl et al., 2018) Description (generate a free-form answer) 6. MS MARCO (v2) (Nguyen et al., 2016) 7. NarrativeQA (summary) (Koˇcisk´y et al., 2018) Multiple choice (choose from multiple options) 8. MCTest (160 + 500) (Richardson et al., 2013) 9. RACE (middle + high) (Lai et al., 2017) 10. MCScript (Ostermann et al., 2018) 11. ARC Easy (ARC-E) (Clark et al., 2018) 12. ARC Challenge (ARC-C) (Clark et al., 2018) Table 1: Examined datasets. • We manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which ski"
D18-1453,N18-1023,0,0.0976911,"ults, are available at https://github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a multi-hop architecture with an attention mechanism. It achieved"
D18-1453,Q18-1023,0,0.0646454,"Missing"
D18-1453,D17-1082,0,0.118876,"et into easy and hard subsets and examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. AddSent (Jia and Liang, 2017) 3. NewsQA (Trischler et al., 2017) 4. TriviaQA (Wikipedia set) (Joshi et al., 2017) 5. QAngaroo (WikiHop) (Welbl et al., 2018) Description (generate a free-form answer) 6. MS MARCO (v2) (Nguyen et al., 2016) 7. NarrativeQA (summary) (Koˇcisk´y et al., 2018) Multiple choice (choose from multiple options) 8. MCTest (160 + 500) (Richardson et al., 2013) 9. RACE (middle + high) (Lai et al., 2017) 10. MCScript (Ostermann et al., 2018) 11. ARC Easy (ARC-E) (Clark et al., 2018) 12. ARC Challenge (ARC-C) (Clark et al., 2018) Table 1: Examined datasets. • We manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions. We observed the following: • The baseline performances for the hard subsets remarkably degrade compared to those of entire datasets. • Our annotation study shows that hard questions require knowledge inference and multiplesentence reasoning in comparis"
D18-1453,W04-1013,0,0.00928634,"It achieved state-of-the-artperformance on the CNN/Daily Mail and Whodid-What datasets. Why we used different baseline systems: The multiple-choice style can be transformed to answer extraction, as mentioned in Clark et al. (2018). However, in some datasets, many questions have no textual overlap to determine the correct answer span in the context. Therefore, in order to avoid underestimating the baseline performance of those datasets, we used the GA system which is applicable to multiple choice questions. We scored the performance using exact match (EM)/F1 (Rajpurkar et al., 2016), Rouge-L (Lin, 2004), and accuracy for the answer extraction, description, and multiple-choice datasets, respectively (henceforth, we refer to these collectively as the score, for simplicity). For the description datasets, we determined in advance the answer span of the context that gives the highest Rouge-L score to the human-generated gold answer. We computed the Rouge-L score between 2 The ARC Easy and Challenge were collected using different methods; hence, we treated them as different datasets (see Clark et al. (2018) for further details). the predicted span and the gold answer.3 Reproduction of the baseline"
D18-1453,P11-2057,0,0.0719576,"Missing"
D18-1453,P18-2124,0,0.0286545,"ultiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a multi-hop architecture with an attention mechanism. It achieved state-of-the-artperformance on the CNN/Daily Mail and Whodid-What datasets"
D18-1453,P18-1160,0,0.0649339,"Missing"
D18-1453,D16-1264,0,0.521837,"lls to investigate which skills explain the difference between easy and hard questions. From this study, we observed that (i) the baseline performances for the hard subsets remarkably degrade compared to those of entire datasets, (ii) hard questions require knowledge inference and multiple-sentence reasoning in comparison with easy questions, and (iii) multiplechoice questions tend to require a broader range of reasoning skills than answer extraction and description questions. These results suggest that one might overestimate recent advances in MRC. 1 Figure 1: Example from the SQuAD dataset (Rajpurkar et al., 2016). The baseline system can answer the token-limited question and, even if there are other candidate answers, it can easily attend to the answercontained sentence (s1 ) by watching word overlaps. able for the detailed testing of NLU. Our motivation originates from studies that demonstrated unintended biases in the sourcing of other NLU tasks, in which questions contain simple patterns and systems can recognize these patterns to answer them (Gururangan et al., 2018; Mostafazadeh et al., 2017). Introduction Evaluating natural language understanding (NLU) systems is a long-established problem in AI"
D18-1453,N16-1098,0,0.0584165,"Missing"
D18-1453,W17-0906,0,0.0162088,"ults suggest that one might overestimate recent advances in MRC. 1 Figure 1: Example from the SQuAD dataset (Rajpurkar et al., 2016). The baseline system can answer the token-limited question and, even if there are other candidate answers, it can easily attend to the answercontained sentence (s1 ) by watching word overlaps. able for the detailed testing of NLU. Our motivation originates from studies that demonstrated unintended biases in the sourcing of other NLU tasks, in which questions contain simple patterns and systems can recognize these patterns to answer them (Gururangan et al., 2018; Mostafazadeh et al., 2017). Introduction Evaluating natural language understanding (NLU) systems is a long-established problem in AI (Levesque, 2014). One approach to doing so is the machine reading comprehension (MRC) task, in which a system answers questions about given texts (Hirschman et al., 1999). Although recent studies have made advances (Yu et al., 2018), it is still unclear to what precise extent questions require understanding of texts (Jia and Liang, 2017). In this study, we examine MRC datasets and discuss what is needed to create datasets suitWe conjecture that a situation similar to this occurs in MRC da"
D18-1453,D13-1020,0,0.317104,"ntences, only one sentence (i.e., s1 ) appears to be related to the question; thus, the system can easily determine the correct answer by attention, that is, by matching the words appearing both in the context and the ques4208 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4208–4219 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics tion. Therefore, this kind of question does not require a complex understanding of language—e.g., multiple-sentence reasoning, which is known as a more challenging task (Richardson et al., 2013). In Section 3, we define two heuristics, namely entity-type recognition and attention. We specifically analyze the differences in the performance of baseline systems for the following two configurations: (i) questions answerable or unanswerable with the first k tokens; and (ii) questions whose correct answer appears or does not appear in the context sentence that is most similar to the question (henceforth referred to as the most similar sentence). Although similar heuristics are proposed by Weissenborn et al. (2017), ours are utilized for question filtering, rather than system development; U"
D18-1453,P15-1024,0,0.0244362,"y et al., 2018; Khashabi et al., 2018). Nonetheless, the description style is difficult to evaluate because the Rouge-L and BLEU scores are insufficient for testing NLU. Whereas it is easy to evaluate the performance on multiple-choice questions, generating multiple reasonable options requires considerable effort. Interpretation of our heuristics: When we regard the MRC task as recognizing textual entailment (RTE) (Dagan et al., 2006), the task requires the reader to construct one or more premises from the context and form the most reasonable hypothesis from the question and candidate answer (Sachan et al., 2015). Thus, easier questions are those (i) where the reader needs to generate only one hypothesis, and (ii) where the premises directly describe the correct hypothesis. Our two heuristics can also be seen as the formalizations of these criteria. Therefore, to make questions more realistic, we need to create multiple hypotheses that require complex reasoning to be distinguished. Moreover, the integration of premises should be complemented by external knowledge to provide sufficient information to verify the correct hypothesis. 6 Related Work Our heuristics and annotation were motivated by unintende"
D18-1453,P18-1156,0,0.0157401,"s to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a multi-hop architecture with an attention mechanism. It achieved state-of-the-artperformance on the CNN/Daily Mail and Whodid-What datasets. Why we used different baselin"
D18-1453,D16-1241,0,0.0358516,"along with the subsets of the datasets and the annotation results, are available at https://github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a"
D18-1453,P10-1122,0,0.0650563,"Missing"
D18-1453,L18-1564,0,0.0175734,"d examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. AddSent (Jia and Liang, 2017) 3. NewsQA (Trischler et al., 2017) 4. TriviaQA (Wikipedia set) (Joshi et al., 2017) 5. QAngaroo (WikiHop) (Welbl et al., 2018) Description (generate a free-form answer) 6. MS MARCO (v2) (Nguyen et al., 2016) 7. NarrativeQA (summary) (Koˇcisk´y et al., 2018) Multiple choice (choose from multiple options) 8. MCTest (160 + 500) (Richardson et al., 2013) 9. RACE (middle + high) (Lai et al., 2017) 10. MCScript (Ostermann et al., 2018) 11. ARC Easy (ARC-E) (Clark et al., 2018) 12. ARC Challenge (ARC-C) (Clark et al., 2018) Table 1: Examined datasets. • We manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions. We observed the following: • The baseline performances for the hard subsets remarkably degrade compared to those of entire datasets. • Our annotation study shows that hard questions require knowledge inference and multiplesentence reasoning in comparison with easy questions. • Compared to"
D18-1453,W17-0907,0,0.0253284,"Missing"
D18-1453,P16-1144,0,0.0228959,"nd 1 All scripts used in this study, along with the subsets of the datasets and the annotation results, are available at https://github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the"
D18-1453,P17-1075,1,0.886477,"Missing"
D18-1453,N18-1140,0,0.0171139,"github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a multi-hop architecture with an attention mechanism. It achieved state-of-the-artperformance on the"
D18-1453,W17-2623,0,0.328136,"es and disadvantages of different question styles with regard to evaluating NLU systems. We also interpret our heuristics for constructing realistic MRC datasets. Our contributions are as follows: • • This study is the first large-scale investigation across recent 12 MRC datasets with three question styles. We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. AddSent (Jia and Liang, 2017) 3. NewsQA (Trischler et al., 2017) 4. TriviaQA (Wikipedia set) (Joshi et al., 2017) 5. QAngaroo (WikiHop) (Welbl et al., 2018) Description (generate a free-form answer) 6. MS MARCO (v2) (Nguyen et al., 2016) 7. NarrativeQA (summary) (Koˇcisk´y et al., 2018) Multiple choice (choose from multiple options) 8. MCTest (160 + 500) (Richardson et al., 2013) 9. RACE (middle + high) (Lai et al., 2017) 10. MCScript (Ostermann et al., 2018) 11. ARC Easy (ARC-E) (Clark et al., 2018) 12. ARC Challenge (ARC-C) (Clark et al., 2018) Table 1: Examined datasets. • We manually annotate questions sampled from each subset with both validity and re"
D18-1453,K17-1028,0,0.0374338,"e.g., multiple-sentence reasoning, which is known as a more challenging task (Richardson et al., 2013). In Section 3, we define two heuristics, namely entity-type recognition and attention. We specifically analyze the differences in the performance of baseline systems for the following two configurations: (i) questions answerable or unanswerable with the first k tokens; and (ii) questions whose correct answer appears or does not appear in the context sentence that is most similar to the question (henceforth referred to as the most similar sentence). Although similar heuristics are proposed by Weissenborn et al. (2017), ours are utilized for question filtering, rather than system development; Using these simple heuristics, we split each dataset into easy and hard subsets for further investigation of the baseline performance. After conducting the experiments, we analyze the following two points in Section 4. First, we consider which questions are valid for testing, i.e., reasonably solvable. Second, we consider what reasoning skills are required and whether this exposes any differences among the subsets. To investigate these two concerns, we manually annotate sample questions from each subset in terms of val"
D18-1453,Q18-1021,0,0.0363293,"interpret our heuristics for constructing realistic MRC datasets. Our contributions are as follows: • • This study is the first large-scale investigation across recent 12 MRC datasets with three question styles. We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. AddSent (Jia and Liang, 2017) 3. NewsQA (Trischler et al., 2017) 4. TriviaQA (Wikipedia set) (Joshi et al., 2017) 5. QAngaroo (WikiHop) (Welbl et al., 2018) Description (generate a free-form answer) 6. MS MARCO (v2) (Nguyen et al., 2016) 7. NarrativeQA (summary) (Koˇcisk´y et al., 2018) Multiple choice (choose from multiple options) 8. MCTest (160 + 500) (Richardson et al., 2013) 9. RACE (middle + high) (Lai et al., 2017) 10. MCScript (Ostermann et al., 2018) 11. ARC Easy (ARC-E) (Clark et al., 2018) 12. ARC Challenge (ARC-C) (Clark et al., 2018) Table 1: Examined datasets. • We manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and"
D18-1453,W16-0103,0,0.049189,"Missing"
D19-1054,P18-1063,0,0.241541,"The encoder first encodes each xi into a vector hi . At each time step, the decoder pays attentions to some source embeddings and outputs the probability of the next token by p(yt |y1:t−1 , Ct ). Ct is a weighted average of source embeddings: X Ct = αt,i hi discrepancy when integrating them together. 2. Soft-select: Learn a soft mask to filter useless information (Mei et al., 2016; Zhou et al., 2017). However, the mask is deterministic without any probabilistic variations, making it hard to model the content-level diversity. 3. Reinforce-select: Train the selector with reinforcement learning (Chen and Bansal, 2018), which has high training variance and low diversity on content selection. i ef (hi ,dt ) αt,i = P f (h ,d ) j t je In this paper, we treat the content selection as latent variables and train with amortized variational inference (Kingma and Welling, 2014; Mnih and Gregor, 2014). This provides a lower training variance than Reinforce-select. The selector and generator are co-trained within the same objective, the generations are thus more faithful to the selected contents than Bottom-up methods. Our model is task-agnostic, end-to-end trainable and can be seamlessly inserted into any encoder-dec"
D19-1054,D18-1443,0,0.369633,"e supposed to have two levels of diversity: (1) content-level diversity reflecting multiple possibilities of content selection (what to say) and (2) surface-level diversity reflecting the linguistic variations of verbalizing the selected contents (how to say) (Reiter and Dale, 2000; Nema et al., 2017). Recent neural network models handle these tasks with the encoder-decoder (Enc-Dec) framework (Sutskever et al., 2014; Bahdanau et al., 2015), which simultaneously performs selecting and verbalizing in a 1. Bottom-up: Train a separate content selector to constrain the attention to source tokens (Gehrmann et al., 2018), but the separate training of selector/generator might lead to ∗ Work mostly done while at RIKEN AIP. Correspondence to xshen@mpi-inf.mpg.de 1 The source code is available on https://github. com/chin-gyou/controllable-selection 579 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 579–590, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2015; Xu et al., 2015). The encoder first encodes each xi into a vector hi . At each time step, the dec"
D19-1054,D16-1011,0,0.0681628,"Missing"
D19-1054,D18-1205,0,0.079091,"), then decode with pθ (Y |X, β). Source-text pairs are available for training, but the ground-truth content selection for each pair is unknown. 3.1 3.2 Soft-select falls back on a deterministic network to output the likelihood function’s first-order Taylor series approximation expanded at Eβ∼B(γ) β: Bottom-up log Eβ∼B(γ) pθ (Y |X, β) The most intuitive way is training the content selector to target some heuristically extracted contents. For example, we can train the selector to select overlapped words between the source and target (Gehrmann et al., 2018), sentences with higher tf-idf scores (Li et al., 2018) or identified image objects that appear in the caption (Wang et al., 2017). A standard encoder-decoder model is independently trained. In the testing stage, the prediction of the content selector is used to hard-mask the attention vector to guide the text generation in a bottom-up way. Though easy to train, Bottomup generation has the following two problems: (1) The heuristically extracted contents might be coarse and cannot reflect the variety of human languages and (2) The selector and decoder are independently trained towards different objectives thus might not adapt to each other well. β"
D19-1054,P18-2027,0,0.0154788,"and target. In Gigaword, as the headline is more abstractive, we select the closest source word for each target word in the embedding space. Stop words and punctuations are prohibited from being selected. Choice of α/: As seen in Sec 3.5, we need to set the hyperparameter α for RS/SS and  for Most content selection models train the selector with heuristic rules (Hsu et al., 2018; Li et al., 2018; Yu et al., 2018; Gehrmann et al., 2018; Yao et al., 2019; Moryossef et al., 2019), which fail to fully capture the relation between selection and generation. Mei et al. (2016); Zhou et al. (2017); Lin et al. (2018); Li et al. (2018) “soft-select” word or sentence embeddings based on a gating function. The output score from the gate is a deterministic vector without any probabilistic variations, so controlling the selection to generate diverse text is impossible. Very few works explicitly define a bernoulli distribution for the selector, then train with the REINFORCE algorithm (Ling and Rush, 2017; Chen and Bansal, 2018), but the selection targets at a high recall regardless of the low precision, so the controllability over generated text is weak. Fan et al. (2018) control the generation by manually conc"
D19-1054,P18-1013,0,0.0209586,"a bi-LSTM encoder for better performance. Heuristically extracted content: This is used to train the selector for bottom up models and pretrain the RS and VRS model. For wikibio, we simply extract overlapped words between the source and target. In Gigaword, as the headline is more abstractive, we select the closest source word for each target word in the embedding space. Stop words and punctuations are prohibited from being selected. Choice of α/: As seen in Sec 3.5, we need to set the hyperparameter α for RS/SS and  for Most content selection models train the selector with heuristic rules (Hsu et al., 2018; Li et al., 2018; Yu et al., 2018; Gehrmann et al., 2018; Yao et al., 2019; Moryossef et al., 2019), which fail to fully capture the relation between selection and generation. Mei et al. (2016); Zhou et al. (2017); Lin et al. (2018); Li et al. (2018) “soft-select” word or sentence embeddings based on a gating function. The output score from the gate is a deterministic vector without any probabilistic variations, so controlling the selection to generate diverse text is impossible. Very few works explicitly define a bernoulli distribution for the selector, then train with the REINFORCE algorith"
D19-1054,W17-4505,0,0.299395,"on weight will first be “soft-masked” by γ before being passed to the decoder. soft-select is fully differentiable and can be easily trained by gradient descent. However, this soft-approximation is normally inaccurate, especially when B(γ) has a high entropy, which is common in one-to-many text generation tasks. The gap between log Eβ∼B(γ) pθ (Y |X, β) and log pθ (Y |X, Eβ∼B(γ) ) will be large (Ma et al., 2017; Deng et al., 2018). In practice, this would lead to unrealistic generations when sampling β from the deterministically trained distribution. 3.3 Reinforce-Select Reinforce-select (RS) (Ling and Rush, 2017; Chen and Bansal, 2018) utilizes reinforcement learning to approximate the marginal likelihood. Specifically, it is trained to maximize a lower bound of the likelihood by applying the Jensen inequalily: log Eβ∼B(γ) pθ (Y |X, β) ≥ Eβ∼B(γ) log pθ (Y |X, β) The gradient to γ is approximated with MonteCarlo sampling by applying the REINFORCE algorithm (Williams, 1992; Glynn, 1990). To speed up convergence, we pre-train the selector by some distant supervision, which is a common practice in reinforcement learning. REINFORCE is unbiased but has a high variance. Many research have proposed sophistic"
D19-1054,D15-1166,0,0.0535494,"elector and generated text (Alemi et al., 2018; Zhao et al., 2018). A higher CMI leads to stronger controllability with a bit more risk of text disfluency. In summary, our contributions are (1) systematically studying the problem of controllable content selection for Enc-Dec text generation, (2) proposing a task-agnostic training framework achieving promising results and (3) introducing an effective way to achieve the trade-off between performance and controllability. 2 (1) dt is the hidden state of the decoder at time step t. f is a score function to compute the similarity between hi and dt (Luong et al., 2015). 3 Content Selection Our goal is to decouple the content selection from the decoder by introducing an extra content selector. We hope the content-level diversity can be fully captured by the content selector for a more interpretable and controllable generation process. Following Gehrmann et al. (2018); Yu et al. (2018), we define content selection as a sequence labeling task. Let β1 , β2 , . . . , βn denote a sequence of binary selection masks. βi = 1 if hi is selected and 0 otherwise. βi is assumed to be independent from each other and is sampled from a bernoulli distribution B(γi )2 . γi is"
D19-1054,N16-1086,0,0.493898,"in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 579–590, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2015; Xu et al., 2015). The encoder first encodes each xi into a vector hi . At each time step, the decoder pays attentions to some source embeddings and outputs the probability of the next token by p(yt |y1:t−1 , Ct ). Ct is a weighted average of source embeddings: X Ct = αt,i hi discrepancy when integrating them together. 2. Soft-select: Learn a soft mask to filter useless information (Mei et al., 2016; Zhou et al., 2017). However, the mask is deterministic without any probabilistic variations, making it hard to model the content-level diversity. 3. Reinforce-select: Train the selector with reinforcement learning (Chen and Bansal, 2018), which has high training variance and low diversity on content selection. i ef (hi ,dt ) αt,i = P f (h ,d ) j t je In this paper, we treat the content selection as latent variables and train with amortized variational inference (Kingma and Welling, 2014; Mnih and Gregor, 2014). This provides a lower training variance than Reinforce-select. The selector and g"
D19-1054,N19-1236,0,0.0404738,"ain the selector for bottom up models and pretrain the RS and VRS model. For wikibio, we simply extract overlapped words between the source and target. In Gigaword, as the headline is more abstractive, we select the closest source word for each target word in the embedding space. Stop words and punctuations are prohibited from being selected. Choice of α/: As seen in Sec 3.5, we need to set the hyperparameter α for RS/SS and  for Most content selection models train the selector with heuristic rules (Hsu et al., 2018; Li et al., 2018; Yu et al., 2018; Gehrmann et al., 2018; Yao et al., 2019; Moryossef et al., 2019), which fail to fully capture the relation between selection and generation. Mei et al. (2016); Zhou et al. (2017); Lin et al. (2018); Li et al. (2018) “soft-select” word or sentence embeddings based on a gating function. The output score from the gate is a deterministic vector without any probabilistic variations, so controlling the selection to generate diverse text is impossible. Very few works explicitly define a bernoulli distribution for the selector, then train with the REINFORCE algorithm (Ling and Rush, 2017; Chen and Bansal, 2018), but the selection targets at a high recall regardles"
D19-1054,P17-1098,0,0.0234923,"ve concerns properly. Current methods can be categorized into the following three classes and have different limits: Introduction Many text generation tasks, e.g., data-to-text, summarization and image captioning, can be naturally divided into two steps: content selection and surface realization. The generations are supposed to have two levels of diversity: (1) content-level diversity reflecting multiple possibilities of content selection (what to say) and (2) surface-level diversity reflecting the linguistic variations of verbalizing the selected contents (how to say) (Reiter and Dale, 2000; Nema et al., 2017). Recent neural network models handle these tasks with the encoder-decoder (Enc-Dec) framework (Sutskever et al., 2014; Bahdanau et al., 2015), which simultaneously performs selecting and verbalizing in a 1. Bottom-up: Train a separate content selector to constrain the attention to source tokens (Gehrmann et al., 2018), but the separate training of selector/generator might lead to ∗ Work mostly done while at RIKEN AIP. Correspondence to xshen@mpi-inf.mpg.de 1 The source code is available on https://github. com/chin-gyou/controllable-selection 579 Proceedings of the 2019 Conference on Empirical"
D19-1054,P04-1011,0,0.221726,"Missing"
D19-1054,D14-1162,0,0.0812737,"Missing"
D19-1054,N19-1269,0,0.0384767,"Missing"
D19-1054,D18-1411,0,0.0685899,"elected : sri lankan, announced, closure, schools Text: sri lanka declares closure of schools. Table 1: Headline generation examples from our model. We can generate text describing various contents by sampling different content selections. The selected source word and its corresponding realizations in the text are highlighted with the same color. black-box way. Therefore, both levels of diversity are entangled within the generation. This entanglement, however, sacrifices the controllability and interpretability, making it diffifcult to specify the content to be conveyed in the generated text (Qin et al., 2018; Wiseman et al., 2018). With this in mind, this paper proposes decoupling content selection from the Enc-Dec framework to allow finer-grained control over the generation. Table 1 shows an example. We can easily modify the content selection to generate text with various focuses, or sample multiple paraphrases by fixing the content selection. Though there has been much work dealing with content selection for the Enc-Dec, none of them is able to address the above concerns properly. Current methods can be categorized into the following three classes and have different limits: Introduction Many te"
D19-1054,D15-1044,0,0.0525735,"etup, then present the evaluation results. In practice, we can set  to adjust the degree of controllability we want. Later we will show it leads to a trade-off with performance. The final algorithm is detailed in Algorithm 1. To keep fairness, we trian RS and VRS with the same control variate and pre-training strategy.4 4 Experiments 5.1 Related Work Tasks and Setup We test content-selection models on the headline and data-to-text generation task. Both tasks share the same framework with the only difference of source-side encoders. Headline Generation: We use English Gigaword preprocessed by Rush et al. (2015), which pairs first sentences of news articles with their headlines. We keep most settings same as in Zhou et al. (2017), but use a vocabulary built by bytepair-encoding (Sennrich et al., 2016). We find it speeds up training with superior performance. Data-to-Text Generation: We use the Wikibio dataset (Lebret et al., 2016). The source is a Wikipedia infobox and the target is a one-sentence biography description. Most settings are the same as in Liu et al. (2018), but we use a bi-LSTM encoder for better performance. Heuristically extracted content: This is used to train the selector for bottom"
D19-1054,P16-1162,0,0.0204755,"algorithm is detailed in Algorithm 1. To keep fairness, we trian RS and VRS with the same control variate and pre-training strategy.4 4 Experiments 5.1 Related Work Tasks and Setup We test content-selection models on the headline and data-to-text generation task. Both tasks share the same framework with the only difference of source-side encoders. Headline Generation: We use English Gigaword preprocessed by Rush et al. (2015), which pairs first sentences of news articles with their headlines. We keep most settings same as in Zhou et al. (2017), but use a vocabulary built by bytepair-encoding (Sennrich et al., 2016). We find it speeds up training with superior performance. Data-to-Text Generation: We use the Wikibio dataset (Lebret et al., 2016). The source is a Wikipedia infobox and the target is a one-sentence biography description. Most settings are the same as in Liu et al. (2018), but we use a bi-LSTM encoder for better performance. Heuristically extracted content: This is used to train the selector for bottom up models and pretrain the RS and VRS model. For wikibio, we simply extract overlapped words between the source and target. In Gigaword, as the headline is more abstractive, we select the clos"
D19-1054,D15-1199,0,0.0719588,"Missing"
D19-1054,D18-1356,0,0.0459964,"an, announced, closure, schools Text: sri lanka declares closure of schools. Table 1: Headline generation examples from our model. We can generate text describing various contents by sampling different content selections. The selected source word and its corresponding realizations in the text are highlighted with the same color. black-box way. Therefore, both levels of diversity are entangled within the generation. This entanglement, however, sacrifices the controllability and interpretability, making it diffifcult to specify the content to be conveyed in the generated text (Qin et al., 2018; Wiseman et al., 2018). With this in mind, this paper proposes decoupling content selection from the Enc-Dec framework to allow finer-grained control over the generation. Table 1 shows an example. We can easily modify the content selection to generate text with various focuses, or sample multiple paraphrases by fixing the content selection. Though there has been much work dealing with content selection for the Enc-Dec, none of them is able to address the above concerns properly. Current methods can be categorized into the following three classes and have different limits: Introduction Many text generation tasks, e."
D19-1054,C18-1091,0,0.133731,"raining framework achieving promising results and (3) introducing an effective way to achieve the trade-off between performance and controllability. 2 (1) dt is the hidden state of the decoder at time step t. f is a score function to compute the similarity between hi and dt (Luong et al., 2015). 3 Content Selection Our goal is to decouple the content selection from the decoder by introducing an extra content selector. We hope the content-level diversity can be fully captured by the content selector for a more interpretable and controllable generation process. Following Gehrmann et al. (2018); Yu et al. (2018), we define content selection as a sequence labeling task. Let β1 , β2 , . . . , βn denote a sequence of binary selection masks. βi = 1 if hi is selected and 0 otherwise. βi is assumed to be independent from each other and is sampled from a bernoulli distribution B(γi )2 . γi is the bernoulli parameter, which we estimate using a two-layer feedforward network on top of the source encoder. Text are generated by first sampling β from B(γ) to decide which content to cover, then decode with the conditional distribution pθ (Y |X, β). The text is expected to faithfully convey all selected contents an"
D19-1054,P17-1101,0,0.347238,"ge Processing and the 9th International Joint Conference on Natural Language Processing, pages 579–590, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2015; Xu et al., 2015). The encoder first encodes each xi into a vector hi . At each time step, the decoder pays attentions to some source embeddings and outputs the probability of the next token by p(yt |y1:t−1 , Ct ). Ct is a weighted average of source embeddings: X Ct = αt,i hi discrepancy when integrating them together. 2. Soft-select: Learn a soft mask to filter useless information (Mei et al., 2016; Zhou et al., 2017). However, the mask is deterministic without any probabilistic variations, making it hard to model the content-level diversity. 3. Reinforce-select: Train the selector with reinforcement learning (Chen and Bansal, 2018), which has high training variance and low diversity on content selection. i ef (hi ,dt ) αt,i = P f (h ,d ) j t je In this paper, we treat the content selection as latent variables and train with amortized variational inference (Kingma and Welling, 2014; Mnih and Gregor, 2014). This provides a lower training variance than Reinforce-select. The selector and generator are co-trai"
D19-1119,W19-4418,1,0.806202,"t, or transposition of adjacent characters. Right-to-left Re-ranking (R2L) Following Sennrich et al. (2016a, 2017); Grundkiewicz et al. (2019), we train four right-to-left models. The ensemble of four left-to-right models generate n-best candidates and their corresponding scores (i.e., conditional probabilities). We then pass each candidate to the ensemble of the four right-to-left models and compute the score. Finally, we re-rank the n-best candidates based on the sum of the two scores. Sentence-level Error Detection (SED) SED classifies whether a given sentence contains a grammatical error. Asano et al. (2019) proposed incorporating SED into the evaluation pipeline and reported improved precision. Here, the GEC model is applied only if SED detects a grammatical error in the given source sentence. The motivation is that SED could potentially reduce the number of false-positive errors of the GEC model. We use the re-implementation of the BERT-based SED model (Asano et al., 2019). Table 5 presents the results of applying SSE, 13 R2L, and SED. It is noteworthy that P RETL ARGE+SSE+R2L achieves state-of-the-art performance on both CoNLL-2014 (F0.5 = 65.0) and BEA-test (F0.5 = 69.8), which are better tha"
D19-1119,N18-2046,0,0.137737,"Missing"
D19-1119,W19-4427,0,0.584056,"s the D IRECT N OISE method. In this study, we investigate these decisions regarding pseudo data, our goal being to provide the research community with an improved understanding of the incorporation of pseudo data. Through extensive experiments, we determine suitable settings for GEC. We justify the reliability of the proposed settings by demonstrating their strong performance on benchmark datasets. Specifically, without any task-specific techniques or architecture, our model outperforms not only all previous single-model results but also all ensemble results except for the ensemble result by Grundkiewicz et al. (2019)1 . By applying task-specific techniques, we further improve the performance and achieve state-of-the-art performance on the CoNLL-2014 test set and the official test set of the BEA-2019 shared task. 2 Problem Formulation and Notation In this section, we formally define the GEC task discussed in this paper. Let D be the GEC training data that comprise pairs of an ungrammatical source sentence X and grammatical target sentence 1 The paper (Grundkiewicz et al. 2019) has not been published yet at the time of submission. 1236 Proceedings of the 2019 Conference on Empirical Methods in Natural Langu"
D19-1119,P17-1074,0,0.175646,". As a seed corpus T , we use SimpleWiki6 , Wikipedia7 or Gigaword8 . We apply the noizing methods described in Section 3 to each corpus and generate pseudo data Dp . The characteristics of each dataset are summarized in Table 1. Evaluation We report results on BEA-valid, the official test set of the BEA-2019 shared task (BEA-test), the CoNLL-2014 test set (CoNLL2014) (Ng et al., 2014), and the JFLEG test set (JFLEG) (Napoles et al., 2017). All reported results (except ensemble) are the average of five distinct trials using five different random seeds. We report the scores measured by ERRANT (Bryant et al., 2017; Felice et al., 2016) for BEA-valid, BEA-test, and CoNLL-2014. As the reference sentences of BEAtest are publicly unavailable, we evaluate the model outputs on CodaLab9 for BEA-test. We also report results measured by the M 2 scorer (Dahlmeier and Ng, 2012) on CoNLL-2014 to compare them with those of previous studies. We use the GLEU metric (Napoles et al., 2015, 2016) for JFLEG. Model We adopt the Transformer EncDec model (Vaswani et al., 2017) using the fairseq toolkit (Ott et al., 2019) and use the “Transformer (big)” settings of Vaswani et al. (2017). Optimization For the J OINT setting,"
D19-1119,P17-1070,0,0.0792226,"making any modifications to the model architecture. 1 Introduction To date, many studies have tackled grammatical error correction (GEC) as a machine translation (MT) task, in which ungrammatical sentences are regarded as the source language and grammatical sentences are regarded as the target language. This approach allows cutting-edge neural MT models to be adopted. For example, the encoder-decoder (EncDec) model (Sutskever et al., 2014; Bahdanau et al., 2015), which was originally proposed for MT, has been applied widely to GEC and has achieved remarkable results in the GEC research field (Ji et al., 2017; Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018). However, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data (Koehn and Knowles, 2017), but the largest set of publicly available parallel data (Lang-8) in GEC has only two million sentence pairs (Mizumoto et al., 2011). Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several de"
D19-1119,N18-1055,0,0.640986,"Missing"
D19-1119,N12-1067,0,0.230458,"ults on BEA-valid, the official test set of the BEA-2019 shared task (BEA-test), the CoNLL-2014 test set (CoNLL2014) (Ng et al., 2014), and the JFLEG test set (JFLEG) (Napoles et al., 2017). All reported results (except ensemble) are the average of five distinct trials using five different random seeds. We report the scores measured by ERRANT (Bryant et al., 2017; Felice et al., 2016) for BEA-valid, BEA-test, and CoNLL-2014. As the reference sentences of BEAtest are publicly unavailable, we evaluate the model outputs on CodaLab9 for BEA-test. We also report results measured by the M 2 scorer (Dahlmeier and Ng, 2012) on CoNLL-2014 to compare them with those of previous studies. We use the GLEU metric (Napoles et al., 2015, 2016) for JFLEG. Model We adopt the Transformer EncDec model (Vaswani et al., 2017) using the fairseq toolkit (Ott et al., 2019) and use the “Transformer (big)” settings of Vaswani et al. (2017). Optimization For the J OINT setting, we opti4 Details of the dataset is in Appendix B. The detailed data preparation process is in Appendix C. 6 https://simple.wikipedia.org 7 We used 2019-02-25 dump file at https://dumps. wikimedia.org/other/cirrussearch/. 8 We used the English Gigaword Fifth"
D19-1119,W13-1703,0,0.626857,"Missing"
D19-1119,D18-1045,0,0.0186027,"l sentence, is trained. The output of the reverse model is paired with the input and then used as pseudo data. BACKTRANS ( NOISY ) is a variant of backtranslation that was proposed by Xie et al. (2018)2 . This method adds rβrandom to the score of each hypothesis in the beam for every time step. Here, noise r is sampled uniformly from the interval [0, 1], and βrandom ∈ R≥0 is a hyper-parameter that controls the noise scale. If we set βrandom = 0, then BACK TRANS ( NOISY ) is identical to standard backtranslation. BACKTRANS ( SAMPLE ) is another variant of backtranslation, which was proposed by Edunov et al. (2018) for MT. In BACKTRANS ( SAMPLE ), sentences are decoded by sampling from the distribution of the reverse model. D IRECT N OISE Whereas BACKTRANS ( NOISY ) and BACKTRANS ( SAMPLE ) generate ungrammatical sentences with a reverse model, D IRECTN OISE injects noise “directly” into grammatical sentences (Edunov et al., 2018; Zhao et al., 2019). Specifically, for each token in the given sentence, this method probabilistically chooses one of the following operations: (i) masking with a placeholder token hmaski, (ii) deletion, (iii) insertion of a random token, and (iv) keeping the original3 . For ea"
D19-1119,C16-1079,0,0.350702,", we use SimpleWiki6 , Wikipedia7 or Gigaword8 . We apply the noizing methods described in Section 3 to each corpus and generate pseudo data Dp . The characteristics of each dataset are summarized in Table 1. Evaluation We report results on BEA-valid, the official test set of the BEA-2019 shared task (BEA-test), the CoNLL-2014 test set (CoNLL2014) (Ng et al., 2014), and the JFLEG test set (JFLEG) (Napoles et al., 2017). All reported results (except ensemble) are the average of five distinct trials using five different random seeds. We report the scores measured by ERRANT (Bryant et al., 2017; Felice et al., 2016) for BEA-valid, BEA-test, and CoNLL-2014. As the reference sentences of BEAtest are publicly unavailable, we evaluate the model outputs on CodaLab9 for BEA-test. We also report results measured by the M 2 scorer (Dahlmeier and Ng, 2012) on CoNLL-2014 to compare them with those of previous studies. We use the GLEU metric (Napoles et al., 2015, 2016) for JFLEG. Model We adopt the Transformer EncDec model (Vaswani et al., 2017) using the fairseq toolkit (Ott et al., 2019) and use the “Transformer (big)” settings of Vaswani et al. (2017). Optimization For the J OINT setting, we opti4 Details of th"
D19-1119,P18-1097,0,0.398051,"ich was originally proposed for MT, has been applied widely to GEC and has achieved remarkable results in the GEC research field (Ji et al., 2017; Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018). However, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data (Koehn and Knowles, 2017), but the largest set of publicly available parallel data (Lang-8) in GEC has only two million sentence pairs (Mizumoto et al., 2011). Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2018) found that a variant of the backtranslation (Sennrich et al., 2016b) method (BACKTRANS ( NOISY )) outperforms the generation of pseudo data from r"
D19-1119,W17-3204,0,0.0149658,"hich ungrammatical sentences are regarded as the source language and grammatical sentences are regarded as the target language. This approach allows cutting-edge neural MT models to be adopted. For example, the encoder-decoder (EncDec) model (Sutskever et al., 2014; Bahdanau et al., 2015), which was originally proposed for MT, has been applied widely to GEC and has achieved remarkable results in the GEC research field (Ji et al., 2017; Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018). However, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data (Koehn and Knowles, 2017), but the largest set of publicly available parallel data (Lang-8) in GEC has only two million sentence pairs (Mizumoto et al., 2011). Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization set"
D19-1119,N19-1333,0,0.693335,"y proposed for MT, has been applied widely to GEC and has achieved remarkable results in the GEC research field (Ji et al., 2017; Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018). However, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data (Koehn and Knowles, 2017), but the largest set of publicly available parallel data (Lang-8) in GEC has only two million sentence pairs (Mizumoto et al., 2011). Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2018) found that a variant of the backtranslation (Sennrich et al., 2016b) method (BACKTRANS ( NOISY )) outperforms the generation of pseudo data from raw grammatical sentences"
D19-1119,I11-1017,1,0.592899,"proach allows cutting-edge neural MT models to be adopted. For example, the encoder-decoder (EncDec) model (Sutskever et al., 2014; Bahdanau et al., 2015), which was originally proposed for MT, has been applied widely to GEC and has achieved remarkable results in the GEC research field (Ji et al., 2017; Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018). However, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data (Koehn and Knowles, 2017), but the largest set of publicly available parallel data (Lang-8) in GEC has only two million sentence pairs (Mizumoto et al., 2011). Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2"
D19-1119,P15-2097,0,0.292767,"L2014) (Ng et al., 2014), and the JFLEG test set (JFLEG) (Napoles et al., 2017). All reported results (except ensemble) are the average of five distinct trials using five different random seeds. We report the scores measured by ERRANT (Bryant et al., 2017; Felice et al., 2016) for BEA-valid, BEA-test, and CoNLL-2014. As the reference sentences of BEAtest are publicly unavailable, we evaluate the model outputs on CodaLab9 for BEA-test. We also report results measured by the M 2 scorer (Dahlmeier and Ng, 2012) on CoNLL-2014 to compare them with those of previous studies. We use the GLEU metric (Napoles et al., 2015, 2016) for JFLEG. Model We adopt the Transformer EncDec model (Vaswani et al., 2017) using the fairseq toolkit (Ott et al., 2019) and use the “Transformer (big)” settings of Vaswani et al. (2017). Optimization For the J OINT setting, we opti4 Details of the dataset is in Appendix B. The detailed data preparation process is in Appendix C. 6 https://simple.wikipedia.org 7 We used 2019-02-25 dump file at https://dumps. wikimedia.org/other/cirrussearch/. 8 We used the English Gigaword Fifth Edition (LDC Catalog No.: LDC2011T07). 9 https://competitions.codalab.org/ competitions/20228 5 Table 2: Pe"
D19-1119,E17-2037,0,0.346319,"periments. Hereinafter, we refer to the training data as BEA-train. We create validation data (BEA-valid) by randomly sampling sentence pairs from the official validation split5 . As a seed corpus T , we use SimpleWiki6 , Wikipedia7 or Gigaword8 . We apply the noizing methods described in Section 3 to each corpus and generate pseudo data Dp . The characteristics of each dataset are summarized in Table 1. Evaluation We report results on BEA-valid, the official test set of the BEA-2019 shared task (BEA-test), the CoNLL-2014 test set (CoNLL2014) (Ng et al., 2014), and the JFLEG test set (JFLEG) (Napoles et al., 2017). All reported results (except ensemble) are the average of five distinct trials using five different random seeds. We report the scores measured by ERRANT (Bryant et al., 2017; Felice et al., 2016) for BEA-valid, BEA-test, and CoNLL-2014. As the reference sentences of BEAtest are publicly unavailable, we evaluate the model outputs on CodaLab9 for BEA-test. We also report results measured by the M 2 scorer (Dahlmeier and Ng, 2012) on CoNLL-2014 to compare them with those of previous studies. We use the GLEU metric (Napoles et al., 2015, 2016) for JFLEG. Model We adopt the Transformer EncDec mo"
D19-1119,P12-2039,0,0.528357,"Missing"
D19-1119,N19-4009,0,0.0231069,"ge of five distinct trials using five different random seeds. We report the scores measured by ERRANT (Bryant et al., 2017; Felice et al., 2016) for BEA-valid, BEA-test, and CoNLL-2014. As the reference sentences of BEAtest are publicly unavailable, we evaluate the model outputs on CodaLab9 for BEA-test. We also report results measured by the M 2 scorer (Dahlmeier and Ng, 2012) on CoNLL-2014 to compare them with those of previous studies. We use the GLEU metric (Napoles et al., 2015, 2016) for JFLEG. Model We adopt the Transformer EncDec model (Vaswani et al., 2017) using the fairseq toolkit (Ott et al., 2019) and use the “Transformer (big)” settings of Vaswani et al. (2017). Optimization For the J OINT setting, we opti4 Details of the dataset is in Appendix B. The detailed data preparation process is in Appendix C. 6 https://simple.wikipedia.org 7 We used 2019-02-25 dump file at https://dumps. wikimedia.org/other/cirrussearch/. 8 We used the English Gigaword Fifth Edition (LDC Catalog No.: LDC2011T07). 9 https://competitions.codalab.org/ competitions/20228 5 Table 2: Performance of models on BEA-valid: a value in bold indicates the best result within the column. The seed corpus T is SimpleWiki. mi"
D19-1119,W17-4739,0,0.044832,"Missing"
D19-1119,W16-2323,0,0.359924,"by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2018) found that a variant of the backtranslation (Sennrich et al., 2016b) method (BACKTRANS ( NOISY )) outperforms the generation of pseudo data from raw grammatical sentences (D IRECT N OISE). By contrast, the current state of the art model (Zhao et al., 2019) uses the D IRECT N OISE method. In this study, we investigate these decisions regarding pseudo data, our goal being to provide the research community with an improved understanding of the incorporation of pseudo data. Through extensive experiments, we determine suitable settings for GEC. We justify the reliability of the proposed settings by demonstrating their strong performance on benchmark datasets. Spe"
D19-1119,P16-1009,0,0.596655,"by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2018) found that a variant of the backtranslation (Sennrich et al., 2016b) method (BACKTRANS ( NOISY )) outperforms the generation of pseudo data from raw grammatical sentences (D IRECT N OISE). By contrast, the current state of the art model (Zhao et al., 2019) uses the D IRECT N OISE method. In this study, we investigate these decisions regarding pseudo data, our goal being to provide the research community with an improved understanding of the incorporation of pseudo data. Through extensive experiments, we determine suitable settings for GEC. We justify the reliability of the proposed settings by demonstrating their strong performance on benchmark datasets. Spe"
D19-1119,N18-1057,0,0.263454,"et al., 2015), which was originally proposed for MT, has been applied widely to GEC and has achieved remarkable results in the GEC research field (Ji et al., 2017; Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018). However, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data (Koehn and Knowles, 2017), but the largest set of publicly available parallel data (Lang-8) in GEC has only two million sentence pairs (Mizumoto et al., 2011). Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2018) found that a variant of the backtranslation (Sennrich et al., 2016b) method (BACKTRANS ( NOISY )) outperforms the generation of p"
D19-1119,P11-1019,0,0.524872,"Missing"
D19-1119,N19-1014,0,0.585128,"een applied widely to GEC and has achieved remarkable results in the GEC research field (Ji et al., 2017; Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018). However, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data (Koehn and Knowles, 2017), but the largest set of publicly available parallel data (Lang-8) in GEC has only two million sentence pairs (Mizumoto et al., 2011). Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2018) found that a variant of the backtranslation (Sennrich et al., 2016b) method (BACKTRANS ( NOISY )) outperforms the generation of pseudo data from raw grammatical sentences (D IRECT N OISE). B"
D19-1119,P16-1162,0,0.635333,"by incorporating pseudo training data has been studied intensively (Xie et al., 2018; Ge et al., 2018; Lichtarge et al., 2019; Zhao et al., 2019). ∗ Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2018) found that a variant of the backtranslation (Sennrich et al., 2016b) method (BACKTRANS ( NOISY )) outperforms the generation of pseudo data from raw grammatical sentences (D IRECT N OISE). By contrast, the current state of the art model (Zhao et al., 2019) uses the D IRECT N OISE method. In this study, we investigate these decisions regarding pseudo data, our goal being to provide the research community with an improved understanding of the incorporation of pseudo data. Through extensive experiments, we determine suitable settings for GEC. We justify the reliability of the proposed settings by demonstrating their strong performance on benchmark datasets. Spe"
D19-1379,N09-1014,0,0.0297852,"ce on Natural Language Processing, pages 3665–3671, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work Transductive learning. Vapnik advocated and formalized transductive learning (Vapnik, 1998; Gammerman et al., 1998), which has been applied to text classification (Joachims, 1999; Ifrim and Weikum, 2006) and image processing (Bruzzone et al., 2006; Sener et al., 2016; Liu et al., 2019). Although some studies have presented transductive methods for linear models in other tasks (Duh and Kirchhoff, 2006; Ueffing et al., 2007; Chen et al., 2008; Alexandrescu and Kirchhoff, 2009), transductive methods for neural models are underexplored in NLP. Unsupervised domain adaptation. Transductive learning is related to unsupervised domain adaptation, in which models are adapted to a target domain by using unlabeled target domain texts (Ben-David et al., 2010; Shi and Sha, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zhai, 2007; Tsuboi et al., 2009; Søgaa"
D19-1379,P98-1013,0,0.412296,"2005)4 . The goal of syntactic chunking is to divide a sentence into non-overlapping phrases that consist of syntactically related words. The goal of SRL is to identify semantic arguments for each predicate. For example, consider the following sentence: S YN C HUNK S EM ROLE 3 The man kept a cat [ NP ] [ NP ] [ A0 ] [ A1 ] In our experiments (Section 4), both losses were given by the negative log-likelihood (Appendix A). 4 This paper addresses span-based, PropBank-style SRL. Detailed descriptions on other lines of SRL research (e.g. dependency-based SRL and FrameNet-based SRL) can be found in Baker et al. (1998); Das et al. (2014); Surdeanu et al. (2008); Hajiˇc et al. (2009). In syntactic chunking, given the input sentence, systems have to recognize “The man” and “a cat” as noun phrases (NP). In SRL, given the input sentence and the target predicate “kept”, systems have to recognize “The man” as the A0 argument and “a cat” as the A1 argument. For syntactic chunking, we adopted the experimental protocol by Ponvert et al. (2011) and for SRL, we followed Ouchi et al. (2018) (details in Appendix A). Datasets. We perform experiments using the CoNLL-2012 dataset5 . To investigate the performances under in"
D19-1379,W06-1615,0,0.0896134,"ing et al., 2007; Chen et al., 2008; Alexandrescu and Kirchhoff, 2009), transductive methods for neural models are underexplored in NLP. Unsupervised domain adaptation. Transductive learning is related to unsupervised domain adaptation, in which models are adapted to a target domain by using unlabeled target domain texts (Ben-David et al., 2010; Shi and Sha, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zhai, 2007; Tsuboi et al., 2009; Søgaard, 2013). In the context of neural models, adversarial domain adaptation (Ganin and Lempitsky, 2015; Ganin et al., 2016; Guo et al., 2018), importance weighting (Wang et al., 2017), structural correspondence learning (Ziser and Reichart, 2017), self/tri/co-training (Saito et al., 2017; Ruder and Plank, 2018), and other techniques orthogonal to transductive LM fine-tuning have been applied successfully in unsupervised domain adaptation2 . Integrating these methods with transductive LM fine-tuning is an interesting direction for future research."
D19-1379,W05-0620,0,0.288203,"Missing"
D19-1379,I08-2132,0,0.0285215,"onal Joint Conference on Natural Language Processing, pages 3665–3671, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work Transductive learning. Vapnik advocated and formalized transductive learning (Vapnik, 1998; Gammerman et al., 1998), which has been applied to text classification (Joachims, 1999; Ifrim and Weikum, 2006) and image processing (Bruzzone et al., 2006; Sener et al., 2016; Liu et al., 2019). Although some studies have presented transductive methods for linear models in other tasks (Duh and Kirchhoff, 2006; Ueffing et al., 2007; Chen et al., 2008; Alexandrescu and Kirchhoff, 2009), transductive methods for neural models are underexplored in NLP. Unsupervised domain adaptation. Transductive learning is related to unsupervised domain adaptation, in which models are adapted to a target domain by using unlabeled target domain texts (Ben-David et al., 2010; Shi and Sha, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zha"
D19-1379,D18-1217,0,0.0173867,"et domain unlabeled data. 8 As target domain unlabeled data, we use the CoNLL2012 training set of each domain. 3668 BC BN MZ NW PT TC WB Syntactic chunking U U+T 90.5 91.0 91.3 91.6 90.2 90.6 92.1 92.5 87.3 87.7 87.2 87.6 91.8 92.2 Semantic role labeling U U+T 79.0 79.4 80.1 80.6 78.3 78.7 81.5 81.9 73.6 74.3 71.4 72.0 76.8 77.2 Table 4: Performance comparison between LM finetuning on target domain unlabeled data (U) and on the combination of the unlabeled data and test sets (U + T). Cells show the F1 scores averaged across the target domains. CoNLL 2000 BASE T RANS 96.6 96.7 97.0 96.4 95.8 - Clark et al. (2018) Peters et al. (2017) Hashimoto et al. (2017) Wang et al. (2019) Li et al. (2019) Ouchi et al. (2018) He et al. (2018) 2005 WSJ Brown 87.7 78.3 87.9* 79.5* 88.2 79.3 87.7 80.5 87.6 78.7 87.4 80.4 2012 86.2 86.6* 86.4 86.0 86.2 85.5 use the CoNLL-2000 dataset (Sang and Buchholz, 2000) and follow the standard experimental protocol (Hashimoto et al., 2017). For SRL, we use the CoNLL-2005 (Carreras and M`arquez, 2005) and CoNLL-2012 datasets (Pradhan et al., 2012) and follow the standard experimental protocol (Ouchi et al., 2018). Table 5 shows the F1 scores of our models and those of existing mod"
D19-1379,P07-1033,0,0.0514405,"Missing"
D19-1379,W17-3203,0,0.0311095,"of using both datasets. Table 4 shows the F1 scores averaged across all the target domains. Fine-tuning the LMs on the target domain unlabeled data as well as each test set (U + T) showed better performance than fine-tuning them only on the target domain unlabeled data (U). This combination of tranduction with unsupervised domain adaptation further improves performance. Effects in standard benchmarks. Some studies indicated that when promising new techniques are only evaluated on very basic models, determining how much (if any) improvement will carry over to stronger models can be difficult (Denkowski and Neubig, 2017; Suzuki et al., 2018). Motivated by such studies, we provide the results in standard benchmark settings. For syntactic chunking, we Conclusion In this study, we investigated the impact of transductive learning on state-of-the-art neural models in syntactic and semantic tasks. Specifically, we fine-tuned an LM on an unlabeled test set. Through extensive experiments, we demonstrated that, despite its simplicity, transductive LM finetuning contributes to consistent performance improvement of state-of-the-art syntactic and semantic models in cross-domain settings. One interesting line of future w"
D19-1379,N19-1423,0,0.161704,"or text classification, transductive methods for linear models have been investigated in only a few tasks, such as lexical acquisition (Duh and Kirchhoff, 2006) and machine translation (Ueffing et al., 2007). In particular, transductive learning with neural networks is underexplored. Here, we investigate the impact of transductive learning on state-of-the-art neural models in syntactic and semantic tasks, namely syntactic chunking and semantic role labeling (SRL). Specifically, inspired by recent findings that language model (LM)-based word representations yield large performance improvement (Devlin et al., 2019), we fine-tune Embeddings from Language Models (ELMo) (Peters et al., 2018) on an unlabeled test set and use them in each task-specific model. Typically, LMs are trained on a large-scale corpus whose word distributions are different from the test set. By contrast, transductive learning allows us to fit LMs directly to the distributions of the test set. Our experiments show the effectiveness of transductive LM fine-tuning. In summary, our main contributions are: • This work is the first to introduce an LM finetuning method to transductive learning1 . • Through extensive experiments in both indo"
D19-1379,W06-1647,0,0.190913,"unlabeled test set is given in the training phase. That is, the inputs of the test set, i.e., the raw texts, can be used during training, but the labels are never used. In the test phase, the trained model is evaluated on the same test set. Despite its practical advantages, transductive learning has received little attention in natural language processing (NLP). After the pioneering work of Joachims (1999), who proposed a transductive support vector machine for text classification, transductive methods for linear models have been investigated in only a few tasks, such as lexical acquisition (Duh and Kirchhoff, 2006) and machine translation (Ueffing et al., 2007). In particular, transductive learning with neural networks is underexplored. Here, we investigate the impact of transductive learning on state-of-the-art neural models in syntactic and semantic tasks, namely syntactic chunking and semantic role labeling (SRL). Specifically, inspired by recent findings that language model (LM)-based word representations yield large performance improvement (Devlin et al., 2019), we fine-tune Embeddings from Language Models (ELMo) (Peters et al., 2018) on an unlabeled test set and use them in each task-specific mode"
D19-1379,J02-3001,0,0.473299,"In the LM pre-training and fine-tuning phases (Eqs. 1 and 2), we first train the initial LM parameters Θ and then fine-tune the pre-trained parameters Θ0 . In the task-specific training phase (Eq. 3), we fix the fine-tuned LM parameters Θ00 used for the embedding layer of a task-specific model, and train only the task-specific model parameters Φ. 4 Experiments Tasks. To investigate the effectiveness of transductive LM fine-tuning for syntactic and semantic analysis, we conduct experiments in syntactic chunking (Ramshaw and Marcus, 1999; Sang and Buchholz, 2000; Ponvert et al., 2011) and SRL (Gildea and Jurafsky, 2002; Palmer et al., 2005; Carreras and M`arquez, 2005)4 . The goal of syntactic chunking is to divide a sentence into non-overlapping phrases that consist of syntactically related words. The goal of SRL is to identify semantic arguments for each predicate. For example, consider the following sentence: S YN C HUNK S EM ROLE 3 The man kept a cat [ NP ] [ NP ] [ A0 ] [ A1 ] In our experiments (Section 4), both losses were given by the negative log-likelihood (Appendix A). 4 This paper addresses span-based, PropBank-style SRL. Detailed descriptions on other lines of SRL research (e.g. dependency-base"
D19-1379,D18-1498,0,0.0229667,"s related to unsupervised domain adaptation, in which models are adapted to a target domain by using unlabeled target domain texts (Ben-David et al., 2010; Shi and Sha, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zhai, 2007; Tsuboi et al., 2009; Søgaard, 2013). In the context of neural models, adversarial domain adaptation (Ganin and Lempitsky, 2015; Ganin et al., 2016; Guo et al., 2018), importance weighting (Wang et al., 2017), structural correspondence learning (Ziser and Reichart, 2017), self/tri/co-training (Saito et al., 2017; Ruder and Plank, 2018), and other techniques orthogonal to transductive LM fine-tuning have been applied successfully in unsupervised domain adaptation2 . Integrating these methods with transductive LM fine-tuning is an interesting direction for future research. LM-based word representations. Recently, LM-based word representations pre-trained on unlabeled data have gained considerable attention (Peters et al., 2018; Radford et al., 2018; Devlin e"
D19-1379,D17-1206,0,0.0802734,"Missing"
D19-1379,P18-2058,0,0.0469233,"Missing"
D19-1379,P18-1031,0,0.0238789,"ues orthogonal to transductive LM fine-tuning have been applied successfully in unsupervised domain adaptation2 . Integrating these methods with transductive LM fine-tuning is an interesting direction for future research. LM-based word representations. Recently, LM-based word representations pre-trained on unlabeled data have gained considerable attention (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). The most related method to ours is Universal Language Model Fine-tuning (ULMFiT), which pre-trains an LM on a large general-domain corpus and fine-tunes it on the target task (Howard and Ruder, 2018). Inspired by these studies, we introduce LM-based word representation in transductive learning. 2 Feature augmentation is considered a supervised domain adaptation method (Daume III, 2007; Kim et al., 2016). (3) (1) Transductive (2) LM Fine-tuning Figure 1: Training procedure. (1) LM pre-training: the LM is firstly pre-trained on the large-scale unlalarge beled corpus Dlarge = {Xilarge }N i=1 . (2) Transductive LM fine-tuning: the LM is then fine-tuned on the test unlabeled test set Dtest = {Xitest }N i=1 . Note that the test set used for training is the identical one used in evaluation. (3)"
D19-1379,P07-1034,0,0.110447,"et al., 2008; Alexandrescu and Kirchhoff, 2009), transductive methods for neural models are underexplored in NLP. Unsupervised domain adaptation. Transductive learning is related to unsupervised domain adaptation, in which models are adapted to a target domain by using unlabeled target domain texts (Ben-David et al., 2010; Shi and Sha, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zhai, 2007; Tsuboi et al., 2009; Søgaard, 2013). In the context of neural models, adversarial domain adaptation (Ganin and Lempitsky, 2015; Ganin et al., 2016; Guo et al., 2018), importance weighting (Wang et al., 2017), structural correspondence learning (Ziser and Reichart, 2017), self/tri/co-training (Saito et al., 2017; Ruder and Plank, 2018), and other techniques orthogonal to transductive LM fine-tuning have been applied successfully in unsupervised domain adaptation2 . Integrating these methods with transductive LM fine-tuning is an interesting direction for future research. LM-based word represe"
D19-1379,P18-1110,0,0.045924,"Missing"
D19-1379,C16-1038,0,0.0178784,"esearch. LM-based word representations. Recently, LM-based word representations pre-trained on unlabeled data have gained considerable attention (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). The most related method to ours is Universal Language Model Fine-tuning (ULMFiT), which pre-trains an LM on a large general-domain corpus and fine-tunes it on the target task (Howard and Ruder, 2018). Inspired by these studies, we introduce LM-based word representation in transductive learning. 2 Feature augmentation is considered a supervised domain adaptation method (Daume III, 2007; Kim et al., 2016). (3) (1) Transductive (2) LM Fine-tuning Figure 1: Training procedure. (1) LM pre-training: the LM is firstly pre-trained on the large-scale unlalarge beled corpus Dlarge = {Xilarge }N i=1 . (2) Transductive LM fine-tuning: the LM is then fine-tuned on the test unlabeled test set Dtest = {Xitest }N i=1 . Note that the test set used for training is the identical one used in evaluation. (3) Task-specific model training: the taskspecific model is trained on the training set Dtrain = train {(Xitrain , Yitrain )}N i=1 . L denotes the loss function. 3 Neural Transductive Learning Motivation. Suppos"
D19-1379,W00-0726,0,0.635178,"ctions for an LM and task-specific model, respectively.3 In the LM pre-training and fine-tuning phases (Eqs. 1 and 2), we first train the initial LM parameters Θ and then fine-tune the pre-trained parameters Θ0 . In the task-specific training phase (Eq. 3), we fix the fine-tuned LM parameters Θ00 used for the embedding layer of a task-specific model, and train only the task-specific model parameters Φ. 4 Experiments Tasks. To investigate the effectiveness of transductive LM fine-tuning for syntactic and semantic analysis, we conduct experiments in syntactic chunking (Ramshaw and Marcus, 1999; Sang and Buchholz, 2000; Ponvert et al., 2011) and SRL (Gildea and Jurafsky, 2002; Palmer et al., 2005; Carreras and M`arquez, 2005)4 . The goal of syntactic chunking is to divide a sentence into non-overlapping phrases that consist of syntactically related words. The goal of SRL is to identify semantic arguments for each predicate. For example, consider the following sentence: S YN C HUNK S EM ROLE 3 The man kept a cat [ NP ] [ NP ] [ A0 ] [ A1 ] In our experiments (Section 4), both losses were given by the negative log-likelihood (Appendix A). 4 This paper addresses span-based, PropBank-style SRL. Detailed descrip"
D19-1379,D18-1191,1,0.909061,"style SRL. Detailed descriptions on other lines of SRL research (e.g. dependency-based SRL and FrameNet-based SRL) can be found in Baker et al. (1998); Das et al. (2014); Surdeanu et al. (2008); Hajiˇc et al. (2009). In syntactic chunking, given the input sentence, systems have to recognize “The man” and “a cat” as noun phrases (NP). In SRL, given the input sentence and the target predicate “kept”, systems have to recognize “The man” as the A0 argument and “a cat” as the A1 argument. For syntactic chunking, we adopted the experimental protocol by Ponvert et al. (2011) and for SRL, we followed Ouchi et al. (2018) (details in Appendix A). Datasets. We perform experiments using the CoNLL-2012 dataset5 . To investigate the performances under in-domain and out-of-domain settings, we use each of the seven domains in the CoNLL-2012 dataset. Table 1 shows the data statistics. Each test set contains at most 2,000 sentences. Compared with previous studies, such as Xiao and Guo (2013) that used 570,000 sentences as unlabeled data for unsupervised domain adaptation of syntactic chunking, our transductive experiments can be regarded as a low-resource adaptation setting. As a large-scale unlabeled raw corpus for L"
D19-1379,J05-1004,0,0.0940305,"fine-tuning phases (Eqs. 1 and 2), we first train the initial LM parameters Θ and then fine-tune the pre-trained parameters Θ0 . In the task-specific training phase (Eq. 3), we fix the fine-tuned LM parameters Θ00 used for the embedding layer of a task-specific model, and train only the task-specific model parameters Φ. 4 Experiments Tasks. To investigate the effectiveness of transductive LM fine-tuning for syntactic and semantic analysis, we conduct experiments in syntactic chunking (Ramshaw and Marcus, 1999; Sang and Buchholz, 2000; Ponvert et al., 2011) and SRL (Gildea and Jurafsky, 2002; Palmer et al., 2005; Carreras and M`arquez, 2005)4 . The goal of syntactic chunking is to divide a sentence into non-overlapping phrases that consist of syntactically related words. The goal of SRL is to identify semantic arguments for each predicate. For example, consider the following sentence: S YN C HUNK S EM ROLE 3 The man kept a cat [ NP ] [ NP ] [ A0 ] [ A1 ] In our experiments (Section 4), both losses were given by the negative log-likelihood (Appendix A). 4 This paper addresses span-based, PropBank-style SRL. Detailed descriptions on other lines of SRL research (e.g. dependency-based SRL and FrameNet-ba"
D19-1379,P17-1161,0,0.0680021,"Missing"
D19-1379,N18-1202,0,0.518315,"vestigated in only a few tasks, such as lexical acquisition (Duh and Kirchhoff, 2006) and machine translation (Ueffing et al., 2007). In particular, transductive learning with neural networks is underexplored. Here, we investigate the impact of transductive learning on state-of-the-art neural models in syntactic and semantic tasks, namely syntactic chunking and semantic role labeling (SRL). Specifically, inspired by recent findings that language model (LM)-based word representations yield large performance improvement (Devlin et al., 2019), we fine-tune Embeddings from Language Models (ELMo) (Peters et al., 2018) on an unlabeled test set and use them in each task-specific model. Typically, LMs are trained on a large-scale corpus whose word distributions are different from the test set. By contrast, transductive learning allows us to fit LMs directly to the distributions of the test set. Our experiments show the effectiveness of transductive LM fine-tuning. In summary, our main contributions are: • This work is the first to introduce an LM finetuning method to transductive learning1 . • Through extensive experiments in both indomain and out-of-domain settings, we demonstrate that transductive LM fine-t"
D19-1379,P11-1108,0,0.0567729,"Missing"
D19-1379,W12-4501,0,0.0181753,"ata and test sets (U + T). Cells show the F1 scores averaged across the target domains. CoNLL 2000 BASE T RANS 96.6 96.7 97.0 96.4 95.8 - Clark et al. (2018) Peters et al. (2017) Hashimoto et al. (2017) Wang et al. (2019) Li et al. (2019) Ouchi et al. (2018) He et al. (2018) 2005 WSJ Brown 87.7 78.3 87.9* 79.5* 88.2 79.3 87.7 80.5 87.6 78.7 87.4 80.4 2012 86.2 86.6* 86.4 86.0 86.2 85.5 use the CoNLL-2000 dataset (Sang and Buchholz, 2000) and follow the standard experimental protocol (Hashimoto et al., 2017). For SRL, we use the CoNLL-2005 (Carreras and M`arquez, 2005) and CoNLL-2012 datasets (Pradhan et al., 2012) and follow the standard experimental protocol (Ouchi et al., 2018). Table 5 shows the F1 scores of our models and those of existing models. The results of the baseline model were comparable with those of the state-of-the-art models, and the transductive model consistently outperformed the baseline model9 . Note that we cannot fairly compare the transductive and existing models due to the difference in settings. These results, however, demonstrate that transductive LM fine-tuning improves state-of-the-art chunking and SRL models. 6 Table 5: Standard benchmark results. Cells show the F1 scores"
D19-1379,P18-1096,0,0.0204231,"a, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zhai, 2007; Tsuboi et al., 2009; Søgaard, 2013). In the context of neural models, adversarial domain adaptation (Ganin and Lempitsky, 2015; Ganin et al., 2016; Guo et al., 2018), importance weighting (Wang et al., 2017), structural correspondence learning (Ziser and Reichart, 2017), self/tri/co-training (Saito et al., 2017; Ruder and Plank, 2018), and other techniques orthogonal to transductive LM fine-tuning have been applied successfully in unsupervised domain adaptation2 . Integrating these methods with transductive LM fine-tuning is an interesting direction for future research. LM-based word representations. Recently, LM-based word representations pre-trained on unlabeled data have gained considerable attention (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). The most related method to ours is Universal Language Model Fine-tuning (ULMFiT), which pre-trains an LM on a large general-domain corpus and fine-tunes it o"
D19-1379,W08-2121,0,0.0654377,"Missing"
D19-1379,P18-2097,1,0.775367,"le 4 shows the F1 scores averaged across all the target domains. Fine-tuning the LMs on the target domain unlabeled data as well as each test set (U + T) showed better performance than fine-tuning them only on the target domain unlabeled data (U). This combination of tranduction with unsupervised domain adaptation further improves performance. Effects in standard benchmarks. Some studies indicated that when promising new techniques are only evaluated on very basic models, determining how much (if any) improvement will carry over to stronger models can be difficult (Denkowski and Neubig, 2017; Suzuki et al., 2018). Motivated by such studies, we provide the results in standard benchmark settings. For syntactic chunking, we Conclusion In this study, we investigated the impact of transductive learning on state-of-the-art neural models in syntactic and semantic tasks. Specifically, we fine-tuned an LM on an unlabeled test set. Through extensive experiments, we demonstrated that, despite its simplicity, transductive LM finetuning contributes to consistent performance improvement of state-of-the-art syntactic and semantic models in cross-domain settings. One interesting line of future work is to explore effe"
D19-1379,P07-1004,0,0.296097,". That is, the inputs of the test set, i.e., the raw texts, can be used during training, but the labels are never used. In the test phase, the trained model is evaluated on the same test set. Despite its practical advantages, transductive learning has received little attention in natural language processing (NLP). After the pioneering work of Joachims (1999), who proposed a transductive support vector machine for text classification, transductive methods for linear models have been investigated in only a few tasks, such as lexical acquisition (Duh and Kirchhoff, 2006) and machine translation (Ueffing et al., 2007). In particular, transductive learning with neural networks is underexplored. Here, we investigate the impact of transductive learning on state-of-the-art neural models in syntactic and semantic tasks, namely syntactic chunking and semantic role labeling (SRL). Specifically, inspired by recent findings that language model (LM)-based word representations yield large performance improvement (Devlin et al., 2019), we fine-tune Embeddings from Language Models (ELMo) (Peters et al., 2018) on an unlabeled test set and use them in each task-specific model. Typically, LMs are trained on a large-scale"
D19-1379,D17-1155,0,0.0196039,"on, in which models are adapted to a target domain by using unlabeled target domain texts (Ben-David et al., 2010; Shi and Sha, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zhai, 2007; Tsuboi et al., 2009; Søgaard, 2013). In the context of neural models, adversarial domain adaptation (Ganin and Lempitsky, 2015; Ganin et al., 2016; Guo et al., 2018), importance weighting (Wang et al., 2017), structural correspondence learning (Ziser and Reichart, 2017), self/tri/co-training (Saito et al., 2017; Ruder and Plank, 2018), and other techniques orthogonal to transductive LM fine-tuning have been applied successfully in unsupervised domain adaptation2 . Integrating these methods with transductive LM fine-tuning is an interesting direction for future research. LM-based word representations. Recently, LM-based word representations pre-trained on unlabeled data have gained considerable attention (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). The most related method to o"
D19-1379,P19-1529,0,0.0343404,"Missing"
D19-1379,K17-1040,0,0.0184403,"ing unlabeled target domain texts (Ben-David et al., 2010; Shi and Sha, 2012). This setting does not allow models to access the test set, which is the main difference between unsupervised domain adaptation and transductive learning. Various unsupervised adaptation methods have been proposed for linear models (Blitzer et al., 2006; Jiang and Zhai, 2007; Tsuboi et al., 2009; Søgaard, 2013). In the context of neural models, adversarial domain adaptation (Ganin and Lempitsky, 2015; Ganin et al., 2016; Guo et al., 2018), importance weighting (Wang et al., 2017), structural correspondence learning (Ziser and Reichart, 2017), self/tri/co-training (Saito et al., 2017; Ruder and Plank, 2018), and other techniques orthogonal to transductive LM fine-tuning have been applied successfully in unsupervised domain adaptation2 . Integrating these methods with transductive LM fine-tuning is an interesting direction for future research. LM-based word representations. Recently, LM-based word representations pre-trained on unlabeled data have gained considerable attention (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). The most related method to ours is Universal Language Model Fine-tuning (ULMFiT), which pre"
D19-3039,W00-0507,0,0.193737,"Missing"
D19-3039,N15-3019,0,0.0338054,"Missing"
D19-3039,N15-3022,0,0.0645812,"frontend AFTER Editor Word Processor Editor Editor TEASPN Research Models GEC Model Suggestion Model Search OSS Spell Checker Commercial API Figure 1: Writing software before and after TEASPN. pletion of a prompt with long, realistic looking yet coherent passages (Radford et al., 2019). However, real-world users such as writers who can and should benefit the most from WATs are yet to reap the fruits from these research efforts. Aside from a small number of commercial products, notably Grammarly2 and Smart Compose (Chen et al., 2019), and research systems such as WriteAhead (Yen et al., 2015; Chang and Chang, 2015) and CroVeWA (Soyer et al., 2015), we see few examples of user-facing applications and experiments that make use of recent development in WATs. Many models are confined in research implementations that are not easily accessible to end users and the larger society. WATs, however, are not truly useful until they are integrated into userfacing writing applications such as editors and word processors (collectively called writing software in this paper) and interact with end users in a dynamic and intuitive manner. This “great divide” (see Figure 1 BEFORE) between applicaIntroduction Language techn"
D19-3039,P12-3027,0,0.0195084,"for assisting writing in a second language (L2) has been extensively explored, especially for non-native English speakers. One of the most active research areas is GEC (Leacock et al., 2010), where several new models are published every year and commercial systems such as Grammarly are actively developed. Other research-based systems include WriteAhead (Yen et al., 2015; Chang and Chang, 2015), an interactive writing environment that provides users with grammatical patterns mined from large corpora, and CroVeWA (Soyer et al., 2015), a crosslingual sentence search system for L2 writers. FLOW (Chen et al., 2012) is another writing assistance system that allows users to type in their first languages (L1) and suggests words and phrases in L2. Running syntactic analysis and visualizing sentence structures have also been explored for L2 reading assistance (Faltin, 2003; Srdanovi´c, 2011). In addition to L2 learners, the use of technologies for assisting human translators has also been a focus of research. TransType (Langlais et al., 2000) is a translation assistance system that suggests completions for the text to the human translator in an interactive manner. In SemEval 2014, van Gompel et al. (2014) pr"
D19-3039,P18-1042,0,0.0141069,"wn in Table 1. See Figure 4 for the screenshot. For syntax highlighting, we used the dependency parser SpaCy9 . Head tokens with specific dependency relation10 were highlighted in different colors. As for the GEC and GED features, we used the open-source GEC tool LanguageTool 3.211 . We implemented two types of completion features: one which suggests the likely next phrases given the context using a neural language model (Radford et al., 2019) and the other one which suggests a set of words consistent with the characters being typed. We built a seq2seq paraphrase model trained on PARANMT-50M (Wieting and Gimpel, 2018) for the text rewriting feature, which allows the writer to select a part of the text and chooses among paraphrases. As for the jump feature, we used a coreference resolution model12 to jump from a selected expression to its antecedent. The hover feature shows the definition of a hovered word using WordNet13 . Finally, we implemented a full-text search feature using the open multilingual sentence dataset Tatoeba14 and used Elasticsearch 7.1.115 for indexing and search. Condition Perplexity # Chars. (mean ± std) BASELINE 37.8 379 ± 116 INTEGRATED 26.4 335 ± 91 Table 2: Statistics of the written"
D19-3039,P15-4024,0,0.0930178,"iting Software Web frontend AFTER Editor Word Processor Editor Editor TEASPN Research Models GEC Model Suggestion Model Search OSS Spell Checker Commercial API Figure 1: Writing software before and after TEASPN. pletion of a prompt with long, realistic looking yet coherent passages (Radford et al., 2019). However, real-world users such as writers who can and should benefit the most from WATs are yet to reap the fruits from these research efforts. Aside from a small number of commercial products, notably Grammarly2 and Smart Compose (Chen et al., 2019), and research systems such as WriteAhead (Yen et al., 2015; Chang and Chang, 2015) and CroVeWA (Soyer et al., 2015), we see few examples of user-facing applications and experiments that make use of recent development in WATs. Many models are confined in research implementations that are not easily accessible to end users and the larger society. WATs, however, are not truly useful until they are integrated into userfacing writing applications such as editors and word processors (collectively called writing software in this paper) and interact with end users in a dynamic and intuitive manner. This “great divide” (see Figure 1 BEFORE) between applicaInt"
D19-6004,D15-1075,0,0.0428713,"πk = αk Table 1: Reported results on COPA. With the exception of (Wang et al., 2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See §2 for model details. * indicates our replication experiments. Finally, the coverage ξk of a token is the proportion of applicable instances among all instances: ξk = gument Reasoning Comprehension Task (Habernal et al., 2018). Similarly, Gururangan et al. (2018); Poliak et al. (2018); Dasgupta et al. (2018) showed that a simple text categorization model can perform well on the Stanford Natural Language Inference dataset (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) when given incomplete input, even though the task should not be solvable without the full input. This suggests that the partial input contains unintended superficial cues that allow the models to take shortcuts without learning the actual task. Sugawara et al. (2018) investigated superficial cues that make questions easier across recent machine reading comprehension datasets. Given the fact that superficial cues were found in benchmark datasets for a wide variety of natural language understanding task, does COPA contain such cues, as well? 2.2 αk n Table 2"
D19-6004,P19-1475,0,0.0232661,"Missing"
D19-6004,N19-1423,0,0.179967,"exploiting these cues, but do not develop any deeper understanding of the task. While superficial cues have been identified in, among others, datasets for NLI (Gururangan et al., 2018; McCoy et al., 2019), machine reading comprehension (Sugawara et al., 2018), and argumentation (Niven and Kao, 2019), one of the main benchmarks for commonsense reasoning, namely the Choice of Plausible Alternatives (COPA, Roemmele et al., 2011), has not been analyzed so far. Here we present an analysis of superficial cues in COPA. Introduction Pretrained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019b) have led to improved performance in benchmarks of natural language understanding, in tasks such as natural language inference (NLI, Liu et al., 2019a), argumentation (Niven and Kao, 2019), and commonsense reasoning (Li et al., 2019; Sap et al., 2019). However, recent work has identified superficial cues in benchmark datasets which are predictive of the correct answer, such as token distributions and lexical overlap. Once these cues are neutralized, models perform poorly, suggesting that their good ∗ 1 Named after the eponymous horse which appeared to be capabl"
D19-6004,P19-1441,0,0.0203589,"evelop any deeper understanding of the task. While superficial cues have been identified in, among others, datasets for NLI (Gururangan et al., 2018; McCoy et al., 2019), machine reading comprehension (Sugawara et al., 2018), and argumentation (Niven and Kao, 2019), one of the main benchmarks for commonsense reasoning, namely the Choice of Plausible Alternatives (COPA, Roemmele et al., 2011), has not been analyzed so far. Here we present an analysis of superficial cues in COPA. Introduction Pretrained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019b) have led to improved performance in benchmarks of natural language understanding, in tasks such as natural language inference (NLI, Liu et al., 2019a), argumentation (Niven and Kao, 2019), and commonsense reasoning (Li et al., 2019; Sap et al., 2019). However, recent work has identified superficial cues in benchmark datasets which are predictive of the correct answer, such as token distributions and lexical overlap. Once these cues are neutralized, models perform poorly, suggesting that their good ∗ 1 Named after the eponymous horse which appeared to be capable of simple mental tasks but ac"
D19-6004,2021.ccl-1.108,0,0.16747,"Missing"
D19-6004,S12-1063,0,0.0305715,"Given the premise and two alternatives, one of which has a causal relation to the premise, while the other does not, models need to choose the more plausible alternative. Figure 1a shows an example of a COPA instance. The overall 1000 instances are split into training set3 and test set of 500 instances each. Prior to neural network approaches, the most dominant way of solving COPA was via Pointwise Mutual Information (PMI)-based statistics using a large background corpus between the content words in the premise and the alternatives (Gordon et al., 2011; Luo et al., 2016; Sasaki et al., 2017; Goodwin et al., 2012). Recent studies show that BERT and RoBERTa achieve considerable improvements on COPA (see Table 1). However, recent work found that the strong performance of BERT and other deep neural models in benchmarks of natural language understanding can be partly or in some cases entirely explained by their capability to exploit superficial cues present in benchmark datasets. For example, Niven and Kao (2019) found that BERT exploits superficial cues, namely the occurrence of certain tokens such as not, in the Ar2 http://people.ict.usc.edu/˜gordon/ copa.html 3 This set is called development set by Roem"
D19-6004,P19-1334,0,0.0255444,"cue exploited by BERT. We neutralize such superficial cues by creating a mirrored instance (b). After mirroring, the highlighted superficial cue becomes ineffective in predicting the correct answer, since it occurs with equal probability in correct and wrong alternatives. performance is an instance of the Clever Hans effect1 (Pfungst, 1911): Models trained on datasets with superficial cues learn heuristics for exploiting these cues, but do not develop any deeper understanding of the task. While superficial cues have been identified in, among others, datasets for NLI (Gururangan et al., 2018; McCoy et al., 2019), machine reading comprehension (Sugawara et al., 2018), and argumentation (Niven and Kao, 2019), one of the main benchmarks for commonsense reasoning, namely the Choice of Plausible Alternatives (COPA, Roemmele et al., 2011), has not been analyzed so far. Here we present an analysis of superficial cues in COPA. Introduction Pretrained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019b) have led to improved performance in benchmarks of natural language understanding, in tasks such as natural language inference (NLI, Liu et al., 2019a)"
D19-6004,P19-1459,0,0.284667,". After mirroring, the highlighted superficial cue becomes ineffective in predicting the correct answer, since it occurs with equal probability in correct and wrong alternatives. performance is an instance of the Clever Hans effect1 (Pfungst, 1911): Models trained on datasets with superficial cues learn heuristics for exploiting these cues, but do not develop any deeper understanding of the task. While superficial cues have been identified in, among others, datasets for NLI (Gururangan et al., 2018; McCoy et al., 2019), machine reading comprehension (Sugawara et al., 2018), and argumentation (Niven and Kao, 2019), one of the main benchmarks for commonsense reasoning, namely the Choice of Plausible Alternatives (COPA, Roemmele et al., 2011), has not been analyzed so far. Here we present an analysis of superficial cues in COPA. Introduction Pretrained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019b) have led to improved performance in benchmarks of natural language understanding, in tasks such as natural language inference (NLI, Liu et al., 2019a), argumentation (Niven and Kao, 2019), and commonsense reasoning (Li et al., 2019; Sap et al., 2"
D19-6004,N18-2017,0,0.0696421,"Missing"
D19-6004,N18-1202,0,0.024916,"l cues learn heuristics for exploiting these cues, but do not develop any deeper understanding of the task. While superficial cues have been identified in, among others, datasets for NLI (Gururangan et al., 2018; McCoy et al., 2019), machine reading comprehension (Sugawara et al., 2018), and argumentation (Niven and Kao, 2019), one of the main benchmarks for commonsense reasoning, namely the Choice of Plausible Alternatives (COPA, Roemmele et al., 2011), has not been analyzed so far. Here we present an analysis of superficial cues in COPA. Introduction Pretrained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019b) have led to improved performance in benchmarks of natural language understanding, in tasks such as natural language inference (NLI, Liu et al., 2019a), argumentation (Niven and Kao, 2019), and commonsense reasoning (Li et al., 2019; Sap et al., 2019). However, recent work has identified superficial cues in benchmark datasets which are predictive of the correct answer, such as token distributions and lexical overlap. Once these cues are neutralized, models perform poorly, suggesting that their good ∗ 1 Named after the eponymous horse"
D19-6004,N18-1175,0,0.158715,"αk = n h i X (i) 1 ∃j, k ∈ T(i) ∧ k ∈ / T j ¬j i=1 The productivity πk of a token is the proportion of applicable instances for which it predicts the correct answer: h i Pn (i) (i) 1 ∃j, k ∈ T ∧ k ∈ / T ∧ y = j i i=1 j ¬j πk = αk Table 1: Reported results on COPA. With the exception of (Wang et al., 2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See §2 for model details. * indicates our replication experiments. Finally, the coverage ξk of a token is the proportion of applicable instances among all instances: ξk = gument Reasoning Comprehension Task (Habernal et al., 2018). Similarly, Gururangan et al. (2018); Poliak et al. (2018); Dasgupta et al. (2018) showed that a simple text categorization model can perform well on the Stanford Natural Language Inference dataset (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) when given incomplete input, even though the task should not be solvable without the full input. This suggests that the partial input contains unintended superficial cues that allow the models to take shortcuts without learning the actual task. Sugawara et al. (2018) investigated superficial cues that make questions easier across recent mac"
D19-6004,S18-2023,0,0.0530956,"Missing"
D19-6004,D19-1454,0,0.0550109,"Missing"
D19-6004,N18-1101,0,0.0375144,"n COPA. With the exception of (Wang et al., 2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See §2 for model details. * indicates our replication experiments. Finally, the coverage ξk of a token is the proportion of applicable instances among all instances: ξk = gument Reasoning Comprehension Task (Habernal et al., 2018). Similarly, Gururangan et al. (2018); Poliak et al. (2018); Dasgupta et al. (2018) showed that a simple text categorization model can perform well on the Stanford Natural Language Inference dataset (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) when given incomplete input, even though the task should not be solvable without the full input. This suggests that the partial input contains unintended superficial cues that allow the models to take shortcuts without learning the actual task. Sugawara et al. (2018) investigated superficial cues that make questions easier across recent machine reading comprehension datasets. Given the fact that superficial cues were found in benchmark datasets for a wide variety of natural language understanding task, does COPA contain such cues, as well? 2.2 αk n Table 2 shows the five tokens with highest c"
D19-6004,2020.tacl-1.3,0,\N,Missing
D19-6119,N19-1416,0,0.0240073,"Missing"
D19-6119,P11-1076,0,0.811462,"formation from rubrics for SAG. In this paper, we present a method to incorporate rubric information into neural SAG models. Our idea is to enable neural models to capture alignments between an answer and each key element. Specifically, we use a word-level attention mechanism to compute alignments and generate an attentional feature vector for each pair of an answer and a key element. The contributions of this study is summarized as follows: Introduction Short Answer Grading (SAG) is the task of automatically evaluating the correctness of students’ answers to a given prompt in an examination (Mohler et al., 2011). It would be beneficial particularly in an educational context where teachers’ availability is limited (Mohler and Mihalcea, 2009). Motivated by this background, SAG has been studied mainly with machine learning-based approaches, where the task is considered as inducing a regression model from a given set of manually scored sample answers (i.e., training instances). As observed in a variety of other NLP tasks, recently proposed neural models have been yielding strong results (Riordan et al., 2017). In general, a prompt is provided along with a scoring rubric. Figure 1 shows a typical example."
D19-6119,P02-1040,0,0.104223,"m Latent Semantic Analysis (LSA) (Mohler et al., 2011), edit distance-based similarity, and knowledgebased similarity using WordNet (Pedersen et al., 2004) (Magooda et al., 2016) to word embeddingbased similarity (Sultan et al., 2016). Recently, Riordan et al. (2017) report that neural networkbased feature representation learning (Taghipour and Ng, 2016) is effective for SAG. In contrast to the popularity of learning answer representations, the use of rubric information for SAG has been gained little attention so far. In Sakaguchi et al. (2015), the authors compute similarities, such as BLEU (Papineni et al., 2002), between an answer and each key element in a rubric, and use them as features in a support vector regression (SVR) model. Ramachandran et al. (2015). Ramachandran et al. (2015) generates text patterns from top answers and rubrics, and reports the automatically generated pattern performances better than manually generated regex pattern. Nevertheless, it still remains an open issue (i) whether a rubric is effective or not even in the context of a neural representation learning paradigm (Riordan et al., 2017), and (ii) what kinds of neural architectures should be employed for the efficient use o"
D19-6119,D14-1162,0,0.0810217,"Missing"
D19-6119,W15-0612,0,0.0544778,"al., 2004) (Magooda et al., 2016) to word embeddingbased similarity (Sultan et al., 2016). Recently, Riordan et al. (2017) report that neural networkbased feature representation learning (Taghipour and Ng, 2016) is effective for SAG. In contrast to the popularity of learning answer representations, the use of rubric information for SAG has been gained little attention so far. In Sakaguchi et al. (2015), the authors compute similarities, such as BLEU (Papineni et al., 2002), between an answer and each key element in a rubric, and use them as features in a support vector regression (SVR) model. Ramachandran et al. (2015). Ramachandran et al. (2015) generates text patterns from top answers and rubrics, and reports the automatically generated pattern performances better than manually generated regex pattern. Nevertheless, it still remains an open issue (i) whether a rubric is effective or not even in the context of a neural representation learning paradigm (Riordan et al., 2017), and (ii) what kinds of neural architectures should be employed for the efficient use of rubrics. Another issue in SAG is on low-resource settings. Heilman and Madnani (2015) investigate the importance of the training data size on nonne"
D19-6119,W17-5017,0,0.451777,"nject Rubrics into Short Answer Grading System Tianqi Wang1,3 1 Naoya Inoue1,3 Hiroki Ouchi3 Tomoya Mizumoto2,3 Kentaro Inui1,3 Tohoku University 2 Future Corporation 3 RIKEN Center for Advanced Intelligence Project {outenki,naoya-i,inui}@ecei.tohoku.ac.jp hiroki.ouchi@riken.jp t.mizumoto.yb@future.co.jp Abstract Prompt Starting with mRNA leaving the nucleus, list and describe four major steps involved in protein synthesis. Short Answer Grading (SAG) is a task of scoring students’ answers in examinations. Most existing SAG systems predict scores based only on the answers, including the model (Riordan et al., 2017) used as baseline in this paper, which gives the-state-of-the-art performance. But they ignore important evaluation criteria such as rubrics, which play a crucial role for evaluating answers in real-world situations. In this paper, we present a method to inject information from rubrics into SAG systems. We implement our approach on top of word-level attention mechanism to introduce the rubric information, in order to locate information in each answer that are highly related to the score. Our experimental results demonstrate that injecting rubric information effectively contributes to the perfo"
D19-6119,W15-0610,0,0.0151552,"them as features in a support vector regression (SVR) model. Ramachandran et al. (2015). Ramachandran et al. (2015) generates text patterns from top answers and rubrics, and reports the automatically generated pattern performances better than manually generated regex pattern. Nevertheless, it still remains an open issue (i) whether a rubric is effective or not even in the context of a neural representation learning paradigm (Riordan et al., 2017), and (ii) what kinds of neural architectures should be employed for the efficient use of rubrics. Another issue in SAG is on low-resource settings. Heilman and Madnani (2015) investigate the importance of the training data size on nonneural SAG models with discrete features. Horbach and Palmer (2016) show that active learning is effective for increasing useful training instances. This is orthogonal to our approach: combining active learning with our rubric-aware SAG model is an interesting future direction. Figure 2: The proposed rubric-aware SAG architecture, consisting of base component and rubric component. 3 3.1 Proposed model Overall architecture Figure 2 illustrates our proposed model, which consists of (i) base component and (ii) rubric component. We assume"
D19-6119,N15-1111,0,0.0180731,"answers. A wide variety of methods have been explored so far, ranging from Latent Semantic Analysis (LSA) (Mohler et al., 2011), edit distance-based similarity, and knowledgebased similarity using WordNet (Pedersen et al., 2004) (Magooda et al., 2016) to word embeddingbased similarity (Sultan et al., 2016). Recently, Riordan et al. (2017) report that neural networkbased feature representation learning (Taghipour and Ng, 2016) is effective for SAG. In contrast to the popularity of learning answer representations, the use of rubric information for SAG has been gained little attention so far. In Sakaguchi et al. (2015), the authors compute similarities, such as BLEU (Papineni et al., 2002), between an answer and each key element in a rubric, and use them as features in a support vector regression (SVR) model. Ramachandran et al. (2015). Ramachandran et al. (2015) generates text patterns from top answers and rubrics, and reports the automatically generated pattern performances better than manually generated regex pattern. Nevertheless, it still remains an open issue (i) whether a rubric is effective or not even in the context of a neural representation learning paradigm (Riordan et al., 2017), and (ii) what"
D19-6119,W16-0535,0,0.0220267,"ext patterns from top answers and rubrics, and reports the automatically generated pattern performances better than manually generated regex pattern. Nevertheless, it still remains an open issue (i) whether a rubric is effective or not even in the context of a neural representation learning paradigm (Riordan et al., 2017), and (ii) what kinds of neural architectures should be employed for the efficient use of rubrics. Another issue in SAG is on low-resource settings. Heilman and Madnani (2015) investigate the importance of the training data size on nonneural SAG models with discrete features. Horbach and Palmer (2016) show that active learning is effective for increasing useful training instances. This is orthogonal to our approach: combining active learning with our rubric-aware SAG model is an interesting future direction. Figure 2: The proposed rubric-aware SAG architecture, consisting of base component and rubric component. 3 3.1 Proposed model Overall architecture Figure 2 illustrates our proposed model, which consists of (i) base component and (ii) rubric component. We assume the base component encodes an answer into a feature vector f a . We also assume that a given rubric stipulates a set of key el"
D19-6119,N16-1123,0,0.155168,"t 2 Embedding ?? Encoder ??? … … … … Embedding ?? Encoder ??? Key element ? Related Work ?? ? Merge … 2 ?? Base Component Answer Concat Rubric Component A lot of existing SAG studies have a main interest in exploring better representations of answers and similarity measures between student answers and reference answers. A wide variety of methods have been explored so far, ranging from Latent Semantic Analysis (LSA) (Mohler et al., 2011), edit distance-based similarity, and knowledgebased similarity using WordNet (Pedersen et al., 2004) (Magooda et al., 2016) to word embeddingbased similarity (Sultan et al., 2016). Recently, Riordan et al. (2017) report that neural networkbased feature representation learning (Taghipour and Ng, 2016) is effective for SAG. In contrast to the popularity of learning answer representations, the use of rubric information for SAG has been gained little attention so far. In Sakaguchi et al. (2015), the authors compute similarities, such as BLEU (Papineni et al., 2002), between an answer and each key element in a rubric, and use them as features in a support vector regression (SVR) model. Ramachandran et al. (2015). Ramachandran et al. (2015) generates text patterns from top a"
D19-6610,P19-1093,0,0.0229163,"considered the best evidence piece (shown in blue). Introduction An argument is composed of two key components: claim and a supporting piece of evidence. Identification of these components and predicting the relationship among them forms the core of an important research area in NLP known as Argument Mining (Peldszus and Stede, 2013). Although claims can be identified with a promising level of accuracy in typical argumentative discourse (Eger et al., 2017; Stab et al., 2018), identification of a supporting evidence piece for a given claim (i.e., evidence detection) still remains a challenge (Gleize et al., 2019). Shown in Figure 1 is an example of a given topic and claim, and three evidence candidates from Wikipedia. In this example, identification of the best supporting piece of evidence is challenging, as all three evidence are related to the topic. Although all evidence candidates appear to be semantically similar to the claim, only E1 supports it, as it has an underlying, implicit link that can be established with the claim (i.e., children’s fundamental right shouldn’t be trumped by parental rights). Thus, for detecting the best piece of evidence for a claim, it is crucial to capture such implici"
D19-6610,D15-1038,0,0.0114702,"9 https://idebate.org/debatabase Each evidence piece can consists of more than one type. Number of warrants Importance αi None MQ (Anecdotal) MQ (Study) MQ (Expert) - 0.47 0.52 0.67 m=1 m = 1 (random) - 0.48 0.47 0.56 0.51 0.70 0.56 m=5 m=5 Equal Weighted 0.44 0.49 0.51 0.51 0.65 0.64 Table 1: Performance of evidence ranking. Results in bold indicate the best MQ score. different claim. In general, when we have N types of claims in one topic, the task is to rank N +1 candidate evidence consisting of one correct and N false evidence. As an evaluation measure, we report Mean Quantile (MQ) score (Guu et al., 2015) which gives a normalized version of Mean Reciprocal Rank. Specifically, for instance, we define the quantile of a correct piece of evidence k as the fraction of incorrect evidence ranked after k. MQ is defined to be the average quantile score over all instances in the dataset, with the quantile ranging from 0 to 1 (1 being optimal). Following Rinott et al. (2015), we use leaveone-out cross validation schema to evaluate our approach. For every topic, we train our model on instances in all other topics and then test the resulting model on the left out topic. Prior to our experiments, we exclude"
D19-6610,N18-1175,0,0.0955372,"example of a given topic and claim, and three evidence candidates from Wikipedia. In this example, identification of the best supporting piece of evidence is challenging, as all three evidence are related to the topic. Although all evidence candidates appear to be semantically similar to the claim, only E1 supports it, as it has an underlying, implicit link that can be established with the claim (i.e., children’s fundamental right shouldn’t be trumped by parental rights). Thus, for detecting the best piece of evidence for a claim, it is crucial to capture such implicit reasoning between them (Habernal et al., 2018). Existing approaches for evidence detection have often relied on lexical features extracted from argument components such as semantic similarity, adjacent sentence relation and discourse indicators (Stab and Gurevych, 2014; Rinott et al., 2015; Nguyen and Litman, 2016; Hua and Wang, 2017). However, no prior work has considered identifying the underlying, implicit reasoning, henceforth warrants (Toulmin, 2003), between a claim and a piece of evidence as a means for improving evidence detection. For example, if a model could establish a warrant between the claim and a piece of evidence (e.g., w"
D19-6610,W16-2815,0,0.0588777,"Missing"
D19-6610,P17-2032,0,0.0181889,"claim, only E1 supports it, as it has an underlying, implicit link that can be established with the claim (i.e., children’s fundamental right shouldn’t be trumped by parental rights). Thus, for detecting the best piece of evidence for a claim, it is crucial to capture such implicit reasoning between them (Habernal et al., 2018). Existing approaches for evidence detection have often relied on lexical features extracted from argument components such as semantic similarity, adjacent sentence relation and discourse indicators (Stab and Gurevych, 2014; Rinott et al., 2015; Nguyen and Litman, 2016; Hua and Wang, 2017). However, no prior work has considered identifying the underlying, implicit reasoning, henceforth warrants (Toulmin, 2003), between a claim and a piece of evidence as a means for improving evidence detection. For example, if a model could establish a warrant between the claim and a piece of evidence (e.g., warrant in Figure 1 for E1 ), the most plausible evidence piece could be detected. Towards filling this reasoning gap, Boltuˇzi´c and 57 Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER), pages 57–62 c Hong Kong, November 3, 2019. 2019 Association for Computatio"
D19-6610,D18-2029,0,0.0258558,"Missing"
D19-6610,P16-1200,0,0.0279929,"lso classified each evidence piece into the types anecdotal, study, and expert. In total, the test and training data consists of 3,057 distinct instances (anecdotal: 385, study: 1,020, and expert: 1,8962 ). Encoding warrants Given a set W of extracted warrants {w1 , w2 , ..., wn }, we first encode each warrant wi into a vector representation wi ∈ Rd in a similar manner to topic, claim, and a piece of evidence. Because the quality and relevance of extracted warrants may vary, we attentively aggregate sentence-level vector representations of all extracted warrants. We take a similar approach to Lin et al. (2016), which demonstrated the advantage of sentence level selective attention for multiple sentences, and take advantage of information present in multiple warrants. Specifically, the final vector representation v(W ) ∈ Rd is computed as a weighted sum over all warrant vectors: n X v(W ) = α i wi , (1) Pn Experiments Setting Evaluation protocol We evaluate our model in the task of evidence ranking (Rinott et al., 2015). Specifically, given a claim and candidate evidence, the task is to rank the candidates properly. For each instance in CDED, we extract one false piece of evidence from instances wit"
D19-6610,D14-1162,0,0.0821843,"esults also indicate that estimating the importance of each warrant is effective on the anecdotal type evidence. To see the importance of the quality of extracted warrants, we experimented with randomly extracted warrants from the database. The results (i.e. “m = 1 (random)”) show that the performance does not improve or degrade over the nonwarrant-aware model. This indicates that extracting relevant warrants is indeed crucial, and that our improvement is attributed to relevant warrants. Hyperparameters For both base and warrant components, we use pre-trained 100-dimensional GloVe embeddings (Pennington et al., 2014) to initialize the word embedding layer (g = 100). For the BiLSTM layer, we set h = 100 (i.e. d = 200) and apply dropout before the linear classifier with probability of 0.5. We optimize the categorical cross-entropy loss using Adagrad (Duchi et al., 2011) with a learning rate of 0.01 and the batch size of 32. We choose the model that performs best on the validation set. 3.3 α 3.4 Qualitative Analysis of Warrants To investigate the quality of the extracted warrants, two annotators (A1 , A2 ) experienced in the field of argumentation were asked to score 20 randomly sampled positive instances fo"
D19-6610,D15-1050,0,0.222685,"ates appear to be semantically similar to the claim, only E1 supports it, as it has an underlying, implicit link that can be established with the claim (i.e., children’s fundamental right shouldn’t be trumped by parental rights). Thus, for detecting the best piece of evidence for a claim, it is crucial to capture such implicit reasoning between them (Habernal et al., 2018). Existing approaches for evidence detection have often relied on lexical features extracted from argument components such as semantic similarity, adjacent sentence relation and discourse indicators (Stab and Gurevych, 2014; Rinott et al., 2015; Nguyen and Litman, 2016; Hua and Wang, 2017). However, no prior work has considered identifying the underlying, implicit reasoning, henceforth warrants (Toulmin, 2003), between a claim and a piece of evidence as a means for improving evidence detection. For example, if a model could establish a warrant between the claim and a piece of evidence (e.g., warrant in Figure 1 for E1 ), the most plausible evidence piece could be detected. Towards filling this reasoning gap, Boltuˇzi´c and 57 Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER), pages 57–62 c Hong Kong, Nov"
D19-6610,D14-1006,0,0.0311311,"hough all evidence candidates appear to be semantically similar to the claim, only E1 supports it, as it has an underlying, implicit link that can be established with the claim (i.e., children’s fundamental right shouldn’t be trumped by parental rights). Thus, for detecting the best piece of evidence for a claim, it is crucial to capture such implicit reasoning between them (Habernal et al., 2018). Existing approaches for evidence detection have often relied on lexical features extracted from argument components such as semantic similarity, adjacent sentence relation and discourse indicators (Stab and Gurevych, 2014; Rinott et al., 2015; Nguyen and Litman, 2016; Hua and Wang, 2017). However, no prior work has considered identifying the underlying, implicit reasoning, henceforth warrants (Toulmin, 2003), between a claim and a piece of evidence as a means for improving evidence detection. For example, if a model could establish a warrant between the claim and a piece of evidence (e.g., warrant in Figure 1 for E1 ), the most plausible evidence piece could be detected. Towards filling this reasoning gap, Boltuˇzi´c and 57 Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER), pages 5"
D19-6610,D18-1402,0,0.0157534,"right shouldn’t be trumped by parental rights. Figure 1: Three evidence candidates (E1 -E3 ) for a given topic and claim, where E1 can be considered the best evidence piece (shown in blue). Introduction An argument is composed of two key components: claim and a supporting piece of evidence. Identification of these components and predicting the relationship among them forms the core of an important research area in NLP known as Argument Mining (Peldszus and Stede, 2013). Although claims can be identified with a promising level of accuracy in typical argumentative discourse (Eger et al., 2017; Stab et al., 2018), identification of a supporting evidence piece for a given claim (i.e., evidence detection) still remains a challenge (Gleize et al., 2019). Shown in Figure 1 is an example of a given topic and claim, and three evidence candidates from Wikipedia. In this example, identification of the best supporting piece of evidence is challenging, as all three evidence are related to the topic. Although all evidence candidates appear to be semantically similar to the claim, only E1 supports it, as it has an underlying, implicit link that can be established with the claim (i.e., children’s fundamental right"
E17-1119,J92-4003,0,0.322738,"development data. 3.1 Sparse Feature Model For each entity mention m, we create a binary feature indicator vector f (m) ∈ {0, 1}Df and feed it to the logistic regression layer. The features used are described in Table 1, which are comparable to those used by Gillick et al. (2014) and Yogatama et al. (2015). It is worth noting that we aimed for this model to resemble the independent classifier model in Gillick et al. (2014) so that it constitutes as a meaningful well-established baseline; however, there are two noteworthy differences. Firstly, we use the more commonly used clustering method of Brown et al. (1992), as opposed to Uszkoreit and Brants (2008), as Gillick 1273 et al. (2014) did not make the data used for their clusters publicly available. Secondly, we learned a set of 15 topics from the OntoNotes dataset using the LDA (Blei et al., 2003) implementation from the popular gensim software package,1 in contrast to Gillick et al. (2014) that used a supervised topic model trained using an unspecified dataset. Despite these differences, we argue that our set of features is comparable and enables a fair comparison given that the original implementation and some of the data used is not publicly avai"
E17-1119,D15-1103,0,0.462101,"eir respective semantic types. Information regarding entity type mentions has proven to be valuable for several natural language processing tasks; such as question answering (Lee et al., 2006), knowledge base population (Carlson et al., 2010), and co-reference resolution (Recasens et al., 2013). A natural extension to traditional entity type classification has been to divide the set of types – which may be too coarsegrained for some applications (Sekine, 2008) – into a larger set of fine-grained entity types (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc. Given the recent successes of attentive neural ∗ This work was conducted during a research visit to University College London. 1271 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1271–1280, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics models for information extraction (Globerson et al., 2016; Shimaoka et al., 2016; Yang et al., 2016), we investigate several variants of an attentive neural model for the task of fine-graine"
E17-1119,P16-1059,0,0.0215587,"– into a larger set of fine-grained entity types (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc. Given the recent successes of attentive neural ∗ This work was conducted during a research visit to University College London. 1271 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1271–1280, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics models for information extraction (Globerson et al., 2016; Shimaoka et al., 2016; Yang et al., 2016), we investigate several variants of an attentive neural model for the task of fine-grained entity classification (e.g. Figure 1). This model category uses a neural attention mechanism – which can be likened to a soft alignment – that enables the model to focus on informative words and phrases. We build upon this line of research and our contributions are three-fold: 1. Despite being a natural comparison and addition, previous work on attentive neural architectures do not consider hand-crafted features. We combine learnt and hand-crafted features and"
E17-1119,Q15-1023,0,0.0194509,"n comparable set of features. What we find is that the performance drop is very dramatic, 9.85 points of loose micro score. Given that the training data for the previously introduced model is not publicly available, we hesi1277 (a) Figure 3: PCA projections of the label embeddings learnt from the OntoNotes dataset where subtypes share the same color as their parent type. Sub-figure (a) uses the non-hierarchical encoding, while sub-figure (b) uses the hierarchical encoding. tate to speculate as to exactly why this drop is so dramatic, but similar observations have been made for entity linking (Ling et al., 2015). This clearly underlines how essential it is to compare models on an equal footing using the same training data. PCA visualisation of label embeddings By visualising the learnt label embeddings (Figure 3) and comparing the non-hierarchical and hierarchical label encodings, we can observe that the hierarchical encoding forms clear distinct clusters. 4.7 Parent Before After Frequent Words /location /organization /art/film /music /award /event 0.319 0.324 0.207 0.259 0.583 0.310 0.228 0.178 0.429 0.116 0.292 0.188 0.070 0.119 0.021 0.018 0.083 0.089 in, at, born at, the, by film, films, in album"
E17-1119,P09-1113,0,0.0916087,"on. Their end goal was to use the resulting types in a question answering system and they developed a conditional random field model that they trained and evaluated on a manually annotated Korean dataset to detect and classify entity mentions. Other early work include Sekine (2008), that emphasised the need for having access to a large set of entity types for several NLP applications. The work primarily discussed design issues for fine-grained set of entity types and served as a basis for much of the future work on fine-grained entity classification. The first work to use distant supervision (Mintz et al., 2009) to induce a large – but noisy – training set and manually label a significantly smaller dataset to evaluate their fine-grained entity classification system, was Ling and Weld (2012) who introduced both a training and evaluation dataset F IGER (GOLD). Arguing that fine-grained sets of types must be organised in a very fine-grained hierarchical taxonomy, Yosef et al. (2012) introduced such a taxonomy covering 505 distinct types. This new set of types lead to improvements on F IGER (GOLD), and they also demonstrated that the fine-grained labels could be used as features to improve coarse-grained"
E17-1119,D14-1162,0,0.10334,"73.94 Attentive Attentive + Hand-crafted 54.53 59.68 74.76 78.97 71.58 75.36 F IGER (Ling and Weld, 2012) F IGER (Ren et al., 2016) 52.30 47.4 69.90 69.2 69.30 65.5 Table 3: Performance on F IGER (GOLD) for models using the same W2M training data. Pre-trained Word Embeddings We use pre-trained word embeddings that were not updated during training to help the model generalise to words not appearing in the training set (Rockt¨aschel et al., 2015). For this purpose, we used the freely available 300-dimensional cased word embeddings trained on 840 billion tokens from the Common Crawl supplied by Pennington et al. (2014). For words not present in the pretrained word embeddings, we use the embedding of the “unk” token. Model Model Data Acc. Macro Micro Attentive + Hand-crafted Attentive (Shimaoka et al., 2016) W2M W2.6M 59.68 58.97 78.97 77.96 75.36 74.94 F IGER + PLE (Ren et al., 2016) HYENA + PLE (Ren et al., 2016) W2M+D W2M+D 59.9 54.2 76.3 69.5 74.9 68.1 K-WASABIE (Yogatama et al., 2015) GN2 n/a n/a 72.25 Table 4: Performance on F IGER (GOLD) for models using different training data. used dropout (Hinton et al., 2012) with probability 0.5 applied to the mention representation and sparse feature representat"
E17-1119,N13-1071,0,0.0177729,"Missing"
E17-1119,sekine-2008-extended,0,0.0444578,"match series against New Zealand is held on Monday”. 1 Introduction Entity type classification aims to label entity mentions in their context with their respective semantic types. Information regarding entity type mentions has proven to be valuable for several natural language processing tasks; such as question answering (Lee et al., 2006), knowledge base population (Carlson et al., 2010), and co-reference resolution (Recasens et al., 2013). A natural extension to traditional entity type classification has been to divide the set of types – which may be too coarsegrained for some applications (Sekine, 2008) – into a larger set of fine-grained entity types (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc. Given the recent successes of attentive neural ∗ This work was conducted during a research visit to University College London. 1271 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1271–1280, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics models for information extraction ("
E17-1119,W16-1313,1,0.661596,"fine-grained entity types (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc. Given the recent successes of attentive neural ∗ This work was conducted during a research visit to University College London. 1271 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1271–1280, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics models for information extraction (Globerson et al., 2016; Shimaoka et al., 2016; Yang et al., 2016), we investigate several variants of an attentive neural model for the task of fine-grained entity classification (e.g. Figure 1). This model category uses a neural attention mechanism – which can be likened to a soft alignment – that enables the model to focus on informative words and phrases. We build upon this line of research and our contributions are three-fold: 1. Despite being a natural comparison and addition, previous work on attentive neural architectures do not consider hand-crafted features. We combine learnt and hand-crafted features and observe that they compl"
E17-1119,P08-1086,0,0.0123156,"e Model For each entity mention m, we create a binary feature indicator vector f (m) ∈ {0, 1}Df and feed it to the logistic regression layer. The features used are described in Table 1, which are comparable to those used by Gillick et al. (2014) and Yogatama et al. (2015). It is worth noting that we aimed for this model to resemble the independent classifier model in Gillick et al. (2014) so that it constitutes as a meaningful well-established baseline; however, there are two noteworthy differences. Firstly, we use the more commonly used clustering method of Brown et al. (1992), as opposed to Uszkoreit and Brants (2008), as Gillick 1273 et al. (2014) did not make the data used for their clusters publicly available. Secondly, we learned a set of 15 topics from the OntoNotes dataset using the LDA (Blei et al., 2003) implementation from the popular gensim software package,1 in contrast to Gillick et al. (2014) that used a supervised topic model trained using an unspecified dataset. Despite these differences, we argue that our set of features is comparable and enables a fair comparison given that the original implementation and some of the data used is not publicly available. corresponding word embeddings. Those"
E17-1119,N16-1174,0,0.00639303,"es (Lee et al., 2006; Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014; Del Corro et al., 2015); for example person into actor, artist, etc. Given the recent successes of attentive neural ∗ This work was conducted during a research visit to University College London. 1271 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1271–1280, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics models for information extraction (Globerson et al., 2016; Shimaoka et al., 2016; Yang et al., 2016), we investigate several variants of an attentive neural model for the task of fine-grained entity classification (e.g. Figure 1). This model category uses a neural attention mechanism – which can be likened to a soft alignment – that enables the model to focus on informative words and phrases. We build upon this line of research and our contributions are three-fold: 1. Despite being a natural comparison and addition, previous work on attentive neural architectures do not consider hand-crafted features. We combine learnt and hand-crafted features and observe that they complement each other. Ad"
E17-1119,P15-2048,0,0.59274,"xt-dependent fine-grained entity type classification where the types of a mention is constrained to what can be deduced from its context and introduced a new OntoNotes-derived manually annotated evaluation dataset. In addition, they addressed the problem of label noise induced by distant supervision and proposed three label cleaning heuristics. Building upon the noise reduction aspects of this work, Ren et al. (2016) introduced a method to reduce label noise even further, leading to significant performance gains on both the evaluation dataset of Ling and Weld (2012) and Gillick et al. (2014). Yogatama et al. (2015) proposed to map handcrafted features and labels to embeddings in or1272 der to facilitate information sharing between both related types and features. A pure feature learning approach was proposed by Dong et al. (2015). They defined 22 types and used a two-part neural classifier that used a recurrent neural network to obtain a vector representation of each entity mention and in its second part used a fixed-size window to capture the context of a mention. A recent workshop paper (Shimaoka et al., 2016) introduced an attentive neural model that unlike previous work obtained vector representatio"
E17-1119,C12-2133,0,0.11954,"Missing"
I05-1079,P98-1013,0,0.0126582,"phrasing has recently been attracting increasing attention due to its potential in a broad range of natural language processing tasks. For example, a system that is capable of simplifying a given text, or showing the user several alternative expressions conveying the same content, would be useful for assisting a reader. There are several classes of paraphrase that exhibit a degree of regularity. For example, paraphrasing associated with verb alternation, lexical derivation, compound word decomposition, and paraphrasing of light-verb constructions (LVC(s)) all fall into such classes. Examples1 (1) and (2) appear to exhibit the same transformation pattern, in which a compound noun is transformed into a verb phrase. Likewise, paraphrases involving an LVC as in (3) and (4) (from [4]) have considerable similarities. (1) s. t. (2) s. t. (3) s. t. (4) s. t. 1 His machine operation is very good. He operates the machine very well. My son’s bat control is unskillful yet. My son controls his bat poorly yet. Steven made an attempt to stop playing. Steven attempted to stop playing. It had a noticeable effect on the trade. It noticeably affected the trade. For each example, “s” and “t” denote an or"
I05-1079,W04-2412,0,0.0472235,"has recently been attracting increasing attention due to its potential in a broad range of natural language processing tasks. For example, a system that is capable of simplifying a given text, or showing the user several alternative expressions conveying the same content, would be useful for assisting a reader. There are several classes of paraphrase that exhibit a degree of regularity. For example, paraphrasing associated with verb alternation, lexical derivation, compound word decomposition, and paraphrasing of light-verb constructions (LVC(s)) all fall into such classes. Examples1 (1) and (2) appear to exhibit the same transformation pattern, in which a compound noun is transformed into a verb phrase. Likewise, paraphrases involving an LVC as in (3) and (4) (from [4]) have considerable similarities. (1) s. t. (2) s. t. (3) s. t. (4) s. t. 1 His machine operation is very good. He operates the machine very well. My son’s bat control is unskillful yet. My son controls his bat poorly yet. Steven made an attempt to stop playing. Steven attempted to stop playing. It had a noticeable effect on the trade. It noticeably affected the trade. For each example, “s” and “t” denote an original s"
I05-1079,J02-3001,0,0.122308,"Missing"
I05-1079,J94-3013,0,0.0841281,"19] have shown that the theory of the LCS provides a systematic explanation of semantic decomposition as well as syntax determines. In particular, Kageyama [9] has shown that even a simple typology of LCS can explain a wide variety of linguistic phenomena including word association within compounds, transitivity alternation, and lexical derivation. Second, large-scale LCS dictionaries have been developed through practical use on machine translation and compound noun analysis [3,19]. The LCS dictionary for English [3] (4,163-verbs with 468 LCS types) was tailored based on a verb classification [12] with an expansion for the semantic role delivered to arguments. For Japanese, Takeuchi et al. [19] developed a 1,210-verbs LCS dictionary (with 12 LCS types) called the T-LCS dictionary, following Kageyama’s analysis [9]. In this paper, we make use of the current version of the T-LCS dictionary, because it provides a set of concrete rules for LCS assignment, which ensures the reliability of the dictionary. Examples of LCS in the T-LCS dictionary are shown in Table 1. An LCS consists of a combination of semantic predicates (“CONTROL,” “BE AT,” etc.) and their argument slots (x, y, and z). Each"
I05-1079,J87-3006,0,0.040094,"Missing"
I05-1079,J05-1004,0,0.0407037,"Missing"
I05-1079,W02-1409,0,0.0544453,"ularities underlying paraphrases can be explained by means of lexical semantics and how, and (ii) how lexical semantics theories can be enhanced with feedback from practical use, namely, paraphrase generation. We make an attempt to exploit the LCS among several lexical semantics frameworks, and propose a paraphrase generation model which utilizes LCS combining with syntactic transformation. 2 Lexical Conceptual Structure 2.1 Basic Framework Among several frameworks of lexical semantics, we focus on the Lexical Conceptual Structure (LCS) [8] due to the following reasons. First, several studies [9,3,19] have shown that the theory of the LCS provides a systematic explanation of semantic decomposition as well as syntax determines. In particular, Kageyama [9] has shown that even a simple typology of LCS can explain a wide variety of linguistic phenomena including word association within compounds, transitivity alternation, and lexical derivation. Second, large-scale LCS dictionaries have been developed through practical use on machine translation and compound noun analysis [3,19]. The LCS dictionary for English [3] (4,163-verbs with 468 LCS types) was tailored based on a verb classification [12"
I05-1079,C98-1013,0,\N,Missing
I05-1079,W05-0620,0,\N,Missing
I05-2030,J95-2003,0,0.00340212,"004), is cost effective, while achieving a better performance than the best-performing rule-based systems for the test sets of MUC-6 and MUC-7 2 . As suggested by Figure 1, anaphora resolution can be decomposed into two subtasks: anaphoricity determination and antecedent identification. Anaphoricity determination is the task of judging whether a given NP is anaphoric or nonanaphoric. Recent research advances have provided several important findings as follows: • Learning-based methods for antecedent identification can also benefit from the use of linguistic clues inspired by Centering Theory (Grosz et al., 1995). • One useful clue for anaphoricity determination is the availability of a plausible candidate for the antecedent. If an appropriate candidate for the antecedent is found in the preceding discourse context, the NP is likely to be anaphoric. For these reasons, an anaphora resolution model performs best if it carries out the following pro2 The 7th Message Understanding Conference (1998): www.itl.nist.gov/iaui/894.02/related projects/muc/ 175 target value Select the best candidate attribute candidates Taro-wa shisetsu-wo（φ-ga）shirabe-te Onaka-ga hetta-node anaphor hungry Tarō-NOM attendance-ACC"
I05-2030,W03-2604,1,0.908937,"indefinite. While the figure shows Japanese examples, the similarity between anaphora resolution and opinion extraction is language independent. This analogy naturally leads us to think of applying existing techniques for anaphora resolution to our opinion extraction task since anaphora resolution has been studied for a considerably longer period in a wider range of disciplines as we briefly review below. 3.2 Existing techniques for anaphora resolution Corpus-based empirical approaches to anaphora resolution have been reasonably successful. This approach, as exemplified by (Soon et al., 2001; Iida et al., 2003; Ng, 2004), is cost effective, while achieving a better performance than the best-performing rule-based systems for the test sets of MUC-6 and MUC-7 2 . As suggested by Figure 1, anaphora resolution can be decomposed into two subtasks: anaphoricity determination and antecedent identification. Anaphoricity determination is the task of judging whether a given NP is anaphoric or nonanaphoric. Recent research advances have provided several important findings as follows: • Learning-based methods for antecedent identification can also benefit from the use of linguistic clues inspired by Centering T"
I05-2030,C04-1071,0,0.0255642,"nd recommendations. Previous approaches to the task of mining a large-scale document collection for opinions can be classified into two groups: the document classification approach and the information extraction approach. In the document classification approach, researchers have been exploring techniques for classifying documents according to semantic/sentiment orientation such as positive vs. negative (e.g. (Dave et al., 2003; Pang and Lee, 2004; Turney, 2002)). The information extraction approach, on the other hand, focuses on the task of extracting elements which constitute opinions (e.g. (Kanayama and Nasukawa, 2004; Hu and Liu, 2004; Tateishi et al., 2001)). The aim of this paper is to extract opinions that represent an evaluation of a products together with the evidence. To achieve this, we consider our task from the information extraction view173 point. We term the above task opinion extraction in this paper. While they can be linguistically realized in many ways, opinions on a product are in fact often expressed in the form of an attribute-value pair. An attribute represents one aspect of a subject and the value is a specific language expression that qualifies or quantifies the aspect. Given this obs"
I05-2030,P04-1020,0,0.0110102,"he figure shows Japanese examples, the similarity between anaphora resolution and opinion extraction is language independent. This analogy naturally leads us to think of applying existing techniques for anaphora resolution to our opinion extraction task since anaphora resolution has been studied for a considerably longer period in a wider range of disciplines as we briefly review below. 3.2 Existing techniques for anaphora resolution Corpus-based empirical approaches to anaphora resolution have been reasonably successful. This approach, as exemplified by (Soon et al., 2001; Iida et al., 2003; Ng, 2004), is cost effective, while achieving a better performance than the best-performing rule-based systems for the test sets of MUC-6 and MUC-7 2 . As suggested by Figure 1, anaphora resolution can be decomposed into two subtasks: anaphoricity determination and antecedent identification. Anaphoricity determination is the task of judging whether a given NP is anaphoric or nonanaphoric. Recent research advances have provided several important findings as follows: • Learning-based methods for antecedent identification can also benefit from the use of linguistic clues inspired by Centering Theory (Gros"
I05-2030,P04-1035,0,0.0333573,"ead of communication on the Web has attracted increasing interest in technologies for automatically mining large numbers of message boards and blog pages for opinions and recommendations. Previous approaches to the task of mining a large-scale document collection for opinions can be classified into two groups: the document classification approach and the information extraction approach. In the document classification approach, researchers have been exploring techniques for classifying documents according to semantic/sentiment orientation such as positive vs. negative (e.g. (Dave et al., 2003; Pang and Lee, 2004; Turney, 2002)). The information extraction approach, on the other hand, focuses on the task of extracting elements which constitute opinions (e.g. (Kanayama and Nasukawa, 2004; Hu and Liu, 2004; Tateishi et al., 2001)). The aim of this paper is to extract opinions that represent an evaluation of a products together with the evidence. To achieve this, we consider our task from the information extraction view173 point. We term the above task opinion extraction in this paper. While they can be linguistically realized in many ways, opinions on a product are in fact often expressed in the form of"
I05-2030,J01-4004,0,0.372641,"licit referent) or indefinite. While the figure shows Japanese examples, the similarity between anaphora resolution and opinion extraction is language independent. This analogy naturally leads us to think of applying existing techniques for anaphora resolution to our opinion extraction task since anaphora resolution has been studied for a considerably longer period in a wider range of disciplines as we briefly review below. 3.2 Existing techniques for anaphora resolution Corpus-based empirical approaches to anaphora resolution have been reasonably successful. This approach, as exemplified by (Soon et al., 2001; Iida et al., 2003; Ng, 2004), is cost effective, while achieving a better performance than the best-performing rule-based systems for the test sets of MUC-6 and MUC-7 2 . As suggested by Figure 1, anaphora resolution can be decomposed into two subtasks: anaphoricity determination and antecedent identification. Anaphoricity determination is the task of judging whether a given NP is anaphoric or nonanaphoric. Recent research advances have provided several important findings as follows: • Learning-based methods for antecedent identification can also benefit from the use of linguistic clues insp"
I05-2030,P02-1053,0,0.0547136,"on the Web has attracted increasing interest in technologies for automatically mining large numbers of message boards and blog pages for opinions and recommendations. Previous approaches to the task of mining a large-scale document collection for opinions can be classified into two groups: the document classification approach and the information extraction approach. In the document classification approach, researchers have been exploring techniques for classifying documents according to semantic/sentiment orientation such as positive vs. negative (e.g. (Dave et al., 2003; Pang and Lee, 2004; Turney, 2002)). The information extraction approach, on the other hand, focuses on the task of extracting elements which constitute opinions (e.g. (Kanayama and Nasukawa, 2004; Hu and Liu, 2004; Tateishi et al., 2001)). The aim of this paper is to extract opinions that represent an evaluation of a products together with the evidence. To achieve this, we consider our task from the information extraction view173 point. We term the above task opinion extraction in this paper. While they can be linguistically realized in many ways, opinions on a product are in fact often expressed in the form of an attribute-v"
I05-5004,N01-1009,0,0.0295265,"neration and recognition has drawn the attention of an increasing number of researchers because of its potential contribution to a broad range of natural language applications. Paraphrases can be viewed as monolingual translations. From this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (MT), such as transformation algorithms (Lavoie et al., 2000; Takahashi et al., 2001), corpus-based techniques for paraphrase pattern acquisition (Barzilay and McKeown, 2001; Shinyama and Sekine, 2003; Quirk et al., 2004), and fluency measurements (Lapata, 2001; Fujita et al., 2004). One thing the paraphrasing community is still lacking is shared collections of paraphrase ex• We define a set of paraphrase classes based on the syntactic features of transformation patterns. • We separately collect paraphrase examples for each paraphrase class that are considered to be linguistically explainable. • We use a paraphrase generation system to exhaustively collect candidate paraphrases from a given text collection, which are then manually labeled. 1 5801 sentence pairs from their comparable corpus have been judged manually and available from http://research"
I05-5004,A00-1009,0,0.0121454,"results, which we have evaluated according to cost-efficiency, exhaustiveness, and reliability. 1 Introduction Paraphrases are alternative ways of conveying the same content. The technology for paraphrase generation and recognition has drawn the attention of an increasing number of researchers because of its potential contribution to a broad range of natural language applications. Paraphrases can be viewed as monolingual translations. From this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (MT), such as transformation algorithms (Lavoie et al., 2000; Takahashi et al., 2001), corpus-based techniques for paraphrase pattern acquisition (Barzilay and McKeown, 2001; Shinyama and Sekine, 2003; Quirk et al., 2004), and fluency measurements (Lapata, 2001; Fujita et al., 2004). One thing the paraphrasing community is still lacking is shared collections of paraphrase ex• We define a set of paraphrase classes based on the syntactic features of transformation patterns. • We separately collect paraphrase examples for each paraphrase class that are considered to be linguistically explainable. • We use a paraphrase generation system to exhaustively col"
I05-5004,J87-3006,0,0.0133468,"Missing"
I05-5004,W04-3219,0,0.123497,"same content. The technology for paraphrase generation and recognition has drawn the attention of an increasing number of researchers because of its potential contribution to a broad range of natural language applications. Paraphrases can be viewed as monolingual translations. From this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (MT), such as transformation algorithms (Lavoie et al., 2000; Takahashi et al., 2001), corpus-based techniques for paraphrase pattern acquisition (Barzilay and McKeown, 2001; Shinyama and Sekine, 2003; Quirk et al., 2004), and fluency measurements (Lapata, 2001; Fujita et al., 2004). One thing the paraphrasing community is still lacking is shared collections of paraphrase ex• We define a set of paraphrase classes based on the syntactic features of transformation patterns. • We separately collect paraphrase examples for each paraphrase class that are considered to be linguistically explainable. • We use a paraphrase generation system to exhaustively collect candidate paraphrases from a given text collection, which are then manually labeled. 1 5801 sentence pairs from their comparable corpus have been judged man"
I05-5004,W03-1609,0,0.101351,"ative ways of conveying the same content. The technology for paraphrase generation and recognition has drawn the attention of an increasing number of researchers because of its potential contribution to a broad range of natural language applications. Paraphrases can be viewed as monolingual translations. From this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (MT), such as transformation algorithms (Lavoie et al., 2000; Takahashi et al., 2001), corpus-based techniques for paraphrase pattern acquisition (Barzilay and McKeown, 2001; Shinyama and Sekine, 2003; Quirk et al., 2004), and fluency measurements (Lapata, 2001; Fujita et al., 2004). One thing the paraphrasing community is still lacking is shared collections of paraphrase ex• We define a set of paraphrase classes based on the syntactic features of transformation patterns. • We separately collect paraphrase examples for each paraphrase class that are considered to be linguistically explainable. • We use a paraphrase generation system to exhaustively collect candidate paraphrases from a given text collection, which are then manually labeled. 1 5801 sentence pairs from their comparable corpus"
I05-5004,P01-1008,0,0.0653514,"ction Paraphrases are alternative ways of conveying the same content. The technology for paraphrase generation and recognition has drawn the attention of an increasing number of researchers because of its potential contribution to a broad range of natural language applications. Paraphrases can be viewed as monolingual translations. From this viewpoint, research on paraphrasing has adapted techniques fostered in the literature of machine translation (MT), such as transformation algorithms (Lavoie et al., 2000; Takahashi et al., 2001), corpus-based techniques for paraphrase pattern acquisition (Barzilay and McKeown, 2001; Shinyama and Sekine, 2003; Quirk et al., 2004), and fluency measurements (Lapata, 2001; Fujita et al., 2004). One thing the paraphrasing community is still lacking is shared collections of paraphrase ex• We define a set of paraphrase classes based on the syntactic features of transformation patterns. • We separately collect paraphrase examples for each paraphrase class that are considered to be linguistically explainable. • We use a paraphrase generation system to exhaustively collect candidate paraphrases from a given text collection, which are then manually labeled. 1 5801 sentence pairs f"
I05-5004,N03-1003,0,0.0642753,"f judgement criteria. We use morpho-syntactic paraphrasing patterns derived from paraphrase samples in an analogous way to previous methods such as (Dras, 1999). For instance, from example (1), we derive a paraphrasing pattern for paraphrasing of light-verb constructions: (6) s. N -o(⇒V ) V 3.2 Automatic paraphrase acquisition Recently, paraphrase examples have been automatically collected as a source of acquiring paraphrase knowledge, such as pairs of synonymous phrases and syntactic transformation templates. Some studies exploit topically related articles derived from multiple news sources (Barzilay and Lee, 2003; Shinyama and Sekine, 2003; Quirk et al., 2004; Dolan et al., 2004). Sentence pairs that are likely to be paraphrases are automatically collected from the parallel or comparable corpora, using such clues as overlaps of content words and named entities, syntactic similarity, and reference description, such as date of the article and positions of sentences in the articles. N -ACC V t. V (N ) V (N ) where N is a variable which matches with a noun, V a verb, V (N ) denotes the verbalized form of 27 (c) second opinion (correct / incorrect) (a) source sentence Given Obligatory (b) automatically gen"
I05-5004,C04-1051,0,0.0259555,"ived from paraphrase samples in an analogous way to previous methods such as (Dras, 1999). For instance, from example (1), we derive a paraphrasing pattern for paraphrasing of light-verb constructions: (6) s. N -o(⇒V ) V 3.2 Automatic paraphrase acquisition Recently, paraphrase examples have been automatically collected as a source of acquiring paraphrase knowledge, such as pairs of synonymous phrases and syntactic transformation templates. Some studies exploit topically related articles derived from multiple news sources (Barzilay and Lee, 2003; Shinyama and Sekine, 2003; Quirk et al., 2004; Dolan et al., 2004). Sentence pairs that are likely to be paraphrases are automatically collected from the parallel or comparable corpora, using such clues as overlaps of content words and named entities, syntactic similarity, and reference description, such as date of the article and positions of sentences in the articles. N -ACC V t. V (N ) V (N ) where N is a variable which matches with a noun, V a verb, V (N ) denotes the verbalized form of 27 (c) second opinion (correct / incorrect) (a) source sentence Given Obligatory (b) automatically generated paraphrase (c) annotator’s judge (correct / incorrect) (f) fr"
I08-1065,I05-1069,0,0.113061,"al (and entailment) relation holds between the verb phrases wash something and something is clean, which reflects the commonsense notion that if someone has washed something, this object is clean as a result of the washing event. A crucial issue is how to obtain and maintain a potentially huge collection of such event relations instances. Motivated by this background, several research groups have reported their experiments on automatic 497 acquisition of causal, temporal and entailment relations between event mentions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006, etc.). The common idea behind them is to use a small number of manually selected generic lexico-syntactic cooccurrence patterns (LSPs or simply patterns). to Verb-X and then Verb-Y, for example, is used to obtain temporal relations such as marry and divorce (Chklovski and Pantel, 2005). The use of such generic patterns, however, tends to be high recall but low precision, which requires an additional component for pruning extracted relations. This issue has been addressed in basically two approaches, either by devising heuristic statistical"
I08-1065,P06-1079,1,0.905366,"Missing"
I08-1065,N06-1023,0,0.0642798,"Missing"
I08-1065,W02-2016,1,0.673894,"Missing"
I08-1065,P06-1015,0,0.508012,"ever, tends to be high recall but low precision, which requires an additional component for pruning extracted relations. This issue has been addressed in basically two approaches, either by devising heuristic statistical scores (Chklovski and Pantel, 2005; Torisawa, 2006; Zanzotto et al., 2006) or training classifiers for disambiguation with heavy supervision (Inui et al., 2003). This paper explores a third way for enhancing present LSP-based methods for event relation acquisition. The basic idea is inspired by the following recent findings in relation extraction (Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006, etc.), which aims at extracting semantic relations between entities (as opposed to events) from texts. (a) The use of generic patterns tends to be high recall but low precision, which requires an additional component for pruning. (b) On the other hand, there are specific patterns that are highly reliable but they are much less frequent than generic patterns and each makes only a small contribution to recall. (c) Combining a few generic patters with a much larger collection of reliable specific patterns boosts both precision and recall. Such specific patterns can be acquired from a very large"
I08-1065,N06-1007,0,0.131442,"he verb phrases wash something and something is clean, which reflects the commonsense notion that if someone has washed something, this object is clean as a result of the washing event. A crucial issue is how to obtain and maintain a potentially huge collection of such event relations instances. Motivated by this background, several research groups have reported their experiments on automatic 497 acquisition of causal, temporal and entailment relations between event mentions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006, etc.). The common idea behind them is to use a small number of manually selected generic lexico-syntactic cooccurrence patterns (LSPs or simply patterns). to Verb-X and then Verb-Y, for example, is used to obtain temporal relations such as marry and divorce (Chklovski and Pantel, 2005). The use of such generic patterns, however, tends to be high recall but low precision, which requires an additional component for pruning extracted relations. This issue has been addressed in basically two approaches, either by devising heuristic statistical scores (Chklovski and Pantel,"
I08-1065,P02-1006,0,0.164278,"of such generic patterns, however, tends to be high recall but low precision, which requires an additional component for pruning extracted relations. This issue has been addressed in basically two approaches, either by devising heuristic statistical scores (Chklovski and Pantel, 2005; Torisawa, 2006; Zanzotto et al., 2006) or training classifiers for disambiguation with heavy supervision (Inui et al., 2003). This paper explores a third way for enhancing present LSP-based methods for event relation acquisition. The basic idea is inspired by the following recent findings in relation extraction (Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006, etc.), which aims at extracting semantic relations between entities (as opposed to events) from texts. (a) The use of generic patterns tends to be high recall but low precision, which requires an additional component for pruning. (b) On the other hand, there are specific patterns that are highly reliable but they are much less frequent than generic patterns and each makes only a small contribution to recall. (c) Combining a few generic patters with a much larger collection of reliable specific patterns boosts both precision and recall. Such specific patterns c"
I08-1065,N06-1008,0,0.149248,"holds between the verb phrases wash something and something is clean, which reflects the commonsense notion that if someone has washed something, this object is clean as a result of the washing event. A crucial issue is how to obtain and maintain a potentially huge collection of such event relations instances. Motivated by this background, several research groups have reported their experiments on automatic 497 acquisition of causal, temporal and entailment relations between event mentions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006, etc.). The common idea behind them is to use a small number of manually selected generic lexico-syntactic cooccurrence patterns (LSPs or simply patterns). to Verb-X and then Verb-Y, for example, is used to obtain temporal relations such as marry and divorce (Chklovski and Pantel, 2005). The use of such generic patterns, however, tends to be high recall but low precision, which requires an additional component for pruning extracted relations. This issue has been addressed in basically two approaches, either by devising heuristic statistical scores (Chklovsk"
I08-1065,P06-1107,0,0.505661,"es wash something and something is clean, which reflects the commonsense notion that if someone has washed something, this object is clean as a result of the washing event. A crucial issue is how to obtain and maintain a potentially huge collection of such event relations instances. Motivated by this background, several research groups have reported their experiments on automatic 497 acquisition of causal, temporal and entailment relations between event mentions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006, etc.). The common idea behind them is to use a small number of manually selected generic lexico-syntactic cooccurrence patterns (LSPs or simply patterns). to Verb-X and then Verb-Y, for example, is used to obtain temporal relations such as marry and divorce (Chklovski and Pantel, 2005). The use of such generic patterns, however, tends to be high recall but low precision, which requires an additional component for pruning extracted relations. This issue has been addressed in basically two approaches, either by devising heuristic statistical scores (Chklovski and Pantel, 2005; Torisawa, 2006;"
I13-1067,W06-3907,0,0.0212641,"ural Language Processing, pages 587–595, Nagoya, Japan, 14-18 October 2013. Inui et al. (2008) have proposed a method of analyzing modality and polarity of event mentions in Japanese text with an approach based on conditional random field. However, it is very difficult that their machine learning-based models precisely identify the minority classes. There are also approaches based on rules. MacCartney and Manning (2009) have proposed a model of natural logic, which has focused on semantic containment and monotonicity. They also infers implicatives and factives based on implication signatures (Nairn et al., 2006) compositionally. But certainty is not considered in their approach. Saur´ı and Pustejovsky (2012) have proposed a rule-based method using information that can influence the factuality of events such as polarity particles, modality markers, and epistemic predicates. In their algorithm, factuality values of the event, consisting of certainty and polarity, are determined by the upper factuality values and rules, one by one, from the top of the dependency tree. Their model is suitable as it assumes the availability of a factuality lexicon and uses it to identify the factuality status of each subo"
I13-1067,P11-2049,0,0.0143963,"This means that our approach based on lexical knowledge works well, especially for minor labels. However, some errors still remain. 5 Lexicon-based scope detection In the previous section, we found that detecting a scope is very crucial. In this section, we investigate the limitation of the lexical knowledge for a scope and identify the technical research issues more precisely through experiments for rule-based scope detection. 5.1 Related work for scope detection In recent years, the detection of negation and speculation scopes is intensively being research for English (Szarvas et al., 2008; Apostolova et al., 2011), such as Shared Task 5.4 Remaining issues We identify the remaining issues through the error analysis of the result. We focus on the 593 Table 11: Expressions to prevent propagating a contextual factuality Category A B C Expressions ∼ながら (-nagara), ∼つつ (-tsutsu), ∼て (-te), ∼で (-de) ∼て (-te), ∼と (-to), ∼ながら (-nagara), ∼ので (-no-de), ∼のに (-no-ni), ∼ば (-ba), ∼たら (-tara), ∼なら (-nara), ∼ても (-te-mo), ∼て (-te), ∼ず (-zu), ∼ずに (-zu-ni), ∼ないで (-nai-de) ∼が (-ga), ∼から (-kara), ∼けれど (-keredo), ∼けれども (-keredo-mo), ∼けども (-kedo-mo), ∼けど (-kedo), ∼し (-shi), ∼て (-te) Table 12: Performance with/without lexical k"
I13-1067,J12-2003,0,0.324831,"Missing"
I13-1067,J12-2002,0,0.319501,"Missing"
I13-1067,W08-0606,0,0.573167,"e and Blanco, 2012; Saur´ı and Pustejovsky, 2012). For Japanese, Matsuyoshi et al. (2010) report that their factuality classes are highly skewed and the minority classes are very difficult for their machine learning-based models to precisely identify. The minority classes include uncertain statements as in example (1b) and counterfactual statements as in (1c). Such “marked” statements are far less frequent than unmarked statements (i.e. certain factual statements) and thus are not as easy to collect as unmarked statements. While the label distribution is reported to be less skewed in English (Szarvas et al., 2008), still uncertain and counterfactual statements constitute minority classes. In addition, uncertain and counterfactual statements exhibit a very broad variety of linguistic devices for expressing uncertainty and negation. For those reasons, the whole task is not as easy as it appears and simple strategies based on supervised machine learning do not work well. Given this background, rather than putting everything simply into a machine learning algorithm, it is now important to take a closer look at the linguistics phenomena involved in factuality analysis and identify the technical research iss"
I13-1067,W10-3001,0,0.0348308,"Missing"
I13-1067,W09-3714,0,0.0155265,"in the real world, the event “kaet” (go home) in (1b) is possibly factual because of the modal auxiliary “-ta-no-darou” (may have -ed), and the event “hassei-suru” 587 International Joint Conference on Natural Language Processing, pages 587–595, Nagoya, Japan, 14-18 October 2013. Inui et al. (2008) have proposed a method of analyzing modality and polarity of event mentions in Japanese text with an approach based on conditional random field. However, it is very difficult that their machine learning-based models precisely identify the minority classes. There are also approaches based on rules. MacCartney and Manning (2009) have proposed a model of natural logic, which has focused on semantic containment and monotonicity. They also infers implicatives and factives based on implication signatures (Nairn et al., 2006) compositionally. But certainty is not considered in their approach. Saur´ı and Pustejovsky (2012) have proposed a rule-based method using information that can influence the factuality of events such as polarity particles, modality markers, and epistemic predicates. In their algorithm, factuality values of the event, consisting of certainty and polarity, are determined by the upper factuality values a"
I13-1067,matsuyoshi-etal-2010-annotating,1,0.552258,"es, Tohoku University, Japan † National Institute of Information and Communications Technology (NICT), Japan {narita, junta-m, inui}@ecei.tohoku.ac.jp Abstract (occurrence) in (1c) is counterfactual because of the implicative predicate “fusei-da” (prevented). Factuality analysis is useful for a broad range of NLP applications such as information extraction, question answering, and textual entailment recognition. Prior work on factuality analysis has made considerable efforts for designing and creating corpora manually annotated with factualityrelated information (Saur´ı and Pustejovsky, 2009; Matsuyoshi et al., 2010; Tanaka et al., 2013, etc.) and several empirical studies on those resources are reported revealing the difficulties of the task (Inui et al., 2008; Matsuyoshi et al., 2010; Morante and Blanco, 2012; Saur´ı and Pustejovsky, 2012). For Japanese, Matsuyoshi et al. (2010) report that their factuality classes are highly skewed and the minority classes are very difficult for their machine learning-based models to precisely identify. The minority classes include uncertain statements as in example (1b) and counterfactual statements as in (1c). Such “marked” statements are far less frequent than unma"
I13-1067,S12-1035,0,\N,Missing
I13-2008,P13-1170,1,0.776241,"Missing"
I13-2008,I13-2012,1,\N,Missing
I17-1048,D16-1245,0,0.0328659,"Missing"
I17-1048,P16-1061,0,0.211469,"c entity-wise representation. Our experiments have demonstrated the benefits of such integration. Another related trend in recent studies is the use of neural network to capture the information flow of a discourse. One approach has been to link RNNs across sentences (Wang and Cho, 2016; Serban et al., 2016), while a second approach has expolited a type of memory space to store contextual information (Sukhbaatar et al., 2015; Tran et al., 2016; Merity et al., 2017). Research on reading comprehension (Kobayashi et al., 2016; Henaff et al., 2017) and coreference resolution (Wiseman et al., 2016; Clark and Manning, 2016b,a) has shown the salience of entitywise context information. Our model could be located within such approaches, but is distinct in being the first model to make use of entity-wise context information in both the input and output layers for sentence generation. Table 4: Mean Quantile of a true coreferent entity among antecedent entities. 6 Related Work An approach to addressing the unknown word problem used in recent studies (Kim et al., 2016; Sennrich et al., 2016; Luong and Manning, 2016; Schuster and Nakajima, 2012) comprises the embeddings of unknown words from character embeddings or sub"
I17-1048,N16-1099,1,0.0606823,"tute of Technology, Japan okazaki@c.titech.ac.jp Kentaro Inui Tohoku University / RIKEN, Japan inui@ecei.tohoku.ac.jp Abstract ... [ 1 ] killed [ 2 ] with bombs … ... police suspects [ 1 ] attacked ... This study addresses the problem of identifying the meaning of unknown words or entities in a discourse with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of word embeddings in both the input and output layers of a neural model by tracking contexts. This extends the dynamic entity representation used in Kobayashi et al. (2016) and incorporates a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. (2016). In addition, we construct a new task and dataset called Anonymized Language Modeling for evaluating the ability to capture word meanings while reading. Experiments conducted using our novel dataset show that the proposed variant of RNN language model outperformed the baseline model. Furthermore, the experiments also demonstrate that dynamic updates of an output layer help a model predict reappearing entities, whereas those of an input layer are effective to predict words following reappear"
I17-1048,D15-1200,0,0.0266978,"e salience of entitywise context information. Our model could be located within such approaches, but is distinct in being the first model to make use of entity-wise context information in both the input and output layers for sentence generation. Table 4: Mean Quantile of a true coreferent entity among antecedent entities. 6 Related Work An approach to addressing the unknown word problem used in recent studies (Kim et al., 2016; Sennrich et al., 2016; Luong and Manning, 2016; Schuster and Nakajima, 2012) comprises the embeddings of unknown words from character embeddings or subword embeddings. Li and Jurafsky (2015) applied word disambiguation and use a sense embedding to the target word. Choi et al. (2017) captured the context-sensitive meanings of common words using word embeddings, applied through a gating function controlled by history words, in the context of machine translation. In future work, we will explore a wider range of models, to integrate our dynamic text modeling with methods that estimate the meaning of unknown words or entities from their constituents. When addressing well-known entities such as Obama and Trump, it makes sense to learn their embeddings from external resources, as well a"
I17-1048,P16-1100,0,0.0306988,"ehension (Kobayashi et al., 2016; Henaff et al., 2017) and coreference resolution (Wiseman et al., 2016; Clark and Manning, 2016b,a) has shown the salience of entitywise context information. Our model could be located within such approaches, but is distinct in being the first model to make use of entity-wise context information in both the input and output layers for sentence generation. Table 4: Mean Quantile of a true coreferent entity among antecedent entities. 6 Related Work An approach to addressing the unknown word problem used in recent studies (Kim et al., 2016; Sennrich et al., 2016; Luong and Manning, 2016; Schuster and Nakajima, 2012) comprises the embeddings of unknown words from character embeddings or subword embeddings. Li and Jurafsky (2015) applied word disambiguation and use a sense embedding to the target word. Choi et al. (2017) captured the context-sensitive meanings of common words using word embeddings, applied through a gating function controlled by history words, in the context of machine translation. In future work, we will explore a wider range of models, to integrate our dynamic text modeling with methods that estimate the meaning of unknown words or entities from their consti"
I17-1048,P15-1002,0,0.123525,"Missing"
I17-1048,P16-1154,0,0.317257,"KEN, Japan inui@ecei.tohoku.ac.jp Abstract ... [ 1 ] killed [ 2 ] with bombs … ... police suspects [ 1 ] attacked ... This study addresses the problem of identifying the meaning of unknown words or entities in a discourse with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of word embeddings in both the input and output layers of a neural model by tracking contexts. This extends the dynamic entity representation used in Kobayashi et al. (2016) and incorporates a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. (2016). In addition, we construct a new task and dataset called Anonymized Language Modeling for evaluating the ability to capture word meanings while reading. Experiments conducted using our novel dataset show that the proposed variant of RNN language model outperformed the baseline model. Furthermore, the experiments also demonstrate that dynamic updates of an output layer help a model predict reappearing entities, whereas those of an input layer are effective to predict words following reappearing entities. 1 d[2],2 ... will arrest [ 1 ] soon … d[1],2 ! x[2] x[1] y[2] !"
I17-1048,K16-1006,0,0.191015,"ccurrences of an entity Train 2725 25.7 15.6 9.3 Valid 335 27.2 16.8 9.9 Test 336 26.4 15.8 9.5 3.2 3.2 3.1 achieve best perplexity on the validation dataset5 . Most hyper-parameters were tuned and fixed by the baseline model on the validation dataset6 . It is difficult to adequately train the all parts of a model using only the small dataset of Anonymized Language Modeling. We therefore pretrained word embeddings and ContextEncoder (the bidirectional RNNs and matrices in Equations 6– 8) on a sentence completion task in which clozes were predicted from the surrounding words in a large corpus (Melamud et al., 2016)7 . We used the objective function with samP negative | ˆ pling (Mikolov et al., 2013): (log σ( x x e e) + e P | ˆ ˆ (log σ(− x x ))). Here, x is a context e v e v∈N eg vector predicted by ContextEncoder, xe denotes the word embedding of a target word e appearing in the corpus, and N eg represents randomly sampled words. These pretrained parameters of ContextEncoder were fixed when the whole language model was trained on the Anonymized Language Modeling dataset. We implemented models in Python using the Chainer neural network library (Tokui et al., 2015). The code and the constructed dataset a"
I17-1048,P16-1014,0,0.312562,"tohoku.ac.jp Abstract ... [ 1 ] killed [ 2 ] with bombs … ... police suspects [ 1 ] attacked ... This study addresses the problem of identifying the meaning of unknown words or entities in a discourse with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of word embeddings in both the input and output layers of a neural model by tracking contexts. This extends the dynamic entity representation used in Kobayashi et al. (2016) and incorporates a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. (2016). In addition, we construct a new task and dataset called Anonymized Language Modeling for evaluating the ability to capture word meanings while reading. Experiments conducted using our novel dataset show that the proposed variant of RNN language model outperformed the baseline model. Furthermore, the experiments also demonstrate that dynamic updates of an output layer help a model predict reappearing entities, whereas those of an input layer are effective to predict words following reappearing entities. 1 d[2],2 ... will arrest [ 1 ] soon … d[1],2 ! x[2] x[1] y[2] ! ! ! y[1] −−−→ = RNN( ... p"
I17-1048,D15-1038,0,0.0604066,"Missing"
I17-1048,P16-1028,0,0.0259706,"se to learn their embeddings from external resources, as well as dynamically from the preceding context in a given discourse (as in our Dynamic Neural Text Model). The integration of these two sources of information is an intriguing challenge in language modeling. A key aspect of our model is its incorporation of the copy mechanism (Gu et al., 2016; Gulcehre et al., 2016), using dynamic word embeddings in the output layer. Independently of this study, several research groups have explored the use of variants of the copy mechanisms in language modeling (Merity et al., 2017; Grave et al., 2017; Peng and Roth, 2016). These studies, however, did not incorporate dynamic representations in the input layer. In contrast, our proposal incorporates the copy mechanism through the use We summarize and compare works for entitycentric neural networks that read a document. Kobayashi et al. (2016) pioneered entity-centric neural models tracking states in a discourse. They proposed Dynamic Entity Representation, which encodes contexts of entities and updates the states using entity-wise memories. Wiseman et al. (2016) also proposed a method for managing similar entity-wise features on neural networks and improved a co"
I17-1048,D17-1195,0,0.16917,"t al. (2016) also proposed a method for managing similar entity-wise features on neural networks and improved a coreference resolution model. Clark and Manning (2016b,a) incorporated such entitywise representations in mention-ranking coreference models. Our paper follows Kobayashi et al. (2016) and exploits dynamic entity reprensetions in a neural language model, where dynamic reporesentations are used not only in the neural encoder but also in the decoder, applicable to various sequence generation tasks, e.g., machine translation and dialog response generation. Simultaneously with our paper, Ji et al. (2017) use dynamic entity representation in a neural language model for reranking outputs of a coreference resolution system. Yang et al. (2017) experiment language modeling with referring to internal contexts or external data. Henaff et al. (2017) focus on neural networks tracking contexts of entities, achieving the state-of-the-art result in bAbI (Weston et al., 2015), a reading comprehension task. They encode the contexts of each entity by an attention480 like gated RNN instead of using coreference links directly. Dhingra et al. (2017) also try to improve a reading comprehension model using coref"
I17-1048,W12-4501,0,0.260955,"ing. Figure 3 gives an example from the dataset. Briefly, the dataset anonymizes certain noun phrases, treating them as unknown words and retaining their coreference relations. This allows a language model to track the context of every noun phrase in the discourse. Other words are left unchanged, allowing the language model to preserve the context of the anonymized (unknown) words, and to infer their meanings from the known words. The process was inspired by Hermann et al. (2015), whose approach has been explored by the research on reading comprehension. More precisely, we used the OntoNotes (Pradhan et al., 2012) corpus, which includes documents with coreferences and named entity tags manually annotated. We assigned an anonymous identifier to every coreference chain in the corpus3 in order of first appearance4 , and replaced mentions of a coreference chain with its identifier. In our experiments, each coreference chain was given a dynamic representation. Following Mikolov et al. (2010), we limited the vocabulary to 10,000 words appearing frequently in the corpus. Finally, we inserted “&lt;bos&gt;” and “&lt;eos&gt;” tokens to mark the beginning and end of each sentence. An important difference between this dataset"
I17-1048,P16-1162,0,0.0444964,"search on reading comprehension (Kobayashi et al., 2016; Henaff et al., 2017) and coreference resolution (Wiseman et al., 2016; Clark and Manning, 2016b,a) has shown the salience of entitywise context information. Our model could be located within such approaches, but is distinct in being the first model to make use of entity-wise context information in both the input and output layers for sentence generation. Table 4: Mean Quantile of a true coreferent entity among antecedent entities. 6 Related Work An approach to addressing the unknown word problem used in recent studies (Kim et al., 2016; Sennrich et al., 2016; Luong and Manning, 2016; Schuster and Nakajima, 2012) comprises the embeddings of unknown words from character embeddings or subword embeddings. Li and Jurafsky (2015) applied word disambiguation and use a sense embedding to the target word. Choi et al. (2017) captured the context-sensitive meanings of common words using word embeddings, applied through a gating function controlled by history words, in the context of machine translation. In future work, we will explore a wider range of models, to integrate our dynamic text modeling with methods that estimate the meaning of unknown words or e"
I17-1048,N16-1036,0,0.0264423,"37±.005 .620±.002 .613±.002 of dynamic representations in the output layer, integrating them with dynamic mechanisms in both the input and output layers by applying dynamic entity-wise representation. Our experiments have demonstrated the benefits of such integration. Another related trend in recent studies is the use of neural network to capture the information flow of a discourse. One approach has been to link RNNs across sentences (Wang and Cho, 2016; Serban et al., 2016), while a second approach has expolited a type of memory space to store contextual information (Sukhbaatar et al., 2015; Tran et al., 2016; Merity et al., 2017). Research on reading comprehension (Kobayashi et al., 2016; Henaff et al., 2017) and coreference resolution (Wiseman et al., 2016; Clark and Manning, 2016b,a) has shown the salience of entitywise context information. Our model could be located within such approaches, but is distinct in being the first model to make use of entity-wise context information in both the input and output layers for sentence generation. Table 4: Mean Quantile of a true coreferent entity among antecedent entities. 6 Related Work An approach to addressing the unknown word problem used in recent s"
I17-1048,P16-1125,0,0.0303919,"namic GRU-ReLU input & output GRU Max pool. Only latest MQ .525±.001 .630±.005 .633±.005 .617±.002 .600±.004 .519±.001 .522±.000 .519±.001 .519±.003 .642±.004 .637±.005 .620±.002 .613±.002 of dynamic representations in the output layer, integrating them with dynamic mechanisms in both the input and output layers by applying dynamic entity-wise representation. Our experiments have demonstrated the benefits of such integration. Another related trend in recent studies is the use of neural network to capture the information flow of a discourse. One approach has been to link RNNs across sentences (Wang and Cho, 2016; Serban et al., 2016), while a second approach has expolited a type of memory space to store contextual information (Sukhbaatar et al., 2015; Tran et al., 2016; Merity et al., 2017). Research on reading comprehension (Kobayashi et al., 2016; Henaff et al., 2017) and coreference resolution (Wiseman et al., 2016; Clark and Manning, 2016b,a) has shown the salience of entitywise context information. Our model could be located within such approaches, but is distinct in being the first model to make use of entity-wise context information in both the input and output layers for sentence generation."
I17-1048,N16-1114,0,0.0580191,"Missing"
I17-1048,D17-1197,0,0.113034,"odel. Clark and Manning (2016b,a) incorporated such entitywise representations in mention-ranking coreference models. Our paper follows Kobayashi et al. (2016) and exploits dynamic entity reprensetions in a neural language model, where dynamic reporesentations are used not only in the neural encoder but also in the decoder, applicable to various sequence generation tasks, e.g., machine translation and dialog response generation. Simultaneously with our paper, Ji et al. (2017) use dynamic entity representation in a neural language model for reranking outputs of a coreference resolution system. Yang et al. (2017) experiment language modeling with referring to internal contexts or external data. Henaff et al. (2017) focus on neural networks tracking contexts of entities, achieving the state-of-the-art result in bAbI (Weston et al., 2015), a reading comprehension task. They encode the contexts of each entity by an attention480 like gated RNN instead of using coreference links directly. Dhingra et al. (2017) also try to improve a reading comprehension model using coreference links. Similarly to our dynamic entity representation, Bahdanau et al. (2017) construct on-the-fly word embeddings of rare words fr"
I17-2022,N15-1114,0,0.0156424,"ted in this study by showing that the performance of a sophisticated local model can be considerably improved with recent feature embedding methods and a feature combination learning based on a neural network, outperforming the state-of-theart global models in F1 on a common benchmark dataset. 1 Introduction A predicate-argument structure (PAS) analysis is the task of analyzing the structural relations between a predicate and its arguments in a text and is considered as a useful sub-process for a wide range of natural language processing applications (Shen and Lapata, 2007; Kudo et al., 2014; Liu et al., 2015). PAS analysis can be decomposed into a set of primitive subtasks that seek a filler token for each argument slot of each predicate. The existing models for PAS analysis fall into two types: local models and global models. Local models independently solve each primitive subtask in the pointwise fashion (Seki et al., 2002; Taira et al., 2008; Imamura et al., 2009; Yoshino et al., 2013). Such models tend to be easy to implement and faster compared with global models but cannot handle dependencies between primitive subtasks. Re2 Task and Dataset In this study, we adopt the specifications of the N"
I17-2022,W14-4012,0,0.0609054,"Missing"
I17-2022,P15-1093,0,0.564351,"Taiwan, November 27 – December 1, 2017 2017 AFNLP for each argument slot of each predicate. The difficulty of finding an argument tends to differ depending on the relative position of the argument filler and the predicate. In particular, if the argument is omitted and the corresponding filler appears outside the sentence, the task is much more difficult because we cannot use the syntactic relationship between the predicate and the filler in a naive way. For this reason, a large part of previous work narrowed the focus to the analysis of arguments in a target sentence (Yoshikawa et al., 2011; Ouchi et al., 2015; Iida et al., 2015), and here, we followed this setting as well. 3 Figure 1: Network structure of our NN model Model Given a tokenized sentence s and a target predicate p in s with the gold dependency tree t, the goal of our task is to select at most one argument token a ˆ for each case slot of the target predicate. Taking xa = (a, p, s, t) as input, our model estimates the probability p(c|xa ) of assigning a case label c ∈ {NOM, ACC, DAT, NONE} for each token a in the sentence, and then selects a token with a maximum probability that exceeds the output threshold θc for c. The probability p(c"
I17-2022,D15-1112,0,0.0636294,"Missing"
I17-2022,P17-1146,0,0.570466,"Missing"
I17-2022,I11-1023,0,0.110993,"r each argument candidate. However, as argued by Toutanova et al. (2008) and Yoshikawa et al. (2011), there is a dependency between the argument labels of a predicate. In Japanese, case markers (case particles) partially represent a semantic relationship between words in direct dependency. We thus introduce a new feature that approximates co-occurrence bias of argument labels by gathering case particles for the other direct dependents of a target predicate. Other binary features The other binary features employed in our models have mostly been discussed in previous work (Imamura et al., 2009; Hayashibe et al., 2011). The entire list of our binary features are presented in Table 1. 2 3 130 https://code.google.com/archive/p/Word2Vec/ https://github.com/attardi/wikiextractor Model Binary feats. F1 (σ) B B all −cases WB WBP-Roth WBP-Shwartz WBP-Shwartz WBP-Shwartz WBP-Shwartz (ens) WBP-Roth BP-Roth WB B All F1 in different dependency distance Dep Zero 2 3 4 ≥5 Prec. Rec. 82.02 (±0.13) 81.64 (±0.19) 83.45 83.88 80.64 79.52 89.11 88.77 49.59 48.04 57.97 56.60 47.2 45.0 37 36 21 21 all all all −word −{word, path} −{word, path} 82.40 (±0.20) 82.43 (±0.15) 83.26 (±0.13) 83.23 (±0.11) 83.28 (±0.16) 83.85 85.30 84."
I17-2022,P16-1113,0,0.310837,"r c. The probability p(c|xa ) is modeled by a neural network (NN) architecture, which is a fully connected multilayer feedforward network stacked with a softmax layer on the top (Figure 1). g = softmax(Wn+1 hn + bn+1 ) (1) hi = ReLU(BN(Wi hi−1 + bi )) (2) h1 = ReLU(BN(W1 m + b1 )) (3) m = [hpath , wp , wa , f (xa )] (4) Figure 2: Path embedding important information. Moreover, in some constructions such as raising, control, and coordination, lexical information of intermediate nodes is also beneficial although a sparsity problem occurs with a conventional binary encoding of lexicalized paths. Roth and Lapata (2016) and Shwartz et al. (2016) recently proposed methods for embedding a lexicalized version of dependency path on a single vector using RNN. Both the methods embed words, parts-of-speech, and directions and labels of dependency in the path into a hidden unit of LSTM and output the final state of the hidden unit. We adopt these methods for Japanese PAS analysis and compare their performances. As shown in Figure 2, given a dependency path from a predicate to an argument candidate, we first create a sequence of POS, lemma, and dependency direction for each token in this order by traversing the path."
I17-2022,I11-1085,0,0.706432,"Missing"
I17-2022,C02-1078,0,0.162026,"argument structure (PAS) analysis is the task of analyzing the structural relations between a predicate and its arguments in a text and is considered as a useful sub-process for a wide range of natural language processing applications (Shen and Lapata, 2007; Kudo et al., 2014; Liu et al., 2015). PAS analysis can be decomposed into a set of primitive subtasks that seek a filler token for each argument slot of each predicate. The existing models for PAS analysis fall into two types: local models and global models. Local models independently solve each primitive subtask in the pointwise fashion (Seki et al., 2002; Taira et al., 2008; Imamura et al., 2009; Yoshino et al., 2013). Such models tend to be easy to implement and faster compared with global models but cannot handle dependencies between primitive subtasks. Re2 Task and Dataset In this study, we adopt the specifications of the NAIST Text Corpus (NTC) (Iida et al., 2007b), a commonly used benchmark corpus annotated with nominative (NOM), accusative (ACC), and dative (DAT) arguments for predicates. Given an input text and the predicate positions, the aim of the PAS analysis is to identify the head of the filler tokens 128 Proceedings of the The 8"
I17-2022,D07-1002,0,0.0267795,"ce of designing a local model is demonstrated in this study by showing that the performance of a sophisticated local model can be considerably improved with recent feature embedding methods and a feature combination learning based on a neural network, outperforming the state-of-theart global models in F1 on a common benchmark dataset. 1 Introduction A predicate-argument structure (PAS) analysis is the task of analyzing the structural relations between a predicate and its arguments in a text and is considered as a useful sub-process for a wide range of natural language processing applications (Shen and Lapata, 2007; Kudo et al., 2014; Liu et al., 2015). PAS analysis can be decomposed into a set of primitive subtasks that seek a filler token for each argument slot of each predicate. The existing models for PAS analysis fall into two types: local models and global models. Local models independently solve each primitive subtask in the pointwise fashion (Seki et al., 2002; Taira et al., 2008; Imamura et al., 2009; Yoshino et al., 2013). Such models tend to be easy to implement and faster compared with global models but cannot handle dependencies between primitive subtasks. Re2 Task and Dataset In this study"
I17-2022,W07-1522,1,0.787732,"composed into a set of primitive subtasks that seek a filler token for each argument slot of each predicate. The existing models for PAS analysis fall into two types: local models and global models. Local models independently solve each primitive subtask in the pointwise fashion (Seki et al., 2002; Taira et al., 2008; Imamura et al., 2009; Yoshino et al., 2013). Such models tend to be easy to implement and faster compared with global models but cannot handle dependencies between primitive subtasks. Re2 Task and Dataset In this study, we adopt the specifications of the NAIST Text Corpus (NTC) (Iida et al., 2007b), a commonly used benchmark corpus annotated with nominative (NOM), accusative (ACC), and dative (DAT) arguments for predicates. Given an input text and the predicate positions, the aim of the PAS analysis is to identify the head of the filler tokens 128 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 128–133, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP for each argument slot of each predicate. The difficulty of finding an argument tends to differ depending on the relative position of the argument filler and the predicate. In pa"
I17-2022,P16-1117,0,0.307152,"five epochs of unimproved loss on the development data. The output thresholds for case labels were optimized on the training data. Table 1: Binary features as the path-embedding vector. We employ GRU (Cho et al., 2014) for our RNN and use two types of input vectors: the adaptations of Roth and Lapata (2016), which we described in Figure 2, and Shwartz et al. (2016), which concatenates vectors of POS, lemma and dependency direction for each token into a single vector. 3.2 Word embedding The generalization of a word representation is one of the major issues in SRL. Fitzgerald et al. (2015) and Shibata et al. (2016) successfully improved the classification accuracy of SRL tasks by generalizing words using embedding techniques. We employ the same approach as Shibata et al. (2016), which uses the concatenation of the embedding vectors of a predicate and an argument candidate. Initialization All the weight matrices in GRU were initialized with random orthonormal matrices. The word embedding vectors were initialized with 256-dimensional Word2Vec2 vectors trained on the entire Japanese Wikipedia articles dumped on September 1st, 2016. We extracted the body texts using WikiExtractor,3 and tokenized them using"
I17-2022,D15-1260,0,0.558742,"– December 1, 2017 2017 AFNLP for each argument slot of each predicate. The difficulty of finding an argument tends to differ depending on the relative position of the argument filler and the predicate. In particular, if the argument is omitted and the corresponding filler appears outside the sentence, the task is much more difficult because we cannot use the syntactic relationship between the predicate and the filler in a naive way. For this reason, a large part of previous work narrowed the focus to the analysis of arguments in a target sentence (Yoshikawa et al., 2011; Ouchi et al., 2015; Iida et al., 2015), and here, we followed this setting as well. 3 Figure 1: Network structure of our NN model Model Given a tokenized sentence s and a target predicate p in s with the gold dependency tree t, the goal of our task is to select at most one argument token a ˆ for each case slot of the target predicate. Taking xa = (a, p, s, t) as input, our model estimates the probability p(c|xa ) of assigning a case label c ∈ {NOM, ACC, DAT, NONE} for each token a in the sentence, and then selects a token with a maximum probability that exceeds the output threshold θc for c. The probability p(c|xa ) is modeled by"
I17-2022,P16-1226,0,0.045138,"Missing"
I17-2022,D16-1132,0,0.699529,"Missing"
I17-2022,D08-1055,0,0.87169,"(PAS) analysis is the task of analyzing the structural relations between a predicate and its arguments in a text and is considered as a useful sub-process for a wide range of natural language processing applications (Shen and Lapata, 2007; Kudo et al., 2014; Liu et al., 2015). PAS analysis can be decomposed into a set of primitive subtasks that seek a filler token for each argument slot of each predicate. The existing models for PAS analysis fall into two types: local models and global models. Local models independently solve each primitive subtask in the pointwise fashion (Seki et al., 2002; Taira et al., 2008; Imamura et al., 2009; Yoshino et al., 2013). Such models tend to be easy to implement and faster compared with global models but cannot handle dependencies between primitive subtasks. Re2 Task and Dataset In this study, we adopt the specifications of the NAIST Text Corpus (NTC) (Iida et al., 2007b), a commonly used benchmark corpus annotated with nominative (NOM), accusative (ACC), and dative (DAT) arguments for predicates. Given an input text and the predicate positions, the aim of the PAS analysis is to identify the head of the filler tokens 128 Proceedings of the The 8th International Joi"
I17-2022,J08-2002,0,0.033193,"trained POS vectors for words that were not contained in a lexicon of Wikipedia word vectors in the PAS analysis task. We used another set of word/POS embedding vectors for lexicalized path embeddings, initialized with 64-dimensional Word2Vec vectors. The embeddings for dependency directions were randomly initialized. All the pre-trained embedding vectors were fine-tuned in the PAS analysis task. The hyperparameters for Word2Vec are “-cbow 3.3 Binary features Case markers of the other dependents Our model independently estimates label scores for each argument candidate. However, as argued by Toutanova et al. (2008) and Yoshikawa et al. (2011), there is a dependency between the argument labels of a predicate. In Japanese, case markers (case particles) partially represent a semantic relationship between words in direct dependency. We thus introduce a new feature that approximates co-occurrence bias of argument labels by gathering case particles for the other direct dependents of a target predicate. Other binary features The other binary features employed in our models have mostly been discussed in previous work (Imamura et al., 2009; Hayashibe et al., 2011). The entire list of our binary features are pres"
I17-2022,P09-2022,0,0.854013,"e task of analyzing the structural relations between a predicate and its arguments in a text and is considered as a useful sub-process for a wide range of natural language processing applications (Shen and Lapata, 2007; Kudo et al., 2014; Liu et al., 2015). PAS analysis can be decomposed into a set of primitive subtasks that seek a filler token for each argument slot of each predicate. The existing models for PAS analysis fall into two types: local models and global models. Local models independently solve each primitive subtask in the pointwise fashion (Seki et al., 2002; Taira et al., 2008; Imamura et al., 2009; Yoshino et al., 2013). Such models tend to be easy to implement and faster compared with global models but cannot handle dependencies between primitive subtasks. Re2 Task and Dataset In this study, we adopt the specifications of the NAIST Text Corpus (NTC) (Iida et al., 2007b), a commonly used benchmark corpus annotated with nominative (NOM), accusative (ACC), and dative (DAT) arguments for predicates. Given an input text and the predicate positions, the aim of the PAS analysis is to identify the head of the filler tokens 128 Proceedings of the The 8th International Joint Conference on Natur"
I17-2022,I11-1126,0,0.376747,"pages 128–133, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP for each argument slot of each predicate. The difficulty of finding an argument tends to differ depending on the relative position of the argument filler and the predicate. In particular, if the argument is omitted and the corresponding filler appears outside the sentence, the task is much more difficult because we cannot use the syntactic relationship between the predicate and the filler in a naive way. For this reason, a large part of previous work narrowed the focus to the analysis of arguments in a target sentence (Yoshikawa et al., 2011; Ouchi et al., 2015; Iida et al., 2015), and here, we followed this setting as well. 3 Figure 1: Network structure of our NN model Model Given a tokenized sentence s and a target predicate p in s with the gold dependency tree t, the goal of our task is to select at most one argument token a ˆ for each case slot of the target predicate. Taking xa = (a, p, s, t) as input, our model estimates the probability p(c|xa ) of assigning a case label c ∈ {NOM, ACC, DAT, NONE} for each token a in the sentence, and then selects a token with a maximum probability that exceeds the output threshold θc for c."
I17-2022,I13-1126,0,0.013033,"e structural relations between a predicate and its arguments in a text and is considered as a useful sub-process for a wide range of natural language processing applications (Shen and Lapata, 2007; Kudo et al., 2014; Liu et al., 2015). PAS analysis can be decomposed into a set of primitive subtasks that seek a filler token for each argument slot of each predicate. The existing models for PAS analysis fall into two types: local models and global models. Local models independently solve each primitive subtask in the pointwise fashion (Seki et al., 2002; Taira et al., 2008; Imamura et al., 2009; Yoshino et al., 2013). Such models tend to be easy to implement and faster compared with global models but cannot handle dependencies between primitive subtasks. Re2 Task and Dataset In this study, we adopt the specifications of the NAIST Text Corpus (NTC) (Iida et al., 2007b), a commonly used benchmark corpus annotated with nominative (NOM), accusative (ACC), and dative (DAT) arguments for predicates. Given an input text and the predicate positions, the aim of the PAS analysis is to identify the head of the filler tokens 128 Proceedings of the The 8th International Joint Conference on Natural Language Processing,"
I17-2022,P14-2091,0,0.0492097,"model is demonstrated in this study by showing that the performance of a sophisticated local model can be considerably improved with recent feature embedding methods and a feature combination learning based on a neural network, outperforming the state-of-theart global models in F1 on a common benchmark dataset. 1 Introduction A predicate-argument structure (PAS) analysis is the task of analyzing the structural relations between a predicate and its arguments in a text and is considered as a useful sub-process for a wide range of natural language processing applications (Shen and Lapata, 2007; Kudo et al., 2014; Liu et al., 2015). PAS analysis can be decomposed into a set of primitive subtasks that seek a filler token for each argument slot of each predicate. The existing models for PAS analysis fall into two types: local models and global models. Local models independently solve each primitive subtask in the pointwise fashion (Seki et al., 2002; Taira et al., 2008; Imamura et al., 2009; Yoshino et al., 2013). Such models tend to be easy to implement and faster compared with global models but cannot handle dependencies between primitive subtasks. Re2 Task and Dataset In this study, we adopt the spec"
I17-2058,N15-1060,0,0.0807805,"s Hiroki Asano12 , Tomoya Mizumoto2 , and Kentaro Inui12 1 Graduate School of Information Sciences, Tohoku University 2 RIKEN Center for Advanced Intelligence Project asano@ecei.tohoku.ac.jp,tomoya.mizumoto@riken.jp inui@ecei.tohoku.ac.jp Abstract A common approach to automatically evaluating GEC systems involves reference-based evaluation, where gold-standard references are manually created for a given test set of original sentences and each system output is scored by comparing it with corresponding gold-standard references with some metrics (referenced-based metric) (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015), analogous to BLEU (Papineni et al., 2002) in machine translation. Reference-based evaluation, however, has a severe drawback. In GEC, multiple outputs can be a right answer for a single input sentence. If the gold-standard references at hand lack coverage, reference-based metrics may unfairly underestimate system performance. One way to cope with this problem is to exhaustively collect potential corrections; however, this is not straightforward and can be of immense cost. As an alternative approach to this problem, Napoles et al. (2016) proposed a new method that does"
I17-2058,D15-1052,0,0.610757,"ting. We selected weights with the highest correlation on the JFLEG dataset, obtaining α = 0.07, β = 0.83, and γ = 0.10. Note that these optimized weights should not be interpreted as the relative importance of the subcomponents because outputs of those subcomponents differ in variance. For testing, following the experiments reported in (Napoles et al., 2016), the 12 system outputs for each input student sentence were scored with each metric, and next for each metric, the 12 systems were ranked according to their averaged scores. Each metric’s ranking was then compared to the human ranking of Grundkiewicz et al. (2015, Table 3c6 ) to compute the correlation coefficients, Spearman’s ρ and Pearson’s r. The results are shown in Table 1. Many interesting findings can be drawn. The grammaticality metric alone, which corresponds to (Napoles et al., 2016), outperformed M2 but did not perform as well as GLEU+. The meaning preservation metric exhibited poor correlation with human ranking; however, when combining meaning preservation with fluency, the prediction capability boosted, prevailing over GLEU+. We believe this result makes good sense because the meaning preservation metric, i.e. METEOR, relies mostly on sh"
I17-2058,N16-1108,0,0.0207299,"s potentially replace the state-of-the-art reference-based metrics in this research field. Our error analysis has revealed that the proposed metric still has room for improvement. One obvious point of improvement is around meaning preservation. The present choice for this component, METEOR, does not allow us to take nearly 346 synonymous phrases into account. For example, METEOR wrongly votes for an original sentence our family and relatives grew us up against a correctly revised sentence our family and relatives brought us up. Recent advances in computational modeling of sentence similarity (He and Lin, 2016; Rychalska et al., 2016, etc.) should be worthwhile to incorporated. It has also turned out that the present fluency metric is undesirably affected by misspelled words. As in Equation 1, the unigram probability regularizes the sentence probability so that the score of fluency will not be underestimated by rare words. However, with misspelled words, the normalization works excessively as they are treated as rare words. This newly provides an interesting issue of how to estimate the fluency of student sentences. Another direction for improvement is to explore methods for combining grammaticalit"
I17-2058,P14-2029,0,0.138775,"Missing"
I17-2058,P15-1156,0,0.0302548,"preserved in a revision, one can consider the use of an evaluation metric devised in the MT field. In this study, we adopt METEOR (Denkowski and Lavie, 2014) because it focuses on semantic similarity much more so than other common metrics, such as BLEU. Meaning score SM (h, s) for input of a source sentence s and a hypothesis h is calculated as follows: SM (h, s) = Fluency The importance of fluency in GEC has been shown by Sakaguchi et al. (2016) and Napoles et al. (2017), however there are no evaluation metrics that consider fluency. Fluency can be captured by statistical language modeling (Lau et al., 2015). More specifically, for a hypothesis h, P ·R , t · P + (1 − t) · R (2) c ,sc ) where P = m(h and R = m(h|scc,s |c ) . hc denotes |hc | content words in the hypothesis h, sc denotes content words in the source sentence s, and m(hc , sc ) denotes the number of matched content words between the output and the original sentence, and 3 In many cases SF (h) is more than 0 and less than 1. When it is less than 0, SF (h) = 0, and when it is more than 1, SF (h) = 1 4 https://github.com/yandex/ faster-rnnlm 1 https://languagetool.org 2 https://github.com/cnap/ grammaticality-metrics/tree/master/ heilma"
I17-2058,P15-2097,0,0.426674,"Mizumoto2 , and Kentaro Inui12 1 Graduate School of Information Sciences, Tohoku University 2 RIKEN Center for Advanced Intelligence Project asano@ecei.tohoku.ac.jp,tomoya.mizumoto@riken.jp inui@ecei.tohoku.ac.jp Abstract A common approach to automatically evaluating GEC systems involves reference-based evaluation, where gold-standard references are manually created for a given test set of original sentences and each system output is scored by comparing it with corresponding gold-standard references with some metrics (referenced-based metric) (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015), analogous to BLEU (Papineni et al., 2002) in machine translation. Reference-based evaluation, however, has a severe drawback. In GEC, multiple outputs can be a right answer for a single input sentence. If the gold-standard references at hand lack coverage, reference-based metrics may unfairly underestimate system performance. One way to cope with this problem is to exhaustively collect potential corrections; however, this is not straightforward and can be of immense cost. As an alternative approach to this problem, Napoles et al. (2016) proposed a new method that does not require gold-standa"
I17-2058,P15-1068,0,0.0519247,"correlates with human ratings. For this experiment, we used the CoNLL-2014 Shared Task (CoNLL) dataset (Ng et al., 2014). The CoNLL dataset is a collection of the outputs produced by the 12 participant GEC systems submitted to the CoNLL-2014 Shared Task, where the 12 GEC systems’ outputs to each input student sentence are ranked by multiple human raters (Grundkiewicz et al., 2015). An advantage of using this dataset is that it includes an extensive set of references for each input student sentence: two references originally provided in the CoNLL-2014 Shared Task, eight references provided by Bryant and Ng (2015), and eight references provided by Sakaguchi et al. (2016). In the experiment, we used all the 18 references for the baseline reference-based metrics in order to bring out the maximal potential of those metrics. For tuning the weights, α, β and γ, of our metric, we used another distinct dataset, the JHU FLuency-Extended GUG (henceforth, JFLEG) dataset (Napoles et al., 2017). This dataset 3.2 Minimal edits vs. fluent edits According to recent work by Sakaguchi et al. (2016), the aspect of fluency is potentially even further important than ever considered in the GEC literature. We expect that th"
I17-2058,D16-1228,0,0.28415,"Missing"
I17-2058,E17-2037,0,0.190486,"ing of sentence (1a), and thus sentence (1b) should be considered as inappropriate. To assess how much of the meaning of an original sentence is preserved in a revision, one can consider the use of an evaluation metric devised in the MT field. In this study, we adopt METEOR (Denkowski and Lavie, 2014) because it focuses on semantic similarity much more so than other common metrics, such as BLEU. Meaning score SM (h, s) for input of a source sentence s and a hypothesis h is calculated as follows: SM (h, s) = Fluency The importance of fluency in GEC has been shown by Sakaguchi et al. (2016) and Napoles et al. (2017), however there are no evaluation metrics that consider fluency. Fluency can be captured by statistical language modeling (Lau et al., 2015). More specifically, for a hypothesis h, P ·R , t · P + (1 − t) · R (2) c ,sc ) where P = m(h and R = m(h|scc,s |c ) . hc denotes |hc | content words in the hypothesis h, sc denotes content words in the source sentence s, and m(hc , sc ) denotes the number of matched content words between the output and the original sentence, and 3 In many cases SF (h) is more than 0 and less than 1. When it is less than 0, SF (h) = 0, and when it is more than 1, SF (h) ="
I17-2058,N12-1067,0,0.474634,"Error Correction Systems Hiroki Asano12 , Tomoya Mizumoto2 , and Kentaro Inui12 1 Graduate School of Information Sciences, Tohoku University 2 RIKEN Center for Advanced Intelligence Project asano@ecei.tohoku.ac.jp,tomoya.mizumoto@riken.jp inui@ecei.tohoku.ac.jp Abstract A common approach to automatically evaluating GEC systems involves reference-based evaluation, where gold-standard references are manually created for a given test set of original sentences and each system output is scored by comparing it with corresponding gold-standard references with some metrics (referenced-based metric) (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015), analogous to BLEU (Papineni et al., 2002) in machine translation. Reference-based evaluation, however, has a severe drawback. In GEC, multiple outputs can be a right answer for a single input sentence. If the gold-standard references at hand lack coverage, reference-based metrics may unfairly underestimate system performance. One way to cope with this problem is to exhaustively collect potential corrections; however, this is not straightforward and can be of immense cost. As an alternative approach to this problem, Napoles et al. (2016) propos"
I17-2058,W14-3348,0,0.041011,"Missing"
I17-2058,P02-1040,0,0.117046,"School of Information Sciences, Tohoku University 2 RIKEN Center for Advanced Intelligence Project asano@ecei.tohoku.ac.jp,tomoya.mizumoto@riken.jp inui@ecei.tohoku.ac.jp Abstract A common approach to automatically evaluating GEC systems involves reference-based evaluation, where gold-standard references are manually created for a given test set of original sentences and each system output is scored by comparing it with corresponding gold-standard references with some metrics (referenced-based metric) (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015), analogous to BLEU (Papineni et al., 2002) in machine translation. Reference-based evaluation, however, has a severe drawback. In GEC, multiple outputs can be a right answer for a single input sentence. If the gold-standard references at hand lack coverage, reference-based metrics may unfairly underestimate system performance. One way to cope with this problem is to exhaustively collect potential corrections; however, this is not straightforward and can be of immense cost. As an alternative approach to this problem, Napoles et al. (2016) proposed a new method that does not require gold-standard references (i.e., reference-less metric)"
I17-2058,S16-1091,0,0.0198201,"ace the state-of-the-art reference-based metrics in this research field. Our error analysis has revealed that the proposed metric still has room for improvement. One obvious point of improvement is around meaning preservation. The present choice for this component, METEOR, does not allow us to take nearly 346 synonymous phrases into account. For example, METEOR wrongly votes for an original sentence our family and relatives grew us up against a correctly revised sentence our family and relatives brought us up. Recent advances in computational modeling of sentence similarity (He and Lin, 2016; Rychalska et al., 2016, etc.) should be worthwhile to incorporated. It has also turned out that the present fluency metric is undesirably affected by misspelled words. As in Equation 1, the unigram probability regularizes the sentence probability so that the score of fluency will not be underestimated by rare words. However, with misspelled words, the normalization works excessively as they are treated as rare words. This newly provides an interesting issue of how to estimate the fluency of student sentences. Another direction for improvement is to explore methods for combining grammaticality, fluency and meaning p"
I17-2058,Q16-1013,0,0.565909,"t does not preserve the meaning of sentence (1a), and thus sentence (1b) should be considered as inappropriate. To assess how much of the meaning of an original sentence is preserved in a revision, one can consider the use of an evaluation metric devised in the MT field. In this study, we adopt METEOR (Denkowski and Lavie, 2014) because it focuses on semantic similarity much more so than other common metrics, such as BLEU. Meaning score SM (h, s) for input of a source sentence s and a hypothesis h is calculated as follows: SM (h, s) = Fluency The importance of fluency in GEC has been shown by Sakaguchi et al. (2016) and Napoles et al. (2017), however there are no evaluation metrics that consider fluency. Fluency can be captured by statistical language modeling (Lau et al., 2015). More specifically, for a hypothesis h, P ·R , t · P + (1 − t) · R (2) c ,sc ) where P = m(h and R = m(h|scc,s |c ) . hc denotes |hc | content words in the hypothesis h, sc denotes content words in the source sentence s, and m(hc , sc ) denotes the number of matched content words between the output and the original sentence, and 3 In many cases SF (h) is more than 0 and less than 1. When it is less than 0, SF (h) = 0, and when it"
I17-2069,P02-1040,0,0.0987931,"tion in style training (see Sec. 3.2) helps to make the generated responses more stylistically consistent (Trans v.s. Trans+alt). Overall, the improvement on Oja corpus is more salient than that on Tetsuko corpus. We attribute this to the fact that Tetsuko corpus is closer to the original dialog corpus. Table 4: Results of human evaluation. AR and SC denotes the appropriateness of response and stylistic consistency, respectively. Superscripts †B,†M,†T (and ‡B,‡M,‡T ) indicate the statistical significance against Base, Mix and Trans (sign test, p &lt; 0.05 for † , p &lt; 0.01 for ‡ ), respectively. (Papineni et al., 2002; Sordoni et al., 2015; Li et al., 2016)) or subjective human evaluation (Walker et al., 2012; Vinyals and Le, 2015; Shang et al., 2015). We employ human evaluation, because human evaluation captures more stylistic consistency than reference-based evaluation, and word-overlap similarity metrics such as BLEU correlates weakly with human judgments (Liu et al., 2016). For query utterances, we randomly extract 50 sentences from Twitter. There is no overlap between these query utterances, and the training and validation data. Each model generates four responses from one query utterance by beam samp"
I17-2069,P15-1152,0,0.0622406,"Missing"
I17-2069,N15-1020,0,0.0377083,"Missing"
I17-2069,P16-1094,0,0.57556,"need some manual annotations. Li et al. Introduction Dialog response-generation (DRG) systems have been studied extensively over the decades. In the recent years, there is a growing interest in a datadriven DRG system leveraging a large amount of conversational texts in social networking services (SNSs) such as Twitter. One successful approach is to train a neural sequence-to-sequence model (seq2seq) (Sutskever et al., 2014) on conversations from SNS, where the seq2seq learns a mapping from a user utterance to the appropriate response with recurrent neural networks (RNNs) (Shang et al., 2015; Li et al., 2016; Serban et al., 2017). The prior data-driven approach, however, suffers a major drawback. Since a wide variety of users, ranging from young to old, and from female to male, participates in an SNS, the learned responses are not guaranteed to be stylistically consistent (e.g. in terms of politeness). For example, we show actual responses generated by 408 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 408–412, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (2016) aim for response generation with a consistent “persona” by conditioning"
I17-2069,walker-etal-2012-annotated,0,0.428475,"consistent DRG model. A vanilla seq2seq cannot generate stylistically consistent responses without training on a large amount of stylistically consistent dialog responses, which is prohibitively costly. To address this issue, we apply transfer learning, namely transferring knowledge about response generation in a general domain into the task of stylistically consistent response generation. In the literature, little attention has been paid to the stylistic consistency of the generated responses. There are some previous attempts on transforming a style of dialog utterances into a specified one (Walker et al., 2012; Miyazaki et al., 2015). Their approaches assume that original utterances are given by a separate independent system and need some manual annotations. Li et al. Introduction Dialog response-generation (DRG) systems have been studied extensively over the decades. In the recent years, there is a growing interest in a datadriven DRG system leveraging a large amount of conversational texts in social networking services (SNSs) such as Twitter. One successful approach is to train a neural sequence-to-sequence model (seq2seq) (Sutskever et al., 2014) on conversations from SNS, where the seq2seq lear"
I17-2069,D16-1230,0,0.0519374,"Missing"
I17-2069,Y15-1035,0,\N,Missing
I17-2074,W12-2006,0,0.0226451,"Missing"
I17-2074,D15-1166,0,0.061551,"Missing"
I17-2074,N16-1042,0,0.015123,"dden vectors at time step t ∈ {1, · · · , n} using those at time step t − 1 and the (predicted) word wt as follows: −−−−→ [ht ; ct ] = LSTM(wt , [ht−1 ; ct−1 ]). (4) ← − Here, we set h0 = h 1 and c0 = 01 . For t &gt; 1, we feed the yt−1 predicted at the previous time step back as the input word wt of the decoder. The decoder predicts the word yt at time step t ¯ t with using a softmax layer on top of the vector h an integrated attention mechanism: Generating proofread sentences using an encoderdecoder model with attention log p(y|x) = t X log p(yt |y&lt;t , x), (5) n=1 Following recent work on GEC (Yuan and Briscoe, 2016), we use an encoder-decoder model with global attention (Luong et al., 2015) to generate proofread sentences. We use bi-directional Long Short-Term Memory (LSTM) to encode the source sentences. LSTMs recurrently compute the memory and hidden vectors at time step s ∈ {1, · · · , m} using those at time step s − 1 or s + 1 and the word xs in the source sentence, as follows: ¯ t ), p(yt |y&lt;t , x) = softmax(Wo h ¯ t = tanh(Wr [vt ; ht ]), h X e s, vt = αt (s)h (6) (7) (8) s αt (s) = e s) exp(h|t Wa h . (9) P | e 0 exp(h Wa hs0 ) s t (2) Here, vt represents a vector computed by the attention mechani"
I17-2074,P15-2097,0,0.0204916,"t to 100 and, improve computational efficiency, each batch consisted of sentences of the same length. The dimensionality of the distributed representations (word embeddings and hidden states) to was 300. The model parameters were trained using Adam. Following Jozefowicz et al. (2015), forget gate bias was initialized to 1.0, and the other gate biases were initialized to 0. In addition, we used dropout (at a rate of 0.2) for the LSTMs. Breadth-first search was used for decoding, with a beam width of 10 (Yuan, 2017). Six measures were utilized to evaluate the performance of the PSG model: GLEU (Napoles et al., 2015)3 , precision, recall, M 2 score (Dahlmeier and Ng, 2012), Word Error Rate (WER) (Jurafsky and Martin, 2008), and BLEU (Papineni et al., 2002). These measures are often used in GEC and machine translation research. Note that the precision, recall, and M 2 score measures excluded words appearing in both the source and proofread sentences from evaluation (Dahlmeier and Ng, 2012). 3.3 Results Table 2 shows the performance of the proposed method according to the above metrics. Two vari3 The hyperparameter λ of GLEU was set to 1. 439 • Proofread: The reserve players tackle it frantically so as not"
I17-2074,W13-3601,0,0.0356549,"Missing"
I17-2074,W14-1701,0,0.0248716,"Missing"
I17-2074,P02-1040,0,0.102307,"epresentations (word embeddings and hidden states) to was 300. The model parameters were trained using Adam. Following Jozefowicz et al. (2015), forget gate bias was initialized to 1.0, and the other gate biases were initialized to 0. In addition, we used dropout (at a rate of 0.2) for the LSTMs. Breadth-first search was used for decoding, with a beam width of 10 (Yuan, 2017). Six measures were utilized to evaluate the performance of the PSG model: GLEU (Napoles et al., 2015)3 , precision, recall, M 2 score (Dahlmeier and Ng, 2012), Word Error Rate (WER) (Jurafsky and Martin, 2008), and BLEU (Papineni et al., 2002). These measures are often used in GEC and machine translation research. Note that the precision, recall, and M 2 score measures excluded words appearing in both the source and proofread sentences from evaluation (Dahlmeier and Ng, 2012). 3.3 Results Table 2 shows the performance of the proposed method according to the above metrics. Two vari3 The hyperparameter λ of GLEU was set to 1. 439 • Proofread: The reserve players tackle it frantically so as not to miss the opportunity. learning the sub-task. That said, the other metrics decreased because the model made changes in many places where no"
I17-2074,D15-1044,0,0.049854,"ed pairs indicate that it is difficult to predict words that do not appear in the source sentences. However, we can see that weighting sub-task losses improved precision, recall, and M 2 score performance. We believe that active proofreading performance improved because of not overExperimental setup The batch size was set to 100 and, improve computational efficiency, each batch consisted of sentences of the same length. The dimensionality of the distributed representations (word embeddings and hidden states) to was 300. The model parameters were trained using Adam. Following Jozefowicz et al. (2015), forget gate bias was initialized to 1.0, and the other gate biases were initialized to 0. In addition, we used dropout (at a rate of 0.2) for the LSTMs. Breadth-first search was used for decoding, with a beam width of 10 (Yuan, 2017). Six measures were utilized to evaluate the performance of the PSG model: GLEU (Napoles et al., 2015)3 , precision, recall, M 2 score (Dahlmeier and Ng, 2012), Word Error Rate (WER) (Jurafsky and Martin, 2008), and BLEU (Papineni et al., 2002). These measures are often used in GEC and machine translation research. Note that the precision, recall, and M 2 score m"
I17-2074,J00-4006,0,\N,Missing
I17-2074,W11-2838,0,\N,Missing
J98-4002,P91-1034,0,0.0172838,"Missing"
J98-4002,P94-1020,0,0.00915099,"ough experiments on about one thousand sentences. Compared to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disa"
J98-4002,J94-4003,0,0.0096378,"d sentences. Compared to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such a"
J98-4002,P96-1042,0,0.41794,"s. During this phase, a human expert supervises samples, that is, provides the correct interpretation for the verbs appearing in the samples. Thereafter, samples are simply incorporated into the database without any computational overhead (as would be associated with globally reestimating parameters in statistics-based systems), meaning that the system can be trained on the remaining examples (the ""residue"") for the next iteration. Iterating between these two 1 Note that these problems are associated with corpus-based approaches in general, and have been identified by a number of researchers (Engelson and Dagan 1996; Lewis and Gale 1994; Uramoto 1994a; Yarowsky 1995). 574 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling sampling ~WSD~sD outu t ~ ~ : ( ~ ~ Figure 1 Flow of control of the example sampling system. phases, the system progressively enhances the database. Note that the selective sampiing procedure gives us an optimally informative database of a given size irrespective of the stage at which processing is terminated. Several researchers have proposed this type of approach for NLP applications. Engelson and Dagan (1996) proposed a committee-based sampling method, which is currently applied to"
J98-4002,C96-1012,1,0.808889,"to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is"
J98-4002,C94-2122,0,0.00946794,"tion because the verb in the input is disambiguated by superimposing the sense of the verb appearing in the example of highest similarity.3 The similarity between an input and an example is estimated based on the similarity between case •lers marked with the same case. Furthermore, since the restrictions imposed by the case fillers in choosing the verb sense are not equally selective, Fujii et al. (1996) proposed a weighted case contribution to disambiguation (CCD) of the verb senses. This CCD factor is taken into account 2 Note that unlike the automatic acquisition of word sense definitions (Fukumoto and Tsujii 1994; Pustejovsky and Boguraev 1993; Utsuro 1996; Zernik 1989), the task of the system is to identify the best matched category with a given input, from candidates. 3 In this paper, we use ""example-based systems"" to refer to systems based on nearest neighbor resolution. predefined 576 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling © nominative accusative Figure 3 The semantic ranges of the nominative and accusative for the verb toru. nq-mc~nc Cs~,c~ nc3-rnllc3 v &,;3 -- v ~s3,c2 ~s3,C3- -- V (S3) J database -- (?) G,c2 (s~)l Figure 4 An input and the database. w h e n c o m p u t i n g the s"
J98-4002,P92-1032,0,0.0186836,"Missing"
J98-4002,P91-1019,0,0.0194231,"approaches to word sense disambiguation should be further investigated, this experimental result gives us good motivation to explore example-based verb sense disambiguation approaches, i.e., to introduce the notion of selective sampling into them. 2.4 Enhancement of Verb Sense Disambiguation Let us discuss how further enhancements to our example-based verb sense disambiguation system could be made. First, since inputs are simple sentences, information for word sense disambiguation is inadequate in some cases. External information such as the discourse or domain dependency of each word sense (Guthrie et al. 1991; Nasukawa 1993; Yarowsky 1995) is expected to lead to system improvement. Second, some idiomatic expressions represent highly restricted collocations, and overgeneralizing them semantically through the use of a thesaurus can cause further errors. Possible solutions would include one proposed by Uramoto, in which idiomatic expressions are described separately in the database so that the system can control their overgeneralization (Uramoto 1994b). Third, a number of existing NLP tools such as JUMAN (a morphological analyzer) (Matsumoto et al. 1993) and QJP (a morphological and syntactic analyze"
J98-4002,P90-1034,0,0.0202315,"ion have been proposed, for example, by Yarowsky (1992). 578 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling IIII I I I kare kanojo (he) (she) otoko joshu hisho kane heya kippu uma (man) (assistant) (secretary)(money) (room) (ticket) (horse) Figure 5 A fragment of the Bunruigoihyo thesaurus. statistical factors, although statistical factors are calculated in terms of the predicate argument structure in which each noun appears. Predicate argument structures, which consist of complements (case filler nouns and case markers) and verbs, have also been used in the task of noun classification (Hindle 1990). This can be expressed by Equation (3), where ff is the vector for the noun in question, and items ti represent the statistics for predicate argument structures including n. ff = (h, t2,..., ti . . . . ) (3) In regard to ti, we used the notion of TF. IDF (Salton and McGill 1983). TF (term frequency) gives each context (a case marker/verb pair) importance proportional to the number of times it occurs with a given noun. The rationale behind IDF (inverse document frequency) is that contexts that rarely occur over collections of nouns are valuable, and that therefore the IDF of a context is inver"
J98-4002,C92-2101,0,0.0102778,"Missing"
J98-4002,C96-2104,0,0.0251221,"sukawa 1993; Yarowsky 1995) is expected to lead to system improvement. Second, some idiomatic expressions represent highly restricted collocations, and overgeneralizing them semantically through the use of a thesaurus can cause further errors. Possible solutions would include one proposed by Uramoto, in which idiomatic expressions are described separately in the database so that the system can control their overgeneralization (Uramoto 1994b). Third, a number of existing NLP tools such as JUMAN (a morphological analyzer) (Matsumoto et al. 1993) and QJP (a morphological and syntactic analyzer) (Kameda 1996) could broaden the coverage of our system, as inputs are currently limited to simple, morphologically analyzed sentences. Finally, it should be noted that in Japanese, case markers can be omitted or topicalized (for example, marked with postposition wa), an issue which our framework does not currently consider. 3. Example Sampling Algorithm 3.1 Overview Let us look again at Figure 1 in Section 1. In this figure, ""WSD outputs"" refers to a corpus in which each sentence is assigned an expected verb interpretation during the WSD phase. In the training phase, the system stores supervised samples (w"
J98-4002,H93-1051,0,0.0535285,"Missing"
J98-4002,W96-0208,0,0.0139467,"ly one verb sense remains. W h e n more than one verb sense is selected for any given m e t h o d (or none of them remains, for the rule-based method), the system simply selects the verb sense that appears most frequently in the database, s In the experiment, we conducted sixfold cross-validation, that is, we divided the training/test data into six equal parts, and conducted six trials in which a different 7 A number of experimental results have shown the effectiveness of the Naive-Bayes method for word sense disambiguation (Gale, Church, and Yarowsky 1993; Leacock, Towell, and Voorhees 1993; Mooney 1996; Ng 1997; Pedersen, Bruce, and Wiebe 1997). 8 One may argue that this goes against the basis of the rule-based method, in that, given a proper threshold value for the association degree, the system could improve on accuracy (potentially sacrificing coverage), and that the trade-off between coverage and accuracy is therefore a more appropriate evaluation criterion. However, our trials on the rule-based method with different threshold values did not show significant correlation between the improvement of accuracy and the degeneration of coverage. 581 Computational Linguistics Volume 24, Number"
J98-4002,1993.tmi-1.15,0,0.0285586,"ense disambiguation should be further investigated, this experimental result gives us good motivation to explore example-based verb sense disambiguation approaches, i.e., to introduce the notion of selective sampling into them. 2.4 Enhancement of Verb Sense Disambiguation Let us discuss how further enhancements to our example-based verb sense disambiguation system could be made. First, since inputs are simple sentences, information for word sense disambiguation is inadequate in some cases. External information such as the discourse or domain dependency of each word sense (Guthrie et al. 1991; Nasukawa 1993; Yarowsky 1995) is expected to lead to system improvement. Second, some idiomatic expressions represent highly restricted collocations, and overgeneralizing them semantically through the use of a thesaurus can cause further errors. Possible solutions would include one proposed by Uramoto, in which idiomatic expressions are described separately in the database so that the system can control their overgeneralization (Uramoto 1994b). Third, a number of existing NLP tools such as JUMAN (a morphological analyzer) (Matsumoto et al. 1993) and QJP (a morphological and syntactic analyzer) (Kameda 1996"
J98-4002,W97-0323,0,0.0563667,"ense remains. W h e n more than one verb sense is selected for any given m e t h o d (or none of them remains, for the rule-based method), the system simply selects the verb sense that appears most frequently in the database, s In the experiment, we conducted sixfold cross-validation, that is, we divided the training/test data into six equal parts, and conducted six trials in which a different 7 A number of experimental results have shown the effectiveness of the Naive-Bayes method for word sense disambiguation (Gale, Church, and Yarowsky 1993; Leacock, Towell, and Voorhees 1993; Mooney 1996; Ng 1997; Pedersen, Bruce, and Wiebe 1997). 8 One may argue that this goes against the basis of the rule-based method, in that, given a proper threshold value for the association degree, the system could improve on accuracy (potentially sacrificing coverage), and that the trade-off between coverage and accuracy is therefore a more appropriate evaluation criterion. However, our trials on the rule-based method with different threshold values did not show significant correlation between the improvement of accuracy and the degeneration of coverage. 581 Computational Linguistics Volume 24, Number 4 Table 2"
J98-4002,P96-1006,0,0.0285131,"verhead for search, without the degeneration of the performance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is, an example-based approach. A preliminary experiment showed that our system performs well when compared with sys"
J98-4002,C94-1049,0,0.00878414,"h, without the degeneration of the performance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is, an example-based approach. A preliminary experiment showed that our system performs well when compared with systems based on other a"
J98-4002,A97-1056,0,0.0176876,"Missing"
J98-4002,E95-1016,0,0.0189486,"allow only those nouns dominated by the given class in the thesaurus structure as verb complements. In order to identify appropriate thesaurus classes, we used the association measure proposed by Resnik (1993), which computes the information-theoretic association degree between case fillers and thesaurus classes, for each verb sense (Equation (7)). 6 P(rls, c) A(s,c,r) = P(rls, c) • log p(rlc) (7) 6 Note that previous research has applied this technique to tasks other than verb sense disambiguation, such as syntactic disambiguation (Resnik 1993) and disambiguation of case filler noun senses (Ribas 1995). 580 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling Here, A(s, c, r) is the association degree between verb sense s and class r (selectional restriction candidate) with respect to case c. P(rls, c) is the conditional probability that a case filler example associated with case c of sense s is d o m i n a t e d b y class r in the thesaurus. P(rlc ) is the conditional probability that a case filler example for case c (disregarding verb sense) is d o m i n a t e d b y class r. Each probability is estimated based on training data. We used the semantic classes defined in the Bunruigoihyo thes"
J98-4002,C94-2114,0,0.257712,"erformance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is, an example-based approach. A preliminary experiment showed that our system performs well when compared with systems based on other approaches, and motivated * De"
J98-4002,C96-2163,0,0.0117674,"perimposing the sense of the verb appearing in the example of highest similarity.3 The similarity between an input and an example is estimated based on the similarity between case •lers marked with the same case. Furthermore, since the restrictions imposed by the case fillers in choosing the verb sense are not equally selective, Fujii et al. (1996) proposed a weighted case contribution to disambiguation (CCD) of the verb senses. This CCD factor is taken into account 2 Note that unlike the automatic acquisition of word sense definitions (Fukumoto and Tsujii 1994; Pustejovsky and Boguraev 1993; Utsuro 1996; Zernik 1989), the task of the system is to identify the best matched category with a given input, from candidates. 3 In this paper, we use ""example-based systems"" to refer to systems based on nearest neighbor resolution. predefined 576 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling © nominative accusative Figure 3 The semantic ranges of the nominative and accusative for the verb toru. nq-mc~nc Cs~,c~ nc3-rnllc3 v &,;3 -- v ~s3,c2 ~s3,C3- -- V (S3) J database -- (?) G,c2 (s~)l Figure 4 An input and the database. w h e n c o m p u t i n g the score for each sense of the verb in question."
J98-4002,C94-2169,0,0.127069,"E TU(x = s) (13) sEK 3.4 Enhancement of Computation In this section, we discuss how to enhance the computation associated with our example sampling algorithm. First, we note that computation of TU(x = s) in Equation (11) above becomes time consuming because the system is required to search the whole set of unsupervised examples for examples whose interpretation certainty will increase after x is used for training. To avoid this problem, we could apply a method used in efficient database search techniques, by which the system can search for neighbor examples of x with optimal time complexity (Utsuro et al. 1994). However, in this section, we will explain another efficient algorithm to identify neighbors of x, in which neighbors of case fillers are considered to be given directly by the thesaurus structure. 12 The basic idea is the following: the system searches for neighbors of each case filler of x instead of x as a whole, and merges them as a set of neighbors of x. Note that by dividing examples along the lines of each case filler, we can retrieve neighbors based on the structure of the Bunruigoihyo thesaurus (instead of the conceptual semantic space as in Figure 7). Let Nx=s,c be a subset of unsup"
J98-4002,C92-2070,0,0.0363747,"pace model"" (VSM) (Frakes and Baeza-Yates 1992; Leacock, Towell, and Voorhees 1993; Salton and McGill 1983; Sch/itze 1992), which has a long history of application in information retrieval (IR) and text categorization (TC) tasks. In the case of IR/TC, VSM is used to compute the similarity between documents, which is represented by a vector comprising statistical factors of content words in a document. Similarly, in our case, each noun is represented by a vector comprising 4 Different types of application of hand-crafted thesauri to word sense disambiguation have been proposed, for example, by Yarowsky (1992). 578 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling IIII I I I kare kanojo (he) (she) otoko joshu hisho kane heya kippu uma (man) (assistant) (secretary)(money) (room) (ticket) (horse) Figure 5 A fragment of the Bunruigoihyo thesaurus. statistical factors, although statistical factors are calculated in terms of the predicate argument structure in which each noun appears. Predicate argument structures, which consist of complements (case filler nouns and case markers) and verbs, have also been used in the task of noun classification (Hindle 1990). This can be expressed by Equation (3), wh"
J98-4002,P95-1026,0,0.654354,"he system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is, an example-based approach. A preliminary experiment showed that our system performs well when compared with systems based on other approaches, and motivated * Department of Libra"
J98-4002,C92-2107,0,\N,Missing
L16-1734,P14-1133,0,0.0503634,"Missing"
L16-1734,fujita-etal-2014-overview,0,0.0148329,"pecific to the game. A lot of research has been done on the answering of real world questions using Freebase (Berant and Liang, 2014; Yao, 2015) or Wikipedia (Pasupat and Liang, 2015). Datasets for these tasks usually favour systems that do simple queries of facts on the knowledge database (Yao, 2015). As the complexity of the questions increases, answering the questions usually becomes considerably difficult (Pasupat and Liang, 2015), due to the vast complexity of the real world. There are efforts to restrict the domain of the task and pursue some advanced reasoning. The Todai Robot Project (Fujita et al., 2014) restricts the domain to university entrance exam questions. Other research includes solving algebra word problems (Kushman et al., 2014) and instructing robots (Misra et al., 2015). As a complement to these previous works, we believe the using of an open world video game as the domain has several merits. Firstly, the logic in a video game is simpler than the real world, which means that it can be handled readily. Therefore, this domain may provide a convenient testbed for integrating logical inference techniques into NLP systems, such as the logical inference using dependency-based compositio"
L16-1734,P14-1026,0,0.0284932,"o, 2015) or Wikipedia (Pasupat and Liang, 2015). Datasets for these tasks usually favour systems that do simple queries of facts on the knowledge database (Yao, 2015). As the complexity of the questions increases, answering the questions usually becomes considerably difficult (Pasupat and Liang, 2015), due to the vast complexity of the real world. There are efforts to restrict the domain of the task and pursue some advanced reasoning. The Todai Robot Project (Fujita et al., 2014) restricts the domain to university entrance exam questions. Other research includes solving algebra word problems (Kushman et al., 2014) and instructing robots (Misra et al., 2015). As a complement to these previous works, we believe the using of an open world video game as the domain has several merits. Firstly, the logic in a video game is simpler than the real world, which means that it can be handled readily. Therefore, this domain may provide a convenient testbed for integrating logical inference techniques into NLP systems, such as the logical inference using dependency-based compositional semantics (Tian et al., 2014). Secondly, despite the rather simple rules, open world video games provide enough liberty for players,"
L16-1734,P15-1096,0,0.0216349,"). Datasets for these tasks usually favour systems that do simple queries of facts on the knowledge database (Yao, 2015). As the complexity of the questions increases, answering the questions usually becomes considerably difficult (Pasupat and Liang, 2015), due to the vast complexity of the real world. There are efforts to restrict the domain of the task and pursue some advanced reasoning. The Todai Robot Project (Fujita et al., 2014) restricts the domain to university entrance exam questions. Other research includes solving algebra word problems (Kushman et al., 2014) and instructing robots (Misra et al., 2015). As a complement to these previous works, we believe the using of an open world video game as the domain has several merits. Firstly, the logic in a video game is simpler than the real world, which means that it can be handled readily. Therefore, this domain may provide a convenient testbed for integrating logical inference techniques into NLP systems, such as the logical inference using dependency-based compositional semantics (Tian et al., 2014). Secondly, despite the rather simple rules, open world video games provide enough liberty for players, and their popularity attracts people to ask"
L16-1734,P15-1142,0,0.0156888,"ing of several verbs into one type of action in the game. Understanding this kind of link is essential for the system to be efficient when the player ask a question with its own words, and not those used by the knowledge database. 9. Conclusion and discussion We have described a Question-Answering corpus and a knowledge database related to the video game Minecraft. Our goal is to build a system that can answer questions using the logic specific to the game. A lot of research has been done on the answering of real world questions using Freebase (Berant and Liang, 2014; Yao, 2015) or Wikipedia (Pasupat and Liang, 2015). Datasets for these tasks usually favour systems that do simple queries of facts on the knowledge database (Yao, 2015). As the complexity of the questions increases, answering the questions usually becomes considerably difficult (Pasupat and Liang, 2015), due to the vast complexity of the real world. There are efforts to restrict the domain of the task and pursue some advanced reasoning. The Todai Robot Project (Fujita et al., 2014) restricts the domain to university entrance exam questions. Other research includes solving algebra word problems (Kushman et al., 2014) and instructing robots (M"
L16-1734,P14-1008,1,0.895419,"Missing"
L16-1734,N15-3014,0,0.0736767,"eady structured data such as infoboxes. A preliminary examination of the data shows that players are asking creative questions about the game, and that the QA corpus can be used for clustering verbs and linking them to predefined actions in the game. Keywords: question-answering, knowledge acquisition, meaning representation 1. Introduction This paper presents a corpus and a knowledge database aiming at creating a Question-Answering system specific to a new context, the open world of a video game. Unlike many QA systems that are designed to answer real world questions (Berant and Liang, 2014; Yao, 2015), the goal of this research is to build a system that can answer questions using the logic specific to the game, which may not be identical to the logic in the real world. We choose a popular game called Minecraft, whose openness provides a great liberty for players, which guarantees a large number of possible questions to ask about the game, and yet the presence of a specific logic that limits the actions of players (Section 2.). We are interested in this problem setting because it could provide a testbed for combining Natural Language Processing with advanced logical inference techniques. Th"
matsuyoshi-etal-2010-annotating,W09-3027,1,\N,Missing
matsuyoshi-etal-2010-annotating,W09-3417,0,\N,Missing
matsuyoshi-etal-2010-annotating,P07-1125,0,\N,Missing
matsuyoshi-etal-2010-annotating,W09-1201,0,\N,Missing
matsuyoshi-etal-2010-annotating,W04-3103,0,\N,Missing
N10-1120,P08-1034,0,0.00994089,"Missing"
N10-1120,D08-1083,0,0.793046,"ion, a sentence which contains positive (or negative) polarity words does not necessarily have the same polarity as a whole, and we need to consider interactions between words instead of handling words independently. Recently, several methods have been proposed to cope with the problem (Zaenen, 2004; Ikeda et al., 2008). However, these methods are based on ﬂat bag-of-features representation, and do not consider syntactic structures which seem essential to infer the polarity of a whole sentence. Other methods have been proposed which utilize composition of sentences (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Jia et al., 2009), but these methods use rules to handle polarity reversal, and whether polarity reversal occurs or not cannot be learned from labeled data. Statistical machine learning can learn useful information from training data and generally robust for noisy data, and using it instead of rigid rules seems useful. Wilson et al. (2005) proposed a method for sentiment classiﬁcation which utilizes head-modiﬁer relation and machine learning. However, the method is based on bag-of-features and polarity reversal occurred by content words is not handled. One issue of the approach to use senten"
N10-1120,I08-1039,0,0.252237,"t classiﬁcation, sentiment polarities can be reversed. For example, let us consider the sentence “The medicine kills cancer cells.” While the phrase cancer cells has negative polarity, the word kills reverses the polarity, and the whole sentence has positive polarity. Thus, in sentiment classiﬁcation, a sentence which contains positive (or negative) polarity words does not necessarily have the same polarity as a whole, and we need to consider interactions between words instead of handling words independently. Recently, several methods have been proposed to cope with the problem (Zaenen, 2004; Ikeda et al., 2008). However, these methods are based on ﬂat bag-of-features representation, and do not consider syntactic structures which seem essential to infer the polarity of a whole sentence. Other methods have been proposed which utilize composition of sentences (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Jia et al., 2009), but these methods use rules to handle polarity reversal, and whether polarity reversal occurs or not cannot be learned from labeled data. Statistical machine learning can learn useful information from training data and generally robust for noisy data, and using it instead of rig"
N10-1120,P06-2059,0,0.00796876,"and sj are equal, we set the initial parameter λi of the feature to a random number in [0.9, 1.1], otherwise we set to a random number in [−0.1, 0.1]7 . By setting such initial values, the initial model parameters have a property that two phrases with head-modiﬁer relation tend to have the same polarity, which is intuitively reasonable. 3 Experiments We conducted experiments of sentiment classiﬁcation on four Japanese corpora and four English corpora. 3.1 Data We used four corpora for experiments of Japanese sentiment classiﬁcation: the Automatically Constructed Polarity-tagged corpus (ACP) (Kaji and Kitsuregawa, 2006), the Kyoto University and NTT Blog corpus (KNB) 8 , the NTCIR Japanese opinion corpus (NTC-J) (Seki et al., 2007; Seki et al., 2008), the 50 Topics Evaluative Information corpus (50 Topics) (Nakagawa et al., 2008). The ACP corpus is an automatically constructed corpus from HTML documents on the Web using lexico-syntactic patterns and layout structures. The size of the corpus is large (it consists of 650,951 instances), and we used 1/100 of the whole corpus. The KNB corpus consists of Japanese blogs, and is manually annotated. The NTC-J corpus consists of Japanese newspaper articles. There are"
N10-1120,W02-1011,0,0.0398013,"ideration of interactions between the hidden variables. Sum-product belief propagation is used for inference. Experimental results of sentiment classiﬁcation for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features. 1 Introduction Sentiment classiﬁcation is a useful technique for analyzing subjective information in a large number of texts, and many studies have been conducted (Pang and Lee, 2008). A typical approach for sentiment classiﬁcation is to use supervised machine learning algorithms with bag-of-words as features (Pang et al., 2002), which is widely used in topic-based text classiﬁcation. In the approach, a subjective sentence is represented as a set of words in the sentence, ignoring word order and head-modiﬁer relation between words. However, sentiment classiﬁcation is different from traditional topic-based text classiﬁcation. Topic-based text classiﬁcation is generally a linearly separable problem ((Chakrabarti, 2002), p.168). For example, when a document contains some domain-speciﬁc words, the document will probably belong to the domain. However, in sentiment classiﬁcation, sentiment polarities can be reversed. For e"
N10-1120,W96-0213,0,0.0529099,"form, coarse-grained part-of-speech (POS) tag, 789 si si &qi si &qi &ri si &ui,1 , · · · , si &ui,mi si &ci,1 , · · · , si &ci,mi si &fi,1 , · · · , si &fi,mi si &ui,1 &ui,2 , · · · , si &ui,mi −1 &ui,mi si &bi,1 &bi,2 , · · · , si &bi,mi −1 &bi,mi Edge Features si &sj si &sj &rj si &sj &rj &qj si &sj &bi,1 , · · · , si &sj &bi,mi si &sj &bj,1 , · · · , si &sj &bj,mj Table 1: Features Used in This Study ﬁne-grained POS tag of the k-th word in the i-th phrase. We used the morphological analysis system JUMAN and the dependency parser KNP2 for processing Japanese data, and the POS tagger MXPOST (Ratnaparkhi, 1996) and the dependency parser MaltParser3 for English data. KNP outputs phrase-based dependency trees, but MaltParser outputs word-based dependency trees, and we converted the word-based ones to phrase-based ones using simple heuristic rules explained in Appendix A. The prior polarity of a phrase qi ∈ {+1, 0, −1} is the innate sentiment polarity of a word contained in the phrase, which can be obtained from sentiment polarity dictionaries. We used sentiment polarity dictionaries made by Kobayashi et al. (2007) and Higashiyama et al. (2008)4 for Japanese experiments (The resulting dictionary contai"
N10-1120,C08-1106,0,0.00864051,"ical sentiment classiﬁcation method incorporating head-modiﬁer relation. Ikeda et al. (2008) proposed a machine learning approach to handle sentiment polarity reversal. For each word with prior polarity, whether the polarity is reversed or not is learned with a statistical learning algorithm using its surrounding words as features. The method can handle only words with prior polarities, and does not use syntactic dependency structures. Conditional random ﬁelds with hidden variables have been studied so far for other tasks. LatentDynamic Conditional Random Fields (LDCRF) (Morency et al., 2007; Sun et al., 2008) are probabilistic models with hidden variables for sequential labeling, and belief propagation is used for inference. Out method is similar to the models, but there are several differences. In our method, only one variable which represents the polarity of the whole sentence is observable, and dependency relation among random variables is not a linear chain but a tree structure which is identical to the syntactic dependency. 5 Conclusion In this paper, we presented a dependency tree-based method for sentiment classiﬁcation using conditional random ﬁelds with hidden variables. In this method, t"
N10-1120,H05-1044,0,0.326676,"s are based on ﬂat bag-of-features representation, and do not consider syntactic structures which seem essential to infer the polarity of a whole sentence. Other methods have been proposed which utilize composition of sentences (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Jia et al., 2009), but these methods use rules to handle polarity reversal, and whether polarity reversal occurs or not cannot be learned from labeled data. Statistical machine learning can learn useful information from training data and generally robust for noisy data, and using it instead of rigid rules seems useful. Wilson et al. (2005) proposed a method for sentiment classiﬁcation which utilizes head-modiﬁer relation and machine learning. However, the method is based on bag-of-features and polarity reversal occurred by content words is not handled. One issue of the approach to use sentence composition and machine learning is that only the whole sentence is labeled with its polarity in general corpora for sentiment classiﬁcation, and each component of the sentence is not labeled, though such information is necessary for supervised ma786 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of"
N16-1099,P14-1062,0,0.119321,"Missing"
N16-1099,D15-1166,0,0.0412953,"Missing"
N16-1099,W10-0911,0,0.0306102,"available at https://github. com/soskek/der-network 1 Raw Highlight ! ""Iron Man"" star Robert Downey Jr. presents a young child with a bionic arm! Context ! ( @entity1 ) @entity0 may be @entity2 in the popular @entity4 superhero ﬁlms , but he recently dealt in some advanced bionic technology himself . @entity0 recently presented a robotic arm to young @entity7 , a @entity8 boy who is missing his right arm from just above his elbow . the arm was made by @entity12 , a …! Query ! "" [X] "" star @entity0 presents a young child with a bionic arm! Answer @entity2! Introduction Machine reading systems (Poon et al., 2010; Richardson et al., 2013) can be tested on their ability to answer queries about contents of documents that they read, thus a central problem is how the information of documents should be organized in the system and retrieved by the queries. Recently, large scale datasets of document-queryanswer triples have been constructed from online newspaper articles and their summaries (Hermann et al., 2015), by replacing named entities in the summaries with placeholders to form Cloze (Taylor, 1953) style questions (Figure 1). These datasets have enabled training and testing of complicated neural networ"
N16-1099,D13-1020,0,0.0514996,"//github. com/soskek/der-network 1 Raw Highlight ! ""Iron Man"" star Robert Downey Jr. presents a young child with a bionic arm! Context ! ( @entity1 ) @entity0 may be @entity2 in the popular @entity4 superhero ﬁlms , but he recently dealt in some advanced bionic technology himself . @entity0 recently presented a robotic arm to young @entity7 , a @entity8 boy who is missing his right arm from just above his elbow . the arm was made by @entity12 , a …! Query ! "" [X] "" star @entity0 presents a young child with a bionic arm! Answer @entity2! Introduction Machine reading systems (Poon et al., 2010; Richardson et al., 2013) can be tested on their ability to answer queries about contents of documents that they read, thus a central problem is how the information of documents should be organized in the system and retrieved by the queries. Recently, large scale datasets of document-queryanswer triples have been constructed from online newspaper articles and their summaries (Hermann et al., 2015), by replacing named entities in the summaries with placeholders to form Cloze (Taylor, 1953) style questions (Figure 1). These datasets have enabled training and testing of complicated neural network models of hypothesized m"
N16-1099,D15-1044,0,0.00800371,"Missing"
N18-1015,D10-1051,0,0.0310598,"tion, the Pseudo-melody model outperformed all other models. These results indicate our pseudo data learning strategy contributes to generating high-quality lyrics. However, the quality of lyrics automatically generated is still worse than the quality of lyrics that humans produce, and it still remains an open challenge for future research to develop computational models that generate high-quality lyrics. 7 Related work In the literature, a broad range of research efforts has been reported for computationally modeling lyrics-specific properties such as meter, rhythm, rhyme, stress, and accent Greene et al. (2010); Reddy and Knight (2011); Watanabe et al. (2014, 2016). While these studies provide insightful findings on the properties of lyrics, none of those takes the approach of using melody-lyrics parallel data for modeling correlations of lyrics and melody structures. One exception is the work of Nichols et al. (2009), who used melody-lyrics parallel data to investigate, for example, the correlation between syllable stress and pitch; however, their exploration covers only correlations at the prosody level but not structural correlations. The same trend can be seen also in the literature of automatic"
N18-1015,W04-3230,0,0.154129,"Missing"
N18-1015,D15-1221,0,0.280906,"ic-specific properties such as the structure of melody, rhythms, etc. (Austin et al., 2010; Ueda, 2010). A simple example is the correlation between word boundaries in lyrics and the rests in a melody. As shown in Figure 1, a single word spanning beyond a long melody rest can sound unnatural. When writing lyrics, a lyricist must consider such constraints in content and lexical selection, which can impose extra cognitive loads. This consideration when writing lyrics has motivated a wide-range of studies for the task of computer-assisted lyrics writing (Barbieri et al., 2012; Abe and Ito, 2012; Potash et al., 2015; Watanabe et al., 2017). Such studies aim to model the melody to constraints, a language model that automatically captures the relationship between lyrics and melody is required. Some studies (Oliveira et al., 2007; Oliveira, 2015; Nichols et al., 2009) have quantitatively analyzed the correlations between melody and phonological aspects of lyrics (e.g., the relationship between a beat and a syllable stress). However, these studies do not address the relationship between melody and the discourse structure of lyrics. Lyrics are not just a sequence of syllables but a meaningful sequence of word"
N18-1015,W09-2006,0,0.0108388,"dy is considered only indirectly; namely, input prosodic/linguistic constraints/preferences on lyrics are assumed to be manually provided by a human user because the proposed models are not capable of interpreting and transforming a given melody to constraints/preferences. For generating lyrics for a given melody, we have so far found in the literature two studies which propose a method. Oliveira et al. (2007) and Oliveira (2015) manually analyze correlations among melodies, beats, and syllables using 42 Portuguese songs and propose a set of heuristic rules for lyrics generation. Ramakrishnan A et al. (2009) attempt to induce a statistical model for generating melodic Tamil lyrics from melody-lyrics parallel data using only ten songs. However, the former captures only phonological aspects of melody-lyrics correlations and can generate a small fragment of lyrics (not an entire lyrics) for a given piece of melody. The latter suffers from the severe shortage of data and fails to conduct empirical experiments. 8 Conclusion and future work This paper has presented a novel data-driven approach for building a melody-conditioned lyrics language model. We created a 1,000-song melodylyrics alignment datase"
N18-1015,P11-2014,0,0.177785,"y model outperformed all other models. These results indicate our pseudo data learning strategy contributes to generating high-quality lyrics. However, the quality of lyrics automatically generated is still worse than the quality of lyrics that humans produce, and it still remains an open challenge for future research to develop computational models that generate high-quality lyrics. 7 Related work In the literature, a broad range of research efforts has been reported for computationally modeling lyrics-specific properties such as meter, rhythm, rhyme, stress, and accent Greene et al. (2010); Reddy and Knight (2011); Watanabe et al. (2014, 2016). While these studies provide insightful findings on the properties of lyrics, none of those takes the approach of using melody-lyrics parallel data for modeling correlations of lyrics and melody structures. One exception is the work of Nichols et al. (2009), who used melody-lyrics parallel data to investigate, for example, the correlation between syllable stress and pitch; however, their exploration covers only correlations at the prosody level but not structural correlations. The same trend can be seen also in the literature of automatic lyrics generation, where"
N18-1015,Y14-1049,1,0.865371,"other models. These results indicate our pseudo data learning strategy contributes to generating high-quality lyrics. However, the quality of lyrics automatically generated is still worse than the quality of lyrics that humans produce, and it still remains an open challenge for future research to develop computational models that generate high-quality lyrics. 7 Related work In the literature, a broad range of research efforts has been reported for computationally modeling lyrics-specific properties such as meter, rhythm, rhyme, stress, and accent Greene et al. (2010); Reddy and Knight (2011); Watanabe et al. (2014, 2016). While these studies provide insightful findings on the properties of lyrics, none of those takes the approach of using melody-lyrics parallel data for modeling correlations of lyrics and melody structures. One exception is the work of Nichols et al. (2009), who used melody-lyrics parallel data to investigate, for example, the correlation between syllable stress and pitch; however, their exploration covers only correlations at the prosody level but not structural correlations. The same trend can be seen also in the literature of automatic lyrics generation, where most studies utilize o"
N18-2073,I08-6014,0,0.0964496,"Missing"
N18-2073,P14-2080,1,0.726088,"rge-Scale CLIR Dataset Table 1: CLIR dataset statistics. For each language X, we show the total number of documents in language X and the number of English queries. The number of ”most relevant” documents is by definition equal to #Query. The number of ”slightly relevant” documents is shown in the column #SR. We construct a large-scale CLIR data from Wikipedia. The idea is to exploit inter-language links: from an English page, we extract a sentence as query, and label the linked foreign-document pages as relevant. See Figure 1 for an illustration. This data construction process is similar to (Schamoni et al., 2014) who made an EnglishGerman CLIR dataset, but ours is at a larger scale. Specifically, we use Wikipedia dumps released on August 23, 2017. English queries are obtained by extracting the first sentence of every English Wikipedia article. The intuition is that the first sentence is usually a well-defined summary of its corresponding article and should be thematically related for articles linked to it from another language. Similar to (Schamoni et al., 2014), title words from the query sentences are removed, because they may be present across different language editions. This deletion prevents the"
N18-2073,D13-1175,0,0.254848,"Missing"
N19-1132,W14-1702,0,0.0308879,"Missing"
N19-1132,C08-1022,0,0.0148737,"Missing"
N19-1132,N06-2015,0,0.0355683,"rpora evaluation so that researchers in the community can adequately and easily evaluate their models based on multiple corpora. 1 2 Related Work We are motivated by the issue of robustness in the parsing community. This field pre1 https://github.com/tomo-wb/GEC_CCE 1309 Proceedings of NAACL-HLT 2019, pages 1309–1314 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics viously focused on improving parsing accuracy on Penn Treebank (Marcus et al., 1993). However, robustness was largely improved by evaluation using multiple corpora including Ontonotes (Hovy et al., 2006) and Google Web Treebank (Petrov and McDonald, 2012). A situation similar to this might also occur in GEC. In other words, evaluation in GEC has relied heavily on the CoNLL-2014 benchmark, which implies that the field is overdeveloping on this dataset. Other corpora are used for evaluation, such as KJ (Mizumoto et al., 2012) and JFLEG (Sakaguchi et al., 2017; Junczys-Dowmunt et al., 2018; Chollampatt and Ng, 2018; Ge et al., 2018; Xie et al., 2018). However, these corpora still depend on one or at most two corpora. 3 Experimental Setup 3.1 Corpora for Evaluation Cross-corpora evaluation is dis"
N19-1132,W14-1703,0,0.0172498,"grammatical errors in a given text, which is typically written by non-native speakers. Previous studies focused on typical errors such as those in the use of articles (Han et al., 2006), prepositions (Felice and Pulman, 2008), and noun numbers (Nagata et al., 2006). Machine translation approaches are being presently applied for GEC (Junczys-Dowmunt et al., 2018; Chollampatt and Ng, 2018; Ge et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2016). In these approaches, GEC is treated as a translation problem from the erroneous text to the correct text (Mizumoto et al., 2012; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). However, the evaluation of GEC performance is unfortunately not complete because researchers tend to evaluate their models on a single corpus. The CoNLL-2014 shared task dataset (Ng et al., 2014) has been recently used for such evaluation. Single-corpus evaluation may be insufficient in cases wherein a GEC model generally aims to robustly correct grammatical errors in any written text partly because the task difficulty varies depending on proficiency levels and essay topics. Although a model outperforms a baseline in one corpus, the model in another corpus may perform better, leading to diff"
N19-1132,D16-1161,0,0.213477,"ably vary depending on the corpus, indicating that single-corpus evaluation is insufficient for GEC models. 1 Introduction Grammatical error correction (GEC) is the task of correcting various grammatical errors in a given text, which is typically written by non-native speakers. Previous studies focused on typical errors such as those in the use of articles (Han et al., 2006), prepositions (Felice and Pulman, 2008), and noun numbers (Nagata et al., 2006). Machine translation approaches are being presently applied for GEC (Junczys-Dowmunt et al., 2018; Chollampatt and Ng, 2018; Ge et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2016). In these approaches, GEC is treated as a translation problem from the erroneous text to the correct text (Mizumoto et al., 2012; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). However, the evaluation of GEC performance is unfortunately not complete because researchers tend to evaluate their models on a single corpus. The CoNLL-2014 shared task dataset (Ng et al., 2014) has been recently used for such evaluation. Single-corpus evaluation may be insufficient in cases wherein a GEC model generally aims to robustly correct grammatical errors in any written text partly because the"
N19-1132,N18-1055,0,0.123383,"LE, and KJ). Evaluation results reveal that the models’ rankings considerably vary depending on the corpus, indicating that single-corpus evaluation is insufficient for GEC models. 1 Introduction Grammatical error correction (GEC) is the task of correcting various grammatical errors in a given text, which is typically written by non-native speakers. Previous studies focused on typical errors such as those in the use of articles (Han et al., 2006), prepositions (Felice and Pulman, 2008), and noun numbers (Nagata et al., 2006). Machine translation approaches are being presently applied for GEC (Junczys-Dowmunt et al., 2018; Chollampatt and Ng, 2018; Ge et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2016). In these approaches, GEC is treated as a translation problem from the erroneous text to the correct text (Mizumoto et al., 2012; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). However, the evaluation of GEC performance is unfortunately not complete because researchers tend to evaluate their models on a single corpus. The CoNLL-2014 shared task dataset (Ng et al., 2014) has been recently used for such evaluation. Single-corpus evaluation may be insufficient in cases wherein a GEC model generally"
N19-1132,J93-2004,0,0.0665821,"ls. • We empirically show that the single-corpus evaluation may be unreliable. • Our source code is published for crosscorpora evaluation so that researchers in the community can adequately and easily evaluate their models based on multiple corpora. 1 2 Related Work We are motivated by the issue of robustness in the parsing community. This field pre1 https://github.com/tomo-wb/GEC_CCE 1309 Proceedings of NAACL-HLT 2019, pages 1309–1314 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics viously focused on improving parsing accuracy on Penn Treebank (Marcus et al., 1993). However, robustness was largely improved by evaluation using multiple corpora including Ontonotes (Hovy et al., 2006) and Google Web Treebank (Petrov and McDonald, 2012). A situation similar to this might also occur in GEC. In other words, evaluation in GEC has relied heavily on the CoNLL-2014 benchmark, which implies that the field is overdeveloping on this dataset. Other corpora are used for evaluation, such as KJ (Mizumoto et al., 2012) and JFLEG (Sakaguchi et al., 2017; Junczys-Dowmunt et al., 2018; Chollampatt and Ng, 2018; Ge et al., 2018; Xie et al., 2018). However, these corpora stil"
N19-1132,C12-2084,1,0.922799,"tion (GEC) is the task of correcting various grammatical errors in a given text, which is typically written by non-native speakers. Previous studies focused on typical errors such as those in the use of articles (Han et al., 2006), prepositions (Felice and Pulman, 2008), and noun numbers (Nagata et al., 2006). Machine translation approaches are being presently applied for GEC (Junczys-Dowmunt et al., 2018; Chollampatt and Ng, 2018; Ge et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2016). In these approaches, GEC is treated as a translation problem from the erroneous text to the correct text (Mizumoto et al., 2012; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). However, the evaluation of GEC performance is unfortunately not complete because researchers tend to evaluate their models on a single corpus. The CoNLL-2014 shared task dataset (Ng et al., 2014) has been recently used for such evaluation. Single-corpus evaluation may be insufficient in cases wherein a GEC model generally aims to robustly correct grammatical errors in any written text partly because the task difficulty varies depending on proficiency levels and essay topics. Although a model outperforms a baseline in one corpus, t"
N19-1132,I17-2062,0,0.0135585,"ne 7, 2019. 2019 Association for Computational Linguistics viously focused on improving parsing accuracy on Penn Treebank (Marcus et al., 1993). However, robustness was largely improved by evaluation using multiple corpora including Ontonotes (Hovy et al., 2006) and Google Web Treebank (Petrov and McDonald, 2012). A situation similar to this might also occur in GEC. In other words, evaluation in GEC has relied heavily on the CoNLL-2014 benchmark, which implies that the field is overdeveloping on this dataset. Other corpora are used for evaluation, such as KJ (Mizumoto et al., 2012) and JFLEG (Sakaguchi et al., 2017; Junczys-Dowmunt et al., 2018; Chollampatt and Ng, 2018; Ge et al., 2018; Xie et al., 2018). However, these corpora still depend on one or at most two corpora. 3 Experimental Setup 3.1 Corpora for Evaluation Cross-corpora evaluation is discussed herein using six corpora, namely CoNLL-2014, CoNLL2013, FCE, JFLEG, KJ, and ICNALE. The following conditions were considered when selecting corpora: • The corpus must be used at least once in the GEC community. • Based on the hypothesis that writers’ proficiency affects the error distribution of any given text, we add a corpus with relatively low prof"
N19-1132,N18-1057,0,0.0334087,"ccuracy on Penn Treebank (Marcus et al., 1993). However, robustness was largely improved by evaluation using multiple corpora including Ontonotes (Hovy et al., 2006) and Google Web Treebank (Petrov and McDonald, 2012). A situation similar to this might also occur in GEC. In other words, evaluation in GEC has relied heavily on the CoNLL-2014 benchmark, which implies that the field is overdeveloping on this dataset. Other corpora are used for evaluation, such as KJ (Mizumoto et al., 2012) and JFLEG (Sakaguchi et al., 2017; Junczys-Dowmunt et al., 2018; Chollampatt and Ng, 2018; Ge et al., 2018; Xie et al., 2018). However, these corpora still depend on one or at most two corpora. 3 Experimental Setup 3.1 Corpora for Evaluation Cross-corpora evaluation is discussed herein using six corpora, namely CoNLL-2014, CoNLL2013, FCE, JFLEG, KJ, and ICNALE. The following conditions were considered when selecting corpora: • The corpus must be used at least once in the GEC community. • Based on the hypothesis that writers’ proficiency affects the error distribution of any given text, we add a corpus with relatively low proficiency (KJ) compared to CoNLL2014. We explicitly describe each learner corpus as follows: C"
N19-1132,P11-1019,0,0.43776,"because the task difficulty varies depending on proficiency levels and essay topics. Although a model outperforms a baseline in one corpus, the model in another corpus may perform better, leading to different conclusions from what we know. This study explores the necessity of performing cross-corpora evaluation for GEC models. The performance of four recent models, namely three neural machine translation (NMT)based models (LSTM, CNN, and transformer) and a statistical machine translation (SMT)based model is evaluated against six learner corpora (CoNLL-2014, CoNLL-2013 (Ng et al., 2013), FCE (Yannakoudakis et al., 2011), JFLEG (Napoles et al., 2017), KJ (Nagata et al., 2011), and ICNLAE (Ishikawa, 2013)). Evaluation results show that the models’ rankings considerably vary depending on the corpus. Empirical results reveal that models must be evaluated using multiple corpora from different perspectives. The contributions of this study are as follows: • We first explore the necessity of performing cross-corpora evaluation for GEC models. • We empirically show that the single-corpus evaluation may be unreliable. • Our source code is published for crosscorpora evaluation so that researchers in the community can a"
N19-1132,I11-1017,1,0.819977,"s. The word embedding size is set to 1024 dimensions, and the size of position-wise feed-forward networks is set to 4096 dimensions at each inner layer. SMT: We essentially follow the idea used in a previous study (Junczys-Dowmunt and Grundkiewicz, 2016), with some key differences. Specifically, we only use English Wikipedia for language model training and only the NUS Corpus of Learner English (NUCLE) and the Lang-8 Learner Corpora (Lang-8) for translation model training to make the experimental settings equal in all models. 3.3 Experimental Settings We use two public datasets, namely Lang8 (Mizumoto et al., 2011) and NUCLE (Dahlmeier et al., 2013), for training. Our pre-processing and experimental setup is similar to that reported previously (Chollampatt and Ng, 2018). In particular, a subset of NUCLE (5.4K) is utilized as the development data for selecting the model; the remaining subset (1.3M) is utilized as the training data. All the models are trained, tuned, and tested in the same way. The models are tested on each test data shown in Table 1. As an evaluation metric, we use F0.5 score computed by applying the MaxMatch scorer (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015). We determine t"
N19-1132,P06-1031,1,0.281203,"SMT-based model, against various learner corpora (CoNLL-2013, CoNLL-2014, FCE, JFLEG, ICNALE, and KJ). Evaluation results reveal that the models’ rankings considerably vary depending on the corpus, indicating that single-corpus evaluation is insufficient for GEC models. 1 Introduction Grammatical error correction (GEC) is the task of correcting various grammatical errors in a given text, which is typically written by non-native speakers. Previous studies focused on typical errors such as those in the use of articles (Han et al., 2006), prepositions (Felice and Pulman, 2008), and noun numbers (Nagata et al., 2006). Machine translation approaches are being presently applied for GEC (Junczys-Dowmunt et al., 2018; Chollampatt and Ng, 2018; Ge et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2016). In these approaches, GEC is treated as a translation problem from the erroneous text to the correct text (Mizumoto et al., 2012; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). However, the evaluation of GEC performance is unfortunately not complete because researchers tend to evaluate their models on a single corpus. The CoNLL-2014 shared task dataset (Ng et al., 2014) has been recently used for su"
N19-1132,P11-1121,1,0.925199,"vels and essay topics. Although a model outperforms a baseline in one corpus, the model in another corpus may perform better, leading to different conclusions from what we know. This study explores the necessity of performing cross-corpora evaluation for GEC models. The performance of four recent models, namely three neural machine translation (NMT)based models (LSTM, CNN, and transformer) and a statistical machine translation (SMT)based model is evaluated against six learner corpora (CoNLL-2014, CoNLL-2013 (Ng et al., 2013), FCE (Yannakoudakis et al., 2011), JFLEG (Napoles et al., 2017), KJ (Nagata et al., 2011), and ICNLAE (Ishikawa, 2013)). Evaluation results show that the models’ rankings considerably vary depending on the corpus. Empirical results reveal that models must be evaluated using multiple corpora from different perspectives. The contributions of this study are as follows: • We first explore the necessity of performing cross-corpora evaluation for GEC models. • We empirically show that the single-corpus evaluation may be unreliable. • Our source code is published for crosscorpora evaluation so that researchers in the community can adequately and easily evaluate their models based on mult"
N19-1132,P15-2097,0,0.315781,"namely Lang8 (Mizumoto et al., 2011) and NUCLE (Dahlmeier et al., 2013), for training. Our pre-processing and experimental setup is similar to that reported previously (Chollampatt and Ng, 2018). In particular, a subset of NUCLE (5.4K) is utilized as the development data for selecting the model; the remaining subset (1.3M) is utilized as the training data. All the models are trained, tuned, and tested in the same way. The models are tested on each test data shown in Table 1. As an evaluation metric, we use F0.5 score computed by applying the MaxMatch scorer (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015). We determine the average F0.5 and average GLEU scores of the four models, which are trained with different random initializations, following a previously reported approach (Chollampatt and Ng, 2018). 4 Cross-Corpora Evaluation Figure 1 shows the performance of each model sorted from best to worst based on their F0.5 score, revealing that the performance substantially varies depending on the corpus. For example, the performance of the transformer ranges from the score of F0.5 , which is as low as 36.20 on CoNLL-2013, to as high as 60.06 on JFLEG. Notably, their rankings also considerably vary"
N19-1132,E17-2037,0,0.742535,"depending on proficiency levels and essay topics. Although a model outperforms a baseline in one corpus, the model in another corpus may perform better, leading to different conclusions from what we know. This study explores the necessity of performing cross-corpora evaluation for GEC models. The performance of four recent models, namely three neural machine translation (NMT)based models (LSTM, CNN, and transformer) and a statistical machine translation (SMT)based model is evaluated against six learner corpora (CoNLL-2014, CoNLL-2013 (Ng et al., 2013), FCE (Yannakoudakis et al., 2011), JFLEG (Napoles et al., 2017), KJ (Nagata et al., 2011), and ICNLAE (Ishikawa, 2013)). Evaluation results show that the models’ rankings considerably vary depending on the corpus. Empirical results reveal that models must be evaluated using multiple corpora from different perspectives. The contributions of this study are as follows: • We first explore the necessity of performing cross-corpora evaluation for GEC models. • We empirically show that the single-corpus evaluation may be unreliable. • Our source code is published for crosscorpora evaluation so that researchers in the community can adequately and easily evaluate"
N19-1132,N12-1067,0,\N,Missing
N19-1132,W13-1703,0,\N,Missing
N19-1353,N09-1003,0,0.186925,"Missing"
N19-1353,Q17-1010,0,0.755952,"ty degradation across several linguistic benchmark datasets, and can simultaneously predict effective embeddings of OOV words. We also demonstrate the effectiveness of our reconstruction method when we apply them to downstream tasks1 . 1 Introduction Pre-trained word embeddings (or embedding vectors), especially those trained on a vast amount of text data, such as the Common Crawl (CC) corpus2 , are now considered as highly beneficial, fundamental language resources. Typical examples of large, well-trained word embeddings are those trained on the CC corpus with 600 billion tokens by fastText (Bojanowski et al., 2017) and with 840 billion tokens by GloVe (Pennington et al., 2014), which we refer to as fastText.600B3 and GloVe.840B4 , respectively. In fact, we often 1 Our code and reconstructed subword-based word embeddings trained from GloVe.840B and fastText.600B are available: https://github.com/losyer/ compact_reconstruction 2 http://commoncrawl.org 3 https://fasttext.cc/docs/en/ english-vectors.html 4 https://nlp.stanford.edu/projects/ glove/ leverage such word embeddings to further improve the task performance of many natural language processing (NLP) tasks, such as constituency parsing (Suzuki et al."
N19-1353,D15-1075,0,0.0128679,"r reconstruction methods outperformed BoS, which was the previous stateof-the-art method, with substantial improvements by 2-6 points. Moreover, KVQ-FH achieved the best performance in this comparison. 6.3 Evaluation on downstream tasks To investigate the effectiveness of our reconstucted embeddings in downstream tasks, we evaluated them in the named entity recognition (NER) and the textual entailment (TE) tasks. 6.3.1 Settings Evaluation data: We used the CoNLL 2003 dataset (Tjong Kim Sang and De Meulder, 2003) for an NER experiment and the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) for a TE experiment. Other settings: We used fastText.600B and GloVe.840B as the target word embeddings for the reconstruction. For our reconstruction embeddings, we calculated the embeddings of all the words in the datasets, thus there exist no OOV words when using our methods. We used AllenNLP10 to train base NER and TE models. We basically used the provided hyperparameter values in their repository for both training and testing. Additionally, we added one hyperparameter K to re-scale embeddings (i.e., multiply all the elements in the embeddings by K) 10 https://allennlp.org/ 7 Conclusion W"
N19-1353,P18-1068,0,0.0145534,"ively. In fact, we often 1 Our code and reconstructed subword-based word embeddings trained from GloVe.840B and fastText.600B are available: https://github.com/losyer/ compact_reconstruction 2 http://commoncrawl.org 3 https://fasttext.cc/docs/en/ english-vectors.html 4 https://nlp.stanford.edu/projects/ glove/ leverage such word embeddings to further improve the task performance of many natural language processing (NLP) tasks, such as constituency parsing (Suzuki et al., 2018; G´omez-Rodr´ıguez and Vilares, 2018), discourse parsing (Yu et al., 2018), semantic parsing (Groschwitz et al., 2018; Dong and Lapata, 2018), and semantic role labeling (Strubell et al., 2018). Despite their significant impact on the NLP community, well-trained word embeddings still have several disadvantages. In this paper, we focus on two issues surrounding well-trained word embeddings: i) the massive memory requirement and ii) the inapplicability of out-of-vocabulary (OOV) words. It is crucial to address such issues, especially when applying them to real-world open systems. The total number of embeddings (i.e., the total memory requirement of such word embeddings) often becomes unacceptably large, especially in limited-memory e"
N19-1353,D18-1162,0,0.0246395,"Missing"
N19-1353,P18-1170,0,0.049615,"Missing"
N19-1353,D17-1030,0,0.0172065,"ord embedding research, which several researches have recently attempted to solve. For example, methods that leverage subword information, such as character N -grams (including character unigrams) (Bojanowski et al., 2017; Pinter et al., 2017; Zhao et al., 2018) and morphological features (Luong et al., 2013), have recently been discussed as means of constructing word embeddings that consider the applicability of OOV words. Moreover, Pilehvar and Collier (2017) have proposed a method called SemLand, which induces OOV word embeddings by leveraging external resources. Bahdanau et al. (2017) and Herbelot and Baroni (2017) have also proposed methods that estimate OOV word embeddings using an additional LSTM and leveraging a small additional dataset, respectively. Among them, the study most closely related to ours is that of Zhao et al. (2018). Their basic idea is to reconstruct each pre-trained word embedding using a bag-of-character N -grams. We refer to their method as ‘BoS’. The motivation for reconstructing pre-trained word embeddings and utilizing character N -grams in our approach is substantially the same, however, an essential difference from BoS is that we additionally consider jointly reducing the tot"
N19-1353,J15-4004,0,0.0664027,"Missing"
N19-1353,P12-1092,0,0.0657688,"o a subword index. Then, we introduce a key-valuequery (KVQ) self-attention operation inspired by 3502 data number of OOV data abbre. size fastText.600B GloVe.840B Word similarity estimation (WordSim) MEN 3,000 0 0 M&C 30 0 0 287 0 0 MTurk RW 2,034 37 36 65 0 0 R&G SCWS 2,003 2 2 SLex 998 0 0 WSR 252 0 0 WSS 203 0 0 Word analogy estimation (Analogy) GL 19,544 0 0 MSYN 8,000 1000 1000 Table 2: Evaluation datasets used in our experiments. MEM (Bruni et al., 2014), M&C (Miller and Charles, 1991), MTurk (Radinsky et al., 2011), RW (Luong et al., 2013), R&G (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), SLex (Hill et al., 2014), WSR and WSS (Agirre et al., 2009), GL (Mikolov et al., 2013a), and MSYN (Mikolov et al., 2013b). method hyper-parameters fastText.600B SUM-F F = 0.5M SUM-H H = 0.5M KVQ-H H = 0.5M SUM-FH F = 1.0M H = 0.5M KVQ-FH F = 1.0M H = 0.5M SUM-F F = 0.2M SUM-H H = 0.2M KVQ-H H = 0.2M SUM-FH F = 1.0M H = 0.2M KVQ-FH F = 1.0M H = 0.2M |W| 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M 2M |S |size (GB) – 2.23GB 0.5M 0.59GB 21.8M 0.59GB 21.8M 0.59GB 1.0M 0.59GB 1.0M 0.59GB 0.2M 0.23GB 21.8M 0.23GB 21.8M 0.23GB 1.0M 0.23GB 1.0M 0.23GB Table 3: Statistics for our methods. benchmark datasets, i.e.,"
N19-1353,W13-3512,0,0.550308,"structed embeddings for representing the embeddings of OOV words. Lastly, we confirm the performance of our reconstructed embeddings on several downstream tasks from the named entity recognition task and the textual entailment task. 2 Related Work The OOV word issue is one of the widely discussed topics in word embedding research, which several researches have recently attempted to solve. For example, methods that leverage subword information, such as character N -grams (including character unigrams) (Bojanowski et al., 2017; Pinter et al., 2017; Zhao et al., 2018) and morphological features (Luong et al., 2013), have recently been discussed as means of constructing word embeddings that consider the applicability of OOV words. Moreover, Pilehvar and Collier (2017) have proposed a method called SemLand, which induces OOV word embeddings by leveraging external resources. Bahdanau et al. (2017) and Herbelot and Baroni (2017) have also proposed methods that estimate OOV word embeddings using an additional LSTM and leveraging a small additional dataset, respectively. Among them, the study most closely related to ours is that of Zhao et al. (2018). Their basic idea is to reconstruct each pre-trained word e"
N19-1353,N13-1090,0,0.132949,"Missing"
N19-1353,D14-1162,0,0.10704,"d can simultaneously predict effective embeddings of OOV words. We also demonstrate the effectiveness of our reconstruction method when we apply them to downstream tasks1 . 1 Introduction Pre-trained word embeddings (or embedding vectors), especially those trained on a vast amount of text data, such as the Common Crawl (CC) corpus2 , are now considered as highly beneficial, fundamental language resources. Typical examples of large, well-trained word embeddings are those trained on the CC corpus with 600 billion tokens by fastText (Bojanowski et al., 2017) and with 840 billion tokens by GloVe (Pennington et al., 2014), which we refer to as fastText.600B3 and GloVe.840B4 , respectively. In fact, we often 1 Our code and reconstructed subword-based word embeddings trained from GloVe.840B and fastText.600B are available: https://github.com/losyer/ compact_reconstruction 2 http://commoncrawl.org 3 https://fasttext.cc/docs/en/ english-vectors.html 4 https://nlp.stanford.edu/projects/ glove/ leverage such word embeddings to further improve the task performance of many natural language processing (NLP) tasks, such as constituency parsing (Suzuki et al., 2018; G´omez-Rodr´ıguez and Vilares, 2018), discourse parsing"
N19-1353,N18-1202,0,0.034374,"ing techniques. Our method can also be interpreted as a kind of parameter reduction method based on the subword features. However, their method only considers the model shrinkage, and does not utilize any subword information nor consider the OOV issue. To summarize, none of the previous studies have attempted to simultaneously achieve a smaller number of embedding vectors and higher applicability of OOV words. Thus, in this paper, we report the first attempt to investigate how we 3499 can simultaneously achieve them. Additionally, deep contextualized pre-trained language models, such as ELMo (Peters et al., 2018), have recently been proposed as alternatives to the pre-trained word embeddings to further improve task performances. However, ELMo still takes advantage of Glove.840B to achieve its state-of-the-art performance. This fact implies that we can still combine the pre-trained word embeddings with strong pre-trained language models; thus, the importance of word embeddings in the literature remains unchanged even though stronger pre-trained models have been established. RD×|Iw |. Then, we assume that the following relation always holds between ew and E: ew = E[ze ] where ze = ζ(w). (2) Therefore, t"
N19-1353,E17-2062,0,0.0205981,"nstream tasks from the named entity recognition task and the textual entailment task. 2 Related Work The OOV word issue is one of the widely discussed topics in word embedding research, which several researches have recently attempted to solve. For example, methods that leverage subword information, such as character N -grams (including character unigrams) (Bojanowski et al., 2017; Pinter et al., 2017; Zhao et al., 2018) and morphological features (Luong et al., 2013), have recently been discussed as means of constructing word embeddings that consider the applicability of OOV words. Moreover, Pilehvar and Collier (2017) have proposed a method called SemLand, which induces OOV word embeddings by leveraging external resources. Bahdanau et al. (2017) and Herbelot and Baroni (2017) have also proposed methods that estimate OOV word embeddings using an additional LSTM and leveraging a small additional dataset, respectively. Among them, the study most closely related to ours is that of Zhao et al. (2018). Their basic idea is to reconstruct each pre-trained word embedding using a bag-of-character N -grams. We refer to their method as ‘BoS’. The motivation for reconstructing pre-trained word embeddings and utilizing"
N19-1353,D17-1010,0,0.158576,"nd analogy tasks. We also demonstrate the effectiveness of our reconstructed embeddings for representing the embeddings of OOV words. Lastly, we confirm the performance of our reconstructed embeddings on several downstream tasks from the named entity recognition task and the textual entailment task. 2 Related Work The OOV word issue is one of the widely discussed topics in word embedding research, which several researches have recently attempted to solve. For example, methods that leverage subword information, such as character N -grams (including character unigrams) (Bojanowski et al., 2017; Pinter et al., 2017; Zhao et al., 2018) and morphological features (Luong et al., 2013), have recently been discussed as means of constructing word embeddings that consider the applicability of OOV words. Moreover, Pilehvar and Collier (2017) have proposed a method called SemLand, which induces OOV word embeddings by leveraging external resources. Bahdanau et al. (2017) and Herbelot and Baroni (2017) have also proposed methods that estimate OOV word embeddings using an additional LSTM and leveraging a small additional dataset, respectively. Among them, the study most closely related to ours is that of Zhao et al"
N19-1353,D18-1548,0,0.0217056,"Missing"
N19-1353,P18-2097,1,0.826503,"et al., 2017) and with 840 billion tokens by GloVe (Pennington et al., 2014), which we refer to as fastText.600B3 and GloVe.840B4 , respectively. In fact, we often 1 Our code and reconstructed subword-based word embeddings trained from GloVe.840B and fastText.600B are available: https://github.com/losyer/ compact_reconstruction 2 http://commoncrawl.org 3 https://fasttext.cc/docs/en/ english-vectors.html 4 https://nlp.stanford.edu/projects/ glove/ leverage such word embeddings to further improve the task performance of many natural language processing (NLP) tasks, such as constituency parsing (Suzuki et al., 2018; G´omez-Rodr´ıguez and Vilares, 2018), discourse parsing (Yu et al., 2018), semantic parsing (Groschwitz et al., 2018; Dong and Lapata, 2018), and semantic role labeling (Strubell et al., 2018). Despite their significant impact on the NLP community, well-trained word embeddings still have several disadvantages. In this paper, we focus on two issues surrounding well-trained word embeddings: i) the massive memory requirement and ii) the inapplicability of out-of-vocabulary (OOV) words. It is crucial to address such issues, especially when applying them to real-world open systems. The total numb"
N19-1353,C18-1047,0,0.0177485,"which we refer to as fastText.600B3 and GloVe.840B4 , respectively. In fact, we often 1 Our code and reconstructed subword-based word embeddings trained from GloVe.840B and fastText.600B are available: https://github.com/losyer/ compact_reconstruction 2 http://commoncrawl.org 3 https://fasttext.cc/docs/en/ english-vectors.html 4 https://nlp.stanford.edu/projects/ glove/ leverage such word embeddings to further improve the task performance of many natural language processing (NLP) tasks, such as constituency parsing (Suzuki et al., 2018; G´omez-Rodr´ıguez and Vilares, 2018), discourse parsing (Yu et al., 2018), semantic parsing (Groschwitz et al., 2018; Dong and Lapata, 2018), and semantic role labeling (Strubell et al., 2018). Despite their significant impact on the NLP community, well-trained word embeddings still have several disadvantages. In this paper, we focus on two issues surrounding well-trained word embeddings: i) the massive memory requirement and ii) the inapplicability of out-of-vocabulary (OOV) words. It is crucial to address such issues, especially when applying them to real-world open systems. The total number of embeddings (i.e., the total memory requirement of such word embedding"
N19-1353,D18-1059,0,0.361126,"also demonstrate the effectiveness of our reconstructed embeddings for representing the embeddings of OOV words. Lastly, we confirm the performance of our reconstructed embeddings on several downstream tasks from the named entity recognition task and the textual entailment task. 2 Related Work The OOV word issue is one of the widely discussed topics in word embedding research, which several researches have recently attempted to solve. For example, methods that leverage subword information, such as character N -grams (including character unigrams) (Bojanowski et al., 2017; Pinter et al., 2017; Zhao et al., 2018) and morphological features (Luong et al., 2013), have recently been discussed as means of constructing word embeddings that consider the applicability of OOV words. Moreover, Pilehvar and Collier (2017) have proposed a method called SemLand, which induces OOV word embeddings by leveraging external resources. Bahdanau et al. (2017) and Herbelot and Baroni (2017) have also proposed methods that estimate OOV word embeddings using an additional LSTM and leveraging a small additional dataset, respectively. Among them, the study most closely related to ours is that of Zhao et al. (2018). Their basi"
P06-1079,W04-3239,1,0.887184,"Missing"
P06-1079,J94-4002,0,0.200142,"logy 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {ryu-i,inui,matsu}@is.naist.jp Abstract can not be interpreted only by shallow syntactic parsing, a model specialized for zero-anaphora resolution needs to be devised on the top of shallow syntactic and semantic processing. Recent work on zero-anaphora resolution can be located in two different research contexts. First, zero-anaphora resolution is studied in the context of anaphora resolution (AR), in which zeroanaphora is regarded as a subclass of anaphora. In AR, the research trend has been shifting from rulebased approaches (Baldwin, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumu"
P06-1079,W97-1303,0,0.0566686,"oma, Nara, 630-0192, Japan {ryu-i,inui,matsu}@is.naist.jp Abstract can not be interpreted only by shallow syntactic parsing, a model specialized for zero-anaphora resolution needs to be devised on the top of shallow syntactic and semantic processing. Recent work on zero-anaphora resolution can be located in two different research contexts. First, zero-anaphora resolution is studied in the context of anaphora resolution (AR), in which zeroanaphora is regarded as a subclass of anaphora. In AR, the research trend has been shifting from rulebased approaches (Baldwin, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura,"
P06-1079,C96-2137,0,0.0779105,"n, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al., 2003). Second, the task of zero-anaphora resolution has some overlap with Propbank3 -style semantic role labeling (SRL), which has been intensively studied, for example, in the context of the CoNLL SRL task4 . In this task, given a sentence “To attract younger listeners, Radio Free Europe intersperses the latest in Western rock groups”, an SRL We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zero"
P06-1079,P04-1020,0,0.119987,"e are two alternative ways for anaphoricity determination: the single-step model and the two-step model. The single-step model (Soon et al., 2001; Ng and Cardie, 2002a) determines the anaphoricity of a given anaphor indirectly as a by-product of the search for its antecedent. If an appropriate candidate antecedent is found, the anaphor is classified as anaphoric; otherwise, it is classified as non-anaphoric. One disadvantage of this model is that it cannot employ the preferencebased model because the preference-based model is not capable of identifying non-anaphoric cases. The two-step model (Ng, 2004; Poesio et al., 2004; Iida et al., 2005), on the other hand, carries out anaphoricity determination in a separate step from antecedent identification. Poesio et al. (2004) and Iida et al. (2005) claim that the latter subtask should be done before the former. For example, given a target anaphor (TA), Iida et al.’s selection-then-classification model: 1. selects the most likely candidate antecedent (CA) of TA using the tournament model, 2. classifies TA paired with CA as either anaphoric or non-anaphoric using an anaphoricity determination model. If the CA-TA pair is classified as anaphoric, CA"
P06-1079,P02-1014,0,0.493031,"nly by shallow syntactic parsing, a model specialized for zero-anaphora resolution needs to be devised on the top of shallow syntactic and semantic processing. Recent work on zero-anaphora resolution can be located in two different research contexts. First, zero-anaphora resolution is studied in the context of anaphora resolution (AR), in which zeroanaphora is regarded as a subclass of anaphora. In AR, the research trend has been shifting from rulebased approaches (Baldwin, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al."
P06-1079,C96-2147,0,0.0563355,"1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al., 2003). Second, the task of zero-anaphora resolution has some overlap with Propbank3 -style semantic role labeling (SRL), which has been intensively studied, for example, in the context of the CoNLL SRL task4 . In this task, given a sentence “To attract younger listeners, Radio Free Europe intersperses the latest in Western rock groups”, an SRL We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution. For t"
P06-1079,W04-0707,0,0.0690539,"unity has recently made two important findings: • A model that identifies the antecedent of an anaphor by a series of comparisons between candidate antecedents has a remarkable advantage over a model that estimates the absolute likelihood of each candidate independently of other candidates (Iida et al., 2003; Yang et al., 2003). • An AR model that carries out antecedent identification before anaphoricity determination, the decision whether a given NP is anaphoric or not (i.e. discourse-new), significantly outperforms a model that executes those subtasks in the reverse order or simultaneously (Poesio et al., 2004; Iida et al., 2005). 2 Zero-anaphora resolution In this paper, we consider only zero-pronouns that function as an obligatory argument of a predicate for two reasons: • Providing a clear definition of zero-pronouns appearing in adjunctive argument positions involves awkward problems, which we believe should be postponed until obligatory zero-anaphora is well studied. • Resolving obligatory zero-anaphora tends to be more important than adjunctive zeropronouns in actual applications. A zero-pronoun may have its antecedent in the discourse; in this case, we say the zero-pronoun is anaphoric. On t"
P06-1079,C02-1078,0,0.701082,"ardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al., 2003). Second, the task of zero-anaphora resolution has some overlap with Propbank3 -style semantic role labeling (SRL), which has been intensively studied, for example, in the context of the CoNLL SRL task4 . In this task, given a sentence “To attract younger listeners, Radio Free Europe intersperses the latest in Western rock groups”, an SRL We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution. For the former problem, syntactic patterns of the appearance of zero-pronouns and their antecedents are"
P06-1079,J01-4004,0,0.746746,"ic parsing, a model specialized for zero-anaphora resolution needs to be devised on the top of shallow syntactic and semantic processing. Recent work on zero-anaphora resolution can be located in two different research contexts. First, zero-anaphora resolution is studied in the context of anaphora resolution (AR), in which zeroanaphora is regarded as a subclass of anaphora. In AR, the research trend has been shifting from rulebased approaches (Baldwin, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al.,"
P06-1079,P03-1022,0,0.0257275,"Missing"
P06-1079,P03-1005,0,0.0206942,"earning algorithm As noted in Section 1, the use of zero-pronouns in Japanese is relatively less constrained by syntax compared, for example, with English. This forces the above way of encoding path information to produce an explosive number of different paths, which inevitably leads to serious data sparseness. This issue can be addressed in several ways. The SRL community has devised a range of variants of the standard path representation to reduce the complexity (Carreras and Marquez, 2005). Applying Kernel methods such as Tree kernels (Collins and Duffy, 2001) and Hierarchical DAG kernels (Suzuki et al., 2003) is another strong option. The Boosting-based algorithm pro• A path is represented by a subtree consisting of backbone nodes: φ (zero-pronoun), Ant (antecedent), Node (the lowest common ancestor), LeftNode (left-branch node) and RightNode. • Each backbone node has daughter nodes, each corresponding to a function word associated with it. • Content words are deleted. This way of encoding syntactic patterns is used in intra-sentential anaphoricity determination. In antecedent identification, on the other hand, the tournament model allows us to incorporate three paths, a path for each pair of a ze"
P06-1079,W05-0620,0,0.0407031,"s anaphoric as its antecedent, ‘shusho (prime minister)’, appears in the same sentence. In sentence (2), on the other hand, φj is considered non-anaphoric if its referent (i.e. the first person) does not appear in the discourse. To our best knowledge, however, existing SRL models do not exploit these advantages. In SRL, on the other hand, it is common to use syntactic features derived from the parse tree of a given input sentence for argument identification. A typical syntactic feature is the path on a parse tree from a target predicate to a noun phrase in question (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005). However, existing AR models deal with intra- and inter-sentential anaphoric relations in a uniform manner; that is, they do not use as rich syntactic features as state-of-the-art SRL models do, even in finding intra-sentential anaphoric relations. We believe that the AR and SRL communities can learn more from each other. Given this background, in this paper, we show that combining the aforementioned techniques derived from each research trend makes significant impact on zero-anaphora resolution, taking Japanese as a target language. More specifically, we demonstrate the following: • Incorpor"
P06-1079,J02-3001,0,0.0236012,"ence (1), zero-pronoun φi is anaphoric as its antecedent, ‘shusho (prime minister)’, appears in the same sentence. In sentence (2), on the other hand, φj is considered non-anaphoric if its referent (i.e. the first person) does not appear in the discourse. To our best knowledge, however, existing SRL models do not exploit these advantages. In SRL, on the other hand, it is common to use syntactic features derived from the parse tree of a given input sentence for argument identification. A typical syntactic feature is the path on a parse tree from a target predicate to a noun phrase in question (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005). However, existing AR models deal with intra- and inter-sentential anaphoric relations in a uniform manner; that is, they do not use as rich syntactic features as state-of-the-art SRL models do, even in finding intra-sentential anaphoric relations. We believe that the AR and SRL communities can learn more from each other. Given this background, in this paper, we show that combining the aforementioned techniques derived from each research trend makes significant impact on zero-anaphora resolution, taking Japanese as a target language. More specifically, we demonstr"
P06-1079,J95-2003,0,0.0943109,"Missing"
P06-1079,W03-2604,1,0.954084,"et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al., 2003). Second, the task of zero-anaphora resolution has some overlap with Propbank3 -style semantic role labeling (SRL), which has been intensively studied, for example, in the context of the CoNLL SRL task4 . In this task, given a sentence “To attract younger listeners, Radio Free Europe intersperses the latest in Western rock groups”, an SRL We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution. For the former problem, syntactic patterns of the appearance of zero-pronouns and their antecedents are useful clues. Taking"
P06-1079,P86-1031,0,0.0835096,"proaches (Baldwin, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al., 2003). Second, the task of zero-anaphora resolution has some overlap with Propbank3 -style semantic role labeling (SRL), which has been intensively studied, for example, in the context of the CoNLL SRL task4 . In this task, given a sentence “To attract younger listeners, Radio Free Europe intersperses the latest in Western rock groups”, an SRL We approach the zero-anaphora resolution problem by decomposing it into intra-sentential"
P06-1079,P03-1023,0,\N,Missing
P09-1073,D08-1069,0,0.336351,"didate in sentence Si . In this situation, for example, candidate c12 is labeled as retained when creating training instances for sentence S1 , but labeled as discarded from S2 onwards, because of the appearance of its zeropronoun. Another candidate c13 which is never referred to in the text is labeled as discarded for all training instances. Second, we need to capture the ‘relative’ salience of candidates appearing in the current discourse for each cache update, as also exploited in the tournament-based or ranking-based approaches to anaphora resolution (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). To solve it, we use a ranker trained on the instances created as described above. In order to train the ranker, we adopt the Ranking SVM algorithm (Joachims, 2002), which learns a weight vector to rank candidates for a given partial ranking of each discourse entity. Each training instance is created from the set of retained candidates, Ri , paired with the set of discarded candidates, Di , in each sentence. To Figure 2: Pseudo-code for creating training instances training instances S1 c11 c12 c13 c14 S 2 c21 c22 c23 φ φ i j S3 c31 c32 c33 φk φ retained c11 c12 discarded c13 c14 retained c11"
P09-1073,J86-3001,0,0.408154,"entity. Each training instance is created from the set of retained candidates, Ri , paired with the set of discarded candidates, Di , in each sentence. To Figure 2: Pseudo-code for creating training instances training instances S1 c11 c12 c13 c14 S 2 c21 c22 c23 φ φ i j S3 c31 c32 c33 φk φ retained c11 c12 discarded c13 c14 retained c11 c22 discarded c12 c13 c14 c21 c23 l Figure 3: Creating training instnaces define the partial ranking of candidates, we simply rank candidates in Ri as first place and candidates in Di as second place. 4.2 Static cache model Other research on discourse such as Grosz and Sidner (1986) has studied global focus, which generally refers to the entity or set of entities that are salient throughout the entire discourse. Since global focus may not be captured by Centeringbased models, we also propose another cache model which directly captures the global salience of a text. To train the model, all the candidates in a text which have an inter-sentential anaphoric relation with zero-pronouns are used as positive instances and the others used as negative ones. Unlike the 650 Table 1: Feature set used in the cache models Feature Description POS Part-of-speech of C followed by IPADIC4"
P09-1073,J95-2003,0,0.832228,"machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-based approach to zero-anaphora resolution searches for an antecedent in the set of candidates appearing in all the preceding contexts. However, computational time makes this approach largely infeasible for long texts. An alternative approach is to heuristically limit the search space (e.g. the system deals with candidates only occurring in the N previous sentences). Various research such as Yang et al. (2008) has ad"
P09-1073,P97-1014,0,0.20415,"to be excluded from target candidate antecedents. On the other hand, rule-based methods derived from theoretical background such as Centering Theory (Grosz et al., 1995) only deal with the salient discourse entities at each point of the discourse status. By incrementally updating the discourse status, the set of candidates in question is automatically limited. Although these methods have a theoretical advantage, they have a serious drawback in that Centering Theory only retains information about the previous sentence. A few methods have attempted to overcome this fault (Suri and McCoy, 1994; Hahn and Strube, 1997), but they are overly dependent upon the restrictions fundamental to the notion of centering. We hope that by relaxing such restrictions it will be possible for an anaphora resolution system to achieve a good balance between accuracy and computational cost. From this background, we focus on the issue of reducing candidate antecedents (discourse entities) for a given anaphor. Inspired by Walker’s argument (Walker, 1996), we propose a machine learning-based caching mechanism that captures the most salient candidates at each point of the discourse for efficient anaphora resolution. More specifica"
P09-1073,W03-2604,1,0.850397,"in Figure 3, where cij is the j-th candidate in sentence Si . In this situation, for example, candidate c12 is labeled as retained when creating training instances for sentence S1 , but labeled as discarded from S2 onwards, because of the appearance of its zeropronoun. Another candidate c13 which is never referred to in the text is labeled as discarded for all training instances. Second, we need to capture the ‘relative’ salience of candidates appearing in the current discourse for each cache update, as also exploited in the tournament-based or ranking-based approaches to anaphora resolution (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). To solve it, we use a ranker trained on the instances created as described above. In order to train the ranker, we adopt the Ranking SVM algorithm (Joachims, 2002), which learns a weight vector to rank candidates for a given partial ranking of each discourse entity. Each training instance is created from the set of retained candidates, Ri , paired with the set of discarded candidates, Di , in each sentence. To Figure 2: Pseudo-code for creating training instances training instances S1 c11 c12 c13 c14 S 2 c21 c22 c23 φ φ i j S3 c31 c32 c33 φk φ r"
P09-1073,W07-1522,1,0.905738,"irical evaluation with Japanese newspaper articles shows that the number of candidate antecedents for each zero-pronoun can be dramatically reduced while preserving the accuracy of resolving it. 1 Introduction There have been recently increasing concerns with the need for anaphora resolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from dis"
P09-1073,W03-1024,0,0.0170854,"nd to the most salient discourse entities within the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). In this paper, we consider only zero-pronouns that function as an obligatory argument of a predicate. A zero-pronoun may or may not have its antecedent in the discourse; in the case it do"
P09-1073,P86-1031,0,0.568605,"ion and then Section 3 gives an overview of previous work. Next, in Section 4 we propose a machine learning-based cache model. Section 5 presents the antecedent identification and anaphoricity determination models used in the experiments. To evaluate the model, we conduct several empirical evaluations and report their results in Section 6. Finally, we conclude and discuss the future direction of this research in Section 7. 2 Zero-anaphora resolution 3 Previous work Early methods for zero-anaphora resolution were developed with rule-based approaches in mind. Theory-oriented rule-based methods (Kameyama, 1986; Walker et al., 1994), for example, focus on the Centering Theory (Grosz et al., 1995) and are designed to collect the salient candidate antecedents in the forward-looking center (Cf ) list, and then choose the most salient candidate, Cp, as an antecedent of a zero-pronoun according to heuristic rules (e.g. topic > subject > indirect object > direct object > others1 ). Although these methods have a theoretical advantage, they have a serious drawback in that the original Centering Theory is restricted to keeping information about the previous sentence only. In order to loosen this restriction,"
P09-1073,W04-3239,1,0.836643,"r or not the model using a small number of discourse entities in the cache achieves performance comparable to the original one in a practical setting. For intra-sentential zero-anaphora resolution, we adopt the model proposed by Iida et al. (2007a), which exploits syntactic patterns as features that appear in the dependency path of a zero-pronoun and its candidate antecedent. Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004). We illustrated the recall-precision curve of each model by altering the threshold parameter of intrasentential anaphoricity determination, which is shown in Figure 5. The results show that all models achieved almost the same performance when decreasing the cache size. It indicates that it is enough to cache a small number of the most salient References candidates in the current zero-anaphora resolution C. Aone and S. W. Bennett. 1995. Evaluating automated and manual acquisition of anaphora resolution strategies. model, while coverage decreases when the cache In Proceedings of 33th Annual Mee"
P09-1073,2002.tmi-papers.15,0,0.0271594,"antecedents when appearing zero-pronouns refer to an antecedent in a preceding sentence, i.e. we evaluate the cases of inter-sentential anaphora resolution. 6.2 Evaluation of the caching mechanism In this experiment, we directly compare the proAs a baseline, we adopt the following two cache posed static and dynamic cache models with the models. One is the Centering-derived model which heuristic methods presented in Section 2. Note that only stores the preceding ‘wa’ (topic)-marked or 652 ‘ga’ (subject)-marked candidate antecedents in the cache. It is an approximation of the model proposed by Nariyama (2002) for extending the local focus transition defined by Centering Theory. We henceforth call this model the centering-based cache model. The other baseline model stores candidates appearing in the N previous sentences of a zero-pronoun to simulate a heuristic approach used in works like Soon et al. (2001). We call this model the sentence-based cache model. By comparing these baselines with our cache models, we can see whether our models contribute to more efficiently storing salient candidates or not. The above dynamic cache model retains the salient candidates independently of the results of ant"
P09-1073,P02-1014,0,0.667329,"e need for anaphora resolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-based approach to zero"
P09-1073,D08-1068,0,0.0470475,"nt candidates. Our empirical evaluation on Japanese zero-anaphora resolution shows that our learningbased cache model drastically reduces the search space while preserving accuracy. The procedure for zero-anaphora resolution adopted in our model assumes that resolution is carried out linearly, i.e. an antecedent is independently selected without taking into account any other zero-pronouns. However, trends in anaphora resolution have shifted from such linear approaches to more sophisticated ones which globally optimize the interpretation of all the referring expressions in a text. For example, Poon and Domingos (2008) has empirically reported that such global approaches achieve performance better than the ones based on incrementally processing a text. Because their work basically builds on inductive logic programing, we can naturally extend this to incorporate our caching mechanism into the global optimization by expressing cache constraints as predicate logic, which is one of our next challenges in this research area. 6.4 Overall zero-anaphora resolution We finally investigate the effects of introducing the proposed model on overall zero-anaphora resolution including intra-sentential cases. The resolution"
P09-1073,C02-1078,0,0.0197281,"f Cp, that correspond to the most salient discourse entities within the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). In this paper, we consider only zero-pronouns that function as an obligatory argument of a predicate. A zero-pronoun may or may not have its antecedent in the dis"
P09-1073,J01-4004,0,0.924987,"ng concerns with the need for anaphora resolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-b"
P09-1073,J94-2006,0,0.330217,"ts anaphor, causing it to be excluded from target candidate antecedents. On the other hand, rule-based methods derived from theoretical background such as Centering Theory (Grosz et al., 1995) only deal with the salient discourse entities at each point of the discourse status. By incrementally updating the discourse status, the set of candidates in question is automatically limited. Although these methods have a theoretical advantage, they have a serious drawback in that Centering Theory only retains information about the previous sentence. A few methods have attempted to overcome this fault (Suri and McCoy, 1994; Hahn and Strube, 1997), but they are overly dependent upon the restrictions fundamental to the notion of centering. We hope that by relaxing such restrictions it will be possible for an anaphora resolution system to achieve a good balance between accuracy and computational cost. From this background, we focus on the issue of reducing candidate antecedents (discourse entities) for a given anaphor. Inspired by Walker’s argument (Walker, 1996), we propose a machine learning-based caching mechanism that captures the most salient candidates at each point of the discourse for efficient anaphora re"
P09-1073,J94-2003,0,0.121523,"tion 3 gives an overview of previous work. Next, in Section 4 we propose a machine learning-based cache model. Section 5 presents the antecedent identification and anaphoricity determination models used in the experiments. To evaluate the model, we conduct several empirical evaluations and report their results in Section 6. Finally, we conclude and discuss the future direction of this research in Section 7. 2 Zero-anaphora resolution 3 Previous work Early methods for zero-anaphora resolution were developed with rule-based approaches in mind. Theory-oriented rule-based methods (Kameyama, 1986; Walker et al., 1994), for example, focus on the Centering Theory (Grosz et al., 1995) and are designed to collect the salient candidate antecedents in the forward-looking center (Cf ) list, and then choose the most salient candidate, Cp, as an antecedent of a zero-pronoun according to heuristic rules (e.g. topic > subject > indirect object > direct object > others1 ). Although these methods have a theoretical advantage, they have a serious drawback in that the original Centering Theory is restricted to keeping information about the previous sentence only. In order to loosen this restriction, the Centering-based m"
P09-1073,J96-2005,0,0.412033,"rious drawback in that Centering Theory only retains information about the previous sentence. A few methods have attempted to overcome this fault (Suri and McCoy, 1994; Hahn and Strube, 1997), but they are overly dependent upon the restrictions fundamental to the notion of centering. We hope that by relaxing such restrictions it will be possible for an anaphora resolution system to achieve a good balance between accuracy and computational cost. From this background, we focus on the issue of reducing candidate antecedents (discourse entities) for a given anaphor. Inspired by Walker’s argument (Walker, 1996), we propose a machine learning-based caching mechanism that captures the most salient candidates at each point of the discourse for efficient anaphora resolution. More specifically, we choose salient candidates for each sentence from the set of candidates appearing in that sentence and the candidates which are already 647 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 647–655, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP in the cache. Searching only through the set of salient candidates, the computational cost of zeroanaphora resolution is"
P09-1073,P03-1023,0,0.080586,"cij is the j-th candidate in sentence Si . In this situation, for example, candidate c12 is labeled as retained when creating training instances for sentence S1 , but labeled as discarded from S2 onwards, because of the appearance of its zeropronoun. Another candidate c13 which is never referred to in the text is labeled as discarded for all training instances. Second, we need to capture the ‘relative’ salience of candidates appearing in the current discourse for each cache update, as also exploited in the tournament-based or ranking-based approaches to anaphora resolution (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). To solve it, we use a ranker trained on the instances created as described above. In order to train the ranker, we adopt the Ranking SVM algorithm (Joachims, 2002), which learns a weight vector to rank candidates for a given partial ranking of each discourse entity. Each training instance is created from the set of retained candidates, Ri , paired with the set of discarded candidates, Di , in each sentence. To Figure 2: Pseudo-code for creating training instances training instances S1 c11 c12 c13 c14 S 2 c21 c22 c23 φ φ i j S3 c31 c32 c33 φk φ retained c11 c12 dis"
P09-1073,P08-1096,0,0.399047,"esolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-based approach to zero-anaphora resolutio"
P09-1073,P95-1017,0,\N,Missing
P09-4001,I08-1025,1,0.821789,"of information credibility. (1) Credibility of information contents, (2) Credibility of the information sender, and (3) Credibility estimated from the document style and superficial characteristics. In order to help people judge the credibility of information from these viewpoints, we have been developing an information analysis system called WISDOM. Figure 1 shows the analysis result of WISDOM on the analysis topic “Is bio-ethanol good for the environment?” Figure 2 shows the system architecture of WISDOM. Given an analysis topic (query), WISDOM sends the query to the search engine TSUBAKI (Shinzato et al., 2008), and TSUBAKI returns a list of the top N relevant Web pages (N is usually set to 1000). Then, those pages are automatically analyzed, and major and contradictory expressions and evaluative expressions are extracted. Furthermore, the information senders of the Web pages, which were analyzed beforehand, are collected and the distribution is calculated. The WISDOM analysis results can be viewed from several viewpoints by changing the tabs using a Web browser. The leftmost tab, “Summary,” shows the summary of the analysis, with major phrases and major/contradictory statements first. http://clusty"
P13-1038,S07-1103,0,0.331928,"illion, 5,200,390) and unit conversions (e.g., square kilometers and acres). 4. We demonstrate the effectiveness of this approach, reporting experimental results and analyses in detail. Although it would be ideal to evaluate the impact of this study on the overall RTE task, we evaluate each phase separately. We do this because the existing RTE data sets tend to exhibit very diverse linguistic phenomena, and it is difficult to employ such data for evaluating the real impact of this study. 2 Related work Some recent studies delve deeper into the semantic interpretation of numerical expressions. Aramaki et al. (2007) focused on the physical size of an entity to predict the semantic relation between entities. For example, knowing that a book has a physical size of 20 cm × 25 cm and that a library has a size of 10 m × 10 m, we can estimate that a library contains a book (content-container relation). Their method acquires knowledge about entity size from the Web (by issuing queries like “book (*cm x *cm)”), and integrates the knowledge as features for the classification of relations. Surprisingly, NLP research has paid little attention to semantic processing of numerical expressions. This is evident when we"
P13-1038,bentivogli-etal-2010-building,0,0.0160476,"ailment recognition (RTE) involves a wide range of semantic inferences to determine whether the meaning of a hypothesis sentence (h) can be inferred from another text (t) (Dagan et al., 2006). Although several evaluation campaigns (e.g., PASCAL/TAC RTE challenges) have made significant progress, the RTE community recognizes the necessity of a deeper understanding of the core phenomena involved in textual inference. Such recognition comes from the ideas that crucial progress may derive from decomposing the complex RTE task into basic phenomena and from solving each basic phenomenon separately (Bentivogli et al., 2010; Sammons et al., 2010; Cabrio and Magnini, 2011; Toledo et al., 2012). 1. We examine instances in existing RTE corpora, categorize them into groups in terms of the necessary semantic inferences, and discuss the impact of this study for solving RTE problems with numerical expressions. 2. We describe a method of normalizing numerical expressions referring to the same amount in text into a unified semantic representation. 3. We present approaches for aggregating numerical common sense from examples of numerical expressions and for judging whether a given amount is large, small, or normal. 382 Pr"
P13-1038,W11-0135,0,0.0119869,"of semantic inferences to determine whether the meaning of a hypothesis sentence (h) can be inferred from another text (t) (Dagan et al., 2006). Although several evaluation campaigns (e.g., PASCAL/TAC RTE challenges) have made significant progress, the RTE community recognizes the necessity of a deeper understanding of the core phenomena involved in textual inference. Such recognition comes from the ideas that crucial progress may derive from decomposing the complex RTE task into basic phenomena and from solving each basic phenomenon separately (Bentivogli et al., 2010; Sammons et al., 2010; Cabrio and Magnini, 2011; Toledo et al., 2012). 1. We examine instances in existing RTE corpora, categorize them into groups in terms of the necessary semantic inferences, and discuss the impact of this study for solving RTE problems with numerical expressions. 2. We describe a method of normalizing numerical expressions referring to the same amount in text into a unified semantic representation. 3. We present approaches for aggregating numerical common sense from examples of numerical expressions and for judging whether a given amount is large, small, or normal. 382 Proceedings of the 51st Annual Meeting of the Asso"
P13-1038,P10-1133,0,0.235393,"uncertainty about the quantity (e.g., weight airbus A380 pounds). Given a query, their approach retrieves documents relevant to the query and identifies the quantities of numerical expressions in the retrieved documents. They also proposed methods for enumerating and ranking the candidates for the consensus quantity intervals. Even though our study shares a similar spirit (modeling of consensus for quantities) with Banerjee et al. (2009), their goal is different: to determine ground-truth values for queries. In question answering, to help “sanity check” answers with numerical values that were Davidov and Rappoport (2010) presented a method for the extraction from the Web and approximation of numerical object attributes such as height and weight. Given an object-attribute pair, the study expands the object into a set of comparable objects and then approximates the numerical values even when no exact value can be found in a text. Aramaki et al. (2007) and Davidov and Rappoport (2010) rely on hand-crafted patterns (e.g., “Object is * [unit] tall”), focusing on a specific set of numerical attributes (e.g., height, weight, size). In contrast, this study can handle any kind of target and situation that is quantifie"
P13-1038,P11-2057,0,0.0339962,"Missing"
P13-1038,W06-1415,0,0.0303271,"ssuing queries like “book (*cm x *cm)”), and integrates the knowledge as features for the classification of relations. Surprisingly, NLP research has paid little attention to semantic processing of numerical expressions. This is evident when we compare with temporal expressions, for which corpora (e.g., ACE20051 , TimeBank2 ) were developed with annotation schemes (e.g., TIMEX3 , TimeML4 ). Several studies deal with numerical expressions in the context of information extraction (Bakalov et al., 2011), information retrieval (Fontoura et al., 2006; Yoshida et al., 2010), and question answering (Moriceau, 2006). Numbers such as product prices and weights have been common targets of information extraction. Fontoura et al. (2006) and Yoshida et al. (2010) presented algorithms and data structures that allow number-range queries for searching documents. However, these studies do not interpret the quantity (e.g., 3,000,000,000) of a numerical expression (e.g., 3b people), but rather treat numerical expressions as strings. Banerjee et al. (2009) focused on quantity consensus queries, in which there is uncertainty about the quantity (e.g., weight airbus A380 pounds). Given a query, their approach retrieves"
P13-1038,P10-1122,0,0.0147428,"involves a wide range of semantic inferences to determine whether the meaning of a hypothesis sentence (h) can be inferred from another text (t) (Dagan et al., 2006). Although several evaluation campaigns (e.g., PASCAL/TAC RTE challenges) have made significant progress, the RTE community recognizes the necessity of a deeper understanding of the core phenomena involved in textual inference. Such recognition comes from the ideas that crucial progress may derive from decomposing the complex RTE task into basic phenomena and from solving each basic phenomenon separately (Bentivogli et al., 2010; Sammons et al., 2010; Cabrio and Magnini, 2011; Toledo et al., 2012). 1. We examine instances in existing RTE corpora, categorize them into groups in terms of the necessary semantic inferences, and discuss the impact of this study for solving RTE problems with numerical expressions. 2. We describe a method of normalizing numerical expressions referring to the same amount in text into a unified semantic representation. 3. We present approaches for aggregating numerical common sense from examples of numerical expressions and for judging whether a given amount is large, small, or normal. 382 Proceedings of the 51st"
P13-1038,N10-4008,0,\N,Missing
P13-3016,W11-2207,0,0.126335,"lly awful” as an example. The complaint gives viewers a negative impression of Company A and can increase the number of people who think the company is bad. Some complaints are expressed by a specific 110 Proceedings of the ACL Student Research Workshop, pages 110–116, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics example. This comment is probably complaining about Company B but not Company A. In contrast, most of the previous work on sentiment analysis in social media does not consider these kinds of problems (Barbosa and Feng, 2010; Davidov et al., 2010; Speriosu et al., 2011). Switching to the behavior of each user, in social media we often see that users who have similar ideas will tend to cooperate with each other. In fact, previous work suggests that users who have the same opinions tend to create links to each other (Conover et al., 2011b; Yang et al., 2012). Because chronic critics share the purpose of attacking some target’s reputation, they may also decide to cooperate. For this reason, to detect chronic critics, we believe that information about the connections among users will be effective. In this paper, we present a method that combines opinion mining b"
P13-3016,C10-2005,0,0.311897,"comment such as “Working for Company A is really awful” as an example. The complaint gives viewers a negative impression of Company A and can increase the number of people who think the company is bad. Some complaints are expressed by a specific 110 Proceedings of the ACL Student Research Workshop, pages 110–116, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics example. This comment is probably complaining about Company B but not Company A. In contrast, most of the previous work on sentiment analysis in social media does not consider these kinds of problems (Barbosa and Feng, 2010; Davidov et al., 2010; Speriosu et al., 2011). Switching to the behavior of each user, in social media we often see that users who have similar ideas will tend to cooperate with each other. In fact, previous work suggests that users who have the same opinions tend to create links to each other (Conover et al., 2011b; Yang et al., 2012). Because chronic critics share the purpose of attacking some target’s reputation, they may also decide to cooperate. For this reason, to detect chronic critics, we believe that information about the connections among users will be effective. In this paper, we p"
P13-3016,C10-2028,0,0.179006,"g for Company A is really awful” as an example. The complaint gives viewers a negative impression of Company A and can increase the number of people who think the company is bad. Some complaints are expressed by a specific 110 Proceedings of the ACL Student Research Workshop, pages 110–116, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics example. This comment is probably complaining about Company B but not Company A. In contrast, most of the previous work on sentiment analysis in social media does not consider these kinds of problems (Barbosa and Feng, 2010; Davidov et al., 2010; Speriosu et al., 2011). Switching to the behavior of each user, in social media we often see that users who have similar ideas will tend to cooperate with each other. In fact, previous work suggests that users who have the same opinions tend to create links to each other (Conover et al., 2011b; Yang et al., 2012). Because chronic critics share the purpose of attacking some target’s reputation, they may also decide to cooperate. For this reason, to detect chronic critics, we believe that information about the connections among users will be effective. In this paper, we present a method that c"
P13-3016,C10-2100,1,0.927109,"proposed method and discusses the experimental results. Section 5 concludes this paper. users. These studies did not identify the target of the polarized sentiment of each comment. Conover et al. (2011a) proposed a method that predicts the political polarity of a social media user based on the connections between users and tags. They demonstrated that label propagation on the graph representing the connections between users is effective. However, this method is not guaranteed to obtain the optimal solution. In contrast, our research uses graph analysis that converges on the optimal solution. Murakami and Raymond (2010) proposed a method that uses the connections between users to predict each user’s opinion, i.e., support or oppose a topic in online debates. They analyzed the content of the discussions to infer the connections. However, in social media, it is difficult to infer connections based on content because of such complexities as incomplete contexts. To address these problem, we analyzed the behavior of the users to predict the connections between users. Our task is similar to spammer detection (Wang, 2010; Yang et al., 2012). Wang (2010) proposed a method using a classifier to detect spammers. They"
P16-1121,D15-1198,0,0.0245378,"stributional information, which is necessary for calculating semantic similarity. Formal Semantics Our model implements a fragment of logic capable of semantic composition, largely due to the simple framework of Dependency-based Compositional Semantics (Liang et al., 2013). It fits in a long tradition of logic-based semantics (Montague, 1970; Dowty et al., 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al., 2001; Copestake et al., 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015). Logic for Natural Language Inference The pursue of a logic more suitable for natural language inference is also not new. For example, MacCartney and Manning (2008) has implemented a model of natural logic (Lakoff, 1970). We would not reach the current formalization of logic of DCS without reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic. Semantic Parsing DCS-related representations have been actively used in semantic parsing and we see potential in applying our model. For example, Berant and Lia"
P16-1121,P02-1041,0,0.0651133,"e previous works did not specify how to integrate contextual distributional information, which is necessary for calculating semantic similarity. Formal Semantics Our model implements a fragment of logic capable of semantic composition, largely due to the simple framework of Dependency-based Compositional Semantics (Liang et al., 2013). It fits in a long tradition of logic-based semantics (Montague, 1970; Dowty et al., 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al., 2001; Copestake et al., 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015). Logic for Natural Language Inference The pursue of a logic more suitable for natural language inference is also not new. For example, MacCartney and Manning (2008) has implemented a model of natural logic (Lakoff, 1970). We would not reach the current formalization of logic of DCS without reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic. Semantic Parsing DCS-related representations have been actively used in semantic parsing and we see potent"
P16-1121,P15-1061,0,0.0147595,"baselines on relation classification (Table 5). It makes 16 errors in misclassifying the direction of a relation, as compared to 144 such errors made by “no matrix”, 23 by “no inverse”, 30 by vecUD, and 161 by GloVe. This suggests that models with syntactic-semantic transformations (i.e. vecDCS, “no inverse”, and vecUD) are indeed good at distinguishing the different roles played by e1 and e2 . VecDCS scores moderately lower than the state-of-the-art (Xu et al., 2015), however we note that these results are achieved by adding additional features and training task-specific neural networks (dos Santos et al., 2015; Xu et al., 2015). Our method only uses features constructed from unlabeled corpora. From this point of view, it is comparable to the MV-RNN model (without features) in Socher et al. (2012), and vecDCS actually does better. Table 4 shows an example of clustered training instances as assessed by cosine similarities between their features. It suggests that the features used in our method can actually cluster similar relations. 5.4 Sentence Completion If vecDCS can compose query vectors of DCS trees, one should be able to “execute” the vectors to get a set of answers, as the original DCS trees c"
P16-1121,J10-4006,0,0.181301,"Missing"
P16-1121,D10-1115,0,0.0239659,"can be concisely illustrated by the DCS tree of “banned drugs” (Figure 1a), which is similar to a dependency tree but possesses precise procedural and logical meaning (Section 2). DCS has been shown useful in question answering (Liang et al., 2013) and textual entailment recognition (Tian et al., 2014). Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words (Turney and Pantel, 2010; Levy et al., 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014). However, less effort is devoted to finding a link between vector-based compositions and the composition operations in any formal semantics. We believe that if a link can be found, then symbolic formulas in the formal semantics will be realized by vectors composed from word embeddings, such that similar things are realized by similar vectors; meanwhile, vectors will acquire formal meanings that can directly be used in execution or inference process. Still, to find a link is challenging becaus"
P16-1121,D11-1129,0,0.296304,"by the DCS tree of “banned drugs” (Figure 1a), which is similar to a dependency tree but possesses precise procedural and logical meaning (Section 2). DCS has been shown useful in question answering (Liang et al., 2013) and textual entailment recognition (Tian et al., 2014). Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words (Turney and Pantel, 2010; Levy et al., 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014). However, less effort is devoted to finding a link between vector-based compositions and the composition operations in any formal semantics. We believe that if a link can be found, then symbolic formulas in the formal semantics will be realized by vectors composed from word embeddings, such that similar things are realized by similar vectors; meanwhile, vectors will acquire formal meanings that can directly be used in execution or inference process. Still, to find a link is challenging because any vector compositions that rea"
P16-1121,2014.lilt-9.5,0,0.0775601,"ntics to build up structured queries. In this section, we discuss several lines of previous research related to this work. 10 http://research.microsoft.com/en-us/ projects/scc/ 1284 Logic and Distributional Semantics Logic is necessary for implementing the functional aspects of meaning and organizing knowledge in a structured and unambiguous way. In contrast, distributional semantics provides an elegant methodology for assessing semantic similarity and is well suited for learning from data. There have been repeated calls for combining the strength of these two approaches (Coecke et al., 2010; Baroni et al., 2014; Liang and Potts, 2015), and several systems (Lewis and Steedman, 2013; Beltagy et al., 2014; Tian et al., 2014) have contributed to this direction. In the remarkable work by Beltagy et al. (to appear), word and phrase similarities are explicitly transformed to weighted logical rules that are used in a probabilistic inference framework. However, this approach requires considerable amount of engineering, including the generation of rule candidates (e.g. by aligning sentence fragments), converting distributional similarities to weights, and efficiently handling the rules and inference. What if"
P16-1121,P14-1114,0,0.0159302,"research related to this work. 10 http://research.microsoft.com/en-us/ projects/scc/ 1284 Logic and Distributional Semantics Logic is necessary for implementing the functional aspects of meaning and organizing knowledge in a structured and unambiguous way. In contrast, distributional semantics provides an elegant methodology for assessing semantic similarity and is well suited for learning from data. There have been repeated calls for combining the strength of these two approaches (Coecke et al., 2010; Baroni et al., 2014; Liang and Potts, 2015), and several systems (Lewis and Steedman, 2013; Beltagy et al., 2014; Tian et al., 2014) have contributed to this direction. In the remarkable work by Beltagy et al. (to appear), word and phrase similarities are explicitly transformed to weighted logical rules that are used in a probabilistic inference framework. However, this approach requires considerable amount of engineering, including the generation of rule candidates (e.g. by aligning sentence fragments), converting distributional similarities to weights, and efficiently handling the rules and inference. What if the distributional representations are equipped with a logical interface, such that the infer"
P16-1121,P14-1133,0,0.0323368,"i et al., 2015; Mineshima et al., 2015). Logic for Natural Language Inference The pursue of a logic more suitable for natural language inference is also not new. For example, MacCartney and Manning (2008) has implemented a model of natural logic (Lakoff, 1970). We would not reach the current formalization of logic of DCS without reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic. Semantic Parsing DCS-related representations have been actively used in semantic parsing and we see potential in applying our model. For example, Berant and Liang (2014) convert λ-DCS queries to canonical utterances and assess paraphrases at the surface level; an alternative could be using vector-based DCS to bring distributional similarity directly into calculation of denotations. We also borrow ideas from previous work, for example our training scheme is similar to Guu et al. (2015) in using paths and composition of matrices, and our method is similar to Poon and Domingos (2009) in building structured knowledge from clustering syntactic parse of unlabeled data. Further Applications Regarding the usability of distributional representations learned by our mod"
P16-1121,D12-1050,0,0.230928,"d to identity (“no matrix”), in order to investigate the effects of meaning changes caused by syntactic-semantic roles and prepositions; (ii) the regularizer enforcing MN−1 to be actually the inverse matrix of MN is set to γ = 0 (“no inverse”), in order to investigate the effects of a semantically motivated constraint; and (iii) applying the same training scheme to UD trees directly, by modeling UD relations as matrices (“vecUD”). In this case, one edge is assigned one UD relation rel, so we implement the transfor1281 vecDCS -no matrix -no inverse vecUD GloVe Grefenstette and Sadrzadeh (2011) Blacoe and Lapata (2012):RAE Grefenstette (2013a) Paperno et al. (2014) Hashimoto et al. (2014):Waddnl Kartsaklis and Sadrzadeh (2014) AN 0.51 0.52 0.47 0.44 0.41 0.31 0.48 - NN 0.49 0.46 0.43 0.46 0.47 0.30 0.40 - VO 0.41 0.42 0.38 0.41 0.41 0.28 0.39 - SVO 0.62 0.62 0.58 0.58 0.60 0.43 GS11 0.29 0.29 0.28 0.25 0.23 0.21 0.34 0.41 GS12 0.33 0.33 0.33 0.25 0.17 0.27 0.36 - Table 3: Spearman’s ρ on phrase similarity mation from child to parent by Mrel , and from par−1 ent to child by Mrel . The same hyper-parameters are used to train vecUD. By comparing vecDCS with vecUD we investigate if applying the semantics framew"
P16-1121,C04-1180,0,0.0490413,"ify how to integrate contextual distributional information, which is necessary for calculating semantic similarity. Formal Semantics Our model implements a fragment of logic capable of semantic composition, largely due to the simple framework of Dependency-based Compositional Semantics (Liang et al., 2013). It fits in a long tradition of logic-based semantics (Montague, 1970; Dowty et al., 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al., 2001; Copestake et al., 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015). Logic for Natural Language Inference The pursue of a logic more suitable for natural language inference is also not new. For example, MacCartney and Manning (2008) has implemented a model of natural logic (Lakoff, 1970). We would not reach the current formalization of logic of DCS without reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic. Semantic Parsing DCS-related representations have been actively used in semantic parsing and we see potential in applying ou"
P16-1121,S13-1001,0,0.491291,"in order to investigate the effects of meaning changes caused by syntactic-semantic roles and prepositions; (ii) the regularizer enforcing MN−1 to be actually the inverse matrix of MN is set to γ = 0 (“no inverse”), in order to investigate the effects of a semantically motivated constraint; and (iii) applying the same training scheme to UD trees directly, by modeling UD relations as matrices (“vecUD”). In this case, one edge is assigned one UD relation rel, so we implement the transfor1281 vecDCS -no matrix -no inverse vecUD GloVe Grefenstette and Sadrzadeh (2011) Blacoe and Lapata (2012):RAE Grefenstette (2013a) Paperno et al. (2014) Hashimoto et al. (2014):Waddnl Kartsaklis and Sadrzadeh (2014) AN 0.51 0.52 0.47 0.44 0.41 0.31 0.48 - NN 0.49 0.46 0.43 0.46 0.47 0.30 0.40 - VO 0.41 0.42 0.38 0.41 0.41 0.28 0.39 - SVO 0.62 0.62 0.58 0.58 0.60 0.43 GS11 0.29 0.29 0.28 0.25 0.23 0.21 0.34 0.41 GS12 0.33 0.33 0.33 0.25 0.17 0.27 0.36 - Table 3: Spearman’s ρ on phrase similarity mation from child to parent by Mrel , and from par−1 ent to child by Mrel . The same hyper-parameters are used to train vecUD. By comparing vecDCS with vecUD we investigate if applying the semantics framework of DCS makes any di"
P16-1121,D13-1143,0,0.0620971,"Missing"
P16-1121,D15-1038,0,0.0335383,"hout reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic. Semantic Parsing DCS-related representations have been actively used in semantic parsing and we see potential in applying our model. For example, Berant and Liang (2014) convert λ-DCS queries to canonical utterances and assess paraphrases at the surface level; an alternative could be using vector-based DCS to bring distributional similarity directly into calculation of denotations. We also borrow ideas from previous work, for example our training scheme is similar to Guu et al. (2015) in using paths and composition of matrices, and our method is similar to Poon and Domingos (2009) in building structured knowledge from clustering syntactic parse of unlabeled data. Further Applications Regarding the usability of distributional representations learned by our model, a strong point is that the representation takes into account syntactic/structural information of context. Unlike several previous models (Pad´o and Lapata, 2007; Levy and Goldberg, 2014; Pham et al., 2015), our approach learns matrices at the same time that can extract the information according to different syntact"
P16-1121,D14-1163,0,0.600236,"cy tree but possesses precise procedural and logical meaning (Section 2). DCS has been shown useful in question answering (Liang et al., 2013) and textual entailment recognition (Tian et al., 2014). Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words (Turney and Pantel, 2010; Levy et al., 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014). However, less effort is devoted to finding a link between vector-based compositions and the composition operations in any formal semantics. We believe that if a link can be found, then symbolic formulas in the formal semantics will be realized by vectors composed from word embeddings, such that similar things are realized by similar vectors; meanwhile, vectors will acquire formal meanings that can directly be used in execution or inference process. Still, to find a link is challenging because any vector compositions that realize such a link must conform to the logic of the formal semantics."
P16-1121,W09-2415,0,0.0986545,"Missing"
P16-1121,P01-1019,0,0.117114,"r calculations that realize logic operations. However, the previous works did not specify how to integrate contextual distributional information, which is necessary for calculating semantic similarity. Formal Semantics Our model implements a fragment of logic capable of semantic composition, largely due to the simple framework of Dependency-based Compositional Semantics (Liang et al., 2013). It fits in a long tradition of logic-based semantics (Montague, 1970; Dowty et al., 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al., 2001; Copestake et al., 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015). Logic for Natural Language Inference The pursue of a logic more suitable for natural language inference is also not new. For example, MacCartney and Manning (2008) has implemented a model of natural logic (Lakoff, 1970). We would not reach the current formalization of logic of DCS without reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic. Semantic Parsing DCS-related representations"
P16-1121,W11-0607,0,0.0601555,"Missing"
P16-1121,P14-2050,0,0.107063,"Missing"
P16-1121,Q15-1016,0,0.0178691,"the results with drug (intersection “∩”). This procedure defined how words can be combined to form a meaning. Better yet, the procedure can be concisely illustrated by the DCS tree of “banned drugs” (Figure 1a), which is similar to a dependency tree but possesses precise procedural and logical meaning (Section 2). DCS has been shown useful in question answering (Liang et al., 2013) and textual entailment recognition (Tian et al., 2014). Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words (Turney and Pantel, 2010; Levy et al., 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014). However, less effort is devoted to finding a link between vector-based compositions and the composition operations in any formal semantics. We believe that if a link can be found, then symbolic formulas in the formal semantics will be realized by vectors composed from word embeddings, such that similar things are realized by similar vectors; meanwh"
P16-1121,Q13-1015,0,0.0260398,"several lines of previous research related to this work. 10 http://research.microsoft.com/en-us/ projects/scc/ 1284 Logic and Distributional Semantics Logic is necessary for implementing the functional aspects of meaning and organizing knowledge in a structured and unambiguous way. In contrast, distributional semantics provides an elegant methodology for assessing semantic similarity and is well suited for learning from data. There have been repeated calls for combining the strength of these two approaches (Coecke et al., 2010; Baroni et al., 2014; Liang and Potts, 2015), and several systems (Lewis and Steedman, 2013; Beltagy et al., 2014; Tian et al., 2014) have contributed to this direction. In the remarkable work by Beltagy et al. (to appear), word and phrase similarities are explicitly transformed to weighted logical rules that are used in a probabilistic inference framework. However, this approach requires considerable amount of engineering, including the generation of rule candidates (e.g. by aligning sentence fragments), converting distributional similarities to weights, and efficiently handling the rules and inference. What if the distributional representations are equipped with a logical interfac"
P16-1121,J13-2005,0,0.169374,"le, DCS can guide building vectors for structured queries that can be directly executed. We evaluate this utility on sentence completion task and report a new state-of-the-art. 1 Introduction A major goal of semantic processing is to map natural language utterances to representations that facilitate calculation of meanings, execution of commands, and/or inference of knowledge. Formal semantics supports such representations by defining words as some functional units and combining them via a specific logic. A simple and illustrative example is the Dependency-based Compositional Semantics (DCS) (Liang et al., 2013). DCS composes meanings from denotations of words (i.e. sets of things to which the words apply); say, the denotations of the concept drug and the event ban is shown in Figure 1b, where drug is a list of drug names and ban is a list of the subjectcomplement pairs in any ban event; then, a list of banned drugs can be constructed by first taking the COMP column of all records in ban (projection “πCOMP ”), and then intersecting the results with drug (intersection “∩”). This procedure defined how words can be combined to form a meaning. Better yet, the procedure can be concisely illustrated by the"
P16-1121,C08-1066,0,0.0252211,"y due to the simple framework of Dependency-based Compositional Semantics (Liang et al., 2013). It fits in a long tradition of logic-based semantics (Montague, 1970; Dowty et al., 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al., 2001; Copestake et al., 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015). Logic for Natural Language Inference The pursue of a logic more suitable for natural language inference is also not new. For example, MacCartney and Manning (2008) has implemented a model of natural logic (Lakoff, 1970). We would not reach the current formalization of logic of DCS without reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic. Semantic Parsing DCS-related representations have been actively used in semantic parsing and we see potential in applying our model. For example, Berant and Liang (2014) convert λ-DCS queries to canonical utterances and assess paraphrases at the surface level; an alternative could be using vector-based DCS to bring distributional similarity directl"
P16-1121,D15-1244,0,0.155487,"Missing"
P16-1121,J07-2002,0,0.177913,"Missing"
P16-1121,D14-1162,0,0.100047,"o words have overlapping context (Tian et al., 2015); therefore, it is suitable to implement an “and” or intersection operation (Section 3). We design our model such that the resulted distributional representations are expected to have additive compositionality. Second, when intersection is realized as addition, it is natural to implement projection as linear mapping, as suggested by the logical interactions between the two operations (Section 3). Experimentally, we show that vectors and matrices learned by our model exhibit favorable characteristics as compared with vectors trained by GloVe (Pennington et al., 2014) or those learned from syntactic dependencies (Section 5.1). Finally, additive composition brings our model a strong ability to calculate similar vectors for similar phrases, whereas syntactic-semantic roles (e.g. SUBJ, COMP) can be distinguished by different projection matrices (e.g. MSUBJ , MCOMP ). We achieve near state-of-the-art performance on a wide range of phrase similarity tasks (Section 5.2) and relation classification (Section 5.3). Furthermore, we show that a vector as constructed above for “banned drugs” can be used as a query vector to retrieve a coarse-grained candidate list of"
P16-1121,P15-1094,0,0.0757346,"Missing"
P16-1121,D09-1001,0,0.0353216,"base semantics in description logic. Semantic Parsing DCS-related representations have been actively used in semantic parsing and we see potential in applying our model. For example, Berant and Liang (2014) convert λ-DCS queries to canonical utterances and assess paraphrases at the surface level; an alternative could be using vector-based DCS to bring distributional similarity directly into calculation of denotations. We also borrow ideas from previous work, for example our training scheme is similar to Guu et al. (2015) in using paths and composition of matrices, and our method is similar to Poon and Domingos (2009) in building structured knowledge from clustering syntactic parse of unlabeled data. Further Applications Regarding the usability of distributional representations learned by our model, a strong point is that the representation takes into account syntactic/structural information of context. Unlike several previous models (Pad´o and Lapata, 2007; Levy and Goldberg, 2014; Pham et al., 2015), our approach learns matrices at the same time that can extract the information according to different syntactic-semantic roles. A related application is selectional preference (Baroni and Lenci, 2010; Lenci,"
P16-1121,W14-2409,0,0.0172808,"butional Models There has been active exploration on how to combine word vectors such that adequate phrase/sentence similarities can be assessed (Mitchell and Lapata, 2010, inter alia), and there is nothing new in using matrices to model changes of meanings. However, previous model designs mostly rely on linguistic intuitions (Paperno et al., 2014, inter alia), whereas our model has an exact logic interpretation. Furthermore, by using additive composition we enjoy a learning guarantee (Tian et al., 2015). Vector-based Logic Models This work also shares the spirit with Grefenstette (2013b) and Rocktaeschel et al. (2014), in exploring vector calculations that realize logic operations. However, the previous works did not specify how to integrate contextual distributional information, which is necessary for calculating semantic similarity. Formal Semantics Our model implements a fragment of logic capable of semantic composition, largely due to the simple framework of Dependency-based Compositional Semantics (Liang et al., 2013). It fits in a long tradition of logic-based semantics (Montague, 1970; Dowty et al., 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representa"
P16-1121,D12-1110,0,0.744461,"(Figure 1a), which is similar to a dependency tree but possesses precise procedural and logical meaning (Section 2). DCS has been shown useful in question answering (Liang et al., 2013) and textual entailment recognition (Tian et al., 2014). Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words (Turney and Pantel, 2010; Levy et al., 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014). However, less effort is devoted to finding a link between vector-based compositions and the composition operations in any formal semantics. We believe that if a link can be found, then symbolic formulas in the formal semantics will be realized by vectors composed from word embeddings, such that similar things are realized by similar vectors; meanwhile, vectors will acquire formal meanings that can directly be used in execution or inference process. Still, to find a link is challenging because any vector compositions that realize such a link must"
P16-1121,P14-1008,1,0.948018,"plement pairs in any ban event; then, a list of banned drugs can be constructed by first taking the COMP column of all records in ban (projection “πCOMP ”), and then intersecting the results with drug (intersection “∩”). This procedure defined how words can be combined to form a meaning. Better yet, the procedure can be concisely illustrated by the DCS tree of “banned drugs” (Figure 1a), which is similar to a dependency tree but possesses precise procedural and logical meaning (Section 2). DCS has been shown useful in question answering (Liang et al., 2013) and textual entailment recognition (Tian et al., 2014). Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words (Turney and Pantel, 2010; Levy et al., 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014). However, less effort is devoted to finding a link between vector-based compositions and the composition operations in any formal semantics. We believe that if a link can be"
P16-1121,D14-1004,0,0.0416244,"Missing"
P16-1121,D15-1062,0,0.0240654,"lassifier is SVM9 with RBF kernel, C = 2 and Γ = 0.25. The hyper-parameters are selected by 5-fold cross validation. Results VecDCS outperforms baselines on relation classification (Table 5). It makes 16 errors in misclassifying the direction of a relation, as compared to 144 such errors made by “no matrix”, 23 by “no inverse”, 30 by vecUD, and 161 by GloVe. This suggests that models with syntactic-semantic transformations (i.e. vecDCS, “no inverse”, and vecUD) are indeed good at distinguishing the different roles played by e1 and e2 . VecDCS scores moderately lower than the state-of-the-art (Xu et al., 2015), however we note that these results are achieved by adding additional features and training task-specific neural networks (dos Santos et al., 2015; Xu et al., 2015). Our method only uses features constructed from unlabeled corpora. From this point of view, it is comparable to the MV-RNN model (without features) in Socher et al. (2012), and vecDCS actually does better. Table 4 shows an example of clustered training instances as assessed by cosine similarities between their features. It suggests that the features used in our method can actually cluster similar relations. 5.4 Sentence Completion"
P16-1121,P12-1063,0,0.0741783,"w that a vector as constructed above for “banned drugs” can be used as a query vector to retrieve a coarse-grained candidate list of banned drugs, by sorting its dot products with answer vectors that are also learned by our model (Figure 1d). This is due to the ability of our approach to provide a language model that can find likely words to fill in the blanks such as “ is a banned drug” or “the drug is banned by . . . ”. A highlight is the calculation being done as if a query is “executed” by the DCS tree of “banned drugs”. We quantitatively evaluate this utility on sentence completion task (Zweig et al., 2012) and report a new state-of-the-art (Section 5.4). 2 DCS Trees DCS composes meanings from denotations, or sets of things to which words apply. A “thing” (i.e. element of a denotation) is represented by a tuple of features of the form Field=Value, with a fixed inventory of fields. For example, a denotation ban might be a set of tuples ban = {(SUBJ=Canada, COMP=Thalidomide), . . .}, in which each tuple records participants of a banning event (e.g. Canada banning Thalidomide). Operations are applied to sets of things to generate new denotations, for modeling semantic composition. An example is the"
P16-1121,P11-1060,0,\N,Missing
P16-1121,P14-1009,0,\N,Missing
P16-1215,S12-1051,0,0.0389772,"ord phrase pairs with semantic similarity judged by human annotators. Korkontzelos et al. (2013) provided a semantic similarity dataset with pairs of two words and a single word. Wieting et al. (2015) annotated a part of PPDB (Ganitkevitch et al., 2013) to evaluate semantic modeling of paraphrases. Although the target unit of semantic modeling is different from that for these previous studies, we follow the annotation guideline and instruction of Mitchell and Lapata (2010) to build the new dataset. The task addressed in this paper is also related to the Semantic Textual Similarity (STS) task (Agirre et al., 2012). STS is the task to measure the degree of semantic similarity between two sentences. Even though a relational pattern appears as a part of a sentence, it may be difficult to transfer findings from one to another: for example, the encoders of RNN and its variants explored 14 In fact, we made substantial efforts to introduce the negative sampling technique. However, Xu et al. (2015) omits the detail of the technique probably because of the severe page limit of short papers. For this reason, we could not reproduce their method in this study. in this study may exhibit different characteristics, i"
P16-1215,D15-1141,0,0.0260687,"trices (parameters), g(.) is the elementwise activation function (tanh). We set h0 = 0 at t = 1. In essence, RNN computes the hidden state ht based on the one at the previous position (ht−1 ) and the word embedding xt . Applying Equation 1 from t = 1 to T , we use hT as the distributed representation of the relational pattern. 3.3 RNN variants We also employ Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) as an encoder for relational patterns. LSTM has been applied successfully to various NLP tasks including word segmentation (Chen et al., 2015), dependency parsing (Dyer et al., 2015), machine translation (Sutskever et al., 2014), and sentiment analysis (Tai et al., 2015). GRU is also successful in machine translation (Cho et al., 2014) and various 5 We do not use a bias term in this study. We set the number of dimensions of hidden states identical to that of word embeddings (d) so that we can adapt the objective function of the Skip-gram model for training (Section 3.5). 2278 Parameter update by Skip-gram model Gated Additive Composition (GAC) hs ~x s-2 ~x s-1 (context vectors) is fs is+1 (hidden vectors) is+2 h s+L-1 is+3 (3) xs x"
P16-1215,P15-1061,0,0.228046,"ickx et al., 2010). In other words, we explore whether high-quality distributed representations of relational patterns are effective to identify a relation type of an entity pair. The dataset consists of 10, 717 relation instances (8, 000 training and 2, 717 test instances) with their relation types annotated. The dataset 2281 Method SVM SVM + NoComp SVM + LSTM SVM + Add SVM + GRU SVM + RNN SVM + GAC Ranking loss + GAC w/ fine-tuning SVM (Rink and Harabagiu, 2010) MV-RNN (Socher et al., 2012) FCM (Gormley et al., 2015) w/o fine-tuning w/ fine-tuning RelEmb (Hashimoto et al., 2015) CR-CNN (dos Santos et al., 2015) w/ Other w/o Other depLCNN (Xu et al., 2015) depLCNN + NS Feature set BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS + dependency, WordNet, NE embeddings, BoW, POS + dependency, WordNet, NE BoW, POS, dependency, Google n-gram, etc. embeddings, parse trees + WordNet, POS, NE embeddings, dependency + WordNet embeddings, dependency + NE embeddings + dependency, WordNet, NE embeddings, word position embeddings embeddings, word position embeddings embeddings, dependency + WordNet embeddings, dependency + WordNe"
P16-1215,P15-1033,0,0.0255448,"wise activation function (tanh). We set h0 = 0 at t = 1. In essence, RNN computes the hidden state ht based on the one at the previous position (ht−1 ) and the word embedding xt . Applying Equation 1 from t = 1 to T , we use hT as the distributed representation of the relational pattern. 3.3 RNN variants We also employ Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) as an encoder for relational patterns. LSTM has been applied successfully to various NLP tasks including word segmentation (Chen et al., 2015), dependency parsing (Dyer et al., 2015), machine translation (Sutskever et al., 2014), and sentiment analysis (Tai et al., 2015). GRU is also successful in machine translation (Cho et al., 2014) and various 5 We do not use a bias term in this study. We set the number of dimensions of hidden states identical to that of word embeddings (d) so that we can adapt the objective function of the Skip-gram model for training (Section 3.5). 2278 Parameter update by Skip-gram model Gated Additive Composition (GAC) hs ~x s-2 ~x s-1 (context vectors) is fs is+1 (hidden vectors) is+2 h s+L-1 is+3 (3) xs x s+1 x s+2 x s+L-1 (word vectors) (3) ~x"
P16-1215,D11-1142,0,0.354603,"rameters of the encoders on a large unlabeled corpus. Experiments show that the new dataset does not only enable detailed analyses of the different encoders, but also provides a gauge to predict successes of distributed representations of relational patterns in another task (relation classification). Figure 1 illustrates the overview of this study. 2 Data Construction 2.1 Target relation instances We build a new dataset upon the work of Zeichner et al. (2012), which consists of relational patterns with semantic inference labels annotated. The dataset includes 5,555 pairs2 extracted by Reverb (Fader et al., 2011), 2,447 pairs with inference relation and 3,108 pairs (the rest) without one. Initially, we considered using this high-quality dataset as it is for semantic modeling of relational patterns. However, we found that inference relations exhibit quite different properties from those of semantic similarity. Take a relational pattern pair “X be the part of Y” and “X be an essential part of Y” filled with “X = the small intestine, Y = the digestive system” as an instance. The pattern “X be the part of Y” does not entail “X be an essential part of Y” because the meaning of the former does not include ‘"
P16-1215,N13-1092,0,0.088,"Missing"
P16-1215,D15-1205,0,0.0317607,"Missing"
P16-1215,D15-1113,0,0.0186742,"detail of the technique probably because of the severe page limit of short papers. For this reason, we could not reproduce their method in this study. in this study may exhibit different characteristics, influenced by the length and complexity of input text expressions. In addition to data construction, this paper addresses semantic modeling of relational patterns. Nakashole et al. (2012) approached the similar task by constructing a taxonomy of relational patterns. They represented a vector of a relational pattern as the distribution of entity pairs co-occurring with the relational pattern. Grycner et al. (2015) extended Nakashole et al. (2012) to generalize dimensions of the vector space (entity pairs) by incorporating hyponymy relation between entities. They also used external resources to recognize the transitivity of pattern pairs and applied transitivities to find patterns in entailment relation. These studies did not consider semantic composition of relational patterns. Thus, they might suffer from the data sparseness problem, as shown by NoComp in Figure 4. Numerous studies have been aimed at encoding distributed representations of phrases and sentences from word embeddings by using: Recursive"
P16-1215,D14-1163,0,0.0243136,"relational patterns. Thus, they might suffer from the data sparseness problem, as shown by NoComp in Figure 4. Numerous studies have been aimed at encoding distributed representations of phrases and sentences from word embeddings by using: Recursive Neural Network (Socher et al., 2011), Matrix Vector Recursive Neural Network (Socher et al., 2012), Recursive Neural Network with different weight matrices corresponding to syntactic categories (Socher et al., 2013) or word types (Takase et al., 2016), RNN (Sutskever et al., 2011), LSTM (Sutskever et al., 2014), GRU (Cho et al., 2014), PAS-CLBLM (Hashimoto et al., 2014), etc. As described in Section 3, we applied RNN, GRU, and LSTM to compute distributed representations of relational patterns because recent papers have demonstrated their superiority in semantic composition (Sutskever et al., 2014; Tang et al., 2015). In this paper, we presented a comparative study of different encoders for semantic modeling of relational patterns. To investigate usefulness of the distributed representations and the new dataset, we adopted the relation classification task (SemEval 2010 Task 8) as a real application. On the SemEval 2010 Task 8, several studies considered seman"
P16-1215,K15-1027,0,0.0185818,"ilarity task. This fact implies two potential impacts. First, the distributed representations of relational patterns are useful and easily transferable to other tasks such as knowledge base population. Second, the pattern similarity dataset provides a gauge to predict successes of distributed representations in another task. We could further improve the performance of SVM + GAC by incorporating external resources in the similar manner as the previous studies did. Concretely, SVM + GAC achieved 83.7 F1 score by adding features for WordNet, named entities (NE), and dependency paths explained in Hashimoto et al. (2015). Moreover, we could obtain 84.2 F1 score, using the ranking based loss function (dos Santos et al., 2015) and fine-tuning of the distributed representations initially trained by GAC. Currently, this is the second best score among the performance values reported in the previous studies on this task (the second group of Table 3). If we could use the negative sampling technique proposed by Xu et al. (2015), we might improve the performance further14 . 5 Related Work Mitchell and Lapata (2010) was a pioneering work in semantic modeling of short phrases. They constructed the dataset that contains"
P16-1215,S10-1006,0,0.248462,"Missing"
P16-1215,P82-1020,0,0.263933,"Missing"
P16-1215,S13-2007,0,0.0307031,"ction (dos Santos et al., 2015) and fine-tuning of the distributed representations initially trained by GAC. Currently, this is the second best score among the performance values reported in the previous studies on this task (the second group of Table 3). If we could use the negative sampling technique proposed by Xu et al. (2015), we might improve the performance further14 . 5 Related Work Mitchell and Lapata (2010) was a pioneering work in semantic modeling of short phrases. They constructed the dataset that contains two-word phrase pairs with semantic similarity judged by human annotators. Korkontzelos et al. (2013) provided a semantic similarity dataset with pairs of two words and a single word. Wieting et al. (2015) annotated a part of PPDB (Ganitkevitch et al., 2013) to evaluate semantic modeling of paraphrases. Although the target unit of semantic modeling is different from that for these previous studies, we follow the annotation guideline and instruction of Mitchell and Lapata (2010) to build the new dataset. The task addressed in this paper is also related to the Semantic Textual Similarity (STS) task (Agirre et al., 2012). STS is the task to measure the degree of semantic similarity between two s"
P16-1215,Y14-1010,1,0.830135,"ons with strong collocations from a training corpus. However, this approach might be affected by data sparseness, which lowers the quality of distributed representations. Another simple but effective approach is additive composition (Mitchell and Lapata, 2010), where the distributed representation of a relational pattern is computed by the mean of embeddings of constituent words. Presuming that a relational pattern consists of a sequence of T words w1 , ..., wT , then we let xt ∈ Rd the embedding of the word 1 ∑T wt . This approach computes T t=1 xt as the embedding of the relational pattern. Muraoka et al. (2014) reported that the additive composition is a strong baseline among various methods. 3.2 Recurrent Neural Network Recently, a number of studies model semantic compositions of phrases and sentences by using (a variant of) Recurrent Neural Network (RNN) (Sutskever et al., 2014; Tang et al., 2015). For a given embedding xt at position t, the vanilla RNN (Elman, 1990) computes the hidden state ht ∈ Rd by the following recursive equation5 , ht = g(Wx xt + Wh ht−1 ). (1) Here, Wx and Wh are d × d matrices (parameters), g(.) is the elementwise activation function (tanh). We set h0 = 0 at t = 1. In ess"
P16-1215,D12-1104,0,0.106757,"emely important because a relation (e.g., causality) can be mentioned by various expressions (e.g., “X cause Y”, “X lead to Y”, “Y is associated with X”). To make matters worse, relational patterns are highly productive: we can produce a emphasized causality pattern “X increase the severe risk of Y” from “X increase the risk of Y” by inserting severe to the pattern. To model the meanings of relational patterns, the previous studies built a co-occurrence matrix between relational patterns (e.g., “X increase the risk of Y”) and entity pairs (e.g., “X: smoking, Y: cancer”) (Lin and Pantel, 2001; Nakashole et al., 2012). Based on the distributional hypothesis (Harris, 1954), we can compute a semantic vector of a relational pattern from the co-occurrence matrix, and measure the similarity of two relational patterns as the cosine similarity of the vectors. Nowadays, several studies adopt distributed representations computed by neural networks for semantic modeling of relational patterns (Yih et al., 2014; Takase et al., 2016). Notwithstanding, the previous studies paid little attention to explicitly evaluate semantic modeling of relational patterns. In this paper, we construct a new dataset that contains a pai"
P16-1215,D14-1162,0,0.0890972,"ation classification (word2vec) Training Evaluation (§4.2) increase: risk: open: Word embeddings SemEval 2010 Task 8 Figure 1: Overview of this study. Introduction Knowledge about entities and their relations (relation instances) are crucial for a wide spectrum of NLP applications, e.g., information retrieval, question answering, and recognizing textual entailment. Learning distributed representations for relation instances is a central technique in downstream applications as a number of recent studies demonstrated the usefulness of distributed representations for words (Mikolov et al., 2013; Pennington et al., 2014) and sentences (Sutskever et al., 2014; Cho et al., 2014; Kiros et al., 2015). In particular, semantic modeling of relations and their textual realizations (relational patterns hereafter) is extremely important because a relation (e.g., causality) can be mentioned by various expressions (e.g., “X cause Y”, “X lead to Y”, “Y is associated with X”). To make matters worse, relational patterns are highly productive: we can produce a emphasized causality pattern “X increase the severe risk of Y” from “X increase the risk of Y” by inserting severe to the pattern. To model the meanings of relational"
P16-1215,N13-1008,0,0.0424569,"ication. On the SemEval 2010 Task 8, several studies considered semantic composition. Gormley et al. (2015) proposed Feature-rich Compositional Embedding Model (FCM) that can combine binary features (e.g., positional indicators) with word embeddings via outer products. dos Santos et al. (2015) addressed the task using Convolutional Neural Network (CNN). Xu et al. (2015) achieved a higher performance than dos 2283 Santos et al. (2015) by application of CNN on dependency paths. In addition to the relation classification task, we briefly describe other applications. To populate a knowledge base, Riedel et al. (2013) jointly learned latent feature vectors of entities, relational patterns, and relation types in the knowledge base. Toutanova et al. (2015) adapted CNN to capture the compositional structure of a relational pattern during the joint learning. For open domain question answering, Yih et al. (2014) proposed the method to map an interrogative sentence on an entity and a relation type contained in a knowledge base by using CNN. Although these reports described good performance on the respective tasks, we are unsure of the generality of distributed representations trained for a specific task such as"
P16-1215,S10-1057,0,0.0360011,"Missing"
P16-1215,D12-1110,0,0.282802,"tions for a different application, we address the task of relation classification on the SemEval 2010 Task 8 dataset (Hendrickx et al., 2010). In other words, we explore whether high-quality distributed representations of relational patterns are effective to identify a relation type of an entity pair. The dataset consists of 10, 717 relation instances (8, 000 training and 2, 717 test instances) with their relation types annotated. The dataset 2281 Method SVM SVM + NoComp SVM + LSTM SVM + Add SVM + GRU SVM + RNN SVM + GAC Ranking loss + GAC w/ fine-tuning SVM (Rink and Harabagiu, 2010) MV-RNN (Socher et al., 2012) FCM (Gormley et al., 2015) w/o fine-tuning w/ fine-tuning RelEmb (Hashimoto et al., 2015) CR-CNN (dos Santos et al., 2015) w/ Other w/o Other depLCNN (Xu et al., 2015) depLCNN + NS Feature set BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS embeddings, BoW, POS + dependency, WordNet, NE embeddings, BoW, POS + dependency, WordNet, NE BoW, POS, dependency, Google n-gram, etc. embeddings, parse trees + WordNet, POS, NE embeddings, dependency + WordNet embeddings, dependency + NE embeddings + dependency, WordNet, NE embeddings, wor"
P16-1215,P13-1045,0,0.034643,"e transitivity of pattern pairs and applied transitivities to find patterns in entailment relation. These studies did not consider semantic composition of relational patterns. Thus, they might suffer from the data sparseness problem, as shown by NoComp in Figure 4. Numerous studies have been aimed at encoding distributed representations of phrases and sentences from word embeddings by using: Recursive Neural Network (Socher et al., 2011), Matrix Vector Recursive Neural Network (Socher et al., 2012), Recursive Neural Network with different weight matrices corresponding to syntactic categories (Socher et al., 2013) or word types (Takase et al., 2016), RNN (Sutskever et al., 2011), LSTM (Sutskever et al., 2014), GRU (Cho et al., 2014), PAS-CLBLM (Hashimoto et al., 2014), etc. As described in Section 3, we applied RNN, GRU, and LSTM to compute distributed representations of relational patterns because recent papers have demonstrated their superiority in semantic composition (Sutskever et al., 2014; Tang et al., 2015). In this paper, we presented a comparative study of different encoders for semantic modeling of relational patterns. To investigate usefulness of the distributed representations and the new d"
P16-1215,P15-1150,0,0.0951605,"Missing"
P16-1215,D15-1167,0,0.166823,"n of a relational pattern is computed by the mean of embeddings of constituent words. Presuming that a relational pattern consists of a sequence of T words w1 , ..., wT , then we let xt ∈ Rd the embedding of the word 1 ∑T wt . This approach computes T t=1 xt as the embedding of the relational pattern. Muraoka et al. (2014) reported that the additive composition is a strong baseline among various methods. 3.2 Recurrent Neural Network Recently, a number of studies model semantic compositions of phrases and sentences by using (a variant of) Recurrent Neural Network (RNN) (Sutskever et al., 2014; Tang et al., 2015). For a given embedding xt at position t, the vanilla RNN (Elman, 1990) computes the hidden state ht ∈ Rd by the following recursive equation5 , ht = g(Wx xt + Wh ht−1 ). (1) Here, Wx and Wh are d × d matrices (parameters), g(.) is the elementwise activation function (tanh). We set h0 = 0 at t = 1. In essence, RNN computes the hidden state ht based on the one at the previous position (ht−1 ) and the word embedding xt . Applying Equation 1 from t = 1 to T , we use hT as the distributed representation of the relational pattern. 3.3 RNN variants We also employ Long Short-Term Memory (LSTM) (Hochr"
P16-1215,D15-1174,0,0.0222456,"itional Embedding Model (FCM) that can combine binary features (e.g., positional indicators) with word embeddings via outer products. dos Santos et al. (2015) addressed the task using Convolutional Neural Network (CNN). Xu et al. (2015) achieved a higher performance than dos 2283 Santos et al. (2015) by application of CNN on dependency paths. In addition to the relation classification task, we briefly describe other applications. To populate a knowledge base, Riedel et al. (2013) jointly learned latent feature vectors of entities, relational patterns, and relation types in the knowledge base. Toutanova et al. (2015) adapted CNN to capture the compositional structure of a relational pattern during the joint learning. For open domain question answering, Yih et al. (2014) proposed the method to map an interrogative sentence on an entity and a relation type contained in a knowledge base by using CNN. Although these reports described good performance on the respective tasks, we are unsure of the generality of distributed representations trained for a specific task such as the relation classification. In contrast, this paper demonstrated the contribution of distributed representations trained in a generic mann"
P16-1215,Q15-1025,0,0.0175169,"Currently, this is the second best score among the performance values reported in the previous studies on this task (the second group of Table 3). If we could use the negative sampling technique proposed by Xu et al. (2015), we might improve the performance further14 . 5 Related Work Mitchell and Lapata (2010) was a pioneering work in semantic modeling of short phrases. They constructed the dataset that contains two-word phrase pairs with semantic similarity judged by human annotators. Korkontzelos et al. (2013) provided a semantic similarity dataset with pairs of two words and a single word. Wieting et al. (2015) annotated a part of PPDB (Ganitkevitch et al., 2013) to evaluate semantic modeling of paraphrases. Although the target unit of semantic modeling is different from that for these previous studies, we follow the annotation guideline and instruction of Mitchell and Lapata (2010) to build the new dataset. The task addressed in this paper is also related to the Semantic Textual Similarity (STS) task (Agirre et al., 2012). STS is the task to measure the degree of semantic similarity between two sentences. Even though a relational pattern appears as a part of a sentence, it may be difficult to trans"
P16-1215,D15-1062,0,0.110593,"es in the similar manner as the previous studies did. Concretely, SVM + GAC achieved 83.7 F1 score by adding features for WordNet, named entities (NE), and dependency paths explained in Hashimoto et al. (2015). Moreover, we could obtain 84.2 F1 score, using the ranking based loss function (dos Santos et al., 2015) and fine-tuning of the distributed representations initially trained by GAC. Currently, this is the second best score among the performance values reported in the previous studies on this task (the second group of Table 3). If we could use the negative sampling technique proposed by Xu et al. (2015), we might improve the performance further14 . 5 Related Work Mitchell and Lapata (2010) was a pioneering work in semantic modeling of short phrases. They constructed the dataset that contains two-word phrase pairs with semantic similarity judged by human annotators. Korkontzelos et al. (2013) provided a semantic similarity dataset with pairs of two words and a single word. Wieting et al. (2015) annotated a part of PPDB (Ganitkevitch et al., 2013) to evaluate semantic modeling of paraphrases. Although the target unit of semantic modeling is different from that for these previous studies, we fo"
P16-1215,P14-2105,0,0.0966919,"ational patterns, the previous studies built a co-occurrence matrix between relational patterns (e.g., “X increase the risk of Y”) and entity pairs (e.g., “X: smoking, Y: cancer”) (Lin and Pantel, 2001; Nakashole et al., 2012). Based on the distributional hypothesis (Harris, 1954), we can compute a semantic vector of a relational pattern from the co-occurrence matrix, and measure the similarity of two relational patterns as the cosine similarity of the vectors. Nowadays, several studies adopt distributed representations computed by neural networks for semantic modeling of relational patterns (Yih et al., 2014; Takase et al., 2016). Notwithstanding, the previous studies paid little attention to explicitly evaluate semantic modeling of relational patterns. In this paper, we construct a new dataset that contains a pair of relational patterns with five similarity ratings judged by human annotators. The new dataset shows a 2276 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2276–2286, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics high inter-annotator agreement, following the annotation guideline of Mitchell and Lapat"
P16-1215,P12-2031,0,0.389909,"t Gated Additive Composition, which is an enhancement of additive composition with the gating mechanism. We utilize the Skip-gram objective for training the parameters of the encoders on a large unlabeled corpus. Experiments show that the new dataset does not only enable detailed analyses of the different encoders, but also provides a gauge to predict successes of distributed representations of relational patterns in another task (relation classification). Figure 1 illustrates the overview of this study. 2 Data Construction 2.1 Target relation instances We build a new dataset upon the work of Zeichner et al. (2012), which consists of relational patterns with semantic inference labels annotated. The dataset includes 5,555 pairs2 extracted by Reverb (Fader et al., 2011), 2,447 pairs with inference relation and 3,108 pairs (the rest) without one. Initially, we considered using this high-quality dataset as it is for semantic modeling of relational patterns. However, we found that inference relations exhibit quite different properties from those of semantic similarity. Take a relational pattern pair “X be the part of Y” and “X be an essential part of Y” filled with “X = the small intestine, Y = the digestive"
P16-3021,C10-1032,0,0.0789266,"Missing"
P16-3021,D11-1072,0,0.0413109,"Missing"
P16-3021,Q15-1023,0,0.0618927,"v Jargalsaikhan Naoaki Okazaki Koji Matsuda Kentaro Inui Tohoku University, Japan {davaajav, okazaki, matsuda, inui}@ecei.toholu.ac.jp Abstract EL is useful for various NLP tasks, e.g., Question-Answering (Khalid et al., 2008), Information Retrieval (Blanco et al., 2015), Knowledge Base Population (Dredze et al., 2010), CoReference Resolution (Hajishirzi et al., 2013). There are about a dozen of datasets targeting EL in English, including UIUC datasets (ACE, MSNBC) (Ratinov et al., 2011), AIDA datasets (Hoffart et al., 2011), and TAC-KBP datasets (2009–2012 datasets) (McNamee and Dang, 2009). Ling et al. (2015) discussed various challenges in EL. They argued that the existing datasets are inconsistent with each other. For instance, TACKBP targets only mentions belonging to PERSON, LOCATION, ORGANIZATION classes. Although these entity classes may be dominant in articles, other tasks may require information on natural phenomena, product names, and institution names. In contrast, the MSNBC corpus does not limit entity classes, linking mentions to any Wikipedia article. However, the MSNBC corpus does not have a NIL label even if a mention belongs to an important class such as PERSON or LOCATION, unlike"
P16-3021,P11-1138,0,0.0287818,"itten Japanese (BCCWJ) 3 annotated with finegrained named entity labels defined by Sekine’s Extended Named Entity Hierarchy (Sekine et al., 2002) 4 . is limited to mentions that belong to PERSON, LOCATION or ORGANIZATION classes only. When an article is not present in Wikipedia, UIUC does not record this information in any way. On the contrary, TAC-KBP5 and our datasets have NIL tag used to mark a mention when it does not have an entry in KB. 2.1 Design Policy To give a better understanding of our dataset we briefly compare it with existing English datasets. The most comparable ones are UIUC (Ratinov et al., 2011) and TAC-KBP 2009–2012 datasets (McNamee and Dang, 2009). Although, AIDA datasets are widely used for Disambiguation of Entities, AIDA uses YAGO, an unique Knowledge Base derived from Wikipedia, GeoNames and Wordnet, which makes it difficult to compare. UIUC is similar to our dataset in a sense that it links to any Wikipedia article without any semantic class restrictions, unlike TAC-KBP which Ling et al. (2015) argued that the task definition of EL itself is challenging: whether to target only named entities (NEs) or to include general nouns; whether to limit semantic classes of target NEs; h"
P16-3021,sekine-etal-2002-extended,0,0.0922538,"of word segmentation for unsegmented languages, like Chinese or Japanese. (Murawaki and Mori, 2016) approach the word segmentation problem from point of view of Wikification. Their focus is on the word segmentation rather than on the linking. In this research, we build a Japanese Wikification corpus in which mentions in Japanese documents are associated with Japanese Wikipedia articles. The corpus consists of 340 newspaper articles from Balanced Corpus of Contemporary Written Japanese (BCCWJ) 3 annotated with finegrained named entity labels defined by Sekine’s Extended Named Entity Hierarchy (Sekine et al., 2002) 4 . is limited to mentions that belong to PERSON, LOCATION or ORGANIZATION classes only. When an article is not present in Wikipedia, UIUC does not record this information in any way. On the contrary, TAC-KBP5 and our datasets have NIL tag used to mark a mention when it does not have an entry in KB. 2.1 Design Policy To give a better understanding of our dataset we briefly compare it with existing English datasets. The most comparable ones are UIUC (Ratinov et al., 2011) and TAC-KBP 2009–2012 datasets (McNamee and Dang, 2009). Although, AIDA datasets are widely used for Disambiguation of Enti"
P16-3021,spitkovsky-chang-2012-cross,0,0.026119,"tity mention recognition. Nested entities It was assumed that the role initially served as a temporary peacemaker to persuade Ali al-Sistani, the spiritual leader of Shia Muslims: Position Vocation. Since the mention in the sentence refers to the highest ranking position of a specific religion, it is inappropriate to link the mention to the article Spiritual Leader nor Shia Muslim. Therefore, we decided to mark this mention as NIL. 3.1 Wikification without fine-grained semantic classes Our experiment is based on the disambiguation method that uses the probability distribution of anchor texts (Spitkovsky and Chang, 2012). Given a mention m, the method predicts an entity eˆ that yields the highest probability p(e|m), Entity changes over time In his greeting speech, the representative Ito expressed his opinion on the upcoming gubernatorial election: Event Other and Sapporo city mayoral election. eˆ = argmax p(e|m). e∈E 141 (1) Category Province Country GPE Other Political Party Pro Sports Organization City Company Group Mammal International Organization Company Game Conference Public Institution Book Political Organization Other Organization Other GOE Other Plan Character Occasion Other Example Fukuoka Prefectu"
P16-3021,D13-1029,0,\N,Missing
P16-3021,E12-2021,0,\N,Missing
P17-1037,W11-1701,0,0.0188519,"from liberal to conservative (Adamic and Glance, 2005; Zhou et al., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016). Even though these studies provide intuitive visualizations and interpretations along the liberalconservative axis, political analysts argue that the axis is flawed and insufficient for representing public opinion and ideologies (Kerlinger, 1984; Maddox and Lilie, 1984). A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.g., free trade, immigration, abortion). However, stance detection across different topics is extremely difficult. Anand et al. (2011) reported that a sophisticated method with topic-dependent features substantially improved the performance of stance detection within a topic, but such an approach could not outperform a baseline method with simple n-gram features when evaluated across topics. More recently, all participants of"
P17-1037,D15-1008,0,0.0175142,"and text data associated with the labels. Employing a single axis (e.g., liberal to conservative) or a few axes (e.g., political parties and candidates of elections), these studies provide intuitive visualizations and interpretations along the respective axes. In contrast, this study is the first attempt to recognize and organize various axes of topics on social media with no prior assumptions regarding the axes. Therefore, we think our study provides a new tool for computational social science and political science that enables researchers to analyze and interpret phenomena on social media. Bamman and Smith (2015) presented an unsupervised method for assessing the political stance of a proposition, such as “global warming is a hoax,” along the political spectrum of liberal to conservative. In their work, a proposition was represented by a tuple in the form hsubject, predicatei, for example, hglobal warming, hoaxi. They presented a generative model for users, subjects, and predicates to find a one-dimensional latent space that corresponded to the political spectrum. Next, we describe previous research focused on acquiring lexical knowledge of politics. Sim et al. (2013) measured ideological positions of"
P17-1037,W03-1210,0,0.0191371,"rget of the opinion from text. Although these previous studies have the potential to improve the quality of the user-topic matrix R, unfortunately, no corpus or resource is available for the Japanese language. We do not currently have a large collection of English tweets, but combining fine-grained opinion analysis with matrix factorization is an immediate future work. Acknowledgements Causality Relation Some of inter-topic preferences in this work can be explained by causality relation, for example, “TPP promotes free trade.” A number of previous studies acquire instances of causal relation (Girju, 2003; Do et al., 2011) and promote/suppress relation (Hashimoto et al., 2012; Fluck et al., 2015) from text. The causality knowledge is useful for predicting (hypotheses of) future events (Radinsky et al., 2012; Radinsky and Davidovich, 2012; Hashimoto et al., 2015). Inter-topic preferences, however, also include pairs of topics in which causality relation hardly holds. As an example, it is unreasonable to infer that nuclear plant and railroading of bills have a causal relation, but those who dislike nuclear plant also oppose railroading of bills because presumably they think the governing politic"
P17-1037,W06-1651,0,0.0427249,"Missing"
P17-1037,D12-1057,0,0.0316662,"Missing"
P17-1037,C10-2028,0,0.0465547,"specifies high requirements for making amendments to Constitution of Japan (including Article 9). 2 399 uct of low-rank user and topic matrices. These low-rank matrices provide latent vector representations of both users and topics. This approach is also useful for completing preferences of “ordinary” (i.e., less vocal) users, which fills the gap between different types of users. The contributions of this paper are threefold. In order to design linguistic patterns, we focus on hashtags appearing in the corpus that have been popular clues for locating subjective statements such as sentiments (Davidov et al., 2010), emotions (Qadir and Riloff, 2014), and ironies (Van Hee et al., 2016). Hashtags are also useful for finding strong supporters and critics, as well as their target topics; for example, #immigrantsWelcome indicates that the author favors immigrants; and #StopAbortion is against abortion. Based on this intuition, we design regular expressions for both pro hashtags “#(.+)sansei”3 and con hashtags “#(.+)hantai”4 , where (.+) matches a target topic. These regular expressions can find users who have strong preferences to topics. Using this approach, we extracted 31,068 occurrences of pro/con hashta"
P17-1037,N15-1146,0,0.0287078,"the topic and user vectors to create a crosstopic stance detector. It is possible to generalize our work to model heterogeneous signals, such as interests and behaviors of people, for example, “those who are interested in A also support B,” and “those who favor A also vote for B”. Therefore, we believe that our work will bring about new applications in the field of NLP and other disciplines. Fine-grained Opinion Analysis The method presented in Section 2 is an instance of finegrained opinion analysis (Wiebe et al., 2005; Choi et al., 2006; Johansson and Moschitti, 2010; Yang and Cardie, 2013; Deng and Wiebe, 2015), which extracts a tuple of a subjective opinion, a holder of the opinion, and a target of the opinion from text. Although these previous studies have the potential to improve the quality of the user-topic matrix R, unfortunately, no corpus or resource is available for the Japanese language. We do not currently have a large collection of English tweets, but combining fine-grained opinion analysis with matrix factorization is an immediate future work. Acknowledgements Causality Relation Some of inter-topic preferences in this work can be explained by causality relation, for example, “TPP promot"
P17-1037,W10-2910,0,0.0535646,"Missing"
P17-1037,C16-1279,0,0.0178466,"l., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016). Even though these studies provide intuitive visualizations and interpretations along the liberalconservative axis, political analysts argue that the axis is flawed and insufficient for representing public opinion and ideologies (Kerlinger, 1984; Maddox and Lilie, 1984). A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.g., free trade, immigration, abortion). However, stance detection across different topics is extremely difficult. Anand et al. (2011) reported that a sophisticated method with topic-dependent features substantially improved the performance of stance detection within a topic, but such an approach could not outperform a baseline method with simple n-gram features when evaluated across topics. More recently, all participants of SemEval 2016 Task 6A (with five topics) could not outperform the baseline"
P17-1037,D13-1010,0,0.0139787,"phenomena on social media. Bamman and Smith (2015) presented an unsupervised method for assessing the political stance of a proposition, such as “global warming is a hoax,” along the political spectrum of liberal to conservative. In their work, a proposition was represented by a tuple in the form hsubject, predicatei, for example, hglobal warming, hoaxi. They presented a generative model for users, subjects, and predicates to find a one-dimensional latent space that corresponded to the political spectrum. Next, we describe previous research focused on acquiring lexical knowledge of politics. Sim et al. (2013) measured ideological positions of candidates in US presidential elections from their Similar to our present work, their work (Bamman and Smith, 2015) did not require labeled data 404 corporate approaches to acquire causality knowledge. to map users and topics (i.e., subjects) onto a latent feature space. In their paper, they reported that the generative model outperformed Principal Component Analysis (PCA), which is a method for matrix factorization. Empirical results here probably reflected the underlying assumptions that PCA treats missing elements as zero and not as missing data. In contra"
P17-1037,P09-1026,0,0.034675,"communities on social media along the political spectrum from liberal to conservative (Adamic and Glance, 2005; Zhou et al., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016). Even though these studies provide intuitive visualizations and interpretations along the liberalconservative axis, political analysts argue that the axis is flawed and insufficient for representing public opinion and ideologies (Kerlinger, 1984; Maddox and Lilie, 1984). A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.g., free trade, immigration, abortion). However, stance detection across different topics is extremely difficult. Anand et al. (2011) reported that a sophisticated method with topic-dependent features substantially improved the performance of stance detection within a topic, but such an approach could not outperform a baseline method with simple n-gram features when evaluated a"
P17-1037,W06-1639,0,0.0619255,"ions, influences, and communities on social media along the political spectrum from liberal to conservative (Adamic and Glance, 2005; Zhou et al., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016). Even though these studies provide intuitive visualizations and interpretations along the liberalconservative axis, political analysts argue that the axis is flawed and insufficient for representing public opinion and ideologies (Kerlinger, 1984; Maddox and Lilie, 1984). A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.g., free trade, immigration, abortion). However, stance detection across different topics is extremely difficult. Anand et al. (2011) reported that a sophisticated method with topic-dependent features substantially improved the performance of stance detection within a topic, but such an approach could not outperform a baseline method with simple n-"
P17-1037,S16-1003,0,0.135031,"Missing"
P17-1037,C10-2100,0,0.200376,"long the political spectrum from liberal to conservative (Adamic and Glance, 2005; Zhou et al., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016). Even though these studies provide intuitive visualizations and interpretations along the liberalconservative axis, political analysts argue that the axis is flawed and insufficient for representing public opinion and ideologies (Kerlinger, 1984; Maddox and Lilie, 1984). A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.g., free trade, immigration, abortion). However, stance detection across different topics is extremely difficult. Anand et al. (2011) reported that a sophisticated method with topic-dependent features substantially improved the performance of stance detection within a topic, but such an approach could not outperform a baseline method with simple n-gram features when evaluated across topics. More recently,"
P17-1037,C16-1257,0,0.050316,"Missing"
P17-1037,D14-1127,0,0.0300221,"making amendments to Constitution of Japan (including Article 9). 2 399 uct of low-rank user and topic matrices. These low-rank matrices provide latent vector representations of both users and topics. This approach is also useful for completing preferences of “ordinary” (i.e., less vocal) users, which fills the gap between different types of users. The contributions of this paper are threefold. In order to design linguistic patterns, we focus on hashtags appearing in the corpus that have been popular clues for locating subjective statements such as sentiments (Davidov et al., 2010), emotions (Qadir and Riloff, 2014), and ironies (Van Hee et al., 2016). Hashtags are also useful for finding strong supporters and critics, as well as their target topics; for example, #immigrantsWelcome indicates that the author favors immigrants; and #StopAbortion is against abortion. Based on this intuition, we design regular expressions for both pro hashtags “#(.+)sansei”3 and con hashtags “#(.+)hantai”4 , where (.+) matches a target topic. These regular expressions can find users who have strong preferences to topics. Using this approach, we extracted 31,068 occurrences of pro/con hashtags used by 18,582 users for 4,899 t"
P17-1037,P13-1161,0,0.0153711,"work, we plan to embed the topic and user vectors to create a crosstopic stance detector. It is possible to generalize our work to model heterogeneous signals, such as interests and behaviors of people, for example, “those who are interested in A also support B,” and “those who favor A also vote for B”. Therefore, we believe that our work will bring about new applications in the field of NLP and other disciplines. Fine-grained Opinion Analysis The method presented in Section 2 is an instance of finegrained opinion analysis (Wiebe et al., 2005; Choi et al., 2006; Johansson and Moschitti, 2010; Yang and Cardie, 2013; Deng and Wiebe, 2015), which extracts a tuple of a subjective opinion, a holder of the opinion, and a target of the opinion from text. Although these previous studies have the potential to improve the quality of the user-topic matrix R, unfortunately, no corpus or resource is available for the Japanese language. We do not currently have a large collection of English tweets, but combining fine-grained opinion analysis with matrix factorization is an immediate future work. Acknowledgements Causality Relation Some of inter-topic preferences in this work can be explained by causality relation, f"
P18-1200,D13-1160,0,0.124765,"n et al., 2016) and might reduce fluctuations in entity vector updates. As a result, all relation matrices trained in this work are very close to orthogonal. Initialization Instead of pure Gaussian, it is better to initialize matrices as (I + G)/2, where G is random. The identity matrix I helps passing information from head to tail (Tian et al., 2016). Negative Sampling Instead of a unigram distribution, it is better to use a uniform distribution for generating noises. This is somehow counterintuitive compared to training word embeddings. 5 Related Works KBs have a wide range of applications (Berant et al., 2013; Hixon et al., 2015; Nickel et al., 2016a) and KBC has inspired a huge amount of research (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Wang et al., 2014b,a; Xiao et al., 2016; Nguyen et al., 2016; Toutanova et al., 2016; Das et al., 2017; Hayashi and Shimbo, 2017). 2151 Among the previous works, TransE (Bordes et al., 2013) is the classic method which represents a relation as a translation of the entity vector space, and is partially inspired by Mikolov et al. (2013)’s vector arithmetic method of solving word analogy tasks. Although competitive in KBC, it is speculated that"
P18-1200,E17-1013,0,0.0876634,"random. The identity matrix I helps passing information from head to tail (Tian et al., 2016). Negative Sampling Instead of a unigram distribution, it is better to use a uniform distribution for generating noises. This is somehow counterintuitive compared to training word embeddings. 5 Related Works KBs have a wide range of applications (Berant et al., 2013; Hixon et al., 2015; Nickel et al., 2016a) and KBC has inspired a huge amount of research (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Wang et al., 2014b,a; Xiao et al., 2016; Nguyen et al., 2016; Toutanova et al., 2016; Das et al., 2017; Hayashi and Shimbo, 2017). 2151 Among the previous works, TransE (Bordes et al., 2013) is the classic method which represents a relation as a translation of the entity vector space, and is partially inspired by Mikolov et al. (2013)’s vector arithmetic method of solving word analogy tasks. Although competitive in KBC, it is speculated that this method is well-suited for 1to-1 relations but might be too simple to represent N -to-N relations accurately(Wang et al., 2017). Thus, extensions such as TransR (Lin et al., 2015b) and STransE (Nguyen et al., 2016) are proposed to map entities into a r"
P18-1200,D15-1038,0,0.0708608,"Missing"
P18-1200,W17-2609,0,0.014865,"sed in this work originates from RESCAL (Nickel et al., 2011), in which relations are naturally represented as analogue to the adjacency matrices (Sec.2). Further developments include HolE (Nickel et al., 2016b) and ConvE (Dettmers et al., 2018) which improve this approach in terms of parameterefficiency, by introducing low dimension factorizations of the matrices. We inherit the basic model of RESCAL but draw additional training techniques from Tian et al. (2016), and show that the base model already can achieve near state-of-the-art performance (Sec.6.1,6.3). This sends a message similar to Kadlec et al. (2017), saying that training tricks might be as important as model designs. Nevertheless, we emphasize the novelty of this work in that the previous models mostly achieve dimension reduction by imposing some pre-designed hard constraints (Bordes et al., 2013; Yang et al., 2015; Trouillon et al., 2016; Nickel et al., 2016b; Xie et al., 2017; Dettmers et al., 2018), whereas the constraints themselves are not learned from data; in contrast, our approach by jointly training an autoencoder does not impose any explicit hard constraints, so it leads to more flexible modeling. Moreover, we additionally focu"
P18-1200,D15-1082,0,0.102168,"Missing"
P18-1200,N13-1090,0,0.0512251,"counterintuitive compared to training word embeddings. 5 Related Works KBs have a wide range of applications (Berant et al., 2013; Hixon et al., 2015; Nickel et al., 2016a) and KBC has inspired a huge amount of research (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Wang et al., 2014b,a; Xiao et al., 2016; Nguyen et al., 2016; Toutanova et al., 2016; Das et al., 2017; Hayashi and Shimbo, 2017). 2151 Among the previous works, TransE (Bordes et al., 2013) is the classic method which represents a relation as a translation of the entity vector space, and is partially inspired by Mikolov et al. (2013)’s vector arithmetic method of solving word analogy tasks. Although competitive in KBC, it is speculated that this method is well-suited for 1to-1 relations but might be too simple to represent N -to-N relations accurately(Wang et al., 2017). Thus, extensions such as TransR (Lin et al., 2015b) and STransE (Nguyen et al., 2016) are proposed to map entities into a relation-specific vector space before translation. The ITransF model (Xie et al., 2017) further enhances this approach by imposing a hard constraint that the relation-specific maps should be linear combinations of a small number of pro"
P18-1200,P17-2088,0,0.0218721,"ty matrix I helps passing information from head to tail (Tian et al., 2016). Negative Sampling Instead of a unigram distribution, it is better to use a uniform distribution for generating noises. This is somehow counterintuitive compared to training word embeddings. 5 Related Works KBs have a wide range of applications (Berant et al., 2013; Hixon et al., 2015; Nickel et al., 2016a) and KBC has inspired a huge amount of research (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Wang et al., 2014b,a; Xiao et al., 2016; Nguyen et al., 2016; Toutanova et al., 2016; Das et al., 2017; Hayashi and Shimbo, 2017). 2151 Among the previous works, TransE (Bordes et al., 2013) is the classic method which represents a relation as a translation of the entity vector space, and is partially inspired by Mikolov et al. (2013)’s vector arithmetic method of solving word analogy tasks. Although competitive in KBC, it is speculated that this method is well-suited for 1to-1 relations but might be too simple to represent N -to-N relations accurately(Wang et al., 2017). Thus, extensions such as TransR (Lin et al., 2015b) and STransE (Nguyen et al., 2016) are proposed to map entities into a relation-specific vector spa"
P18-1200,P15-1016,0,0.0359283,"f this work in that the previous models mostly achieve dimension reduction by imposing some pre-designed hard constraints (Bordes et al., 2013; Yang et al., 2015; Trouillon et al., 2016; Nickel et al., 2016b; Xie et al., 2017; Dettmers et al., 2018), whereas the constraints themselves are not learned from data; in contrast, our approach by jointly training an autoencoder does not impose any explicit hard constraints, so it leads to more flexible modeling. Moreover, we additionally focus on leveraging composition in KBC. Although this idea has been frequently explored before (Guu et al., 2015; Neelakantan et al., 2015; Lin et al., 2015a), our discussion about the concept of compositional constraints and its connection to dimension reduction has not been addressed similarly in previous research. In experiments, we will show (Sec.6.2,6.3) that joint training with an autoencoder indeed helps finding compositional constraints and benefits from compositional training. Autoencoders have been used solo for learning distributed representations of syntactic trees (Socher et al., 2011), words and images (Silberer and Lapata, 2014), or semantic roles (Titov and Khoddam, 2015). It is also used for pretraining other de"
P18-1200,N15-1086,0,0.032212,"ight reduce fluctuations in entity vector updates. As a result, all relation matrices trained in this work are very close to orthogonal. Initialization Instead of pure Gaussian, it is better to initialize matrices as (I + G)/2, where G is random. The identity matrix I helps passing information from head to tail (Tian et al., 2016). Negative Sampling Instead of a unigram distribution, it is better to use a uniform distribution for generating noises. This is somehow counterintuitive compared to training word embeddings. 5 Related Works KBs have a wide range of applications (Berant et al., 2013; Hixon et al., 2015; Nickel et al., 2016a) and KBC has inspired a huge amount of research (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Wang et al., 2014b,a; Xiao et al., 2016; Nguyen et al., 2016; Toutanova et al., 2016; Das et al., 2017; Hayashi and Shimbo, 2017). 2151 Among the previous works, TransE (Bordes et al., 2013) is the classic method which represents a relation as a translation of the entity vector space, and is partially inspired by Mikolov et al. (2013)’s vector arithmetic method of solving word analogy tasks. Although competitive in KBC, it is speculated that this method is well"
P18-1200,N16-1054,0,0.520222,"initialize matrices as (I + G)/2, where G is random. The identity matrix I helps passing information from head to tail (Tian et al., 2016). Negative Sampling Instead of a unigram distribution, it is better to use a uniform distribution for generating noises. This is somehow counterintuitive compared to training word embeddings. 5 Related Works KBs have a wide range of applications (Berant et al., 2013; Hixon et al., 2015; Nickel et al., 2016a) and KBC has inspired a huge amount of research (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Wang et al., 2014b,a; Xiao et al., 2016; Nguyen et al., 2016; Toutanova et al., 2016; Das et al., 2017; Hayashi and Shimbo, 2017). 2151 Among the previous works, TransE (Bordes et al., 2013) is the classic method which represents a relation as a translation of the entity vector space, and is partially inspired by Mikolov et al. (2013)’s vector arithmetic method of solving word analogy tasks. Although competitive in KBC, it is speculated that this method is well-suited for 1to-1 relations but might be too simple to represent N -to-N relations accurately(Wang et al., 2017). Thus, extensions such as TransR (Lin et al., 2015b) and STransE (Nguyen et al., 2"
P18-1200,N13-1008,0,0.0456648,"very close to orthogonal. Initialization Instead of pure Gaussian, it is better to initialize matrices as (I + G)/2, where G is random. The identity matrix I helps passing information from head to tail (Tian et al., 2016). Negative Sampling Instead of a unigram distribution, it is better to use a uniform distribution for generating noises. This is somehow counterintuitive compared to training word embeddings. 5 Related Works KBs have a wide range of applications (Berant et al., 2013; Hixon et al., 2015; Nickel et al., 2016a) and KBC has inspired a huge amount of research (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Wang et al., 2014b,a; Xiao et al., 2016; Nguyen et al., 2016; Toutanova et al., 2016; Das et al., 2017; Hayashi and Shimbo, 2017). 2151 Among the previous works, TransE (Bordes et al., 2013) is the classic method which represents a relation as a translation of the entity vector space, and is partially inspired by Mikolov et al. (2013)’s vector arithmetic method of solving word analogy tasks. Although competitive in KBC, it is speculated that this method is well-suited for 1to-1 relations but might be too simple to represent N -to-N relations accurately(Wang et al., 2017)"
P18-1200,W17-2608,0,0.250986,".5 82.5 4233 4371 .461 .459 53.4 52.9 212 215 .336 .337∗ 52.3∗ 52.3∗ 191∗ 195 94.8 94.8 53 54 69.7 69.4 2268∗ 2447 .343 .310 54.8∗ 54.1 197∗ 203 .331 .328 51.6 51.5 TransE (Bordes et al., 2013) TransR (Lin et al., 2015b) RESCAL (Nickel et al., 2011) HolE (Nickel et al., 2016b) 292 281 911 724 92.0 93.6 58.0 94.3 66 76 163 293 70.4 74.4 41.0 66.8 4311 4222 9689 8096 .202 .210 .105 .376 45.6 47.1 20.3 40.0 278 320 457 1172 .236 .282 .178 .169 41.6 45.9 31.9 30.9 STransE (Nguyen et al., 2016) ITransF (Xie et al., 2017) ComplEx (Trouillon et al., 2016) Ensemble DistMult (Kadlec et al., 2017) IRN (Shen et al., 2017) ConvE (Dettmers et al., 2018) R-GCN+ (Schlichtkrull et al., 2017) ProjE (Shi and Weninger, 2017) 206 205 457 249 504 - 93.4 94.2 94.7 95.0 95.3 95.5 96.4∗ - 69 65 35.9 38 64 34∗ 79.9 81.0 84.0 90.4 92.7∗ 87.3 84.2 88.4 5261 5277 - .44 .46 - 51 48 - 339 246 - .247 .316 .249 - 42.8 49.1 41.7 - JOINT BASE JOINT + COMP BASE + COMP Table 2: KBC results on the WN18, FB15k, WN18RR, and FB15k-237 datasets. The first and second sectors compare our joint to the base models with and without compositional training, respectively; the third sector shows our re-experiments and the fourth shows previous publ"
P18-1200,P14-1068,0,0.0216782,"sition in KBC. Although this idea has been frequently explored before (Guu et al., 2015; Neelakantan et al., 2015; Lin et al., 2015a), our discussion about the concept of compositional constraints and its connection to dimension reduction has not been addressed similarly in previous research. In experiments, we will show (Sec.6.2,6.3) that joint training with an autoencoder indeed helps finding compositional constraints and benefits from compositional training. Autoencoders have been used solo for learning distributed representations of syntactic trees (Socher et al., 2011), words and images (Silberer and Lapata, 2014), or semantic roles (Titov and Khoddam, 2015). It is also used for pretraining other deep neural networks (Erhan et al., 2010). However, when combined with other models, the learning of autoencoders, or more generally sparse codings (Rubinstein et al., 2010), is usually conveyed in an alternating manner, fixing one part of the model while optimizing the other, such as in Xie et al. (2017). To our knowledge, joint training with an autoencoder is not widely used previously for reducing dimensionality. Jointly training an autoencoder is not simple because it takes non-stationary inputs. In this w"
P18-1200,D11-1014,0,0.0822351,"Missing"
P18-1200,P16-1121,1,0.869445,"Missing"
P18-1200,N15-1001,0,0.0202336,"uently explored before (Guu et al., 2015; Neelakantan et al., 2015; Lin et al., 2015a), our discussion about the concept of compositional constraints and its connection to dimension reduction has not been addressed similarly in previous research. In experiments, we will show (Sec.6.2,6.3) that joint training with an autoencoder indeed helps finding compositional constraints and benefits from compositional training. Autoencoders have been used solo for learning distributed representations of syntactic trees (Socher et al., 2011), words and images (Silberer and Lapata, 2014), or semantic roles (Titov and Khoddam, 2015). It is also used for pretraining other deep neural networks (Erhan et al., 2010). However, when combined with other models, the learning of autoencoders, or more generally sparse codings (Rubinstein et al., 2010), is usually conveyed in an alternating manner, fixing one part of the model while optimizing the other, such as in Xie et al. (2017). To our knowledge, joint training with an autoencoder is not widely used previously for reducing dimensionality. Jointly training an autoencoder is not simple because it takes non-stationary inputs. In this work, we modified SGD so that it shares traits"
P18-1200,W15-4007,0,0.129018,"t learning rates for different parameters. While Adagrad sets them adaptively by keeping track of gradients for all parameters, our modification of SGD is more efficient and allows us to grasp a rough intuition about which parameter gets how much update. We believe our techniques and findings in joint training with an autoencoder could be helpful to reducing dimensionality and improving interpretability in other neural network architectures as well. 6 Experiments We evaluate on standard KBC datasets, including WN18 and FB15k (Bordes et al., 2013), WN18RR (Dettmers et al., 2018) and FB15k-237 (Toutanova and Chen, 2015). The statistical information of these datasets are shown in Table 1. WN18 collects word relations from WordNet (Miller, 1995), and FB15k is taken from Freebase (Bollacker et al., 2008); both have filtered out low frequency entities. However, it is reported in Toutanova and Chen (2015) that both WN18 and FB15k have information leaks because the inverses of some test triples appear in the training set. FB15k-237 and WN18RR fix this problem by deleting such triples from training and test data. In this work, we do evaluate on WN18 and FB15k, but our models are mainly tuned on FB15k-237. 2152 Data"
P18-1200,P16-1136,0,0.0260491,"s (I + G)/2, where G is random. The identity matrix I helps passing information from head to tail (Tian et al., 2016). Negative Sampling Instead of a unigram distribution, it is better to use a uniform distribution for generating noises. This is somehow counterintuitive compared to training word embeddings. 5 Related Works KBs have a wide range of applications (Berant et al., 2013; Hixon et al., 2015; Nickel et al., 2016a) and KBC has inspired a huge amount of research (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Wang et al., 2014b,a; Xiao et al., 2016; Nguyen et al., 2016; Toutanova et al., 2016; Das et al., 2017; Hayashi and Shimbo, 2017). 2151 Among the previous works, TransE (Bordes et al., 2013) is the classic method which represents a relation as a translation of the entity vector space, and is partially inspired by Mikolov et al. (2013)’s vector arithmetic method of solving word analogy tasks. Although competitive in KBC, it is speculated that this method is well-suited for 1to-1 relations but might be too simple to represent N -to-N relations accurately(Wang et al., 2017). Thus, extensions such as TransR (Lin et al., 2015b) and STransE (Nguyen et al., 2016) are proposed to map"
P18-1200,D14-1167,0,0.0301826,"nstead of pure Gaussian, it is better to initialize matrices as (I + G)/2, where G is random. The identity matrix I helps passing information from head to tail (Tian et al., 2016). Negative Sampling Instead of a unigram distribution, it is better to use a uniform distribution for generating noises. This is somehow counterintuitive compared to training word embeddings. 5 Related Works KBs have a wide range of applications (Berant et al., 2013; Hixon et al., 2015; Nickel et al., 2016a) and KBC has inspired a huge amount of research (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Wang et al., 2014b,a; Xiao et al., 2016; Nguyen et al., 2016; Toutanova et al., 2016; Das et al., 2017; Hayashi and Shimbo, 2017). 2151 Among the previous works, TransE (Bordes et al., 2013) is the classic method which represents a relation as a translation of the entity vector space, and is partially inspired by Mikolov et al. (2013)’s vector arithmetic method of solving word analogy tasks. Although competitive in KBC, it is speculated that this method is well-suited for 1to-1 relations but might be too simple to represent N -to-N relations accurately(Wang et al., 2017). Thus, extensions such as TransR (Lin e"
P18-1200,P17-1088,0,0.535427,"nce of a composition of two relations (assumed to be modeled by matrices M1 , M2 ) matching a third (M3 ) also justifies dimension reduction, because it implies a compositional constraint M1 · M2 ≈ M3 that can be satisfied only by a lower dimension sub-manifold in the parameter space1 . Previous approaches reduce dimensionality of relations by imposing pre-designed hard constraints on the parameter space, such as constraining that relations are translations (Bordes et al., 2013) or diagonal matrices (Yang et al., 2015), or assuming they are linear combinations of a small number of prototypes (Xie et al., 2017). However, pre-designed hard constraints do not seem to cope well with compositional constraints, because it is difficult to know a priori which two relations compose to which third relation, hence difficult to choose a pre-design; and compositional constraints are not always exact (e.g. the composition of currency of country and headquarter location usually matches business operation currency but not always), so hard constraints are less suited. In this paper, we investigate an alternative approach by training relation parameters jointly with an autoencoder (Figure 1). During training, the au"
P18-2091,I17-2069,1,0.851664,"ess) (Mairesse and Walker, 2007; Pavlick and Nenkova, 2015; Flekova et al., 2016; PreotiucPietro et al., 2016; Sennrich et al., 2016; Niu et al., 2017); however, it is not straightforward to create strict guidelines for identifying the stylistic profile of a given text. The systematic evaluations of stylesensitive word representations and the learning of style-sensitive word representations in a supervised manner are hampered by this. In addition, there is another trend of research forward controlling style-sensitive utterance generation without defining the style dimensions (Li et al., 2016; Akama et al., 2017); however, this line of research considers style to be something associated with a given specific character, i.e., a persona, and does not aim to capture the stylistic variation space. The contributions of this paper are three-fold. (1) We propose a novel architecture that acquires style-sensitive word vectors (Figure 1) in an unsupervised manner. (2) We construct a novel dataset for style, which consists of pairs of stylesensitive words with each pair scored according to its stylistic similarity. (3) We demonstrate that our word vectors capture the stylistic similarity between two words succe"
P18-2091,P16-2051,0,0.0632724,"Missing"
P18-2091,D16-1235,0,0.0657839,"Missing"
P18-2091,P16-1094,0,0.0264389,"le (e.g., politeness) (Mairesse and Walker, 2007; Pavlick and Nenkova, 2015; Flekova et al., 2016; PreotiucPietro et al., 2016; Sennrich et al., 2016; Niu et al., 2017); however, it is not straightforward to create strict guidelines for identifying the stylistic profile of a given text. The systematic evaluations of stylesensitive word representations and the learning of style-sensitive word representations in a supervised manner are hampered by this. In addition, there is another trend of research forward controlling style-sensitive utterance generation without defining the style dimensions (Li et al., 2016; Akama et al., 2017); however, this line of research considers style to be something associated with a given specific character, i.e., a persona, and does not aim to capture the stylistic variation space. The contributions of this paper are three-fold. (1) We propose a novel architecture that acquires style-sensitive word vectors (Figure 1) in an unsupervised manner. (2) We construct a novel dataset for style, which consists of pairs of stylesensitive words with each pair scored according to its stylistic similarity. (3) We demonstrate that our word vectors capture the stylistic similarity be"
P18-2091,P07-1063,0,0.0437348,"udies on what is said. However, recently, capturing how it is said, such as stylistic variations, has also proven to be useful for natural language processing tasks such as classification, analysis, and generation (Pavlick and Tetreault, 2016; Niu and Carpuat, 2017; Wang et al., 2017). This paper studies the stylistic variations of words in the context of the representation learning of words. The lack of subjective or objective definitions is a major difficulty in studying style (Xu, 2017). Previous attempts have been made to define a selected aspect of the notion of style (e.g., politeness) (Mairesse and Walker, 2007; Pavlick and Nenkova, 2015; Flekova et al., 2016; PreotiucPietro et al., 2016; Sennrich et al., 2016; Niu et al., 2017); however, it is not straightforward to create strict guidelines for identifying the stylistic profile of a given text. The systematic evaluations of stylesensitive word representations and the learning of style-sensitive word representations in a supervised manner are hampered by this. In addition, there is another trend of research forward controlling style-sensitive utterance generation without defining the style dimensions (Li et al., 2016; Akama et al., 2017); however, t"
P18-2091,Y15-1035,0,0.0379503,"Missing"
P18-2091,W17-4903,0,0.0361612,"Missing"
P18-2091,D17-1299,0,0.0176665,"or natural language processing tasks such as classification, analysis, and generation (Pavlick and Tetreault, 2016; Niu and Carpuat, 2017; Wang et al., 2017). This paper studies the stylistic variations of words in the context of the representation learning of words. The lack of subjective or objective definitions is a major difficulty in studying style (Xu, 2017). Previous attempts have been made to define a selected aspect of the notion of style (e.g., politeness) (Mairesse and Walker, 2007; Pavlick and Nenkova, 2015; Flekova et al., 2016; PreotiucPietro et al., 2016; Sennrich et al., 2016; Niu et al., 2017); however, it is not straightforward to create strict guidelines for identifying the stylistic profile of a given text. The systematic evaluations of stylesensitive word representations and the learning of style-sensitive word representations in a supervised manner are hampered by this. In addition, there is another trend of research forward controlling style-sensitive utterance generation without defining the style dimensions (Li et al., 2016; Akama et al., 2017); however, this line of research considers style to be something associated with a given specific character, i.e., a persona, and do"
P18-2091,N15-1023,0,0.0250012,"ver, recently, capturing how it is said, such as stylistic variations, has also proven to be useful for natural language processing tasks such as classification, analysis, and generation (Pavlick and Tetreault, 2016; Niu and Carpuat, 2017; Wang et al., 2017). This paper studies the stylistic variations of words in the context of the representation learning of words. The lack of subjective or objective definitions is a major difficulty in studying style (Xu, 2017). Previous attempts have been made to define a selected aspect of the notion of style (e.g., politeness) (Mairesse and Walker, 2007; Pavlick and Nenkova, 2015; Flekova et al., 2016; PreotiucPietro et al., 2016; Sennrich et al., 2016; Niu et al., 2017); however, it is not straightforward to create strict guidelines for identifying the stylistic profile of a given text. The systematic evaluations of stylesensitive word representations and the learning of style-sensitive word representations in a supervised manner are hampered by this. In addition, there is another trend of research forward controlling style-sensitive utterance generation without defining the style dimensions (Li et al., 2016; Akama et al., 2017); however, this line of research consid"
P18-2091,Q16-1005,0,0.023976,"hat the proposed extensions contribute to the acquisition of stylesensitive word embeddings. 1 Figure 1: Word vector capturing stylistic and syntactic/semantic similarity. Introduction Analyzing and generating natural language texts requires the capturing of two important aspects of language: what is said and how it is said. In the literature, much more attention has been paid to studies on what is said. However, recently, capturing how it is said, such as stylistic variations, has also proven to be useful for natural language processing tasks such as classification, analysis, and generation (Pavlick and Tetreault, 2016; Niu and Carpuat, 2017; Wang et al., 2017). This paper studies the stylistic variations of words in the context of the representation learning of words. The lack of subjective or objective definitions is a major difficulty in studying style (Xu, 2017). Previous attempts have been made to define a selected aspect of the notion of style (e.g., politeness) (Mairesse and Walker, 2007; Pavlick and Nenkova, 2015; Flekova et al., 2016; PreotiucPietro et al., 2016; Sennrich et al., 2016; Niu et al., 2017); however, it is not straightforward to create strict guidelines for identifying the stylistic pr"
P18-2091,L18-1152,0,0.0378424,"the x of CBOW- SEP - CTX used the same context as that of CBOW- ALL - CTX, the syntactic sensitivity of x was suppressed. We speculate that the syntactic sensitivity was distilled off by the other part of the CBOW- SEP - CTX vector, i.e., y learned using only the near context, which captured more syntactic information. In the next section, we analyze CBOW- SEP - CTX for the different characteristics of x and y. 3.3.2 Semantic and Topical Sensitivities To test the model’s ability to capture the semantic similarity, we also measured correlations with the Japanese Word Similarity Dataset (JWSD) (Sakaizawa and Komachi, 2018), which consists of 4,000 Japanese word pairs annotated with semantic similarity scores by human workers. For each model, we calculate and show the Spearman rank correlation score (ρsem ) between the cosine similarity score cos(v w , v w0 ) and the human judgements on JWSD in Table 12 . CBOW- DISTCTX has the lowest score (ρsem = 15.9); however, surprisingly, the stylistic vector xwt has the highest score (ρsem = 28.9), while both vectors have a high ρstyle . This result indicates that the proposed stylistic vector xwt captures not only the stylistic similarity but also the captures semantic si"
P18-2091,N16-1005,0,0.0257657,"o proven to be useful for natural language processing tasks such as classification, analysis, and generation (Pavlick and Tetreault, 2016; Niu and Carpuat, 2017; Wang et al., 2017). This paper studies the stylistic variations of words in the context of the representation learning of words. The lack of subjective or objective definitions is a major difficulty in studying style (Xu, 2017). Previous attempts have been made to define a selected aspect of the notion of style (e.g., politeness) (Mairesse and Walker, 2007; Pavlick and Nenkova, 2015; Flekova et al., 2016; PreotiucPietro et al., 2016; Sennrich et al., 2016; Niu et al., 2017); however, it is not straightforward to create strict guidelines for identifying the stylistic profile of a given text. The systematic evaluations of stylesensitive word representations and the learning of style-sensitive word representations in a supervised manner are hampered by this. In addition, there is another trend of research forward controlling style-sensitive utterance generation without defining the style dimensions (Li et al., 2016; Akama et al., 2017); however, this line of research considers style to be something associated with a given specific character, i.e."
P18-2091,D17-1228,0,0.0457817,"Missing"
P18-2091,W17-4901,0,0.058899,"Missing"
P19-1464,P17-1091,0,0.146498,"span models and the current state-of-the-art model to predict, focusing on the most challenging subtask, i.e., LI. Stab and Gurevych (2017) reported that their model tends to output shallow trees even if the corresponding gold trees are deeper. We therefore investigated the performance according to different depths of ADU trees. Figure 3 indi4694 Overall LI LTC ATC Avg. Macro Link No-Link Macro Support Attack Macro MC Claim Premise LSTM+dist (ELMo) 81.8 80.7 67.8 93.7 79.0 96.8 61.1 85.7 91.6 73.3 92.1 PEC Joint PointerNet (Potash et al., 2017) 76.7 60.8 92.5 84.9 89.4 73.2 92.1 St. SVM-full (Niculae et al., 2017) - 60.1 77.6 78.2 64.5 90.2 ILP Joint (Stab and Gurevych 2017) 75.2 75.1 58.5 91.8 68.0 94.7 41.3 82.6 89.1 68.2 90.3 LSTM+dist (ELMo) 78.2 73.9 57.5 90.3 77.2 84.2 70.3 83.5 - 72.9 94.0 MTC Joint PointerNet (Potash et al., 2017) 74.0 57.7 90.3 81.3 - 69.2 93.4 New Best EG (Afantenos et al., 2018) 78.5 72.2 75.7 87.6 ILP Joint (Stab and Gurevych 2017) 76.2 68.3 48.6 88.1 74.5 85.5 62.8 85.7 - 77.0 94.3 Data. Model Table 3: Comparison with existing models on the PEC and the MTC. MC denotes M AJOR C LAIM. Gold structure support ADU1: In addition, I believe that city provides more work opportunit"
P19-1464,D18-1191,1,0.851766,"parsing (Mann and Thompson, 1987; Prasad et al., 2008), which is closely related to ASP, discourse connectives are strong clues for identifying discourse relations (Marcu, 2000; Braud and Denis, 2016). Exploring effective ways to use the discourse connective information has received wide attention in various NLP fields (Sileo et al., 2019; Pan et al., 2018). Span representation Span representation design is gaining considerable attention for several NLP tasks, such as syntactic parsing (Wang and Chang, 2016; Stern et al., 2017; Kitaev and Klein, 2018), semantic role labeling (He et al., 2018; Ouchi et al., 2018), and coreference resolution (Lee et al., 2017, 2018). One common practice for effective design is to use bidirectional long shortterm memory networks (BiLSTMs). Wang and Chang (2016) proposed span representation called “LSTM-minus,” which represents each span (i, j) as the difference between the LSTM’s hidden states over time steps, i.e. hj − hi . The work most similar to ours is Li et al. (2016), where LSTM-minus is used for discourse structure prediction. This study extends it by integrating discourse properties into span representation. 3 Model This section describes (i) an LSTM-minus-base"
P19-1464,N18-1202,0,0.0740259,"Missing"
P19-1464,D17-1143,0,0.127346,"ive text. Figure 1 shows an example of an argumentative text and its structure. The structure forms a tree, the nodes of which are referred to as argumentative discourse units (ADUs) and the edges represent argumentative relations between the ADUs. ASP systems must identify such edges, edge labels (e.g., S UPPORT and ATTACK), and node labels (e.g., P REMISE, C LAIM, and M AJOR C LAIM). A key to achieving high performance is feature representation design for segmental discourse units (spans), such as ADUs. The aim of this study is to update the foundation of span representation design for ASP. Potash et al. (2017) introduced a model exploiting neural network-based span representation for ASP and reported state-of-the-art performance. Similarly, for other natural language processing (NLP) tasks, such as syntactic and semantic parsing, neural network-based span representation design is attracting considerable attention as a promising new technique; some effective designs have been reported (Wang and Chang, 2016; Stern et al., 2017; He et al., 2018). Starting from such basis, task-dependent extensions for ASP are an interesting direction to explore. Although the neural network-based approach (Potash et al"
P19-1464,prasad-etal-2008-penn,0,0.0527405,"for argument structures, has been published (Stab and Gurevych, 2017); moreover, a variety of models have been proposed for ASP. There are two main approaches for ASP: (i) a discrete feature-based approach (Stab and Gurevych, 2017; Nguyen and Litman, 2016; Peldszus and Stede, 2015) and (ii) a neural network-based approach (Potash et al., 2017; Eger et al., 2017). Because neural network-based models have achieved high performance for this task (Potash et al., 2017), this study also explores the neural network-based approach. Discourse connectives In discourse parsing (Mann and Thompson, 1987; Prasad et al., 2008), which is closely related to ASP, discourse connectives are strong clues for identifying discourse relations (Marcu, 2000; Braud and Denis, 2016). Exploring effective ways to use the discourse connective information has received wide attention in various NLP fields (Sileo et al., 2019; Pan et al., 2018). Span representation Span representation design is gaining considerable attention for several NLP tasks, such as syntactic parsing (Wang and Chang, 2016; Stern et al., 2017; Kitaev and Klein, 2018), semantic role labeling (He et al., 2018; Ouchi et al., 2018), and coreference resolution (Lee e"
P19-1464,N19-1351,0,0.0235569,"(ii) a neural network-based approach (Potash et al., 2017; Eger et al., 2017). Because neural network-based models have achieved high performance for this task (Potash et al., 2017), this study also explores the neural network-based approach. Discourse connectives In discourse parsing (Mann and Thompson, 1987; Prasad et al., 2008), which is closely related to ASP, discourse connectives are strong clues for identifying discourse relations (Marcu, 2000; Braud and Denis, 2016). Exploring effective ways to use the discourse connective information has received wide attention in various NLP fields (Sileo et al., 2019; Pan et al., 2018). Span representation Span representation design is gaining considerable attention for several NLP tasks, such as syntactic parsing (Wang and Chang, 2016; Stern et al., 2017; Kitaev and Klein, 2018), semantic role labeling (He et al., 2018; Ouchi et al., 2018), and coreference resolution (Lee et al., 2017, 2018). One common practice for effective design is to use bidirectional long shortterm memory networks (BiLSTMs). Wang and Chang (2016) proposed span representation called “LSTM-minus,” which represents each span (i, j) as the difference between the LSTM’s hidden states ov"
P19-1464,J17-3005,0,0.451822,"ttention as a promising new technique; some effective designs have been reported (Wang and Chang, 2016; Stern et al., 2017; He et al., 2018). Starting from such basis, task-dependent extensions for ASP are an interesting direction to explore. Although the neural network-based approach (Potash et al., 2017; Eger et al., 2017) achieves high performances for ASP, it does not explicitly take into account useful linguistic clues. However, prior works demonstrate that linguistic features, particularly discourse connectives, are strong clues to predict the structure for ASP (Lawrence and Reed, 2015; Stab and Gurevych, 2017). We integrate such linguistic properties into span representation design as task-dependent extensions for ASP. In summary, our contributions are as follows. • We investigate (i) span representation originally developed for other NLP tasks and (ii) a simple task-dependent extension for ASP. • Empirical results show that such representations improve the performance of ASP and yield state-of-the-art scores. • Extensive analysis reveals that such representations especially improve the performance when parsing argumentative texts with a complex structure (deeper ADU trees). To facilitate ASP resea"
P19-1464,P17-1076,0,0.184751,"is feature representation design for segmental discourse units (spans), such as ADUs. The aim of this study is to update the foundation of span representation design for ASP. Potash et al. (2017) introduced a model exploiting neural network-based span representation for ASP and reported state-of-the-art performance. Similarly, for other natural language processing (NLP) tasks, such as syntactic and semantic parsing, neural network-based span representation design is attracting considerable attention as a promising new technique; some effective designs have been reported (Wang and Chang, 2016; Stern et al., 2017; He et al., 2018). Starting from such basis, task-dependent extensions for ASP are an interesting direction to explore. Although the neural network-based approach (Potash et al., 2017; Eger et al., 2017) achieves high performances for ASP, it does not explicitly take into account useful linguistic clues. However, prior works demonstrate that linguistic features, particularly discourse connectives, are strong clues to predict the structure for ASP (Lawrence and Reed, 2015; Stab and Gurevych, 2017). We integrate such linguistic properties into span representation design as task-dependent extens"
P19-1464,P16-1218,0,0.338201,"ving high performance is feature representation design for segmental discourse units (spans), such as ADUs. The aim of this study is to update the foundation of span representation design for ASP. Potash et al. (2017) introduced a model exploiting neural network-based span representation for ASP and reported state-of-the-art performance. Similarly, for other natural language processing (NLP) tasks, such as syntactic and semantic parsing, neural network-based span representation design is attracting considerable attention as a promising new technique; some effective designs have been reported (Wang and Chang, 2016; Stern et al., 2017; He et al., 2018). Starting from such basis, task-dependent extensions for ASP are an interesting direction to explore. Although the neural network-based approach (Potash et al., 2017; Eger et al., 2017) achieves high performances for ASP, it does not explicitly take into account useful linguistic clues. However, prior works demonstrate that linguistic features, particularly discourse connectives, are strong clues to predict the structure for ASP (Lawrence and Reed, 2015; Stab and Gurevych, 2017). We integrate such linguistic properties into span representation design as t"
P19-2053,C16-1158,0,0.0358318,"Missing"
P19-2053,P12-2018,0,0.0228275,"us text network, which was then embedded into a low dimensional space. Although these approaches have been proven useful for several document classification and regression tasks, their focus is not on capturing the discourse structure of documents. Related Work The focus of this study is the unsupervised encapsulation of discourse structure (coherence and cohesion) into document representation for essay scoring. A popular approach for document representation is the use of fixed-length features such as bag-of-words (BOW) and bag-of-ngrams due to their simplicity and highly competitive results (Wang and Manning, 2012). However, such approaches fail to capture the semantic similarity of words and phrases since they treat each word or 1 Our implementation is publicly available https://github.com/FarjanaSultanaMim/ DiscoShuffle One exception is the study by Ji and Smith (2017) who illustrated the role of discourse structure for document representation by implementing a discourse structure (defined by RST) aware model and showed that their model improves text categorization performance (e.g., sentiment classification of movies and Yelp reviews, and prediction of news article frames). The authors utilized an RS"
P19-2053,P17-1092,0,0.0312082,"focus of this study is the unsupervised encapsulation of discourse structure (coherence and cohesion) into document representation for essay scoring. A popular approach for document representation is the use of fixed-length features such as bag-of-words (BOW) and bag-of-ngrams due to their simplicity and highly competitive results (Wang and Manning, 2012). However, such approaches fail to capture the semantic similarity of words and phrases since they treat each word or 1 Our implementation is publicly available https://github.com/FarjanaSultanaMim/ DiscoShuffle One exception is the study by Ji and Smith (2017) who illustrated the role of discourse structure for document representation by implementing a discourse structure (defined by RST) aware model and showed that their model improves text categorization performance (e.g., sentiment classification of movies and Yelp reviews, and prediction of news article frames). The authors utilized an RST-parser to obtain the discourse dependency at 379 Specifically, these encoders first produce the representations, hbase and haux . Then, these representations are concatenated into one vector, which is mapped to a feature vector z. tree of a document and then"
P19-2053,P18-1052,0,0.0714807,"RST-parser to obtain the discourse dependency at 379 Specifically, these encoders first produce the representations, hbase and haux . Then, these representations are concatenated into one vector, which is mapped to a feature vector z. tree of a document and then built a recursive neural network on top of it. The issue with their approach is that texts need to be parsed by an RST parser which is computationally expensive. Furthermore, the performance of RST parsing is dependent on the genre of documents (Ji and Smith, 2017). Previous studies have modeled text coherence (Li and Jurafsky, 2016; Joty et al., 2018; Mesgar and Strube, 2018). Farag et al. (2018) demonstrated that state-of-the-art neural automated essay scoring (AES) is not well-suited for capturing adversarial input of grammatically correct but incoherent sequences of sentences. Therefore, they developed a neural local coherence model and jointly trained it with a state-of-the-art AES model to build an adversarially robust AES system. Mesgar and Strube (2018) used a local coherence model to assess essay scoring performance on a dataset of holistic scores where it is unclear which criteria of the essay the score considers. We target Organ"
P19-2053,D18-1482,0,0.29492,"is shown in Figure 1. The high-scored essay (i.e., Organization score of 4) first states its position regarding the prompt and then provides several reasons to strengthen the claim. It is considered coherent because it follows a logical order. However, the low-scored essay is not clear on its position and what it is arguing about. Therefore, it can be considered incoherent since it lacks logical sequencing. Previous studies on document embedding have primarily focused on capturing word similarity, word dependencies and semantic information of documents (Le and Mikolov, 2014; Liu et al., 2017; Wu et al., 2018; Tang et al., 2015). However, less attention has been paid to capturing discourse structure for document embedding in an unsupervised manner and no prior work applies unsupervised document representation learning to essay scoring. In short, it has not yet been explored how some of the discourse properties can Existing document embedding approaches mainly focus on capturing sequences of words in documents. However, some document classification and regression tasks such as essay scoring need to consider discourse structure of documents. Although some prior approaches consider this issue and uti"
P19-2053,D13-1141,0,0.089527,"Missing"
P19-2053,D18-1464,0,0.168779,"in the discourse dependency at 379 Specifically, these encoders first produce the representations, hbase and haux . Then, these representations are concatenated into one vector, which is mapped to a feature vector z. tree of a document and then built a recursive neural network on top of it. The issue with their approach is that texts need to be parsed by an RST parser which is computationally expensive. Furthermore, the performance of RST parsing is dependent on the genre of documents (Ji and Smith, 2017). Previous studies have modeled text coherence (Li and Jurafsky, 2016; Joty et al., 2018; Mesgar and Strube, 2018). Farag et al. (2018) demonstrated that state-of-the-art neural automated essay scoring (AES) is not well-suited for capturing adversarial input of grammatically correct but incoherent sequences of sentences. Therefore, they developed a neural local coherence model and jointly trained it with a state-of-the-art AES model to build an adversarially robust AES system. Mesgar and Strube (2018) used a local coherence model to assess essay scoring performance on a dataset of holistic scores where it is unclear which criteria of the essay the score considers. We target Organization and Argument Stren"
P19-2053,D10-1023,0,0.0810883,"Missing"
P19-2053,P15-1053,0,0.363648,"xt in addition to dependency relations and predicateargument structures. One example of such tasks is essay scoring, where discourse structure (e.g., coherence and cohesion) plays a crucial role, especially when considering Organization and Argument Strength criteria, since they refer to logicalsequence awareness in texts. Organization refers to how good an essay structure is, where wellstructured essays logically develop arguments and state positions by supporting them (Persing et al., 2010). Argument Strength means how strongly an essay argues in favor of its thesis to persuade the readers (Persing and Ng, 2015). An example of the relation between coherence and an essay’s Organization is shown in Figure 1. The high-scored essay (i.e., Organization score of 4) first states its position regarding the prompt and then provides several reasons to strengthen the claim. It is considered coherent because it follows a logical order. However, the low-scored essay is not clear on its position and what it is arguing about. Therefore, it can be considered incoherent since it lacks logical sequencing. Previous studies on document embedding have primarily focused on capturing word similarity, word dependencies and"
P19-2053,D16-1193,0,0.214447,"ever, Wachsmuth et al. (2016) used an expensive argument parser to obtain such units. 3 3.1 z = tanh(W · [hbase ; haux ]) , (1) where W is a weight matrix. Finally, z is mapped to a scalar value by the sigmoid function. y = sigmoid(w · z + b) , where w is a weight vector, b is a bias value, and y is a score in the range of (0, 1). In the following subsections, we describe the details of each encoder. 3.2 Base Document Encoder The base document encoder produces a document representation hbase in Equation 1. For the base document encoder, we use the Neural Essay Assessor (NEA) model proposed by Taghipour and Ng (2016). This model uses three types of layers: an embedding layer, a Bi-directional Long ShortTerm Memory (BiLSTM) (Schuster and Paliwal, 1997) layer and a mean-over-time layer. Given the input essay of T words w1:T = (w1 , w2 , · · · , wT ), the embedding layer (Emb) produces a sequence of word embeddings w1:T = (w1 , w2 , · · · , wT ). w1:T = Emb(w1:T ) , where each word embedding is a dword dimenword sional vector, i.e. wi ∈ Rd . Then, taking x1:T as input, the BiLSTM layer produces a sequence of contextual representations h1:T = (h1 , h2 , · · · , hT ). h1:T = BiLSTM(x1:T ) , hidden where each r"
S16-1065,C14-1008,0,0.0838307,"aches. In traditional feature-based approaches, we extract various features from a text. The features are usually constructed from n-grams (e.g., bigrams) of the texts and external resources such as lexicons and unlabeled corpora. In Neural Network based approaches, a number of models for text classifications exist; for example, Feed-Forward Neural Network model using an average of embeddings of target word sequences as the input layer (Iyyer et al., 2015), Recursive Neural Network (Socher et al., 2011; Socher et al., 2013), and Convolutional Neural Network (CNN) (Johnson and Zhang, 2015; dos Santos and Gatti, 2014; Kim, 2014). In this paper, we compare feature-based and Neural Network based approaches on the supervised stance classification task for tweets, SemEval2016 Task 6 Subtask A (Mohammad et al., 2016). The feature-based approach classifies tweets using logistic regression model. The features are extracted using external knowledge such as SentiWordNet (Esuli and Sebastiani, 2006) and a collection of crawled tweets, in addition to unigrams or bigrams in the target tweet. For the Neural Network approach, we implement CNN based on Kim (2014). As the input embeddings, we use word embeddings trained"
S16-1065,esuli-sebastiani-2006-sentiwordnet,0,0.0360146,"verage of embeddings of target word sequences as the input layer (Iyyer et al., 2015), Recursive Neural Network (Socher et al., 2011; Socher et al., 2013), and Convolutional Neural Network (CNN) (Johnson and Zhang, 2015; dos Santos and Gatti, 2014; Kim, 2014). In this paper, we compare feature-based and Neural Network based approaches on the supervised stance classification task for tweets, SemEval2016 Task 6 Subtask A (Mohammad et al., 2016). The feature-based approach classifies tweets using logistic regression model. The features are extracted using external knowledge such as SentiWordNet (Esuli and Sebastiani, 2006) and a collection of crawled tweets, in addition to unigrams or bigrams in the target tweet. For the Neural Network approach, we implement CNN based on Kim (2014). As the input embeddings, we use word embeddings trained by Continuous Bag-Of-Words (CBOW) model (Mikolov et al., 2013) on Wikipedia articles. The experimental results show that the CNN based approach performed the best in the cross validation on the training data. However the tendency was opposite on the test data probably because the CNN model overfitted to the training data. In contrast, the feature-based approach was more robust,"
S16-1065,P15-1162,0,0.0332708,"Missing"
S16-1065,P11-1016,0,0.0265287,"target word “feminist”. Then we get a feature TS=n using the same rules in SWN features. (e.g. TS=p, TS=n) SentiWordSubject (SWS): This feature focuses on sentiment expressed by subjective pronouns such as “I” or “we”, which may indicate emotions or stances of the user of a tweet. We obtain a sentiment polarity from the word modifying a subjective pronoun in a tweet, and include it as a feature. A sentiment polarity is obtained by SentiWordNet using the same rules for SWN features. (e.g. SWS=I=love=&gt;p, SWS=We=hate=&gt;n) TargetSentiment (TS): We also consider sentiment or emotion for the topics. Jiang et al. (2011) HighPMI (P): We crawled tweets containing target words, and collected words cooccuring with seed keywords (Table 3) in all crawled tweets for each topic. Table 2 shows query words and the number of crawled tweets for each topic. Then we calculate Point-wise Mutual Information (PMI) for all words. If the word in a tweet is in top 300 of the PMI, we generate a feature. This feature detects a tweet containing words related to the topic. This feature may be effective to classify whether NONE or not. (e.g. P=humanist, P=meninist) 403 e.g. Feminists ?Q Embedding vector ? ? ? 1×? Feminist are are a"
S16-1065,N15-1011,0,0.0507734,"nd Neural Network based approaches. In traditional feature-based approaches, we extract various features from a text. The features are usually constructed from n-grams (e.g., bigrams) of the texts and external resources such as lexicons and unlabeled corpora. In Neural Network based approaches, a number of models for text classifications exist; for example, Feed-Forward Neural Network model using an average of embeddings of target word sequences as the input layer (Iyyer et al., 2015), Recursive Neural Network (Socher et al., 2011; Socher et al., 2013), and Convolutional Neural Network (CNN) (Johnson and Zhang, 2015; dos Santos and Gatti, 2014; Kim, 2014). In this paper, we compare feature-based and Neural Network based approaches on the supervised stance classification task for tweets, SemEval2016 Task 6 Subtask A (Mohammad et al., 2016). The feature-based approach classifies tweets using logistic regression model. The features are extracted using external knowledge such as SentiWordNet (Esuli and Sebastiani, 2006) and a collection of crawled tweets, in addition to unigrams or bigrams in the target tweet. For the Neural Network approach, we implement CNN based on Kim (2014). As the input embeddings, we"
S16-1065,D14-1181,0,0.0201032,"ature-based approaches, we extract various features from a text. The features are usually constructed from n-grams (e.g., bigrams) of the texts and external resources such as lexicons and unlabeled corpora. In Neural Network based approaches, a number of models for text classifications exist; for example, Feed-Forward Neural Network model using an average of embeddings of target word sequences as the input layer (Iyyer et al., 2015), Recursive Neural Network (Socher et al., 2011; Socher et al., 2013), and Convolutional Neural Network (CNN) (Johnson and Zhang, 2015; dos Santos and Gatti, 2014; Kim, 2014). In this paper, we compare feature-based and Neural Network based approaches on the supervised stance classification task for tweets, SemEval2016 Task 6 Subtask A (Mohammad et al., 2016). The feature-based approach classifies tweets using logistic regression model. The features are extracted using external knowledge such as SentiWordNet (Esuli and Sebastiani, 2006) and a collection of crawled tweets, in addition to unigrams or bigrams in the target tweet. For the Neural Network approach, we implement CNN based on Kim (2014). As the input embeddings, we use word embeddings trained by Continuou"
S16-1065,S16-1003,0,0.1106,"Missing"
S16-1065,W02-1011,0,0.0207052,"eatures. (e.g. BoD=hate=&gt;i, BoD=like=&gt;not) BagOfPOSTag (BoP): We also extract features from POS tags. For example, if a tweet contains several interjections, the user probably has a negative opinion to the topic. We include all unigrams of POS tags in a tweet as features. (e.g. BoP=NOUN, BoP=UH) SentiWordNet (SWN): Content words in a tweet may express some sentiment, which indicates stances and emotions of the user. We use SentiWordNet (Esuli and Sebastiani, 2006) for introducing sentiment of a word. It assigns positive/negative/objective scores to each word. In sentiment classification task, Pang et al. (2002) introduce SentiWordNet features. Following their work, we include sentiment polarity features for nouns, verbs, Classifier Atheism 3-way Polarity Topic + 2-way Polarity 0.5314 0.5327 ClimateChange is a Real concern 0.5144 0.5248 Feminist Movement Hillary Clinton 0.5735 0.5860 0.5273 0.5502 Legalization Of Abortion 0.5277 0.5290 ALL topics 0.6083 0.6188 Table 4: Comparison of 3-way Polarity Classifier with Topic + 2-way Polarity Classifier on 10-fold cross validation using the feature-based approach. The scores were measured in a macro average of micro-F1 scores of FAVOR and AGAINST for each t"
S16-1065,D11-1014,0,0.0876461,"Missing"
S16-1065,D13-1170,0,0.00549138,"ication tasks, there are two major approaches; feature-based and Neural Network based approaches. In traditional feature-based approaches, we extract various features from a text. The features are usually constructed from n-grams (e.g., bigrams) of the texts and external resources such as lexicons and unlabeled corpora. In Neural Network based approaches, a number of models for text classifications exist; for example, Feed-Forward Neural Network model using an average of embeddings of target word sequences as the input layer (Iyyer et al., 2015), Recursive Neural Network (Socher et al., 2011; Socher et al., 2013), and Convolutional Neural Network (CNN) (Johnson and Zhang, 2015; dos Santos and Gatti, 2014; Kim, 2014). In this paper, we compare feature-based and Neural Network based approaches on the supervised stance classification task for tweets, SemEval2016 Task 6 Subtask A (Mohammad et al., 2016). The feature-based approach classifies tweets using logistic regression model. The features are extracted using external knowledge such as SentiWordNet (Esuli and Sebastiani, 2006) and a collection of crawled tweets, in addition to unigrams or bigrams in the target tweet. For the Neural Network approach, w"
S19-1027,E17-2039,1,0.846664,"Missing"
S19-1027,P18-2103,0,0.357987,"inferences. Bowman et al. (2015b) proposed an artificial dataset for logical reasoning, whose premise and hypothesis are automatically generated from a simple English-like grammar. Following this line of work, Geiger et al. (2018) presented a method to construct a complex dataset for multiple quantifiers (e.g., Every dwarf licks no rifle ⇒ No ugly dwarf licks some rifle). These datasets contain downward inferences, but they are designed not to require lexical knowledge. There are also NLI datasets which expand lexical knowledge by replacing words using lexical rules (Monz and de Rijke, 2001; Glockner et al., 2018; Naik et al., 2018; Introduction Natural language inference (NLI) has been proposed as a benchmark task for natural language understanding. This task is to determine whether a given statement (premise) semantically entails another statement (hypothesis) (Dagan et al., 2013). Large crowdsourced datasets such as SNLI (Bowman et al., 2015a) and MultiNLI (Williams et al., 2018) have been created from naturally-occurring texts for training and testing neural models on NLI. Recent reports showed that these crowdsourced datasets contain undesired biases that allow prediction of entailment labels onl"
S19-1027,W17-6901,1,0.779854,"iers (dancing w happily dancing), or adding 3.1 Source corpus We use sentences from the Parallel Meaning Bank (PMB, Abzianidze et al., 2017) as a source while creating the inference dataset. The reason behind choosing the PMB is threefold. First, the finegrained annotations in the PMB facilitate our automatic monotonicity-driven construction of inference problems. In particular, semantic tokenization and WordNet (Fellbaum, 1998) senses make narrow and broad concept substitutions easy while the syntactic analyses in Combinatory Categorial Grammar (CCG, Steedman, 2000) format and semantic tags (Abzianidze and Bos, 2017) contribute to monotonicity and polarity detection. Second, the PMB contains lexically and syntactically diverse texts from a wide range of genres. Third, the gold (silver) documents are fully (partially) manually verified, which control noise in the automated generated dataset. To prevent easy inferences, we use the sentences with more than five tokens from 5K gold and 5K silver portions of the PMB. 1 Our dataset and its generation code will be made publicly available at https://github.com/verypluming/HELP. 251 Section Size Up 7784 Down 21192 All [NP kids↓] were [VP dancing on the floor↑] Non"
S19-1027,N18-2017,0,0.0624952,"Missing"
S19-1027,D15-1075,0,0.0783552,"sentence can become longer, yet the inference is valid. FraCaS (Cooper et al., 1994) contains such logically challenging problems as downward inferences. However, it is small in size (only 346 examples) for training neural models, and it covers only simple syntactic patterns with severely restricted vocabularies. The lack of such a dataset on a large scale is due to at least two factors: it is hard to instruct crowd workers without deep knowledge of natural language syntax and semantics, and it is also unfeasible to employ experts to obtain a large number of logically challenging inferences. Bowman et al. (2015b) proposed an artificial dataset for logical reasoning, whose premise and hypothesis are automatically generated from a simple English-like grammar. Following this line of work, Geiger et al. (2018) presented a method to construct a complex dataset for multiple quantifiers (e.g., Every dwarf licks no rifle ⇒ No ugly dwarf licks some rifle). These datasets contain downward inferences, but they are designed not to require lexical knowledge. There are also NLI datasets which expand lexical knowledge by replacing words using lexical rules (Monz and de Rijke, 2001; Glockner et al., 2018; Naik et a"
S19-1027,2014.lilt-9.7,0,0.233344,"ators and syntactic structures. 3 Monotonicity Reasoning Data Creation We address three issues when creating the inference problems: (a) Detect the monotone operators and their arguments; (b) Based on the syntactic structure, induce the polarity of the argument positions; (c) Using lexical knowledge or logical connectives, narrow or broaden the arguments. Monotonicity reasoning is a sort of reasoning based on word replacement. Based on the monotonicity properties of words, it determines whether a certain word replacement results in a sentence entailed from the original one (van Benthem, 1983; Icard and Moss, 2014). A polarity is a characteristic of a word position imposed by monotone operators. Replacements with more general (or specific) phrases in ↑ (or ↓) polarity positions license entailment. Polarities are determined by a function which is always upward monotone (+) (i.e., an order preserving function that licenses entailment from specific to general phrases), always downward monotone (−) (i.e., an order reversing function) or neither, non-monotone. Determiners are modeled as binary operators, taking noun and verb phrases as the first and second arguments, respectively, and they entail sentences w"
S19-1027,W15-4002,0,0.172203,"sentence can become longer, yet the inference is valid. FraCaS (Cooper et al., 1994) contains such logically challenging problems as downward inferences. However, it is small in size (only 346 examples) for training neural models, and it covers only simple syntactic patterns with severely restricted vocabularies. The lack of such a dataset on a large scale is due to at least two factors: it is hard to instruct crowd workers without deep knowledge of natural language syntax and semantics, and it is also unfeasible to employ experts to obtain a large number of logically challenging inferences. Bowman et al. (2015b) proposed an artificial dataset for logical reasoning, whose premise and hypothesis are automatically generated from a simple English-like grammar. Following this line of work, Geiger et al. (2018) presented a method to construct a complex dataset for multiple quantifiers (e.g., Every dwarf licks no rifle ⇒ No ugly dwarf licks some rifle). These datasets contain downward inferences, but they are designed not to require lexical knowledge. There are also NLI datasets which expand lexical knowledge by replacing words using lexical rules (Monz and de Rijke, 2001; Glockner et al., 2018; Naik et a"
S19-1027,P17-1152,0,0.115701,"Missing"
S19-1027,marelli-etal-2014-sick,0,0.0337584,"ho don’t value my time). These problems contain disjunction or modifiers in downward environments where either (i) the premise P contains all words in the hypothesis H yet the inference is invalid or (ii) H contains more words than those in P yet the inference is valid.2 Although HELP contains 21K such problems, the models nevertheless misclassified them. This indicates that the difficulty in learning these non-lexical downward inferences might not come from the lack of training datasets. conjunction, and disjunction sections), (ii) FraCaS (the generalized quantifier section), (iii) the SICK (Marelli et al., 2014) test set, and (iv) MultiNLI matched/mismatched test set. We used the Matthews correlation coefficient (ranging [−1, 1]) as the evaluation metric for GLUE. Regarding other datasets, we used accuracy as the metric. We also check if our data augmentation does not decrease the performance on MultiNLI. 4.2 Results and discussion Table 3 shows that adding HELP to MultiNLI improved the accuracy of all models on GLUE, FraCaS, and SICK. Regarding MultiNLI, note that adding data for downward inference can be harmful for performing upward inference, because lexical replacements work in an opposite way i"
S19-1027,C18-1198,0,0.0606078,"al. (2015b) proposed an artificial dataset for logical reasoning, whose premise and hypothesis are automatically generated from a simple English-like grammar. Following this line of work, Geiger et al. (2018) presented a method to construct a complex dataset for multiple quantifiers (e.g., Every dwarf licks no rifle ⇒ No ugly dwarf licks some rifle). These datasets contain downward inferences, but they are designed not to require lexical knowledge. There are also NLI datasets which expand lexical knowledge by replacing words using lexical rules (Monz and de Rijke, 2001; Glockner et al., 2018; Naik et al., 2018; Introduction Natural language inference (NLI) has been proposed as a benchmark task for natural language understanding. This task is to determine whether a given statement (premise) semantically entails another statement (hypothesis) (Dagan et al., 2013). Large crowdsourced datasets such as SNLI (Bowman et al., 2015a) and MultiNLI (Williams et al., 2018) have been created from naturally-occurring texts for training and testing neural models on NLI. Recent reports showed that these crowdsourced datasets contain undesired biases that allow prediction of entailment labels only from hypothesis s"
S19-1027,W18-5441,0,0.0640217,"Missing"
S19-1027,S18-2023,0,0.0660154,"Missing"
S19-1027,L18-1239,0,0.0559481,"proposed as a benchmark task for natural language understanding. This task is to determine whether a given statement (premise) semantically entails another statement (hypothesis) (Dagan et al., 2013). Large crowdsourced datasets such as SNLI (Bowman et al., 2015a) and MultiNLI (Williams et al., 2018) have been created from naturally-occurring texts for training and testing neural models on NLI. Recent reports showed that these crowdsourced datasets contain undesired biases that allow prediction of entailment labels only from hypothesis sentences (Gururangan et al., 2018; Poliak et al., 2018b; Tsuchiya, 2018). Moreover, these standard datasets come with the so-called 250 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 250–255 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics Poliak et al., 2018a). In these works, however, little attention has been paid to downward inferences. The GLUE leaderboard (Wang et al., 2019) reported that neural models did not perform well on downward inferences, and this leaves us guessing whether the lack of large datasets for such kind of inferences that involve the interaction between lexical a"
S19-1027,N18-1101,0,0.0822646,"These datasets contain downward inferences, but they are designed not to require lexical knowledge. There are also NLI datasets which expand lexical knowledge by replacing words using lexical rules (Monz and de Rijke, 2001; Glockner et al., 2018; Naik et al., 2018; Introduction Natural language inference (NLI) has been proposed as a benchmark task for natural language understanding. This task is to determine whether a given statement (premise) semantically entails another statement (hypothesis) (Dagan et al., 2013). Large crowdsourced datasets such as SNLI (Bowman et al., 2015a) and MultiNLI (Williams et al., 2018) have been created from naturally-occurring texts for training and testing neural models on NLI. Recent reports showed that these crowdsourced datasets contain undesired biases that allow prediction of entailment labels only from hypothesis sentences (Gururangan et al., 2018; Poliak et al., 2018b; Tsuchiya, 2018). Moreover, these standard datasets come with the so-called 250 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 250–255 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics Poliak et al., 2018a). In these works, h"
S19-1027,D18-1007,0,\N,Missing
S19-1027,N19-1423,0,\N,Missing
S19-1027,W19-4810,0,\N,Missing
S19-2185,P15-1162,0,0.0525847,"Missing"
S19-2185,S19-2145,0,0.132591,"Missing"
S19-2185,D14-1181,0,0.0107513,"f1 ∈ RD1 , f2 ∈ RD2 and f3 ∈ RD3 and D = D1 + D2 + D3 . As f1 , f2 and f3 , we design the following features. • f1 : BERT feature (Section 2.2) 1057 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1057–1061 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics • f2 : Article length feature (Section 2.3) 2. BiLSTM: Using the representations as input to BiLSTM. This is the same method as the best performing one reported by Devlin et al. (2018). 3. CNN: Using the representations as input to CNN in the same way as Kim (2014). • f3 : Informative phrase feature (Section 2.4) For training our classifiers, we used only the by-article dataset but not the by-publisher dataset. This is because the labels of the by-publisher dataset turned out rather noisy. In our preliminary experiments, we found that the performance drops when training the classifiers on the by-publisher dataset. Furthermore, we apply the following three techniques. 1. Word dropout: We adopted word dropout (Iyyer et al., 2015) for regularization. The dropout rate was set to 0.3. 2. Over sampling: As mentioned above, the gold label distribution of the t"
S19-2185,D14-1162,0,0.100011,"Spl is defined as follows: Spl = {xj |ftrue (xj ) > Tf , j ≤ Tm }. (8) 4. ftrue -ffalse ratio-based: The fourth setting is to select N -grams based on ratios between ftrue and ffalse . Concretely, Spl is defined as follows:   ftrue (xj ) Spl = xj > Tr , j ≤ Tm . (9) ffalse (xj ) To , Tc , Tf , Tr and Tm are hyper-parameters2 . Next, we obtain Sp defined as follows: [ Sp = Spl . (10) l At last, we obtain an filtered N -gram set S defined as follows: S = Sh  Sp . (11) 2.4.2 Phrase Embedding We map each of the obtained N -gram phrase set S to a feature vector f3 . We exploited GloVe vectors (Pennington et al., 2014) instead of one-hot vectors in order to facilitate generalization. First, we enumerate N -grams included in an article and compute each N -gram vector. Each vector is the average of GloVe vectors of included words. For example, the vector for the phrase “news article” is computed as follows: GloVe(news) + GloVe(article) . 2 Here, GloVe(w) denotes the GloVe (glove.840B3 ) vector of the word w. Then, we compute f3 as the average of all N -gram vectors included in the article. 2 In our experiments, we fix Tm to 200,000. https://nlp.stanford.edu/projects/ glove/ 1059 3 Method Average BiLSTM CNN Ac"
S19-2185,P18-1022,0,0.0392065,"Missing"
W01-0814,J98-4002,1,\N,Missing
W01-0814,A00-2002,0,\N,Missing
W01-0814,J94-2003,0,\N,Missing
W01-0814,W96-0401,0,\N,Missing
W01-0814,C00-1031,0,\N,Missing
W01-0814,P99-1072,0,\N,Missing
W01-0814,J95-2003,0,\N,Missing
W01-0814,W98-1415,0,\N,Missing
W01-0814,P86-1031,0,\N,Missing
W01-0814,W98-1414,0,\N,Missing
W03-1602,P01-1008,0,0.0389931,"To create such a system, one needs to feed it with a large collection of paraphrase patterns. Very timely, the acquisition of paraphrase patterns has been actively studied in recent years:  Manual collection of paraphrases in the context of language generation, e.g. (Robin and McKeown, 1996),  Derivation of paraphrases through existing lexical resources, e.g. (Kurohashi et al., 1999),  Corpus-based statistical methods inspired by the work on information extraction, e.g. (Jacquemin, 1999; Lin and Pantel, 2001), and  Alignment-based acquisition of paraphrases from comparable corpora, e.g. (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). One remaining issue is how effectively these methods contribute to the generation of paraphrases in our application-oriented context. 2.3 Paraphrase representation One of the findings obtained in the previous studies for paraphrase acquisition is that the automatic acquisition of candidates of paraphrases is quite realizable for various types of source data but acquired collections tend to be rather noisy and need manual cleaning as reported in, for example, (Lin and Pantel, 2001). Given that, it turns out to be important to devise an effective"
W03-1602,N03-1003,0,0.0148725,"a large collection of paraphrase patterns. Very timely, the acquisition of paraphrase patterns has been actively studied in recent years:  Manual collection of paraphrases in the context of language generation, e.g. (Robin and McKeown, 1996),  Derivation of paraphrases through existing lexical resources, e.g. (Kurohashi et al., 1999),  Corpus-based statistical methods inspired by the work on information extraction, e.g. (Jacquemin, 1999; Lin and Pantel, 2001), and  Alignment-based acquisition of paraphrases from comparable corpora, e.g. (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). One remaining issue is how effectively these methods contribute to the generation of paraphrases in our application-oriented context. 2.3 Paraphrase representation One of the findings obtained in the previous studies for paraphrase acquisition is that the automatic acquisition of candidates of paraphrases is quite realizable for various types of source data but acquired collections tend to be rather noisy and need manual cleaning as reported in, for example, (Lin and Pantel, 2001). Given that, it turns out to be important to devise an effective way of facilitating manual correction and a sta"
W03-1602,P98-1056,0,0.15982,"lly expressible formalism for representing paraphrases at the level of tree-to-tree transformation and (b) devise an additional layer of representation on its top that is designed to facilitate handcoding transformation rules. 2.4 Post-transfer text revision In paraphrasing, the morpho-syntactic information of a source sentence should be accessible throughout the transfer process since a morphosyntactic transformation in itself can often be a motivation or goal of paraphrasing. Therefore, such an approach as semantic transfer, where morphosyntactic information is highly abstracted away as in (Dorna et al., 1998; Richardson et al., 2001), does not suit this task. Provided that the morphosyntactic stratum be an optimal level of abstraction for representing paraphrasing/transfer patterns, one must recall that semantic-transfer approaches such as those cited above were motivated mainly by the need for reducing the complexity of transfer knowledge, which could be unmanageable in morpho-syntactic transfer. Our approach to this problem is to (a) leave the description of each transfer pattern underspecified and (b) implement the knowledge about linguistic constraints that are independent of a particular tra"
W03-1602,W01-0814,1,0.810902,"Missing"
W03-1602,P99-1044,0,0.0104458,"ding assistance system is, therefore, hoped to be able to generate sufficient varieties of paraphrases of a given input. To create such a system, one needs to feed it with a large collection of paraphrase patterns. Very timely, the acquisition of paraphrase patterns has been actively studied in recent years:  Manual collection of paraphrases in the context of language generation, e.g. (Robin and McKeown, 1996),  Derivation of paraphrases through existing lexical resources, e.g. (Kurohashi et al., 1999),  Corpus-based statistical methods inspired by the work on information extraction, e.g. (Jacquemin, 1999; Lin and Pantel, 2001), and  Alignment-based acquisition of paraphrases from comparable corpora, e.g. (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). One remaining issue is how effectively these methods contribute to the generation of paraphrases in our application-oriented context. 2.3 Paraphrase representation One of the findings obtained in the previous studies for paraphrase acquisition is that the automatic acquisition of candidates of paraphrases is quite realizable for various types of source data but acquired collections tend to be rather noisy and need m"
W03-1602,P99-1062,0,0.025313,"Missing"
W03-1602,A00-1009,0,0.0218102,"Meaning-preserving vs. reference-preserving paraphrases It is also useful to distinguish reference-preserving paraphrases from meaningpreserving ones. The above example in (3) is of the reference-preserving type. This types of paraphrasing requires the computation of reference to objects outside discourse and thus should be excluded from our scope for the present purpose. 4.2 Dependency trees (MDSs) Previous work on transfer-based machine translation (MT) suggests that the dependency-based representation has the advantage of facilitating syntactic transforming operations (Meyers et al., 1996; Lavoie et al., 2000). Following this, we adopt dependency trees as the internal representations of target texts. We suppose that a dependency tree consists of a set of nodes each of which corresponds to a lexeme or compound and a set of edges each of which represents the dependency relation between its ends. We call such a dependency tree a morpheme-based dependency structure (MDS). Each node in an MDS is supposed to be annotated with an open set of typed features that indicate morpho-syntactic and semantic information. We also assume a type hierarchy in dependency relations that consists of an open set of depend"
W03-1602,W97-0508,0,0.0659332,"Missing"
W03-1602,C96-1078,0,0.0432722,"cope our discussion. Meaning-preserving vs. reference-preserving paraphrases It is also useful to distinguish reference-preserving paraphrases from meaningpreserving ones. The above example in (3) is of the reference-preserving type. This types of paraphrasing requires the computation of reference to objects outside discourse and thus should be excluded from our scope for the present purpose. 4.2 Dependency trees (MDSs) Previous work on transfer-based machine translation (MT) suggests that the dependency-based representation has the advantage of facilitating syntactic transforming operations (Meyers et al., 1996; Lavoie et al., 2000). Following this, we adopt dependency trees as the internal representations of target texts. We suppose that a dependency tree consists of a set of nodes each of which corresponds to a lexeme or compound and a set of edges each of which represents the dependency relation between its ends. We call such a dependency tree a morpheme-based dependency structure (MDS). Each node in an MDS is supposed to be annotated with an open set of typed features that indicate morpho-syntactic and semantic information. We also assume a type hierarchy in dependency relations that consists of"
W03-1602,P01-1051,0,0.0130024,"evision would make the system tolerant to flows in paraphrasing rules. While many researchers have addressed the issue of paraphrase acquisition reporting promising results as cited above, the other three issues have been left relatively unexplored in spite of their significance in the above sense. Motivated by this context, in the rest of this paper, we address these remaining three. 3 Readability assessment To the best of our knowledge, there have never been no reports on research to build a computational model of the language proficiency of deaf people, except for the remarkable reports by Michaud and McCoy (2001). As a subpart of their research aimed at developing the ICICLE system (McCoy and Masterman, 1997), a language-tutoring application for deaf learners of written English, Michaud and McCoy developed an architecture for modeling the writing proficiency of a user called SLALOM. SLALOM is designed to capture the stereotypic linear order of acquisition within certain categories of morphological and/or syntactic features of language. Unfortunately, the modeling method used in SLALOM cannot be directly applied to our domain for three reasons.  Unlike writing tutoring, in reading assistance, target s"
W03-1602,W03-2314,0,0.0708175,"Missing"
W03-1602,W03-2317,0,0.0406903,"ree subprocesses: a. Problem identification: identify which portions of a given text will be difficult for a given user to read, b. Paraphrase generation: generate possible candidate paraphrases from the identified portions, and c. Evaluation: re-assess the resultant texts to choose the one in which the problems have been resolved. Given this decomposition, it is clear that one of the key issues in reading assistance is the problem of assessing the readability or comprehensibility1 of text because it is involved in subprocesses (a) and (c). Readability assessment is doubtlessly a tough issue (Williams et al., 2003). In this project, however, we argue that, if one targets only a particular population segment and if an adequate collection of data is available, then corpus-based empirical approaches may well be feasible. We have already proven that one can collect such readability assessment data by conducting survey questionnaires targeting teachers at schools for the deaf. 1 In this paper, we use the terms readability and comprehensibility interchangeably, while strictly distinguishing them from legibility of each fragment (typically, a sentence or paragraph) of a given text. 2.2 Paraphrase acquisition O"
W03-1602,W01-1402,0,\N,Missing
W03-1602,C98-1054,0,\N,Missing
W04-0402,P01-1008,0,0.0672653,"as those indicating that “to make an attempt” can be paraphrased into “to attempt,” and that “potential” can be paraphrased into “possibility.” Several attempts have been made to develop such resources manually (Sato, 1999; Dras, 1999; Inui and Nogami, 2001); those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct a sufficiently comprehensive resource for practical applications. There is another trend in the research in this field, namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Pang et al., 2003; Shinyama and Sekine, 2003, etc.). This type of approach may be able to reduce the cost of resource development. There are problems that must be overcome, however, before they can work practically. First, automatically acquired patterns tend to be complex. For example, from the paraphrase of (4s) into (4t), we can naively obtain the pattern: “X is purchased by Y ⇒ Y buys X.” (4) s. This car was purchased by him. t. He bought this car. This could also, however, be regarded as a combination of a simpler pattern of lexical paraphrasing (“purchase ⇒ buy”)"
W04-0402,E99-1042,0,0.0232588,"ing LCS dictionary. Experimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases. 1 Introduction Automatic paraphrase generation technology offers the potential to bridge gaps between the authors and readers of documents. For example, a system that is capable of simplifying a given text, or showing the user several alternative expressions conveying the same content, would be useful for assisting a reader (Carroll et al., 1999; Inui et al., 2003). In Japanese, like other languages, there are several classes of paraphrasing that exhibit a degree of regularity that allows them to be explained by a handful of sophisticated general rules and lexical semantic knowledge. For example, paraphrases associated with voice alteration, verb/case alteration, compounds, and lexical derivations all fall into such classes. In this paper, we focus our discussion on another useful class of paraphrases, namely, the paraphrasing of light-verb constructions (LVCs), and propose a computational model for generating paraphrases of this cla"
W04-0402,W01-0814,1,0.592452,"iments (Section 5). Finally, we conclude this paper with a brief of description of work to be done in the future (Section 6). 2 Motivation, target, and related work 2.1 Motivation One of the critical issues that we face in paraphrase generation is how to develop and maintain knowledge resources that covers a sufficiently wide range of paraphrasing patterns such as those indicating that “to make an attempt” can be paraphrased into “to attempt,” and that “potential” can be paraphrased into “possibility.” Several attempts have been made to develop such resources manually (Sato, 1999; Dras, 1999; Inui and Nogami, 2001); those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct a sufficiently comprehensive resource for practical applications. There is another trend in the research in this field, namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Pang et al., 2003; Shinyama and Sekine, 2003, etc.). This type of approach may be able to reduce the cost of resource development. There are problems that must be overcome, however, before they can work practi"
W04-0402,W03-1602,1,0.844244,"perimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases. 1 Introduction Automatic paraphrase generation technology offers the potential to bridge gaps between the authors and readers of documents. For example, a system that is capable of simplifying a given text, or showing the user several alternative expressions conveying the same content, would be useful for assisting a reader (Carroll et al., 1999; Inui et al., 2003). In Japanese, like other languages, there are several classes of paraphrasing that exhibit a degree of regularity that allows them to be explained by a handful of sophisticated general rules and lexical semantic knowledge. For example, paraphrases associated with voice alteration, verb/case alteration, compounds, and lexical derivations all fall into such classes. In this paper, we focus our discussion on another useful class of paraphrases, namely, the paraphrasing of light-verb constructions (LVCs), and propose a computational model for generating paraphrases of this class. Sentence (1s) is"
W04-0402,W02-2016,1,0.676943,"alized verbs. We retrieved 1,210 nominalized verbs from the TLCS dictionary. Light-verbs: Since a verb takes different meanings when it is a part of LVCs with different case particles, we collected pairs c, v of case particle c and verb v in the following way: Step 1. We collected 876,101 types of triplets n, c, v of nominalized verb n, case particle c, and base form of verb v from the parsed5 sentences of newspaper articles6 . 4 A sahen-noun is a verbal noun in Japanese, which acts as a verb in the form of “sahen-noun + suru”. 5 We used the statistical Japanese dependency parser CaboCha (Kudo and Matsumoto, 2002) for parsing. http://chasen.naist.jp/˜taku/software/cabocha/ 6 Excerpts from 9 years of the Mainichi Shinbun and 10 years of the Nihon Keizai Shinbun, giving a total of 25,061,504 sentences, were used. Table 2: Extensions of LCS Ext.1 Verb hankou-suru (resist) Ext.2 ukeru (receive) Ext.3 motomeru (ask) Ext.4 kandou-suru (be impressed) Verb phrase and its LCS representation [[Ken]y BE AGAINST [parents]z] Ken-ga oya-ni hankou-suru. Ken-NOM parents-DAT resist- PRES (Ken resists his parents.) [BECOME [[salesclerk]z BE WITH [[complaint]y MOVE FROM [customer]x TO [salesclerk]z]]] Ten’in-ga kyaku-kar"
W04-0402,J87-3006,0,0.0613912,"Missing"
W04-0402,N03-1024,0,0.0405069,"be paraphrased into “to attempt,” and that “potential” can be paraphrased into “possibility.” Several attempts have been made to develop such resources manually (Sato, 1999; Dras, 1999; Inui and Nogami, 2001); those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct a sufficiently comprehensive resource for practical applications. There is another trend in the research in this field, namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Pang et al., 2003; Shinyama and Sekine, 2003, etc.). This type of approach may be able to reduce the cost of resource development. There are problems that must be overcome, however, before they can work practically. First, automatically acquired patterns tend to be complex. For example, from the paraphrase of (4s) into (4t), we can naively obtain the pattern: “X is purchased by Y ⇒ Y buys X.” (4) s. This car was purchased by him. t. He bought this car. This could also, however, be regarded as a combination of a simpler pattern of lexical paraphrasing (“purchase ⇒ buy”) and a voice activization (“X (b) Noun + C"
W04-0402,W03-1609,0,0.0123349,"o “to attempt,” and that “potential” can be paraphrased into “possibility.” Several attempts have been made to develop such resources manually (Sato, 1999; Dras, 1999; Inui and Nogami, 2001); those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct a sufficiently comprehensive resource for practical applications. There is another trend in the research in this field, namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Pang et al., 2003; Shinyama and Sekine, 2003, etc.). This type of approach may be able to reduce the cost of resource development. There are problems that must be overcome, however, before they can work practically. First, automatically acquired patterns tend to be complex. For example, from the paraphrase of (4s) into (4t), we can naively obtain the pattern: “X is purchased by Y ⇒ Y buys X.” (4) s. This car was purchased by him. t. He bought this car. This could also, however, be regarded as a combination of a simpler pattern of lexical paraphrasing (“purchase ⇒ buy”) and a voice activization (“X (b) Noun + Case Particle (c) Noun + Cas"
W07-1522,P06-1079,1,0.472904,"rpus version 4.0 (Kawahara et al., 2002) and the GDATagged Corpus (Hasida, 2005). However, there is still much room for reﬁning their speciﬁcations. For this reason, we discuss issues in annotating these two types of relations, and propose a new speciﬁcation for each. In accordance with the speciﬁcation, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1 , which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing ﬁeld of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several anno"
W07-1522,kawahara-etal-2002-construction,0,0.390972,"Japanese Text Corpus with Predicate-Argument and Coreference Relations Ryu Iida, Mamoru Komachi, Kentaro Inui and Yuji Matsumoto Graduate School of Information Science, Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {ryu-i,mamoru-k,inui,matsu}@is.naist.jp Abstract In this paper, we discuss how to annotate coreference and predicate-argument relations in Japanese written text. There have been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations as are done in the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDATagged Corpus (Hasida, 2005). However, there is still much room for reﬁning their speciﬁcations. For this reason, we discuss issues in annotating these two types of relations, and propose a new speciﬁcation for each. In accordance with the speciﬁcation, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1 , which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coref"
W07-1522,P02-1014,0,0.136204,"ference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing ﬁeld of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed for several years. On t"
W07-1522,J05-1004,0,0.129176,"notated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed for several years. On the other hand, the speciﬁcation of predicate-argument structure analysis has mainly been discussed in the context of the CoNLL shared task2 on the basis of the PropBank (Palmer et al., 2005). In parallel with these efforts, there have also been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations such as the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDA3 -Tagged Corpus (Hasida, 2005). However, as we discuss in this paper, there is still much room for arguing and reﬁning the speciﬁcation of such sorts of semantic annotation. In fact, for neither of the above two corpora, the adequacy and reliability of the annotation scheme has been deeply examined. In this paper, we discuss how to annotate coreferen"
W07-1522,P04-1019,0,0.0132674,"uch as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed for several years. On the other hand, the speciﬁcation of predicate-argument structure analysis has mainly been discussed in the context of the CoNLL shared task2 on the basis of the PropBank (Palmer et al., 2005). In parallel with these efforts, there have also been research a"
W07-1522,J01-4004,0,0.294742,"ata set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing ﬁeld of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed fo"
W07-1522,P06-2105,0,0.00720716,"ikely to be more cost-efﬁcient than semantic roles 134 because they are often explicitly marked by case markers. This fact also allows us to avoid the difﬁculties in deﬁning a label set. • In Japanese, the mapping from syntactic cases to semantic roles tends to be reasonably straightforward if a semantically rich lexicon of verbs like the VerbNet (Kipper et al., 2000) is available. • Furthermore, we have not yet found many NLP applications for which the utility of semantic roles is actually demonstrated. One may think of using semantic roles in textual inference as exempliﬁed by, for example, Tatu and Moldovan (2006). However, similar sort of inference may well be realized with syntactic cases as demonstrated in the information extraction and question answering literature. Taking these respects into account, we choose to label predicate-argument relations in terms of syntactic cases, which follows the annotation scheme adopted in the Kyoto Corpus. 3.2 Syntactic case alternation Once the level of syntactic cases is chosen for our annotation, another issue immediately arises, alteration of syntactic cases by syntactic transformations such as passivization and causativization. For example, sentence (5) is an"
W07-1522,W99-0213,0,0.0702454,"Missing"
W07-1522,M95-1005,0,0.107471,"Missing"
W07-1522,W04-2705,0,\N,Missing
W07-1522,M98-1029,0,\N,Missing
W09-3027,I05-5002,0,0.0382202,"ated as a gold standard and the other’s as the output of the system, as shown in Talbe 2. Corpus Construction Procedure We automatically gather sentences on related topics by following the procedure below: 1. Retrieve documents related to a set number of topics using a search engine 2. Extract real sentences that include major subtopic words which are detected based on TF or DF in the document set 3. Reduce noise in data by using heuristics to eliminate advertisements and comment spam 4. Reduce the search space for identifying sentence pairs and prepare pairs, which look feasible to annotate. Dolan and Brockett (2005) proposed a method to narrow the range of sentence pair candidates and collect candidates of sentence-level paraphrases which correspond [EQUIVALENCE] in [AGREEMENT] class in our task. It worked well 152 scheme complete with the necessary semantic relations to support the development of statement maps that show [AGREEMENT], [CONFLICT], and [EVIDENCE] between statements for assisting users in analyzing credibility of information in Web. We discussed the revelations made from annotating our corpus, and discussed future directions for refining our specifications of the corpus. We are planning to"
W09-3027,W00-1009,0,0.0898713,"diagnosed with autism. !Vaccinations are given around the same time children can be first diagnosed. [CONFLICT]! !The plural of anecdote is not data. Figure 1: An example S TATEMENT M AP for the query “Do vaccines cause autism?” b. Vaccines can trigger autism in a vulnerable subset of children. While it is difficult to assign any relation to this pair in an RTE framework, in order to construct statement maps we need to recognize a contradiction between (1a) and (1b). There is another task of recognizing relations between sentences, CST (Cross-Document Structure Theory) which was developed by Radev (2000). CST is an expanded rhetorical structure analysis based on RST (Mann and Thompson, 1988), and attempts to describe relations between two or more sentences from both single and multiple document sets. The CSTBank corpus (Radev et al., 2003) was constructed to annotate crossdocument relations. CSTBank is divided into clusters in which topically-related articles are gathered. There are 18 kinds of relations in this corpus, including [EQUIVALENCE], [ELABORATION], and [REFINEMENT]. 2.2 Facts and Opinions RTE is used to recognize logical and factual relations between sentences in a pair, and CST is"
W09-3027,sumida-etal-2008-boosting,0,0.0540518,"ed autism in children. A: I don’t think vaccines cause autism. B: I believe vaccines are the cause of my son’s autism. (E) [EQUIVALENCE]! There is no link between the MMR vaccine and autism.! for collecting valid sentence pairs from a large cluster which was constituted by topic-related sentences. The method also seem to work well for [CONFLICT] relations, because lexical similarity based on bag-of-words (BOW) can narrow the range of candidates with this relation as well. We calculate the lexical similarity between the two sentences based on BOW. We also used hyponym and synonym dictionaries (Sumida et al., 2008) and a database of relations between predicate argument structures (Matsuyoshi et al., 2008) as resources. According to our preliminary experiments, unigrams of KANJI and KATAKANA expressions, single and compound nouns, verbs and adjectives worked well as features, and we calculate the similarity using cosine distance. We did not use HIRAGANA expressions because they are also used in function words. The weight of the evidence indicates that vaccines are not associated with autism.! (D) Statement (2)! Vaccines are not associated with autism.! Figure 2: Extracting statements from sentences and a"
W09-3027,radev-etal-2004-cst,0,\N,Missing
W10-3201,P98-1013,0,0.101887,"” can be described? The shared meaning of lend and give in the above sentences is that they are categorized to Giving Verbs, as in Levin’s English Verb Classes and Alternations (EVCA) (Levin, 1993), while the different meaning will be that lend does not imply ownership of the theme, i.e., a bicycle. One of the problematic issues with describing shared meaning among verbs is that semantic classes such as Giving Verbs should be dependent on the granularity of meanings we assumed. For example, the meaning of lend and give in the above sentences is not categorized into the same Frame in FrameNet (Baker et al., 1998). The reason for this different categorization can be considered to be that the granularity of the semantic class of Giving Verbs is larger than that of the Giving Frame in FrameNet1 . From the view of natural language processing, especially dealing the with propositional meaning of verbs, all of the above classes, i.e., the wider class of Giving Verbs containing lend and give as well as the narrower class of Giving Frame containing give and donate, are needed. Therefore, in this work, in order to describe verb meanings with several granularities of semantic classes, a thesaurus form is adopte"
W10-3201,J05-1004,0,0.0127323,"to solve ne disambiguation between verbs when we map a verb in a sentence to WordNet; (3) the basic verbs of Japanese (i.e., highly ambiguous verbs) are wrongly assigned to unrelated synsets because they are constructed by translation from English to Japanese. 2 Existing Lexical Resources and Drawbacks 2.1 Lexical Resources in Japanese Lexical Resources in English From the view of previous lexical databases In English, several well-considered lexical databases are available, e.g., EVCA, Dorr’s LCS (Dorr, 1997), FrameNet, WordNet (Fellbaum, 1998), VerbNet (Kipper-Schuler, 2005) and PropBank (Palmer et al., 2005). Besides there is the research project (Pustejovsky and Meyers, 2005) to nd general descriptional framework of predicate argument structure by merging several lexical databases such as PropBank, NomBank, TimeBank and PennDiscouse Treebank. Our approach corresponds partly to each lexical database, (i.e., FrameNet’s Frame and FrameElements correspond to our verb class and semantic role labels, and the way to organize verb similarity classes with thesaurus corresponds with WordNet’s synset), but is not exactly the same; namely, there is no lexical database describing several granularities of se"
W10-3201,W05-0302,0,0.0150667,"in a sentence to WordNet; (3) the basic verbs of Japanese (i.e., highly ambiguous verbs) are wrongly assigned to unrelated synsets because they are constructed by translation from English to Japanese. 2 Existing Lexical Resources and Drawbacks 2.1 Lexical Resources in Japanese Lexical Resources in English From the view of previous lexical databases In English, several well-considered lexical databases are available, e.g., EVCA, Dorr’s LCS (Dorr, 1997), FrameNet, WordNet (Fellbaum, 1998), VerbNet (Kipper-Schuler, 2005) and PropBank (Palmer et al., 2005). Besides there is the research project (Pustejovsky and Meyers, 2005) to nd general descriptional framework of predicate argument structure by merging several lexical databases such as PropBank, NomBank, TimeBank and PennDiscouse Treebank. Our approach corresponds partly to each lexical database, (i.e., FrameNet’s Frame and FrameElements correspond to our verb class and semantic role labels, and the way to organize verb similarity classes with thesaurus corresponds with WordNet’s synset), but is not exactly the same; namely, there is no lexical database describing several granularities of semantic classes between verbs with arguments. Of course, since the abov"
W10-3201,P06-4017,0,0.218472,"le inheritance. The most ne-grained verb class before individual verb sense is a little wider than alternations. Currently, for the ne-grained verb class, we are organizing what kind of differentiated classes can be assumed (e.g., manner, background, presupposition, and etc.). 3.3 To organize hierarchical semantic verb class, we take a top down and a bottom up approaches. As for a bottom up approach, we use verb senses dened by a dictionary as the most ne-grained meaning; and then we group verbs that can be considered to share some meaning. As for a dictionary, we use the Lexeed database (Fujita et al., 2006), which consists of more than 20,000 verbs with explanations of word sense and example sentences. As a top down approach, we take three semantic classes: State, Change of State, and Activity as top level semantic classes of the thesaurus according to Vendler’s aspectual analysis (Vendler, 1967) (See Figure 4). This is because the above three classes can be useful for dealing with the propositional, especially, resultative aspect of verbs. For example “He threw a ball” can be an Activity and have no special result; but “He broke the door” can be a Change of State and then we can imagine a resul"
W10-3201,C98-1013,0,\N,Missing
W10-3904,bond-etal-2008-boot,0,0.0143804,"in Figure 3, our system correctly identiﬁed negation and analyzed the description “Xylitol alone can not completely” as playing a role of requirement. MENT ] (6) a. キシリトールは虫歯予防に効果がある (Xylitol is effective at preventing cavities.) b. キシリトールだけでは完全な予防は出 来ません (Xylitol alone can not completely prevent cavities.) Our system correctly identiﬁes [AGREE relations in other examples about reduced water from Table 1 by structurally aligning phrases like “promoting good health” and “supports the health” to “good for the health.” These examples show how resources like (Matsuyoshi et al., 2010) and WordNet (Bond et al., 2008) have contributed to the relation classiﬁcation improvement of structural alignment over them baseline in Table 3. Focusing on similarity of syntactic and semantic structures gives our alignment method greater ﬂexibility. However, there are still various examples which the system cannot recognized correctly. In examples on cavity prevention, the phrase “effective at preventing cavities” could not be aligned with “can prevent cavities” or “good for cavity prevention,” nor can “cavity prevention” and “cavity-causing bacteria control.” The above examples illustrate the importance of the role play"
W10-3904,D09-1122,0,0.0154188,"rocess is shown in Figure 1. It consists of the following two phases: 1. lexical alignment 2. structural alignment We developed a heuristic-based algorithm to align chunk based on lexical similarity information. We incorporate the following information into an alignment conﬁdence score that has a range of 0.0-1.0 and align chunk whose scores cross an empirically-determined threshold. • surface level similarity: identical content words or cosine similarity of chunk contents • semantic similarity of predicate-argument structures predicates we check for matches in predicate entailment databases (Hashimoto et al., 2009; Matsuyoshi et al., 2008) considering the default case frames reported by ChaPAS arguments we check for synonym or hypernym matches in the Japanese WordNet (2008) or the Japanese hypernym collection of Sumida et al. (2008) 25 T :! &gt;?@???????????????????AB?C????DEF)! (field) (in)!(agricultural chemicals) (ACC)! (use)! I! H :! &gt;?'???????????????????AB?C????GHF)! (field) (on)!(agricultural chemicals) (ACC)! (spray)! Figure 2: Determining the compatibility of semantic structures We compare the predicate-argument structure of the query to that of the text and determine if the argument structures a"
W10-3904,W02-2016,1,0.759598,"Missing"
W10-3904,N06-1006,0,0.0256939,"Missing"
W10-3904,W05-1201,0,0.0270925,"ral years, several corpora annotated with thousands of (T,H) pairs have been constructed for this task. In these corpora, each pair was tagged indicating its related task (e.g. Information Extraction, Question Answering, Information Retrieval or Summarization). The RTE Challenge has successfully employed a variety of techniques in order to recognize instances of textual entailment, including methods based on: measuring the degree of lexical overlap between bag of words (Glickman et al., 2005; Jijkoun and de Rijke, 2005), the alignment of graphs created from syntactic or semantic dependencies (Marsi and Krahmer, 2005; MacCartney et al., 2006), statistical classiﬁers which leverage a wide range of features (Hickl et al., 2005), or reference rule generation (Szpektor et al., 2007). These approaches have shown great promise in RTE for entailment pairs in the corpus, but more robust models of recognizing logical relations are still desirable. The deﬁnition of contradiction in RTE is that T contradicts H if it is very unlikely that both T and H can be true at the same time. However, in real documents on the Web, there are many pairs of examples which are contradictory in part, or where one statement conﬁnes th"
W10-3904,matsuyoshi-etal-2010-annotating,1,0.82442,"example (6), recognized as [C ONFINE in Figure 3, our system correctly identiﬁed negation and analyzed the description “Xylitol alone can not completely” as playing a role of requirement. MENT ] (6) a. キシリトールは虫歯予防に効果がある (Xylitol is effective at preventing cavities.) b. キシリトールだけでは完全な予防は出 来ません (Xylitol alone can not completely prevent cavities.) Our system correctly identiﬁes [AGREE relations in other examples about reduced water from Table 1 by structurally aligning phrases like “promoting good health” and “supports the health” to “good for the health.” These examples show how resources like (Matsuyoshi et al., 2010) and WordNet (Bond et al., 2008) have contributed to the relation classiﬁcation improvement of structural alignment over them baseline in Table 3. Focusing on similarity of syntactic and semantic structures gives our alignment method greater ﬂexibility. However, there are still various examples which the system cannot recognized correctly. In examples on cavity prevention, the phrase “effective at preventing cavities” could not be aligned with “can prevent cavities” or “good for cavity prevention,” nor can “cavity prevention” and “cavity-causing bacteria control.” The above examples illustrate"
W10-3904,I08-1019,0,0.0193577,"re a lot of opinions on the Web, and it is important to survey opinions in addition to facts to give Internet users a comprehensive view of the discussions on topics of interest. 2.3 Cross-document Summarization Based on CST Relations between Sentences Zhang and Radev (2004) attempted to classify CST relations between sentence pairs extracted from topically related documents. However, they used a vector space model and tried multi-class classiﬁcation. The results were not satisfactory. This observation may indicate that the recognition methods for each relation should be developed separately. Miyabe et al. (2008) attempted to recognize relations that were deﬁned in a Japanese cross-document relation corpus (Etoh and Okumura, 2005). However, their target relations were limited to [E QUIVALENCE ] and [T RANSITION ]; other relations were not targeted. Recognizing [E VIDENCE ] is indispensable for organizing information on the Internet. We need to develop satisfactory methods of [E VIDENCE ] recognition. 2.4 Opinion Mining and Sentiment Analysis Subjective statements, such as opinions, have recently been the focus of much NLP research including review analysis, opinion extraction, opinion question answeri"
W10-3904,W09-3027,1,0.844622,"ons, however the assumptions made by current approaches are often incompatible with this goal. For example, the existing semantic relations considered in Recognizing Textual Entailment (RTE) (Dagan et al., 2005) are often too narrow in scope to be directly applicable to text on the Internet, and theories like Cross-document Structure Theory (CST) (Radev, 2000) are only applicable to facts or second-hand reporting of opinions rather than relations between both. As part of the S TATEMENT M AP project we proposed the development of a system to support information credibility analysis on the Web (Murakami et al., 2009b) by automatically summarizing facts and opinions on topics of interest to users and showing them the evidence and conﬂicts for each viewpoint. To facilitate the detection of semantic relations in Internet data, we deﬁned a sentence-like unit of information called the statement that encompasses both facts and opinions, started compiling a corpus of statements annotated with semantic relations (Murakami et al., 2009a), and begin constructing a system to automatically identify semantic relations between statements. In this paper, we describe the construction and evaluation of a prototype semant"
W10-3904,W00-1009,0,0.283136,"ation, opinion mining, and question answering are illustrative of the great interest in making relevant information easier to ﬁnd. Providing Internet users with thorough information requires recognizing semantic relations between both facts and opinions, however the assumptions made by current approaches are often incompatible with this goal. For example, the existing semantic relations considered in Recognizing Textual Entailment (RTE) (Dagan et al., 2005) are often too narrow in scope to be directly applicable to text on the Internet, and theories like Cross-document Structure Theory (CST) (Radev, 2000) are only applicable to facts or second-hand reporting of opinions rather than relations between both. As part of the S TATEMENT M AP project we proposed the development of a system to support information credibility analysis on the Web (Murakami et al., 2009b) by automatically summarizing facts and opinions on topics of interest to users and showing them the evidence and conﬂicts for each viewpoint. To facilitate the detection of semantic relations in Internet data, we deﬁned a sentence-like unit of information called the statement that encompasses both facts and opinions, started compiling a"
W10-3904,sumida-etal-2008-boosting,0,0.0171268,"e the following information into an alignment conﬁdence score that has a range of 0.0-1.0 and align chunk whose scores cross an empirically-determined threshold. • surface level similarity: identical content words or cosine similarity of chunk contents • semantic similarity of predicate-argument structures predicates we check for matches in predicate entailment databases (Hashimoto et al., 2009; Matsuyoshi et al., 2008) considering the default case frames reported by ChaPAS arguments we check for synonym or hypernym matches in the Japanese WordNet (2008) or the Japanese hypernym collection of Sumida et al. (2008) 25 T :! &gt;?@???????????????????AB?C????DEF)! (field) (in)!(agricultural chemicals) (ACC)! (use)! I! H :! &gt;?'???????????????????AB?C????GHF)! (field) (on)!(agricultural chemicals) (ACC)! (spray)! Figure 2: Determining the compatibility of semantic structures We compare the predicate-argument structure of the query to that of the text and determine if the argument structures are compatible. This process is illustrated in Figure 2 where the T(ext) “Agricultural chemicals are used in the ﬁeld.” is aligned with the H(ypothesis) “Over the ﬁeld, agricultural chemicals are sprayed.” Although the verbs"
W10-3904,P07-1058,0,0.0314166,"task (e.g. Information Extraction, Question Answering, Information Retrieval or Summarization). The RTE Challenge has successfully employed a variety of techniques in order to recognize instances of textual entailment, including methods based on: measuring the degree of lexical overlap between bag of words (Glickman et al., 2005; Jijkoun and de Rijke, 2005), the alignment of graphs created from syntactic or semantic dependencies (Marsi and Krahmer, 2005; MacCartney et al., 2006), statistical classiﬁers which leverage a wide range of features (Hickl et al., 2005), or reference rule generation (Szpektor et al., 2007). These approaches have shown great promise in RTE for entailment pairs in the corpus, but more robust models of recognizing logical relations are still desirable. The deﬁnition of contradiction in RTE is that T contradicts H if it is very unlikely that both T and H can be true at the same time. However, in real documents on the Web, there are many pairs of examples which are contradictory in part, or where one statement conﬁnes the applicability of another, as shown in the examples in Table 1. 2.2 Cross-document Structure Theory Cross-document Structure Theory (CST), developed by Radev (2000)"
W10-3904,P10-2018,1,0.826265,"Missing"
W11-0123,W07-1401,0,0.0528608,"ching with a particular query on the Internet, we want information that tells us what other people think about the query: e.g. do they believe it is true or not; what are the necessary conditions for it to apply. For example, consider the hypothetical search results for the query given in (1). You get opinion (2a), which supports the query, and opinion (2b) which opposes it. (1) Xylitol is effective at preventing tooth decay. (2) a. Xylitol can prevent tooth decay. b. Xylitol is not effective at all at preventing tooth decay. A major task in the Recognizing Textual Entailment (RTE) Challenge (Giampiccolo et al. (2007)) is classifying the semantic relation between a Text and a Hypothesis into E NTAILMENT, C ONTRADICTION, or U NKNOWN. Murakami et al. (2009) report on the S TATEMENT M AP project, the goal of which is to help Internet users evaluate the credibility of information sources by analyzing supporting evidence from a variety of viewpoints on their topics of interest and presenting them to users together with the supporting evidence in a way that makes it clear how they are related. A variety of techniques have been successfully employed in the RTE Challenge in order to recognize instances of textual"
W11-0123,W02-2016,1,0.706871,"rsative conjunction identification, and (VI) semantic template application. Figure 1 shows the work flow of the system. This system takes as input corresponding to S0 and S1 , and return a semantic relation. 5.1 I. Linguistic Analysis In linguistic analysis, we conduct word segmentation, POS tagging, dependency parsing, and extended modality analysis. This linguistic analysis acts as the basis for alignment and semantic feature extraction. For syntactic analysis, we identify words and POS tags with the Japanese morphological analyser Mecab2 , and we use the Japanese dependency parser CaboCha (Kudo and Matsumoto (2002)) to produce dependency trees. We also conduct extended modality analysis using the resources provided by Matsuyoshi et al. (2010). 5.2 II. Structural Alignment To identify the consequence of S0 in S1 , we use Structural Alignment (Mizuno et al. (2010)). In Structural Alignment, dependency parent-child links are aligned across sentences using a variety of resources to ensure semantic relatedness. 5.3 III. Premise and Consequence identification In this step, we identify the Premise and the Consequence in S1 . When a sentence pair satisfies all items is satisfying, we can identify a focused chun"
W11-0123,matsuyoshi-etal-2010-annotating,1,0.797535,"takes as input corresponding to S0 and S1 , and return a semantic relation. 5.1 I. Linguistic Analysis In linguistic analysis, we conduct word segmentation, POS tagging, dependency parsing, and extended modality analysis. This linguistic analysis acts as the basis for alignment and semantic feature extraction. For syntactic analysis, we identify words and POS tags with the Japanese morphological analyser Mecab2 , and we use the Japanese dependency parser CaboCha (Kudo and Matsumoto (2002)) to produce dependency trees. We also conduct extended modality analysis using the resources provided by Matsuyoshi et al. (2010). 5.2 II. Structural Alignment To identify the consequence of S0 in S1 , we use Structural Alignment (Mizuno et al. (2010)). In Structural Alignment, dependency parent-child links are aligned across sentences using a variety of resources to ensure semantic relatedness. 5.3 III. Premise and Consequence identification In this step, we identify the Premise and the Consequence in S1 . When a sentence pair satisfies all items is satisfying, we can identify a focused chunk as the Consequence in S1 : 1. A chunk’s modality in S0 is assertion, this chunk is the Consequence in S0 2. A chunk in S1 align"
W11-0123,W00-1009,0,0.0560636,"stantiating the semantic templates using rules and a list of lexico-semantic patterns. Finally, we conduct empirical evaluation of recognition of the C ONFINEMENT relation between queries and sentences in Japanese-language Web texts. 2 Related Work In RTE research, only three types of relations are defined: E NTAILMENT, C ONTRADICTION, and U NKNOWN. RTE is an important task and has been the target of much research (Szpektor et al. (2007); Sammons et al. (2009)). However, none of the previous research has introduced relations corresponding to C ONFINEMENT. Cross-document Structure Theory (CST, Radev (2000)) is another approach to recognizing semantic relations between sentences. CST is an extended rhetorical structure analysis based on Rhetorical Structure Theory (RST). It attempts to describe the semantic relations between two or more sentences from different source documents that are related to the same topic. It defines 18 kinds of semantic relations between sentences. Etoh and Okumura (2005) constructed a Japanese Cross-document Relation Corpus and defined 14 kinds of semantic relations. It is difficult to consider C ONFINEMENT relations in the CST categorical semantic relations because it"
W11-0123,sumida-etal-2008-boosting,0,0.0290746,"that “Xylitol” and “eating Xylitol” and “using Xylitol” are equivalent. 5.4.3 Consequence Feature Extraction This feature is used to indicate the semantic relationship between Consequences of the sentences pair. Sentences with Consequences that share a certain amount of similarity in polarity and syntax are judged to have E NTAILMENT, otherwise they are in C ONTRADICTION. In order to be judged as E NTAILMENT, the following conditions must all be true: 1. The modality of the Consequences must be identical. 2. The polarity of the Consequences must be identical as indicated by the resources in (Sumida et al. (2008)) 3. The Premises of both sentences must align with each other 221 4. ⋆ The sentences must not contain expressions that limit range or degree such as “ほとんど (almost)” or “程度 (degree)” When all item are satisfied, the Consequence is set to “C”, otherwise it is set to “notC.” We identify whether the consequence has expressions which limit the degree or not. The Consequence is set to “Cr ” or “notCr ” when the following all conditions is satisfied: 1. Any of the children of the Consequence align with a chunk in S0 ’s dependency tree. 2. ⋆ There are expressions limiting the degree of the Consequenc"
W11-0123,P07-1058,0,0.0982338,"h a series of semantic templates that take logical and semantic features as input. We implement a system that detects C ONFINEMENT relations between sentence pairs in Japanese by instantiating the semantic templates using rules and a list of lexico-semantic patterns. Finally, we conduct empirical evaluation of recognition of the C ONFINEMENT relation between queries and sentences in Japanese-language Web texts. 2 Related Work In RTE research, only three types of relations are defined: E NTAILMENT, C ONTRADICTION, and U NKNOWN. RTE is an important task and has been the target of much research (Szpektor et al. (2007); Sammons et al. (2009)). However, none of the previous research has introduced relations corresponding to C ONFINEMENT. Cross-document Structure Theory (CST, Radev (2000)) is another approach to recognizing semantic relations between sentences. CST is an extended rhetorical structure analysis based on Rhetorical Structure Theory (RST). It attempts to describe the semantic relations between two or more sentences from different source documents that are related to the same topic. It defines 18 kinds of semantic relations between sentences. Etoh and Okumura (2005) constructed a Japanese Cross-do"
W13-4502,C12-1064,0,0.0137063,"merical expression identification and normalizing, relation extraction (RE), location name disambiguation and slot filling. In Message Understanding Conference (MUC) (Grishman and Sundheim, 1996) and Automatic Content Extraction (ACE) (Doddington et al., 2004) communities, various information extraction tasks including named entity recognition, relation extraction and slot filling have been explored. The task of disambiguation location expressions is called toponymy disambiguation, and has been explored by (Buscaldi and Rosso, 2008b; Buscaldi, 2010; Buscaldi, 2011; Habib and van Keulen, 2013; Bo et al., 2012; Lee et al., 2013) and GeoNLP Project2 . For the task of disambiguation location names, Buscaldi and Rosso developed Geo-WordNet (2008a) in which entries 2 2.3 Task 3: Environment Improvement for the Usage of Emergency Management Information Systems In the Great East Japan Earthquake in 2011, it was indicated that emergency response requires information sharing between departments or organizations. In order to implement cross-organizational information sharing in disasters, it is essential to regularly hold emergency drills. Regarding drills, Hu et al. (2007) developed techniques for municipa"
W13-4502,buscaldi-rosso-2008-geo,0,0.0176913,"as location names (city, road, etc.), facility names (shelter, shop, school, etc.), numerical expression identification and normalizing, relation extraction (RE), location name disambiguation and slot filling. In Message Understanding Conference (MUC) (Grishman and Sundheim, 1996) and Automatic Content Extraction (ACE) (Doddington et al., 2004) communities, various information extraction tasks including named entity recognition, relation extraction and slot filling have been explored. The task of disambiguation location expressions is called toponymy disambiguation, and has been explored by (Buscaldi and Rosso, 2008b; Buscaldi, 2010; Buscaldi, 2011; Habib and van Keulen, 2013; Bo et al., 2012; Lee et al., 2013) and GeoNLP Project2 . For the task of disambiguation location names, Buscaldi and Rosso developed Geo-WordNet (2008a) in which entries 2 2.3 Task 3: Environment Improvement for the Usage of Emergency Management Information Systems In the Great East Japan Earthquake in 2011, it was indicated that emergency response requires information sharing between departments or organizations. In order to implement cross-organizational information sharing in disasters, it is essential to regularly hold emergenc"
W13-4502,doddington-etal-2004-automatic,0,0.0640613,"s with users and machine learning approaches which enable us to dynamically improve the performance of the system by user feedback. 2.2.3 Research Issues and Plan The structurization includes diverse information extraction subtasks including named entity recognition (NER) such as location names (city, road, etc.), facility names (shelter, shop, school, etc.), numerical expression identification and normalizing, relation extraction (RE), location name disambiguation and slot filling. In Message Understanding Conference (MUC) (Grishman and Sundheim, 1996) and Automatic Content Extraction (ACE) (Doddington et al., 2004) communities, various information extraction tasks including named entity recognition, relation extraction and slot filling have been explored. The task of disambiguation location expressions is called toponymy disambiguation, and has been explored by (Buscaldi and Rosso, 2008b; Buscaldi, 2010; Buscaldi, 2011; Habib and van Keulen, 2013; Bo et al., 2012; Lee et al., 2013) and GeoNLP Project2 . For the task of disambiguation location names, Buscaldi and Rosso developed Geo-WordNet (2008a) in which entries 2 2.3 Task 3: Environment Improvement for the Usage of Emergency Management Information Sy"
W13-4502,C96-1079,0,0.34432,"ion technologies. These technologies are improved through interactions with users and machine learning approaches which enable us to dynamically improve the performance of the system by user feedback. 2.2.3 Research Issues and Plan The structurization includes diverse information extraction subtasks including named entity recognition (NER) such as location names (city, road, etc.), facility names (shelter, shop, school, etc.), numerical expression identification and normalizing, relation extraction (RE), location name disambiguation and slot filling. In Message Understanding Conference (MUC) (Grishman and Sundheim, 1996) and Automatic Content Extraction (ACE) (Doddington et al., 2004) communities, various information extraction tasks including named entity recognition, relation extraction and slot filling have been explored. The task of disambiguation location expressions is called toponymy disambiguation, and has been explored by (Buscaldi and Rosso, 2008b; Buscaldi, 2010; Buscaldi, 2011; Habib and van Keulen, 2013; Bo et al., 2012; Lee et al., 2013) and GeoNLP Project2 . For the task of disambiguation location names, Buscaldi and Rosso developed Geo-WordNet (2008a) in which entries 2 2.3 Task 3: Environment"
W13-4505,D11-1147,0,0.100748,"Missing"
W14-4910,D11-1145,0,0.0280298,"exist, we conduct a corpus study to identify such relations on Twitter, a popular microblogging service. We create annotation guidelines, conduct a large-scale annotation phase, and develop a corpus of annotated evidence relations. Finally, we report our observations, annotation difficulties, and data statistics. 1 Introduction Microblogs have become a popular method for users to express their ideas and communicate with other users. Twitter1 , a popular microblogging service, has recently been the attraction of many natural language processing (NLP) tasks ranging from flu epidemic detection (Aramaki et al., 2011) to gender inference for its users (Ciot et al., 2013). While various tasks are available, despite its daily, rapid largescale data, evidence relation studies have yet to be explored using Twitter data. Previous research exists for determining the credibility of information on Twitter (Castillo et al., 2011); however, the focus of this work is to determine and annotate evidence relations on microblogs. Our primary motivation behind focusing on evidence relations includes the possibility of discovering support for a claim which can support the debunking of false information. During the March 20"
W14-4910,D13-1114,0,0.0140061,"ons on Twitter, a popular microblogging service. We create annotation guidelines, conduct a large-scale annotation phase, and develop a corpus of annotated evidence relations. Finally, we report our observations, annotation difficulties, and data statistics. 1 Introduction Microblogs have become a popular method for users to express their ideas and communicate with other users. Twitter1 , a popular microblogging service, has recently been the attraction of many natural language processing (NLP) tasks ranging from flu epidemic detection (Aramaki et al., 2011) to gender inference for its users (Ciot et al., 2013). While various tasks are available, despite its daily, rapid largescale data, evidence relation studies have yet to be explored using Twitter data. Previous research exists for determining the credibility of information on Twitter (Castillo et al., 2011); however, the focus of this work is to determine and annotate evidence relations on microblogs. Our primary motivation behind focusing on evidence relations includes the possibility of discovering support for a claim which can support the debunking of false information. During the March 2011 Great East Japan Earthquake and Tsunami disaster, v"
W14-4910,P08-1051,0,0.367227,"h identification of potentially false information is necessary in order to provide accurate information to victims and others relying on and trusting in the Internet. Therefore, as a start to find support for counterclaims for false information such as the Cosmo Oil explosion, we focus on dialogue between two individuals: a topic starter, or a post with no parent; and a respondent who provides either an agreeing or disagreeing claim and support for their claim. An example is provided in Figure 1. We note that our task can appear similar to the field of Why-QA (Verberne, 2006; Oh et al., 2013; Mrozinski et al., 2008), which attempts to discover the answer for Why questions. Given our task of discovering agreeing or conflicting claims, and finding specific reasoning to support the claim, we end up with a Why question similar to Why is it true/not true that X, where X is the contents of the claim found in the parent post. However, we consider source mentions or hyperlinks, which can either stand alone or be contained in a statement, question, or request, as a way to answer the above question. To the best of our knowledge, no corpora for evidence relations on microblogs currently exists. In terms of argument"
W14-4910,P13-1170,0,0.182584,"Missing"
W14-4910,W13-4505,1,0.830061,"ehind focusing on evidence relations includes the possibility of discovering support for a claim which can support the debunking of false information. During the March 2011 Great East Japan Earthquake and Tsunami disaster, victims turned to the Internet in order to obtain information on current conditions, such as family member whereabouts, refuge center information, and general information (Sakaki et al., 2011). However, false information, such as the popular Cosmo Oil explosion causing toxic rain, interfered with those looking to find correct information on the status of the disaster areas (Okazaki et al., 2013). This is a scenario in which identification of potentially false information is necessary in order to provide accurate information to victims and others relying on and trusting in the Internet. Therefore, as a start to find support for counterclaims for false information such as the Cosmo Oil explosion, we focus on dialogue between two individuals: a topic starter, or a post with no parent; and a respondent who provides either an agreeing or disagreeing claim and support for their claim. An example is provided in Figure 1. We note that our task can appear similar to the field of Why-QA (Verbe"
W14-4910,prasad-etal-2008-penn,0,0.148516,"Missing"
W14-4910,E06-3005,0,0.352283,"2013). This is a scenario in which identification of potentially false information is necessary in order to provide accurate information to victims and others relying on and trusting in the Internet. Therefore, as a start to find support for counterclaims for false information such as the Cosmo Oil explosion, we focus on dialogue between two individuals: a topic starter, or a post with no parent; and a respondent who provides either an agreeing or disagreeing claim and support for their claim. An example is provided in Figure 1. We note that our task can appear similar to the field of Why-QA (Verberne, 2006; Oh et al., 2013; Mrozinski et al., 2008), which attempts to discover the answer for Why questions. Given our task of discovering agreeing or conflicting claims, and finding specific reasoning to support the claim, we end up with a Why question similar to Why is it true/not true that X, where X is the contents of the claim found in the parent post. However, we consider source mentions or hyperlinks, which can either stand alone or be contained in a statement, question, or request, as a way to answer the above question. To the best of our knowledge, no corpora for evidence relations on microbl"
W15-0507,baccianella-etal-2010-sentiwordnet,0,0.0165839,"[9] in order to acquire both dependency, named entity, and coreference resolution features. In the case of coreference resolution, in order to reduce parsing time, the search distance was restricted to the previous two sentences. At this time, we limit our extraction on a simple noun subject/direct objects opposed to passive sentences (e.g. cancer is caused by smoking). In future work, we will integrate more state of the art relation extraction methods for handling such cases. 4.2.1 Sentiment Polarity Calculation For calculating the sentiment of each argument’s head noun, we use SentiWordNet [2], Takamura et al. [19]’s sentiment corpus, and the Subjectivity Lexicon [22]. For each corpus, we assign a value of 1.0 if the sentiment is positive, -1.0 if negative, or otherwise neutral. We base positive and negative as a value greater than 0 and less than 0, respectively. In the case of SentiWordNet, we focus only on the top-ranked synset polarity value for each noun. Afterwards, we combine the values per noun and calculatesentiment using the following:  pos if num pos votes(w) ≥ 2  sp(w)= neg if num neg votes(w) ≤ −2 ,   neutral otherwise where w is the head noun of the direct objec"
W15-0507,W14-2107,0,0.04117,"be banned?. In this case, a passage such as Alcohol causes heart disease can be retrieved; however, the passage is not necessarily concerned with Why is heart disease negative? which can act as a link between the question and answer. In this work, in addition to a claim and it data, or evidence, we explore finding the link, or warrant, and its backing, in order to strengthen the relationship between the claim and data, one of the aspects of the Toulmin model. In terms of determining stance, previous work has utilized attack or support claims in user comments as a method for determining stance [3]. Inspired by Hashimoto et al. [6]’s excitatory and inhibitory templates, in this work, we similarly compose a manual list of P ROMOTE(X,Y) and S UPPRESS(X,Y) relations and rely on these relations, coupled with positive and negative sentiment values, as a means to signify stance. Simultaneously, not only does this assist with stance, but it is an important feature for argument construction in our first round of constructing automatic Toulmin instantiations. Finally, we generate arguments spanned across multiple documents using the P ROMOTE(X,Y) and S UPPRESS(X,Y) relations. Previous work such"
W15-0507,D11-1142,0,0.0772016,"Missing"
W15-0507,P11-1099,0,0.103629,"ction. In Section 5, we experiment with constructing Toulmin instantiations for a given claim and report our findings. In Section 6, we discuss our results. Finally, in Section 7, we conclude our work and describe our future work. 2 Related Work To the best of our knowledge, no prior work has developed a computation model for automatically constructing Toulmin instantiations. However, various components of the Toulmin model have individually been researched and are discussed below. The most similar work to ours is the automatic detection of enthymemes using Walton [21]’s argumentation schemes [5]. Similarly, we aim to discover enthymemes in the Toulmin model explicitly through computational modeling in order to assist with generating constructive debate speeches. In fu46 ture work, we plan to adopt different, less general argumentation theories. Given a motion-like topic, previous work has found relevant claims to support the topic [8]. Other work has utilized a list of controversial topics in order to find relevant claim and evidence segments utilizing discourse markers [17]. Previous Why-QA work [20, 15, 13] has dealt with finding answers for questions such as Why should alcohol be"
W15-0507,D12-1057,0,0.18995,"e such as Alcohol causes heart disease can be retrieved; however, the passage is not necessarily concerned with Why is heart disease negative? which can act as a link between the question and answer. In this work, in addition to a claim and it data, or evidence, we explore finding the link, or warrant, and its backing, in order to strengthen the relationship between the claim and data, one of the aspects of the Toulmin model. In terms of determining stance, previous work has utilized attack or support claims in user comments as a method for determining stance [3]. Inspired by Hashimoto et al. [6]’s excitatory and inhibitory templates, in this work, we similarly compose a manual list of P ROMOTE(X,Y) and S UPPRESS(X,Y) relations and rely on these relations, coupled with positive and negative sentiment values, as a means to signify stance. Simultaneously, not only does this assist with stance, but it is an important feature for argument construction in our first round of constructing automatic Toulmin instantiations. Finally, we generate arguments spanned across multiple documents using the P ROMOTE(X,Y) and S UPPRESS(X,Y) relations. Previous work such as Cross Document Structure theory"
W15-0507,P14-5010,0,0.00785184,"serving as the surface object of R. In our experiment, we used a collection of web pages extracted from ClueWeb12 as a source corpus of knowledge base construction. ClueWeb121 consists of roughly 733 million Web documents ranging from blogs to news articles. All web pages containing less than 30 words were filtered out which resulted in 222 million total web pages. From these web pages, we extract 22,973,104 relations using a manually composed list of 40 P ROMOTE (e.g. increase, cause, raise) and 76 S UPPRESS (e.g. harm, kill, prevent) predicates. We parse each document using Stanford CoreNLP [9] in order to acquire both dependency, named entity, and coreference resolution features. In the case of coreference resolution, in order to reduce parsing time, the search distance was restricted to the previous two sentences. At this time, we limit our extraction on a simple noun subject/direct objects opposed to passive sentences (e.g. cancer is caused by smoking). In future work, we will integrate more state of the art relation extraction methods for handling such cases. 4.2.1 Sentiment Polarity Calculation For calculating the sentiment of each argument’s head noun, we use SentiWordNet [2],"
W15-0507,D12-1048,0,0.0493718,"Missing"
W15-0507,P08-1051,0,0.0816909,"ns, namely P ROMOTE or S UPPRESS in this paper. By utilizing these relations, our task is reduced to finding relation tuples that can satisfy the definitions. We use our evaluation results as a basis of justification as to whether or not the these relation tuples are sufficient for argumentation construction. To ensure the coherency of overall argumentation, we find contextually similar relations. In future work, we plan to apply state-of-the-art technologies from discourse relation recognition and QAs for generating each Toulmin component, where a significant amount of research has been done [20, 15, 13, 8, 17]. The rest of the paper is as follows. We first describe related work in Section 2 and an overview of the Toulmin model in Section 3. In Section 4, we describe our methodology for generating patterns for Toulmin construction. In Section 5, we experiment with constructing Toulmin instantiations for a given claim and report our findings. In Section 6, we discuss our results. Finally, in Section 7, we conclude our work and describe our future work. 2 Related Work To the best of our knowledge, no prior work has developed a computation model for automatically constructing Toulmin instantiations. Ho"
W15-0507,P13-1170,0,0.0836432,"ns, namely P ROMOTE or S UPPRESS in this paper. By utilizing these relations, our task is reduced to finding relation tuples that can satisfy the definitions. We use our evaluation results as a basis of justification as to whether or not the these relation tuples are sufficient for argumentation construction. To ensure the coherency of overall argumentation, we find contextually similar relations. In future work, we plan to apply state-of-the-art technologies from discourse relation recognition and QAs for generating each Toulmin component, where a significant amount of research has been done [20, 15, 13, 8, 17]. The rest of the paper is as follows. We first describe related work in Section 2 and an overview of the Toulmin model in Section 3. In Section 4, we describe our methodology for generating patterns for Toulmin construction. In Section 5, we experiment with constructing Toulmin instantiations for a given claim and report our findings. In Section 6, we discuss our results. Finally, in Section 7, we conclude our work and describe our future work. 2 Related Work To the best of our knowledge, no prior work has developed a computation model for automatically constructing Toulmin instantiations. Ho"
W15-0507,W00-1009,0,0.0392236,"s excitatory and inhibitory templates, in this work, we similarly compose a manual list of P ROMOTE(X,Y) and S UPPRESS(X,Y) relations and rely on these relations, coupled with positive and negative sentiment values, as a means to signify stance. Simultaneously, not only does this assist with stance, but it is an important feature for argument construction in our first round of constructing automatic Toulmin instantiations. Finally, we generate arguments spanned across multiple documents using the P ROMOTE(X,Y) and S UPPRESS(X,Y) relations. Previous work such as Cross Document Structure theory [16] has organized information from multiple documents via relations. Furthermore, the Statement Map [14] project, for a given query, has detected agreeing and conflicting support which are spanned across multiple documents. In this work, we attempt to construct an implicit Warrant and generate its Backing for a Claim (query) and its Data (support). 3 Toulmin Model Toulmin was the first to believe that most arguments could simply be modeled using the following six components: claim, data, warrant, backing, qualifier, and rebuttal [18]. This model is referred to as the Toulmin model and is shown in"
W15-0507,W14-4910,1,0.886531,"ns, namely P ROMOTE or S UPPRESS in this paper. By utilizing these relations, our task is reduced to finding relation tuples that can satisfy the definitions. We use our evaluation results as a basis of justification as to whether or not the these relation tuples are sufficient for argumentation construction. To ensure the coherency of overall argumentation, we find contextually similar relations. In future work, we plan to apply state-of-the-art technologies from discourse relation recognition and QAs for generating each Toulmin component, where a significant amount of research has been done [20, 15, 13, 8, 17]. The rest of the paper is as follows. We first describe related work in Section 2 and an overview of the Toulmin model in Section 3. In Section 4, we describe our methodology for generating patterns for Toulmin construction. In Section 5, we experiment with constructing Toulmin instantiations for a given claim and report our findings. In Section 6, we discuss our results. Finally, in Section 7, we conclude our work and describe our future work. 2 Related Work To the best of our knowledge, no prior work has developed a computation model for automatically constructing Toulmin instantiations. Ho"
W15-0507,P05-1017,0,0.0261901,"ire both dependency, named entity, and coreference resolution features. In the case of coreference resolution, in order to reduce parsing time, the search distance was restricted to the previous two sentences. At this time, we limit our extraction on a simple noun subject/direct objects opposed to passive sentences (e.g. cancer is caused by smoking). In future work, we will integrate more state of the art relation extraction methods for handling such cases. 4.2.1 Sentiment Polarity Calculation For calculating the sentiment of each argument’s head noun, we use SentiWordNet [2], Takamura et al. [19]’s sentiment corpus, and the Subjectivity Lexicon [22]. For each corpus, we assign a value of 1.0 if the sentiment is positive, -1.0 if negative, or otherwise neutral. We base positive and negative as a value greater than 0 and less than 0, respectively. In the case of SentiWordNet, we focus only on the top-ranked synset polarity value for each noun. Afterwards, we combine the values per noun and calculatesentiment using the following:  pos if num pos votes(w) ≥ 2  sp(w)= neg if num neg votes(w) ≤ −2 ,   neutral otherwise where w is the head noun of the direct object in each P ROMOTE and"
W15-0507,E06-3005,0,0.0995363,"ns, namely P ROMOTE or S UPPRESS in this paper. By utilizing these relations, our task is reduced to finding relation tuples that can satisfy the definitions. We use our evaluation results as a basis of justification as to whether or not the these relation tuples are sufficient for argumentation construction. To ensure the coherency of overall argumentation, we find contextually similar relations. In future work, we plan to apply state-of-the-art technologies from discourse relation recognition and QAs for generating each Toulmin component, where a significant amount of research has been done [20, 15, 13, 8, 17]. The rest of the paper is as follows. We first describe related work in Section 2 and an overview of the Toulmin model in Section 3. In Section 4, we describe our methodology for generating patterns for Toulmin construction. In Section 5, we experiment with constructing Toulmin instantiations for a given claim and report our findings. In Section 6, we discuss our results. Finally, in Section 7, we conclude our work and describe our future work. 2 Related Work To the best of our knowledge, no prior work has developed a computation model for automatically constructing Toulmin instantiations. Ho"
W15-0507,H05-1044,0,0.0106515,"olution features. In the case of coreference resolution, in order to reduce parsing time, the search distance was restricted to the previous two sentences. At this time, we limit our extraction on a simple noun subject/direct objects opposed to passive sentences (e.g. cancer is caused by smoking). In future work, we will integrate more state of the art relation extraction methods for handling such cases. 4.2.1 Sentiment Polarity Calculation For calculating the sentiment of each argument’s head noun, we use SentiWordNet [2], Takamura et al. [19]’s sentiment corpus, and the Subjectivity Lexicon [22]. For each corpus, we assign a value of 1.0 if the sentiment is positive, -1.0 if negative, or otherwise neutral. We base positive and negative as a value greater than 0 and less than 0, respectively. In the case of SentiWordNet, we focus only on the top-ranked synset polarity value for each noun. Afterwards, we combine the values per noun and calculatesentiment using the following:  pos if num pos votes(w) ≥ 2  sp(w)= neg if num neg votes(w) ≤ −2 ,   neutral otherwise where w is the head noun of the direct object in each P ROMOTE and S UPPRESS relation. The functions num pos votes(w) an"
W15-0507,N07-4013,0,0.0384393,"Missing"
W15-0512,W97-0703,0,0.131902,"f a certain event tend to follow the order of publication. It is, however, not suitable for opinion generation because such generation requires statements and evidence rather than the simple summarization of an event. Topical similarity is based on an assumption that neighboring sentences have a higher similarity than non-neighboring ones. For example, bag-of-wordsbased cosine similarities of sentence pairs are used in (Bollegala et al., 2006; Tan et al., 2013). Another method, the Lexical Chain, models the semantic distances of word pairs on the basis of synonym dictionaries such as WordNet (Barzilay and Elhadad, 1997; Chen et al., 2005). The effectiveness of this feature depends highly on the method used to calculate similarity. Transitional association is used to measure the likelihood of two consecutive sentences. Lapata proposed a sentence ordering method based on a probabilistic model (Lapata, 2003). This method uses conditional probability to represent transitional probability from the previous sentence to the target sentence. Dias et al. used rhetorical structures to order sentences (de S. Dias et al., 2014). The rhetorical structure theory (RST) (Mann and Thompson, 1988) explains the textual organi"
W15-0512,P06-1049,0,0.272384,"ibes a claim that is the main statement of the opinion and the second sentence supports the main statement. In this paper, we focus on this claim-support structure to order sentences. Regarding the structures of arguments, we can find research on the modeling of arguments (Freeley and Steinberg, 2008) and on recognition such as claim detection (Aharoni et al., 2014). To the best of our knowledge, there is no research that examines the claim-support structure of debate texts for the sentence ordering problem. Most of the previous works on sentence ordering (Barzilay et al., 2002; Lapata, 2003; Bollegala et al., 2006; Tan et al., 2013) focus on the sentence order of news articles and do not consider the structures of arguments. These methods mingle claim and supportive sentences together, which decreases the persuasiveness of generated opinions. In this paper, we propose a sentence ordering method in which a motion and a set of sentences are given as input. Ordering all paragraphs of debate texts at once is a quite difficult task, so we have International Debate Education Association. Reserved. 94 Proceedings of the 2nd Workshop on Argumentation Mining, pages 94–103, c Denver, Colorado, June 4, 2015. 2015"
W15-0512,P03-1069,0,0.335292,"sentence describes a claim that is the main statement of the opinion and the second sentence supports the main statement. In this paper, we focus on this claim-support structure to order sentences. Regarding the structures of arguments, we can find research on the modeling of arguments (Freeley and Steinberg, 2008) and on recognition such as claim detection (Aharoni et al., 2014). To the best of our knowledge, there is no research that examines the claim-support structure of debate texts for the sentence ordering problem. Most of the previous works on sentence ordering (Barzilay et al., 2002; Lapata, 2003; Bollegala et al., 2006; Tan et al., 2013) focus on the sentence order of news articles and do not consider the structures of arguments. These methods mingle claim and supportive sentences together, which decreases the persuasiveness of generated opinions. In this paper, we propose a sentence ordering method in which a motion and a set of sentences are given as input. Ordering all paragraphs of debate texts at once is a quite difficult task, so we have International Debate Education Association. Reserved. 94 Proceedings of the 2nd Workshop on Argumentation Mining, pages 94–103, c Denver, Colo"
W15-0512,P14-5010,0,0.0071132,"the results. Second, we select the first sentence, which is likely to be the claim sentence, from the candidate sentences. This problem is formulated as a binary-classification problem, where first sentences of constructive speech items are positive and all others are negative. Third, we order the remaining sentences on the basis of connectivity of pairs of sentences. This problem is formulated as a ranking problem, similarly to (Tan et al., 2013). 4.2 Feature Extraction We obtained the part of speech, lemma, syntactic parse tree, and NEs of each input sentence by using the Stanford Core NLP (Manning et al., 2014). The following features, which are commonly used in sentence ordering methods to measure local coherence (Bollegala et al., 2006; Tan et al., 2013; Lapata, 2003), are then extracted. Sentence similarity: Cosine similarity between sentence u and v. We simply counted the frequency of each word to measure cosine similarity. In addition to that, we also measured the cosine similarity between latter half of u (denoted as latter(u)) and former half of v (denoted as former(v)). The sentences are separated by the most centered comma (if exists) or word (if no comma exists). Overlap: Commonly shared w"
W15-0512,P13-2016,0,0.357355,"main statement of the opinion and the second sentence supports the main statement. In this paper, we focus on this claim-support structure to order sentences. Regarding the structures of arguments, we can find research on the modeling of arguments (Freeley and Steinberg, 2008) and on recognition such as claim detection (Aharoni et al., 2014). To the best of our knowledge, there is no research that examines the claim-support structure of debate texts for the sentence ordering problem. Most of the previous works on sentence ordering (Barzilay et al., 2002; Lapata, 2003; Bollegala et al., 2006; Tan et al., 2013) focus on the sentence order of news articles and do not consider the structures of arguments. These methods mingle claim and supportive sentences together, which decreases the persuasiveness of generated opinions. In this paper, we propose a sentence ordering method in which a motion and a set of sentences are given as input. Ordering all paragraphs of debate texts at once is a quite difficult task, so we have International Debate Education Association. Reserved. 94 Proceedings of the 2nd Workshop on Argumentation Mining, pages 94–103, c Denver, Colorado, June 4, 2015. 2015 Association for Co"
W15-0512,W14-2109,0,\N,Missing
W15-1606,baker-etal-2010-modality,0,0.014395,"(1053/1151) 53.33( 521/ 977) 77.32 ( 890/1151) Recall 86.50 (5184/5993) 95.93 (5749/5993) 70.43(4221/5993) 81.18 (4865/5993) 82.85 (2299/2775) 94.77 (2630/2775) 70.09 (1945/2775) 82.05 (2277/2775) 58.49 ( 672/1149) 92.08 (1058/1149) 45.52( 523/1149) 79.11 ( 909/1149) F 90.51 95.66 73.31 80.50 88.51 94.36 74.44 81.32 68.76 91.78 49.11 78.21 only selected events in head clauses because the factuality in subordinate clauses is determined not only by FEs, but also by other factors such as predicates. In our corpus, extended modality is also annotated for each event mentioned by Matsuyoshi et al. (2010). The actuality of extended modality denotes the author’s degree of certainty and corresponds to factuality. In this paper, by comparing the results of factuality analysis based on each of the four FE types, we show that annotating events with both FEs and factuality leads to some quantitative investigations such as i) how much effect our FE redesign has on factuality analysis, ii) how much does FE disambiguation contribute to factuality analysis, and iii) for how many events can we analyze factuality based on FEs. FE I and II are the results of the longest matches using POS-attachment rules b"
W15-1606,J12-2003,0,0.0247226,"Missing"
W15-1606,W09-3012,0,0.0357349,"Missing"
W15-1606,matsuyoshi-etal-2010-annotating,1,0.891735,"Missing"
W15-1606,I13-1067,1,0.862804,"Missing"
W15-1606,A00-2007,0,0.0999956,". Therefore, we introduced some new categories such as EXPERIENCE to annotate appropriate labels to these expressions. As a result, meanings of Japanese FEs are classified into 72 categories in our annotation scheme. Note that the number of categories is less than that of Tsutsuji because we left some FEs in Tsutsuji out of consideration in our scheme. Some FEs, such as が and を, have no information that is useful to us, as they are related more closely to predicate-argument structure. 3.4 Category Redesign Labels are annotated at the token level. To annotate CFEs, we employed the IOB2 format (Sang, 2000) to express the range of FEs, and we used the label P for predicates. An example is shown in Table 1. We categorized the meanings of FEs by referring to Tsutsuji. Because some categories were not compatible with application tasks, we added and segmentalized some of them. For example, かもしれない (possibly) and だろう (probably) are categorized as SPECULATION in Tsutsuji. However, these are actually different in the following aspects: 食べるだろ う (probably eat) has more certainty than 食べるか もしれない (possibly eat). This fact is useful when determining the author’s degree of conviction. Thus, we segmentalized t"
W15-1606,J12-2002,0,0.0355546,"Missing"
W15-1606,P11-1017,0,0.0681798,"Missing"
W15-1606,W08-0606,0,0.10968,"Missing"
W15-1606,suzuki-etal-2012-detecting,0,\N,Missing
W15-1606,W06-2404,0,\N,Missing
W15-1609,P14-1016,0,0.033703,"Missing"
W15-1609,I13-2008,1,0.896852,"Missing"
W15-1609,C10-1096,1,0.887204,"Missing"
W15-1609,peters-peters-2000-lexicalised,0,0.0549864,"les of geoparsing for Japanese text, GeoNLP (Kitamoto and Sagara, 2012) exist, but there are no reports of quantitative evaluations of the performance, because there is no corpus for evaluation. 3 Challenges in Annotating LREs on Microblog Text In this section, we describe the new research challenges associated with annotating geographical entities in Microblog text and our policies for addressing these issues. 3.1 Systematic Polysemy of LREs One prominent issue in annotating facility entities is the so-called systematic polysemy inherent in mentions referring to facilities (see, for example, Peters and Peters (2000)). For example, the mention “the Ministry of the Environment” in the sentence (1) below refers to a specific location while the mention “the Ministry of the Environment” in (2) should be interpreted as an organization and does not refer to the location of the organization. (1) 午 後 は 環境省 に い ま す / I’ll be at the Ministry of the Environment this afternoon. (2) こ れ か ら 環境省 の 職 員 に 会って き ま す / I will go to meet a staff member of the Ministry of the Environment. This distinction can be crucial in potential applications of annotated geographical entities. In our annotation guidelines, ambiguities of"
W15-1609,sekine-etal-2002-extended,0,0.0823175,"L:82db] で [岩手県 TYPE=LOC, EN=岩手県, ID=CB:3b4c] まで行く / I go to [Iwate prefecture TYPE=LOC, EN=岩手県, ID=CB:3b4c] from [Tokyo TYPE=FAC, EN=東京駅, ID=LC:4d3b] in [Shinkansen TYPE=FAC, EN=東北新幹線, ID=NL:82db]. sues discussed in Section 3. 4.1 Annotation In the existing named entity tagged corpora in Japanese, expressions are annotated with a named entity class and its boundaries. However, the corpora does not contain annotations as to whether each of the expressions actually relates to an entity. Partly following the annotation guidelines in TAC KBP (Ji et al., 2014)2 , the extended named entity tag set (Sekine et al., 2002) and the Japanese extended Named Entity-tagged corpus, we followed the approach illustrated in Figure 2 to annotate microblog texts. The annotation task consists of the following two subtasks: Mention Detection (MD) Given a microblog text (i.e., a tweet), an annotator annotates all the mentions which refer to specific geographic entities with a predefined set of tags given in Table 1. Entity Resolution (ER) For each detected mention, an annotator searches the gazetteer for its referred entity and annotates the linking. We allow a mention to be linked to multiple gazetteer entries. If the refer"
W15-1609,P13-1144,0,0.198684,"Missing"
W15-1609,P13-1159,0,\N,Missing
W16-1313,D15-1103,0,0.215392,"population (Carlson et al., 2010). Unfortunately, most entity type classification systems use a relatively small number of types (e.g. person, organization, location, time, and miscellaneous (Grishman and Sundheim, 1996)) which may be too coarse-grained for some NLP applications (Sekine, 2008). To address this shortcoming, a series of recent work has investigated entity type classification with a large set of fine-grained types (Lee et al., ∗ This work was conducted during a research visit to University College London. 2006; Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Del Corro et al., 2015). Existing fine-grained entity type classification systems have used approaches ranging from sparse binary features to dense vector representations of entities to model the entity mention and its context. However, no previously proposed system has attempted to learn to recursively compose representations of entity context. For example, one can see that a phrase “got a Ph.D. from” is indicative of the next words being an educational institution, something which would be helpful for fine-grained entity type classification. In this work our main contributions are two-fold: 1. A first model for fi"
W16-1313,C96-1079,0,0.231995,"for the mention “New York” in the sentence “She got a Ph.D from New York in Feb. 1995.”. Introduction Entity type classification is the task of assigning semantic types to mentions of entities in sentences. Identifying the types of entities is useful for various natural language processing tasks, such as relation extraction (Ling and Weld, 2012), question answering (Lee et al., 2006), and knowledge base population (Carlson et al., 2010). Unfortunately, most entity type classification systems use a relatively small number of types (e.g. person, organization, location, time, and miscellaneous (Grishman and Sundheim, 1996)) which may be too coarse-grained for some NLP applications (Sekine, 2008). To address this shortcoming, a series of recent work has investigated entity type classification with a large set of fine-grained types (Lee et al., ∗ This work was conducted during a research visit to University College London. 2006; Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Del Corro et al., 2015). Existing fine-grained entity type classification systems have used approaches ranging from sparse binary features to dense vector representations of entities to model the entity mention and its contex"
W16-1313,P09-1113,0,0.151619,"owledge, Lee et al. (2006) were the first to address the task of fine-grained entity type classification. They defined 147 finegrained entity types and evaluated a conditional random fields-based model on a manually annotated Korean dataset. Sekine (2008) advocated the necessity of a large set of types for entity type classification and defined 200 types which served as a basis for future work on fine-grained entity type classification. Ling and Weld (2012) defined a set of 112 types based on Freebase and created a training dataset from Wikipedia using a distant supervision method inspired by Mintz et al. (2009). For evaluation, they created a small manually annotated dataset of newspaper articles and also demonstrated that their system, FIGER, could improve the performance of a relation extraction system by providing fine-grained entity type predictions as features. Yosef et al. (2012) organised 505 types in a hierarchical taxonomy, with several hundreds of types at different levels. Based on this taxonomy they developed a multi-label hierarchical classification system. In Yogatama et al. (2015) the authors proposed to use label embeddings to allow information sharing between related labels. This ap"
W16-1313,D14-1162,0,0.0832247,"i=1 • loose micro Dataset Pre-trained Word Embeddings Evaluation Criteria Following Ling and Weld (2012), we evaluate the model performances by strict, loose macro, and 72 PN ˆ i=1 |Ti ∩ Ti | P N ˆ i=1 |Ti | PN ˆ i=1 |Ti ∩ Ti | Recall = P N i=1 |Ti | P recision = Experiment The only features used by our model are pretrained word embeddings that were not updated during training to help the model generalize for words not appearing in the training set. Specifically, we used the freely available 300 dimensional cased word embeddings trained on 840 billion tokens from the Common Crawl supplied by Pennington et al. (2014). As embeddings for out-ofvocabulary words, we used the embedding of the “unk” token from the pre-trained embeddings. 4.3 P recision = Recall = (10) To train and evaluate our model we use the publicly available FIGER dataset with 112 fine-grained types from Ling and Weld (2012). The sizes of our datasets are 2, 600, 000 for training, 90, 000 for development, and 563 for testing. Note that the train and development sets were created from Wikipedia, whereas the test set is a manually annotated dataset of newspaper articles. 4.2 • strict (9) The equations for computing eri , a ˜ri , and ari were"
W16-1313,sekine-2008-extended,0,0.304162,"Introduction Entity type classification is the task of assigning semantic types to mentions of entities in sentences. Identifying the types of entities is useful for various natural language processing tasks, such as relation extraction (Ling and Weld, 2012), question answering (Lee et al., 2006), and knowledge base population (Carlson et al., 2010). Unfortunately, most entity type classification systems use a relatively small number of types (e.g. person, organization, location, time, and miscellaneous (Grishman and Sundheim, 1996)) which may be too coarse-grained for some NLP applications (Sekine, 2008). To address this shortcoming, a series of recent work has investigated entity type classification with a large set of fine-grained types (Lee et al., ∗ This work was conducted during a research visit to University College London. 2006; Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Del Corro et al., 2015). Existing fine-grained entity type classification systems have used approaches ranging from sparse binary features to dense vector representations of entities to model the entity mention and its context. However, no previously proposed system has attempted to learn to recurs"
W16-1313,P15-2048,0,0.230963,", 2006), and knowledge base population (Carlson et al., 2010). Unfortunately, most entity type classification systems use a relatively small number of types (e.g. person, organization, location, time, and miscellaneous (Grishman and Sundheim, 1996)) which may be too coarse-grained for some NLP applications (Sekine, 2008). To address this shortcoming, a series of recent work has investigated entity type classification with a large set of fine-grained types (Lee et al., ∗ This work was conducted during a research visit to University College London. 2006; Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Del Corro et al., 2015). Existing fine-grained entity type classification systems have used approaches ranging from sparse binary features to dense vector representations of entities to model the entity mention and its context. However, no previously proposed system has attempted to learn to recursively compose representations of entity context. For example, one can see that a phrase “got a Ph.D. from” is indicative of the next words being an educational institution, something which would be helpful for fine-grained entity type classification. In this work our main contributions are two-fold"
W16-1313,C12-2133,0,0.387061,"nswering (Lee et al., 2006), and knowledge base population (Carlson et al., 2010). Unfortunately, most entity type classification systems use a relatively small number of types (e.g. person, organization, location, time, and miscellaneous (Grishman and Sundheim, 1996)) which may be too coarse-grained for some NLP applications (Sekine, 2008). To address this shortcoming, a series of recent work has investigated entity type classification with a large set of fine-grained types (Lee et al., ∗ This work was conducted during a research visit to University College London. 2006; Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Del Corro et al., 2015). Existing fine-grained entity type classification systems have used approaches ranging from sparse binary features to dense vector representations of entities to model the entity mention and its context. However, no previously proposed system has attempted to learn to recursively compose representations of entity context. For example, one can see that a phrase “got a Ph.D. from” is indicative of the next words being an educational institution, something which would be helpful for fine-grained entity type classification. In this work our main con"
W17-4208,W04-3238,0,0.0799744,"f deletions that involve one or two words. Table 3 shows some instances of the replace operations. It may not be straightforward to use the revision logs for error correction because some edit operations add new information and remove useless information. Nevertheless, the logs record the daily activities of how drafts are improved by the editors. In future, we plan to build an editing system that detects errors and suggests wording while the reporters write drafts. We can use natural language processing techniques for these tasks because local error correction has been previously researched (Cucerzan and Brill, 2004). Splitting (S1) 新たな窓口を設けるなど内部通報制度も強化し、 通報は問題が発覚する前の 88 件 (14 年度) から 263 件 (15 年度) と 3 倍に増えた。 They enhance whistle-blowing systems by providing such as new counseling offices, and the number of whistle-blowing was increased three times from 88 in 2014, in which this issue was found out, to 263 in 2015. (S2) 新たな窓口を設けるなど内部通報制度も強化。通 報は 2015 年度に 263 件と、不正会計問題の発覚 前の 14 年度の 88 件から約 3 倍に増えたという。 They enhance whistle-blowing systems by providing such as new counseling offices. As a result, the number of whistle-blowing was increased three times from 88 in 2014, in which this issue was found out, to 263"
W17-4208,P06-1030,0,0.0440693,"ns separated by different part-of-speech. The most frequent target for revisions is nouns, followed by particles (postpositions). This result indicates that revisions in terms of both content and readability are important for improving the quality of articles. 48 Original 同政府関係者 this Government officials 放射線汚染 contamination by radial ray 破顔し broke into a smile 3.2 Establishing guidelines for writing articles Most textbooks on Japanese writing (including the internal handbook for reporters produced by the newspaper company) recommend that a Japanese sentence should be 40 to 50 characters long (Ishioka and Kameda, 2006). We could confirm that the newspaper articles satisfy this criterion: the revised sentences are 41.10 characters long on average. In this way, we can analyze the revision logs to extract various criteria for establishing the guidelines for ‘good’ articles. バラティ Varety タンパク質 Protain: written in Katakana and Kanji 買えた could buy 3.3 Automatic article revision within sentences Table 3: Examples of commonly replaced words/phrases. Another future direction is to build a corpus for improving the quality of articles. The revision logs collected for a year (excluding duplicates) provide 517,545 instan"
W17-4208,C16-1109,0,0.026073,"Missing"
W17-4208,W04-3230,0,0.0322271,"Missing"
W17-6937,P98-1013,0,0.0840769,"e as an event unit. Extending an event unit beyond a multiword expression would impose a severe data sparseness problem, which is to be addressed in our future work. 4 Related Work Previous studies proposed a wide variety of approaches to causality estimation (Do et al., 2011; Kozareva, 2012; Riaz and Girju, 2014). Do et al. (2011) employed statistical measures such as PMI and inverse document frequency (IDF) from a corpus to model causality between events. Kozareva (2012) applied a bootstrap algorithm to acquire causal event pairs. Riaz and Girju (2014) extracted training data from FrameNet (Baker et al., 1998) to learn a classifier for a causal relation. However, these studies assume that an event is representable by a single word. Chambers and Jurafsky (2008)’s Narrative Schema uses a predicate-argument structure as an event unit, but a predicate is restricted to a single word. Roemmele et al. (2011) introduced a baseline model of COPA that uses PMI between words on English documents in Project Gutenberg. Gordon et al. (2011) improved the baseline model introduced by Roemmele et al. (2011) by using personal stories extracted from Weblogs instead of Project Gutenberg. Luo et al. (2016) refined PMI"
W17-6937,P08-1090,0,0.562654,"ng the product. The task is to estimate that sentence (1a) is more causally related to sentence (1b) than non-causally related sentences such as “John opened a door.”. Causality estimation is considered as an essential component of common sense reasoning. A conventional approach to causality estimation is to construct a statistical model of causality relying on a large corpus in a semi-supervised manner. The main idea is two-fold: (i) collect causally related word pairs (e.g. typhoon-die) by exploiting the contextual proximity or discourse markers and (ii) apply them to a correlation measure (Chambers and Jurafsky, 2008; Luo et al., 2016) or a supervised classifier (Riaz and Girju, 2014; Granroth-Wilding and Clark, 2016). A key limitation of the previous studies is that they model causality in terms of word pairs, not taking into account the causality represented by multiword expressions. For example, in example (1), a causality estimation model is expected to consider the causality between tired and gave up (i.e. stop something). However, the previous models consider only word pairs; therefore, it would improperly estimate the causality based on word pairs such as tired-give and tired-up. Because each indiv"
W17-6937,D11-1027,0,0.0257836,"ssions. It reveals that proper causality estimation often requires the system to expand an event unit to another word as well as to recognize a multiword expression; for instance, when the causality between “The stain came out of the shirt.” and “I bleached the shirt” is estimated, stain come out, rather than come out, is more appropriate as an event unit. Extending an event unit beyond a multiword expression would impose a severe data sparseness problem, which is to be addressed in our future work. 4 Related Work Previous studies proposed a wide variety of approaches to causality estimation (Do et al., 2011; Kozareva, 2012; Riaz and Girju, 2014). Do et al. (2011) employed statistical measures such as PMI and inverse document frequency (IDF) from a corpus to model causality between events. Kozareva (2012) applied a bootstrap algorithm to acquire causal event pairs. Riaz and Girju (2014) extracted training data from FrameNet (Baker et al., 1998) to learn a classifier for a causal relation. However, these studies assume that an event is representable by a single word. Chambers and Jurafsky (2008)’s Narrative Schema uses a predicate-argument structure as an event unit, but a predicate is restricted"
W17-6937,W12-4107,0,0.0124448,"s that proper causality estimation often requires the system to expand an event unit to another word as well as to recognize a multiword expression; for instance, when the causality between “The stain came out of the shirt.” and “I bleached the shirt” is estimated, stain come out, rather than come out, is more appropriate as an event unit. Extending an event unit beyond a multiword expression would impose a severe data sparseness problem, which is to be addressed in our future work. 4 Related Work Previous studies proposed a wide variety of approaches to causality estimation (Do et al., 2011; Kozareva, 2012; Riaz and Girju, 2014). Do et al. (2011) employed statistical measures such as PMI and inverse document frequency (IDF) from a corpus to model causality between events. Kozareva (2012) applied a bootstrap algorithm to acquire causal event pairs. Riaz and Girju (2014) extracted training data from FrameNet (Baker et al., 1998) to learn a classifier for a causal relation. However, these studies assume that an event is representable by a single word. Chambers and Jurafsky (2008)’s Narrative Schema uses a predicate-argument structure as an event unit, but a predicate is restricted to a single word"
W17-6937,P14-5010,0,0.00494728,"Missing"
W17-6937,W14-0707,0,0.0720111,"related to sentence (1b) than non-causally related sentences such as “John opened a door.”. Causality estimation is considered as an essential component of common sense reasoning. A conventional approach to causality estimation is to construct a statistical model of causality relying on a large corpus in a semi-supervised manner. The main idea is two-fold: (i) collect causally related word pairs (e.g. typhoon-die) by exploiting the contextual proximity or discourse markers and (ii) apply them to a correlation measure (Chambers and Jurafsky, 2008; Luo et al., 2016) or a supervised classifier (Riaz and Girju, 2014; Granroth-Wilding and Clark, 2016). A key limitation of the previous studies is that they model causality in terms of word pairs, not taking into account the causality represented by multiword expressions. For example, in example (1), a causality estimation model is expected to consider the causality between tired and gave up (i.e. stop something). However, the previous models consider only word pairs; therefore, it would improperly estimate the causality based on word pairs such as tired-give and tired-up. Because each individual word in multiword expressions might have a completely differen"
W17-6937,W93-0310,0,0.314862,"n.wiktionary.org/ During the extraction, all words are lemmatized and lowercased. 3 https://en.wiktionary.org/wiki/Wiktionary:Main_Page 2 and the sufficient factor. Formally, for a causal event ic and a effect event je , the two factors are defined by the following equations: CSnec (ic , je ) = p(ic , ie ) p(ic |je ) = α , α p (ic ) p (ic )p(je ) (1) CSsuf (ic , je ) = p(je |ic ) p(ic , ie ) = , α p (je ) p(ic )pα (je ) (2) where CSnec (ic , je ) is the necessary factor, CSsuf (ic , je ) is the sufficient factor, and α is a hyperparameter. We set α = 0.66, the same value as Luo et al. (2016); Wettler and Rapp (1993). By using CSnec (ic , je ) and CSsuf (ic , je ), Causal Strength is defined as, CS(ic , je ) = CSnec (ic , je )λ CSsuf (ic , je )1−λ , (3) where λ is a hyper-parameter. 3 Experiment To examine the necessity of proper treatment of multiword expressions, we compare the proposed method against existing causality estimation models, and conduct an ablation study. 3.1 Dataset To extract causal event pairs, we used the ClueWeb124 , a large-scale corpus consisting of 700 million documents crawled from the Web. We evaluated the proposed method on the task of Choice of Plausible Alternatives (COPA) (Ro"
W17-6937,C98-1013,0,\N,Missing
W18-5210,D17-1144,0,0.0315042,"such as Rhetorical Structure Theory (Mann and Thompson, 1987). The Penn Discourse TreeBank, the largest manually annotated corpus for discourse relations, targeted both implicit and explicit relation detection for either adjacent sentences or clauses (Prasad et al., 2008). However, these studies do not aim for capturing implicit reasoning behind arguments. AT ingredients Although we adopted a simple approach for AT ingredient identification for our first attempt (see Section 3.2), many sophisticated approaches have been proposed. Shallow discourse analysis of ARs has been extensively studied (Cocarascu and Toni, 2017; Niculae et al., 2017; Peldszus and Stede, 2015a,b). VJ identification is similar to targeted sentiment analysis (Mitchell et al., 2013; Dong et al., 2014). Somasundaran and Wiebe (2010) developed an annotation method for targeted sentiment. However, we aim to expand the annotation to other types of arguments, and their work only considers the task setting of stance classification. Finally, causal relation identification between an entity pair in a sentence has been studied (Zhang and Wang, Acknowledgements This work was supported by JSPS KAKENHI Grant Number 15H01702 and JST CREST Grant Numb"
W18-5210,D13-1171,0,0.0227663,"Missing"
W18-5210,W02-1001,0,0.0865065,"l (x, BAD), mcau (PROMOTE) , 0, 0, mval (y, BAD)}. We Causal Relations (mcau ) We develop a simple rule-based classifier for identifying causal relations between the given slot-fillers x and y. We use a predefined list of causal phrases (i.e. causes, will lead to, etc. for PROMOTE, and destroy, 10 We parse each segment using Spacy (Honnibal and Johnson, 2015). 11 We use an existing sentiment lexicon (Warriner et al., 2013) to extract the sentiment polarity of each adjective. 85 Table 1: Performance of our AT instantiation modlearn w on training data by using an averaged structured perceptron (Collins, 2002). We call this a template-constrained inference model, or TCI. To see the effectiveness, we consider the model without hr, vx , c, vy i ∈ T , which we call non-constrained inference model, or NI. If the NI model’s output does not match an AT, we output hSUPPORT, GOOD, PROMOTE, GOODi (AT-S1), the majority AT in the dev set. The advantage of TCI is that if a model of each ingredient is not confident about its prediction and the most-likely AT is invalid, the wrong prediction can be fixed by combining the knowledge of ATs and other confident AT ingredient predictions. The NI model entirely depend"
W18-5210,W16-2810,0,0.0972343,"ibute towards determining implicit ARs not indicated with an explicit discourse marker. Towards automatically identifying the underlying reasoning of argumentative texts, Reed (2006) created Araucaria, a corpus consisting of argumentative texts annotated with Walton et al. (2008)’s ASs. Feng and Hirst (2011) used Araucaria for creating a computational model for identifying the type of argumentation scheme. Although Araucaria is a well-known corpus in the argumentation mining community, it suffers from complex annotation guidelines which makes the annotation task difficult.1 A follow up study (Musi et al., 2016) reports that the interannotator agreement of annotating a simplified taxonomy of the Argumentum Model of Topics argumentation schemes (Rigotti, 2006; Palmieri, 2014) results in Fleiss’ κ = 0.31 (“fair agreement”) even if the annotators are trained and only a subset (8 types) of schemes are annotated. In this work, we assume the following: (i) annotating multiple types of ASs is difficult, and (ii) the reliability of annotating reasoning patterns for a single AS with implicit slot-fillers is low because when slot-fillers are not explicitly written in the original text, they must manually be ge"
W18-5210,P17-1091,0,0.026068,"re Theory (Mann and Thompson, 1987). The Penn Discourse TreeBank, the largest manually annotated corpus for discourse relations, targeted both implicit and explicit relation detection for either adjacent sentences or clauses (Prasad et al., 2008). However, these studies do not aim for capturing implicit reasoning behind arguments. AT ingredients Although we adopted a simple approach for AT ingredient identification for our first attempt (see Section 3.2), many sophisticated approaches have been proposed. Shallow discourse analysis of ARs has been extensively studied (Cocarascu and Toni, 2017; Niculae et al., 2017; Peldszus and Stede, 2015a,b). VJ identification is similar to targeted sentiment analysis (Mitchell et al., 2013; Dong et al., 2014). Somasundaran and Wiebe (2010) developed an annotation method for targeted sentiment. However, we aim to expand the annotation to other types of arguments, and their work only considers the task setting of stance classification. Finally, causal relation identification between an entity pair in a sentence has been studied (Zhang and Wang, Acknowledgements This work was supported by JSPS KAKENHI Grant Number 15H01702 and JST CREST Grant Number JPMJCR1513. We woul"
W18-5210,P14-2009,0,0.0120306,"d both implicit and explicit relation detection for either adjacent sentences or clauses (Prasad et al., 2008). However, these studies do not aim for capturing implicit reasoning behind arguments. AT ingredients Although we adopted a simple approach for AT ingredient identification for our first attempt (see Section 3.2), many sophisticated approaches have been proposed. Shallow discourse analysis of ARs has been extensively studied (Cocarascu and Toni, 2017; Niculae et al., 2017; Peldszus and Stede, 2015a,b). VJ identification is similar to targeted sentiment analysis (Mitchell et al., 2013; Dong et al., 2014). Somasundaran and Wiebe (2010) developed an annotation method for targeted sentiment. However, we aim to expand the annotation to other types of arguments, and their work only considers the task setting of stance classification. Finally, causal relation identification between an entity pair in a sentence has been studied (Zhang and Wang, Acknowledgements This work was supported by JSPS KAKENHI Grant Number 15H01702 and JST CREST Grant Number JPMJCR1513. We would like to thank Jan Šnajder and all reviewers of this work for their useful comments and feedback. 14 https://github.com/preisert/ arg"
W18-5210,P17-1002,0,0.0332912,"Missing"
W18-5210,P11-1099,0,0.126014,"., 2017; Schulz et al., 2018; Ajjour et al., 2017). We plan to use other available argumentative corpora for conducting our experiments. We will also work towards expanding our templates and integrating them into the argument reasoning task proposed in SemEval2018 (Habernal et al., 2017). Finally, we plan to capture the causal information lost by annotating other factors of the causality such as severity, truthfulness, likelihood, to name a few. Related Work ATs Reed (2006) annotated the Araucaria corpus (Reed, 2006) with Walton et al. (2008)’s argumentation schemes (AS), and successive work (Feng and Hirst, 2011) created a machine learning-model to classify an argument into five sets of schemes. However, Reed (2006) does not report the inter-annotator agreement. Lawrence and Reed (2016) created a model for instantiating ASs with a natural language representation, whereas we instantiate using templates and slotfillers. Green (2015) conducted work on identifying new ASs used in biomedical articles. Several argumentative corpora have been created for argumentation mining fields such as argument component identification, argument component classification, and structure identification (Reed et al., 2008; R"
W18-5210,D15-1110,0,0.321517,"nnot be instantiated as “OTHER”. In fact, as we report in Section 2.3, the variety of reasoning underlying ARs in the corpus we use can be largely captured by only a small number of predefined templates. Although the expressibility of a slot-filler will be reduced by embedding causal labels into our templates, the feasibility of the computational task will be increased. In the future, we plan to capture the causal information lost by annotating other factors of the causality such as severity, truthfulness, likelihood, etc. 2.1 Dataset We create our set of ATs using the arg-microtexts corpus3 (Peldszus and Stede, 2015a), a corpus of manually composed arguments, due to its high reliability of annotated relations amongst 3 annotators (Fleiss κ = 0.83).4 . The corpus contains 112 argumentative texts, each consisting of roughly five segments composed of a policy topic question, a main claim, and several premises. Each argument in a text is comprised of a policy argument, where each topic supports that one should or should not do something. Additionally, each argumentative segment was annotated with its stance (i.e. opponent or proponent) towards the topic question. 357 ARs between segments have been manually a"
W18-5210,P16-2089,0,0.0338145,"Missing"
W18-5210,P16-1205,0,0.0219006,"about, bad consequences y will occur. Introduction • Conclusion: x should not be brought about. Recognizing argumentative structures in unstructured texts is an important task for many natural language processing (NLP) applications. Argument mining is an emerging, leading field of argumentative structure identification in the NLP community. It involves a wide variety of subtasks for argumentative structure identification such as explicit premise and claim identification/classification (Reed et al., 2008; Rinott et al., 2015; Stab and Gurevych, 2014), stance classification (Hasan and Ng, 2014; Persing and Ng, 2016), and argumentative relation detection (Cocarascu where both x and y are slot-fillers and x=“charge tuition fees” and y=“a right to education will be violated”. Each AS identifies a scheme (from 65 total schemes) and appropriate slot-fillers. Instantiations of such reasoning patterns for an argument have several advantages. First, identifying such reasoning will be useful for a range of argumentation mining applications, such as aggregating multiple arguments for producing a logic-based abstractive summary. Second, we believe that it will contribute towards 79 Proceedings of the 5th Workshop o"
W18-5210,W15-0502,0,0.0165561,"rmation lost by annotating other factors of the causality such as severity, truthfulness, likelihood, to name a few. Related Work ATs Reed (2006) annotated the Araucaria corpus (Reed, 2006) with Walton et al. (2008)’s argumentation schemes (AS), and successive work (Feng and Hirst, 2011) created a machine learning-model to classify an argument into five sets of schemes. However, Reed (2006) does not report the inter-annotator agreement. Lawrence and Reed (2016) created a model for instantiating ASs with a natural language representation, whereas we instantiate using templates and slotfillers. Green (2015) conducted work on identifying new ASs used in biomedical articles. Several argumentative corpora have been created for argumentation mining fields such as argument component identification, argument component classification, and structure identification (Reed et al., 2008; Rinott et al., 2015; Stab and Gurevych, 2014). Earlier work on discourse structure analysis includes discourse theories such as Rhetorical Structure Theory (Mann and Thompson, 1987). The Penn Discourse TreeBank, the largest manually annotated corpus for discourse relations, targeted both implicit and explicit relation detec"
W18-5210,prasad-etal-2008-penn,0,0.0703974,"biomedical articles. Several argumentative corpora have been created for argumentation mining fields such as argument component identification, argument component classification, and structure identification (Reed et al., 2008; Rinott et al., 2015; Stab and Gurevych, 2014). Earlier work on discourse structure analysis includes discourse theories such as Rhetorical Structure Theory (Mann and Thompson, 1987). The Penn Discourse TreeBank, the largest manually annotated corpus for discourse relations, targeted both implicit and explicit relation detection for either adjacent sentences or clauses (Prasad et al., 2008). However, these studies do not aim for capturing implicit reasoning behind arguments. AT ingredients Although we adopted a simple approach for AT ingredient identification for our first attempt (see Section 3.2), many sophisticated approaches have been proposed. Shallow discourse analysis of ARs has been extensively studied (Cocarascu and Toni, 2017; Niculae et al., 2017; Peldszus and Stede, 2015a,b). VJ identification is similar to targeted sentiment analysis (Mitchell et al., 2013; Dong et al., 2014). Somasundaran and Wiebe (2010) developed an annotation method for targeted sentiment. Howev"
W18-5210,D14-1083,0,0.0272917,"action x is brought about, bad consequences y will occur. Introduction • Conclusion: x should not be brought about. Recognizing argumentative structures in unstructured texts is an important task for many natural language processing (NLP) applications. Argument mining is an emerging, leading field of argumentative structure identification in the NLP community. It involves a wide variety of subtasks for argumentative structure identification such as explicit premise and claim identification/classification (Reed et al., 2008; Rinott et al., 2015; Stab and Gurevych, 2014), stance classification (Hasan and Ng, 2014; Persing and Ng, 2016), and argumentative relation detection (Cocarascu where both x and y are slot-fillers and x=“charge tuition fees” and y=“a right to education will be violated”. Each AS identifies a scheme (from 65 total schemes) and appropriate slot-fillers. Instantiations of such reasoning patterns for an argument have several advantages. First, identifying such reasoning will be useful for a range of argumentation mining applications, such as aggregating multiple arguments for producing a logic-based abstractive summary. Second, we believe that it will contribute towards 79 Proceeding"
W18-5210,reed-etal-2008-language,0,0.194148,"sequences scheme, the reasoning of Example 1 can be explained as follows: • Premise : If action x is brought about, bad consequences y will occur. Introduction • Conclusion: x should not be brought about. Recognizing argumentative structures in unstructured texts is an important task for many natural language processing (NLP) applications. Argument mining is an emerging, leading field of argumentative structure identification in the NLP community. It involves a wide variety of subtasks for argumentative structure identification such as explicit premise and claim identification/classification (Reed et al., 2008; Rinott et al., 2015; Stab and Gurevych, 2014), stance classification (Hasan and Ng, 2014; Persing and Ng, 2016), and argumentative relation detection (Cocarascu where both x and y are slot-fillers and x=“charge tuition fees” and y=“a right to education will be violated”. Each AS identifies a scheme (from 65 total schemes) and appropriate slot-fillers. Instantiations of such reasoning patterns for an argument have several advantages. First, identifying such reasoning will be useful for a range of argumentation mining applications, such as aggregating multiple arguments for producing a logic-b"
W18-5210,D12-1057,0,0.0686948,"Missing"
W18-5210,W15-0507,1,0.897842,"Missing"
W18-5210,D15-1162,0,0.0244329,"), mval (y, BAD)}. We use the confidence values of each AT ingredient calculated by the separate models described in Section 3.2. For instance, given an AT instantiation hSUPPORT, BAD, PROMOTE, BADi, we create the following feature vector: {marg (SUPPORT), 0, 0, mval (x, BAD), mcau (PROMOTE) , 0, 0, mval (y, BAD)}. We Causal Relations (mcau ) We develop a simple rule-based classifier for identifying causal relations between the given slot-fillers x and y. We use a predefined list of causal phrases (i.e. causes, will lead to, etc. for PROMOTE, and destroy, 10 We parse each segment using Spacy (Honnibal and Johnson, 2015). 11 We use an existing sentiment lexicon (Warriner et al., 2013) to extract the sentiment polarity of each adjective. 85 Table 1: Performance of our AT instantiation modlearn w on training data by using an averaged structured perceptron (Collins, 2002). We call this a template-constrained inference model, or TCI. To see the effectiveness, we consider the model without hr, vx , c, vy i ∈ T , which we call non-constrained inference model, or NI. If the NI model’s output does not match an AT, we output hSUPPORT, GOOD, PROMOTE, GOODi (AT-S1), the majority AT in the dev set. The advantage of TCI i"
W18-5210,D15-1050,0,0.0756046,"Missing"
W18-5210,N18-2006,0,0.0355159,"Missing"
W18-5210,W10-0214,0,0.0900013,"explicit relation detection for either adjacent sentences or clauses (Prasad et al., 2008). However, these studies do not aim for capturing implicit reasoning behind arguments. AT ingredients Although we adopted a simple approach for AT ingredient identification for our first attempt (see Section 3.2), many sophisticated approaches have been proposed. Shallow discourse analysis of ARs has been extensively studied (Cocarascu and Toni, 2017; Niculae et al., 2017; Peldszus and Stede, 2015a,b). VJ identification is similar to targeted sentiment analysis (Mitchell et al., 2013; Dong et al., 2014). Somasundaran and Wiebe (2010) developed an annotation method for targeted sentiment. However, we aim to expand the annotation to other types of arguments, and their work only considers the task setting of stance classification. Finally, causal relation identification between an entity pair in a sentence has been studied (Zhang and Wang, Acknowledgements This work was supported by JSPS KAKENHI Grant Number 15H01702 and JST CREST Grant Number JPMJCR1513. We would like to thank Jan Šnajder and all reviewers of this work for their useful comments and feedback. 14 https://github.com/preisert/ argument-reasoning-patterns 87 Ref"
W18-5210,W14-2110,0,0.0505695,"Missing"
W18-5210,C14-1142,0,0.0938583,"le 1 can be explained as follows: • Premise : If action x is brought about, bad consequences y will occur. Introduction • Conclusion: x should not be brought about. Recognizing argumentative structures in unstructured texts is an important task for many natural language processing (NLP) applications. Argument mining is an emerging, leading field of argumentative structure identification in the NLP community. It involves a wide variety of subtasks for argumentative structure identification such as explicit premise and claim identification/classification (Reed et al., 2008; Rinott et al., 2015; Stab and Gurevych, 2014), stance classification (Hasan and Ng, 2014; Persing and Ng, 2016), and argumentative relation detection (Cocarascu where both x and y are slot-fillers and x=“charge tuition fees” and y=“a right to education will be violated”. Each AS identifies a scheme (from 65 total schemes) and appropriate slot-fillers. Instantiations of such reasoning patterns for an argument have several advantages. First, identifying such reasoning will be useful for a range of argumentation mining applications, such as aggregating multiple arguments for producing a logic-based abstractive summary. Second, we believe th"
W18-5210,J17-3005,0,0.0334397,"Missing"
W18-5210,C16-1158,0,0.0325667,"Missing"
W18-5410,D14-1179,0,0.0293295,"Missing"
W18-5410,P17-1106,0,0.0248262,"et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has been an epoch-making development that has led to great progress in many natural language generation tasks, such as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku University and NTT Communication Science Laboratories. 1 Our code for reproducing the experiments is available at https://github.com/butsugiri/UAM 74 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 74–81 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics 3.1 sp"
W18-5410,I17-1004,0,0.0451013,"EN Center for Advanced Intelligence Project {kiyono,jun.suzuki,inui}@ecei.tohoku.ac.jp, {takase.sho, nagata.masaaki}@lab.ntt.co.jp, okazaki@c.titech.ac.jp Abstract The assumption behind this interpretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this pa"
W18-5410,P15-1152,0,0.0223308,"se a method that explicitly models the token-wise alignment between the source and target sequences to provide a better analysis. Experiments show that our method can acquire token-wise alignments that are superior to those of an attention mechanism1 . 1 Introduction The Encoder-Decoder model with an attention mechanism (EncDec) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has been an epoch-making development that has led to great progress in many natural language generation tasks, such as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku"
W18-5410,D17-1227,0,0.0166013,"ns q˜ with the sum of the source-side ˜ as an objective function `src . tokens x Since oj for each j is a vector representation of the ˆ 0:j−1 , X, θ) over the target probabilities of p(y|y vocabularies yˆ ∈ Vt , we can calculate `trg as (8) `trg (Y , X, θ) = − J+1 X  yj> · log oj . (12) j=1 3.3 Inference of EncDec In the inference step, we use the trained parameters to search for the best target sequence. We use beam search to find the target sequence that maximizes the product of the conditional probabilities as described in Equation 1. From among several stopping criteria for beam search (Huang et al., 2017), we adopt the widely used “shrinking beam” implemented in RNNsearch (Bahdanau et al., 2015)3 . (9) where Ws ∈ RH×2H is a parameter matrix. Finally, zj is fed into the softmax layer. The model generates a target-side token based on the probability distribution oj ∈ RVt as (10) where Wo ∈ RVt ×H is a parameter matrix and bo ∈ RVt is a bias term. 3.2 & ? + (?"":$ ) Next, the source-side information is mixed with the decoder hidden state to derive final hidden state zj . Concretely, the context vector cj is concate~j to form vector uj ∈ R2H . uj is then nated with z fed into a single fully-connect"
W18-5410,E17-2047,1,0.939025,"ecent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation task, which is categorized as a loss-less generation (lossless-gen) task, the headline generation task additionally requires EncDec models to appropriately select salient ideas in given source sentences (Suzuki and Nagata, 2017). Therefore, the lossy-gen task seems to make modeling by EncDec much harder. In fact, our preliminary experiments revealed that the attention mechanism in EncDec models largely fails to capture token-wise alignments, e.g., less than 10 percent accuracy, even if we use one of the current state-of-the-art EncDec models (Table 3). To obtain a better analysis of how EncDec models translate a given source sentence to the correDeveloping a method for understanding the inner workings of black-box neural methods is an important research endeavor. Conventionally, many studies have used an attention ma"
W18-5410,W17-3204,0,0.0232542,"attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation task, which is categorized a"
W18-5410,N06-1014,0,0.0899348,"pretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation ta"
W18-5410,D16-1033,0,0.017834,"hat used in Rush et al. (2015). The dataset consists of pairs of the first sentence of each article and its headline from the annotated English Gigaword corpus (Napoles et al., 2012). Rush et al. (2015) defined the training, validation and test splits, which contain approximately 3.8M, 200K and 400K source-headline pairs, respectively We used the entire training split for training as in the previous studies. We randomly sampled 8K instances as validation data and 10K instances as test data from the validation split. Moreover, we experimented on the test data provided by Zhou et al. (2017) and Toutanova et al. (2016) for comparison with the reported state-of-the-art performance (Zhou et al., 2017). We refer to those test data sets as Test (Ours), Test (Zhou), and MSRATC respectively. Among these test sets, MSRATC is the only dataset created by a human worker. 5.2 5131 5131 200 400 Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) 2-layer bidirectional-LSTM 2-layer LSTM with attention (Luong et al., 2015) Adam (Kingma and Ba, 2015) 0.001 0.5 for each epoch (after epoch 9) 10 256 (shuffled at each epoch) 5 Max 15 epochs with early stopping Dropout (rate 0.3) Beam size 20 with the length norma"
W18-5410,C16-1291,0,0.0193027,"Intelligence Project {kiyono,jun.suzuki,inui}@ecei.tohoku.ac.jp, {takase.sho, nagata.masaaki}@lab.ntt.co.jp, okazaki@c.titech.ac.jp Abstract The assumption behind this interpretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclu"
W18-5410,P16-1008,0,0.10152,"as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku University and NTT Communication Science Laboratories. 1 Our code for reproducing the experiments is available at https://github.com/butsugiri/UAM 74 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 74–81 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics 3.1 sponding target sentence in the headline generation task, this paper introduces the Unsupervised tokenwise Alignment Module (UAM), a novel component that can be plugged into"
W18-5410,D15-1166,0,0.627426,".suzuki,inui}@ecei.tohoku.ac.jp, {takase.sho, nagata.masaaki}@lab.ntt.co.jp, okazaki@c.titech.ac.jp Abstract The assumption behind this interpretation is that the attention matrix has a “soft” token-wise alignment between the source and target sequences, and thus we can use EncDec models to skim which tokens in the source are converted to which tokens in the target. However, recent studies have empirically revealed that an attention model can operate not only for token-wise alignment but also for other functionalities, such as reordering (Ghader and Monz, 2017; Liu et al., 2016). In addition, Luong et al. (2015) reported that the quality of attention matrixbased alignment is quantitatively inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation"
W18-5410,P17-1101,0,0.406026,"ments is identical to that used in Rush et al. (2015). The dataset consists of pairs of the first sentence of each article and its headline from the annotated English Gigaword corpus (Napoles et al., 2012). Rush et al. (2015) defined the training, validation and test splits, which contain approximately 3.8M, 200K and 400K source-headline pairs, respectively We used the entire training split for training as in the previous studies. We randomly sampled 8K instances as validation data and 10K instances as test data from the validation split. Moreover, we experimented on the test data provided by Zhou et al. (2017) and Toutanova et al. (2016) for comparison with the reported state-of-the-art performance (Zhou et al., 2017). We refer to those test data sets as Test (Ours), Test (Zhou), and MSRATC respectively. Among these test sets, MSRATC is the only dataset created by a human worker. 5.2 5131 5131 200 400 Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) 2-layer bidirectional-LSTM 2-layer LSTM with attention (Luong et al., 2015) Adam (Kingma and Ba, 2015) 0.001 0.5 for each epoch (after epoch 9) 10 256 (shuffled at each epoch) 5 Max 15 epochs with early stopping Dropout (rate 0.3) Beam s"
W18-5410,K16-1028,0,0.201448,"y inferior to that of the Berkeley aligner (Liang et al., 2006). Koehn and Knowles (2017) also reported that attention matrix-based alignment is significantly different from that acquired from an off-the-shelf aligner for English-German language pairs. From these recent findings, the goal of this paper is to provide a method that can offer a better interpretation of how EncDec models translate a given source sentence to the corresponding target sentence. In this paper, we focus exclusively on the headline generation task, which is categorized as a lossycompression generation (lossy-gen) task (Nallapati et al., 2016). Compared with a machine translation task, which is categorized as a loss-less generation (lossless-gen) task, the headline generation task additionally requires EncDec models to appropriately select salient ideas in given source sentences (Suzuki and Nagata, 2017). Therefore, the lossy-gen task seems to make modeling by EncDec much harder. In fact, our preliminary experiments revealed that the attention mechanism in EncDec models largely fails to capture token-wise alignments, e.g., less than 10 percent accuracy, even if we use one of the current state-of-the-art EncDec models (Table 3). To"
W18-5410,W12-3018,0,0.197412,"Missing"
W18-5410,D15-1044,0,0.46313,"ise alignment between the source and target sequences to provide a better analysis. Experiments show that our method can acquire token-wise alignments that are superior to those of an attention mechanism1 . 1 Introduction The Encoder-Decoder model with an attention mechanism (EncDec) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has been an epoch-making development that has led to great progress in many natural language generation tasks, such as machine translation (Bahdanau et al., 2015), dialog generation (Shang et al., 2015), and headline generation (Rush et al., 2015). An enormous number of studies have attempted to enhance the ability of EncDec. Furthermore, several intensive studies have also attempted to analyze and interpret the inside of black-box EncDec models, especially how they translate a given source sentence to the corresponding target sentence (Ding et al., 2017). One typical approach to this is to visualize an attention matrix, which is a collection of attention vectors (Bahdanau et al., 2015; Luong et al., 2015; Tu et al., 2016). ∗ This work is a product of collaborative research program of Tohoku University and NTT Communication Science Lab"
W18-5410,P16-1162,0,0.0510147,"ly for MSR-ATC and a performance comparable to EncDec+sGate in Test (Ours) and Test (Zhou). Considering that the MSR-ATC dataset was created by a human worker, we believe that the improvement in MSR-ATC is the most remarkable result among the three test sets, since it indicates that our model most closely fits the human-generated summary. Implementation Details In the experiment, we selected the hyper-parameter settings commonly used in previous studies e.g., (Rush et al., 2015; Nallapati et al., 2016; Suzuki and Nagata, 2017) We constructed the vocabulary set using byte pair encoding4 (BPE) (Sennrich et al., 2016) to handle low-frequency words, since this is now a common practice in the field of neural machine translation. The BPE merge operations are jointly learned from the source and the target. We set the number of BPE merge operations at 5, 000. 6 Discussion We investigated whether the UAM improves tokenwise alignment between the source and target se5 We restored sub-words to the standard token split for the evaluation. 6 ROUGE script option is: “-n2 -m -w 1.2” 4 https://github.com/rsennrich/ subword-nmt 78 Test (Ours) Test (Zhou) MSR-ATC Model RG-1 RG-2 RG-L RG-1 RG-2 RG-L RG-1 RG-2 RG-L EncDec+s"
W18-5607,S16-1192,0,0.112671,"5 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 55–64 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics sion with a variety of features (lexical, syntactic, morphological, and many others) were the predominant approach. In fact, the best performance was achieved by UTHealth team (Lee et al., 2016) using an end-to-end system based on linear and structural Hidden Markov Model (HMM)-SVM. Just a few teams tried a neural based method, including RNN-based models (Fries, 2016) and CNN-based models (Chikka, 2016), (Li and Huang, 2016). Furthermore, among those teams just Chikka (2016) participated in the CONTAINS identification task, being around 0.30 below UTHealth's top performance. Recent work by Lin et al. (2016), Dligach et al. (2017) and Leeuwenberg and Moens (2017) followed the settings of 2016 Clinical TempEval challenge but they did not participate in the competition. Out of these, our results are only directly comparable to those of Lin et al. (2016) and Leeuwenberg and Moens (2017) since the work of Dligach et al. (2017) was not evaluated using the Clinical TempEval official scorer. Even th"
W18-5607,E12-1027,0,0.0610249,"he time expressions combined with actions that we perceive as ongoing. For example, in sentence 9 the action of moving is an activity, done indeterminably throughout the day as multiple times a day imply. In sentence 7, on the other hand, there is a time expression with a definite time interval overlapping the patient's state of being hospitalized. Temporally locating two events on a timeline requires a high level of reasoning that even for humans can turn into a complicated task. All of the aforementioned inferences were done heavily relying on the internal constituency of an event, implying Costa and Branco (2012) claim that temporal information processing can profit from information about aspectual type is valid in the clinical domain. Due to the high similarity of CONTAINS and OVERLAP relations it does not come as a surprise that these two types are easily confused by our system, which performed reasonably well on identifying other TLINK types with similar number of instances. This suggests than the main problem is not the amount of data available but how temporal properties are encoded in language. Aspectual information proved useful for differentiating between two of the most frequent and most simi"
W18-5607,E17-2118,0,0.392835,"ety of features (lexical, syntactic, morphological, and many others) were the predominant approach. In fact, the best performance was achieved by UTHealth team (Lee et al., 2016) using an end-to-end system based on linear and structural Hidden Markov Model (HMM)-SVM. Just a few teams tried a neural based method, including RNN-based models (Fries, 2016) and CNN-based models (Chikka, 2016), (Li and Huang, 2016). Furthermore, among those teams just Chikka (2016) participated in the CONTAINS identification task, being around 0.30 below UTHealth's top performance. Recent work by Lin et al. (2016), Dligach et al. (2017) and Leeuwenberg and Moens (2017) followed the settings of 2016 Clinical TempEval challenge but they did not participate in the competition. Out of these, our results are only directly comparable to those of Lin et al. (2016) and Leeuwenberg and Moens (2017) since the work of Dligach et al. (2017) was not evaluated using the Clinical TempEval official scorer. Even though Leeuwenberg and Moens (2017) established a new state-of-the-art in temporal relation extraction, their result is still below human performance. Moreover, none of the aforementioned works provides a detailed discussion of why i"
W18-5607,S16-1198,0,0.02191,"ll around 0.21. Regardless of the 55 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 55–64 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics sion with a variety of features (lexical, syntactic, morphological, and many others) were the predominant approach. In fact, the best performance was achieved by UTHealth team (Lee et al., 2016) using an end-to-end system based on linear and structural Hidden Markov Model (HMM)-SVM. Just a few teams tried a neural based method, including RNN-based models (Fries, 2016) and CNN-based models (Chikka, 2016), (Li and Huang, 2016). Furthermore, among those teams just Chikka (2016) participated in the CONTAINS identification task, being around 0.30 below UTHealth's top performance. Recent work by Lin et al. (2016), Dligach et al. (2017) and Leeuwenberg and Moens (2017) followed the settings of 2016 Clinical TempEval challenge but they did not participate in the competition. Out of these, our results are only directly comparable to those of Lin et al. (2016) and Leeuwenberg and Moens (2017) since the work of Dligach et al. (2017) was not evaluated using the Clinic"
W18-5607,W09-2415,0,0.0315951,"Missing"
W18-5607,S16-1201,0,0.220199,"Missing"
W18-5607,S16-1199,0,0.015557,"ate in the competition. Out of these, our results are only directly comparable to those of Lin et al. (2016) and Leeuwenberg and Moens (2017) since the work of Dligach et al. (2017) was not evaluated using the Clinical TempEval official scorer. Even though Leeuwenberg and Moens (2017) established a new state-of-the-art in temporal relation extraction, their result is still below human performance. Moreover, none of the aforementioned works provides a detailed discussion of why is current performance so low and how can we improve further the results on temporal relation extraction, except from Leeuwenberg and Moens (2016), which in their first attempt on tackling this task on 2016 Clinical TempEval identified false negatives as their major problem. Our contribution is a deep error analysis taking into account the performance of our model on predicting all TLINK types. As a result, we were able to identify important clues on temporal relation extraction and based on these findings, we discuss the next step towards human-like temporal reasoning performance. increase in annotation agreement of temporal relations by relying on narrative containers, there is a consensus within the research community regarding TIE d"
W18-5607,S15-2136,0,0.178224,"Missing"
W18-5607,E17-1108,0,0.581988,"r annotation schema, based on the widely used TIE annotation standard ISOTimeML (Pustejovsky et al., 2010). Narrative containers were defined by Pustejovsky and Stubbs Results of the systems participating in Clinical TempEval suggest that they perform well on timeentity identification tasks. Nevertheless, temporal relation extraction has shown to be the most difficult. UTHealth (Lee et al., 2016), the best ranked system in 2016 Clinical TempEval, showed a significant gap of 0.25 when compared to human performance even with gold-standard entity annotations. Recent work by Lin et al. (2016) and Leeuwenberg and Moens (2017) improved UTHealth's results further but the gap with respect to humans is still around 0.21. Regardless of the 55 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 55–64 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics sion with a variety of features (lexical, syntactic, morphological, and many others) were the predominant approach. In fact, the best performance was achieved by UTHealth team (Lee et al., 2016) using an end-to-end system based on linear and structural Hidden Markov Model (HMM)-S"
W18-5607,S16-1165,0,0.0459765,"Missing"
W18-5607,S16-1197,0,0.0183951,"the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 55–64 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics sion with a variety of features (lexical, syntactic, morphological, and many others) were the predominant approach. In fact, the best performance was achieved by UTHealth team (Lee et al., 2016) using an end-to-end system based on linear and structural Hidden Markov Model (HMM)-SVM. Just a few teams tried a neural based method, including RNN-based models (Fries, 2016) and CNN-based models (Chikka, 2016), (Li and Huang, 2016). Furthermore, among those teams just Chikka (2016) participated in the CONTAINS identification task, being around 0.30 below UTHealth's top performance. Recent work by Lin et al. (2016), Dligach et al. (2017) and Leeuwenberg and Moens (2017) followed the settings of 2016 Clinical TempEval challenge but they did not participate in the competition. Out of these, our results are only directly comparable to those of Lin et al. (2016) and Leeuwenberg and Moens (2017) since the work of Dligach et al. (2017) was not evaluated using the Clinical TempEval official scorer. Even though Leeuwenberg and M"
W18-5607,S17-2093,0,0.236971,"Missing"
W18-5607,W16-2914,0,0.0123189,"ive containers to their annotation schema, based on the widely used TIE annotation standard ISOTimeML (Pustejovsky et al., 2010). Narrative containers were defined by Pustejovsky and Stubbs Results of the systems participating in Clinical TempEval suggest that they perform well on timeentity identification tasks. Nevertheless, temporal relation extraction has shown to be the most difficult. UTHealth (Lee et al., 2016), the best ranked system in 2016 Clinical TempEval, showed a significant gap of 0.25 when compared to human performance even with gold-standard entity annotations. Recent work by Lin et al. (2016) and Leeuwenberg and Moens (2017) improved UTHealth's results further but the gap with respect to humans is still around 0.21. Regardless of the 55 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 55–64 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics sion with a variety of features (lexical, syntactic, morphological, and many others) were the predominant approach. In fact, the best performance was achieved by UTHealth team (Lee et al., 2016) using an end-to-end system based on linear and struc"
W18-5607,P16-1105,0,0.185049,"}@ecei.tohoku.ac.jp okazaki@dc.titech.ac.jp Abstract Temporal reasoning remains as an unsolved task for Natural Language Processing (NLP), particularly demonstrated in the clinical domain. The complexity of temporal representation in language is evident as results of the 2016 Clinical TempEval challenge indicate: the current state-of-the-art systems perform well in solving mention-identification tasks of event and time expressions but poorly in temporal relation extraction, showing a gap of around 0.25 point below human performance. We explore to adapt the tree-based LSTMRNN model proposed by Miwa and Bansal (2016) to temporal relation extraction from clinical text, obtaining a five point improvement over the best 2016 Clinical TempEval system and two points over the state-of-theart. We deliver a deep analysis of the results and discuss the next step towards human-like temporal reasoning. 1 Figure 1: Example temporal relation annotation with and without using narrative containers. (2011) as an effort to reduce the scope of temporal relations between pairs of events and time expressions. As illustrated in Figure 1, narrative containers can be thought of as temporal buckets in which an event or series of"
W18-5607,pustejovsky-etal-2010-iso,0,0.0224165,"is a key to text processing tasks including Question Answering and Text Summarization and follows the traditional pipeline of named entity recognition (NER) and relation extraction separately. Research on this area has been led by TempEval shared tasks (Verhagen et al., 2007, 2010; UzZaman et al., 2013) but in recent years, the target domain has been shifted to the clinical domain. The resulting Clinical TempEval challenges (Bethard et al., 2015, 2016, 2017) introduced the adoption of narrative containers to their annotation schema, based on the widely used TIE annotation standard ISOTimeML (Pustejovsky et al., 2010). Narrative containers were defined by Pustejovsky and Stubbs Results of the systems participating in Clinical TempEval suggest that they perform well on timeentity identification tasks. Nevertheless, temporal relation extraction has shown to be the most difficult. UTHealth (Lee et al., 2016), the best ranked system in 2016 Clinical TempEval, showed a significant gap of 0.25 when compared to human performance even with gold-standard entity annotations. Recent work by Lin et al. (2016) and Leeuwenberg and Moens (2017) improved UTHealth's results further but the gap with respect to humans is sti"
W18-5607,W11-0419,0,0.2467,"ess unless explicitly qualified (“We resected the adenocarcinoma, and since margins were clear, we can say it is gone”). This is a non-trivial task for a computer even when relying on context information. Up to now, there have been several attempts on tackling temporal relation extraction from clinical text mostly led by the Clinical TempEval challenges. However, the results are still far from human performance and there is little information of Table 6: Distribution of misclassified CONTAINS and OVERLAP Event-Event pairs by type of EVENT . Abbreviations: V, Verb; NV, Non-Verb As mentioned by Pustejovsky and Stubbs (2011) and further discussed in Styler IV et al. (2014), EVENT-EVENT pairings are a complex and vital component, particularly in clinical narratives where doctors rely on shared domain knowledge and it is essential to read “between the lines”. The distribution of verb/non-verb entities in Table 6 indicates that most of EVENT-EVENT missclasified pairings were either of NV-NV type or include a NV entity. Time intervals of NV entities like “pain” or “resection” are more difficult to understand, while V entities like “removed” or “improving” have their time properties morphologically encoded. Thus, rega"
W18-5607,S13-2001,0,0.0364528,"tification of other temporal relation types. Until now, the only corpus annotated with narrative containers is limited to clinical texts. Introduction Temporal Information Extraction (TIE) is an active research area in NLP, where the ultimate goal is to be able to represent the development of a story over time. TIE is a key to text processing tasks including Question Answering and Text Summarization and follows the traditional pipeline of named entity recognition (NER) and relation extraction separately. Research on this area has been led by TempEval shared tasks (Verhagen et al., 2007, 2010; UzZaman et al., 2013) but in recent years, the target domain has been shifted to the clinical domain. The resulting Clinical TempEval challenges (Bethard et al., 2015, 2016, 2017) introduced the adoption of narrative containers to their annotation schema, based on the widely used TIE annotation standard ISOTimeML (Pustejovsky et al., 2010). Narrative containers were defined by Pustejovsky and Stubbs Results of the systems participating in Clinical TempEval suggest that they perform well on timeentity identification tasks. Nevertheless, temporal relation extraction has shown to be the most difficult. UTHealth (Lee"
W18-5607,S07-1014,0,0.127556,"text and facilitate the identification of other temporal relation types. Until now, the only corpus annotated with narrative containers is limited to clinical texts. Introduction Temporal Information Extraction (TIE) is an active research area in NLP, where the ultimate goal is to be able to represent the development of a story over time. TIE is a key to text processing tasks including Question Answering and Text Summarization and follows the traditional pipeline of named entity recognition (NER) and relation extraction separately. Research on this area has been led by TempEval shared tasks (Verhagen et al., 2007, 2010; UzZaman et al., 2013) but in recent years, the target domain has been shifted to the clinical domain. The resulting Clinical TempEval challenges (Bethard et al., 2015, 2016, 2017) introduced the adoption of narrative containers to their annotation schema, based on the widely used TIE annotation standard ISOTimeML (Pustejovsky et al., 2010). Narrative containers were defined by Pustejovsky and Stubbs Results of the systems participating in Clinical TempEval suggest that they perform well on timeentity identification tasks. Nevertheless, temporal relation extraction has shown to be the m"
W19-2601,P15-1067,0,0.438175,"octurnal pain that prevents sleep and that is alleviated with aspirin. 1 https://www.nlm.nih.gov/research/ umls/ 1 Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications, pages 1–10 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics Therefore, given a new dataset (e.g., a biomedical dataset), it is necessary to extend the base model with other competitive KGC models, and choose the best fit for the given dataset. However, the base model only implements two KGC models, which are based on TransE (Bordes et al., 2013) and TransD (Ji et al., 2015) respectively. Thus, the second question of this work is how other competitive KGC models such as ComplEx (Trouillon et al., 2016) and SimplE (Kazemi and Poole, 2018) influence the performance of the base model on biomedical dataset. At last but not least, in biomedical KG, a relation is scientifically restricted by entity type (ET). For instance, in the relation (h, may treat, t), the ET of t should be Disease or Syndrome. Therefore, ET information is an important feature for biomedical RE and KGC. For leveraging the ET information, which the base model lacks, in this work, we propose an end-"
W19-2601,S17-2097,0,0.019743,"approaches for non-scientific RE, which include Convolutional Neural Network (CNN)-based frameworks (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015) Recurrent Neural Network (RNN)-based frameworks (Zhang and Wang, 2015; Miwa and Bansal, 2016; Zhou et al., 2016). NN-based approaches are also used in scientific RE. For instance, (Gu et al., 2017) utilizes a CNN-based model for identifying chemical-disease relations from Medline corpus. (Hahn-Powell et al., 2016) proposes an LSTM-based model for identifying causal precedence relationship between two event mentions in biomedical papers. (Ammar et al., 2017) applies (Miwa and Bansal, 2016)’s model for scientific 2 RC RE. KGS sﬁnal ... Knowledge Graph Scoring Attention Scoring ATS Although remarkably good performances are achieved by the models mentioned above, they still train and extract relations on sentence-level and thus need a large amount of annotation data, which is expensive and time-consuming. To address this issue, distant supervision is proposed by (Mintz et al., 2009). To alleviate the noisy data from the distant supervision, many studies model distant supervision for RE as a Multiple Instance Learning (MIL) problem (Riedel et al., 20"
W19-2601,P16-1200,0,0.205144,"mation stored in those neglected sentences, For instance, among Example 1, Example 2 and Example 3, Example 1 is undoubtedly the most informative one for detecting relation may treat, but it unnecessarily means other sentences such as Example 3 could not contribute to the relation detection. In Example 3, entity aspirin and entity pentoxifylline have alternative relation, and the latter is a drug to treat muscle pain, therefore the former is also likely to be a pain-killing drug. To address this issue, recently, attention mechanism is applied to extract features from all collected sentences. (Lin et al., 2016) proposes a relation vector based attention mechanism for distantly supervised RE. (Han et al., 2018) proposes a novel joint model that leverages the KG-based attention mechanism and achieves better performance than (Lin et al., 2016) on distantly supervised RE from New York Times (NYT) corpus. The success that the joint model (Han et al., 2018) has attained in the newswire domain (or non-scientific domain) inspires us to choose the strong model as our base model and assess its feasibility on biomedical domain. Specifically, the first question of this research is how the joint model behaves wh"
W19-2601,P09-1113,0,0.705963,"and this would impede their usefulness in real-world applications. Scientific KGs, on the one hand, face the data sparsity problem. On the other hand, scientific publications have become the largest repository ever for scientific KGs and continue to increase at an unprecedented rate (Munroe, 2013). Therefore, it is an essential and fundamental task to turn the unstructured scientific publications into well organized KG, and it belongs to the task of RE. In RE, one obstacle that is encountered when building a RE system is the generation of training instances. For coping with this difficulty, (Mintz et al., 2009) proposes distant supervision to automatically generate training samples via leveraging the alignment between KGs and texts. They assumes that if two entities are connected by a relation in a KG, then all sentences that contain these entity pairs will express the relation. For instance, (aspirin, may treat, pain) is a fact triplet in UMLS. Distant supervision will automatically label all sentences, such as Example 1, Example 2 and Example 3, as positive instances for the relation may treat. Although distant supervision could provide a large amount of training data at low cost, it always suffer"
W19-2601,P16-1105,0,0.0269766,"g., Medline corpus). (Han et al., 2018) indicates that the performance of the base model could be affected the representation ability of KGC model. The representation ability of a KGC model also varies with dataset (Wang et al., 2017). 2 Related Work RE is a fundamental task in the NLP community. In recent years, Neural Network (NN)-based models have been the dominant approaches for non-scientific RE, which include Convolutional Neural Network (CNN)-based frameworks (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015) Recurrent Neural Network (RNN)-based frameworks (Zhang and Wang, 2015; Miwa and Bansal, 2016; Zhou et al., 2016). NN-based approaches are also used in scientific RE. For instance, (Gu et al., 2017) utilizes a CNN-based model for identifying chemical-disease relations from Medline corpus. (Hahn-Powell et al., 2016) proposes an LSTM-based model for identifying causal precedence relationship between two event mentions in biomedical papers. (Ammar et al., 2017) applies (Miwa and Bansal, 2016)’s model for scientific 2 RC RE. KGS sﬁnal ... Knowledge Graph Scoring Attention Scoring ATS Although remarkably good performances are achieved by the models mentioned above, they still train and ext"
W19-2601,W16-2920,0,0.0705316,"Missing"
W19-2601,P11-1055,0,0.187698,"biomedical datasets in which KG is collected from UMLS and textual data is extracted from Medline corpus. The experimental results not only show the feasibility of the base model on the biomedical domain, but also prove the effectiveness of our proposed extensions for the base model. (2) The tumor was remarkably large in size , and pain unrelieved by aspirin. (3) The level of pain did not change significantly with either aspirin or pentoxifylline , but the walking distance was farther with the pentoxifylline group . To automatically alleviate the wrong labelling problem, (Riedel et al., 2010; Hoffmann et al., 2011) apply multi-instance learning. In order to avoid the handcrafted features and errors propagated from NLP tools, (Zeng et al., 2015) proposes a Convolutional Neural Network (CNN), which incorporate mutli-instance learning with neural network model, and achieves significant improvement in distantly supervised RE. Despite the impressive achievement in RE, this model still has the limitation that it only selects the most informative sentence and ignores the rest, thereby loses the rich information stored in those neglected sentences, For instance, among Example 1, Example 2 and Example 3, Example"
W19-2601,D13-1136,0,0.126369,"lions of biomedical concepts and relations between them. We follow (Wang et al., 2014), and only collect the fact triplet with RO relation category (RO stands for “has Relationship Other than synonymous, narrower, or broader”), which covers the interesting relations like may treat, my prevent, etc. From the UMLS 2018 release, we extract about 60 thousand such RO fact triplets (i.e., (e1 , r, e2 )) under the restriction that their entity pairs (i.e., e1 and e2 ) should coexist within a sentence in Medline corpus. They are then randomly divided into training and testing sets for KGC. Following (Weston et al., 2013), we keep high entity overlap between training and testing set, but zero fact triplet overlap. The statistics of the extracted KG is shown in Table 1. For training the ET Classification Part in Section 4.3, we also collect about 35 thousand entity-ET pairs (e.g., heart rates-Clinical Attribute) from the UMLS 2018 release. Textual Data. Medline corpus is a collection of bimedical abstracts maintained by the National Library of Medicine. From the Medline corpus, by applying a string matching model 2 , we extract 732, 771 sentences that contain the entity pairs (i.e., e1 and e2 ) in the KG mentio"
W19-2601,D15-1062,0,0.0587367,"Missing"
W19-2601,D15-1203,0,0.156107,"nly show the feasibility of the base model on the biomedical domain, but also prove the effectiveness of our proposed extensions for the base model. (2) The tumor was remarkably large in size , and pain unrelieved by aspirin. (3) The level of pain did not change significantly with either aspirin or pentoxifylline , but the walking distance was farther with the pentoxifylline group . To automatically alleviate the wrong labelling problem, (Riedel et al., 2010; Hoffmann et al., 2011) apply multi-instance learning. In order to avoid the handcrafted features and errors propagated from NLP tools, (Zeng et al., 2015) proposes a Convolutional Neural Network (CNN), which incorporate mutli-instance learning with neural network model, and achieves significant improvement in distantly supervised RE. Despite the impressive achievement in RE, this model still has the limitation that it only selects the most informative sentence and ignores the rest, thereby loses the rich information stored in those neglected sentences, For instance, among Example 1, Example 2 and Example 3, Example 1 is undoubtedly the most informative one for detecting relation may treat, but it unnecessarily means other sentences such as Exam"
W19-2601,C14-1220,0,0.167946,"f this research is how the joint model behaves when the system is trained on biomedical KG (e.g., UMLS) and biomeical corpus (e.g., Medline corpus). (Han et al., 2018) indicates that the performance of the base model could be affected the representation ability of KGC model. The representation ability of a KGC model also varies with dataset (Wang et al., 2017). 2 Related Work RE is a fundamental task in the NLP community. In recent years, Neural Network (NN)-based models have been the dominant approaches for non-scientific RE, which include Convolutional Neural Network (CNN)-based frameworks (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015) Recurrent Neural Network (RNN)-based frameworks (Zhang and Wang, 2015; Miwa and Bansal, 2016; Zhou et al., 2016). NN-based approaches are also used in scientific RE. For instance, (Gu et al., 2017) utilizes a CNN-based model for identifying chemical-disease relations from Medline corpus. (Hahn-Powell et al., 2016) proposes an LSTM-based model for identifying causal precedence relationship between two event mentions in biomedical papers. (Ammar et al., 2017) applies (Miwa and Bansal, 2016)’s model for scientific 2 RC RE. KGS sﬁnal ... Knowledge Graph Scor"
W19-2601,P16-2034,0,0.025872,"an et al., 2018) indicates that the performance of the base model could be affected the representation ability of KGC model. The representation ability of a KGC model also varies with dataset (Wang et al., 2017). 2 Related Work RE is a fundamental task in the NLP community. In recent years, Neural Network (NN)-based models have been the dominant approaches for non-scientific RE, which include Convolutional Neural Network (CNN)-based frameworks (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015) Recurrent Neural Network (RNN)-based frameworks (Zhang and Wang, 2015; Miwa and Bansal, 2016; Zhou et al., 2016). NN-based approaches are also used in scientific RE. For instance, (Gu et al., 2017) utilizes a CNN-based model for identifying chemical-disease relations from Medline corpus. (Hahn-Powell et al., 2016) proposes an LSTM-based model for identifying causal precedence relationship between two event mentions in biomedical papers. (Ammar et al., 2017) applies (Miwa and Bansal, 2016)’s model for scientific 2 RC RE. KGS sﬁnal ... Knowledge Graph Scoring Attention Scoring ATS Although remarkably good performances are achieved by the models mentioned above, they still train and extract relations on se"
W19-2605,S17-2091,0,0.064957,"Missing"
W19-2605,N18-1202,0,0.0114985,"del We formulate the automatic extraction task as a BIO sequence tagging task. Specifically, given a sentence, the model tags each word as one of {O, B-POS, I-POS, B-NEG, I-NEG, B-NEU, I-NEU}, where a combination of BI tags represents a Positive (POS), Negative (NEG), and Neutral (NEU) technical term span. We use the BiLSTM-CRF model proposed by Lample et al. (2016) which was originally designed for the task of named entity recognition.5 Regarding word embedding, we use word2vec (Mikolov et al., 2013) embeddings trained on ACL Anthology Corpus (Aizawa et al., 2018) (henceforth, CL), and ELMo (Peters et al., 2018) embeddings trained on 1 Billion Word Benchmark (henceforth, EL). (4) In essence, ranking models directly capture ::::::::::::: during training the competition among potential antecedent candidates, instead of considering them independently. (D08-1069) We found a large number of cases where sentences took the form of concession. In Example (5), one annotator labeled the pairwise approach as Negative and the other Neutral. We speculate that annotators were confused because the pairwise approach is evaluated positively by the phrase high precision in the subordinate clause, but negatively by the"
W19-2605,W16-3002,0,0.0661037,"Missing"
W19-2605,S15-2082,0,0.0611279,"Missing"
W19-2605,S18-1111,0,0.0446898,"Missing"
W19-2605,E12-2021,0,0.129566,"Missing"
W19-2605,P11-1016,0,0.414103,"pproaches such as BioNLP (Del´eger et al., 2016). These technologies are the foundation of scientific search engines or knowledge discovery tools, such as Semantic Scholar2 and Dr. Inventor (Ronzano and Saggion, 2015). Nevertheless, less attention has been paid to the mining of the pros and cons of technologies. This study performs a preliminary investigation on automatically identifying technologies and their pros/cons from computer science papers (henceforth referred to as pros/cons identification). We frame pros/cons identification as the well-known NLP task of targeted sentiment analysis (Jiang et al., 2011) and conduct an annotation study. Futhermore, we build a neural baseline model to identify the challenges of pros/cons identification task. The annotation study indicates that the pros/cons identification task can be reasonably framed as the task of targeted sentiment analysis. The experimental results of automatic extraction show that pros/cons identification is difficult mainly owing to the requirement of domainspecific knowledge. The annotated dataset is made Introduction The number of scientific publications has been rapidly increasing. Johnson et al. (2018) showed that over 3 million rese"
W19-2605,N16-1030,0,0.0352673,"ositive and the other labeled them as Neutral. To judge the sentiment attributes correctly, one required the domain knowledge of coreference resolution that directly capturing the competition among potential antecedent candidates is appropriate. 4.2 Model We formulate the automatic extraction task as a BIO sequence tagging task. Specifically, given a sentence, the model tags each word as one of {O, B-POS, I-POS, B-NEG, I-NEG, B-NEU, I-NEU}, where a combination of BI tags represents a Positive (POS), Negative (NEG), and Neutral (NEU) technical term span. We use the BiLSTM-CRF model proposed by Lample et al. (2016) which was originally designed for the task of named entity recognition.5 Regarding word embedding, we use word2vec (Mikolov et al., 2013) embeddings trained on ACL Anthology Corpus (Aizawa et al., 2018) (henceforth, CL), and ELMo (Peters et al., 2018) embeddings trained on 1 Billion Word Benchmark (henceforth, EL). (4) In essence, ranking models directly capture ::::::::::::: during training the competition among potential antecedent candidates, instead of considering them independently. (D08-1069) We found a large number of cases where sentences took the form of concession. In Example (5), o"
W19-4433,Q13-1032,0,0.736654,"onse (a subsequence of words) that causes the response to be awarded points in the analytic score. In Figure 1, for example, the phrase Western culture is identified as a justification for criterion A, whereas the phrase Conflicts of interest is a justification for criterion B. Justification cues not only explain the model’s prediction but also help students learn how to improve their responses. One crucial issue in addressing such analytical assessment tasks is the lack of data. The datasets that are presently available for SAS research (Mohler et al., 2011; ASAP-SAS; Dzikovska et al., 2013; Basu et al., 2013, etc.) are all accompanied by annotations of holistic scores alone. In this study, we developed a new dataset with annotated analytic scores and justification cues as well as holistic scores. The dataset contains 2,100 sample student responses for each of six distinct reading comprehension test prompts, collected from commercial achievement tests for Japanese high school students. The dataset is publicly available for research purposes.1 SAS requires content-based, prompt-specific rubrics, which means that one needs to create a labeled dataset to train a model for each given prompt. This natu"
W19-4433,N12-1021,0,0.0239498,"rd issue is the comparison between the “NN base” model and the “+just.” model trained on both the analytic score and justification signals. We can observe that using justification signals as well as analytic score signals for training further boosts the performance at holistic score prediction, particularly when the training set is smaller. 6 Related Work Short answer scoring Previous research on SAS has solely focused on holistic score prediction. We believe that this is partly because, to date, the publicly available datasets for SAS have contained holistic scores only (Mohler et al., 2011; Dzikovska et al., 2012, 2013; ASAP-SAS) . To the best of our knowledge, our dataset is the first to provide both annotated analytic scores and their justification cues. Analytical assessment Analytical assessment has been studied in the context of automated essay scoring (Persing and Ng, 2016, 2015, etc.). The analytic criteria adopted in essay scoring tend to be more general, e.g., organization, clarity, and argument strength. In contrast, analytic criteria in SAS are typically prompt-specific as in our examples in Figure 1. Thus, the analytic criteria need to be learned by the model separately for each individual"
W19-4433,I17-1004,0,0.0527303,"Missing"
W19-4433,W15-0610,0,0.0184508,"rompt is a text which either elicits recall of information that was given in a reading passage, asks for a summary of a reading passage, or asks students to draw on knowledge they already have. The task is to assess the responses based on context and writing quality, in accordance with the criteria prespecified for each assessment by a scoring rubric. Automation of this process has the potential to significantly reduce the workload of human raters and has attracted a considerable amount of attention from both academia and industry (Riordan et al., 2017; Zhao et al., 2017; Sultan et al., 2016; Heilman and Madnani, 2015; Pulman and Sukkarieh, 2005; Leacock and Chodorow, 2003; Vigilante, 1999, etc.). It should be emphasized that, in admissions tests and other tests, such as writing proficiency tests, large groups of students receive and respond to the exact same set of problems, for which ∗ Current affiliation: Future moto.tomoya.mh7@is.naist.jp Corporation, mizu316 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 316–325 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics spelling). Assessing student responses by analyt"
W19-4433,I17-2002,0,0.0135897,"evaluation phase, the predicted scores are re-scaled back to their original range. Training with holistic scores. To train the whole network on holistic score annotations, we minimize the MSE calculated with gold and predicted holistic scores (Equation 1) as follows: N 1 X (n) (n) (shol − sˆhol )2 , N where N is the number of training instances, and (n) (n) shol and sˆhol are the predicted score and gold score, respectively. Supervised attention. We further train the attention mechanism for each criterion in a supervised manner, called supervised attention (Mi et al., 2016; Liu et al., 2016; Kamigaito et al., 2017). In supervised attention, attention is learned from the difference between the span where the attention is focused and the given gold signal of a justification cue. Following a previous study by Liu et al. (2016), we add a soft constraint method to obtain the following objective function: att is defined as follows: An attention mechanism fm T X αm,t ht (4) t=1 An attention value αm,t denotes the importance weight, which represents relative importance of the t-th word for predicting analytic score sm . 4.4 Justification identification method N X 1 X (n) (n) { (si − sˆi )2 N The attention mecha"
W19-4433,N06-1023,0,0.0363905,"only have analytic scores annotated to a small set of responses. Thus we can train a model on these annotations for each task. We consider this scenario as our baseline scenario. We refer to the model for this scenario as “NN base.” SVR Baseline We also implemented another simpler baseline model based on the support vector regression model (SVR) following Sakaguchi et al. (2015) to provide sparse feature-based baseline results. We adopted the feature set proposed by Sakaguchi et al. (2015), which includes word 1gram, word 2-gram, and predicate-argument structure features2 . We used KNP 4.16 (Kawahara and Kurohashi, 2006) to extract Japanese predicateargument structure features. 5.2 SVR NN base +just. +hol. Q2 Q3 Q4 Q5 Q6 Analytic/Justification: 25 .60 .20 .54 .58 .45 .62 .19 .58 .64 .47 .73 .29 .64 .74 .53 .84 .48 .72 .86 .75 Analytic/Justification: 50 .73 .29 .64 .68 .56 .78 .29 .68 .72 .59 .85 .38 .71 .78 .64 .93 .59 .71 .87 .79 Analytic/Justification: 100 .80 .35 .72 .73 .66 .84 .40 .74 .79 .67 .88 .52 .76 .81 .72 .93 .67 .81 .87 .82 Analytic/Justification: 200 .87 .44 .77 .78 .71 .91 .57 .78 .83 .76 .92 .65 .80 .84 .78 .94 .72 .82 .88 .83 .94 .76 .84 .82 .90 Scenario (ii): (i) + justification signals In a"
W19-4433,N15-1111,0,0.022321,"nymous URL once the paper is accepted. We chose the same hyperparameters and training settings as in Riordan et al. (2017)’s holistic scoring model. Scenario (i): Basic setting (analytic score signals only) The first scenario assumes that we only have analytic scores annotated to a small set of responses. Thus we can train a model on these annotations for each task. We consider this scenario as our baseline scenario. We refer to the model for this scenario as “NN base.” SVR Baseline We also implemented another simpler baseline model based on the support vector regression model (SVR) following Sakaguchi et al. (2015) to provide sparse feature-based baseline results. We adopted the feature set proposed by Sakaguchi et al. (2015), which includes word 1gram, word 2-gram, and predicate-argument structure features2 . We used KNP 4.16 (Kawahara and Kurohashi, 2006) to extract Japanese predicateargument structure features. 5.2 SVR NN base +just. +hol. Q2 Q3 Q4 Q5 Q6 Analytic/Justification: 25 .60 .20 .54 .58 .45 .62 .19 .58 .64 .47 .73 .29 .64 .74 .53 .84 .48 .72 .86 .75 Analytic/Justification: 50 .73 .29 .64 .68 .56 .78 .29 .68 .72 .59 .85 .38 .71 .78 .64 .93 .59 .71 .87 .79 Analytic/Justification: 100 .80 .35"
W19-4433,W04-3250,0,0.0171153,", which was improved by the extra holistic score signals. A more in-depth analysis of this matter is needed, but our findings do raise the nontrivial question of which architecture is optimal to maximize the gain that results from including justification identification from holistic score signals. Summary These results suggest that our scenarios (ii) and (iii) are both worth considering in order to improve the performance of analytic score prediction. Note that the gains achieved by incorporating scenarios (ii) and (iii) are both statistically significant (p &lt; 0.01 by a paired bootstrap test (Koehn, 2004)). Specifically, the performance of the “+just.” model was significantly better than that of the “NN base” model for all the prompts. The performance of the “+hol.” model was also significantly better than that of the “+just.” model for all the prompts. Additional analysis Another interesting question deals with how well the accuracy of analytic score prediction correlates with the accuracy of justification identification. We observed that the neural baseline models showed strong performance for justification identification. These results raise the simple question of whether the sys3 Since our"
W19-4433,W04-3230,0,0.227915,"Missing"
W19-4433,N16-1123,0,0.0257892,"rompt. Typically, a prompt is a text which either elicits recall of information that was given in a reading passage, asks for a summary of a reading passage, or asks students to draw on knowledge they already have. The task is to assess the responses based on context and writing quality, in accordance with the criteria prespecified for each assessment by a scoring rubric. Automation of this process has the potential to significantly reduce the workload of human raters and has attracted a considerable amount of attention from both academia and industry (Riordan et al., 2017; Zhao et al., 2017; Sultan et al., 2016; Heilman and Madnani, 2015; Pulman and Sukkarieh, 2005; Leacock and Chodorow, 2003; Vigilante, 1999, etc.). It should be emphasized that, in admissions tests and other tests, such as writing proficiency tests, large groups of students receive and respond to the exact same set of problems, for which ∗ Current affiliation: Future moto.tomoya.mh7@is.naist.jp Corporation, mizu316 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 316–325 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics spelling). Assessing"
W19-4433,C16-1291,0,0.0253373,"the scale. In the evaluation phase, the predicted scores are re-scaled back to their original range. Training with holistic scores. To train the whole network on holistic score annotations, we minimize the MSE calculated with gold and predicted holistic scores (Equation 1) as follows: N 1 X (n) (n) (shol − sˆhol )2 , N where N is the number of training instances, and (n) (n) shol and sˆhol are the predicted score and gold score, respectively. Supervised attention. We further train the attention mechanism for each criterion in a supervised manner, called supervised attention (Mi et al., 2016; Liu et al., 2016; Kamigaito et al., 2017). In supervised attention, attention is learned from the difference between the span where the attention is focused and the given gold signal of a justification cue. Following a previous study by Liu et al. (2016), we add a soft constraint method to obtain the following objective function: att is defined as follows: An attention mechanism fm T X αm,t ht (4) t=1 An attention value αm,t denotes the importance weight, which represents relative importance of the t-th word for predicting analytic score sm . 4.4 Justification identification method N X 1 X (n) (n) { (si − sˆi"
W19-4433,D16-1249,0,0.0661177,"Missing"
W19-4433,P11-1076,0,0.333807,"justification cue, we refer to the segment of the response (a subsequence of words) that causes the response to be awarded points in the analytic score. In Figure 1, for example, the phrase Western culture is identified as a justification for criterion A, whereas the phrase Conflicts of interest is a justification for criterion B. Justification cues not only explain the model’s prediction but also help students learn how to improve their responses. One crucial issue in addressing such analytical assessment tasks is the lack of data. The datasets that are presently available for SAS research (Mohler et al., 2011; ASAP-SAS; Dzikovska et al., 2013; Basu et al., 2013, etc.) are all accompanied by annotations of holistic scores alone. In this study, we developed a new dataset with annotated analytic scores and justification cues as well as holistic scores. The dataset contains 2,100 sample student responses for each of six distinct reading comprehension test prompts, collected from commercial achievement tests for Japanese high school students. The dataset is publicly available for research purposes.1 SAS requires content-based, prompt-specific rubrics, which means that one needs to create a labeled data"
W19-4433,P15-1053,0,0.0747804,"Missing"
W19-4433,P16-1205,0,0.0157497,"stic score prediction, particularly when the training set is smaller. 6 Related Work Short answer scoring Previous research on SAS has solely focused on holistic score prediction. We believe that this is partly because, to date, the publicly available datasets for SAS have contained holistic scores only (Mohler et al., 2011; Dzikovska et al., 2012, 2013; ASAP-SAS) . To the best of our knowledge, our dataset is the first to provide both annotated analytic scores and their justification cues. Analytical assessment Analytical assessment has been studied in the context of automated essay scoring (Persing and Ng, 2016, 2015, etc.). The analytic criteria adopted in essay scoring tend to be more general, e.g., organization, clarity, and argument strength. In contrast, analytic criteria in SAS are typically prompt-specific as in our examples in Figure 1. Thus, the analytic criteria need to be learned by the model separately for each individual prompt. It is an interesting open question whether the insights gained from essay scoring research can be applicable to analytic SAS research. Interpretability of neural models In recent years, the interpretability of neural models has received widespread attention. Som"
W19-4433,W05-0202,0,0.0689774,"r elicits recall of information that was given in a reading passage, asks for a summary of a reading passage, or asks students to draw on knowledge they already have. The task is to assess the responses based on context and writing quality, in accordance with the criteria prespecified for each assessment by a scoring rubric. Automation of this process has the potential to significantly reduce the workload of human raters and has attracted a considerable amount of attention from both academia and industry (Riordan et al., 2017; Zhao et al., 2017; Sultan et al., 2016; Heilman and Madnani, 2015; Pulman and Sukkarieh, 2005; Leacock and Chodorow, 2003; Vigilante, 1999, etc.). It should be emphasized that, in admissions tests and other tests, such as writing proficiency tests, large groups of students receive and respond to the exact same set of problems, for which ∗ Current affiliation: Future moto.tomoya.mh7@is.naist.jp Corporation, mizu316 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 316–325 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics spelling). Assessing student responses by analytic scores as well as holisti"
W19-4433,W17-5017,0,0.366746,"free-text student responses to a given prompt. Typically, a prompt is a text which either elicits recall of information that was given in a reading passage, asks for a summary of a reading passage, or asks students to draw on knowledge they already have. The task is to assess the responses based on context and writing quality, in accordance with the criteria prespecified for each assessment by a scoring rubric. Automation of this process has the potential to significantly reduce the workload of human raters and has attracted a considerable amount of attention from both academia and industry (Riordan et al., 2017; Zhao et al., 2017; Sultan et al., 2016; Heilman and Madnani, 2015; Pulman and Sukkarieh, 2005; Leacock and Chodorow, 2003; Vigilante, 1999, etc.). It should be emphasized that, in admissions tests and other tests, such as writing proficiency tests, large groups of students receive and respond to the exact same set of problems, for which ∗ Current affiliation: Future moto.tomoya.mh7@is.naist.jp Corporation, mizu316 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 316–325 c Florence, Italy, August 2, 2019. 2019 Association for Computa"
W19-4804,E17-2039,1,0.838305,"Missing"
W19-4804,D15-1075,0,0.650059,"lity in upward and downward reasoning. 1 Introduction Natural language inference (NLI), also known as recognizing textual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise P and a hypothesis H, the task is to determine whether the premise semantically entails the hypothesis (Dagan et al., 2013). A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences (Glockner et al., 2018; Naik et al., 2018; Poliak et al., 2018) and logical inferences (Bowman et al., 2015b; Evans et al., 2018). Concerning logical inferences, monotonicity reasoning (van Benthem, 1983; Icard and Moss, 2014), which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in (1) and (2). (1) a. All [ workers ↓] [joined for a French dinner ↑] b. All workers joined for a dinner c. All new workers joined for a French dinner (2) a. Not all [new workers ↑] joined for a dinner b. Not all workers joined for a dinner A context is upward entailing (shown by [... ↑]) that allows an inference"
W19-4804,W15-4002,0,0.598488,"lity in upward and downward reasoning. 1 Introduction Natural language inference (NLI), also known as recognizing textual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise P and a hypothesis H, the task is to determine whether the premise semantically entails the hypothesis (Dagan et al., 2013). A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences (Glockner et al., 2018; Naik et al., 2018; Poliak et al., 2018) and logical inferences (Bowman et al., 2015b; Evans et al., 2018). Concerning logical inferences, monotonicity reasoning (van Benthem, 1983; Icard and Moss, 2014), which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in (1) and (2). (1) a. All [ workers ↓] [joined for a French dinner ↑] b. All workers joined for a dinner c. All new workers joined for a French dinner (2) a. Not all [new workers ↑] joined for a dinner b. Not all workers joined for a dinner A context is upward entailing (shown by [... ↑]) that allows an inference"
W19-4804,P18-1224,0,0.0386824,"Missing"
W19-4804,P17-1152,0,0.0677033,"Missing"
W19-4804,N19-1423,0,0.0446964,"any, ever, at all, anything, anyone, anymore, anyhow, anywhere) in the hypothesis 4 Results and Discussion 4.1 Baselines To test the difficulty of our dataset, we checked the majority class label and the accuracies of five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; Wang et al., 2017), ESIM (Enhanced Sequential Inference Model; Chen et al., 2017), Decomposable Attention Model (Parikh et al., 2016), KIM (Knowledge-based Inference Model; Chen et al., 2018), and BERT (Bidirectional Encoder Representations from Transformers model; Devlin et al., 2019). Regarding BERT, we checked the performance of a model pretrained on Wikipedia and BookCorpus for language modeling and trained with SNLI and MultiNLI. For other models, we checked the performance trained with SNLI. In agreement with our dataset, we regarded the prediction label contradiction as non-entailment. Table 6 shows that the accuracies of all models were better on upward inferences, in accordance with the reported results of the GLUE leaderboard. The overall accuracy of each model was low. In particular, all models underperformed the majority baseline on downward inferences, despite"
W19-4804,P18-2103,0,0.0768747,"gmentation method showed that these models might be limited in their generalization ability in upward and downward reasoning. 1 Introduction Natural language inference (NLI), also known as recognizing textual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise P and a hypothesis H, the task is to determine whether the premise semantically entails the hypothesis (Dagan et al., 2013). A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences (Glockner et al., 2018; Naik et al., 2018; Poliak et al., 2018) and logical inferences (Bowman et al., 2015b; Evans et al., 2018). Concerning logical inferences, monotonicity reasoning (van Benthem, 1983; Icard and Moss, 2014), which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in (1) and (2). (1) a. All [ workers ↓] [joined for a French dinner ↑] b. All workers joined for a dinner c. All new workers joined for a French dinner (2) a. Not all [new workers ↑] joined for a dinner b. Not all workers joined f"
W19-4804,N18-2017,0,0.0596308,"Missing"
W19-4804,W18-5446,0,0.0755493,"Missing"
W19-4804,N18-1101,0,0.578916,"xamples by crowdsourcing and well-designed ones from linguistics publications. To enable the evaluation of 1 The dataset will be made publicly available at https://github.com/verypluming/MED. skills required for monotonicity reasoning, we annotate each example in our dataset with linguistic tags associated with monotonicity reasoning. We measure the performance of state-of-the-art NLI models on monotonicity reasoning and investigate their generalization ability in upward and downward reasoning (Section 4). The results show that all models trained with SNLI (Bowman et al., 2015b) and MultiNLI (Williams et al., 2018) perform worse on downward inferences than on upward inferences. In addition, we analyzed the performance of models trained with an automatically created monotonicity dataset, HELP (Yanaka et al., 2019). The analysis with monotonicity data augmentation shows that models tend to perform better in the same direction of monotonicity with the training set, while they perform worse in the opposite direction. This indicates that the accuracy on monotonicity reasoning depends solely on the majority direction in the training set, and models might lack the ability to capture the structural relations be"
W19-4804,S18-2015,0,0.053669,"ght for both tasks. 3.1.1 Premise collection As a resource, we use declarative sentences with more than five tokens from the Parallel Meaning Bank (PMB) (Abzianidze et al., 2017). The PMB contains syntactically correct sentences annotated with its syntactic category in Combinatory Categorial Grammar (CCG; Steedman, 2000) format, which is suitable for our purpose. To get a whole CCG derivation tree, we parse each sentence by the state-of-the-art CCG parser, depccg (Yoshikawa et al., 2017). Then, we add a polarity to every constituent of the CCG tree by the polarity computation system ccg2mono (Hu and Moss, 2018) and make the polarized part a blank field. We ran a trial rephrasing task on 500 examples and detected 17 expressions that were too general and thus difficult to rephrase them in a natural way (e.g., every one, no time). We removed examples involving such expressions. To collect more downward inference examples, we select examples involving determiners in Table 1 and downward operators in Table 2. As a result, we selected 1,485 examples involving expressions having arguments with upward monotonicity and 1,982 examples involving expressions having arguments with downward monotonicity. 3.1.2 Hy"
W19-4804,J16-4012,0,0.0701837,"oriented dataset We also collect monotonicity inference problems from previous manually curated datasets and linguistics publications. The motivation is that previous linguistics publications related to monotonicity reasoning are expected to contain welldesigned inference problems, which might be challenging problems for NLI models. We collected 1,184 examples from 11 linguistics publications (Barwise and Cooper, 1981; Hoeksema, 1986; Heim and Kratzer, 1998; Bonevac et al., 1999; Fyodorov et al., 2003; Geurts, 2003; Geurts and van der Slik, 2005; Zamansky et al., 2006; Szabolcsi et al., 2008; Winter, 2016; Denic et al., 2019). Regarding previous manually-curated datasets, we collected 93 examples for monotonicity reasoning from the GLUE diagnostic dataset, and 37 single-premise problems from FraCaS. Genre Crowd Paper Tags up up: cond up:rev: conj up:lex down:lex down:conj down:cond down up:rev up:disj up:lex: rev non down Premise There is a cat on the chair If you heard her speak English, you would take her for a native American Dogs and cats have all the good qualities of people without at the same time possessing their weaknesses He approached the boy reading a magazine Tom hardly ever liste"
W19-4804,2014.lilt-9.7,0,0.235188,"tual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise P and a hypothesis H, the task is to determine whether the premise semantically entails the hypothesis (Dagan et al., 2013). A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences (Glockner et al., 2018; Naik et al., 2018; Poliak et al., 2018) and logical inferences (Bowman et al., 2015b; Evans et al., 2018). Concerning logical inferences, monotonicity reasoning (van Benthem, 1983; Icard and Moss, 2014), which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in (1) and (2). (1) a. All [ workers ↓] [joined for a French dinner ↑] b. All workers joined for a dinner c. All new workers joined for a French dinner (2) a. Not all [new workers ↑] joined for a dinner b. Not all workers joined for a dinner A context is upward entailing (shown by [... ↑]) that allows an inference from (1a) to (1b), where French dinner is replaced by a more general concept dinner. On the other hand, a downward ent"
W19-4804,S19-1027,1,0.650772,"red for monotonicity reasoning, we annotate each example in our dataset with linguistic tags associated with monotonicity reasoning. We measure the performance of state-of-the-art NLI models on monotonicity reasoning and investigate their generalization ability in upward and downward reasoning (Section 4). The results show that all models trained with SNLI (Bowman et al., 2015b) and MultiNLI (Williams et al., 2018) perform worse on downward inferences than on upward inferences. In addition, we analyzed the performance of models trained with an automatically created monotonicity dataset, HELP (Yanaka et al., 2019). The analysis with monotonicity data augmentation shows that models tend to perform better in the same direction of monotonicity with the training set, while they perform worse in the opposite direction. This indicates that the accuracy on monotonicity reasoning depends solely on the majority direction in the training set, and models might lack the ability to capture the structural relations between monotonicity operators and their arguments. 2 Monotonicity As an example of a monotonicity inference, consider the example with the determiner every in (3); here the premise P entails the hypothes"
W19-4804,J88-2003,0,0.468338,"Missing"
W19-4804,C18-1198,0,0.039856,"d that these models might be limited in their generalization ability in upward and downward reasoning. 1 Introduction Natural language inference (NLI), also known as recognizing textual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise P and a hypothesis H, the task is to determine whether the premise semantically entails the hypothesis (Dagan et al., 2013). A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences (Glockner et al., 2018; Naik et al., 2018; Poliak et al., 2018) and logical inferences (Bowman et al., 2015b; Evans et al., 2018). Concerning logical inferences, monotonicity reasoning (van Benthem, 1983; Icard and Moss, 2014), which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in (1) and (2). (1) a. All [ workers ↓] [joined for a French dinner ↑] b. All workers joined for a dinner c. All new workers joined for a French dinner (2) a. Not all [new workers ↑] joined for a dinner b. Not all workers joined for a dinner A conte"
W19-4804,D16-1244,0,0.0952364,"Missing"
W19-4804,S18-2023,0,0.0661357,"Missing"
W19-4804,L18-1239,0,0.100411,"Missing"
W19-4804,P17-1026,0,0.0754829,"rated in (i). Figure 1 summarizes the overview of our human-oriented dataset creation. We used the crowdsourcing platform Figure Eight for both tasks. 3.1.1 Premise collection As a resource, we use declarative sentences with more than five tokens from the Parallel Meaning Bank (PMB) (Abzianidze et al., 2017). The PMB contains syntactically correct sentences annotated with its syntactic category in Combinatory Categorial Grammar (CCG; Steedman, 2000) format, which is suitable for our purpose. To get a whole CCG derivation tree, we parse each sentence by the state-of-the-art CCG parser, depccg (Yoshikawa et al., 2017). Then, we add a polarity to every constituent of the CCG tree by the polarity computation system ccg2mono (Hu and Moss, 2018) and make the polarized part a blank field. We ran a trial rephrasing task on 500 examples and detected 17 expressions that were too general and thus difficult to rephrase them in a natural way (e.g., every one, no time). We removed examples involving such expressions. To collect more downward inference examples, we select examples involving determiners in Table 1 and downward operators in Table 2. As a result, we selected 1,485 examples involving expressions having arg"
W19-8606,D15-1075,0,0.0754827,"Missing"
W19-8606,P12-2049,0,0.0255439,"ompletion The drafts in the revising stage may contain gaps denoted with &lt;*>. This setting is similar to text infilling (Zhu et al., 2019), masking-based language modeling (Fedus et al., 2018; Devlin et al., 2019), or the sentence completion task (Zweig et al., 2012), where the models are required to replace mask tokens with plausible words. Notably, SentRev differs from such tasks because systems for these tasks are expected to keep all the original tokens unchanged and only fill the &lt;*> token, with one or more other tokens. 8 Other corpora for revisions are available in the academic domain (Lee and Webster, 2012; Tan and Lee, 2014; Zhang et al., 2017). Thus, we provide a notable contribution by exploring the methods to create a dataset of revisions with a scalable crowdsourcing approach. By contrast, Zhang et al. (2017) recruited 60 students over 2 weeks and Lee and Webster (2012) collected data from a language learning project where over 300 tutors reviewed academic essays written by 4500 students. 7.2 Style transfer Conclusion and future work We proposed the SentRev task, where an incomplete, rough draft sentence is transformed into a more fluent, complete sentence in the academic writing domain. W"
W19-8606,P00-1037,0,0.405963,"ent perfomance good results in this task. in this task. proofreading final version Our model shows excellent performance in this task. Our model shows excellent performance in this task. Figure 1: Overview of the estimated process of writing a sentence Our model shows excellent performance in this task.. Writing activity consists of four stages: (i) drafting, (ii) revising, (iii) editing, and (iv) proofreading. thus a concern for both individual researchers and the academic community. Writing assistance technologies have been extensively studied in the natural language processing (NLP) field (Brill and Moore, 2000; Ng et al., 2014; Grangier and Auli, 2018). We focus on helping inexperienced authors in writing fluent grammatical sentences. Models developed for academic writing assistance using existing datasets can serve as a support system during the final stages by editing a nearly finished version of the draft. For example, Daudaravicius (2015) collects scientific papers before and after professional editing from publishing companies, and Dale and Kilgarriff (2011) extract already published papers that still contain errors and correct the errors to obtain target fragments of text. Introduction Academ"
W19-8606,N19-1333,0,0.0124588,"of posts . Style removal This experiment suggested that there were ambiguities in these pointing gestures and led to a redesign of the system . This experiment indicated the ambiguity found in the pointing gestures and caused a renewal of the system . Entailed sentence generation Figure 2 illustrates the effectiveness of different features class. There is different feature in figure 2 . Table 5: Examples of generated training dataset. baseline model for the SentRev task (henceforth, H-ND). have gained attention in the GEC and machine translation fields (Edunov et al., 2018; Xie et al., 2018; Lichtarge et al., 2019). We combined these two factors to train baseline models on noised final version sentences. 5.1.2 Enc-Dec noising and denoising model As an extension of the heuristic noising and denoising model, we changed the noising methods to better simulate the characteristics of X in S MITH than the heuristic rules in Section 5.1.1. As described in Section 4, the drafts tended to (i) contain grammatical errors, (ii) use stylistically improper wording, and (iii) lack certain words. We used the following three neural Encoder-Decoder (Enc-Dec) models to generate the synthetic draft sentences. First, we coll"
W19-8606,P17-1074,0,0.0218339,"from the set of accepted papers (reference sentences Y ). We randomly selected a set of 500 pairs from S MITH as the development set for analysis. Statistics 4.1 Table 3 shows the statistics of our S MITH dataset and a comparison with major datasets for building a writing assistance system (Napoles et al., 2017; Mizumoto et al., 2011; Daudaravicius, 2015). The size of our dataset (10k sentence pairs) is six times greater than that of JFLEG, which contains both Error type comparison To obtain the approximate distributions of error types between the source and target sentences, we used ERRANT (Bryant et al., 2017; Felice et al., 2016). Next, we compared them with three datasets: S MITH, AESW (the same domain as S MITH), and JFLEG (has a relatively close Levenshtein distance to S MITH). To calculate the er6 We corrected spelling errors using https: //github.com/barrust/pyspellchecker 43 Draft: the best models are very effective on the that they are far greater than human. condition OTHER Reference: The best models are very effective in the local context condition where they significantly outperform humans. Draft: Results show MARM tend to generate &lt;*> and very short responces. OTHER Reference: The resu"
W19-8606,I11-1017,0,0.024963,"compared with major GEC corpora. This finding implies that our dataset emulates more drastic rephrasing. 4 Analysis of the S MITH dataset In this section, we run extensive analyses on the sentences written by non-native workers (draft sentences X), and the original sentences extracted from the set of accepted papers (reference sentences Y ). We randomly selected a set of 500 pairs from S MITH as the development set for analysis. Statistics 4.1 Table 3 shows the statistics of our S MITH dataset and a comparison with major datasets for building a writing assistance system (Napoles et al., 2017; Mizumoto et al., 2011; Daudaravicius, 2015). The size of our dataset (10k sentence pairs) is six times greater than that of JFLEG, which contains both Error type comparison To obtain the approximate distributions of error types between the source and target sentences, we used ERRANT (Bryant et al., 2017; Felice et al., 2016). Next, we compared them with three datasets: S MITH, AESW (the same domain as S MITH), and JFLEG (has a relatively close Levenshtein distance to S MITH). To calculate the er6 We corrected spelling errors using https: //github.com/barrust/pyspellchecker 43 Draft: the best models are very effect"
W19-8606,D16-1228,0,0.0496988,"Missing"
W19-8606,W15-0607,0,0.315697,"stages: (i) drafting, (ii) revising, (iii) editing, and (iv) proofreading. thus a concern for both individual researchers and the academic community. Writing assistance technologies have been extensively studied in the natural language processing (NLP) field (Brill and Moore, 2000; Ng et al., 2014; Grangier and Auli, 2018). We focus on helping inexperienced authors in writing fluent grammatical sentences. Models developed for academic writing assistance using existing datasets can serve as a support system during the final stages by editing a nearly finished version of the draft. For example, Daudaravicius (2015) collects scientific papers before and after professional editing from publishing companies, and Dale and Kilgarriff (2011) extract already published papers that still contain errors and correct the errors to obtain target fragments of text. Introduction Academic writing can be a daunting task, even for experienced writers with a native or near-native command of English. Inexperienced, non-native speakers find themselves in an even more difficult situation—in addition to grammatical or spelling errors, their sentences may lack fluidity, have an awkward style, contain collocation errors, or hav"
W19-8606,E17-2037,0,0.0204427,"ferences between them compared with major GEC corpora. This finding implies that our dataset emulates more drastic rephrasing. 4 Analysis of the S MITH dataset In this section, we run extensive analyses on the sentences written by non-native workers (draft sentences X), and the original sentences extracted from the set of accepted papers (reference sentences Y ). We randomly selected a set of 500 pairs from S MITH as the development set for analysis. Statistics 4.1 Table 3 shows the statistics of our S MITH dataset and a comparison with major datasets for building a writing assistance system (Napoles et al., 2017; Mizumoto et al., 2011; Daudaravicius, 2015). The size of our dataset (10k sentence pairs) is six times greater than that of JFLEG, which contains both Error type comparison To obtain the approximate distributions of error types between the source and target sentences, we used ERRANT (Bryant et al., 2017; Felice et al., 2016). Next, we compared them with three datasets: S MITH, AESW (the same domain as S MITH), and JFLEG (has a relatively close Levenshtein distance to S MITH). To calculate the er6 We corrected spelling errors using https: //github.com/barrust/pyspellchecker 43 Draft: the best"
W19-8606,N19-1423,0,0.00930276,"ors, and the AESW dataset contains a collection of text extracts from published journal articles before and after proofreading. Rather than adding finishing touches to almost completed sentences, our task is to convert unfinished, rough drafts into complete sentences. In addition, these studies tackled the task of the identification of errors while SentRev goes further by rewriting the drafts. 7.4 Text completion The drafts in the revising stage may contain gaps denoted with &lt;*>. This setting is similar to text infilling (Zhu et al., 2019), masking-based language modeling (Fedus et al., 2018; Devlin et al., 2019), or the sentence completion task (Zweig et al., 2012), where the models are required to replace mask tokens with plausible words. Notably, SentRev differs from such tasks because systems for these tasks are expected to keep all the original tokens unchanged and only fill the &lt;*> token, with one or more other tokens. 8 Other corpora for revisions are available in the academic domain (Lee and Webster, 2012; Tan and Lee, 2014; Zhang et al., 2017). Thus, we provide a notable contribution by exploring the methods to create a dataset of revisions with a scalable crowdsourcing approach. By contrast,"
W19-8606,D18-1045,0,0.0205145,"of sentiments express in these sequence of posts . Style removal This experiment suggested that there were ambiguities in these pointing gestures and led to a redesign of the system . This experiment indicated the ambiguity found in the pointing gestures and caused a renewal of the system . Entailed sentence generation Figure 2 illustrates the effectiveness of different features class. There is different feature in figure 2 . Table 5: Examples of generated training dataset. baseline model for the SentRev task (henceforth, H-ND). have gained attention in the GEC and machine translation fields (Edunov et al., 2018; Xie et al., 2018; Lichtarge et al., 2019). We combined these two factors to train baseline models on noised final version sentences. 5.1.2 Enc-Dec noising and denoising model As an extension of the heuristic noising and denoising model, we changed the noising methods to better simulate the characteristics of X in S MITH than the heuristic rules in Section 5.1.1. As described in Section 4, the drafts tended to (i) contain grammatical errors, (ii) use stylistically improper wording, and (iii) lack certain words. We used the following three neural Encoder-Decoder (Enc-Dec) models to generate th"
W19-8606,N19-4009,0,0.0335665,"Missing"
W19-8606,P18-1080,0,0.030559,"th ERRANT. In addition, to handle the lexical and compositional diversity of valid revisions, we used BERT-score (Zhang et al., 2019), a contextualized embedding-based evaluation metric. Furthermore, we used two reference-less evaluation metrics: grammaticality score (Napoles et al., 2016) and PPL. Grammaticality was scored as 10 https://github.com/languagetool-org/ languagetool/releases/tag/v3.2 47 7 7.1 Related work 7.3 Style transfer is the task of rephrasing the text to conform to specific stylistic properties while preserving the text’s original semantic content (Logeswaran et al., 2018; Prabhumoye et al., 2018). From the perspective of automatic academic writing assistance, the assistance systems are required to convert nonacademic-style drafts into academic-style drafts. This type of transfer is regarded as a subproblem in the revising stage of the writing process. Writing assistance in the academic domain Several shared tasks for assisting academic writing have been organized. The Helping Our Own (HOO) 2011 Pilot Shared Task (Dale and Kilgarriff, 2011) aimed to promote the development of tools and techniques to assist authors in writing, with a specific focus on writing within the NLP community. T"
W19-8606,C16-1079,0,0.0192342,"pted papers (reference sentences Y ). We randomly selected a set of 500 pairs from S MITH as the development set for analysis. Statistics 4.1 Table 3 shows the statistics of our S MITH dataset and a comparison with major datasets for building a writing assistance system (Napoles et al., 2017; Mizumoto et al., 2011; Daudaravicius, 2015). The size of our dataset (10k sentence pairs) is six times greater than that of JFLEG, which contains both Error type comparison To obtain the approximate distributions of error types between the source and target sentences, we used ERRANT (Bryant et al., 2017; Felice et al., 2016). Next, we compared them with three datasets: S MITH, AESW (the same domain as S MITH), and JFLEG (has a relatively close Levenshtein distance to S MITH). To calculate the er6 We corrected spelling errors using https: //github.com/barrust/pyspellchecker 43 Draft: the best models are very effective on the that they are far greater than human. condition OTHER Reference: The best models are very effective in the local context condition where they significantly outperform humans. Draft: Results show MARM tend to generate &lt;*> and very short responces. OTHER Reference: The results indicate that MARM"
W19-8606,Q16-1013,0,0.0598999,"Missing"
W19-8606,N18-1025,0,0.0234959,". in this task. proofreading final version Our model shows excellent performance in this task. Our model shows excellent performance in this task. Figure 1: Overview of the estimated process of writing a sentence Our model shows excellent performance in this task.. Writing activity consists of four stages: (i) drafting, (ii) revising, (iii) editing, and (iv) proofreading. thus a concern for both individual researchers and the academic community. Writing assistance technologies have been extensively studied in the natural language processing (NLP) field (Brill and Moore, 2000; Ng et al., 2014; Grangier and Auli, 2018). We focus on helping inexperienced authors in writing fluent grammatical sentences. Models developed for academic writing assistance using existing datasets can serve as a support system during the final stages by editing a nearly finished version of the draft. For example, Daudaravicius (2015) collects scientific papers before and after professional editing from publishing companies, and Dale and Kilgarriff (2011) extract already published papers that still contain errors and correct the errors to obtain target fragments of text. Introduction Academic writing can be a daunting task, even for"
W19-8606,P17-4015,0,0.0217931,"Missing"
W19-8606,P14-2066,0,0.0171537,"the revising stage may contain gaps denoted with &lt;*>. This setting is similar to text infilling (Zhu et al., 2019), masking-based language modeling (Fedus et al., 2018; Devlin et al., 2019), or the sentence completion task (Zweig et al., 2012), where the models are required to replace mask tokens with plausible words. Notably, SentRev differs from such tasks because systems for these tasks are expected to keep all the original tokens unchanged and only fill the &lt;*> token, with one or more other tokens. 8 Other corpora for revisions are available in the academic domain (Lee and Webster, 2012; Tan and Lee, 2014; Zhang et al., 2017). Thus, we provide a notable contribution by exploring the methods to create a dataset of revisions with a scalable crowdsourcing approach. By contrast, Zhang et al. (2017) recruited 60 students over 2 weeks and Lee and Webster (2012) collected data from a language learning project where over 300 tutors reviewed academic essays written by 4500 students. 7.2 Style transfer Conclusion and future work We proposed the SentRev task, where an incomplete, rough draft sentence is transformed into a more fluent, complete sentence in the academic writing domain. We created the S MIT"
W19-8606,P18-1042,0,0.0595374,"Missing"
W19-8606,N18-1101,0,0.0606623,"Missing"
W19-8606,N18-1057,0,0.0648811,"in these sequence of posts . Style removal This experiment suggested that there were ambiguities in these pointing gestures and led to a redesign of the system . This experiment indicated the ambiguity found in the pointing gestures and caused a renewal of the system . Entailed sentence generation Figure 2 illustrates the effectiveness of different features class. There is different feature in figure 2 . Table 5: Examples of generated training dataset. baseline model for the SentRev task (henceforth, H-ND). have gained attention in the GEC and machine translation fields (Edunov et al., 2018; Xie et al., 2018; Lichtarge et al., 2019). We combined these two factors to train baseline models on noised final version sentences. 5.1.2 Enc-Dec noising and denoising model As an extension of the heuristic noising and denoising model, we changed the noising methods to better simulate the characteristics of X in S MITH than the heuristic rules in Section 5.1.1. As described in Section 4, the drafts tended to (i) contain grammatical errors, (ii) use stylistically improper wording, and (iii) lack certain words. We used the following three neural Encoder-Decoder (Enc-Dec) models to generate the synthetic draft"
W19-8606,N16-1042,0,0.0139549,"t. We believe that this task can increase the effectiveness of the process of academic writing. In future work, we plan to improve the information gap-filling aspect of revision by considering the surrounding context of target sentences. In addition, to develop a more holistic writing assistance tool, we plan to extend our system to be able to suggest diverse correction candidates, provide interactive assistance, and integrate translation systems. Grammatical error correction GEC is the task of correcting errors in text such as spelling, punctuation, grammar, and word choice (Ng et al., 2014; Yuan and Briscoe, 2016). GEC falls within the editing and proofreading phases of the writing process, while SentRev subsumes GEC and a broader range of text generation (e.g., increasing the fluency of the sentence and complementing missing information). Napoles et al. (2017) and Sakaguchi et al. (2016) explored fluency edits to correct grammatical errors and to make a text more “native sounding.” Although this direction is similar to SentRev, our task used sentences that required many more corrections. 9 Acknowledgements We thank the Tohoku NLP laboratory members who provided us with their valuable advice. We are gr"
W19-8606,P17-1144,0,0.0253616,"may contain gaps denoted with &lt;*>. This setting is similar to text infilling (Zhu et al., 2019), masking-based language modeling (Fedus et al., 2018; Devlin et al., 2019), or the sentence completion task (Zweig et al., 2012), where the models are required to replace mask tokens with plausible words. Notably, SentRev differs from such tasks because systems for these tasks are expected to keep all the original tokens unchanged and only fill the &lt;*> token, with one or more other tokens. 8 Other corpora for revisions are available in the academic domain (Lee and Webster, 2012; Tan and Lee, 2014; Zhang et al., 2017). Thus, we provide a notable contribution by exploring the methods to create a dataset of revisions with a scalable crowdsourcing approach. By contrast, Zhang et al. (2017) recruited 60 students over 2 weeks and Lee and Webster (2012) collected data from a language learning project where over 300 tutors reviewed academic essays written by 4500 students. 7.2 Style transfer Conclusion and future work We proposed the SentRev task, where an incomplete, rough draft sentence is transformed into a more fluent, complete sentence in the academic writing domain. We created the S MITH dataset with crowds"
W19-8606,N19-1014,0,0.0516118,"eline model by error types. The results are shown in Figure 7. Overall, typical grammatical errors such as noun number errors or orthographic errors are well corrected, but the model struggles with drastic revisions (“OTHER” type errors). 0.3 0.2 0.1 AD J H OR T SP EL L B P CT PU N VE R M PR E :N U NO UN DE T NO UN OT HE R 0 Figure 7: Performance of the ED-ND baseline model on top 10 most error types in SMITH. sentence generation model caused a lack of information. 5.1.3 GEC model The GEC task is closely related to SentRev. We examined the performance of the current state-ofthe-art GEC model (Zhao et al., 2019) in our task. We applied spelling correction before evaluation following Zhao et al. (2019). 5.2 Evaluation metrics The SentRev task has a very diverse space of valid revisions to a given context, which is challenging to evaluate. As one solution, we evaluated the performance from multiple aspects by using various reference and reference-less evaluation metrics. We used BLEU, ROUGE-L, and F0.5 score, which are widely used metrics in related tasks (machine translation, style-transfer, GEC). We used nlgeval (Sharma et al., 2017) to compute the BLEU and ROUGE-L scores and calculated F0.5 scores w"
W19-8606,P12-1063,0,0.0329555,"extracts from published journal articles before and after proofreading. Rather than adding finishing touches to almost completed sentences, our task is to convert unfinished, rough drafts into complete sentences. In addition, these studies tackled the task of the identification of errors while SentRev goes further by rewriting the drafts. 7.4 Text completion The drafts in the revising stage may contain gaps denoted with &lt;*>. This setting is similar to text infilling (Zhu et al., 2019), masking-based language modeling (Fedus et al., 2018; Devlin et al., 2019), or the sentence completion task (Zweig et al., 2012), where the models are required to replace mask tokens with plausible words. Notably, SentRev differs from such tasks because systems for these tasks are expected to keep all the original tokens unchanged and only fill the &lt;*> token, with one or more other tokens. 8 Other corpora for revisions are available in the academic domain (Lee and Webster, 2012; Tan and Lee, 2014; Zhang et al., 2017). Thus, we provide a notable contribution by exploring the methods to create a dataset of revisions with a scalable crowdsourcing approach. By contrast, Zhang et al. (2017) recruited 60 students over 2 week"
W19-8606,W14-1701,0,\N,Missing
W19-8606,W11-2838,0,\N,Missing
W19-8641,P18-1015,0,0.106173,"Missing"
W19-8641,N18-1154,0,0.0117554,"s. In this example, ‘電動車’(Electric cars) and ‘全’(all) are represented by red letters and are not included in the 24character headline. These tokens cannot be evaluated by 24-character headlines. The blue tokens are not included in 9- and 13-character headlines. These tokens should not be included in shorter headlines. allowed because of limitations in the space where the headline appears. The technology of automatic headline generation has the potential to contribute greatly to this domain, and the problems of news headline generation have motivated a wide range of studies (Wang et al., 2018; Chen et al., 2018; Kiyono et al., 2018; Zhou et al., 2018; Cao et al., 2018; Wang et al., 2019). Table 1 shows sample headlines in three different lengths written by professional editors of a media company for the same news article: The length of the first headline for the digital media is restricted to 10 characters, the second to 13 characIntroduction The news media publish newspapers in print form and in electronic form. In the electric form, articles might be read on various types of devices using any application; thus, news media companies have an increasing need to produce multiple headlines for the same"
W19-8641,N16-1012,0,0.0569508,"racters long covered the content of those generated to be 13 characters long. These facts suggest that we should explore in further research a method not only trained by generic supervision data (print headlines) but also tuned for the desired length. 5 Related Work Rush et al. (2015) created the first approach to neural abstractive summarization. They generated a headline from the first sentence of a news article in the AEG (Napoles et al., 2012), which contains an enormous number of pairs of headlines and articles. After their study, a number of researchers addressed this task: For example, Chopra et al. (2016) used the encoderdecoder framework (Sutskever et al., 2014; Bahdanau et al., 2015) and Nallapati et al. (2016) incorporated additional features into the model, such as parts-of-speech tags and named entities. Suzuki and Nagata (2017) proposed word-frequency estimation to reduce the repeated phrases being generated. Zhou et al. (2017) proposed a gating mechanism (sGate) to ensure that important information is selected at each decoding step. Furthermore, attempts to control the output How Do Length Control Mechanisms Work? We wondered whether a method that could control the output length would p"
W19-8641,P19-1099,1,0.834743,"lengths could be controlled by embedding special tokens given to an input sequence. These two studies used DUC 2004 (Over et al., 2007), which comprises only 75byte summaries, to evaluate the outputs in multiple lengths. Liu et al. (2018) also proposed a method for controlling the number of output tokens in the ConvS2S model. In Transformer (Vaswani et al., 2017), Takase and Okazaki (2019) proposed two length control methods by extending positional embedding. They additionally evaluated the system outputs by using the reconstructed test set of AEG which consists of the fixed length headlines. Makino et al. (2019) proposed a global optimization method under a length constraint. Sun et al. (2019) examined how to compare summarizers by considering the length bias of generated summaries in the test set that includes various length summaries. However, no previous work built a dataset for evaluating headlines of multiple lengths or reported an in-depth perspective on this task during the process of new production in the real world. However, a single length reference that could appropriately evaluate multiple length summaries in multiple document summarization was reported (Shapira et al., 2018). In that stu"
W19-8641,K16-1028,0,0.470615,"th International Conference on Natural Language Generation, pages 333–343, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics ters, and the third to 26 characters. From a practical perspective, headlines must be generated under a rigid length constraint. The first study to consider the length of system outputs in the encoder-decoder framework was Rush et al. (2015). This study controlled the length of an output sequence by reducing the score of the end-of-sentence token to −∞ until the method generated the desired number of words. Subsequently, Kikuchi et al. (2016) and Fan et al. (2018) proposed mechanisms for length control; however, these studies produced summaries of 30, 50, and 75 bytes, and the studies evaluated the summaries by using the reference summaries of a single length (approximately 75 bytes long) in DUC 20041 . In addition, Takase and Okazaki (2019) proposed the mechanism for length control and evaluated their method with part of the test set which is consisted by summaries satisfying some length constraints in Annotated English Gigaword corpus (AEG) (Napoles et al., 2012). Thus, some questions can be posed: (1) Can previous evaluation se"
W19-8641,W12-3018,0,0.135595,"Missing"
W19-8641,P82-1020,0,0.722983,"Missing"
W19-8641,D16-1140,1,0.940941,"dings of The 12th International Conference on Natural Language Generation, pages 333–343, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics ters, and the third to 26 characters. From a practical perspective, headlines must be generated under a rigid length constraint. The first study to consider the length of system outputs in the encoder-decoder framework was Rush et al. (2015). This study controlled the length of an output sequence by reducing the score of the end-of-sentence token to −∞ until the method generated the desired number of words. Subsequently, Kikuchi et al. (2016) and Fan et al. (2018) proposed mechanisms for length control; however, these studies produced summaries of 30, 50, and 75 bytes, and the studies evaluated the summaries by using the reference summaries of a single length (approximately 75 bytes long) in DUC 20041 . In addition, Takase and Okazaki (2019) proposed the mechanism for length control and evaluated their method with part of the test set which is consisted by summaries satisfying some length constraints in Annotated English Gigaword corpus (AEG) (Napoles et al., 2012). Thus, some questions can be posed: (1) Can previous evaluation se"
W19-8641,N19-4009,0,0.0236053,"ormance differences between the methods. Although Transformer + SP-token remained the clear winner, the ranking in ROUGE scores of the other methods are flipped. We also computed rank correlation coefficients (Kendall’s τ ) to assess the discrepancy in the ranking among the methods presented in Table 7 and Table 8. The last row of Table 8 reveals that the rank correlation is not perfect (lower than one) but moderate. We understand that τ is maintained high to some extent because of two reasons: (1) Most of the Implementation We employed OpenNMT8 (Klein et al., 2017) for Seq2Seq, and fairseq9 (Ott et al., 2019) for ConvS2S and Transformer. We extended the implementations to realize LenEmb, LenInit, and LC. We set the dimensions for the token and length embeddings to 512, those for hidden states to 512, and the beam width to 5. These parameters are common in all the models. Table 6 summarizes other parameters specific to each sequenceto-sequence model. We used Nesterov’s accelerated gradient method (NAG) (Sutskever et al., 2013) with a momentum of 0.99 in ConvS2S. In Transformer, we set the number of attention heads to 8, the dimensions for the feed-forward network to 2,048, Adam’s β to 0.98, the war"
W19-8641,W18-5410,1,0.851959,"rence on Natural Language Generation, pages 333–343, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics ters, and the third to 26 characters. From a practical perspective, headlines must be generated under a rigid length constraint. The first study to consider the length of system outputs in the encoder-decoder framework was Rush et al. (2015). This study controlled the length of an output sequence by reducing the score of the end-of-sentence token to −∞ until the method generated the desired number of words. Subsequently, Kikuchi et al. (2016) and Fan et al. (2018) proposed mechanisms for length control; however, these studies produced summaries of 30, 50, and 75 bytes, and the studies evaluated the summaries by using the reference summaries of a single length (approximately 75 bytes long) in DUC 20041 . In addition, Takase and Okazaki (2019) proposed the mechanism for length control and evaluated their method with part of the test set which is consisted by summaries satisfying some length constraints in Annotated English Gigaword corpus (AEG) (Napoles et al., 2012). Thus, some questions can be posed: (1) Can previous evaluation settings adequately eval"
W19-8641,P17-4012,0,0.012013,"adlines. This evaluation setup reduced the performance differences between the methods. Although Transformer + SP-token remained the clear winner, the ranking in ROUGE scores of the other methods are flipped. We also computed rank correlation coefficients (Kendall’s τ ) to assess the discrepancy in the ranking among the methods presented in Table 7 and Table 8. The last row of Table 8 reveals that the rank correlation is not perfect (lower than one) but moderate. We understand that τ is maintained high to some extent because of two reasons: (1) Most of the Implementation We employed OpenNMT8 (Klein et al., 2017) for Seq2Seq, and fairseq9 (Ott et al., 2019) for ConvS2S and Transformer. We extended the implementations to realize LenEmb, LenInit, and LC. We set the dimensions for the token and length embeddings to 512, those for hidden states to 512, and the beam width to 5. These parameters are common in all the models. Table 6 summarizes other parameters specific to each sequenceto-sequence model. We used Nesterov’s accelerated gradient method (NAG) (Sutskever et al., 2013) with a momentum of 0.99 in ConvS2S. In Transformer, we set the number of attention heads to 8, the dimensions for the feed-forwar"
W19-8641,D15-1044,0,0.405379,"an array of devices. All devices and applications used for viewing articles have strict upper bounds regarding the number of characters ∗ This work was done at Retrieva, Inc. within Project. 333 Proceedings of The 12th International Conference on Natural Language Generation, pages 333–343, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics ters, and the third to 26 characters. From a practical perspective, headlines must be generated under a rigid length constraint. The first study to consider the length of system outputs in the encoder-decoder framework was Rush et al. (2015). This study controlled the length of an output sequence by reducing the score of the end-of-sentence token to −∞ until the method generated the desired number of words. Subsequently, Kikuchi et al. (2016) and Fan et al. (2018) proposed mechanisms for length control; however, these studies produced summaries of 30, 50, and 75 bytes, and the studies evaluated the summaries by using the reference summaries of a single length (approximately 75 bytes long) in DUC 20041 . In addition, Takase and Okazaki (2019) proposed the mechanism for length control and evaluated their method with part of the tes"
W19-8641,D18-2012,0,0.0327774,"Missing"
W19-8641,W04-3230,0,0.373641,"Missing"
W19-8641,D16-1248,0,0.0286382,"length. In the analysis, the existing methods could not take into account the word selection according to length constraint. We also found it difficult to evaluate methods to controlling output length, because headlines of different lengths are written based on different goals, and because the training data does not necessarily reflect the goal of the headlines of a specific length. In the future, we plan to explore an approach to adapt a model trained on print headlines to those which dedicated to a different length. length in neural abstractive summarization have been gradually increasing. Shi et al. (2016) reported that hidden states in recurrent neural networks in the encoder-decoder framework could implicitly model the length of the output sequences. Kikuchi et al. (2016) was the first to propose the idea of controlling the output length in the encoder-decoder framework. Their approach inserts length information for the output length into the decoder. Additionally, Fan et al. (2018) reported that output lengths could be controlled by embedding special tokens given to an input sequence. These two studies used DUC 2004 (Over et al., 2007), which comprises only 75byte summaries, to evaluate the"
W19-8641,W04-1013,0,0.0269053,"Missing"
W19-8641,W19-2303,0,0.0153687,"ese two studies used DUC 2004 (Over et al., 2007), which comprises only 75byte summaries, to evaluate the outputs in multiple lengths. Liu et al. (2018) also proposed a method for controlling the number of output tokens in the ConvS2S model. In Transformer (Vaswani et al., 2017), Takase and Okazaki (2019) proposed two length control methods by extending positional embedding. They additionally evaluated the system outputs by using the reconstructed test set of AEG which consists of the fixed length headlines. Makino et al. (2019) proposed a global optimization method under a length constraint. Sun et al. (2019) examined how to compare summarizers by considering the length bias of generated summaries in the test set that includes various length summaries. However, no previous work built a dataset for evaluating headlines of multiple lengths or reported an in-depth perspective on this task during the process of new production in the real world. However, a single length reference that could appropriately evaluate multiple length summaries in multiple document summarization was reported (Shapira et al., 2018). In that study, the authors confirmed the correlation coefficient of ROUGE scores between the s"
W19-8641,D18-1444,0,0.0233964,"in recurrent neural networks in the encoder-decoder framework could implicitly model the length of the output sequences. Kikuchi et al. (2016) was the first to propose the idea of controlling the output length in the encoder-decoder framework. Their approach inserts length information for the output length into the decoder. Additionally, Fan et al. (2018) reported that output lengths could be controlled by embedding special tokens given to an input sequence. These two studies used DUC 2004 (Over et al., 2007), which comprises only 75byte summaries, to evaluate the outputs in multiple lengths. Liu et al. (2018) also proposed a method for controlling the number of output tokens in the ConvS2S model. In Transformer (Vaswani et al., 2017), Takase and Okazaki (2019) proposed two length control methods by extending positional embedding. They additionally evaluated the system outputs by using the reconstructed test set of AEG which consists of the fixed length headlines. Makino et al. (2019) proposed a global optimization method under a length constraint. Sun et al. (2019) examined how to compare summarizers by considering the length bias of generated summaries in the test set that includes various length"
W19-8641,E17-2047,0,0.0123436,"or the desired length. 5 Related Work Rush et al. (2015) created the first approach to neural abstractive summarization. They generated a headline from the first sentence of a news article in the AEG (Napoles et al., 2012), which contains an enormous number of pairs of headlines and articles. After their study, a number of researchers addressed this task: For example, Chopra et al. (2016) used the encoderdecoder framework (Sutskever et al., 2014; Bahdanau et al., 2015) and Nallapati et al. (2016) incorporated additional features into the model, such as parts-of-speech tags and named entities. Suzuki and Nagata (2017) proposed word-frequency estimation to reduce the repeated phrases being generated. Zhou et al. (2017) proposed a gating mechanism (sGate) to ensure that important information is selected at each decoding step. Furthermore, attempts to control the output How Do Length Control Mechanisms Work? We wondered whether a method that could control the output length would produce similar headlines even for different lengths for the same news article. To confirm this suspicion, we reported ROUGE-1 recall scores in Figure 4 with three different configurations: (a) evaluating the first 13 characters of he"
W19-8641,N19-1401,1,0.906635,"t. The first study to consider the length of system outputs in the encoder-decoder framework was Rush et al. (2015). This study controlled the length of an output sequence by reducing the score of the end-of-sentence token to −∞ until the method generated the desired number of words. Subsequently, Kikuchi et al. (2016) and Fan et al. (2018) proposed mechanisms for length control; however, these studies produced summaries of 30, 50, and 75 bytes, and the studies evaluated the summaries by using the reference summaries of a single length (approximately 75 bytes long) in DUC 20041 . In addition, Takase and Okazaki (2019) proposed the mechanism for length control and evaluated their method with part of the test set which is consisted by summaries satisfying some length constraints in Annotated English Gigaword corpus (AEG) (Napoles et al., 2012). Thus, some questions can be posed: (1) Can previous evaluation settings adequately evaluate system outputs in headline generation task? (2) What type of problem should we solve in this task according to the target length? (3) How well do systems solve the problems? In this study, we present novel corpora to investigate these research questions. The contributions of th"
W19-8641,P19-1207,0,0.195072,"letters and are not included in the 24character headline. These tokens cannot be evaluated by 24-character headlines. The blue tokens are not included in 9- and 13-character headlines. These tokens should not be included in shorter headlines. allowed because of limitations in the space where the headline appears. The technology of automatic headline generation has the potential to contribute greatly to this domain, and the problems of news headline generation have motivated a wide range of studies (Wang et al., 2018; Chen et al., 2018; Kiyono et al., 2018; Zhou et al., 2018; Cao et al., 2018; Wang et al., 2019). Table 1 shows sample headlines in three different lengths written by professional editors of a media company for the same news article: The length of the first headline for the digital media is restricted to 10 characters, the second to 13 characIntroduction The news media publish newspapers in print form and in electronic form. In the electric form, articles might be read on various types of devices using any application; thus, news media companies have an increasing need to produce multiple headlines for the same news article based on what would be most appropriate and most compelling on a"
W19-8641,P17-1101,0,0.0581178,"mmarization. They generated a headline from the first sentence of a news article in the AEG (Napoles et al., 2012), which contains an enormous number of pairs of headlines and articles. After their study, a number of researchers addressed this task: For example, Chopra et al. (2016) used the encoderdecoder framework (Sutskever et al., 2014; Bahdanau et al., 2015) and Nallapati et al. (2016) incorporated additional features into the model, such as parts-of-speech tags and named entities. Suzuki and Nagata (2017) proposed word-frequency estimation to reduce the repeated phrases being generated. Zhou et al. (2017) proposed a gating mechanism (sGate) to ensure that important information is selected at each decoding step. Furthermore, attempts to control the output How Do Length Control Mechanisms Work? We wondered whether a method that could control the output length would produce similar headlines even for different lengths for the same news article. To confirm this suspicion, we reported ROUGE-1 recall scores in Figure 4 with three different configurations: (a) evaluating the first 13 characters of headlines generated to be 26 characters long on 13char-ref headlines (blue); (b) evaluating headlines ge"
W96-0105,P91-1034,0,0.0225877,"). S =XUT (5) We introduce a utility function TUF(x), which computes the training utility figure for an example x. The sampling algorithm gives preference to examples of maximum utility, by way of equation (6). arg max TUF(x) (6) xEX We will explain in the following sections how one could estimate TUF, based on the estimation of the certainty figure of an interpretation. Ideally the sampling size, i.e. the number of samples selected at each iteration would be such as to avoid retraining of similar examples. It should be noted that this can be a critical problem for statistics-based approaches [1, 3, 18, 20, 24], as the reconstruction of statistic classifiers is expensive. However, example-based systems [5, 12, 21] do not require the reconstruction of the system, but examples have to be stored in the database. It also should be noted that in each iteration, the system needs only compute the similarity between each example x belonging to X and the newly stored example, instead of every example belonging to T, because of the following reasons: • storing an example of verb sense interpretation senses, 61 si, will not affect the score of other verb • if the system memorizes the current score of si for ea"
W96-0105,J94-4003,0,0.0277881,"). S =XUT (5) We introduce a utility function TUF(x), which computes the training utility figure for an example x. The sampling algorithm gives preference to examples of maximum utility, by way of equation (6). arg max TUF(x) (6) xEX We will explain in the following sections how one could estimate TUF, based on the estimation of the certainty figure of an interpretation. Ideally the sampling size, i.e. the number of samples selected at each iteration would be such as to avoid retraining of similar examples. It should be noted that this can be a critical problem for statistics-based approaches [1, 3, 18, 20, 24], as the reconstruction of statistic classifiers is expensive. However, example-based systems [5, 12, 21] do not require the reconstruction of the system, but examples have to be stored in the database. It also should be noted that in each iteration, the system needs only compute the similarity between each example x belonging to X and the newly stored example, instead of every example belonging to T, because of the following reasons: • storing an example of verb sense interpretation senses, 61 si, will not affect the score of other verb • if the system memorizes the current score of si for ea"
W96-0105,1993.mtsummit-1.10,0,0.0328193,"is small in both figure l l - a and ll-b. However, in the situation as in figure ll-b, since (a) the task of distinction between the verb senses 1 and 2 is easier, and (b) instances where the sense ambiguity of case fillers corresponds to distinct verb senses will be rare, training using either ""xl"" or ""x2"" will be less effective than as in figure ll-a. It should also be noted that since Bunruigoihyo is a relatively small-sized thesaurus and does not enumerate many word senses, this problem is not critical in our case. However, given other existing thesauri like the EDR electronic dictionary [4] or WordNet [15], these two situations should be strictly differentiated. 6 Conclusion In this paper we proposed an example sampling method for example-based verb sense disambiguation. We also reported on the system&apos;s performance by way of experiments. The experiments showed that our method, which is based on the notion of training utility, has reduced the overhead for the training of the system, as well as the size of the database. As pointed out in section 1, the generalization of examples [8, 19] is another method for reducing the size of the database. Whether coupling these two methods wou"
W96-0105,C96-1012,1,0.79323,"x. The sampling algorithm gives preference to examples of maximum utility, by way of equation (6). arg max TUF(x) (6) xEX We will explain in the following sections how one could estimate TUF, based on the estimation of the certainty figure of an interpretation. Ideally the sampling size, i.e. the number of samples selected at each iteration would be such as to avoid retraining of similar examples. It should be noted that this can be a critical problem for statistics-based approaches [1, 3, 18, 20, 24], as the reconstruction of statistic classifiers is expensive. However, example-based systems [5, 12, 21] do not require the reconstruction of the system, but examples have to be stored in the database. It also should be noted that in each iteration, the system needs only compute the similarity between each example x belonging to X and the newly stored example, instead of every example belonging to T, because of the following reasons: • storing an example of verb sense interpretation senses, 61 si, will not affect the score of other verb • if the system memorizes the current score of si for each x, the system simply needs to compare it with the newly computed score between x and the newly stored"
W96-0105,P92-1032,0,0.0179473,"cles. Each of the sentences in the training/test data used 64 in our experiment contained one or several complement(s) followed by one of the ten verbs enumerated in table 2. In table 2, the column of ""English gloss"" describes typical English translations of the Japanese verbs. The column of "" # of sentences"" denotes the number of sentences in the corpus, "" # of senses"" denotes the number of verb senses based on IPAL, and ""lower bound"" denotes the precision gained by using a naive method, where the system systematically chooses the most frequently appearing interpretation in the training data [6]. Table 2: The corpus used for the experiments verb II English gloss ataeru kakeru kuwaeru noru osameru tsukuru torn umu wakaru yameru total [I # of sentences # of senses lower bound give hang add ride govern make take bear offspring understand stop 136 160 167 126 108 126 84 90 60 54 4 29 5 10 8 15 29 2 5 2 66.9 25.6 53.9 45.2 25.0 19.8 26.2 81.1 48.3 59.3 -- 1111 -- 43.7 We at first estimated the system&apos;s performance by its precision, that is the ratio of the number of correct outputs, compared to the number of inputs. In this experiment, we set = 0.5 in equation (7), and k = 1 in equation ("
W96-0105,C92-2101,0,0.216335,"with respect to the correctness of the answer. Dagan et al. proposed a committee-based sampling method, which is currently applied to HMM training for part-of-speech tagging [2]. This method selects samples based on the training utility factor of the examples, i.e. the informativity of the data with respect to future training. However, as all these methods are implemented for statistics-based models, there is a need to explore how to formalize and map these concepts into the examplebased approach. With respect to problem 3, a possible solution would be the generalization of redundant examples [8, 19]. However, such an approach implies a significant overhead for the manual training of each example prior to the generalization. This shortcoming is precisely what our approach allows to avoid: reducing both the overhead as well as the size of the database. Section 2 briefly describes our method for a verb sense disambiguation system. The next Section 3 elaborates on the example sampling method, while section 4 reports on the results of our experiment. Before concluding in section 6, discussion is added in section 5. 57 2 E x a m p l e - b a s e d verb sense disambiguation s y s t e m suri kano"
W96-0105,C94-1049,0,0.0253196,"). S =XUT (5) We introduce a utility function TUF(x), which computes the training utility figure for an example x. The sampling algorithm gives preference to examples of maximum utility, by way of equation (6). arg max TUF(x) (6) xEX We will explain in the following sections how one could estimate TUF, based on the estimation of the certainty figure of an interpretation. Ideally the sampling size, i.e. the number of samples selected at each iteration would be such as to avoid retraining of similar examples. It should be noted that this can be a critical problem for statistics-based approaches [1, 3, 18, 20, 24], as the reconstruction of statistic classifiers is expensive. However, example-based systems [5, 12, 21] do not require the reconstruction of the system, but examples have to be stored in the database. It also should be noted that in each iteration, the system needs only compute the similarity between each example x belonging to X and the newly stored example, instead of every example belonging to T, because of the following reasons: • storing an example of verb sense interpretation senses, 61 si, will not affect the score of other verb • if the system memorizes the current score of si for ea"
W96-0105,P95-1026,0,0.0470033,"). S =XUT (5) We introduce a utility function TUF(x), which computes the training utility figure for an example x. The sampling algorithm gives preference to examples of maximum utility, by way of equation (6). arg max TUF(x) (6) xEX We will explain in the following sections how one could estimate TUF, based on the estimation of the certainty figure of an interpretation. Ideally the sampling size, i.e. the number of samples selected at each iteration would be such as to avoid retraining of similar examples. It should be noted that this can be a critical problem for statistics-based approaches [1, 3, 18, 20, 24], as the reconstruction of statistic classifiers is expensive. However, example-based systems [5, 12, 21] do not require the reconstruction of the system, but examples have to be stored in the database. It also should be noted that in each iteration, the system needs only compute the similarity between each example x belonging to X and the newly stored example, instead of every example belonging to T, because of the following reasons: • storing an example of verb sense interpretation senses, 61 si, will not affect the score of other verb • if the system memorizes the current score of si for ea"
W96-0105,C92-2107,0,\N,Missing
W98-1510,1997.iwpt-1.16,1,0.671327,"that we don&apos;t cousider any lexical association, VC estimate• the probability of its derivation as follows. Fignre 1: A parse derivation for an input string &quot;11Ji!c/J&quot;;&apos; 1 ii: ]t~t.: (She ate a pie)&quot; 2.1 P(taR) &quot;&apos;P(ta!Av.1&apos;) P(t.abeR, ta)&quot;&apos; P(tabeV) ing a wide range of existing syntactic-based language modeling frameworks, from simple PCFG models to more context-sensitive models including those proposed in [2, 13, 19]. Am.olg these, we, at present, use probabilistic GLil (PGLH.) language modeling, which is given by incorpo~ rating probabilistic distributions into the GLR parsing framework [10, 21]. The advantages of PGLR modeling are (a) PGLR. models are mildly context~sensitive, compared with PCFG models, and (b) PGLR. models inherently capture both structural preferences and POS bigram statistics, which meets our integration requirement. For further discussion, see [10]. &apos;l&apos;(gaR, tabe, ta) &quot;&apos; P(gal&apos;r[h(tabe,[Pr,Pz])]) P(oR, ga, tabe, ta) &quot;&apos; P(oPz[h(tabe, [l&apos;r :ga, Pz])]) The lexical model P(WIR) is the product of the probability of each lexical derivation li ·-7 Wi, where 11 E L (L C R) is the POS tag of w; E W: =II l&apos;(w;R,w 1 , ... ,w 1_1) (5) (G) where h(h, [s1, ... , sn]) is"
W98-1510,E91-1004,0,0.0223692,"ile maintaining the probabilistically wcll-foundedness of the overall model. ta (PAST) -; tabe (eat) -+ ga (NO III) (ACC) ·-+ kanojo (she) -; pai (pie) --t o First, for each lcxieal item that we don&apos;t cousider any lexical association, VC estimate• the probability of its derivation as follows. Fignre 1: A parse derivation for an input string &quot;11Ji!c/J&quot;;&apos; 1 ii: ]t~t.: (She ate a pie)&quot; 2.1 P(taR) &quot;&apos;P(ta!Av.1&apos;) P(t.abeR, ta)&quot;&apos; P(tabeV) ing a wide range of existing syntactic-based language modeling frameworks, from simple PCFG models to more context-sensitive models including those proposed in [2, 13, 19]. Am.olg these, we, at present, use probabilistic GLil (PGLH.) language modeling, which is given by incorpo~ rating probabilistic distributions into the GLR parsing framework [10, 21]. The advantages of PGLR modeling are (a) PGLR. models are mildly context~sensitive, compared with PCFG models, and (b) PGLR. models inherently capture both structural preferences and POS bigram statistics, which meets our integration requirement. For further discussion, see [10]. &apos;l&apos;(gaR, tabe, ta) &quot;&apos; P(gal&apos;r[h(tabe,[Pr,Pz])]) P(oR, ga, tabe, ta) &quot;&apos; P(oPz[h(tabe, [l&apos;r :ga, Pz])]) The lexical model P(WIR) is t"
W98-1510,J96-1002,0,0.0088305,"Missing"
W98-1510,P95-1037,0,0.159717,"Missing"
W98-1510,P93-1005,0,0.0208939,"ile maintaining the probabilistically wcll-foundedness of the overall model. ta (PAST) -; tabe (eat) -+ ga (NO III) (ACC) ·-+ kanojo (she) -; pai (pie) --t o First, for each lcxieal item that we don&apos;t cousider any lexical association, VC estimate• the probability of its derivation as follows. Fignre 1: A parse derivation for an input string &quot;11Ji!c/J&quot;;&apos; 1 ii: ]t~t.: (She ate a pie)&quot; 2.1 P(taR) &quot;&apos;P(ta!Av.1&apos;) P(t.abeR, ta)&quot;&apos; P(tabeV) ing a wide range of existing syntactic-based language modeling frameworks, from simple PCFG models to more context-sensitive models including those proposed in [2, 13, 19]. Am.olg these, we, at present, use probabilistic GLil (PGLH.) language modeling, which is given by incorpo~ rating probabilistic distributions into the GLR parsing framework [10, 21]. The advantages of PGLR modeling are (a) PGLR. models are mildly context~sensitive, compared with PCFG models, and (b) PGLR. models inherently capture both structural preferences and POS bigram statistics, which meets our integration requirement. For further discussion, see [10]. &apos;l&apos;(gaR, tabe, ta) &quot;&apos; P(gal&apos;r[h(tabe,[Pr,Pz])]) P(oR, ga, tabe, ta) &quot;&apos; P(oPz[h(tabe, [l&apos;r :ga, Pz])]) The lexical model P(WIR) is t"
W98-1510,H94-1048,0,0.0696817,"Missing"
W98-1510,P96-1025,0,0.155593,"Missing"
W98-1510,C92-2065,0,0.0419749,"Missing"
W98-1510,P97-1003,0,0.058909,"ng A) we rank its pars&lt;~ dr:rlYations according to the joint distribution J&apos;(H, lr). where H1 is a word sequence candidate for A, and R is a parse derivation candidate for H-- whos(&apos; terminal symbols constitute a POS tag scquc-;nce L (see Figure 1 2 ). We first. decompose 1&apos;( fl. lr) • Pmbabilistically well-fottnded semantics: The language model used in a statistical parser should have probabilistically well-founclecl semantics) whieh vould a.lso facilitate the anal:,&apos;·· sis of the model&apos;s behavior. , 1 For further discussion, see [8]. This is also tlH&apos; case with recent works such as [3] and [5] due to t.hc lad&lt; of modularity of statistical types. &apos;-Although syntactic structure. R is represented af&apos; a dependency structure in this figure, our framework 80 into two submodels, the syntactic model l&apos;(R) and the lexical model P(WR): P(R, W) = P(R) · P(WR) depends only on a certain small part of its whole context. We first assume that syntactic structure R in P(wiiR,w 1 , .. . ,wi_- 1 ) can always be reduced to l; ( E R), which allows us to deal with the lexical model separately from the syntactic model. The question then is which subset C of { ·uJI, ... , Wi-l} has the strongest influen"
W98-1510,H92-1027,0,0.075636,"Missing"
W98-1510,1995.iwpt-1.26,0,0.109798,"Missing"
W98-1510,W96-0112,0,\N,Missing
W98-1510,C92-2066,0,\N,Missing
W98-1510,1991.iwpt-1.22,0,\N,Missing
Y09-2021,D09-1003,0,0.0219219,"r several tasks, such as improving parsing. Since latent variables group the kind of arguments expected for a sentence, it is possible to infer the meaning of unknown words, as in the well-known example about tezguino (Lin, 1998b) where it is possible to know what is tezguino from the sentences: A bottle of tezguino is on the table; Everybody likes tezguino; Tezguino makes you drunk; and We make tezguino out of corn. Another application is Semantic Role Labeling, since grouping verb arguments and measuring their plausibility increases performance, as shown by Merlo and Van Der Plas (2009) and Deschacht and Moens (2009). Some other applications are metaphora recognition, since we are able to know common usages of arguments, and an uncommon usage would suggest its presence, or a coherence mistake (v. gr. to drink the moon in a glass). Malapropism detection can use the measure of the plausibility of an argument to determine misuses of words (Bolshakov, 2005) as in hysteric center, instead of historic center; density has brought me to you; It looks like a tattoo subject; and Why you say that with ironing?. 2 Models for Plausible Argument Estimation We explore the models proposed in Calvo et al. (2009a, 2009b);"
Y09-2021,P98-2127,0,0.233288,"roposal model overcomes this and results are shown in Section 3.4. Afterwards, we explore how our model works with ngrams instead of dependency triple relationships in experiments from Section 3.5. Finally, we draw our conclusions in Section 4 mentioning future work and possible applications. 1.1 Possible applications Correct Plausible Verb Argument identification can be used for several tasks, such as improving parsing. Since latent variables group the kind of arguments expected for a sentence, it is possible to infer the meaning of unknown words, as in the well-known example about tezguino (Lin, 1998b) where it is possible to know what is tezguino from the sentences: A bottle of tezguino is on the table; Everybody likes tezguino; Tezguino makes you drunk; and We make tezguino out of corn. Another application is Semantic Role Labeling, since grouping verb arguments and measuring their plausibility increases performance, as shown by Merlo and Van Der Plas (2009) and Deschacht and Moens (2009). Some other applications are metaphora recognition, since we are able to know common usages of arguments, and an uncommon usage would suggest its presence, or a coherence mistake (v. gr. to drink the m"
Y09-2021,P09-1033,0,0.0259356,"Missing"
Y09-2021,P09-1070,0,0.0155601,"detailed categories and examples that fit these categories. For this purpose, recent works take advantage ∗ We thank the support of the Japanese Government and the Mexican Government (SNI, SIP-IPN, COFAA-IPN, and PIFI-IPN). Second author is a JSPS fellow. We also thank our anonymous reviewers for their useful comments and discussion. Copyright 2009 by Hiram Calvo, Kentaro Inui, and Yuji Matsumoto 23rd Pacific Asia Conference on Language, Information and Computation, pages 622–629 622 of existing manually crafted resources such as WordNet, Wikipedia, FrameNet, VerbNet or PropBank. For example, Reisinger and Paşca (2009) annotate existing WordNet concepts with attributes, and extend is-a relations, based on Latent Dirichlet Allocation on Web documents and Wikipedia. Yamada et al. (2009) explore extracting hyponym relations from Wikipedia using pattern-based discovery and distributional similarity clustering. Nevertheless, the approach of using handcrafted resources prevents from obtaining information for languages where those resources do not exist. Calvo et al. (2009a, 2009b) propose a non language-dependent model based on K-Nearest Neighbors for calculating the plausibility of candidate arguments given one"
Y09-2021,W03-1011,0,0.0171257,"-algorithm of this model. for each triple &lt;v,a1,a2&gt; with observed count c, for each argument a1,a2 Find its k most similar words a1s1…a1sk, a2s1…a2sk with similarities s1s1, ..., s1sk and s2s1,...,s2sk. Add votes for each new triple &lt;v,a1si,a2sj&gt; += c·s1si·s2sj Figure 1: Pseudo-algorithm for the K-nearest neighbors DLM algorithm As votes are accumulative, triples that have words with many similar words will get more votes. Common similarity measures range from Euclidean distance, cosine and Jaccard’s coefficient (Lee, 1999), to measures such as Hindle’s measure and Lin’s measure (Lin, 1998a). Weeds and Weir (2003) show that the distributional measure with best performance is the Lin similarity, so this measure is used for smoothing the co-occurrence space, following the procedure as described by Lin (1998a). 2.2 PLSI – Probabilistic Latent Semantic Indexing The probabilistic Latent Semantic Indexing Model (PLSI) was introduced in Hofmann (1999), arose from Latent Semantic Indexing (Deerwester et al., 1990). The model attempts to associate an unobserved class variable z∈Z={z1, ..., zk}, (in our case a generalization of correlation of the co-occurrence of v,a1 and a2), and two sets of observables: argume"
Y09-2021,D09-1097,0,0.0408554,"and examples that fit these categories. For this purpose, recent works take advantage ∗ We thank the support of the Japanese Government and the Mexican Government (SNI, SIP-IPN, COFAA-IPN, and PIFI-IPN). Second author is a JSPS fellow. We also thank our anonymous reviewers for their useful comments and discussion. Copyright 2009 by Hiram Calvo, Kentaro Inui, and Yuji Matsumoto 23rd Pacific Asia Conference on Language, Information and Computation, pages 622–629 622 of existing manually crafted resources such as WordNet, Wikipedia, FrameNet, VerbNet or PropBank. For example, Reisinger and Paşca (2009) annotate existing WordNet concepts with attributes, and extend is-a relations, based on Latent Dirichlet Allocation on Web documents and Wikipedia. Yamada et al. (2009) explore extracting hyponym relations from Wikipedia using pattern-based discovery and distributional similarity clustering. Nevertheless, the approach of using handcrafted resources prevents from obtaining information for languages where those resources do not exist. Calvo et al. (2009a, 2009b) propose a non language-dependent model based on K-Nearest Neighbors for calculating the plausibility of candidate arguments given one"
Y09-2021,P09-1048,0,\N,Missing
Y09-2021,C98-2122,0,\N,Missing
Y09-2021,P99-1004,0,\N,Missing
Y12-1057,J04-3004,0,0.0221972,"expansion is the task of expanding a list of named entities from a few named entities (seed instances). For example, given a few instances of car vehicles “Prius”, “Lexus” and “Insight”, the task outputs new car instances such as “Corolla”, “Civic”, and “Fit”. Set expansion has many applications in NLP including named entity recognition (Collins and Singer, 1999), word sense disambiguation (Pantel and Lin, 2002), document categorization (Pantel et al., 2009), and query suggestion (Cao et al., 2008). Set expansion is often implemented as bootstrapping algorithms (Hearst, 1992; Yarowsky, 1995; Abney, 2004; Pantel and Ravichandran, 2004; Pantel 525 However, bootstrapping algorithms often suffer from patterns that retrieve instances not only of the target category but also of other categories. For example, given the seed instances “Prius” and “Lexus”, a bootstrapping algorithm might choose the pattern “new type of X”, which might extract unrelated instances such as “iPhone” and “ThinkPad”. The semantic drift problem (Curran et al., 2007), the phenomenon by which a bootstrapping algorithm deviates from the target category, has persisted as the major impediment of bootstrapping algorithms. Bootstr"
Y12-1057,W99-0613,0,0.109147,"tic categories as an additional type of prior knowledge. We demonstrate the effectiveness of sibling relations in set expansion on the dataset in which instances and sibling relations are extracted from Wikipedia in a semi-automatic manner. 1 Introduction Set expansion is the task of expanding a list of named entities from a few named entities (seed instances). For example, given a few instances of car vehicles “Prius”, “Lexus” and “Insight”, the task outputs new car instances such as “Corolla”, “Civic”, and “Fit”. Set expansion has many applications in NLP including named entity recognition (Collins and Singer, 1999), word sense disambiguation (Pantel and Lin, 2002), document categorization (Pantel et al., 2009), and query suggestion (Cao et al., 2008). Set expansion is often implemented as bootstrapping algorithms (Hearst, 1992; Yarowsky, 1995; Abney, 2004; Pantel and Ravichandran, 2004; Pantel 525 However, bootstrapping algorithms often suffer from patterns that retrieve instances not only of the target category but also of other categories. For example, given the seed instances “Prius” and “Lexus”, a bootstrapping algorithm might choose the pattern “new type of X”, which might extract unrelated instanc"
Y12-1057,C92-2082,0,0.222983,"tic manner. 1 Introduction Set expansion is the task of expanding a list of named entities from a few named entities (seed instances). For example, given a few instances of car vehicles “Prius”, “Lexus” and “Insight”, the task outputs new car instances such as “Corolla”, “Civic”, and “Fit”. Set expansion has many applications in NLP including named entity recognition (Collins and Singer, 1999), word sense disambiguation (Pantel and Lin, 2002), document categorization (Pantel et al., 2009), and query suggestion (Cao et al., 2008). Set expansion is often implemented as bootstrapping algorithms (Hearst, 1992; Yarowsky, 1995; Abney, 2004; Pantel and Ravichandran, 2004; Pantel 525 However, bootstrapping algorithms often suffer from patterns that retrieve instances not only of the target category but also of other categories. For example, given the seed instances “Prius” and “Lexus”, a bootstrapping algorithm might choose the pattern “new type of X”, which might extract unrelated instances such as “iPhone” and “ThinkPad”. The semantic drift problem (Curran et al., 2007), the phenomenon by which a bootstrapping algorithm deviates from the target category, has persisted as the major impediment of boot"
Y12-1057,P06-1015,0,0.260119,"s on the dataset (seed and test instances) extracted from Wikipedia. This paper is organized as follows. Section 2 reviews the Espresso algorithm as the baseline algorithm of this study. The section also describes the problem of semantic drift and previous approaches to the problem. Section 3 presents the proposed method, which uses sibling relations of semantic categories as an additional source of prior knowledge. Section 4 demonstrates the effectiveness of the proposed method and discusses the experimental results. In section 5, we conclude this paper. 2 Related Work 2.1 Espresso algorithm Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which fundamentally iterates two steps: candidate extraction and ranking. In candidate extraction, the algorithm collects patterns that are co-occurring with seed instances and instances acquired in the previous iteration. The algorithm also ﬁnds candidates of new instances using patterns extracted in the previous iteration. In the ranking step, the algorithm ﬁnds the top N candidates of patterns and instances based on their scores. The espresso algorithm deﬁnes score rπ (p) for candidate pattern p and score rι (i) for the candidate instance i as rπ (p) = 1"
Y12-1057,N04-1041,0,0.0397058,"the task of expanding a list of named entities from a few named entities (seed instances). For example, given a few instances of car vehicles “Prius”, “Lexus” and “Insight”, the task outputs new car instances such as “Corolla”, “Civic”, and “Fit”. Set expansion has many applications in NLP including named entity recognition (Collins and Singer, 1999), word sense disambiguation (Pantel and Lin, 2002), document categorization (Pantel et al., 2009), and query suggestion (Cao et al., 2008). Set expansion is often implemented as bootstrapping algorithms (Hearst, 1992; Yarowsky, 1995; Abney, 2004; Pantel and Ravichandran, 2004; Pantel 525 However, bootstrapping algorithms often suffer from patterns that retrieve instances not only of the target category but also of other categories. For example, given the seed instances “Prius” and “Lexus”, a bootstrapping algorithm might choose the pattern “new type of X”, which might extract unrelated instances such as “iPhone” and “ThinkPad”. The semantic drift problem (Curran et al., 2007), the phenomenon by which a bootstrapping algorithm deviates from the target category, has persisted as the major impediment of bootstrapping algorithms. Bootstrapping algorithms assume prior"
Y12-1057,W11-0319,0,0.0123588,"vehicle manufacturers using seed instances “Saturn” and “Subaru”, a bootstrapping algorithm might ﬁnd instances representing the star category (e.g., “Jupiter” and “Uranus”). This is because “Saturn” and “Subaru” are polysemous words, belonging not only to motor vehicle manufacture but also to astronomical objects: planets and stars. 2.3 Approaches to semantic drift Many researchers have presented various approaches to reduce the effects of semantic drift. The approaches range from reﬁnement of the seed set (Vyas et al., 2009), applying classiﬁer (Bellare et al., 2007; Sadamitsu et al., 2011; Pennacchiotti and Pantel, 2011), using human judges (Vyas and Pantel, 2009), to using relationships between semantic categories (Curran et al., 2007; Carlson et al., 2010). Vyas et al. (2009) investigated the inﬂuence of seed instances on bootstrapping algorithms. They reported that seed instances selected by human who are not specialists sometimes yield worse results than those selected randomly. They proposed a method that reﬁnes seed sets generated by humans to improve the set expansion performance. Bellare et al. (2007) proposed a method using a classiﬁer instead of scoring functions in the ranking step of bootstrapping"
Y12-1057,P11-2128,0,0.015674,"expand the set of motor vehicle manufacturers using seed instances “Saturn” and “Subaru”, a bootstrapping algorithm might ﬁnd instances representing the star category (e.g., “Jupiter” and “Uranus”). This is because “Saturn” and “Subaru” are polysemous words, belonging not only to motor vehicle manufacture but also to astronomical objects: planets and stars. 2.3 Approaches to semantic drift Many researchers have presented various approaches to reduce the effects of semantic drift. The approaches range from reﬁnement of the seed set (Vyas et al., 2009), applying classiﬁer (Bellare et al., 2007; Sadamitsu et al., 2011; Pennacchiotti and Pantel, 2011), using human judges (Vyas and Pantel, 2009), to using relationships between semantic categories (Curran et al., 2007; Carlson et al., 2010). Vyas et al. (2009) investigated the inﬂuence of seed instances on bootstrapping algorithms. They reported that seed instances selected by human who are not specialists sometimes yield worse results than those selected randomly. They proposed a method that reﬁnes seed sets generated by humans to improve the set expansion performance. Bellare et al. (2007) proposed a method using a classiﬁer instead of scoring functions in"
Y12-1057,sumida-etal-2008-boosting,0,0.0177124,"recision of each method when each method acquires ﬁxed quantities of instances. We asked three human annotators to judge the correctness of acquired instances. We conducted the experiments in Japanese. The results described herein have been translated into English for presentation. Table 2 reports all categories used for the experiments. Each category belongs to only one sibling group. Each sibling group consists of two or more categories. We prepared sibling groups by manually based on Wikipedia. Each category starts with 15 seed instances extracted from Wikipedia in a semi-automatic manner (Sumida et al., 2008). Because the automatic method yields incorrect seed instances, we removed errors manually. We used 110 million Japanese web pages from which patterns and instances are extracted. We parsed sentences in the web pages using KNP, a Japanese dependency parser (Kurohashi et al., 1994). To reduce the computational time for the Espresso algorithm, we removed patterns and instances occurring fewer than three times. 4.2 Results Figure 4 shows the precision of each method in accordance with the number of acquired instances. The dotted line depicts the precision curve of the Espresso algorithm. Espresso"
Y12-1057,N09-1033,0,0.0243643,"nd “Subaru”, a bootstrapping algorithm might ﬁnd instances representing the star category (e.g., “Jupiter” and “Uranus”). This is because “Saturn” and “Subaru” are polysemous words, belonging not only to motor vehicle manufacture but also to astronomical objects: planets and stars. 2.3 Approaches to semantic drift Many researchers have presented various approaches to reduce the effects of semantic drift. The approaches range from reﬁnement of the seed set (Vyas et al., 2009), applying classiﬁer (Bellare et al., 2007; Sadamitsu et al., 2011; Pennacchiotti and Pantel, 2011), using human judges (Vyas and Pantel, 2009), to using relationships between semantic categories (Curran et al., 2007; Carlson et al., 2010). Vyas et al. (2009) investigated the inﬂuence of seed instances on bootstrapping algorithms. They reported that seed instances selected by human who are not specialists sometimes yield worse results than those selected randomly. They proposed a method that reﬁnes seed sets generated by humans to improve the set expansion performance. Bellare et al. (2007) proposed a method using a classiﬁer instead of scoring functions in the ranking step of bootstrapping algorithms. The classiﬁer approach can use"
Y12-1057,P95-1026,0,0.106266,"Introduction Set expansion is the task of expanding a list of named entities from a few named entities (seed instances). For example, given a few instances of car vehicles “Prius”, “Lexus” and “Insight”, the task outputs new car instances such as “Corolla”, “Civic”, and “Fit”. Set expansion has many applications in NLP including named entity recognition (Collins and Singer, 1999), word sense disambiguation (Pantel and Lin, 2002), document categorization (Pantel et al., 2009), and query suggestion (Cao et al., 2008). Set expansion is often implemented as bootstrapping algorithms (Hearst, 1992; Yarowsky, 1995; Abney, 2004; Pantel and Ravichandran, 2004; Pantel 525 However, bootstrapping algorithms often suffer from patterns that retrieve instances not only of the target category but also of other categories. For example, given the seed instances “Prius” and “Lexus”, a bootstrapping algorithm might choose the pattern “new type of X”, which might extract unrelated instances such as “iPhone” and “ThinkPad”. The semantic drift problem (Curran et al., 2007), the phenomenon by which a bootstrapping algorithm deviates from the target category, has persisted as the major impediment of bootstrapping algori"
Y12-1057,D09-1098,0,\N,Missing
Y14-1010,E12-1004,0,0.12775,"su Muraoka† Yotaro Watanabe† Sonse Shimaoka‡ Naoaki Okazaki†∗ Kazeto Yamamoto† Kentaro Inui† Tohoku University†‡ Japan Science and Technology Agency (JST)∗ {muraoka,kazeto,yotaro-w,okazaki,inui} @ecei.tohoku.ac.jp† simaokasonse@yahoo.co.jp‡ Abstract semantic information reliably from co-occurrence statistics of a phrase. Recently, numerous studies have explored compositional semantics, in which the meaning of a phrase, clause, or sentence is computed from those of its constituents (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Guevara, 2010; Zanzotto et al., 2010; Socher et al., 2011; Baroni et al., 2012; Socher et al., 2012; Socher et al., 2013a; Socher et al., 2014). These studies mostly address theories and methods for computing a vector of a phrase from the vectors of its constituents; the simplest but effective approach is to take the average of the two input vectors. The field of distributional-compositional semantics has yielded a range of computational models for composing the vector of a phrase from those of constituent word vectors. Existing models have various ranges of their expressiveness, recursivity, and trainability. However, these models have not been examined closely for the"
Y14-1010,D12-1050,0,0.432171,"orpus. Numerous studies have demonstrated learned word vectors from a large text corpus (Bullinaria and Levy, 2007; Collobert and Weston, 2008; Turney and Pantel, 2010; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013). In contrast, the same approach is not scalable to a complex linguistic unit (e.g., phrase or sentence) because of the data sparseness problem: the longer the length of a phrase, the fewer times the phrase occurs in a corpus. For this reason, we cannot acquire A simple approach such as additive and multiplicative compositions has been a strong baseline over more complex models (Blacoe and Lapata, 2012; Socher et al., 2013b). However, Erk and Pad´o (2008) argued the importance of syntax relations: the simple additive/multiplicative approach yields the same vector for phrases a horse draws and draw a horse, ignoring the syntactic structure by which horse in the former phrase is a subject whereas horse in the latter is the object. They formulated a generalized composition function including such a composition. However, this generalized composition is too complex to learn. These models usually do not work well for now. As described in this paper, through a humancorrelation experiment, we explo"
Y14-1010,W13-3206,0,0.0514921,"e number of instances of the relation in the dataset, some matrixes are updated frequently, and some are rarely updated. Therefore, we use the diagonal variant of AdaGrad (Duchi et al., 2011; Socher et al., 2013a). This enables the learning rate to vary each matrix Wr . 4 Experiment In this section, we explain the method for constructing vectors for words and phrases for the supervision data, followed by an explanation of some details of the training procedure. We then report experimentally obtained results. 4.1 Obtaining vectors for words and phrases as supervision data Following the work of Dinu et al. (2013), we constructed word and phrase vectors as follows. We used a concatenation of three large corpora: PukWaC2 (Baroni et al., 2009) (2 billion tokens), WaCkypedia EN(Wikipedia 2009 dump) (Baroni et al., 2009) (about 800 million tokens), and ClueWeb093 (5 billion pages in English). The distribution of PukWaC and WaCkypedia EN includes parse results from TreeTagger and MaltParser. We used Stanford CoreNLP4 to parse ClueWeb09. Counting frequencies of occurrences of lemmas of content words (nouns, adjectives, verbs, and adverbs), we identified the top 10,000 most frequent words; we represent the se"
Y14-1010,D08-1094,0,0.0952973,"Missing"
Y14-1010,W10-2805,0,0.125661,"Best Model Among Representative Compositional Models Masayasu Muraoka† Yotaro Watanabe† Sonse Shimaoka‡ Naoaki Okazaki†∗ Kazeto Yamamoto† Kentaro Inui† Tohoku University†‡ Japan Science and Technology Agency (JST)∗ {muraoka,kazeto,yotaro-w,okazaki,inui} @ecei.tohoku.ac.jp† simaokasonse@yahoo.co.jp‡ Abstract semantic information reliably from co-occurrence statistics of a phrase. Recently, numerous studies have explored compositional semantics, in which the meaning of a phrase, clause, or sentence is computed from those of its constituents (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Guevara, 2010; Zanzotto et al., 2010; Socher et al., 2011; Baroni et al., 2012; Socher et al., 2012; Socher et al., 2013a; Socher et al., 2014). These studies mostly address theories and methods for computing a vector of a phrase from the vectors of its constituents; the simplest but effective approach is to take the average of the two input vectors. The field of distributional-compositional semantics has yielded a range of computational models for composing the vector of a phrase from those of constituent word vectors. Existing models have various ranges of their expressiveness, recursivity, and trainabil"
Y14-1010,P08-1028,0,0.433594,"rises the meanings of the constituents and the rule for combining the constituents. Equation 1 formulates this principle mathematically: p = f (u, v). (1) Here, given two input (e.g. word) vectors u ∈ Rd1 and v ∈ Rd1 , the model f yields a phrase vector p ∈ Rd2 as a composition of the input vectors. In other words, the model f is a function that computes a phrase vector p for the inputs u and v. Setting d = d1 = d2 allows recursive compositions, i.e., generating phrase or sentence vectors consisting of three or more words. Table 1 shows representative models from earlier works. The Add model (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010) computes a linear combination of two input vectors u, v ∈ Rd with weights w1 , w2 ∈ R. This model works surprisingly well in practice despite its simplicity. The Fulladd model (Guevara, 2010; Zanzotto et al., 2010) PACLIC 28 extends the Add model, applying a linear transformation to inputs with a weight matrix W ∈ Rd×2d . This model can not only scale but also rotate input vectors, unlike the Add model. Regarding linear transformation with a matrix W , Recursive Neural Network (RNN) model (Socher et al., 2011) achieves a nonlinear transformation through the use of"
Y14-1010,D12-1110,0,0.653556,"tanabe† Sonse Shimaoka‡ Naoaki Okazaki†∗ Kazeto Yamamoto† Kentaro Inui† Tohoku University†‡ Japan Science and Technology Agency (JST)∗ {muraoka,kazeto,yotaro-w,okazaki,inui} @ecei.tohoku.ac.jp† simaokasonse@yahoo.co.jp‡ Abstract semantic information reliably from co-occurrence statistics of a phrase. Recently, numerous studies have explored compositional semantics, in which the meaning of a phrase, clause, or sentence is computed from those of its constituents (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Guevara, 2010; Zanzotto et al., 2010; Socher et al., 2011; Baroni et al., 2012; Socher et al., 2012; Socher et al., 2013a; Socher et al., 2014). These studies mostly address theories and methods for computing a vector of a phrase from the vectors of its constituents; the simplest but effective approach is to take the average of the two input vectors. The field of distributional-compositional semantics has yielded a range of computational models for composing the vector of a phrase from those of constituent word vectors. Existing models have various ranges of their expressiveness, recursivity, and trainability. However, these models have not been examined closely for their compositionality."
Y14-1010,P13-1045,0,0.557909,"a‡ Naoaki Okazaki†∗ Kazeto Yamamoto† Kentaro Inui† Tohoku University†‡ Japan Science and Technology Agency (JST)∗ {muraoka,kazeto,yotaro-w,okazaki,inui} @ecei.tohoku.ac.jp† simaokasonse@yahoo.co.jp‡ Abstract semantic information reliably from co-occurrence statistics of a phrase. Recently, numerous studies have explored compositional semantics, in which the meaning of a phrase, clause, or sentence is computed from those of its constituents (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Guevara, 2010; Zanzotto et al., 2010; Socher et al., 2011; Baroni et al., 2012; Socher et al., 2012; Socher et al., 2013a; Socher et al., 2014). These studies mostly address theories and methods for computing a vector of a phrase from the vectors of its constituents; the simplest but effective approach is to take the average of the two input vectors. The field of distributional-compositional semantics has yielded a range of computational models for composing the vector of a phrase from those of constituent word vectors. Existing models have various ranges of their expressiveness, recursivity, and trainability. However, these models have not been examined closely for their compositionality. We implement and comp"
Y14-1010,Q14-1017,0,0.168223,"zeto Yamamoto† Kentaro Inui† Tohoku University†‡ Japan Science and Technology Agency (JST)∗ {muraoka,kazeto,yotaro-w,okazaki,inui} @ecei.tohoku.ac.jp† simaokasonse@yahoo.co.jp‡ Abstract semantic information reliably from co-occurrence statistics of a phrase. Recently, numerous studies have explored compositional semantics, in which the meaning of a phrase, clause, or sentence is computed from those of its constituents (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Guevara, 2010; Zanzotto et al., 2010; Socher et al., 2011; Baroni et al., 2012; Socher et al., 2012; Socher et al., 2013a; Socher et al., 2014). These studies mostly address theories and methods for computing a vector of a phrase from the vectors of its constituents; the simplest but effective approach is to take the average of the two input vectors. The field of distributional-compositional semantics has yielded a range of computational models for composing the vector of a phrase from those of constituent word vectors. Existing models have various ranges of their expressiveness, recursivity, and trainability. However, these models have not been examined closely for their compositionality. We implement and compare these models under"
Y14-1010,P09-1054,0,0.0244736,"a weight matrix W is updated by the following equation, ′ W =W −α ∂J(θ) , ∂W (10) where α is a learning rate. Using stochastic gradient descent, we update a weight matrix W every time one training instance is processed. The gradient of the objective is ⎡ ⎤T ut ∂J(θ) ∂J(θ) ∂pt ∂ = = e t ⎣ vt ⎦ + λ ∥θ∥1 . ∂W ∂pt ∂W ∂W b (11) Here, et represents a d-dimensional column vector with k-th element of et,k = (pt,k − qt,k )(1 − p2t,k ). (12) d We used dx tanh(x) = 1 − tanh(x)2 to derive this equation. The second term of Equation 11 is not differentiable. Following the work of Langford et al. (2008) and Tsuruoka et al. (2009), we first update the weight matrix W without consideration of the L1 penalty. Then, we use Equation 13 to apply the L1 regularization, ⎧ ⎪ ⎨max(0, wij − αλ) if wij &gt; 0 ′ wij = min(0, wij + αλ) if wij &lt; 0 , (13) ⎪ ⎩ 0 otherwise where wij denotes the (i, j) element of W . A neural network model such as RNN, Relfunc, and Fulllex is nonlinear, which means that the naive training procedure might be trapped with a local minimum. To prevent local minima, we employ some technical methods. We update the learning rate α for every iteration epoch l using the temperature of the simulated annealing algori"
Y14-1010,C10-1142,0,0.0203636,"50 words from a target word as contexts. Then we transform each element of the co-occurrence matrix into Pointwise Mutual 2 http://wacky.sslmit.unibo.it/ http://lemurproject.org/clueweb09/ 4 http://nlp.stanford.edu/software/ corenlp.shtml 3 PACLIC 28 Information (PMI) (Evert, 2005). Finally, we compress the matrix into d dimension using Principal Component Analysis (PCA) (Roweis, 1998) with EM algorithm5 . In this way, we obtained 10,000 word vectors and 17,433 phrase vectors. 4.2 Gold-standard data We conducted a human-correlation experiment using the dataset6 created in Mitchell and Lapata (2010). Each instance in the dataset is a triplet ⟨phrase1, phrase2, similarity⟩: a similarity is a semantic similarity between the phrases annotated by humans, with a value ranging from 1 (least similar) to 7 (most similar). We designate this as humansimilarity. For example, the similarity between vast amount and large quantity is 7 (most similar) whereas the similarity between hear word and remember name is 1 (least similar). For each POS pair (adjective–noun, noun–noun, verb–noun), the dataset includes 108 instances annotated by 18 human subjects (1,944 in total). We measure Spearman’s ρ between"
Y14-1010,N10-1069,0,\N,Missing
Y14-1042,D13-1178,0,0.0141615,"are not sufficient to solve the WS Challenge. Rahman and Ng (2012) proposed a ranking-based model that combines sophisticated linguistic features derived from different sources of world knowledge, such as narrative chains (Chambers and Jurafsky, 2008) and page counts returned by Google. Narrative chains are built by considering temporal relations between two events. However, the WS Challenge contains various discourse 2 A collection of Winograd schemas has been updated and is available at: http://www.cs.nyu.edu/davise/papers/WS.html. !359 PACLIC 28 relations, such as explanation and contrast. Balasubramanian et al. (2013) found another issue of narrative chains in which unrelated actors are often mixed into the same chains. Lapata and Keller (2005) and Levesque et al. (2012) examined the use of page counts and found the stability issue. The contribution of our work is the exploration of the necessary background knowledge for resolving the WS Challenge. To better understand the nature of the WS sentences, we propose to examine similar sentences having less ambiguity and develop a method for automatically acquiring those similar sentences from the Web. 3 Approach Our goal is to acquire useful examples that are s"
Y14-1042,W13-2322,0,0.0365491,"Missing"
Y14-1042,P06-1005,0,0.0496719,"Missing"
Y14-1042,P08-1090,0,0.0609749,"w shot the sheriff, but he did not shoot the deputy. b. The outlaw shot the sheriff, but he shot back. Suppose that the target pronoun is he, and its two candidate antecedents are the outlaw and the sheriff. The question is which of the two candidates is the correct antecedent for the target pronoun in each sentence? Most people resolve he to the outlaw in (1a) but to the sheriff in (1b) without noticing any 1 The sentences are taken from the dataset created by Rahman and Ng (2012). A previous work by Rahman and Ng (2012) showed that two sources of world knowledge, including narrative chains (Chambers and Jurafsky, 2008) and page counts returned by a search engine, are useful for resolving Winograd schemas. However, these two knowledge sources have their own weaknesses and need some heuristics to bridge the gap. Narrative chains suffer from the lack of discourse relations. For example, both sentences in (1) have a contrast relation indicated by but. However, narrative chains rely only on temporal relations between two events (e.g., before and after). Page counts used for estimating n-gram statistics are unstable and vary considerably over time (Lapata and Keller, 2005; Levesque et al., 2012). Therefore the an"
Y14-1042,de-marneffe-etal-2006-generating,0,0.0533323,"Missing"
Y14-1042,D13-1203,0,0.0161816,"ecutive asterisks are combined into one. For example, we generate the search queries for (1a) and (1b) as follows: (8) a. “* shot * but * not shoot *” b. “* shot * but * shot back” and for (4) and (5) as: (9) a. “* beats * but * is beaten by *” b. “* beats * but * beat *” 3.3 Alignment After retrieving snippets, we analyze them using the Stanford CoreNLP (Manning et al., 2014). We use the standard pipeline, ranging from tokenization to dependency parsing. A snippet may contain several fragments or sentences, so we consider it as a short document. We then use the Berkeley coreference resolver (Durrett and Klein, 2013) for predicting coreference chains within each snippet. We consider the processed snippets as candidate examples. For example, (2a) has the following coreference chain: !361 PACLIC 28 Relation nsubj xsubj csubj agent dobj iobj pobj nsubjpass Description subject nominal subject controlling subject clausal subject agent object direct object indirect object object of preposition passive nominal subject date analogy I6 :I1 ::he8 :outlaw2 from the alignment of (6) and (10). Consider the following dependency structure, which corresponds to (2b): (11) Russ ... was shot in the leg, but he shot back. c"
Y14-1042,P05-1045,0,0.0303966,"Missing"
Y14-1042,Y09-1024,0,0.0598015,"Missing"
Y14-1042,W11-1902,0,0.0418551,"Missing"
Y14-1042,P11-1060,0,0.032421,"Missing"
Y14-1042,P14-5010,0,0.0082714,"entence, we keep the conjunction and the heads of the two candidate antecedents and the target pronoun. We then check the dependents of the heads, keeping only those that meet our criteria. We replace other words with asterisks. Multiple consecutive asterisks are combined into one. For example, we generate the search queries for (1a) and (1b) as follows: (8) a. “* shot * but * not shoot *” b. “* shot * but * shot back” and for (4) and (5) as: (9) a. “* beats * but * is beaten by *” b. “* beats * but * beat *” 3.3 Alignment After retrieving snippets, we analyze them using the Stanford CoreNLP (Manning et al., 2014). We use the standard pipeline, ranging from tokenization to dependency parsing. A snippet may contain several fragments or sentences, so we consider it as a short document. We then use the Berkeley coreference resolver (Durrett and Klein, 2013) for predicting coreference chains within each snippet. We consider the processed snippets as candidate examples. For example, (2a) has the following coreference chain: !361 PACLIC 28 Relation nsubj xsubj csubj agent dobj iobj pobj nsubjpass Description subject nominal subject controlling subject clausal subject agent object direct object indirect objec"
Y14-1042,P10-1142,0,0.0132019,"source sentence. Then, we find the alignments of the source sentence and its retrieved examples. Finally, we rank the most likely antecedent for the target pronoun using our score function. In the following section, we discuss related work. Section 3 presents our approach. Section 4 shows our experimental results and error analysis. Section 5 concludes the paper with some directions of future research. 2 Related work We classify the problem of pronoun resolution into two main categories: traditional anaphora and Winograd schemas. Anaphora (or coreference) resolution has a long history in NLP. Ng (2010) and Poesio et al. (2011) provided excellent surveys of approaches to anaphora resolution. A variety of corpora and evaluation metrics also made it difficult for researchers to compare the performance of their systems. To establish benchmarking data and evaluation metrics, the CoNLL-2011 and CoNLL-2012 shared tasks mainly focused on coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012). The term “Winograd schema” was coined by Hector Levesque (2011), named after Terry Winograd who first used a pair of twin sentences to show the difficulty in natural language understanding (Winogr"
Y14-1042,W11-1901,0,0.0186456,"h some directions of future research. 2 Related work We classify the problem of pronoun resolution into two main categories: traditional anaphora and Winograd schemas. Anaphora (or coreference) resolution has a long history in NLP. Ng (2010) and Poesio et al. (2011) provided excellent surveys of approaches to anaphora resolution. A variety of corpora and evaluation metrics also made it difficult for researchers to compare the performance of their systems. To establish benchmarking data and evaluation metrics, the CoNLL-2011 and CoNLL-2012 shared tasks mainly focused on coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012). The term “Winograd schema” was coined by Hector Levesque (2011), named after Terry Winograd who first used a pair of twin sentences to show the difficulty in natural language understanding (Winograd, 1972). Levesque proposed the Winograd Schema (WS) Challenge as an alternative to the Turing Test, which aims to test artificially intelligent systems. Unlike the Turing Test, the WS Challenge just requires systems to answer a collection of binary questions. These questions called Winograd schemas are pairs of sentences containing referential ambiguities that are easy for p"
Y14-1042,W12-4501,0,0.0124984,"uture research. 2 Related work We classify the problem of pronoun resolution into two main categories: traditional anaphora and Winograd schemas. Anaphora (or coreference) resolution has a long history in NLP. Ng (2010) and Poesio et al. (2011) provided excellent surveys of approaches to anaphora resolution. A variety of corpora and evaluation metrics also made it difficult for researchers to compare the performance of their systems. To establish benchmarking data and evaluation metrics, the CoNLL-2011 and CoNLL-2012 shared tasks mainly focused on coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012). The term “Winograd schema” was coined by Hector Levesque (2011), named after Terry Winograd who first used a pair of twin sentences to show the difficulty in natural language understanding (Winograd, 1972). Levesque proposed the Winograd Schema (WS) Challenge as an alternative to the Turing Test, which aims to test artificially intelligent systems. Unlike the Turing Test, the WS Challenge just requires systems to answer a collection of binary questions. These questions called Winograd schemas are pairs of sentences containing referential ambiguities that are easy for people to resolve but di"
Y14-1042,D10-1048,0,0.160985,"Missing"
Y14-1042,D12-1071,0,0.700671,"of world knowledge useful for difficult pronoun resolution. 1 Introduction Consider the following pair of sentences:1 (1) a. The outlaw shot the sheriff, but he did not shoot the deputy. b. The outlaw shot the sheriff, but he shot back. Suppose that the target pronoun is he, and its two candidate antecedents are the outlaw and the sheriff. The question is which of the two candidates is the correct antecedent for the target pronoun in each sentence? Most people resolve he to the outlaw in (1a) but to the sheriff in (1b) without noticing any 1 The sentences are taken from the dataset created by Rahman and Ng (2012). A previous work by Rahman and Ng (2012) showed that two sources of world knowledge, including narrative chains (Chambers and Jurafsky, 2008) and page counts returned by a search engine, are useful for resolving Winograd schemas. However, these two knowledge sources have their own weaknesses and need some heuristics to bridge the gap. Narrative chains suffer from the lack of discourse relations. For example, both sentences in (1) have a contrast relation indicated by but. However, narrative chains rely only on temporal relations between two events (e.g., before and after). Page counts used fo"
Y14-1042,J06-3003,0,0.292676,"le Web Search API expands some queries and returns results containing related words. As a result, we use the synonym instead of the exact match to increase coverage.4 We also generalize grammatical relations to a coarser level. Here we focus on two main categories: subject and object. Table 2 shows our generalized grammatical relations. Based on our scheme, the dependency structures (6) and (10), where their original sentences are (1a) and (2a), are a good alignment since their heads and grammatical roles match exactly. We write an analogy in the form A:B::C:D, meaning A is to B as C is to D (Turney, 2006). Therefore we derive a candi4 nsubj nsubjpass We use WordNet in Natural Language Toolkit (Bird et al., 2009). Note that we omit some words due to the limited space. Although (11) has one actor, Russ3 , and his grammatical role, nsubjpass, does not match exactly with those of the actors in (3), the dependency structures (3) and (11), where their original sentences are (1b) and (2b), are still a good alignment since the grammatical roles nsubjpass and dobj are in the same object category. Therefore we obtain a candidate analogy he19 :Russ3 ::he8 :sheriff5 from the alignment of (3) and (11). 3.4"
Y14-1042,H89-1033,0,0.0392603,"(2010) and Poesio et al. (2011) provided excellent surveys of approaches to anaphora resolution. A variety of corpora and evaluation metrics also made it difficult for researchers to compare the performance of their systems. To establish benchmarking data and evaluation metrics, the CoNLL-2011 and CoNLL-2012 shared tasks mainly focused on coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012). The term “Winograd schema” was coined by Hector Levesque (2011), named after Terry Winograd who first used a pair of twin sentences to show the difficulty in natural language understanding (Winograd, 1972). Levesque proposed the Winograd Schema (WS) Challenge as an alternative to the Turing Test, which aims to test artificially intelligent systems. Unlike the Turing Test, the WS Challenge just requires systems to answer a collection of binary questions. These questions called Winograd schemas are pairs of sentences containing referential ambiguities that are easy for people to resolve but difficult for systems. A Winograd schema is designed to satisfy the following constraints (Levesque et al., 2012): • Easily disambiguated by people; • Not solvable by simple linguistic techniques; • No obvious"
Y14-1042,J03-4003,0,\N,Missing
Y14-1049,N04-1015,0,0.547874,"in each line, subject to metrical constraints. Moreover, they generated English love poetry and translated Italian poetry into English following a user-defined rhythmic scheme. Although these works capture lyric-specific characteristics to some extent, the structural relations are limited to lines or local word contexts. To the best of our knowledge, no existing method accounts for the semantic relations among large structures, such as verses and choruses. Inter-text structural relations are frequently considered in text summarization and conversation modeling. The summarization technique of Barzilay and Lee (2004) captures topic transitions in the text span by a hidden Markov model (HMM), referred to as a content model. Using HMM and a large amount !423 of tweet data, Ritter et al. (2010) and Higashinaka et al. (2011) modeled the transition of speech acts in an unsupervised manner. 3 Survey on Lyric Writing Techniques To create a comprehensive model for lyrics generation, we first investigated the characteristics or rules by which people proceed with lyrics writing in general. We surveyed five textbooks on Japanese lyrics writing (Endo, 2005; Takada, 2007; Aku, 2009; Ueda, 2010; Taguchi, 2012) and iden"
Y14-1049,D10-1051,0,0.174794,"Missing"
Y14-1049,W14-0901,0,0.0634655,"Missing"
Y14-1049,D12-1054,0,0.0483189,"Missing"
Y14-1049,W09-2006,0,0.337993,"neration models that capture the topic transitions between units peculiar to the lyrics, such as verse/chorus and line. These transitions are modeled by a Hidden Markov Model (HMM) for representing topics and topic transitions. However, lyrics generation has yet to be thoroughly explored in the natural language processing field. While several works have tackled lyrics generation based on lyric-specific characteristics, current methods are limited to local contexts, such as single sentences, which cannot capture the overall structure of the generated lyrics (Barbieri et al., 2012; Ramakrishnan A et al., 2009; Reddy and Knight, 2011; Wu et al., 2013; Greene et al., 2010). To verify that our models generate contextsuitable lyrics, we evaluate the models using a log probability of lyrics generation and fill-in-the-blanks-type test. The results show that the language model is far more effective than HMM-based models, but the HMM-based approach successfully captures the inter-verse/chorus and inter-line relations. In the result of experimental evaluation, our approach captures the inter-verse/chorus and inter-line relations. 1 Introduction Recent music creation technologies such as digital audio works"
Y14-1049,P11-2014,0,0.671624,"that capture the topic transitions between units peculiar to the lyrics, such as verse/chorus and line. These transitions are modeled by a Hidden Markov Model (HMM) for representing topics and topic transitions. However, lyrics generation has yet to be thoroughly explored in the natural language processing field. While several works have tackled lyrics generation based on lyric-specific characteristics, current methods are limited to local contexts, such as single sentences, which cannot capture the overall structure of the generated lyrics (Barbieri et al., 2012; Ramakrishnan A et al., 2009; Reddy and Knight, 2011; Wu et al., 2013; Greene et al., 2010). To verify that our models generate contextsuitable lyrics, we evaluate the models using a log probability of lyrics generation and fill-in-the-blanks-type test. The results show that the language model is far more effective than HMM-based models, but the HMM-based approach successfully captures the inter-verse/chorus and inter-line relations. In the result of experimental evaluation, our approach captures the inter-verse/chorus and inter-line relations. 1 Introduction Recent music creation technologies such as digital audio workstations and singing voic"
Y14-1049,N10-1020,0,0.130667,"hough these works capture lyric-specific characteristics to some extent, the structural relations are limited to lines or local word contexts. To the best of our knowledge, no existing method accounts for the semantic relations among large structures, such as verses and choruses. Inter-text structural relations are frequently considered in text summarization and conversation modeling. The summarization technique of Barzilay and Lee (2004) captures topic transitions in the text span by a hidden Markov model (HMM), referred to as a content model. Using HMM and a large amount !423 of tweet data, Ritter et al. (2010) and Higashinaka et al. (2011) modeled the transition of speech acts in an unsupervised manner. 3 Survey on Lyric Writing Techniques To create a comprehensive model for lyrics generation, we first investigated the characteristics or rules by which people proceed with lyrics writing in general. We surveyed five textbooks on Japanese lyrics writing (Endo, 2005; Takada, 2007; Aku, 2009; Ueda, 2010; Taguchi, 2012) and identified the common features as follows. 3.1 Consistency of Entire Lyrics The lyrics preferably follow a consistent theme. Authors usually desire to convey a message in their lyric"
Y14-1049,D13-1011,0,0.0554698,"ransitions between units peculiar to the lyrics, such as verse/chorus and line. These transitions are modeled by a Hidden Markov Model (HMM) for representing topics and topic transitions. However, lyrics generation has yet to be thoroughly explored in the natural language processing field. While several works have tackled lyrics generation based on lyric-specific characteristics, current methods are limited to local contexts, such as single sentences, which cannot capture the overall structure of the generated lyrics (Barbieri et al., 2012; Ramakrishnan A et al., 2009; Reddy and Knight, 2011; Wu et al., 2013; Greene et al., 2010). To verify that our models generate contextsuitable lyrics, we evaluate the models using a log probability of lyrics generation and fill-in-the-blanks-type test. The results show that the language model is far more effective than HMM-based models, but the HMM-based approach successfully captures the inter-verse/chorus and inter-line relations. In the result of experimental evaluation, our approach captures the inter-verse/chorus and inter-line relations. 1 Introduction Recent music creation technologies such as digital audio workstations and singing voice synthesizers (K"
Y15-1012,C12-1002,0,0.0248089,"Missing"
Y15-1012,D11-1142,0,0.0159856,"ions because A PPROX -PMI PACLIC 29 would take 7,441 hours (about a year) to calculate all pattern similarity with one thread. We conclude that it is necessary to prepare low dimensional feature vectors using dimension reduction or word vectors for completing similarity calculation in a realistic time. 4 Related work Unsupervised relation extraction poses three major challenges: extraction of relation instances, representing the meaning of relational patterns, and efficient similarity computation. A great number of studies proposed methods for extracting relation instances (Wu and Weld, 2010; Fader et al., 2011; Fader et al., 2011; Akbik et al., 2012). We do not describe the detail of these studies, which are out of the scope of this paper. Previous studies explored various approaches to represent the meaning of relational patterns (Lin and Pantel, 2001; Yao et al., 2012; Mikolov et al., 2013). Lin and Pantel (2001) used co-occurrence statistics of PMI between an entity and a relational pattern. Even though the goal of their research is not on relation extraction but on paraphrase (inference rule) discovery, the work had a great impact to the research on unsupervised relation extraction. Yao et al."
Y15-1012,D12-1098,0,0.0699727,"Missing"
Y15-1012,P04-1053,0,0.688367,"recent years, to acquire a wider range knowledge, Open Information Extraction (Open IE) has received much attention (Banko et al., 2007). Open IE identifies relational patterns and instances automatically without predefined target relations (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Mausam et al., 2012). In other words, Open IE acquires knowledge to handle open domains. In Open IE paradigm, it is necessary to enumerate semantic relations in open domains and to learn mappings between surface patterns and semantic relations. This task is called unsupervised relation extraction (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Rosenfeld and Feldman, 2007). A common approach to unsupervised relation extraction builds clusters of patterns expressing the same relation. In order to obtain clusters of relational patterns of good quality, we have two major challenges: the semantic representation of relational patterns and the scalability to large data. In this paper, we explore various methods for modeling the meaning of a pattern and for computing the similarity of patterns mined from huge data. In order to achieve this goal, we apply algorithms for approximate frequency counting and efficien"
Y15-1012,D12-1048,0,0.0153259,"pus, and extract noun phrases occurring no less than 1,000 times as a set of entities. 2.2.2 Extracting entity pairs After determining a set of entities, we discover entity pairs that may have semantic relationships in order to locate relational patterns. In this study, we extract a pair of entities if the entities co-occur in more than 5,000 sentences. We denote the set of entity pairs extracted by this procedure E. 2.2.3 Extracting patterns As a relational pattern, this study employs the shortest path between two entities in a dependency tree, following the previous work (Wu and Weld, 2010; Mausam et al., 2012; Akbik et al., 2012). Here, we introduce a restriction that a relational pattern must include a predicate in order to reject semantically1 The threshold values are 10 (first time), 5 (second time), and 0 (third and fourth times). PACLIC 29 nsubj tity pair e ∈ E. P MI refines the strength of cooccurrences with this equation, prep_in dobj det Kafka wrote The Metamorphosis in Germany. PMI(p, e) = log entity nsubj dobj X wrote Y nsubj prep_in X wrote dobj Y prep_in X wrote Y Figure 2: Example of parsed sentence and extracting patterns ambiguous patterns such as “X of Y”. Additionally, we convert"
Y15-1012,D12-1094,0,0.0858617,"ledge base consisting of instances of semantic relations (e.g., authorOf) such as authorOf ⟨Franz Kafka, The Metamorphosis⟩. To recognize these instances in a corpus, we need to obtain patterns (e.g., “X write Y”) that signal instances of the semantic relations. For a long time, many researches have targeted at extracting instances and patterns of specific relations (Riloff, 1996; Pantel and Pennacchiotti, 2006; A common approach to unsupervised relation extraction builds clusters of patterns that represent the same relation (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Yao et al., 2011; Min et al., 2012; Rosenfeld and Feldman, 2007; Nakashole et al., 2012). In brief, each cluster includes patterns corresponding to a semantic relation. For example, consider three patterns, “X write Y”, “X is author of Y” and “X is located in Y”. When we group these patterns into clusters representing the same relation, patterns “X write Y” and “X is author of Y” form a cluster representing the relation authorOf, and the pattern “X is located in Y” does a cluster for locatedIn. In order to obtain these clusters, we need to know the similarity between patterns. The better we model the similarity of patterns, th"
Y15-1012,D12-1104,0,0.145691,"relations (e.g., authorOf) such as authorOf ⟨Franz Kafka, The Metamorphosis⟩. To recognize these instances in a corpus, we need to obtain patterns (e.g., “X write Y”) that signal instances of the semantic relations. For a long time, many researches have targeted at extracting instances and patterns of specific relations (Riloff, 1996; Pantel and Pennacchiotti, 2006; A common approach to unsupervised relation extraction builds clusters of patterns that represent the same relation (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Yao et al., 2011; Min et al., 2012; Rosenfeld and Feldman, 2007; Nakashole et al., 2012). In brief, each cluster includes patterns corresponding to a semantic relation. For example, consider three patterns, “X write Y”, “X is author of Y” and “X is located in Y”. When we group these patterns into clusters representing the same relation, patterns “X write Y” and “X is author of Y” form a cluster representing the relation authorOf, and the pattern “X is located in Y” does a cluster for locatedIn. In order to obtain these clusters, we need to know the similarity between patterns. The better we model the similarity of patterns, the better a clustering result correspond to semantic re"
Y15-1012,P06-1015,0,0.0533433,"ential for many NLP applications such as question answering, textual inference and information extraction (Ravichandran and Hovy, 2002; Szpektor et al., 2004). Therefore, it is important to build a comprehensive knowledge base consisting of instances of semantic relations (e.g., authorOf) such as authorOf ⟨Franz Kafka, The Metamorphosis⟩. To recognize these instances in a corpus, we need to obtain patterns (e.g., “X write Y”) that signal instances of the semantic relations. For a long time, many researches have targeted at extracting instances and patterns of specific relations (Riloff, 1996; Pantel and Pennacchiotti, 2006; A common approach to unsupervised relation extraction builds clusters of patterns that represent the same relation (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Yao et al., 2011; Min et al., 2012; Rosenfeld and Feldman, 2007; Nakashole et al., 2012). In brief, each cluster includes patterns corresponding to a semantic relation. For example, consider three patterns, “X write Y”, “X is author of Y” and “X is located in Y”. When we group these patterns into clusters representing the same relation, patterns “X write Y” and “X is author of Y” form a cluster representing the relation authorOf"
Y15-1012,P02-1006,0,0.0685394,"ing the meaning of a pattern and for computing the similarity of patterns mined from huge data. In order to achieve this goal, we apply algorithms for approximate frequency counting and efficient dimension reduction to unsupervised relation extraction. The experimental results show that approximate frequency counting and dimension reduction not only speeds up similarity computation but also improves the quality of pattern vectors. 1 Introduction Semantic relations between entities are essential for many NLP applications such as question answering, textual inference and information extraction (Ravichandran and Hovy, 2002; Szpektor et al., 2004). Therefore, it is important to build a comprehensive knowledge base consisting of instances of semantic relations (e.g., authorOf) such as authorOf ⟨Franz Kafka, The Metamorphosis⟩. To recognize these instances in a corpus, we need to obtain patterns (e.g., “X write Y”) that signal instances of the semantic relations. For a long time, many researches have targeted at extracting instances and patterns of specific relations (Riloff, 1996; Pantel and Pennacchiotti, 2006; A common approach to unsupervised relation extraction builds clusters of patterns that represent the s"
Y15-1012,N13-1008,0,0.262045,"major challenges in computing the similarity of patterns. First, it is not clear how to represent the semantic meaning of a relational pattern. Previous studies define a feature space for patterns, and express the meaning of patterns by using such as the co-occurrence statistics between a pattern and an entity pair, e.g., co-occurrence frequency and pointwise mutual information (PMI) (Lin and Pantel, 2001). Some studies employed vector representations of a fixed dimension, e.g., Principal Component Analysis (PCA) (Collins et al., 2002) and Latent Dirichlet Allocation (LDA) (Yao et al., 2011; Riedel et al., 2013). However, the previous work did not compare the effectiveness of these representations when applied to a collection of large-scaled unstructured texts. Second, we need design a method scalable to a large data. In Open IE, we utilize a large amount of data in order to improve the quality of unsupervised relation extraction. For this reason, we cannot use a complex and inefficient algorithm that consumes the computation time and memory storage. In this paper, we explore methods for computing pattern similarity of good quality that are scalable to huge data, for example, with several billion sen"
Y15-1012,N06-1039,0,0.189579,"e a wider range knowledge, Open Information Extraction (Open IE) has received much attention (Banko et al., 2007). Open IE identifies relational patterns and instances automatically without predefined target relations (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Mausam et al., 2012). In other words, Open IE acquires knowledge to handle open domains. In Open IE paradigm, it is necessary to enumerate semantic relations in open domains and to learn mappings between surface patterns and semantic relations. This task is called unsupervised relation extraction (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Rosenfeld and Feldman, 2007). A common approach to unsupervised relation extraction builds clusters of patterns expressing the same relation. In order to obtain clusters of relational patterns of good quality, we have two major challenges: the semantic representation of relational patterns and the scalability to large data. In this paper, we explore various methods for modeling the meaning of a pattern and for computing the similarity of patterns mined from huge data. In order to achieve this goal, we apply algorithms for approximate frequency counting and efficient dimension reduction to un"
Y15-1012,W04-3206,0,0.0525221,"and for computing the similarity of patterns mined from huge data. In order to achieve this goal, we apply algorithms for approximate frequency counting and efficient dimension reduction to unsupervised relation extraction. The experimental results show that approximate frequency counting and dimension reduction not only speeds up similarity computation but also improves the quality of pattern vectors. 1 Introduction Semantic relations between entities are essential for many NLP applications such as question answering, textual inference and information extraction (Ravichandran and Hovy, 2002; Szpektor et al., 2004). Therefore, it is important to build a comprehensive knowledge base consisting of instances of semantic relations (e.g., authorOf) such as authorOf ⟨Franz Kafka, The Metamorphosis⟩. To recognize these instances in a corpus, we need to obtain patterns (e.g., “X write Y”) that signal instances of the semantic relations. For a long time, many researches have targeted at extracting instances and patterns of specific relations (Riloff, 1996; Pantel and Pennacchiotti, 2006; A common approach to unsupervised relation extraction builds clusters of patterns that represent the same relation (Hasegawa e"
Y15-1012,P10-1013,0,0.113866,"phrases in the corpus, and extract noun phrases occurring no less than 1,000 times as a set of entities. 2.2.2 Extracting entity pairs After determining a set of entities, we discover entity pairs that may have semantic relationships in order to locate relational patterns. In this study, we extract a pair of entities if the entities co-occur in more than 5,000 sentences. We denote the set of entity pairs extracted by this procedure E. 2.2.3 Extracting patterns As a relational pattern, this study employs the shortest path between two entities in a dependency tree, following the previous work (Wu and Weld, 2010; Mausam et al., 2012; Akbik et al., 2012). Here, we introduce a restriction that a relational pattern must include a predicate in order to reject semantically1 The threshold values are 10 (first time), 5 (second time), and 0 (third and fourth times). PACLIC 29 nsubj tity pair e ∈ E. P MI refines the strength of cooccurrences with this equation, prep_in dobj det Kafka wrote The Metamorphosis in Germany. PMI(p, e) = log entity nsubj dobj X wrote Y nsubj prep_in X wrote dobj Y prep_in X wrote Y Figure 2: Example of parsed sentence and extracting patterns ambiguous patterns such as “X of Y”. Addi"
Y15-1012,D11-1135,0,0.373896,"comprehensive knowledge base consisting of instances of semantic relations (e.g., authorOf) such as authorOf ⟨Franz Kafka, The Metamorphosis⟩. To recognize these instances in a corpus, we need to obtain patterns (e.g., “X write Y”) that signal instances of the semantic relations. For a long time, many researches have targeted at extracting instances and patterns of specific relations (Riloff, 1996; Pantel and Pennacchiotti, 2006; A common approach to unsupervised relation extraction builds clusters of patterns that represent the same relation (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Yao et al., 2011; Min et al., 2012; Rosenfeld and Feldman, 2007; Nakashole et al., 2012). In brief, each cluster includes patterns corresponding to a semantic relation. For example, consider three patterns, “X write Y”, “X is author of Y” and “X is located in Y”. When we group these patterns into clusters representing the same relation, patterns “X write Y” and “X is author of Y” form a cluster representing the relation authorOf, and the pattern “X is located in Y” does a cluster for locatedIn. In order to obtain these clusters, we need to know the similarity between patterns. The better we model the similari"
Y15-1012,P12-1075,0,0.0181752,"ity calculation in a realistic time. 4 Related work Unsupervised relation extraction poses three major challenges: extraction of relation instances, representing the meaning of relational patterns, and efficient similarity computation. A great number of studies proposed methods for extracting relation instances (Wu and Weld, 2010; Fader et al., 2011; Fader et al., 2011; Akbik et al., 2012). We do not describe the detail of these studies, which are out of the scope of this paper. Previous studies explored various approaches to represent the meaning of relational patterns (Lin and Pantel, 2001; Yao et al., 2012; Mikolov et al., 2013). Lin and Pantel (2001) used co-occurrence statistics of PMI between an entity and a relational pattern. Even though the goal of their research is not on relation extraction but on paraphrase (inference rule) discovery, the work had a great impact to the research on unsupervised relation extraction. Yao et al. (2012) modeled sentence themes and document themes by using LDA, and represented the meaning of a pattern with the themes together with the co-occurrence statistics between patterns and entities. Recently, methods inspired by neural language modeling received much"
Y15-1013,P14-2133,0,0.210642,"komatsu, tianran, okazaki, inui}@ecei.tohoku.ac.jp Abstract s0w The high-dimensionality of lexical features in parsing can be memory consuming and cause over-fitting problems. We propose a general framework to replace all lexical feature templates by low-dimensional features induced from word embeddings. Applied to a near state-of-the-art dependency parser (Huang et al., 2012), our method improves the baseline, performs better than using cluster bit string features, and outperforms a recent neural network based parser. A further analysis shows that our framework has the effect hypothesized by Andreas and Klein (2014), namely (i) connecting unseen words to known ones, and (ii) encouraging common behaviors among invocabulary words. 1 Weights: Templates: q0w Lexicon: … saw look … Lexicon: … you me … s0wsaw = (… 1 0 …) q0wyou = (… 1 0 …) s0wlook = (… 0 1 …) q0wme = (… 0 1 …) … … Replace lexical feature templates by embedding features ⋮ W(s0wsaw) W(s0wlook) · = Scores ⋮ W(q0wyou) W(q0wme) ⋮ Features: s0e1, … , s0ed q0e1, … , q0ed s0esaw = (0.6, … , 0.2) q0eyou = (0.5, … , 0.8) s0elook = (0.4, … , 0.3) q0eme = (0.7, … , 0.9) … … · W(s0e1) ⋮ W(s0ed) = Scores W(q0e1) ⋮ W(q0ed) Figure 1: Each lexical feature templ"
Y15-1013,P14-2131,0,0.238473,"rent word embeddings trained from unlabeled or automatically labeled corpora. We expect word embeddings to augment parsing accuracy, by the mechanism hypothesized in Andreas and Klein (2014), namely (i) to connect unseen words to known ones, and (ii) to encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008; Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014). 2 Related Work A lot of recent work has been done on training word vectors (Mnih and Hinton, 2009; Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al., 2014), and utilizing word vectors in various NLP tasks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature spa"
Y15-1013,D12-1091,0,0.0150916,"elated to the parsing model of the baseline, which implies that the basein Chicago, Ohio, and a fewwould other cities . simline...parser and Columbus, our modified parser have ilar behaviors. This may explain the significance 1 1: though our improveresults reported in Table ments against the baseline is fairly moderate, they are still statistically significant because our modified parser behaves similarly as the baseline parser, but would correct the mistakes made by the baseline while preserving most originally correct labels. Such improvements are easier to achieve statistical significance (Berg-Kirkpatrick et al., 2012), and are arguably indicating better generalization. So how does our modified parser improve from the baseline? In Figure 4, we plot cosine similarities between word vectors as X, and cosine similarities between weight vectors of all one-word lexical features as Y , compared to the similarities of weights of the corresponding embedding features. The plots show that, for similar words, the learned weights for the corresponding lexical features are only slightly similar; but after the lexical features are reduced to low-dimensional embedding features, the learned weights for the corresponding fe"
Y15-1013,D14-1082,0,0.184568,"expect word embeddings to augment parsing accuracy, by the mechanism hypothesized in Andreas and Klein (2014), namely (i) to connect unseen words to known ones, and (ii) to encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008; Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014). 2 Related Work A lot of recent work has been done on training word vectors (Mnih and Hinton, 2009; Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al., 2014), and utilizing word vectors in various NLP tasks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddin"
Y15-1013,P15-1033,0,0.0114541,"2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddings. To our own surprise, though the feature space gets much smaller, the resulted system performs better. Another stream of research is to use word embeddings in whole neural network architectures (Collobert et al., 2011; Socher et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Watanabe and Sumita, 2015). Though this is a promising direction and has brought breakthroughs in the field, the question is left open on what exactly 107 s3 s2 stack s1 s0 saw q0 you s0l q1 q2 her with queue I Figure 2: An internal state of a dependency parser. has contributed to the power of neural based approaches. In this work, we conjecture that the power may partly come from the low-dimensionality of word embeddings, and this advantage can be transferred to traditional feature based systems. Our experiments support this conjecture, and we expect the proposed method to help more mature,"
Y15-1013,P10-1110,0,0.182598,"ny NLP tasks, but the very highdimensional feature space brought by these features can be memory consuming and cause over-fitting problems. Is it possible to use low-dimensional word embeddings to reduce the high-dimensionality of lexical features? In this paper, we propose a general framework for this purpose. As a proof of concept, we apply the framework to dependency parsing, since this is a task where lexical features are essential. Our approach is illustrated in Figure 1. Consider a transition-based dependency parser (Yamada and Matsumoto, 2003; Nivre et al., 2006; Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011), in which the words on top of the stack and the queue (denoted by s0 w and q0 w, respectively) are typically used as features to calculate scores of transitions. When s0 w is used as a feature template, the features in this template (e.g. s0 wsaw and s0 wlook ) can be viewed as one-hot vectors of a dimension of the lexicon size (Figure 1). Corresponding to s0 w, a weight is assigned to each word (e.g. W (s0 wsaw ) and W (s0 wlook )) for calculating a transition score. Instead, we propose to utilize a d-dimensional word embedding, and replace the feature template s0 w b"
Y15-1013,N12-1015,0,0.302065,"lexicon size assigned to s0 w, now we use d 106 29th Pacific Asia Conference on Language, Information and Computation pages 106 - 113 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Hiroya Komatsu, Ran Tian, Naoaki Okazaki and Kentaro Inui PACLIC 29 weights (i.e., W (s0 e1 ), . . . , W (s0 ed )) to calculate a transition score. In this work, we reduce feature space dimensionality by replacing all lexical features, including combined features such as s0 wq0 w, by the word embedding features. In experiments, we applied the framework to a near state-of-the-art dependency parser (Huang et al., 2012), evaluated different vector operations for replacing combined lexical features, and explored different word embeddings trained from unlabeled or automatically labeled corpora. We expect word embeddings to augment parsing accuracy, by the mechanism hypothesized in Andreas and Klein (2014), namely (i) to connect unseen words to known ones, and (ii) to encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our meth"
Y15-1013,P08-1068,0,0.274886,"and explored different word embeddings trained from unlabeled or automatically labeled corpora. We expect word embeddings to augment parsing accuracy, by the mechanism hypothesized in Andreas and Klein (2014), namely (i) to connect unseen words to known ones, and (ii) to encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008; Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014). 2 Related Work A lot of recent work has been done on training word vectors (Mnih and Hinton, 2009; Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al., 2014), and utilizing word vectors in various NLP tasks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a r"
Y15-1013,E14-1051,0,0.0192615,"to known ones, and (ii) to encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008; Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014). 2 Related Work A lot of recent work has been done on training word vectors (Mnih and Hinton, 2009; Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al., 2014), and utilizing word vectors in various NLP tasks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddings. To our own surprise, though the feature space gets much smaller, the resulted system performs better. Another stream of research is to use word e"
Y15-1013,P14-1130,0,0.0224427,"d to help more mature, proven-towork existing systems. Machine learning techniques have been proposed for reducing model size and imposing feature sparsity (Suzuki et al., 2011; Yogatama and Smith, 2014). Compared to these methods, our approach is simple, without extra twists of objective functions or learning algorithms. More importantly, by using word embeddings to reduce lexical features, we explicitly exploit the inherited syntactic and semantic similarities between words. Another technique to reduce features is dimension reduction by matrix or tensor factorization (Argyriou et al., 2007; Lei et al., 2014), but typically applied to supervised learning. In contrast, we use word embeddings trained from unlabeled or automatically labeled corpora, bringing the aspects of semi-supervised learning or self-training. 3 Formalization In this section, we formalize the framework of reducing lexical features. We take transition-based parsing as an example, but the framework can be applied to other systems using lexical features. 3.1 Transition-based Parsing In typical transition-based parsing, input words are put into a queue and partially built parse trees are cached in a stack (Figure 2). At each step, a"
Y15-1013,P14-2050,0,0.022694,"): Random 86.37 86.19 81.06 P LAIN 90.68 90.48 87.02 T REE 91.06 90.82 87.38 S TATE 91.03 90.57 87.88 Table 1: Parsing Results (UAS). Numbers marked by asterisk (∗ ) are statistically significant (p &lt; 0.05), compared to the baseline (Huang et al., 2012) under a paired bootstrap test. of the embeddings but unseen in PTB training data (Unseen). We built 300 dimensional word embeddings from 6 months articles in New York Times Corpus4 (01/2007-06/2007, 1.5M sentences), for words of frequencies greater than 50. Word vectors are obtained from singular value decomposition (SVD) of the PPMI matrices (Levy and Goldberg, 2014b), for co-occurrence matrices of target words with various types of contexts (Levy and Goldberg, 2014a), to be specified later. We choose SVD for training word vectors because it is fast; and recent research suggests that SVD can perform as well as other embedding methods (Levy et al., 2015). We investigated the following types of contexts for training word vectors: P LAIN, which uses words within a window of 3 to each side of the target word as contexts; T REE, which uses words within 3 steps of the target in the dependency trees, obtained from applying Huang et al. (2012)’s parser to the co"
Y15-1013,Q15-1016,0,0.0747407,"Missing"
Y15-1013,W06-2933,0,0.0976665,"Missing"
Y15-1013,J08-4003,0,0.0284044,"f-training. 3 Formalization In this section, we formalize the framework of reducing lexical features. We take transition-based parsing as an example, but the framework can be applied to other systems using lexical features. 3.1 Transition-based Parsing In typical transition-based parsing, input words are put into a queue and partially built parse trees are cached in a stack (Figure 2). At each step, a shiftreduce action is selected, which consumes words from the queue and/or build new structures in the stack. For the set of actions, we adopt the arcstandard system (Yamada and Matsumoto, 2003; Nivre, 2008; Huang and Sagae, 2010), in which the actions are: PACLIC 29 1. Shift, which pops the top of the queue and pushes it to the stack; 2. Reduce-Left, which replaces the top two trees in the stack by their consolidated tree, left as child; d-dimensional vector representation of the word w, where vi is the i-th entry. Then, we replace sw by se, a linear combination of se1 , . . . , sed : se := d X vi · (sei ). i=1 3. Reduce-Right, which replaces the top two trees in the stack by their consolidated tree, right as child. Following Huang et al. (2012), we use the maxviolation perceptron for global le"
Y15-1013,D14-1162,0,0.0866269,"encourage common behaviors among in-vocabulary words. In contrast to the negative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008; Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014). 2 Related Work A lot of recent work has been done on training word vectors (Mnih and Hinton, 2009; Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al., 2014), and utilizing word vectors in various NLP tasks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddings. To our own surprise, though the feature space gets much smaller, the resulted system performs better. Another stream of research is to use word embeddings in whole neural"
Y15-1013,P13-1045,0,0.0371597,"ks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddings. To our own surprise, though the feature space gets much smaller, the resulted system performs better. Another stream of research is to use word embeddings in whole neural network architectures (Collobert et al., 2011; Socher et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Watanabe and Sumita, 2015). Though this is a promising direction and has brought breakthroughs in the field, the question is left open on what exactly 107 s3 s2 stack s1 s0 saw q0 you s0l q1 q2 her with queue I Figure 2: An internal state of a dependency parser. has contributed to the power of neural based approaches. In this work, we conjecture that the power may partly come from the low-dimensionality of word embeddings, and this advantage can be transferred to traditional feature based systems. Our experiments support this con"
Y15-1013,P11-2112,0,0.0227465,"n on what exactly 107 s3 s2 stack s1 s0 saw q0 you s0l q1 q2 her with queue I Figure 2: An internal state of a dependency parser. has contributed to the power of neural based approaches. In this work, we conjecture that the power may partly come from the low-dimensionality of word embeddings, and this advantage can be transferred to traditional feature based systems. Our experiments support this conjecture, and we expect the proposed method to help more mature, proven-towork existing systems. Machine learning techniques have been proposed for reducing model size and imposing feature sparsity (Suzuki et al., 2011; Yogatama and Smith, 2014). Compared to these methods, our approach is simple, without extra twists of objective functions or learning algorithms. More importantly, by using word embeddings to reduce lexical features, we explicitly exploit the inherited syntactic and semantic similarities between words. Another technique to reduce features is dimension reduction by matrix or tensor factorization (Argyriou et al., 2007; Lei et al., 2014), but typically applied to supervised learning. In contrast, we use word embeddings trained from unlabeled or automatically labeled corpora, bringing the aspec"
Y15-1013,P10-1040,0,0.0610323,"gative results reported in Andreas and Klein (2014), we find that our framework indeed has these effects, and significantly improves the baseline. As a comparison, our method performs better than the technique of replacing words by cluster bit strings (Koo et al., 2008; Bansal et al., 2014), and the results outperform a neural network based parser (Chen and Manning, 2014). 2 Related Work A lot of recent work has been done on training word vectors (Mnih and Hinton, 2009; Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al., 2014), and utilizing word vectors in various NLP tasks (Turian et al., 2010; Andreas and Klein, 2014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddings. To our own surprise, though the feature space gets much smaller, the resulted system performs better. Another stream of research is to use word embeddings in whole neural network architectures (Collobert et al., 2011; Socher et al., 2013; Ch"
Y15-1013,P15-1113,0,0.0203522,"pproach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddings. To our own surprise, though the feature space gets much smaller, the resulted system performs better. Another stream of research is to use word embeddings in whole neural network architectures (Collobert et al., 2011; Socher et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Watanabe and Sumita, 2015). Though this is a promising direction and has brought breakthroughs in the field, the question is left open on what exactly 107 s3 s2 stack s1 s0 saw q0 you s0l q1 q2 her with queue I Figure 2: An internal state of a dependency parser. has contributed to the power of neural based approaches. In this work, we conjecture that the power may partly come from the low-dimensionality of word embeddings, and this advantage can be transferred to traditional feature based systems. Our experiments support this conjecture, and we expect the proposed method to help more mature, proven-towork existing syst"
Y15-1013,P15-1032,0,0.0112835,"014; Bansal et al., 2014). The common approach (Turian et al., 2010; Koo et al., 2008; Bansal et al., 2014) is to use vector representations in new features, added to (near) state-of-the-art systems, and make improvement. As a result, the feature space gets even larger. We instead propose to reduce lexical features by word embeddings. To our own surprise, though the feature space gets much smaller, the resulted system performs better. Another stream of research is to use word embeddings in whole neural network architectures (Collobert et al., 2011; Socher et al., 2013; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Watanabe and Sumita, 2015). Though this is a promising direction and has brought breakthroughs in the field, the question is left open on what exactly 107 s3 s2 stack s1 s0 saw q0 you s0l q1 q2 her with queue I Figure 2: An internal state of a dependency parser. has contributed to the power of neural based approaches. In this work, we conjecture that the power may partly come from the low-dimensionality of word embeddings, and this advantage can be transferred to traditional feature based systems. Our experiments support this conjecture, and we expect the proposed method t"
Y15-1013,W03-3023,0,0.392446,"ction Lexical features are powerful machine learning ingredients for many NLP tasks, but the very highdimensional feature space brought by these features can be memory consuming and cause over-fitting problems. Is it possible to use low-dimensional word embeddings to reduce the high-dimensionality of lexical features? In this paper, we propose a general framework for this purpose. As a proof of concept, we apply the framework to dependency parsing, since this is a task where lexical features are essential. Our approach is illustrated in Figure 1. Consider a transition-based dependency parser (Yamada and Matsumoto, 2003; Nivre et al., 2006; Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011), in which the words on top of the stack and the queue (denoted by s0 w and q0 w, respectively) are typically used as features to calculate scores of transitions. When s0 w is used as a feature template, the features in this template (e.g. s0 wsaw and s0 wlook ) can be viewed as one-hot vectors of a dimension of the lexicon size (Figure 1). Corresponding to s0 w, a weight is assigned to each word (e.g. W (s0 wsaw ) and W (s0 wlook )) for calculating a transition score. Instead, we propose to utilize a d-d"
Y15-1013,P14-1074,0,0.0223262,"s3 s2 stack s1 s0 saw q0 you s0l q1 q2 her with queue I Figure 2: An internal state of a dependency parser. has contributed to the power of neural based approaches. In this work, we conjecture that the power may partly come from the low-dimensionality of word embeddings, and this advantage can be transferred to traditional feature based systems. Our experiments support this conjecture, and we expect the proposed method to help more mature, proven-towork existing systems. Machine learning techniques have been proposed for reducing model size and imposing feature sparsity (Suzuki et al., 2011; Yogatama and Smith, 2014). Compared to these methods, our approach is simple, without extra twists of objective functions or learning algorithms. More importantly, by using word embeddings to reduce lexical features, we explicitly exploit the inherited syntactic and semantic similarities between words. Another technique to reduce features is dimension reduction by matrix or tensor factorization (Argyriou et al., 2007; Lei et al., 2014), but typically applied to supervised learning. In contrast, we use word embeddings trained from unlabeled or automatically labeled corpora, bringing the aspects of semi-supervised learn"
Y15-1013,D08-1059,0,0.0307488,"ning ingredients for many NLP tasks, but the very highdimensional feature space brought by these features can be memory consuming and cause over-fitting problems. Is it possible to use low-dimensional word embeddings to reduce the high-dimensionality of lexical features? In this paper, we propose a general framework for this purpose. As a proof of concept, we apply the framework to dependency parsing, since this is a task where lexical features are essential. Our approach is illustrated in Figure 1. Consider a transition-based dependency parser (Yamada and Matsumoto, 2003; Nivre et al., 2006; Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011), in which the words on top of the stack and the queue (denoted by s0 w and q0 w, respectively) are typically used as features to calculate scores of transitions. When s0 w is used as a feature template, the features in this template (e.g. s0 wsaw and s0 wlook ) can be viewed as one-hot vectors of a dimension of the lexicon size (Figure 1). Corresponding to s0 w, a weight is assigned to each word (e.g. W (s0 wsaw ) and W (s0 wlook )) for calculating a transition score. Instead, we propose to utilize a d-dimensional word embedding, and replace the"
Y15-1013,P11-2033,0,0.0263584,"ery highdimensional feature space brought by these features can be memory consuming and cause over-fitting problems. Is it possible to use low-dimensional word embeddings to reduce the high-dimensionality of lexical features? In this paper, we propose a general framework for this purpose. As a proof of concept, we apply the framework to dependency parsing, since this is a task where lexical features are essential. Our approach is illustrated in Figure 1. Consider a transition-based dependency parser (Yamada and Matsumoto, 2003; Nivre et al., 2006; Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011), in which the words on top of the stack and the queue (denoted by s0 w and q0 w, respectively) are typically used as features to calculate scores of transitions. When s0 w is used as a feature template, the features in this template (e.g. s0 wsaw and s0 wlook ) can be viewed as one-hot vectors of a dimension of the lexicon size (Figure 1). Corresponding to s0 w, a weight is assigned to each word (e.g. W (s0 wsaw ) and W (s0 wlook )) for calculating a transition score. Instead, we propose to utilize a d-dimensional word embedding, and replace the feature template s0 w by d features, namely s0"
Y15-1063,J96-1002,0,0.0224266,"“may” or “likely,” and situation selecting predicates such as “prevent” or “suggest.” They used these cue words to detect polarity (positive, negative, or unknown) and epistemic modality (certain, probable, possible, or unknown) and combined these two values to indicate an event’s factuality. de Marneffe et al. (2012) annotated the veridicality which roughly corresponds to the factuality for each event by ten annotators to the FactBank corpus (Saur´ı and Pustejovsky, 2009) and trained a classifier to predict the probabilistic distribution of event veridicality using a maximum entropy method (Berger et al., 1996). They compared the distributions predicted by the classifier and the distributions annotated by human annotators. Soni et al. (2014) examined the factuality of quoted statements in tweets. They used the cue words defined in Saur´ı (2008) that introduced the quoted event negation or speculation and the tweet’s author. They reported that conducting factuality analysis for quoted statements is quite difficult due to the error rate. PACLIC 29 核爆発 が 起きる なんて nuclear explosion occur caseadverbial noun marker verb particle デマ だ false rumor be noun verb Figure 1: Japanese bunsetsu example 3 Approach O"
Y15-1063,J12-2003,0,0.0367224,"Missing"
Y15-1063,W07-1522,1,0.725345,"hquake (from March 11 2011 to April 11 2011) and instances whose source is artificial in the table were manually composed of tweetlike texts that included typical examples of complex forms of negations to expand the number of complex negations. Note that we also used the training set as the development set for parameter tuning by 5-fold cross-validation. All of the test set instances were extracted from tweets and there was no overlap between the training and test sets. In both sets, each predicate was annotated by a single annotator by the following steps: 1. We annotated predicates based on Iida et al. (2007). All of the verbs and adjectives were annotated as predicates, and some nouns were annotated as nominal predicates. 2. We annotated negation by the negation cues surrounding the predicate. Both such functional expressions as “ない nai (not)” and such content words as “嘘 だ uso da (it is doubtful that)” are used as negation cues. 3. The predicates (interpreted by the annotator as negated by the cues) are annotated as negated predicates and used as positive instances for SVM, and the others are used as negative instances. Note that recognizing negated instances as either simple or complex is done"
Y15-1063,P08-1047,1,0.870758,"f using the n-gram cluster features with three types of corpora. The n-gram cluster feature generated from Twitter outperforms the other 549 two corpora. The “ML+all” column indicates using three corpora at once. These features are distinguished by their sources. It outperforms the other settings of using each corpora. The comparisons between the “ML” and “ML+all” and “Marneffe12” and “ML+all” suggest that n-gram clusters successfully generalize complex negations forms by their cluster IDs. We compare our n-gram clustering method with “noun-cls,” another clustering method that was proposed by Kazama and Torisawa (2008). We applied their clustering algorithm to nouns extracted from roughly six hundred million Web documents. The Web documents that we used for our clustering are a subset of these documents. The variation of words in the noun-cluster is wider than in other clusters. We set the clustering number to 2,000 in our n-gram clustering method and “noun-cls.” Table 5 compares the clustering method and the corpora that we used. We tried eight dimensions (50, 100, 150, 200, 250, 300, 350, and 500), and the number of dimensions in Table 5 achieved the best performance for each corpus. The “noun-cls” column"
Y15-1063,W04-3230,0,0.0293617,"that we modified the k-means clustering of the word2vec tool so that the word vectors are normalized to the length of the vector. Three distinct corpora were given to the word2vec tool: 1. All of the articles from Japanese Wikipedia (revision of 18 Jan. 2015), 2. Web pages crawled in 2007, i.e., about four years before the earthquake, 3. Twitter data posted from Feb. 14 to 28, 2015, i.e., about four years after the earthquake. We randomly sampled sentences for corpora 2 (4.5 GB) and 3 (4.3 GB) to match Wikipedia’s size (4.2 GB). We tokenized each document with a morphological analyzer MeCab (Kudo et al., 2004) and the Juman PoS tag set (Kurohashi et al., 1994) and applied the word2vec tool and k-means clustering. We needed to choose several parameters, including the 3 https://code.google.com/p/word2vec/ 547 numbers of vector dimensions and clusters, whose values were based on 5-fold cross-validation on our annotated training data, as described in Section 4.1. We tried eight dimensions (50, 100, 150, 200, 250, 300, 350, and 500) of vectors and six numbers of clusters (100, 500, 1,000, 2,000 5,000, and 10,000) for each corpus. For the optimal parameters, which worked best in our preliminary experimen"
Y15-1063,D12-1034,1,0.904234,"Missing"
Y15-1063,P14-2068,0,0.0146095,"sitive, negative, or unknown) and epistemic modality (certain, probable, possible, or unknown) and combined these two values to indicate an event’s factuality. de Marneffe et al. (2012) annotated the veridicality which roughly corresponds to the factuality for each event by ten annotators to the FactBank corpus (Saur´ı and Pustejovsky, 2009) and trained a classifier to predict the probabilistic distribution of event veridicality using a maximum entropy method (Berger et al., 1996). They compared the distributions predicted by the classifier and the distributions annotated by human annotators. Soni et al. (2014) examined the factuality of quoted statements in tweets. They used the cue words defined in Saur´ı (2008) that introduced the quoted event negation or speculation and the tweet’s author. They reported that conducting factuality analysis for quoted statements is quite difficult due to the error rate. PACLIC 29 核爆発 が 起きる なんて nuclear explosion occur caseadverbial noun marker verb particle デマ だ false rumor be noun verb Figure 1: Japanese bunsetsu example 3 Approach Our negation recognition algorithm takes an input sentence and classifies each predicate in it as “negated” or “non-negated.” We train"
Y15-1063,D09-1160,0,0.0236669,"a false rumor.” The vertical bars indicate the bunsetsu boundaries. A Japanese dependency tree is defined as a tree of the dependencies among the bunsetsus. In the above example, the first bunsetsu “核爆発 が kakubakuhatsu ga (nuclear explosion)” depends on another bunsetsu “起きる なんて okiru nante (occur),” which in turn depends on “デマ だ dema da (be false rumor)”. The final bunsetsu is an exceptional case in which both a verb and a noun are included unlike the other bunsetsus that contain either a noun or a verb. Note that in this paper, bunsetsu boundaries and a dependency tree are given by J.DepP (Yoshinaga and Kitsuregawa, 2009). 3.1 Baseline Features Basic Uni-, bi-, and tri-grams of words (surface, base form, and part of speech) in the bunsetsu include the target predicate and its head bunsetsu are used as basic features. The words in the two bunsetsus are distinguished in the feature set. If a bunsetsu includes a target predicate, n-grams are taken only from the strings following it. In the above example sentence, bi-gram “デマ だ dema da (be false rumor)” and uni-gram “デマ dema (false rumor)” are included in this feature set when the target predicate is “起きる okiru (occur).” Negation Words We manually created a short"
Y16-2022,J93-2003,0,0.0439778,"e MS COCO dataset contains only 80 object categories (e.g., PERSON or CAR), each object category is referred to by a number of expressions. For example, the object category PERSON can be described by man, person, skateboarder, skate boarder, etc., as shown in Figure 2. Thus, we need to identify the correspondences between objects in an image and their referring expressions in the dataset. In this study, we cast the problem of object-word alignment as a translation task, where the input language is a set of object categories and the output language is a description. Here, we use the IBM Model (Brown et al., 1993) to obtain the translation probability P (w|c), where c denotes an object category in an image and w denotes a word in its description. For instance, the IBM Model gives a higher probability for P (w = man|c = PERSON) after seeing the training instances: PERSON, SKATEBOARD a man is riding a skateboard PERSON, DONUT a man who is eating a donut We use the GIZA++ (Och and Ney, 2003) implementation6 to estimate the alignments. 6 https://github.com/moses-smt/giza-pp PACLIC 30 Proceedings Table 1: Result of object-word alignment. Precision .880 .738 IBM Model WordNet Recall .743 .565 F1 .806 .638 We"
Y16-2022,P15-1005,0,0.0270296,"Missing"
Y16-2022,D13-1128,0,0.163671,"the third sentence in Figure 2 expresses the ride on relation between a man and a skateboard. If we could ground the man with a yellow bounding box (PERSON) and a skateboard with a green box (SKATEBOARD), we could understand the meaning of ride on(o1 , o2 ) relation via the image: the object o1 has a contact with o2 , and o1 is usually located above o2 . Unfortunately, because the MS COCO dataset does not have alignments between images and words in its descriptions, we estimate the alignments, as will be explained in Section 5.1. There are other publicly available datasets, such as the VLT2K (Elliott and Keller, 2013), PASCAL VOC (Everingham et al., 2014), Stanford 40 Actions (Yao et al., 2011), and HICO (Chao et al., 2015). Those datasets contain relation information, although the information restricts only positional or action ones: VLT2K has only positional relations, PASCAL VOC and Stanford 40 Actions contain action relations (e.g., walking and running), and HICO has human-object relations (e.g., riding a bike). We aim at a generic natural image understanding that might involve object-object relationships other than people, and we consider the MS 4 https://creativecommons.org/licenses/ by/4.0/legalcode"
Y16-2022,P14-5010,0,0.00291283,", on(a skateboard/SKATEBOARD, a picnic table/DINING TABLE) Because PERSON, SKATEBOARD, and DINING TABLE are associated with the objects in the image, the two relation instances describe the relations between PERSON and SKATEBOARD objects as ride and between SKATEBOARD and DINING TABLE objects as on. We design a method for extracting relation instances from dependency trees of image descriptions, inspired by the methods for Open Information Extraction (Schmitz et al., 2012; Nakashole et al., 2012; Xu et al., 2013; Moro and Navigli, 2013). We ﬁrst parse a description using the Stanford CoreNLP (Manning et al., 2014)7 . We ﬁnd a set of the longest noun phrases (NPs) whose phrase structures are located at nodes of height no greater than three from their leaves (in blue in Figure 4)8 . 7 We used Stanford CoreNLP 3.5.2. http://stanfordnlp.github.io/CoreNLP/ 8 This ﬁnds noun phrases with four words at most. 243 Templates 1 and 3 extract their example instances from Figure 4. Template 2 is used to extract the example from the sentence, “A man is riding on a skateboard.” In Template 1, we attach a particle (compound:prt) if any to the verb for extracting take off(a man, the hat) from the sentence, “A man is tak"
Y16-2022,D12-1104,0,0.0334189,"wed by a word and a slash. We extract two relation instances from the example: ride(a man/PERSON, a skateboard/SKATEBOARD), on(a skateboard/SKATEBOARD, a picnic table/DINING TABLE) Because PERSON, SKATEBOARD, and DINING TABLE are associated with the objects in the image, the two relation instances describe the relations between PERSON and SKATEBOARD objects as ride and between SKATEBOARD and DINING TABLE objects as on. We design a method for extracting relation instances from dependency trees of image descriptions, inspired by the methods for Open Information Extraction (Schmitz et al., 2012; Nakashole et al., 2012; Xu et al., 2013; Moro and Navigli, 2013). We ﬁrst parse a description using the Stanford CoreNLP (Manning et al., 2014)7 . We ﬁnd a set of the longest noun phrases (NPs) whose phrase structures are located at nodes of height no greater than three from their leaves (in blue in Figure 4)8 . 7 We used Stanford CoreNLP 3.5.2. http://stanfordnlp.github.io/CoreNLP/ 8 This ﬁnds noun phrases with four words at most. 243 Templates 1 and 3 extract their example instances from Figure 4. Template 2 is used to extract the example from the sentence, “A man is riding on a skateboard.” In Template 1, we att"
Y16-2022,J03-1002,0,0.00882758,"Missing"
Y16-2022,D12-1048,0,0.0392453,"ppercase letters followed by a word and a slash. We extract two relation instances from the example: ride(a man/PERSON, a skateboard/SKATEBOARD), on(a skateboard/SKATEBOARD, a picnic table/DINING TABLE) Because PERSON, SKATEBOARD, and DINING TABLE are associated with the objects in the image, the two relation instances describe the relations between PERSON and SKATEBOARD objects as ride and between SKATEBOARD and DINING TABLE objects as on. We design a method for extracting relation instances from dependency trees of image descriptions, inspired by the methods for Open Information Extraction (Schmitz et al., 2012; Nakashole et al., 2012; Xu et al., 2013; Moro and Navigli, 2013). We ﬁrst parse a description using the Stanford CoreNLP (Manning et al., 2014)7 . We ﬁnd a set of the longest noun phrases (NPs) whose phrase structures are located at nodes of height no greater than three from their leaves (in blue in Figure 4)8 . 7 We used Stanford CoreNLP 3.5.2. http://stanfordnlp.github.io/CoreNLP/ 8 This ﬁnds noun phrases with four words at most. 243 Templates 1 and 3 extract their example instances from Figure 4. Template 2 is used to extract the example from the sentence, “A man is riding on a skateboard"
Y16-2022,N13-1107,0,0.0225482,"h. We extract two relation instances from the example: ride(a man/PERSON, a skateboard/SKATEBOARD), on(a skateboard/SKATEBOARD, a picnic table/DINING TABLE) Because PERSON, SKATEBOARD, and DINING TABLE are associated with the objects in the image, the two relation instances describe the relations between PERSON and SKATEBOARD objects as ride and between SKATEBOARD and DINING TABLE objects as on. We design a method for extracting relation instances from dependency trees of image descriptions, inspired by the methods for Open Information Extraction (Schmitz et al., 2012; Nakashole et al., 2012; Xu et al., 2013; Moro and Navigli, 2013). We ﬁrst parse a description using the Stanford CoreNLP (Manning et al., 2014)7 . We ﬁnd a set of the longest noun phrases (NPs) whose phrase structures are located at nodes of height no greater than three from their leaves (in blue in Figure 4)8 . 7 We used Stanford CoreNLP 3.5.2. http://stanfordnlp.github.io/CoreNLP/ 8 This ﬁnds noun phrases with four words at most. 243 Templates 1 and 3 extract their example instances from Figure 4. Template 2 is used to extract the example from the sentence, “A man is riding on a skateboard.” In Template 1, we attach a particle (c"
Y16-2026,P98-1013,0,0.223038,"extraction of knowledge of usable goods. The corpus and guidelines will be available when this paper is presented. (iii) We present our initial attempts toward the automatic extraction of such knowledge using a sequence labeling method. The results in this experiment provide measures to estimate the complexity of this task and suggest future directions to build a large-scale corpus. 2 Related work To our knowledge, there is no resource that focuses on knowledge of usable goods. There are manually constructed and relatively accurate lexical resources such as WordNet (Miller 1995) and FrameNet (Baker et al. 1998), but their coverage is inevitably limited and these ontologies 278 do not contain knowledge of our interest. Current large-scale knowledge bases focus on knowledge of entities and their relations, but the coverage of knowledge of usable goods is still sparse as shown in Section 4.3. OpenIE systems (Etzioni et al. 2011) such as TextRunner (Etzioni et al. 2008) and ReVerb (Fader et al. 2011) extract a large number of relations such as treadmill, burns, more calories using lexico-syntactic patterns from massive corpora drawn from the Web. Though these systems cover a wide variety of relational"
Y16-2026,P07-1112,0,0.0229659,"ormation of usable goods. As for extracting information of objects, there is a body of research on the acquisition of telic and agentive roles in the context of generative lexicon theory (Pustejovsky 1991). Pustejovsky proposes qualia structures that deﬁne prototypical aspects of word’s meaning (Pustejovsky et al. 1993). Of four semantic roles in the qualia structures, the telic role describes the purpose or function of an object (e.g. read is a typical telic role for book). Computational approaches are suggested to automatically extract expressions of this role from text (Yamada et al. 2007, Cimiano and Wenderoth 2007), but these models tend to focus on taking paraphrases of “using X”, rather than the expressions of purpose or function of objects. While the telic roles cover a broader range of expressions (probably due to the unspeciﬁed deﬁnition of telicity in the original theory), our work focuses on eﬀects caused by using/consuming objects, standing as complementary to these previous studies. Information extraction research in biomedical domains concerns eﬀects caused by using drugs such that drug X causes adverse eﬀect Y (Gurulingappa et al. 2012). This kind of information may overlap with what we aim t"
Y16-2026,D11-1142,0,0.0509114,"ed work To our knowledge, there is no resource that focuses on knowledge of usable goods. There are manually constructed and relatively accurate lexical resources such as WordNet (Miller 1995) and FrameNet (Baker et al. 1998), but their coverage is inevitably limited and these ontologies 278 do not contain knowledge of our interest. Current large-scale knowledge bases focus on knowledge of entities and their relations, but the coverage of knowledge of usable goods is still sparse as shown in Section 4.3. OpenIE systems (Etzioni et al. 2011) such as TextRunner (Etzioni et al. 2008) and ReVerb (Fader et al. 2011) extract a large number of relations such as treadmill, burns, more calories using lexico-syntactic patterns from massive corpora drawn from the Web. Though these systems cover a wide variety of relational expressions, they do not intend to extract information of usable goods. As for extracting information of objects, there is a body of research on the acquisition of telic and agentive roles in the context of generative lexicon theory (Pustejovsky 1991). Pustejovsky proposes qualia structures that deﬁne prototypical aspects of word’s meaning (Pustejovsky et al. 1993). Of four semantic roles i"
Y16-2026,P14-5010,0,0.0044019,"ce labeling problem. We use Conditional 282 Random Fields (CRFs), a popular approach to solve sequence labeling problems (Laﬀerty et al. 2001). CRFsuite 7 is used as an implementation of CRF for our purpose. 5.1 Experimental Settings The training and test data consists of 792 sentences from 200 Wikipedia snippets (see Section 4.1). We select the four most frequent labels in the corpus, Effect, Means of use, Composed of and Version, for evaluation. For the data pre-processing, we ﬁrst parse the raw text and assign a part of speech tag and a named entity tag to each word using Stanford CoreNLP (Manning et al. 2014). Then we add a semantic label to each word with BIO format (Beginning, Inside and Outside). 5.2 Features Features shown in Table 5 are used for training. We use these features within a window of ±3 around the current word. Some of these features are used in combination with another feature as shown in Table 5. In addition to standard features, we add three features to exploit the characteristics of this corpus: Target, Disease and Repeat. Target feature is true when the current word is same as the title of Wikipedia article. Disease feature is true when the current word is in a list of diseas"
Y16-2026,J91-4003,0,0.655739,"Missing"
Y16-2026,J93-2005,0,0.402281,"tzioni et al. 2008) and ReVerb (Fader et al. 2011) extract a large number of relations such as treadmill, burns, more calories using lexico-syntactic patterns from massive corpora drawn from the Web. Though these systems cover a wide variety of relational expressions, they do not intend to extract information of usable goods. As for extracting information of objects, there is a body of research on the acquisition of telic and agentive roles in the context of generative lexicon theory (Pustejovsky 1991). Pustejovsky proposes qualia structures that deﬁne prototypical aspects of word’s meaning (Pustejovsky et al. 1993). Of four semantic roles in the qualia structures, the telic role describes the purpose or function of an object (e.g. read is a typical telic role for book). Computational approaches are suggested to automatically extract expressions of this role from text (Yamada et al. 2007, Cimiano and Wenderoth 2007), but these models tend to focus on taking paraphrases of “using X”, rather than the expressions of purpose or function of objects. While the telic roles cover a broader range of expressions (probably due to the unspeciﬁed deﬁnition of telicity in the original theory), our work focuses on eﬀec"
Y16-2026,speer-havasi-2012-representing,0,0.0846175,"Missing"
Y16-2026,E12-2021,0,0.100439,"Missing"
Y16-3027,Y09-1009,0,0.387638,"tion (NER) deﬁned a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set wh"
Y16-3027,D15-1103,0,0.0232597,"ally signiﬁcant improvement separately in classiﬁcation accuracy. 1 Introduction Recognizing named entities (NEs) in text is a crucial component task of a broad range of NLP applications including information extraction and question answering. Early work on named entity recognition (NER) deﬁned a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challen"
Y16-3027,I08-1071,0,0.74711,"small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set whereas all of them fall int"
Y16-3027,doddington-etal-2004-automatic,0,0.0532304,"context of automatic construction of an NE gazetteer 1http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/ jawiki_vector/ PACLIC 30 Proceedings from Wikipedia articles. Toral and Muñoz (2006) proposed a method to classify Wikipedia articles into three NE types (Location, Organization, Person) using words included in the body of the article. They used WordNet as an external knowledge base for collecting hypernym information. They also applied weighted voting heuristics to determine NE types of articles. Dakka and Cucerzan (2008) classiﬁed articles into four NE types (PER, ORG, LOC, MISC) deﬁned in ACE (Doddington et al., 2004) using supervised machine learning algorithms based on SVMs and naive Bayes. They used the bag-of-words in the target article as well as context words from the anchor text linking to the target article. Watanabe et al. (2007) focused on the HTML tree/link structure in Wikipedia articles. They formalized an NE categorization problem as assigning of NE labels to anchor texts in Wikipedia. They constructed graph-based representations of articles and estimated assignments of NE labels over the graphs using conditional random ﬁelds. In addition to these studies, there have been eﬀorts toward automa"
Y16-3027,P98-1068,0,0.0357389,"n in the ﬁrst sentence Headings of the article Direct categories deﬁned in Wikipedia Upper categories deﬁned in Wikipedia • An anchor text is not always identical to the article title to which the anchor refers. For this reason, we need to normalize an anchor text to the title of the article linked by the anchor. able to reproduce features T8, T12, T14, and M22 described in the original paper (Higashinaka et al., 2012) because those features require the authors’ internal resources to implement. For similar reasons, we used MeCab (Kudo et al., 2004) as a morphological analyzer instead of JTAG (Fuchi and Takagi, 1998), which was unavailable to us. For extracting text from Wikipedia dump, we used Wikipedia Extractor (http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor). We denote this baseline feature set as Fb . 4.2.2 Article Vectors To extend the aforementioned basic feature set, we hypothesize that the way how each article (i.e. named entity) is mentioned in other articles can also be a useful clue for classifying that article. To test this hypothesis, we introduce distributed representations of Wikipedia articles. Consider an article “Mount Everest”. This article is hyperlinked from other articles as"
Y16-3027,C12-1071,0,0.253416,"ined entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set whereas all of them fall into the same type Location i"
Y16-3027,P08-1047,0,0.0208216,"Missing"
Y16-3027,W04-3230,0,0.0226318,"haracters in the title Last character type in the title Last noun in the ﬁrst sentence Headings of the article Direct categories deﬁned in Wikipedia Upper categories deﬁned in Wikipedia • An anchor text is not always identical to the article title to which the anchor refers. For this reason, we need to normalize an anchor text to the title of the article linked by the anchor. able to reproduce features T8, T12, T14, and M22 described in the original paper (Higashinaka et al., 2012) because those features require the authors’ internal resources to implement. For similar reasons, we used MeCab (Kudo et al., 2004) as a morphological analyzer instead of JTAG (Fuchi and Takagi, 1998), which was unavailable to us. For extracting text from Wikipedia dump, we used Wikipedia Extractor (http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor). We denote this baseline feature set as Fb . 4.2.2 Article Vectors To extend the aforementioned basic feature set, we hypothesize that the way how each article (i.e. named entity) is mentioned in other articles can also be a useful clue for classifying that article. To test this hypothesis, we introduce distributed representations of Wikipedia articles. Consider an article"
Y16-3027,Q15-1023,0,0.033578,"s a crucial component task of a broad range of NLP applications including information extraction and question answering. Early work on named entity recognition (NER) deﬁned a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, a"
Y16-3027,W02-1111,0,0.0133169,"Missing"
Y16-3027,P13-1146,0,0.0145923,"Auer et al., 2007) have been devoted to provide Wikipedia articles with ontology class labels by applying simple heuristic or hand-crafted rules. However, these approaches heavily rely on metadata (e.g., infobox templates and category labels) and suﬀer from insuﬃcient coverage of rules due to the lack of metadata, as reported by Aprosio et al. (2013). Another trend of research which may seem relevant to our work can be found in eﬀorts for automatically annotating entity mentions in text with ﬁne-grained NE type labels deﬁned in an existing type hierarchy such as Freebase (Ling and Weld, 2012; Nakashole et al., 2013; Shimaoka et al., 2016). While these studies focus on the identiﬁcation and classiﬁcation of individual mentions, our work aims at the classiﬁcation of Wikipedia articles. The two tasks are related and may well beneﬁt from each other. However, they are not the same; techniques proposed for mention classiﬁcation cannot directly apply to our task nor can be compared with our methods. 537 The work closest to our study is done by Higashinaka et al. (2012), who proposed a supervised machine learning model for classifying Wikipedia articles into the 200 ﬁne-grained NE types deﬁned by Sekine et al."
Y16-3027,sekine-etal-2002-extended,1,0.691678,"hat both ideas gained their own statistically signiﬁcant improvement separately in classiﬁcation accuracy. 1 Introduction Recognizing named entities (NEs) in text is a crucial component task of a broad range of NLP applications including information extraction and question answering. Early work on named entity recognition (NER) deﬁned a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coa"
Y16-3027,W16-1313,1,0.800285,"been devoted to provide Wikipedia articles with ontology class labels by applying simple heuristic or hand-crafted rules. However, these approaches heavily rely on metadata (e.g., infobox templates and category labels) and suﬀer from insuﬃcient coverage of rules due to the lack of metadata, as reported by Aprosio et al. (2013). Another trend of research which may seem relevant to our work can be found in eﬀorts for automatically annotating entity mentions in text with ﬁne-grained NE type labels deﬁned in an existing type hierarchy such as Freebase (Ling and Weld, 2012; Nakashole et al., 2013; Shimaoka et al., 2016). While these studies focus on the identiﬁcation and classiﬁcation of individual mentions, our work aims at the classiﬁcation of Wikipedia articles. The two tasks are related and may well beneﬁt from each other. However, they are not the same; techniques proposed for mention classiﬁcation cannot directly apply to our task nor can be compared with our methods. 537 The work closest to our study is done by Higashinaka et al. (2012), who proposed a supervised machine learning model for classifying Wikipedia articles into the 200 ﬁne-grained NE types deﬁned by Sekine et al. (2002). They conducted e"
Y16-3027,U09-1015,0,0.778436,"Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set whereas all of them fall into the same type Location in a common coarse-gra"
Y16-3027,W06-2809,0,0.899903,"nd explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set whereas all of them fall into the same type Location in a common coarse-grained type set. Given th"
Y16-3027,D07-1068,0,0.814403,"al models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set whereas all of them fall into the same type Location in a common coarse-grained type set. Given the same number of labeled"
Y16-3027,C12-2133,0,0.0668656,"their own statistically signiﬁcant improvement separately in classiﬁcation accuracy. 1 Introduction Recognizing named entities (NEs) in text is a crucial component task of a broad range of NLP applications including information extraction and question answering. Early work on named entity recognition (NER) deﬁned a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁca"
Y16-3027,C98-1065,0,\N,Missing
Y17-1045,doddington-etal-2004-automatic,0,0.470149,"onsense knowledge such as entities and events, and their causal relationships, are indispensable in various natural language processing (NLP) applications, including question answering (Oh et al., 2013; Oh et al., 2016; Sharp et al., 2016), hypothesis generation (Radinsky et al., 2012; Hashimoto et al., 2015), stance detection (Sasaki et al., 2016), and literature curation for systems biology (Pyysalo et al., 2015; Rinaldi et al., 2016). In many previous researches, corpora for acquiring causal relations were built by annotating two text spans (e.g., entities) and their relations in the text (Doddington et al., 2004; Hendrickx et al., 2010; Pyysalo et al., 2015; Rinaldi et al., 2016; This paper presents an approach for harnessing causal relation instances to Wikipedia articles via crowdsourcing. Wikipedia is the central infrastructure for knowledge curation, as exemplified by Freebase (Bollacker et al., 2008) and Wikification (Mihalcea and Csomai, 2007). Therefore, we base Wikipedia articles for building a corpus with causal relation instances. This work represents a first step toward organizing the causal knowledge in Wikipedia articles covering various topics. Recently, researchers have recognized the"
Y17-1045,W17-0812,0,0.0259259,"tool has not been released to the public. In contrast, we combine a crowdsourcing service with brat, a popular open-source annotation tool, to provide an easy-to-use interface and quality control for the annotation work. This approach is not limited to causal relations but can be adapted to any bratsupported tasks (e.g., part-of-speech tagging and information extraction). We also present a quality control mechanism that is applicable to any crowdsourcing services accepting free text for a micro-task. Several studies have dedicated to identify causal relations mentioned in text. For instance, Dunietz et al. (2017) present the version 2.0 of Bank of Effects and Causes Stated Explicitly (BECauSE). The corpus includes annotations of causes and effects as well as seven semantic relations that are frequently associated with causation. Rehbein and Ruppenhofer (2017) use the similar annotation scheme for building a German corpus with some changes in the label set and the scope of causality. Built on top of well-established lingustic theories, these studies focus more on “causal language” (expressions of causation) than real-world causation. In contrast, our ultimate goal is acquisition of real-world causal kn"
Y17-1045,W10-0713,0,0.227381,"ssing causal relation instances to Wikipedia articles via crowdsourcing. Wikipedia is the central infrastructure for knowledge curation, as exemplified by Freebase (Bollacker et al., 2008) and Wikification (Mihalcea and Csomai, 2007). Therefore, we base Wikipedia articles for building a corpus with causal relation instances. This work represents a first step toward organizing the causal knowledge in Wikipedia articles covering various topics. Recently, researchers have recognized the value of crowdsourcing services in constructing wideranging language resources at low cost (Brew et al., 2010; Finin et al., 2010; Gormley et al., 2010; Jha et al., 2010; Fort et al., 2011; Kawahara et al., 2014; Lawson et al., 2010; Hovy et al., 2014; Takase et al., 2016). Unfortunately, causal relations cannot be directly annotated by crowdsourcing. For this purpose, non-expert workers on crowdsourcing services require a clear and simple micro-task. A crowdsourcing service only provides a standardized interface for workers. The micro-tasks on this interface 336 31st Pacific Asia Conference on Language, Information and Computation (PACLIC 31), pages 336–345 Cebu City, Philippines, November 16-18, 2017 c Copyright 2017"
Y17-1045,J11-2010,0,0.0311036,"owdsourcing. Wikipedia is the central infrastructure for knowledge curation, as exemplified by Freebase (Bollacker et al., 2008) and Wikification (Mihalcea and Csomai, 2007). Therefore, we base Wikipedia articles for building a corpus with causal relation instances. This work represents a first step toward organizing the causal knowledge in Wikipedia articles covering various topics. Recently, researchers have recognized the value of crowdsourcing services in constructing wideranging language resources at low cost (Brew et al., 2010; Finin et al., 2010; Gormley et al., 2010; Jha et al., 2010; Fort et al., 2011; Kawahara et al., 2014; Lawson et al., 2010; Hovy et al., 2014; Takase et al., 2016). Unfortunately, causal relations cannot be directly annotated by crowdsourcing. For this purpose, non-expert workers on crowdsourcing services require a clear and simple micro-task. A crowdsourcing service only provides a standardized interface for workers. The micro-tasks on this interface 336 31st Pacific Asia Conference on Language, Information and Computation (PACLIC 31), pages 336–345 Cebu City, Philippines, November 16-18, 2017 c Copyright 2017 Kazuaki Hanawa, Akira Sasaki, Naoaki Okazaki and Kentaro In"
Y17-1045,W10-0732,0,0.146368,"n instances to Wikipedia articles via crowdsourcing. Wikipedia is the central infrastructure for knowledge curation, as exemplified by Freebase (Bollacker et al., 2008) and Wikification (Mihalcea and Csomai, 2007). Therefore, we base Wikipedia articles for building a corpus with causal relation instances. This work represents a first step toward organizing the causal knowledge in Wikipedia articles covering various topics. Recently, researchers have recognized the value of crowdsourcing services in constructing wideranging language resources at low cost (Brew et al., 2010; Finin et al., 2010; Gormley et al., 2010; Jha et al., 2010; Fort et al., 2011; Kawahara et al., 2014; Lawson et al., 2010; Hovy et al., 2014; Takase et al., 2016). Unfortunately, causal relations cannot be directly annotated by crowdsourcing. For this purpose, non-expert workers on crowdsourcing services require a clear and simple micro-task. A crowdsourcing service only provides a standardized interface for workers. The micro-tasks on this interface 336 31st Pacific Asia Conference on Language, Information and Computation (PACLIC 31), pages 336–345 Cebu City, Philippines, November 16-18, 2017 c Copyright 2017 Kazuaki Hanawa, Akira"
Y17-1045,D12-1057,0,0.0655374,"Missing"
Y17-1045,S10-1006,0,0.0818631,"Missing"
Y17-1045,P14-2062,0,0.0366395,"Missing"
Y17-1045,W10-0702,0,0.13337,"ia articles via crowdsourcing. Wikipedia is the central infrastructure for knowledge curation, as exemplified by Freebase (Bollacker et al., 2008) and Wikification (Mihalcea and Csomai, 2007). Therefore, we base Wikipedia articles for building a corpus with causal relation instances. This work represents a first step toward organizing the causal knowledge in Wikipedia articles covering various topics. Recently, researchers have recognized the value of crowdsourcing services in constructing wideranging language resources at low cost (Brew et al., 2010; Finin et al., 2010; Gormley et al., 2010; Jha et al., 2010; Fort et al., 2011; Kawahara et al., 2014; Lawson et al., 2010; Hovy et al., 2014; Takase et al., 2016). Unfortunately, causal relations cannot be directly annotated by crowdsourcing. For this purpose, non-expert workers on crowdsourcing services require a clear and simple micro-task. A crowdsourcing service only provides a standardized interface for workers. The micro-tasks on this interface 336 31st Pacific Asia Conference on Language, Information and Computation (PACLIC 31), pages 336–345 Cebu City, Philippines, November 16-18, 2017 c Copyright 2017 Kazuaki Hanawa, Akira Sasaki, Naoaki Oka"
Y17-1045,C14-1027,0,0.119846,"dia is the central infrastructure for knowledge curation, as exemplified by Freebase (Bollacker et al., 2008) and Wikification (Mihalcea and Csomai, 2007). Therefore, we base Wikipedia articles for building a corpus with causal relation instances. This work represents a first step toward organizing the causal knowledge in Wikipedia articles covering various topics. Recently, researchers have recognized the value of crowdsourcing services in constructing wideranging language resources at low cost (Brew et al., 2010; Finin et al., 2010; Gormley et al., 2010; Jha et al., 2010; Fort et al., 2011; Kawahara et al., 2014; Lawson et al., 2010; Hovy et al., 2014; Takase et al., 2016). Unfortunately, causal relations cannot be directly annotated by crowdsourcing. For this purpose, non-expert workers on crowdsourcing services require a clear and simple micro-task. A crowdsourcing service only provides a standardized interface for workers. The micro-tasks on this interface 336 31st Pacific Asia Conference on Language, Information and Computation (PACLIC 31), pages 336–345 Cebu City, Philippines, November 16-18, 2017 c Copyright 2017 Kazuaki Hanawa, Akira Sasaki, Naoaki Okazaki and Kentaro Inui are often limited to"
Y17-1045,W10-0712,0,0.356389,"astructure for knowledge curation, as exemplified by Freebase (Bollacker et al., 2008) and Wikification (Mihalcea and Csomai, 2007). Therefore, we base Wikipedia articles for building a corpus with causal relation instances. This work represents a first step toward organizing the causal knowledge in Wikipedia articles covering various topics. Recently, researchers have recognized the value of crowdsourcing services in constructing wideranging language resources at low cost (Brew et al., 2010; Finin et al., 2010; Gormley et al., 2010; Jha et al., 2010; Fort et al., 2011; Kawahara et al., 2014; Lawson et al., 2010; Hovy et al., 2014; Takase et al., 2016). Unfortunately, causal relations cannot be directly annotated by crowdsourcing. For this purpose, non-expert workers on crowdsourcing services require a clear and simple micro-task. A crowdsourcing service only provides a standardized interface for workers. The micro-tasks on this interface 336 31st Pacific Asia Conference on Language, Information and Computation (PACLIC 31), pages 336–345 Cebu City, Philippines, November 16-18, 2017 c Copyright 2017 Kazuaki Hanawa, Akira Sasaki, Naoaki Okazaki and Kentaro Inui are often limited to multiple choice ques"
Y17-1045,W17-0813,0,0.0291906,"limited to causal relations but can be adapted to any bratsupported tasks (e.g., part-of-speech tagging and information extraction). We also present a quality control mechanism that is applicable to any crowdsourcing services accepting free text for a micro-task. Several studies have dedicated to identify causal relations mentioned in text. For instance, Dunietz et al. (2017) present the version 2.0 of Bank of Effects and Causes Stated Explicitly (BECauSE). The corpus includes annotations of causes and effects as well as seven semantic relations that are frequently associated with causation. Rehbein and Ruppenhofer (2017) use the similar annotation scheme for building a German corpus with some changes in the label set and the scope of causality. Built on top of well-established lingustic theories, these studies focus more on “causal language” (expressions of causation) than real-world causation. In contrast, our ultimate goal is acquisition of real-world causal knowledge by exploting Wikipedia as an encyclopedia. We thus design a curation process with crowdworkers involved in, focussing on how humans ‘read’ Wikipedia articles for causal knowledge. 3 Annotating promotion/suppression relations in Wikipedia artic"
Y17-1045,D16-1014,0,0.0315376,"We issued the micro-tasks to crowd workers, and collected 95,008 annotations of causal relation instances among 8,745 summary sentences in 1,494 Wikipedia articles. The annotated corpus not only provides supervision data for automatic recognition of causal relation instances, but also reveals valuable facts for improving the annotation process of this task. 1 Introduction Commonsense knowledge such as entities and events, and their causal relationships, are indispensable in various natural language processing (NLP) applications, including question answering (Oh et al., 2013; Oh et al., 2016; Sharp et al., 2016), hypothesis generation (Radinsky et al., 2012; Hashimoto et al., 2015), stance detection (Sasaki et al., 2016), and literature curation for systems biology (Pyysalo et al., 2015; Rinaldi et al., 2016). In many previous researches, corpora for acquiring causal relations were built by annotating two text spans (e.g., entities) and their relations in the text (Doddington et al., 2004; Hendrickx et al., 2010; Pyysalo et al., 2015; Rinaldi et al., 2016; This paper presents an approach for harnessing causal relation instances to Wikipedia articles via crowdsourcing. Wikipedia is the central infrast"
Y17-1045,E12-2021,0,0.100698,"Missing"
Y17-1045,P16-1215,1,0.811198,"Missing"
Y17-1045,W10-0701,0,\N,Missing
Y18-1001,N18-1032,0,0.0243039,"as been found to be helpful in learning translation models for particular dialects. Several previous studies have investigated the characteristics of translation models for closely-related dialects (Meftouh et al., 2015; Honnet et al., 2018). For example, Honnet et al. (2018) reported that a character-level NMT model trained on one Swiss-German dialect performed moderately well at translating sentences in closely-related dialects. Therefore, given this, we use multilingual NMT (Johnson et al., 2016) to learn parameters that encode knowledge of dialects’ shared lexical and syntactic structure. Gu et al. (2018) demonstrated that multilingual NMT can be useful for lowresource language pairs, while Lakew et al. (2018) found that a multilingual NMT system trained on multiple related languages showed improved zeroshot translation performance. We believe that multilingual NMT can be effective for closely-related dialects, and can compensate for a lack of translation data for the different dialects. Multilingual NMT can also help us to analyze the ¨ characteristics of each language. Ostling and Tiedemann (2016) found that clustering the language embeddings learned by a character-level multilingual system"
Y18-1001,W16-4824,0,0.02542,". This approach utilizes dialect embeddings, namely vector representations of Japanese dialects, to inform the model of the input dialect. An interesting by-product of this approach is that the dialect embeddings the system learns illustrate the difference between different dialect types from different geographical areas. In addition, we present an example of using these dialect embeddings for dialectom1 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 etry (Nerbonne and Kretzschmar, 2011; Kumagai, 2016; Guggilla, 2016; Rama and C¸o¨ ltekin, 2016). Another advantage of adopting a the multilingual architecture for multiple related languages is it can enable the system to acquire knowledge of their lexical and syntactic similarities. For example, Lakew et al. (2018) reported that including several related languages in supervised training data can improve multilingual NMT. Our results confirm the effectiveness of using closely-related languages (namely Japanese dialects) in multilingual NMT. (dialect) source ( = Right, definitely. ) divided into “bunsetsu” input sequence ( = Right) dialect label dialect label"
Y18-1001,P07-2045,0,0.0082992,"Missing"
Y18-1001,W11-2123,0,0.0328063,"MT (w/ labels) Mono SMT Multi SMT (w/o labels) standard-to-dialect Multi NMT (w/ labels) BLEU 35.10 22.45 71.29 69.74 75.66 52.98 73.54 64.04 Table 2: Syllable-level BLEU scores for all models (without translation). We used OpenNMT-py2 with its default hyperparameter settings, except for the number of the training epochs (which we set to 20) and selected the model that performed best on the development set. In addition, we employed Moses3 (Koehn et al., 2007) as the baseline SMT model and set the distortion limit to 0. The standard Japanese language model used in Moses was trained with KenLM (Heafield, 2011). Regarding the dialect label order used for the input, our preliminary experiments indicated that the best models were obtained using input sequence (d) (Table 1) for dialect-to-standard translation and input sequence (c) for standard-to-dialect translation. Finally, we used MeCab 0.9964 to analyze the kanji characters’ pronunciations. 5.2 Multi-Dialect NMT Model Performance Table 2 shows the dialect translation performance of all the models considered, with the first row group comparing their scores for dialect-to-standard translation with different input settings. Mono-lingual vs multi-ling"
Y18-1001,Y15-1004,0,0.016121,"t label (=definitely) Multi-dialect NMT output sequence Attention concatenate (standard JPN) “Hyogo” target 2 Related Work Little dialectal text is available since dialects are generally spoken rather than written. For this reason, many dialect MT researchers work in low-resource settings (Zbib et al., 2012; Scherrer and Ljubeˇsi´c, 2016; Hassan et al., 2017). However, the use of similar dialects has been found to be helpful in learning translation models for particular dialects. Several previous studies have investigated the characteristics of translation models for closely-related dialects (Meftouh et al., 2015; Honnet et al., 2018). For example, Honnet et al. (2018) reported that a character-level NMT model trained on one Swiss-German dialect performed moderately well at translating sentences in closely-related dialects. Therefore, given this, we use multilingual NMT (Johnson et al., 2016) to learn parameters that encode knowledge of dialects’ shared lexical and syntactic structure. Gu et al. (2018) demonstrated that multilingual NMT can be useful for lowresource language pairs, while Lakew et al. (2018) found that a multilingual NMT system trained on multiple related languages showed improved zero"
Y18-1001,L18-1597,0,0.0389463,"Multi-dialect NMT output sequence Attention concatenate (standard JPN) “Hyogo” target 2 Related Work Little dialectal text is available since dialects are generally spoken rather than written. For this reason, many dialect MT researchers work in low-resource settings (Zbib et al., 2012; Scherrer and Ljubeˇsi´c, 2016; Hassan et al., 2017). However, the use of similar dialects has been found to be helpful in learning translation models for particular dialects. Several previous studies have investigated the characteristics of translation models for closely-related dialects (Meftouh et al., 2015; Honnet et al., 2018). For example, Honnet et al. (2018) reported that a character-level NMT model trained on one Swiss-German dialect performed moderately well at translating sentences in closely-related dialects. Therefore, given this, we use multilingual NMT (Johnson et al., 2016) to learn parameters that encode knowledge of dialects’ shared lexical and syntactic structure. Gu et al. (2018) demonstrated that multilingual NMT can be useful for lowresource language pairs, while Lakew et al. (2018) found that a multilingual NMT system trained on multiple related languages showed improved zeroshot translation perfo"
Y18-1001,P12-2059,0,0.0245438,"cteristics of each language. Ostling and Tiedemann (2016) found that clustering the language embeddings learned by a character-level multilingual system provided an illustration of the language families involved. In the light of this, we also analyze our dialect embeddings to investigate whether our Figure 1: Proposed multi-dialect NMT model. multi-dialect model can capture similarities between dialects (Section 5). Previous work reported that character-level statistical machine translation (SMT) using words as translation units was effective for translating between closely-related languages (Nakov and Tiedemann, 2012; Scherrer and Ljubeˇsi´c, 2016). There are two reasons for this: the character-level information enables the system to exploit lexical overlaps, while using words as translation units takes advantage of related languages’ syntactic overlaps. In this study, we present a method of translating between Japanese dialects that combines three ideas: multilingual NMT, character-level NMT, and the use of base phrases (i.e., bunsetsu) as translation units. We believe this enables our approach to fully exploit the similarities among dialects and standard Japanese, even in low-resource settings. 3 Data:"
Y18-1001,P17-4012,0,0.0321359,"nerally written in a mix of kanji and kana; therefore, we converted the kanji in the sentences into kana and, then, segmented them into bunsetsu.1 After preprocessing, the average sentence lengths were 14.62 and 15.57 characters for the dialects and standard Japanese, respectively, and the average number of bunsetsus per sentence was 3.42. 4 NMT Model Figure 1 gives an overview of our multi-dialect NMT system’s network structure. Since our focus is on examining the effectiveness of multi-dialect NMT and its detailed behavior, rather than on creating a novel translation model, we used OpenNMT (Klein et al., 2017), a stacking LSTM encoder–decoder model with a multilingual extension similar to that of Johnson et al. (2016). However, to improve its direct translation accuracy, we make the following three modifications. Dialect labels Following a previous multilingual NMT study (Johnson et al., 2016), we train a unified model that handled all 48 dialects simultaneously 1 This is the smallest Japanese phrase unit, containing a single content word and attached postpositions. ID a b c d Encoder input order source label, sentence target label, sentence source label, target label, sentence source label, senten"
Y18-1001,W16-4803,0,0.0668626,"Missing"
Y18-1001,Y18-1000,0,0.271859,"etween different dialect types from different geographical areas. In addition, we present an example of using these dialect embeddings for dialectom1 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 etry (Nerbonne and Kretzschmar, 2011; Kumagai, 2016; Guggilla, 2016; Rama and C¸o¨ ltekin, 2016). Another advantage of adopting a the multilingual architecture for multiple related languages is it can enable the system to acquire knowledge of their lexical and syntactic similarities. For example, Lakew et al. (2018) reported that including several related languages in supervised training data can improve multilingual NMT. Our results confirm the effectiveness of using closely-related languages (namely Japanese dialects) in multilingual NMT. (dialect) source ( = Right, definitely. ) divided into “bunsetsu” input sequence ( = Right) dialect label dialect label (=definitely) Multi-dialect NMT output sequence Attention concatenate (standard JPN) “Hyogo” target 2 Related Work Little dialectal text is available since dialects are generally spoken rather than written. For this reason, many dialect MT researcher"
Y18-1001,N12-1006,0,0.0281621,"related languages in supervised training data can improve multilingual NMT. Our results confirm the effectiveness of using closely-related languages (namely Japanese dialects) in multilingual NMT. (dialect) source ( = Right, definitely. ) divided into “bunsetsu” input sequence ( = Right) dialect label dialect label (=definitely) Multi-dialect NMT output sequence Attention concatenate (standard JPN) “Hyogo” target 2 Related Work Little dialectal text is available since dialects are generally spoken rather than written. For this reason, many dialect MT researchers work in low-resource settings (Zbib et al., 2012; Scherrer and Ljubeˇsi´c, 2016; Hassan et al., 2017). However, the use of similar dialects has been found to be helpful in learning translation models for particular dialects. Several previous studies have investigated the characteristics of translation models for closely-related dialects (Meftouh et al., 2015; Honnet et al., 2018). For example, Honnet et al. (2018) reported that a character-level NMT model trained on one Swiss-German dialect performed moderately well at translating sentences in closely-related dialects. Therefore, given this, we use multilingual NMT (Johnson et al., 2016) to"
Y18-1001,D16-1163,0,0.0219347,"h the same model trained via the standard approach of using entire sentences as input/output sequences (Multi NMT-sentence) shows that Multi NMT outperforms Multi NMTsentence by 5.92 points. One disadvantage of chunk-wise translation is it cannot capture the context beyond each chunk’s boundary. However, despite this disadvantage, our Multi NMT model was still able to outperform a model that had access to a broader context (Multi NMT-sentence), indicating that our fixed-order translation approach is suitable for translating Japanese dialects despite its limited context sensitivity. NMT vs SMT Zoph et al. (2016) found that SMT models largely outperformed state-of-the-art NMT models for low-resource languages. Therefore, for comparison, the second row group in Table 2 shows results for a fixed-order character-based SMT baseline. In these experiments, even though the NMT model trained using a single dialect (Mono NMT) gave the poorest performance, the one with dialect labels outperformed the baseline Multi SMT model, achieving the best performance overall. 5.3 Example Translation Results To demonstrate how each of the proposed components contributed to generating accurate translations, we now present s"
Y18-1015,Y18-1000,0,0.269137,"Missing"
Y18-1015,D14-1164,0,0.0147035,"relation, in comparison with only using the original general entity type, PROCESS. For instances, in Example 3 and Example 4, both target entities “predicted” and “combine” belong to the same entity type, PROCESS, but the former specifically belongs to the TSS, OUTPUT-PROCESS, and the latter does not. Therefore, based on this difference, a RC system could easily distinguish them, and classify the former as OUTPUT relation. For identifying the TSS, one possibility is to manually annotate the TSS in target sentences. However, manual annotation is time-consuming (Kim et al., 2008) and expensive (Angeli et al., 2014). To address this issue, in this work, we propose a minimally supervised approach that utilizes supersense embeddings. Specifically, we manually prepare a small number of seed instance words for the predefined supersense (or TSS) (e.g., “survey” for RESEARCH-PROCESS) and train the embedding of word and supersense in the same vector space, like the method Flekova and Gurevych (Flekova and Gurevych, 2016) proposed, which will be detailed in Section 3. By comparing the emebdding between supersense and a given word, we determine its TSS. Our evaluation empirically demonstrates that incorporating t"
Y18-1015,S17-2091,0,0.0262967,"atasets prove the effectiveness of the task specific supersense on relation classification in scientific articles. 1 Introduction In recent years, along with the number of scientific papers increasing, it is prohibitively time-consuming for researchers to review and fully-comprehend all papers. To effectively and quickly access a large amount of scientific papers and acquire useful knowledge, a wide variety of computational studies for structuralizing scientific papers has been conducted, such as Argumentative Zoning (Teufel and others, 2000), BioNLP Shared Task (2017), ScienceIE Shared Task (Augenstein et al., 2017) and Semantic Relation Extraction and Classification in Scientific Papers (Gábor et al., 2018). One fundamental study is Relation Classification (RC). In this paper, we tackle the task of RC. RC is the task of capturing predefined semantic relations between entities from text. Thus, our task consists of the following: given a sentence that has been annotated with entity1 mentions, we aim towards categorizing relations between entities. Suppose the following sentence: entit y (1) An efficient bit-vector-based CKY-style parser entit y X for context-free parsing is presented. Y In Example 1, one"
Y18-1015,bird-etal-2008-acl,0,0.0412589,"k , we manually prepare a small number of seed instances for the predefined TSS as shown in the second column of Table 1. 3.2 Building TSS Embeddings Similar to the method proposed by Flekova and Gurevych (Flekova and Gurevych, 2016), we replace each word in a corpus by its corresponding TSS according to seed instances prepared in the previous step. In this way, besides the original corpus (see Table 2, first row), we obtain an alternative corpus where each word is replaced by its corresponding TSS (see Table 2, second row). We trained the TSS embeddings on the ACL Anthology Reference Corpus (Bird et al., 2008) and its alternative corpus jointly (e.g., both first and second row in Table 2) by the skip-gram NN architecture made available by the Gensim word2vec tool 3. Thereby, we produce continuous representation of words and the predefined TSS in one vector space 4. Table 3 shows the most 2As a preliminary study, we only select four representative types of TSS, but in the future, we will investigate more types of TSS for scientific RC. 3https://radimrehurek.com/gensim 4The embedding is trained with negative sampling of 25 noise words, minimal word frequency of 10, window size of 2 and alpha of 0.002"
Y18-1015,C10-1018,0,0.192637,"1, one of the scientific relations we aim to identify is the relation USAGE(X, Y), which means that bit-vector-based CKY-style parser is used for the action of context-free parsing. For notational convenience, we refer to a sentence where a relation is identified as a target sentence, and we refer to the related entity pair as a target entity pair. Many previous works on RC exist in the general domain (Kumar, 2017; Zhou et al., 2014). The earlier approaches depend on complex feature engineering such as manually prepared lexical-syntactic patterns (Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010, etc.). Recently, Neural Network (NN)-based approaches achieve close or even better performance to earlier approaches without complicated manually prepared features (Zeng et al., 2014; Zhang and Wang, 2015; Santos et al., 2015). In the context of scientific RC, Ammar et al. (Ammar et al., 2017) enhanced Miwa and Bansal (Miwa 1In this work, entity refers not merely to concepts denoted by noun or noun phrase, it could be actions denoted by verb or verb phrase, and evaluation denoted by adjective or adverb etc. 129 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong,"
Y18-1015,P16-1191,0,0.387161,"former as OUTPUT relation. For identifying the TSS, one possibility is to manually annotate the TSS in target sentences. However, manual annotation is time-consuming (Kim et al., 2008) and expensive (Angeli et al., 2014). To address this issue, in this work, we propose a minimally supervised approach that utilizes supersense embeddings. Specifically, we manually prepare a small number of seed instance words for the predefined supersense (or TSS) (e.g., “survey” for RESEARCH-PROCESS) and train the embedding of word and supersense in the same vector space, like the method Flekova and Gurevych (Flekova and Gurevych, 2016) proposed, which will be detailed in Section 3. By comparing the emebdding between supersense and a given word, we determine its TSS. Our evaluation empirically demonstrates that incorporating the TSS could improve the performance of scientific RC. can be 2 PROCESS predicted ... Related Work X PL AN D−OR−PROCESS (4) ... statistical generation to D AT A−IT E M common phrases into a Y PROCESS combineX D AT A−IT E M sentence . In this dataset, both entity mentions and entity types (e.g., PROCESS, PLAN, DATA-ITEM) are annotated. Conventional approaches for RC rely on human-designed, complex lexica"
Y18-1015,S18-1111,0,0.391535,"tific articles. 1 Introduction In recent years, along with the number of scientific papers increasing, it is prohibitively time-consuming for researchers to review and fully-comprehend all papers. To effectively and quickly access a large amount of scientific papers and acquire useful knowledge, a wide variety of computational studies for structuralizing scientific papers has been conducted, such as Argumentative Zoning (Teufel and others, 2000), BioNLP Shared Task (2017), ScienceIE Shared Task (Augenstein et al., 2017) and Semantic Relation Extraction and Classification in Scientific Papers (Gábor et al., 2018). One fundamental study is Relation Classification (RC). In this paper, we tackle the task of RC. RC is the task of capturing predefined semantic relations between entities from text. Thus, our task consists of the following: given a sentence that has been annotated with entity1 mentions, we aim towards categorizing relations between entities. Suppose the following sentence: entit y (1) An efficient bit-vector-based CKY-style parser entit y X for context-free parsing is presented. Y In Example 1, one of the scientific relations we aim to identify is the relation USAGE(X, Y), which means that b"
Y18-1015,P05-1053,0,0.115937,"n empirically demonstrates that incorporating the TSS could improve the performance of scientific RC. can be 2 PROCESS predicted ... Related Work X PL AN D−OR−PROCESS (4) ... statistical generation to D AT A−IT E M common phrases into a Y PROCESS combineX D AT A−IT E M sentence . In this dataset, both entity mentions and entity types (e.g., PROCESS, PLAN, DATA-ITEM) are annotated. Conventional approaches for RC rely on human-designed, complex lexical-syntactic patterns (Boschee et al., 2005), statistical cooccurrences (Suchanek et al., 2006) and structuralized knowledge bases such as WordNet (GuoDong et al., 2005; Chan and Roth, 2010). In recent years, exploring Neural Network (NN)-based models has 130 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 been the dominant approach in the field. Zeng et al. (Zeng et al., 2014) and Xu et al. (Xu et al., 2015) proposed a Convolutional Neural Network (CNN)based framework, which depends on sentence-level features collected from an entire target sentence and lexical-level features from lexical resources such as WordNet (Fellbaum, 1998). Santos et al. (Santos et al., 2015)"
Y18-1015,W16-2920,0,0.0184964,"ed by a pairwise ranking loss function. To improve the ability of sequential modeling, Zhang et al. (Zhang and Wang, 2015) proposed a recurrent neural network (RNN)-based model for RC. Other variants of RNN-based models have been proposed, such as Miwa et al. (Miwa and Bansal, 2016), who proposed a bidirectional tree-structured LSTM model. Additionally, similar NN-based approaches are used in scientific relation classification. For instance, Gu et al. (Gu et al., 2017) utilized a CNNbased model for identifying chemical-disease relations from the abstracts of MEDLINE papers. HahnPowell et al. (Hahn-Powell et al., 2016) proposed an LSTM-based RNN model for identifying causal precedence relationship between two event mentions in biomedical papers. Ammar et al. (Ammar et al., 2017) enhanced Miwa and Bansal (Miwa and Bansal, 2016)’s relation extraction model via extensions such as gazetteer-like information extracted from Wikipedia. Pratap et al. (Pratap et al., 2018) incorporate WordNet hypernyms as the feature for scientific RC. However, none of these approaches leverage the task specific supersense for RC. Flekova and Gurevych (Flekova and Gurevych, 2016) integrated supersense into distributional word repres"
Y18-1015,S18-1134,0,0.022048,"Missing"
Y18-1015,S18-1127,0,0.0218272,"Missing"
Y18-1015,S17-2171,0,0.0485228,"Missing"
Y18-1015,S18-1125,0,0.036498,"Missing"
Y18-1015,P16-1105,0,0.0189703,"nd Xu et al. (Xu et al., 2015) proposed a Convolutional Neural Network (CNN)based framework, which depends on sentence-level features collected from an entire target sentence and lexical-level features from lexical resources such as WordNet (Fellbaum, 1998). Santos et al. (Santos et al., 2015) proposed a ranking CNN model, which is trained by a pairwise ranking loss function. To improve the ability of sequential modeling, Zhang et al. (Zhang and Wang, 2015) proposed a recurrent neural network (RNN)-based model for RC. Other variants of RNN-based models have been proposed, such as Miwa et al. (Miwa and Bansal, 2016), who proposed a bidirectional tree-structured LSTM model. Additionally, similar NN-based approaches are used in scientific relation classification. For instance, Gu et al. (Gu et al., 2017) utilized a CNNbased model for identifying chemical-disease relations from the abstracts of MEDLINE papers. HahnPowell et al. (Hahn-Powell et al., 2016) proposed an LSTM-based RNN model for identifying causal precedence relationship between two event mentions in biomedical papers. Ammar et al. (Ammar et al., 2017) enhanced Miwa and Bansal (Miwa and Bansal, 2016)’s relation extraction model via extensions su"
Y18-1015,S18-1139,0,0.0483187,"e context of scientific RC, Ammar et al. (Ammar et al., 2017) enhanced Miwa and Bansal (Miwa 1In this work, entity refers not merely to concepts denoted by noun or noun phrase, it could be actions denoted by verb or verb phrase, and evaluation denoted by adjective or adverb etc. 129 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 and Bansal, 2016)’s end-to-end general relation extraction model by incorporating external knowledge such as gazetteer-like information extracted from Wikipedia. Pratap et al. (Pratap et al., 2018) incorporate WordNet hypernyms as the feature for scientific RC. However, no previous work leverages task specific supersense as a feature for RC. In this work, we define the task specific supersense (TSS) as a new semantic category that is proposed according to the property of a given RC task, such as the definitions of target relations and selectional tendency of target relations. We hypothesize that TSS can be utilized to improve the performance of scientific RC. Suppose the following target sentence taken from the SemEval-2018 task 7 dataset (Gábor et al., 2018): entit y (2) This paper pre"
Y18-1015,S18-1112,0,0.0353618,"Missing"
Y18-1015,P15-1061,0,0.165249,"ntence where a relation is identified as a target sentence, and we refer to the related entity pair as a target entity pair. Many previous works on RC exist in the general domain (Kumar, 2017; Zhou et al., 2014). The earlier approaches depend on complex feature engineering such as manually prepared lexical-syntactic patterns (Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010, etc.). Recently, Neural Network (NN)-based approaches achieve close or even better performance to earlier approaches without complicated manually prepared features (Zeng et al., 2014; Zhang and Wang, 2015; Santos et al., 2015). In the context of scientific RC, Ammar et al. (Ammar et al., 2017) enhanced Miwa and Bansal (Miwa 1In this work, entity refers not merely to concepts denoted by noun or noun phrase, it could be actions denoted by verb or verb phrase, and evaluation denoted by adjective or adverb etc. 129 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 and Bansal, 2016)’s end-to-end general relation extraction model by incorporating external knowledge such as gazetteer-like information extracted from Wikipedia. Pratap e"
Y18-1015,tateisi-etal-2014-annotation,0,0.403413,"nto several predefined semantic relations. One of them is TOPIC relation. The relation TOPIC(X, Y) namely means the entity X deals with the topic Y. Therefore, the entity X tends to be a research activity, such as “analysis”, “survey” and “discussion” etc. Based on this selectional tendency, we define a TSS to cover these words, called RESEARCH-PROCESS. Identifying RESEARCH-PROCESS for a given word such as “discussion” in Example 2, could help a RC system to correctly classify the target entity pair into TOPIC relation. Similarly, suppose the following target sentences from the RANIS dataset (Tateisi et al., 2014): D AT A−IT E M (3) A verb D AT A−IT E M ’s aspectual category Y The target relations includes relation OUTPUT(X, Y) (as in Example 3), and INPUT(X, Y) (as in Example 4). They namely mean entity Y is the output/input of a process X. Based on the definition, we propose a TSS called OUTPUT-PROCESS, verbs like “show”, “identify” and “extract” belong to this TSS, because “a system can show/identify/extract Y” represents that the system can output Y. If we could correctly identify the OUTPUT-PROCESS in a given target sentence, and apply the new specific TSS , it could help a RC system more effectiv"
Y18-1015,D15-1062,0,0.0193198,"g., PROCESS, PLAN, DATA-ITEM) are annotated. Conventional approaches for RC rely on human-designed, complex lexical-syntactic patterns (Boschee et al., 2005), statistical cooccurrences (Suchanek et al., 2006) and structuralized knowledge bases such as WordNet (GuoDong et al., 2005; Chan and Roth, 2010). In recent years, exploring Neural Network (NN)-based models has 130 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 been the dominant approach in the field. Zeng et al. (Zeng et al., 2014) and Xu et al. (Xu et al., 2015) proposed a Convolutional Neural Network (CNN)based framework, which depends on sentence-level features collected from an entire target sentence and lexical-level features from lexical resources such as WordNet (Fellbaum, 1998). Santos et al. (Santos et al., 2015) proposed a ranking CNN model, which is trained by a pairwise ranking loss function. To improve the ability of sequential modeling, Zhang et al. (Zhang and Wang, 2015) proposed a recurrent neural network (RNN)-based model for RC. Other variants of RNN-based models have been proposed, such as Miwa et al. (Miwa and Bansal, 2016), who pr"
Y18-1015,S18-1129,0,0.0216144,"Missing"
Y18-1015,C14-1220,0,0.121267,"notational convenience, we refer to a sentence where a relation is identified as a target sentence, and we refer to the related entity pair as a target entity pair. Many previous works on RC exist in the general domain (Kumar, 2017; Zhou et al., 2014). The earlier approaches depend on complex feature engineering such as manually prepared lexical-syntactic patterns (Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010, etc.). Recently, Neural Network (NN)-based approaches achieve close or even better performance to earlier approaches without complicated manually prepared features (Zeng et al., 2014; Zhang and Wang, 2015; Santos et al., 2015). In the context of scientific RC, Ammar et al. (Ammar et al., 2017) enhanced Miwa and Bansal (Miwa 1In this work, entity refers not merely to concepts denoted by noun or noun phrase, it could be actions denoted by verb or verb phrase, and evaluation denoted by adjective or adverb etc. 129 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 and Bansal, 2016)’s end-to-end general relation extraction model by incorporating external knowledge such as gazetteer-like in"
Y18-1074,C18-1158,0,0.0540668,"Missing"
Y18-1074,N16-1062,0,0.0357378,"Missing"
Y18-1074,I17-2043,0,0.0117174,"receives enough flags, it is directed to a coalition of third-party fact-checking organizations, such as Snoops2 or FactCheck3 . To detect suspicious news articles and stop the propagation of fake news in the network, Tschiatschek et al. (2017) used the flags as a clue. Kim et al. (2018) also aimed to stop the spread of misinformation by leveraging user’s flags. Fake News Detection Using Textual Information Another line of studies on fake news detection on social media effectively used textual information (Mitra and Gilbert, 2015; Wang, 2017; Tacchini et al., 2017; P´erez-Rosas et al., 2017; Long et al., 2017; Vo and Lee, 2018; Yang et al., 2018). In particular, Volkova et al. (2017) is similar to our work. They built a computational model to judge whether a news article on social media is suspicious or verified. Also, if it is suspicious news, they classify it to one of the classes, satire, hoaxes, clickbait and propaganda. One main difference is that while the main goal of their task is to classify the input text, our goal is to detect suspicious news articles using SNS posts. 3 Tasks Our main objective is to detect suspicious news articles to be verified. In this section, we firstly explain our"
Y18-1074,D15-1167,0,0.055916,"Missing"
Y18-1074,Y18-1000,0,0.272203,"Missing"
Y18-1074,P17-2102,0,0.153041,"suspicious news detection dataset, which is publicly available.1 • We provide benchmark results on the dataset by using several basic machine learning techniques. 2 Related Work This section describes previous studies that tackle fake news detection. We firstly overview basic task settings of fake news detection. Then, we discuss several studies that share similar motivations with ours and deal with fake news detection on social media. 2.1 Task Settings of Fake News Detection Typically, fake news detection or fact-checking is defined and solved as binary prediction (P´erezRosas et al., 2017; Volkova et al., 2017; Gilda, 2017) or multi-class classification (Wang, 2017; Hassan et al., 2017). In this setting, given an input text x, the goal is to predict an appropriate class label y ∈ Y. 1 https://github.com/t-tagami/Suspicious-News-Detection The input text x can be a sentence (e.g., news headline, claim or statement) or document (e.g., news article or some passage). The class labels Y can be binary values or multi-class labels. One example of this task is the one defined and introduced by the pioneering work, Vlachos and Riedel (2014). Given an input claim x, the goal is to predict a label y from the f"
Y18-1074,P17-2067,0,0.439534,"ssue. One problematic issue of fake news detection is that human fact-checking experts cannot keep up with the amount of misinformation generated every day. Fact-checking requires advanced research techniques and is intellectually demanding. It takes about one day to fact-check a typical article and write a report to persuade readers whether it was true, false or somewhere in between (Hassan et al., 2015). Existing Approach. As a solution to the problem, various techniques and computational models for automatic fact-checking or fake news detection have been proposed (Vlachos and Riedel, 2014; Wang, 2017; Hanselowski et al., 2018). However, in practice, current computational models for automatic fake news detection cannot be used yet now due to the performance limitation. Thus, at the present, manual or partially automatic verification is a practical solution. Our Approach. To mitigate the problem, we aim to automate suspicious news detection. Specifically, we develop computational models for detecting suspicious news articles to be verified by human experts. We assume human-machine hybrid systems, in which suspicious articles are detected and sent to human experts and they verify the article"
