2021.findings-emnlp.301,Disentangling Generative Factors in Natural Language with Discrete Variational Autoencoders,2021,-1,-1,2,0,7151,giangiacomo mercatali,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"The ability of learning disentangled representations represents a major step for interpretable NLP systems as it allows latent linguistic features to be controlled. Most approaches to disentanglement rely on continuous variables, both for images and text. We argue that despite being suitable for image datasets, continuous variables may not be ideal to model features of textual data, due to the fact that most generative factors in text are discrete. We propose a Variational Autoencoder based method which models language features as discrete variables and encourages independence between variables for learning disentangled representations. The proposed model outperforms continuous and discrete baselines on several qualitative and quantitative benchmarks for disentanglement as well as on a text style transfer downstream application."
2021.findings-acl.1,Explainable Inference Over Grounding-Abstract Chains for Science Questions,2021,-1,-1,3,1,754,mokanarangan thayaparan,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.eval4nlp-1.21,What is {S}em{E}val evaluating? A Systematic Analysis of Evaluation Campaigns in {NLP},2021,-1,-1,4,0,8616,oskar wysocki,Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems,0,None
2021.eacl-main.15,Unification-based Reconstruction of Multi-hop Explanations for Science Questions,2021,-1,-1,3,1,755,marco valentino,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"This paper presents a novel framework for reconstructing multi-hop explanations in science Question Answering (QA). While existing approaches for multi-hop reasoning build explanations considering each question in isolation, we propose a method to leverage explanatory patterns emerging in a corpus of scientific explanations. Specifically, the framework ranks a set of atomic facts by integrating lexical relevance with the notion of unification power, estimated analysing explanations for similar questions in the corpus. An extensive evaluation is performed on the Worldtree corpus, integrating k-NN clustering and Information Retrieval (IR) techniques. We present the following conclusions: (1) The proposed method achieves results competitive with Transformers, yet being orders of magnitude faster, a feature that makes it scalable to large explanatory corpora (2) The unification-based mechanism has a key role in reducing semantic drift, contributing to the reconstruction of many hops explanations (6 or more facts) and the ranking of complex inference facts (+12.0 Mean Average Precision) (3) Crucially, the constructed explanations can support downstream QA models, improving the accuracy of BERT by up to 10{\%} overall."
2021.eacl-main.282,{STAR}: Cross-modal [{STA}]tement [{R}]epresentation for selecting relevant mathematical premises,2021,-1,-1,2,1,10931,deborah ferreira,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Mathematical statements written in natural language are usually composed of two different modalities: mathematical elements and natural language. These two modalities have several distinct linguistic and semantic properties. State-of-the-art representation techniques have demonstrated an inability in capturing such an entangled style of discourse. In this work, we propose STAR, a model that uses cross-modal attention to learn how to represent mathematical text for the task of Natural Language Premise Selection. This task uses conjectures written in both natural and mathematical language to recommend premises that most likely will be relevant to prove a particular statement. We found that STAR not only outperforms baselines that do not distinguish between natural language and mathematical elements, but it also achieves better performance than state-of-the-art models."
2021.acl-demo.23,Does My Representation Capture {X}? Probe-Ably,2021,-1,-1,5,1,10931,deborah ferreira,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations,0,"Probing (or diagnostic classification) has become a popular strategy for investigating whether a given set of intermediate features is present in the representations of neural models. Naive probing studies may have misleading results, but various recent works have suggested more reliable methodologies that compensate for the possible pitfalls of probing. However, these best practices are numerous and fast-evolving. To simplify the process of running a set of probing experiments in line with suggested methodologies, we introduce Probe-Ably: an extendable probing framework which supports and automates the application of probing methods to the user{'}s inputs."
2020.lrec-1.266,Natural Language Premise Selection: Finding Supporting Statements for Mathematical Text,2020,20,0,2,1,10931,deborah ferreira,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Mathematical text is written using a combination of words and mathematical expressions. This combination, along with a specific way of structuring sentences makes it challenging for state-of-art NLP tools to understand and reason on top of mathematical discourse. In this work, we propose a new NLP task, the natural premise selection, which is used to retrieve supporting definitions and supporting propositions that are useful for generating an informal mathematical proof for a particular statement. We also make available a dataset, NL-PS, which can be used to evaluate different approaches for the natural premise selection task. Using different baselines, we demonstrate the underlying interpretation challenges associated with the task."
2020.lrec-1.660,A Framework for Evaluation of Machine Reading Comprehension Gold Standards,2020,44,1,3,1,10979,viktor schlegel,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Machine Reading Comprehension (MRC) is the task of answering a question over a paragraph of text. While neural MRC systems gain popularity and achieve noticeable performance, issues are being raised with the methodology used to establish their performance, particularly concerning the data design of gold standards that are used to evaluate them. There is but a limited understanding of the challenges present in this data, which makes it hard to draw comparisons and formulate reliable hypotheses. As a first step towards alleviating the problem, this paper proposes a unifying framework to systematically investigate the present linguistic features, required reasoning and background knowledge and factual correctness on one hand, and the presence of lexical cues as a lower bound for the requirement of understanding on the other hand. We propose a qualitative annotation schema for the first and a set of approximative metrics for the latter. In a first application of the framework, we analyse modern MRC gold standards and present our findings: the absence of features that contribute towards lexical ambiguity, the varying factual correctness of the expected answers and the presence of lexical cues, all of which potentially lower the reading comprehension complexity and quality of the evaluation data."
2020.acl-main.657,Premise Selection in Natural Language Mathematical Texts,2020,-1,-1,2,1,10931,deborah ferreira,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"The discovery of supporting evidence for addressing complex mathematical problems is a semantically challenging task, which is still unexplored in the field of natural language processing for mathematical text. The natural language premise selection task consists in using conjectures written in both natural language and mathematical formulae to recommend premises that most likely will be useful to prove a particular statement. We propose an approach to solve this task as a link prediction problem, using Deep Convolutional Graph Neural Networks. This paper also analyses how different baselines perform in this task and shows that a graph structure can provide higher F1-score, especially when considering multi-hop premise selection."
W19-8615,{M}in{W}iki{S}plit: A Sentence Splitting Corpus with Minimal Propositions,2019,22,0,2,1,13150,christina niklaus,Proceedings of the 12th International Conference on Natural Language Generation,0,"We compiled a new sentence splitting corpus that is composed of 203K pairs of aligned complex source and simplified target sentences. Contrary to previously proposed text simplification corpora, which contain only a small number of split examples, we present a dataset where each input sentence is broken down into a set of minimal propositions, i.e. a sequence of sound, self-contained utterances with each of them presenting a minimal semantic unit that cannot be further decomposed into meaningful propositions. This corpus is useful for developing sentence splitting approaches that learn how to transform sentences with a complex linguistic structure into a fine-grained representation of short sentences that present a simple and more regular structure which is easier to process for downstream applications and thus facilitates and improves their performance."
W19-8662,{D}is{S}im: A Discourse-Aware Syntactic Text Simplification Framework for {E}nglish and {G}erman,2019,0,0,3,1,13150,christina niklaus,Proceedings of the 12th International Conference on Natural Language Generation,0,"We introduce DisSim, a discourse-aware sentence splitting framework for English and German whose goal is to transform syntactically complex sentences into an intermediate representation that presents a simple and more regular structure which is easier to process for downstream semantic applications. For this purpose, we turn input sentences into a two-layered semantic hierarchy in the form of core facts and accompanying contexts, while identifying the rhetorical relations that hold between them. In that way, we preserve the coherence structure of the input and, hence, its interpretability for downstream tasks."
P19-1333,Transforming Complex Sentences into a Semantic Hierarchy,2019,52,1,3,1,13150,christina niklaus,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We present an approach for recursively splitting and rephrasing complex English sentences into a novel semantic hierarchy of simplified sentences, with each of them presenting a more regular structure that may facilitate a wide variety of artificial intelligence tasks, such as machine translation (MT) or information extraction (IE). Using a set of hand-crafted transformation rules, input sentences are recursively transformed into a two-layered hierarchical representation in the form of core sentences and accompanying contexts that are linked via rhetorical relations. In this way, the semantic relationship of the decomposed constituents is preserved in the output, maintaining its interpretability for downstream applications. Both a thorough manual analysis and automatic evaluation across three datasets from two different domains demonstrate that the proposed syntactic simplification approach outperforms the state of the art in structural text simplification. Moreover, an extrinsic evaluation shows that when applying our framework as a preprocessing step the performance of state-of-the-art Open IE systems can be improved by up to 346{\%} in precision and 52{\%} in recall. To enable reproducible research, all code is provided online."
D19-5306,Identifying Supporting Facts for Multi-hop Question Answering with Document Graph Networks,2019,30,0,4,1,754,mokanarangan thayaparan,Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13),0,"Recent advances in reading comprehension have resulted in models that surpass human performance when the answer is contained in a single, continuous passage of text. However, complex Question Answering (QA) typically requires multi-hop reasoning - i.e. the integration of supporting facts from different sources, to infer the correct answer. This paper proposes Document Graph Network (DGN), a message passing architecture for the identification of supporting facts over a graph-structured representation of text. The evaluation on HotpotQA shows that DGN obtains competitive results when compared to a reading comprehension baseline operating on raw text, confirming the relevance of structured representations for supporting multi-hop reasoning."
D19-5322,{DB}ee: A Database for Creating and Managing Knowledge Graphs and Embeddings,2019,0,0,2,1,10979,viktor schlegel,Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13),0,"This paper describes DBee, a database to support the construction of data-intensive AI applications. DBee provides a unique data model which operates jointly over large-scale knowledge graphs (KGs) and embedding vector spaces (VSs). This model supports queries which exploit the semantic properties of both types of representations (KGs and VSs). Additionally, DBee aims to facilitate the construction of KGs and VSs, by providing a library of generators, which can be used to create, integrate and transform data into KGs and VSs."
D19-1440,Identifying and Explaining Discriminative Attributes,2019,0,0,2,0,27025,armins stepanjans,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Identifying what is at the center of the meaning of a word and what discriminates it from other words is a fundamental natural language inference task. This paper describes an explicit word vector representation model (WVM) to support the identification of discriminative attributes. A core contribution of the paper is a quantitative and qualitative comparative analysis of different types of data sources and Knowledge Bases in the construction of explainable and explicit WVMs: (i) knowledge graphs built from dictionary definitions, (ii) entity-attribute-relationships graphs derived from images and (iii) commonsense knowledge graphs. Using a detailed quantitative and qualitative analysis, we demonstrate that these data sources have complementary semantic aspects, supporting the creation of explicit semantic vector spaces. The explicit vector spaces are evaluated using the task of discriminative attribute identification, showing comparable performance to the state-of-the-art systems in the task (F1-score = 0.69), while delivering full model transparency and explainability."
L18-1211,{I}ndra: A Word Embedding and Semantic Relatedness Server,2018,0,5,5,1,29731,juliano sales,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1398,A Multilingual Test Collection for the Semantic Search of Entity Categories,2018,0,0,7,1,29731,juliano sales,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1423,"The {SSIX} Corpora: Three Gold Standard Corpora for Sentiment Analysis in {E}nglish, {S}panish and {G}erman Financial Microblogs",2018,0,1,3,0,12225,thomas gaillat,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"This paper introduces the three SSIX corpora for sentiment analysis. These corpora address the need to provide annotated data for supervised learning methods. They focus on stock-market related messages extracted from two financial microblog platforms, i.e., StockTwits and Twitter. In total they include 2,886 messages with opinion targets. These messages are provided with polarity annotation set on a continuous scale by three or four experts in each language. The annotation information identifies the targets with a sentiment score. The annotation process includes manual annotation verified and consolidated by financial experts. The creation of the annotated corpora took into account principled sampling strategies as well as inter-annotator agreement before consolidation in order to maximize data quality"
L18-1542,Building a Knowledge Graph from Natural Language Definitions for Interpretable Text Entailment Recognition,2018,10,0,2,0,30113,vivian silva,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Natural language definitions of terms can serve as a rich source of knowledge, but structuring them into a comprehensible semantic model is essential to enable them to be used in semantic interpretation tasks. We propose a method and provide a set of tools for automatically building a graph world knowledge base from natural language definitions. Adopting a conceptual model composed of a set of semantic roles for dictionary definitions, we trained a classifier for automatically labeling definitions, preparing the data to be later converted to a graph representation. WordNetGraph, a knowledge graph built out of noun and verb WordNet definitions according to this methodology, was successfully used in an interpretable text entailment recognition approach which uses paths in this graph to provide clear justifications for entailment decisions."
L18-1618,{S}em{R}-11: A Multi-Lingual Gold-Standard for Semantic Similarity and Relatedness for Eleven Languages,2018,17,7,5,0,29733,siamak barzegar,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"This publication has emanated from research funded in part from the Europeann Unionxe2x80x99s Horizon 2020 research and innovation programme under grant agreementn No 645425 SSIX and Science Foundationn Ireland (SFI) under Grant Number SFI/12/RC/2289.n We would like in particular to thank Alexandros Poulis andn Juha Vilhunen from the Global Services for Machine Intelligence Group, Lionbridge Finland 6n ensuring the production word of high quality translations for our similarityn datasets."
C18-2021,{G}raphene: a Context-Preserving Open Information Extraction System,2018,7,0,3,1,23361,matthias cetto,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"We introduce Graphene, an Open IE system whose goal is to generate accurate, meaningful and complete propositions that may facilitate a variety of downstream semantic applications. For this purpose, we transform syntactically complex input sentences into clean, compact structures in the form of core facts and accompanying contexts, while identifying the rhetorical relations that hold between them in order to maintain their semantic relationship. In that way, we preserve the context of the relational tuples extracted from a source sentence, generating a novel lightweight semantic representation for Open IE that enhances the expressiveness of the extracted propositions."
C18-1195,{G}raphene: Semantically-Linked Propositions in Open Information Extraction,2018,0,2,3,1,23361,matthias cetto,Proceedings of the 27th International Conference on Computational Linguistics,0,"We present an Open Information Extraction (IE) approach that uses a two-layered transformation stage consisting of a clausal disembedding layer and a phrasal disembedding layer, together with rhetorical relation identification. In that way, we convert sentences that present a complex linguistic structure into simplified, syntactically sound sentences, from which we can extract propositions that are represented in a two-layered hierarchy in the form of core relational tuples and accompanying contextual information which are semantically linked via rhetorical relations. In a comparative evaluation, we demonstrate that our reference implementation Graphene outperforms state-of-the-art Open IE systems in the construction of correct n-ary predicate-argument structures. Moreover, we show that existing Open IE approaches can benefit from the transformation process of our framework."
C18-1326,A Survey on Open Information Extraction,2018,0,11,3,1,13150,christina niklaus,Proceedings of the 27th International Conference on Computational Linguistics,0,"We provide a detailed overview of the various approaches that were proposed to date to solve the task of Open Information Extraction. We present the major challenges that such systems face, show the evolution of the suggested approaches over time and depict the specific issues they address. In addition, we provide a critique of the commonly applied evaluation procedures for assessing the performance of Open IE systems and highlight some directions for future work."
S17-2089,{S}em{E}val-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News,2017,19,29,2,0,26600,keith cortis,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper discusses the {``}Fine-Grained Sentiment Analysis on Financial Microblogs and News{''} task as part of SemEval-2017, specifically under the {``}Detecting sentiment, humour, and truth{''} theme. This task contains two tracks, where the first one concerns Microblog messages and the second one covers News Statements and Headlines. The main goal behind both tracks was to predict the sentiment score for each of the mentioned companies/stocks. The sentiment scores for each text instance adopted floating point values in the range of -1 (very negative/bearish) to 1 (very positive/bullish), with 0 designating neutral sentiment. This task attracted a total of 32 participants, with 25 participating in Track 1 and 29 in Track 2."
S17-2092,{S}em{E}val-2017 Task 11: End-User Development using Natural Language,2017,11,5,3,1,29731,juliano sales,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This task proposes a challenge to support the interaction between users and applications, micro-services and software APIs using natural language. The task aims for supporting the evaluation and evolution of the discussions surrounding the natural language processing approaches within the context of end-user natural language programming, under scenarios of high semantic heterogeneity/gap."
W16-5305,Semantic Relation Classification: Task Formalisation and Refinement,2016,0,0,5,0,33491,vivian santos,Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V),0,"The identification of semantic relations between terms within texts is a fundamental task in Natural Language Processing which can support applications requiring a lightweight semantic interpretation model. Currently, semantic relation classification concentrates on relations which are evaluated over open-domain data. This work provides a critique on the set of abstract relations used for semantic relation classification with regard to their ability to express relationships between terms which are found in a domain-specific corpora. Based on this analysis, this work proposes an alternative semantic relation model based on reusing and extending the set of abstract relations present in the DOLCE ontology. The resulting set of relations is well grounded, allows to capture a wide range of relations and could thus be used as a foundation for automatic classification of semantic relations."
W16-5323,Categorization of Semantic Roles for Dictionary Definitions,2016,0,1,3,0,30113,vivian silva,Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V),0,"Understanding the semantic relationships between terms is a fundamental task in natural language processing applications. While structured resources that can express those relationships in a formal way, such as ontologies, are still scarce, a large number of linguistic resources gathering dictionary definitions is becoming available, but understanding the semantic structure of natural language definitions is fundamental to make them useful in semantic interpretation tasks. Based on an analysis of a subset of WordNet{'}s glosses, we propose a set of semantic roles that compose the semantic structure of a dictionary definition, and show how they are related to the definition{'}s syntactic configuration, identifying patterns that can be used in the development of information extraction frameworks and semantic models."
S16-2025,A Compositional-Distributional Semantic Model for Searching Complex Entity Categories,2016,27,7,2,1,29731,juliano sales,Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics,0,None
L16-1330,{NNB}locks: A Deep Learning Framework for Computational Linguistics Neural Network Models,2016,6,0,2,0,35064,frederico caroli,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Lately, with the success of Deep Learning techniques in some computational linguistics tasks, many researchers want to explore new models for their linguistics applications. These models tend to be very different from what standard Neural Networks look like, limiting the possibility to use standard Neural Networks frameworks. This work presents NNBlocks, a new framework written in Python to build and train Neural Networks that are not constrained by a specific kind of architecture, making it possible to use it in computational linguistics."
C16-2036,A Sentence Simplification System for Improving Relation Extraction,2016,0,3,4,1,13150,christina niklaus,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"We present a text simplification approach that is directed at improving the performance of state-of-the-art Open Relation Extraction (RE) systems. As syntactically complex sentences often pose a challenge for current Open RE approaches, we have developed a simplification framework that performs a pre-processing step by taking a single sentence as input and using a set of syntactic-based transformation rules to create a textual input that is easier to process for subsequently applied Open RE systems."
W15-0133,How hard is this query? Measuring the Semantic Complexity of Schema-agnostic Queries,2015,13,3,1,1,7152,andre freitas,Proceedings of the 11th International Conference on Computational Semantics,0,"The growing size, heterogeneity and complexity of databases demand the creation of strategies to facilitate users and systems to consume data. Ideally, query mechanisms should be schema-agnostic, i.e. they should be able to match user queries in their own vocabulary and syntax to the data, abstracting data consumers from the representation of the data. This work provides an informationtheoretical framework to evaluate the semantic complexity involved in the query-database communication, under a schema-agnostic query scenario. Different entropy measures are introduced to quantify the semantic phenomena involved in the user-database communication, including structural complexity, ambiguity, synonymy and vagueness. The entropy measures are validated using natural language queries over Semantic Web databases. The analysis of the semantic complexity is used to improve the understanding of the core semantic dimensions present at the query-data matching process, allowing the improvement of the design of schema-agnostic query mechanisms and defining measures which can be used to assess the semantic uncertainty or difficulty behind a schema-agnostic querying task."
