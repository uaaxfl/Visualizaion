2020.lrec-1.642,W15-1804,1,0.772302,"rom formal to informal, and from informative to entertaining, including both news text and blogs. Eukalyptus has been manually annotated with part-ofspeech tags, morphological features, word senses, and syntactic structure. The syntactic description is similar to for example the German NEGRA/TIGER scheme (Brants et al., 1999). Tokens are connected into phrases, and each child (edge) has a function label. Phrases may be discontinuous. Secondary edges are used to mark various types of shared information in constructions such as coordination and control. The syntactic description is described in Adesam et al. (2015a). The treebank also uses special phrase labels to connect multiword units, detailed in Adesam et al. (2015b). We distinguish two types of multiword units: analyzable – which have an internal syntactic representation, and where a multiword node is added to attach the multiword label, gathering the parts of the multiword unit with secondary edges – and unanalyzable – which do not receive a syntactic analysis but are attached to the tree through their multiword node. A full example tree can be seen in Figure 4. The parts of speech together with the phrases and syntactic functions create a whole"
2020.lrec-1.642,A00-2031,0,0.277936,"Sri Lanka’ Figure 4: An annotated Eukalyptus tree including all layers of annotation. random seeds, and select the model that gives the highest score on a development set. The experiment uses two different evaluation metrics: (1) the F-score for finding and labeling constituents, such as S (sentence) and PP (prepositional phrase) in Figure 1a; (2) the accuracy of labeling edges, such as SB (subject) and OO (object) in Figure 1a. When evaluating, predicted and a gold-standard constituents are considered equal if their labels match and their yields (sets of covered tokens) are equal. Following Blaheta and Charniak (2000), only the correctly parsed constituents are included when computing the edge labeling accuracy. We used the evaluation module of D ISCO DOP (van Cranenburgh et al., 2016) to compute all scores. 5.3. that is more relevant when learning to create and label constituents in the target treebank. Treebanks Primary Primary + Dep Primary + Const Primary + Const + Dep sentences before dividing into cross-validation folds, each test fold will typically be sampled from a single genre, which will then also be under-represented in the training set. It seems likely that shuffling would give us slightly hig"
2020.lrec-1.642,P16-2006,0,0.0129138,"this context, and especially not in a training process that uses both constituency and dependency treebanks. 5219 2. Neural Transition-based Parsing The parsers considered in this work belong to the class of transition-based parsers. In this approach, the parser builds the output structure in a step-by-step fashion by executing actions in a state machine. This state machine uses a stack to store partially built structures and a buffer that keeps the remaining part of the input. Our parser uses a transition system based on the shift/promote/adjoin system for constituency parsing introduced by Cross and Huang (2016), to which Stanojevi´c and Garrido Alhama (2017) added a swap transition (Nivre, 2009) to allow for discontinuous constituents (e.g. the PP in Figure 1a). This transition system allows five different actions: S HIFT, which moves an item from the buffer onto the stack; P RO MOTE , which takes the top item of the stack and starts to build a constituent; L EFT- ADJOIN and R IGHT- ADJOIN, which attach an item to the left or to the right of a constituent, respectively; and S WAP, which moves the secondto-last item of the stack back into the buffer. Figure 2 states these actions formally; for brevit"
2020.lrec-1.642,P07-1033,0,0.11132,"Missing"
2020.lrec-1.642,P15-1033,0,0.0160564,"of a constituent, respectively; and S WAP, which moves the secondto-last item of the stack back into the buffer. Figure 2 states these actions formally; for brevity, we omit the R IGHT- ADJOIN action and the preconditions that determine whether an action is applicable. S HIFT h S, x|B i h S|x, B i P ROMOTE[C] h S|x, B i h S |C(x), B i L EFT- ADJOIN h S|x|C(X), B i h S |C(x|X), B i S WAP h S|x1 |x2 , B i h S|x2 , x1 |B i Figure 2: Actions in the transition system. The model used in the parser by Stanojevi´c and Garrido Alhama (2017) can be seen as a constituent-based variation of the model by Dyer et al. (2015). It relies on several variants of the long short-term memory (LSTM), a well-known model for representing sequential computations (Hochreiter and Schmidhuber, 1997). The components of the parsing model are the following: • word representations: a bidirectional LSTM applied to word and part-of-speech tag embeddings;1 • constituent representations: complex linguistic units are represented by applying a tree LSTM (Tai et al., 2015) compositionally; • state representations: two separate stack LSTMs (Dyer et al., 2015) represent the stack and buffer, respectively;2 1 The word embeddings were traine"
2020.lrec-1.642,Q13-1033,0,0.0197338,"hows the representation of an intermediate state when selecting an action during the generation of the tree in Figure 1a; the correct action in this situation would be to S WAP the token S˚ant back into the buffer, in order to build the discontinuous constituent. Figure 3: Representation of the parser’s state for determining the next action. The parser is trained using a static oracle, since no efficient dynamic oracle is known for this transition system (or its dependency counterpart, the arc-standard system). We note that dynamic oracles are available for closely related transition systems (Goldberg and Nivre, 2013), but require special care when a S WAP transition is used (de Lhoneux et al., 2017). We leave dynamic oracle training to future work. We extended the model by Stanojevi´c and Garrido Alhama (2017) so that it outputs function edge labels as well, e.g. OO, HD, OA, etc. in Figure 1a. The edge labels are generated when executing the P ROMOTE, L EFT- ADJOIN, and R IGHTADJOIN actions. Taking edge labels into account required modifications of the tree LSTM that builds constituent representations compositionally, as well as a new feedforward unit to predict the edge labels. 3. Multitask Learning for"
2020.lrec-1.642,P07-2053,0,0.13777,"Missing"
2020.lrec-1.642,N13-1013,1,0.922585,"r a language such as Swedish, for which the largest treebank (Nilsson et al., 2005) is significantly smaller than treebanks for e.g. English, Chinese, and German. At the same time multiple treebanks with different types of annotation – different types of encoded linguistic knowledge – are available. We are aware of at least five unique treebanks for Swedish, most of which are annotated according to their own respective models (some of them more than one model). Is there a remedy: can we develop an approach that can utilize additional treebanks? For non-neural, featurebased dependency parsers, Johansson (2013) proposed two S HD OA SB MD PP OO HD Sånt tror jag inte på . (a) Constituency annotation. root punct case advmod nsubj obl S˚ant tror jag inte p˚a . (b) Dependency annotation. Figure 1: An example sentence annotated according to two syntactic annotation models. feature-based approaches to multi-treebank training and evaluated them using treebank pairs in four languages. The first approach was based on multitask learning (Caruana, 1997) using a shared feature representation (Daum´e III, 2007), while the second approach used stacking or guided parsing (Nivre and McDonald, 2008). A more recent ap"
2020.lrec-1.642,W17-6314,0,0.029336,"Missing"
2020.lrec-1.642,P08-1108,0,0.0397178,"ebased dependency parsers, Johansson (2013) proposed two S HD OA SB MD PP OO HD Sånt tror jag inte på . (a) Constituency annotation. root punct case advmod nsubj obl S˚ant tror jag inte p˚a . (b) Dependency annotation. Figure 1: An example sentence annotated according to two syntactic annotation models. feature-based approaches to multi-treebank training and evaluated them using treebank pairs in four languages. The first approach was based on multitask learning (Caruana, 1997) using a shared feature representation (Daum´e III, 2007), while the second approach used stacking or guided parsing (Nivre and McDonald, 2008). A more recent approach, based on treebank embeddings in a neural dependency parser, was presented by Stymne et al. (2018); however, their approach has so far only been evaluated for sets of treebanks that are very close in annotation style. In this work, we present the first results for parsing with the Eukalyptus treebank of written Swedish (Adesam et al., 2018), a function-tagged constituency treebank including discontinuous constituents. Furthermore, we show how a transition-based neural parser can be improved by using a multitask architecture that allows us to train the parser using a nu"
2020.lrec-1.642,P09-1040,0,0.0481032,"cy treebanks. 5219 2. Neural Transition-based Parsing The parsers considered in this work belong to the class of transition-based parsers. In this approach, the parser builds the output structure in a step-by-step fashion by executing actions in a state machine. This state machine uses a stack to store partially built structures and a buffer that keeps the remaining part of the input. Our parser uses a transition system based on the shift/promote/adjoin system for constituency parsing introduced by Cross and Huang (2016), to which Stanojevi´c and Garrido Alhama (2017) added a swap transition (Nivre, 2009) to allow for discontinuous constituents (e.g. the PP in Figure 1a). This transition system allows five different actions: S HIFT, which moves an item from the buffer onto the stack; P RO MOTE , which takes the top item of the stack and starts to build a constituent; L EFT- ADJOIN and R IGHT- ADJOIN, which attach an item to the left or to the right of a constituent, respectively; and S WAP, which moves the secondto-last item of the stack back into the buffer. Figure 2 states these actions formally; for brevity, we omit the R IGHT- ADJOIN action and the preconditions that determine whether an a"
2020.lrec-1.642,P16-2038,0,0.0853656,"atible with our target model do not need to be wasted. Constituent treebanks seem to be more useful as auxiliary treebanks when training a constituent parser, although the dependency treebanks also give an improvement. This is our first investigation of multi-treebank training for neural transition-based constituency parsers and it remains an open research problem to fully explore the spectrum of sharing architectures and find the one that best utilizes the auxiliary treebanks. The importance of this design choice has been discussed extensively for other NLP tasks (Ruder, 2017); for instance, Søgaard and Goldberg (2016) designed a carefully crafted sharing architecture for sequence labeling tasks. Ruder et al. (2017) discuss a method to learn the sharing architecture. In addition, it would be useful to investigate how the utility of a multitask setup is affected by the size of the primary treebank (Johansson, 2013). 5222 Acknowledgments We are grateful to Miloˇs Stanojevi´c and Raquel Alhama for making their implementation available. RJ was funded by the Swedish Research Council (VR) under grant 2013– 4944. The Eukalyptus corpus was developed in a project funded by Riksbankens Jubilemsfond, grant In13-0320:1"
2020.lrec-1.642,D17-1174,0,0.0301543,"Missing"
2020.lrec-1.642,P18-2098,0,0.300675,"Missing"
2020.lrec-1.642,P15-1150,0,0.041979,"Missing"
2020.lrec-1.642,L16-1262,0,0.234974,"nd especially not in a training process that uses both constituency and dependency treebanks. 5219 2. Neural Transition-based Parsing The parsers considered in this work belong to the class of transition-based parsers. In this approach, the parser builds the output structure in a step-by-step fashion by executing actions in a state machine. This state machine uses a stack to store partially built structures and a buffer that keeps the remaining part of the input. Our parser uses a transition system based on the shift/promote/adjoin system for constituency parsing introduced by Cross and Huang (2016), to which Stanojevi´c and Garrido Alhama (2017) added a swap transition (Nivre, 2009) to allow for discontinuous constituents (e.g. the PP in Figure 1a). This transition system allows five different actions: S HIFT, which moves an item from the buffer onto the stack; P RO MOTE , which takes the top item of the stack and starts to build a constituent; L EFT- ADJOIN and R IGHT- ADJOIN, which attach an item to the left or to the right of a constituent, respectively; and S WAP, which moves the secondto-last item of the stack back into the buffer. Figure 2 states these actions formally; for brevit"
2020.osact-1.1,W15-3202,0,0.044828,"Missing"
2020.osact-1.1,P16-1066,0,0.0344178,"Missing"
2020.osact-1.1,P13-2088,0,0.00876999,"etrained sentiment analysis models that have been built and trained on existing datasets. The following datasets are used in our experiments and shown in Table 3: Corpus 40k tweets LABR 2 Balanced ASTD Shami-Senti • 40k dataset (Mohammed and Kora, 2019): as mentioned in the related work section, this is a tweets dataset containing 40,000 instances. It is manually annotated into positive and negative and the tweets are subsequently manually cleaned. NEG 20,002 6,578 1,496 935 POS 19,998 6,580 665 1,064 Table 3: The number of instances per category in the corpora used in our experiments • LABR (Aly and Atiya, 2013): a large SA dataset for Arabic sentiment analysis. The data are extracted from a book review website and contain over 63k book reviews written in MSA with some dialectal phrases. We build a model on each corpus and apply the resulting model to our Twitter corpus. The model uses a combination of (1-3) word grams and a LinearSVC classifier. Table 4 4 shows the accuracy of the models built (trained and tested) on the original datasets, while the ASTAD column shows the accuracy of the trained model when we use it to predict the class on our Twitter dataset. It is clear that none of the models wor"
2020.osact-1.1,W12-3704,0,0.0278826,"Missing"
2020.osact-1.1,C16-1228,0,0.0413582,"Missing"
2020.osact-1.1,P11-1055,0,0.0219689,"Missing"
2020.osact-1.1,D15-1299,0,0.0231222,"Missing"
2020.osact-1.1,W19-5606,1,0.721135,"ic dialect keywords efficiently. The corpus contains 3,550 Jordanian dialect tweets manually annotated as follows: 616 positive tweets, 1,313 negative tweets, and 1,621 neutral tweets. They conducted several experiments both with and without stemming/rooting applying them to several models with uni-grams/bi-grams and trying NB and SVM classifiers. The result shows that the SVM classifier performs better than the NB classifier. The ROC performance reached an average of 0.71, 0.77 on NB and SVM respectively on all experiments. A similar corpus for Levantine dialects is presented in Shami-Senti (Qwaider et al., 2019). 3 2 https://grammarist.com/new-words/emoji-vs-emoticon/ can then be used as a gold standard for any tweets sentiment analysis task and as the test set for our corpus. 3. icon contains both positive and negative emojis expressing the feelings corresponding to different sentiment categories. We collect the emojis as well as their indicated sentiment from “Emojis Sentiment Ranking Lexicon” (Kralj Novak et al., 2015) which is available at http://kt. ijs.si/data/Emoji_sentiment_ranking/ and Emojipedia4 . Then, this lexicon is employed as the seed for the Twitter retrieval procedure. The Lexicon i"
2020.osact-1.1,W14-3624,0,0.0392756,"Missing"
2020.osact-1.1,W11-2207,0,0.100848,"Missing"
2020.osact-1.1,D10-1099,0,0.0455286,"Missing"
2020.osact-1.1,P95-1026,0,0.653369,"Missing"
2021.blackboxnlp-1.10,2020.tacl-1.28,0,0.18681,"e models to hold large amounts of linguistic as well as factual knowledge in their parameters. While impressive, without strong task-specific fine-tuning these models are prone to outputting false or inconsistent statements, often also referred to as hallucination (Logan et al., 2019). This has been particularly studied for generative tasks such as abstractive text summarization (Maynez et al., ∗ Equal contribution. 2020) and dialog systems (Roller et al., 2021; Li et al., 2020a), but the problem is also apparent for models applied to cloze-style fill-in-the-blank tasks (Petroni et al., 2019; Jiang et al., 2020). Having truthful NLP systems is a core requirement for most applications, which is why this is an important problem to address. Grounding has been proposed as a potential way to mitigate this problem, e.g by providing broader world information from for example multimodal perception (Bisk et al., 2020; Bender and Koller, 2020). Information from multimodal perception may actually provide a significant amount of additional world information to an NLP model, since text data suffers from the problem of reporting bias. That is, humans generally communicate novel information rather than trivial, lea"
2021.blackboxnlp-1.10,N18-1038,0,0.15913,"information to an NLP model, since text data suffers from the problem of reporting bias. That is, humans generally communicate novel information rather than trivial, leading to a discrepancy between reality and what gets described in text (Gordon and Van Durme, 2013). Consequently, perceptual information may contain complementing world knowledge that cannot be found in text data, and has the potential to mitigate the aforementioned problem of hallucinating NLP models. Previous works have evaluated how grounded language representations impact performance on common NLP benchmarks (Sileo, 2021; Kiela et al., 2018; Elliott and K´ad´ar, 2017), but little has been done on investigating grounding specifically as an additional source of knowledge. In this work, we take a focused look at how data from a visual modality can augment the knowledge a language model expresses. We design an experimental setup to enable the development of strategies for maximizing visual-to-textual knowledge transfer. In the setup, we create a small knowledgecentric cloze-style task in English named Memory Colors that is tailored to test for visual knowledge by querying for the typical colors of well-known items. We also build a l"
2021.blackboxnlp-1.10,2020.acl-main.428,0,0.0760571,"vised multitask learners (Radford et al., 2019; Brown et al., 2020). An important contributing factor to this is the capability of the models to hold large amounts of linguistic as well as factual knowledge in their parameters. While impressive, without strong task-specific fine-tuning these models are prone to outputting false or inconsistent statements, often also referred to as hallucination (Logan et al., 2019). This has been particularly studied for generative tasks such as abstractive text summarization (Maynez et al., ∗ Equal contribution. 2020) and dialog systems (Roller et al., 2021; Li et al., 2020a), but the problem is also apparent for models applied to cloze-style fill-in-the-blank tasks (Petroni et al., 2019; Jiang et al., 2020). Having truthful NLP systems is a core requirement for most applications, which is why this is an important problem to address. Grounding has been proposed as a potential way to mitigate this problem, e.g by providing broader world information from for example multimodal perception (Bisk et al., 2020; Bender and Koller, 2020). Information from multimodal perception may actually provide a significant amount of additional world information to an NLP model, sin"
2021.blackboxnlp-1.10,P19-1598,0,0.022828,"ng results for leveraging multimodal knowledge in a unimodal setting. 1 Introduction Large language models have proved performant across a diverse set of tasks in NLP, and most recently even as unsupervised multitask learners (Radford et al., 2019; Brown et al., 2020). An important contributing factor to this is the capability of the models to hold large amounts of linguistic as well as factual knowledge in their parameters. While impressive, without strong task-specific fine-tuning these models are prone to outputting false or inconsistent statements, often also referred to as hallucination (Logan et al., 2019). This has been particularly studied for generative tasks such as abstractive text summarization (Maynez et al., ∗ Equal contribution. 2020) and dialog systems (Roller et al., 2021; Li et al., 2020a), but the problem is also apparent for models applied to cloze-style fill-in-the-blank tasks (Petroni et al., 2019; Jiang et al., 2020). Having truthful NLP systems is a core requirement for most applications, which is why this is an important problem to address. Grounding has been proposed as a potential way to mitigate this problem, e.g by providing broader world information from for example mult"
2021.blackboxnlp-1.10,D19-1250,0,0.0607941,"Missing"
2021.blackboxnlp-1.10,2021.eacl-main.24,0,0.0429938,"ently even as unsupervised multitask learners (Radford et al., 2019; Brown et al., 2020). An important contributing factor to this is the capability of the models to hold large amounts of linguistic as well as factual knowledge in their parameters. While impressive, without strong task-specific fine-tuning these models are prone to outputting false or inconsistent statements, often also referred to as hallucination (Logan et al., 2019). This has been particularly studied for generative tasks such as abstractive text summarization (Maynez et al., ∗ Equal contribution. 2020) and dialog systems (Roller et al., 2021; Li et al., 2020a), but the problem is also apparent for models applied to cloze-style fill-in-the-blank tasks (Petroni et al., 2019; Jiang et al., 2020). Having truthful NLP systems is a core requirement for most applications, which is why this is an important problem to address. Grounding has been proposed as a potential way to mitigate this problem, e.g by providing broader world information from for example multimodal perception (Bisk et al., 2020; Bender and Koller, 2020). Information from multimodal perception may actually provide a significant amount of additional world information to"
2021.blackboxnlp-1.10,P18-1238,0,0.0709346,"Missing"
2021.nodalida-main.13,2020.repl4nlp-1.10,0,0.21353,"ansson@cse.gu.se done on model efficiency. More efficient models are needed both for the sake of the environment and for the sake of equal research opportunities. Here we define an “efficient model” based on both performance and computational cost, such that a model is more efficient if it has better performance or lower computational cost, and vice versa. Knowledge distillation (Hinton et al., 2015) is one way to improve model efficiency during deployment. There are several works on successful application of knowledge distillation both for pretraining tasks and for specific downstream tasks. Adhikari et al. (2020) show that knowledge distillation can be used to improve deployment efficiency of models for the downstream task of document classification in English. In this article we investigate the effect of knowledge distillation on models for named entity recognition (NER) in Swedish.1 The intention is to shed some light on how well knowledge distillation performs for different sequence tagging models and in the Swedish language. Our main goal is to contribute to better model efficiency within NLP. Naturally, this entails that we also focus on measuring the efficiency of each model investigated. Hopefu"
2021.nodalida-main.13,N19-1423,0,0.480298,"distillation for named entity recognition models in Swedish. We show that while some sequence tagging models benefit from knowledge distillation, not all models do. This prompts us to ask questions about in which situations and for which models knowledge distillation is beneficial. We also reason about the effect of knowledge distillation on computational costs. 1 Introduction Currently, most research that pushes the boundary for state-of-the-art performance within natural language processing involves the increase of number of model parameters as well as the computations needed for training (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020). The trend seems to be that the larger the model, the better the performance. As noted by Strubell et al. (2019) these state-of-the-art models require significant computational resources during training as well as deployment. While it certainly is a good thing that state-of-the-art performance within NLP is continuously improving, there is work to be Richard Johansson University of Gothenburg Sweden richard.johansson@cse.gu.se done on model efficiency. More efficient models are needed both for the sake of the environment and for the sake of equal res"
2021.nodalida-main.13,2020.emnlp-main.393,0,0.0721138,"Missing"
2021.nodalida-main.13,P84-1044,0,0.278221,"Missing"
2021.nodalida-main.13,2020.lrec-1.565,0,0.0250759,"either names of person (PER), location (LOC), organization (ORG), miscellaneous (MISC) or not an entity (O). Much work has been done on NER for English, with several models trained on the data, as seen in section 2.2. However, the same cannot be said for other languages. Firstly, there is the issue of obtaining an adequate training, development and test dataset for NER. The largest Swedish dataset which can be used for NER is built on the SUC 3.0 dataset (Ejerhed et al., 1992). NER data resources have also been developed in other North-Germanic languages and work on this is ongoing. Recently, Hvingelby et al. (2020) created a novel NER dataset for Danish. In the same article, they provide an overview of the available NER datasets for similar languages, such as Swedish and Norwegian. They also train a BERT model for their Danish NER task and obtain an f1 score of 83.76. 2.2 Named Entity Recognition models When Devlin et al. (2019) tested their BERT model on the downstream task of NER they used the CoNLL-2003 English data and obtained an f1 score of 92.4 with their base model. One previous state of the art model for NER before BERT, named “CCNN+WLSTM+CRF”, is provided by Yang and Zhang (2018) and Ma and Ho"
2021.nodalida-main.13,P16-1101,0,0.291627,"l. (2020) created a novel NER dataset for Danish. In the same article, they provide an overview of the available NER datasets for similar languages, such as Swedish and Norwegian. They also train a BERT model for their Danish NER task and obtain an f1 score of 83.76. 2.2 Named Entity Recognition models When Devlin et al. (2019) tested their BERT model on the downstream task of NER they used the CoNLL-2003 English data and obtained an f1 score of 92.4 with their base model. One previous state of the art model for NER before BERT, named “CCNN+WLSTM+CRF”, is provided by Yang and Zhang (2018) and Ma and Hovy (2016).2 It does not use hand-crafted features or deep contextualized word embeddings. 2.3 Model efficiency Research that focuses on model efficiency and energy consumption is seemingly on the rise. The most noteworthy contribution within the field of NLP is that of Strubell et al. (2019). In their work, Strubell et al. claim that the NLP field would benefit from reporting training time and sensitivity to hyperparameters for developed models. Additionally, Clark et al. (2020) argue that compute efficiency should be taken in consideration together with downstream performance for representation learni"
2021.nodalida-main.13,P19-1355,0,0.10838,"l models do. This prompts us to ask questions about in which situations and for which models knowledge distillation is beneficial. We also reason about the effect of knowledge distillation on computational costs. 1 Introduction Currently, most research that pushes the boundary for state-of-the-art performance within natural language processing involves the increase of number of model parameters as well as the computations needed for training (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020). The trend seems to be that the larger the model, the better the performance. As noted by Strubell et al. (2019) these state-of-the-art models require significant computational resources during training as well as deployment. While it certainly is a good thing that state-of-the-art performance within NLP is continuously improving, there is work to be Richard Johansson University of Gothenburg Sweden richard.johansson@cse.gu.se done on model efficiency. More efficient models are needed both for the sake of the environment and for the sake of equal research opportunities. Here we define an “efficient model” based on both performance and computational cost, such that a model is more efficient if it has bet"
2021.nodalida-main.13,E99-1023,0,0.671702,"Missing"
2021.nodalida-main.13,2020.emnlp-demos.6,0,0.0980983,"Missing"
2021.nodalida-main.13,P18-4013,0,0.169633,". Recently, Hvingelby et al. (2020) created a novel NER dataset for Danish. In the same article, they provide an overview of the available NER datasets for similar languages, such as Swedish and Norwegian. They also train a BERT model for their Danish NER task and obtain an f1 score of 83.76. 2.2 Named Entity Recognition models When Devlin et al. (2019) tested their BERT model on the downstream task of NER they used the CoNLL-2003 English data and obtained an f1 score of 92.4 with their base model. One previous state of the art model for NER before BERT, named “CCNN+WLSTM+CRF”, is provided by Yang and Zhang (2018) and Ma and Hovy (2016).2 It does not use hand-crafted features or deep contextualized word embeddings. 2.3 Model efficiency Research that focuses on model efficiency and energy consumption is seemingly on the rise. The most noteworthy contribution within the field of NLP is that of Strubell et al. (2019). In their work, Strubell et al. claim that the NLP field would benefit from reporting training time and sensitivity to hyperparameters for developed models. Additionally, Clark et al. (2020) argue that compute efficiency should be taken in consideration together with downstream performance fo"
berglund-etal-2006-extraction,W01-1311,0,\N,Missing
berglund-etal-2006-extraction,P04-1074,0,\N,Missing
berglund-etal-2006-extraction,N03-2019,0,\N,Missing
berglund-etal-2006-extraction,J88-2006,0,\N,Missing
berglund-etal-2006-extraction,P05-3021,0,\N,Missing
berglund-etal-2006-extraction,E95-1035,0,\N,Missing
berglund-etal-2006-extraction,N04-1020,0,\N,Missing
C08-1050,P98-1013,0,0.28491,"ntax was used in the best-performing system in the SemEval-2007 task on Frame-semantic Structure Extraction (Baker et al., 2007), and the conversion method (in two different forms) was used for the English data in the CoNLL Shared Tasks of 2007 and 2008. 3 Automatic Semantic Role Labeling with Constituents and Dependencies To study the influence of syntactic representation on SRL performance, we developed a framework that could be easily parametrized to process either constituent or dependency input1 . This section describes its implementation. As the role-semantic paradigm, we used FrameNet (Baker et al., 1998). 3.1 Systems We built SRL systems based on six different parsers. All parsers were trained on the Penn Treebank, either directly for the constituent parsers or through the LTH constituent-to-dependency converter (Johansson and Nugues, 2007). Our systems are identified as follows: LTH. A dependency-based system using the LTH parser (Johansson and Nugues, 2008). Malt. A dependency-based system using MaltParser (Nivre et al., 2007). MST. A dependency-based system using MSTParser (McDonald et al., 2005). C&J. A constituent-based system using the reranking parser (the May 2006 version) by Charniak"
C08-1050,S07-1018,0,0.293971,"re robust to domain changes and makes them learn more efficiently with respect to the amount of training data. 1 Introduction The role-semantic paradigm has a long and rich history in linguistics, and the NLP community has recently devoted much attention to developing accurate and robust methods for performing c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. role-semantic analysis automatically (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007). It is widely conjectured that an increased SRL accuracy will lead to improvements in certain NLP applications, especially template-filling systems. SRL has also been used in prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-semantic analysis have used Pennstyle constituents (Marcus et al., 199"
C08-1050,W06-2920,0,0.0142862,"s then what information this input should represent. By habit, most systems for automatic role-semantic analysis have used Pennstyle constituents (Marcus et al., 1993) produced by Collins’ (1997) or Charniak’s (2000) parsers. The influence of the syntactic formalism on SRL has only been considered in a few previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of interest in dependency parsing during the last few years (Buchholz and Marsi, 2006). Early examples of dependency-based SRL systems, which used goldstandard dependency treebanks, include Žabokrtský et al. (2002) and Hacioglu (2004). Two studies that compared the respective performances of constituent-based and dependency-based SRL systems (Pradhan et al., 2005; Swanson and Gordon, 2006), both using automatic parsers, reported that the constituent-based systems outperformed the dependency-based ones by a very wide margin. However, the figures reported in these studies can be misleading since the comparison involved a 10-year-old rule-based dependency parser versus a state-of-"
C08-1050,W08-2123,1,0.772855,"tactic representation on SRL performance, we developed a framework that could be easily parametrized to process either constituent or dependency input1 . This section describes its implementation. As the role-semantic paradigm, we used FrameNet (Baker et al., 1998). 3.1 Systems We built SRL systems based on six different parsers. All parsers were trained on the Penn Treebank, either directly for the constituent parsers or through the LTH constituent-to-dependency converter (Johansson and Nugues, 2007). Our systems are identified as follows: LTH. A dependency-based system using the LTH parser (Johansson and Nugues, 2008). Malt. A dependency-based system using MaltParser (Nivre et al., 2007). MST. A dependency-based system using MSTParser (McDonald et al., 2005). C&J. A constituent-based system using the reranking parser (the May 2006 version) by Charniak and Johnson (2005). Charniak. A constituent-based system using Charniak’s parser (Charniak, 2000). Collins. A constituent-based system using Collins’ parser (Collins, 1997). 2 Statistical Dependency Parsing for English Except for small-scale efforts, there is no dependency treebank of significant size for English. Statistical dependency parsers of English mus"
C08-1050,W04-0803,0,0.0283014,"on lexicalized features, which makes them more robust to domain changes and makes them learn more efficiently with respect to the amount of training data. 1 Introduction The role-semantic paradigm has a long and rich history in linguistics, and the NLP community has recently devoted much attention to developing accurate and robust methods for performing c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. role-semantic analysis automatically (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007). It is widely conjectured that an increased SRL accuracy will lead to improvements in certain NLP applications, especially template-filling systems. SRL has also been used in prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-semantic analysis hav"
C08-1050,J93-2004,0,0.0403742,"aker et al., 2007). It is widely conjectured that an increased SRL accuracy will lead to improvements in certain NLP applications, especially template-filling systems. SRL has also been used in prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-semantic analysis have used Pennstyle constituents (Marcus et al., 1993) produced by Collins’ (1997) or Charniak’s (2000) parsers. The influence of the syntactic formalism on SRL has only been considered in a few previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of interest in dependency parsing during the last few years (Buchholz and Marsi, 2006). Early examples of dependency-based SRL systems, which used goldstandard dependency treebanks, include Žabokrtský et al. (2002) and Hacioglu"
C08-1050,P05-1012,0,0.0949956,"rthermore, we show that semantic role classifiers using a dependency parser learn faster than their constituentbased counterparts and therefore need less training data to achieve similar performances. Finally, dependency-based role classifiers are more robust to vocabulary change and outperform constituentbased systems when using out-of-domain test sets. its roots in lexicalized constituent parsing (Magerman, 1994; Collins, 1997). The head rules created by Yamada and Matsumoto (2003) have been used in almost all recent work on statistical dependency parsing of English (Nivre and Scholz, 2004; McDonald et al., 2005). Recently, Johansson and Nugues (2007) extended the head percolation strategy to incorporate long-distance links such as wh-movement and topicalization, and used the full set of grammatical function tags from Penn in addition to a number of inferred tags (in total 57 function tags). A dependency parser based on this syntax was used in the best-performing system in the SemEval-2007 task on Frame-semantic Structure Extraction (Baker et al., 2007), and the conversion method (in two different forms) was used for the English data in the CoNLL Shared Tasks of 2007 and 2008. 3 Automatic Semantic Rol"
C08-1050,C04-1010,0,0.00978901,"rform nearly as well. Furthermore, we show that semantic role classifiers using a dependency parser learn faster than their constituentbased counterparts and therefore need less training data to achieve similar performances. Finally, dependency-based role classifiers are more robust to vocabulary change and outperform constituentbased systems when using out-of-domain test sets. its roots in lexicalized constituent parsing (Magerman, 1994; Collins, 1997). The head rules created by Yamada and Matsumoto (2003) have been used in almost all recent work on statistical dependency parsing of English (Nivre and Scholz, 2004; McDonald et al., 2005). Recently, Johansson and Nugues (2007) extended the head percolation strategy to incorporate long-distance links such as wh-movement and topicalization, and used the full set of grammatical function tags from Penn in addition to a number of inferred tags (in total 57 function tags). A dependency parser based on this syntax was used in the best-performing system in the SemEval-2007 task on Frame-semantic Structure Extraction (Baker et al., 2007), and the conversion method (in two different forms) was used for the English data in the CoNLL Shared Tasks of 2007 and 2008."
C08-1050,W05-0620,0,0.0662063,"eatures, which makes them more robust to domain changes and makes them learn more efficiently with respect to the amount of training data. 1 Introduction The role-semantic paradigm has a long and rich history in linguistics, and the NLP community has recently devoted much attention to developing accurate and robust methods for performing c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. role-semantic analysis automatically (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007). It is widely conjectured that an increased SRL accuracy will lead to improvements in certain NLP applications, especially template-filling systems. SRL has also been used in prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-semantic analysis have used Pennstyle constituent"
C08-1050,P05-1072,0,0.0163709,"een considered in a few previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of interest in dependency parsing during the last few years (Buchholz and Marsi, 2006). Early examples of dependency-based SRL systems, which used goldstandard dependency treebanks, include Žabokrtský et al. (2002) and Hacioglu (2004). Two studies that compared the respective performances of constituent-based and dependency-based SRL systems (Pradhan et al., 2005; Swanson and Gordon, 2006), both using automatic parsers, reported that the constituent-based systems outperformed the dependency-based ones by a very wide margin. However, the figures reported in these studies can be misleading since the comparison involved a 10-year-old rule-based dependency parser versus a state-of-the-art statistical constituent parser. The recent progress in statistical dependency parsing gives grounds for a new evaluation. 393 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 393–400 Manchester, August 2008 In addition, t"
C08-1050,P05-1022,0,0.0188074,"Missing"
C08-1050,J08-2006,0,0.090515,"Missing"
C08-1050,A00-2018,0,0.232178,"Missing"
C08-1050,J08-2005,0,0.0369206,"ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. role-semantic analysis automatically (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007). It is widely conjectured that an increased SRL accuracy will lead to improvements in certain NLP applications, especially template-filling systems. SRL has also been used in prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-semantic analysis have used Pennstyle constituents (Marcus et al., 1993) produced by Collins’ (1997) or Charniak’s (2000) parsers. The influence of the syntactic formalism on SRL has only been considered in a few previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of interest in dependency parsi"
C08-1050,P97-1003,0,0.215131,"tic role labeling in English. Contrary to previously reported results, we show that dependencybased systems are on a par with constituent-based systems or perform nearly as well. Furthermore, we show that semantic role classifiers using a dependency parser learn faster than their constituentbased counterparts and therefore need less training data to achieve similar performances. Finally, dependency-based role classifiers are more robust to vocabulary change and outperform constituentbased systems when using out-of-domain test sets. its roots in lexicalized constituent parsing (Magerman, 1994; Collins, 1997). The head rules created by Yamada and Matsumoto (2003) have been used in almost all recent work on statistical dependency parsing of English (Nivre and Scholz, 2004; McDonald et al., 2005). Recently, Johansson and Nugues (2007) extended the head percolation strategy to incorporate long-distance links such as wh-movement and topicalization, and used the full set of grammatical function tags from Penn in addition to a number of inferred tags (in total 57 function tags). A dependency parser based on this syntax was used in the best-performing system in the SemEval-2007 task on Frame-semantic Str"
C08-1050,W03-1008,0,0.0176731,"prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-semantic analysis have used Pennstyle constituents (Marcus et al., 1993) produced by Collins’ (1997) or Charniak’s (2000) parsers. The influence of the syntactic formalism on SRL has only been considered in a few previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of interest in dependency parsing during the last few years (Buchholz and Marsi, 2006). Early examples of dependency-based SRL systems, which used goldstandard dependency treebanks, include Žabokrtský et al. (2002) and Hacioglu (2004). Two studies that compared the respective performances of constituent-based and dependency-based SRL systems (Pradhan et al., 2005; Swanson and Gordon, 2006), both using automatic parsers, reporte"
C08-1050,J02-3001,0,0.950855,"role classifiers rely less on lexicalized features, which makes them more robust to domain changes and makes them learn more efficiently with respect to the amount of training data. 1 Introduction The role-semantic paradigm has a long and rich history in linguistics, and the NLP community has recently devoted much attention to developing accurate and robust methods for performing c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. role-semantic analysis automatically (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007). It is widely conjectured that an increased SRL accuracy will lead to improvements in certain NLP applications, especially template-filling systems. SRL has also been used in prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-sema"
C08-1050,P02-1031,0,0.0193685,"ercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. role-semantic analysis automatically (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007). It is widely conjectured that an increased SRL accuracy will lead to improvements in certain NLP applications, especially template-filling systems. SRL has also been used in prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-semantic analysis have used Pennstyle constituents (Marcus et al., 1993) produced by Collins’ (1997) or Charniak’s (2000) parsers. The influence of the syntactic formalism on SRL has only been considered in a few previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of in"
C08-1050,C04-1186,0,0.0179146,"l., 1993) produced by Collins’ (1997) or Charniak’s (2000) parsers. The influence of the syntactic formalism on SRL has only been considered in a few previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of interest in dependency parsing during the last few years (Buchholz and Marsi, 2006). Early examples of dependency-based SRL systems, which used goldstandard dependency treebanks, include Žabokrtský et al. (2002) and Hacioglu (2004). Two studies that compared the respective performances of constituent-based and dependency-based SRL systems (Pradhan et al., 2005; Swanson and Gordon, 2006), both using automatic parsers, reported that the constituent-based systems outperformed the dependency-based ones by a very wide margin. However, the figures reported in these studies can be misleading since the comparison involved a 10-year-old rule-based dependency parser versus a state-of-the-art statistical constituent parser. The recent progress in statistical dependency parsing gives grounds for a new evaluation. 393 Proceedings of"
C08-1050,P83-1010,0,0.569858,"as wh-word extraction and topicalization can be transparently represented by allowing nonprojective dependency links. These links also justify why dependency syntax is often considered superior for free-word-order languages; it is even very questionable whether the traditional constituent-based SRL strategies are viable for such languages. Second, grammatical function such as subject and object is an integral concept in dependency syntax. This concept is intuitive when reasoning about the link between syntax and semantics, and it has been used earlier in semantic interpreters such as Absity (Hirst, 1983). However, except from a few tentative experiments (Toutanova et al., 2005), grammatical function is not explicitly used by current automatic SRL systems, but instead emulated from constituent trees by features like the constituent position and the governing category. More generally, these linguistic reasons have made a number of linguists argue that dependency structures are more suitable for explaining the syntax-semantics interface (Mel’ˇcuk, 1988; Hudson, 1984). In this work, we provide a new evaluation of the influence of the syntactic representation on semantic role labeling in English."
C08-1050,W07-2416,1,0.406304,"ole classifiers using a dependency parser learn faster than their constituentbased counterparts and therefore need less training data to achieve similar performances. Finally, dependency-based role classifiers are more robust to vocabulary change and outperform constituentbased systems when using out-of-domain test sets. its roots in lexicalized constituent parsing (Magerman, 1994; Collins, 1997). The head rules created by Yamada and Matsumoto (2003) have been used in almost all recent work on statistical dependency parsing of English (Nivre and Scholz, 2004; McDonald et al., 2005). Recently, Johansson and Nugues (2007) extended the head percolation strategy to incorporate long-distance links such as wh-movement and topicalization, and used the full set of grammatical function tags from Penn in addition to a number of inferred tags (in total 57 function tags). A dependency parser based on this syntax was used in the best-performing system in the SemEval-2007 task on Frame-semantic Structure Extraction (Baker et al., 2007), and the conversion method (in two different forms) was used for the English data in the CoNLL Shared Tasks of 2007 and 2008. 3 Automatic Semantic Role Labeling with Constituents and Depend"
C08-1050,P06-2104,0,0.0326014,"w previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of interest in dependency parsing during the last few years (Buchholz and Marsi, 2006). Early examples of dependency-based SRL systems, which used goldstandard dependency treebanks, include Žabokrtský et al. (2002) and Hacioglu (2004). Two studies that compared the respective performances of constituent-based and dependency-based SRL systems (Pradhan et al., 2005; Swanson and Gordon, 2006), both using automatic parsers, reported that the constituent-based systems outperformed the dependency-based ones by a very wide margin. However, the figures reported in these studies can be misleading since the comparison involved a 10-year-old rule-based dependency parser versus a state-of-the-art statistical constituent parser. The recent progress in statistical dependency parsing gives grounds for a new evaluation. 393 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 393–400 Manchester, August 2008 In addition, there are a number of lingui"
C08-1050,N03-1033,0,0.00339342,"and MSTParser have achieved state-of-the-art results for a wide range of languages in the 2006 and 2007 CoNLL Shared Tasks on dependency parsing, and the LTH parser obtained the best result in the 2008 CoNLL Shared Task on joint syntactic and semantic parsing. Charniak’s and Collins’ parsers are widely used constituent parsers for English, and the C&J parser is the best-performing freely available constituent parser at the time of writing according to published figures. Charniak’s parser and the C&J parser come with a built-in part-ofspeech tagger; all other systems used the Stanford tagger (Toutanova et al., 2003). Following Gildea and Jurafsky (2002), the SRL problem is traditionally divided into two subtasks: identifying the arguments and labeling them with semantic roles. Although state-of-the-art SRL systems use sophisticated statistical models to perform these two tasks jointly (e.g. Toutanova et al., 2005, Johansson and Nugues, 2008), we implemented them as two independent support vector classifiers to be able to analyze the impact of syntactic representation on each task separately. The features used by the classifiers are traditional, although the features for the dependency-based classifiers n"
C08-1050,P05-1073,0,0.13641,"epresented by allowing nonprojective dependency links. These links also justify why dependency syntax is often considered superior for free-word-order languages; it is even very questionable whether the traditional constituent-based SRL strategies are viable for such languages. Second, grammatical function such as subject and object is an integral concept in dependency syntax. This concept is intuitive when reasoning about the link between syntax and semantics, and it has been used earlier in semantic interpreters such as Absity (Hirst, 1983). However, except from a few tentative experiments (Toutanova et al., 2005), grammatical function is not explicitly used by current automatic SRL systems, but instead emulated from constituent trees by features like the constituent position and the governing category. More generally, these linguistic reasons have made a number of linguists argue that dependency structures are more suitable for explaining the syntax-semantics interface (Mel’ˇcuk, 1988; Hudson, 1984). In this work, we provide a new evaluation of the influence of the syntactic representation on semantic role labeling in English. Contrary to previously reported results, we show that dependencybased syste"
C08-1050,zabokrtsky-etal-2002-machine,0,0.0170348,"Missing"
C08-1050,W04-3212,0,0.0578539,"A RG W ORD /POS L EFT W ORD /POS R IGHT W ORD /POS PARENT W ORD /POS C-S UBCAT C-PATH P HRASE T YPE G OV C AT D-S UBCAT D-PATH C HILD D EP S ET PARENT H AS O BJ R ELT O PARENT F UNCTION Argument identification C,D C,D C,D C,D C,D C,D C,D C,D C,D C C C C D D D D D Argument classification C,D C,D C,D C,D C,D C,D C,D C,D C C C C D D D D Table 1: Classifier features. The features used by the constituent-based and the dependency-based systems are marked C and D, respectively. fication step was preceded by a pruning stage that heuristically removes parse tree nodes unlikely to represent arguments (Xue and Palmer, 2004). To score the performance of the argument identifier, traditional evaluation procedures treat the identification as a bracketing problem, meaning that the entities scored by the evaluation procedure are labeled snippets of text; however, it is questionable whether this is the proper way to evaluate a task whose purpose is to find semantic relations between logical entities. We believe that the same criticisms that have been leveled at the PARSEVAL metric for constituent structures are equally valid for the bracket-based evaluation of SRL systems. The inappropriateness of the traditional metri"
C08-1050,W03-3023,0,0.0206916,"previously reported results, we show that dependencybased systems are on a par with constituent-based systems or perform nearly as well. Furthermore, we show that semantic role classifiers using a dependency parser learn faster than their constituentbased counterparts and therefore need less training data to achieve similar performances. Finally, dependency-based role classifiers are more robust to vocabulary change and outperform constituentbased systems when using out-of-domain test sets. its roots in lexicalized constituent parsing (Magerman, 1994; Collins, 1997). The head rules created by Yamada and Matsumoto (2003) have been used in almost all recent work on statistical dependency parsing of English (Nivre and Scholz, 2004; McDonald et al., 2005). Recently, Johansson and Nugues (2007) extended the head percolation strategy to incorporate long-distance links such as wh-movement and topicalization, and used the full set of grammatical function tags from Penn in addition to a number of inferred tags (in total 57 function tags). A dependency parser based on this syntax was used in the best-performing system in the SemEval-2007 task on Frame-semantic Structure Extraction (Baker et al., 2007), and the convers"
C08-1050,N07-1070,0,\N,Missing
C08-1050,C98-1013,0,\N,Missing
C10-1059,W06-1651,0,0.695308,"ch tags and functional words (Wiebe et al., 1999). This is not unexpected since these problems have typically been formulated as text categorization problems, and it has long been agreed in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search (Moschitti and Basili, 2004). As the field moves towards increasingly sophisticated tasks requiring a detailed analysis of the text, the benefit of syntactic and semantic analysis becomes more clear. For the task of subjective expression detection, Choi et al. (2006) and Breck et al. (2007) used syntactic features in a sequence model. In addition, syntactic and shallowsemantic relations have repeatedly proven useful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Choi et al. (2006) is notable for the use of a global model based on hand-crafted constraints and an integer linear programming optimization step to ensure a globally consistent set of opinions and holders. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2"
C10-1059,W02-1001,0,0.174029,"A2), which are realized on the surface-syntactic level as a subject, a direct object, and an object predicative complement, respectively. OPRD OBJ SBJ NMOD They [called ]DSE him a [liar]ESE A0 A1 A2 call.01 Figure 1: Syntactic and shallow semantic structure. 3.2 Base Sequence Labeling Model To solve the first subtask, we implemented a standard sequence labeler for subjective expression markup, similar to the approach by Breck et al. (2007). We encoded the opinionated expression brackets using the IOB2 encoding scheme (Tjong Kim Sang and Veenstra, 1999) and trained the model using the metod by Collins (2002). The sequence labeler used word, POS tag, and lemma features in a window of size 3. In addition, we used prior polarity and intensity features derived from the lexicon created by Wilson et al. (2005). It is important to note that prior subjectivity does not always imply subjectivity in a particular context; this is why contextual features are essential for this task. This sequence labeler was used to generate the candidate set for the reranker. To generate reranking training data, we carried out a 5-fold hold-out procedure: We split the training set into 5 pieces, 521 trained a sequence label"
C10-1059,W08-2123,1,0.892449,"Missing"
C10-1059,P09-2079,0,0.0607521,"Missing"
C10-1059,W06-0301,0,0.355539,"e repeatedly proven useful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Choi et al. (2006) is notable for the use of a global model based on hand-crafted constraints and an integer linear programming optimization step to ensure a globally consistent set of opinions and holders. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, 520 which has recently attracted some attention in the subjectivity analysis community (Somasundaran et al., 2009). 3 Modeling Inter"
C10-1059,D07-1114,0,0.03973,"Breck et al. (2007) used syntactic features in a sequence model. In addition, syntactic and shallowsemantic relations have repeatedly proven useful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Choi et al. (2006) is notable for the use of a global model based on hand-crafted constraints and an integer linear programming optimization step to ensure a globally consistent set of opinions and holders. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, 520 which has"
C10-1059,W04-2705,0,0.0606724,"x – we don’t have to think about how the features affect the algorithmic complexity of the inference step. A common objection to reranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate. 3.1 Syntactic and Semantic Structures We used the syntactic–semantic parser by Johansson and Nugues (2008) to annnotate the sentences with dependency syntax (Mel’ˇcuk, 1988) and shallow semantic structures in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows an example of the annotation: The sentence they called him a liar, where called is a DSE and liar is an ESE, has been annotated with dependency syntax (above the text) and PropBank-based semantic role structure (below the text). The predicate called, which is an instance of the PropBank frame call.01, has three semantic arguments: the Agent (A0), the Theme (A1), and the Predicate (A2), which are realized on the surface-syntactic level as a subject, a direct object, and an object predicative complement, respectively. OPRD OBJ SBJ NMOD They [called ]DSE him a [liar]ES"
C10-1059,J05-1004,0,0.00659733,"features can be arbitrarily complex – we don’t have to think about how the features affect the algorithmic complexity of the inference step. A common objection to reranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate. 3.1 Syntactic and Semantic Structures We used the syntactic–semantic parser by Johansson and Nugues (2008) to annnotate the sentences with dependency syntax (Mel’ˇcuk, 1988) and shallow semantic structures in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows an example of the annotation: The sentence they called him a liar, where called is a DSE and liar is an ESE, has been annotated with dependency syntax (above the text) and PropBank-based semantic role structure (below the text). The predicate called, which is an instance of the PropBank frame call.01, has three semantic arguments: the Agent (A0), the Theme (A1), and the Predicate (A2), which are realized on the surface-syntactic level as a subject, a direct object, and an object predicative complement, respectively. OPRD OBJ SBJ NMO"
C10-1059,W02-1011,0,0.0123042,"ed a 10-point absolute improvement in soft recall, and a 5-point improvement in F-measure, over the baseline sequence labeler. Similarly, the recall is boosted by almost 11 points for the holder extraction (3 points in F-measure) by modeling the interaction of opinion expressions with respect to holders. 2 Related Work Since the most significant body of work in subjectivity analysis has been dedicated to coarsegrained tasks such as document polarity classification, most approaches to analysing the sentiment of natural-language text have relied fundamentally on purely lexical information (see (Pang et al., 2002; Yu and Hatzivassiloglou, 2003), inter alia) or low-level grammatical information such as part-of-speech tags and functional words (Wiebe et al., 1999). This is not unexpected since these problems have typically been formulated as text categorization problems, and it has long been agreed in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search (Moschitti and Basili, 2004). As the field moves towards increasingly sophisticated tasks requiring a detailed analysis of the text, the benefit of syntactic"
C10-1059,ruppenhofer-etal-2008-finding,0,0.045649,"sed on hand-crafted constraints and an integer linear programming optimization step to ensure a globally consistent set of opinions and holders. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, 520 which has recently attracted some attention in the subjectivity analysis community (Somasundaran et al., 2009). 3 Modeling Interaction over Syntactic and Semantic Structure Previous systems for opinion expression markup have typically used simple feature sets which have allowed the use of efficient off-the-shelf sequence labeling methods based on Viterbi se"
C10-1059,D09-1018,0,0.178321,"has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, 520 which has recently attracted some attention in the subjectivity analysis community (Somasundaran et al., 2009). 3 Modeling Interaction over Syntactic and Semantic Structure Previous systems for opinion expression markup have typically used simple feature sets which have allowed the use of efficient off-the-shelf sequence labeling methods based on Viterbi search (Choi et al., 2006; Breck et al., 2007). This is not possible in our case since we would like to extract structural, relational features that involve pairs of opinion expressions and may apply over an arbitrarily long distance in the sentence. While it is possible that search algorithms for exact or approximate inference can be constructured fo"
C10-1059,W06-1640,0,0.0200935,"published results, which have been precision-oriented and scored quite low on recall. We analyzed the impact of the syntactic and semantic features and saw that the best model is the one that makes use of both types of features. The most effective features we have found are purely structural, i.e. based on tree fragments in a syntactic or semantic tree. Features involving words did not seem to have the same impact. There are multiple opportunities for future work in this area. An important issue that we have left open is the coreference problem for holder extraction, which has been studied by Stoyanov and Cardie (2006). Similarly, recent work has tried to incorporate complex, high-level linguistic structure such as discourse representations (Somasundaran et al., 2009); it is clear that these structures are very relevant for explaining the way humans organize their expressions of opinions rhetorically. However, theoretical depth does not necessarily guarantee practical applicability, and the challenge is as usual to find a middle ground that balances our goals: explanatory power in theory, significant performance gains in practice, computational tractability, and robustness in difficult circumstances. 6 Ackn"
C10-1059,E99-1023,0,0.0560222,"Missing"
C10-1059,P99-1032,0,0.0264183,"Missing"
C10-1059,H05-1044,0,0.0433488,"1 A2 call.01 Figure 1: Syntactic and shallow semantic structure. 3.2 Base Sequence Labeling Model To solve the first subtask, we implemented a standard sequence labeler for subjective expression markup, similar to the approach by Breck et al. (2007). We encoded the opinionated expression brackets using the IOB2 encoding scheme (Tjong Kim Sang and Veenstra, 1999) and trained the model using the metod by Collins (2002). The sequence labeler used word, POS tag, and lemma features in a window of size 3. In addition, we used prior polarity and intensity features derived from the lexicon created by Wilson et al. (2005). It is important to note that prior subjectivity does not always imply subjectivity in a particular context; this is why contextual features are essential for this task. This sequence labeler was used to generate the candidate set for the reranker. To generate reranking training data, we carried out a 5-fold hold-out procedure: We split the training set into 5 pieces, 521 trained a sequence labeler on pieces 1 to 4, applied it to piece 5 and so on. 3.3 Base Opinion Holder Extractor For every opinion expression, we extracted opinion holders, i.e. mentions of the entity holding the opinion deno"
C10-1059,D09-1159,0,0.0306229,"e model. In addition, syntactic and shallowsemantic relations have repeatedly proven useful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Choi et al. (2006) is notable for the use of a global model based on hand-crafted constraints and an integer linear programming optimization step to ensure a globally consistent set of opinions and holders. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, 520 which has recently attracted some attention in the subjectiv"
C10-1059,W03-1017,0,0.139931,"ute improvement in soft recall, and a 5-point improvement in F-measure, over the baseline sequence labeler. Similarly, the recall is boosted by almost 11 points for the holder extraction (3 points in F-measure) by modeling the interaction of opinion expressions with respect to holders. 2 Related Work Since the most significant body of work in subjectivity analysis has been dedicated to coarsegrained tasks such as document polarity classification, most approaches to analysing the sentiment of natural-language text have relied fundamentally on purely lexical information (see (Pang et al., 2002; Yu and Hatzivassiloglou, 2003), inter alia) or low-level grammatical information such as part-of-speech tags and functional words (Wiebe et al., 1999). This is not unexpected since these problems have typically been formulated as text categorization problems, and it has long been agreed in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search (Moschitti and Basili, 2004). As the field moves towards increasingly sophisticated tasks requiring a detailed analysis of the text, the benefit of syntactic and semantic analysis becomes mo"
C16-1078,borin-etal-2012-korp,0,0.016527,"constructions are nontrivial in the sense that their formal description cannot be translated into a standard corpus search query (based on surface features such as words or part-of-speech tags) that captures exactly their true instances – for instance, occurrences of the well-studied let alone construction (Fillmore et al., 1988) are easy to spot in corpora. The initial pass of creating the benchmark was to extract a collection of potential instances for each of the six constructions. We extracted these instances by querying the Korp corpus search engine, hosted by the Swedish Language Bank (Borin et al., 2012). Korp stores a large collection of corpora, currently around 10 billion tokens, and allows structural queries based on surface strings but also several types of linguistic annotation; for this experiment, we used a corpus of contemporary fiction. The web service of Korp allows users to pose queries using the CQP language (Christ, 1994), and we selected the instances by translating the structure sketch (as in the example in Figure 1) of each construction into a corresponding CQP query. The corpus search engine then returns a (possibly very large) number of hits, and depending on the formal pro"
C16-1078,W15-0703,0,0.0251307,"presented that mine corpora for frequent patterns. For instance, Wible and Tsao (2010) collected generalized n-grams (that is, combinations of words, PoS tags and phrase labels) and applied standard measures of collocational strength to select n-grams that seem to be recurrent patterns. The Swedish constructicon project used similar methods (Forsberg et al., 2014), but also extended the approach by Wible and Tsao (2010) by considering patterns containing phrase labels (e.g. NP-and-NP, as-Adj-as-NP). The only related work we found that treats the construction detection as a ranking task is by Dubremetz and Nivre (2015), who used a ranking approach to retrieve occurrences of the rare rhetorical chiasmus construction. They preferred ranking over classification since the complexity of this construction and the number of borderline cases made a hard classification infeasible, and similarly to our position, they argued that the existence of borderline cases should be embraced instead of ignored. However, since their work is limited to searching for just chiasmi, their system was completely tailored for this case. 4 A benchmark collection for evaluating construction retrieval systems We built a new benchmark for"
C16-1078,heppin-gronostaj-2012-rocky,0,0.0473308,"Missing"
C16-1078,L16-1482,1,0.826856,"milarity computed from corpora. 5.2.1 Network-based similarity SALDO (Borin et al., 2013) is a large lexical resource that connects senses of Swedish words into a hierarchical semantic network. To measure similarity between two SALDO entries, we use the measure 819 by Wu and Palmer (1994), based on proximity in the tree and the depth of the lowest common ancestor. This measure is a number between 1 and 0, where 1 is the score for two identical entries. A complication is that our corpora lacks sense annotation; however, since the first sense dominates overwhelmingly in corpora for most lemmas (Johansson et al., 2016), we use the first sense to compute the similarities. 5.2.2 Frame-based similarity An alternative lexicon-based similarity function is based on the Swedish FrameNet (Friberg Heppin and Toporowska Gronostaj, 2012). This resource, similar to its English counterpart (Fillmore and Baker, 2009), maps lemmas to one or more frames, which for the current purposes can be seen as semantic classes. Intuitively, two words have a similar meaning if they belong to the same frame; for instance, timme ‘hour’ and minut ‘minute’ are related because they both belong to the frame C ALENDRIC UNIT. Again, we have t"
C16-1078,W10-0804,0,0.0316308,"urselves]REFL [forward]L OCATIVE in the restaurant. ¨ Figure 1: Swedish Constructicon entry for V REFL . R ORELSE (R EFLEXIVE MOTION). 816 3 Related work While we are aware of no previous work approaching the general problem of searching for construction occurrences from a retrieval perspective, quantitative and corpus-based methods have been a crucial part of the construction-linguistic toolbox from its early days (Gries, 2003; Stefanowitsch and Gries, 2003; Hilpert, 2013). In particular, a number of automatic methods have been presented that mine corpora for frequent patterns. For instance, Wible and Tsao (2010) collected generalized n-grams (that is, combinations of words, PoS tags and phrase labels) and applied standard measures of collocational strength to select n-grams that seem to be recurrent patterns. The Swedish constructicon project used similar methods (Forsberg et al., 2014), but also extended the approach by Wible and Tsao (2010) by considering patterns containing phrase labels (e.g. NP-and-NP, as-Adj-as-NP). The only related work we found that treats the construction detection as a ranking task is by Dubremetz and Nivre (2015), who used a ranking approach to retrieve occurrences of the"
C16-1078,P94-1019,0,\N,Missing
D07-1123,W06-2920,0,0.160704,"se number of actions is linear with respect to the number of words in the sentence. Abstract We describe an incremental parser that was trained to minimize cost over sentences rather than over individual parsing actions. This is an attempt to use the advantages of the two top-scoring systems in the CoNLL-X shared task. In the evaluation, we present the performance of the parser in the Multilingual task, as well as an evaluation of the contribution of bidirectional parsing and beam search to the parsing performance. 2.2 1 Introduction The two best-performing systems in the CoNLL-X shared task (Buchholz and Marsi, 2006) can be classified along two lines depending on the method they used to train the parsing models. Although the parsers are quite different, their creators could report near-tie scores. The approach of the top system (McDonald et al., 2006) was to fit the model to minimize cost over sentences, while the secondbest system (Nivre et al., 2006) trained the model to maximize performance over individual decisions in an incremental algorithm. This difference is a natural consequence of their respective parsing strategies: CKY-style maximization of link score and incremental parsing. In this paper, we"
D07-1123,C96-1058,0,0.318666,"itions where W is the initial word list; I, the current input word list; A, the graph of dependencies; and S, the stack. (n′ , n) denotes a dependency relations between n′ and n, where n′ is the head and n the dependent. Actions Initialize Terminate Left-arc Right-arc Reduce Shift Parser actions hnil, W, ∅i hS, nil, Ai hn|S, n′ |I, Ai → hS, n′ |I, A ∪ {(n′ , n)}i hn|S, n′ |I, Ai → hn′ |n|S, I, A ∪ {(n, n′ )}i hn|S, I, Ai → hS, I, Ai hS, n|I, Ai → hn|S, I, Ai between the two parses in a manner that makes the tree projective, single-head, rooted, and cycle-free, we applied the Eisner algorithm (Eisner, 1996). 2.4 Beam Search As in our previous parser (Johansson and Nugues, 2006), we used a beam-search extension to Nivre’s original algorithm (which is greedy in its original formulation). Each parsing action was assigned a score, and the beam search allows us to find a better overall score of the sequence of actions. In this work, we used a beam width of 8 for Catalan, Chinese, Czech, and English and 16 for the other languages. 3 Learning Method 3.1 Overview We model the parsing problem for a sentence x as finding the parse yˆ = arg maxy F (x, y) that maximizes a discriminant function F . In this w"
D07-1123,W06-2930,1,0.943317,"ojective. To be able to recover the nonprojective arcs after parsing, the projectivization operation replaces the labels of the arcs it modifies with traces indicating which links should be moved and where attach to attach them (the “Head+Path” encoding). The model is trained with these new labels that makes it possible to carry out the reverse operation and produce nonprojective structures. 2.3 Bidirectional Parsing Shift-reduce is by construction a directional parser, typically applied from left to right. To make better use of the training set, we applied the algorithm in both directions as Johansson and Nugues (2006) and Sagae and Lavie (2006) for all languages except Catalan and Hungarian. This, we believe, also has the advantage of making the parser less sensitive to whether the language is head-initial or head-final. We trained the model on projectivized graphs from left to right and right to left and used a voting strategy based on link scores. Each link was assigned a score (simply by using the score of the la or ra actions for each link). To resolve the conflicts 1134 Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1134–1138, c Prague, June 2007. 2007 Association for Computatio"
D07-1123,W07-2416,1,0.828537,"Missing"
D07-1123,J93-2004,0,0.0288737,"Missing"
D07-1123,W06-2932,0,0.226356,"use the advantages of the two top-scoring systems in the CoNLL-X shared task. In the evaluation, we present the performance of the parser in the Multilingual task, as well as an evaluation of the contribution of bidirectional parsing and beam search to the parsing performance. 2.2 1 Introduction The two best-performing systems in the CoNLL-X shared task (Buchholz and Marsi, 2006) can be classified along two lines depending on the method they used to train the parsing models. Although the parsers are quite different, their creators could report near-tie scores. The approach of the top system (McDonald et al., 2006) was to fit the model to minimize cost over sentences, while the secondbest system (Nivre et al., 2006) trained the model to maximize performance over individual decisions in an incremental algorithm. This difference is a natural consequence of their respective parsing strategies: CKY-style maximization of link score and incremental parsing. In this paper, we describe an attempt to unify the two approaches: an incremental parsing strategy that is trained to maximize performance over sentences rather than over individual parsing actions. 2 Parsing Method 2.1 Nivre’s Parser We used Nivre’s algor"
D07-1123,P05-1013,0,0.0394371,"oaches: an incremental parsing strategy that is trained to maximize performance over sentences rather than over individual parsing actions. 2 Parsing Method 2.1 Nivre’s Parser We used Nivre’s algorithm (Nivre et al., 2006), which is a variant of the shift–reduce parser. Like the regular shift–reduce, it uses a stack S and a list Handling Nonprojective Parse Trees While the parsing algorithm produces projective trees only, nonprojective arcs can be handled using a preprocessing step before training the model and a postprocessing step after parsing the sentences. The projectivization algorithm (Nivre and Nilsson, 2005) iteratively moves each nonprojective arc upward in the tree until the whole tree is projective. To be able to recover the nonprojective arcs after parsing, the projectivization operation replaces the labels of the arcs it modifies with traces indicating which links should be moved and where attach to attach them (the “Head+Path” encoding). The model is trained with these new labels that makes it possible to carry out the reverse operation and produce nonprojective structures. 2.3 Bidirectional Parsing Shift-reduce is by construction a directional parser, typically applied from left to right."
D07-1123,W06-2933,0,0.281203,"Missing"
D07-1123,N06-2033,0,0.150302,"the nonprojective arcs after parsing, the projectivization operation replaces the labels of the arcs it modifies with traces indicating which links should be moved and where attach to attach them (the “Head+Path” encoding). The model is trained with these new labels that makes it possible to carry out the reverse operation and produce nonprojective structures. 2.3 Bidirectional Parsing Shift-reduce is by construction a directional parser, typically applied from left to right. To make better use of the training set, we applied the algorithm in both directions as Johansson and Nugues (2006) and Sagae and Lavie (2006) for all languages except Catalan and Hungarian. This, we believe, also has the advantage of making the parser less sensitive to whether the language is head-initial or head-final. We trained the model on projectivized graphs from left to right and right to left and used a voting strategy based on link scores. Each link was assigned a score (simply by using the score of the la or ra actions for each link). To resolve the conflicts 1134 Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1134–1138, c Prague, June 2007. 2007 Association for Computational Linguistics Table 1: Ni"
D08-1008,S07-1018,0,0.118818,"al., 2003). It has also been used in prototypes of NLP systems that carry out complex reasoning, such as entailment recognition systems (Haghighi et al., 2005; Hickl et al., 2006). In addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), a"
D08-1008,W04-2412,0,0.00983598,"sky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also been much research on the use of shallow syntax (Carreras and Màrquez, 2004) in SRL. In comparison, dependency syntax has received relatively little attention for the SRL task, despite the fact that dependency structures offer a more transparent encoding of predicate–argument relations. Furthermore, the few systems based on dependencies that have been presented have generally performed much worse than their constituent-based 69 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 69–78, c Honolulu, October 2008. 2008 Association for Computational Linguistics counterparts. For instance, Pradhan et al. (2005) reported that a syst"
D08-1008,W05-0620,0,0.055546,"u et al., 2003; Moschitti et al., 2003). It has also been used in prototypes of NLP systems that carry out complex reasoning, such as entailment recognition systems (Haghighi et al., 2005; Hickl et al., 2006). In addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Mar"
D08-1008,D07-1101,0,0.0142776,"nitialize w to zeros repeat N times for (xt , yt ) in T let y˜t = arg max  y F (xt , y) + ρ(yt , y) yt )−F (xt ,yt )+ρ(yt ,˜ yt ) t ,˜ let τt = min C, F (xkΨ(x,y yt )k2 t )−Ψ(x,˜ w ← w + τt (Ψ(x, yt ) − Ψ(x, y˜t )) return waverage  We used a C value of 0.01, and the number of iterations was 6. 2.1.1 Features and Search The feature function Ψsyn is a factored representation, meaning that we compute the score of the complete parse tree by summing the scores of its parts, referred to as factors: X Ψ(x, y) · w = ψ(x, f ) · w f ∈y We used a second-order factorization (McDonald and Pereira, 2006; Carreras, 2007), meaning that the factors are subtrees consisting of four links: the governor–dependent link, its sibling link, and the leftmost and rightmost dependent links of the dependent. This factorization allows us to express useful features, but also forces us to adopt the expensive search procedure by Carreras (2007), which extends Eisner’s span-based dynamic programming algorithm (1996) to allow second-order feature dependencies. This algorithm has a time complexity of O(n4 ), where n is the number of words in the sentence. The search was constrained to disallow multiple root links. To evaluate the"
D08-1008,P07-1071,0,0.01719,"008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also been much research on the use of shallow syntax (Carreras and Màrquez, 2004) in SRL. In comparison, dependency syntax has received relatively little attention for the SRL task, despite the fact that dependency structures offer a more transparent encoding of predicate–argument relations. Furthermore, the few systems based"
D08-1008,C96-1058,0,0.0458927,"Missing"
D08-1008,J02-3001,0,0.263593,"out complex reasoning, such as entailment recognition systems (Haghighi et al., 2005; Hickl et al., 2006). In addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also been much research on the use of shallow syntax (Carreras a"
D08-1008,P02-1031,0,0.0101279,"r performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also been much research on the use of shallow syntax (Carreras and Màrquez, 2004) in SRL. In comparison, dependency syntax has received relatively little attention for the SRL task, despite the fact that dependency structures offer a more transparent encoding of predicate–argument relations. Furthermore, the few systems based on dependencies that have been presented have generally performed much worse than their constituent"
D08-1008,H05-1049,0,0.023382,"system is the first dependency-based semantic role labeler for PropBank that rivals constituent-based systems in terms of performance. 1 Introduction Automatic semantic role labeling (SRL), the task of determining who does what to whom, is a useful intermediate step in NLP applications performing semantic analysis. It has obvious applications for template-filling tasks such as information extraction and question answering (Surdeanu et al., 2003; Moschitti et al., 2003). It has also been used in prototypes of NLP systems that carry out complex reasoning, such as entailment recognition systems (Haghighi et al., 2005; Hickl et al., 2006). In addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning model"
D08-1008,P08-1067,0,0.0234466,"n the future, we would like to further investigate whether syntactic and semantic analysis could be integrated more tightly. In this work, we used a sim1 Our system is freely available for download at http://nlp.cs.lth.se/lth_srl. plistic loose coupling by means of reranking a small set of complete structures. The same criticisms that are often leveled at reranking-based models clearly apply here too: The set of tentative analyses from the submodules is too small, and the correct analysis is often pruned too early. An example of a method to mitigate this shortcoming is the forest reranking by Huang (2008), in which complex features are evaluated as early as possible. A Classifier Features Features Used in Predicate Disambiguation P REDW ORD, P RED L EMMA. The lexical form and lemma of the predicate. P RED PARENT W ORD and P RED PARENT POS. Form and part-of-speech tag of the parent node of the predicate. C HILD D EP S ET, C HILDW ORD S ET, C HILD W ORD D EP S ET, C HILD POSS ET, C HILD POSD EP S ET. These features represent the set of dependents of the predicate using combinations of dependency labels, words, and parts of speech. D EP S UBCAT. Subcategorization frame: the concatenation of the d"
D08-1008,W08-2123,1,0.864795,"Missing"
D08-1008,C08-1050,1,0.813889,"istics counterparts. For instance, Pradhan et al. (2005) reported that a system using a rule-based dependency parser achieved much inferior results compared to a system using a state-of-the-art statistical constituent parser: The F-measure on WSJ section 23 dropped from 78.8 to 47.2, or from 83.7 to 61.7 when using a head-based evaluation. In a similar vein, Swanson and Gordon (2006) reported that parse tree path features extracted from a rule-based dependency parser are much less reliable than those from a modern constituent parser. In contrast, we recently carried out a detailed comparison (Johansson and Nugues, 2008b) between constituent-based and dependency-based SRL systems for FrameNet, in which the results of the two types of systems where almost equivalent when using modern statistical dependency parsers. We suggested that the previous lack of progress in dependency-based SRL was due to low parsing accuracy. The experiments showed that the grammatical function information available in dependency representations results in a steeper learning curve when training semantic role classifiers, and it also seemed that the dependency-based role classifiers were more resilient to lexical problems caused by ch"
D08-1008,W04-0803,0,0.0225849,"nswering (Surdeanu et al., 2003; Moschitti et al., 2003). It has also been used in prototypes of NLP systems that carry out complex reasoning, such as entailment recognition systems (Haghighi et al., 2005; Hickl et al., 2006). In addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax"
D08-1008,J93-2004,0,0.0322215,"005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also been much research on the use of shallow syntax (Carreras and Màrquez, 2004) in SRL. In comparison, dependency syntax has received relatively little attention for the SRL task, despite the fact that dependency structures offer a more transparent encoding of predicate–argument relations. Furthermore, the few systems based on dependencies that have been presented have generally performed much worse than their constituent-based 69 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 69–78, c Honolulu, October 2008. 2008 Associatio"
D08-1008,E06-1011,0,0.0218347,"Regularization parameter C Initialize w to zeros repeat N times for (xt , yt ) in T let y˜t = arg max  y F (xt , y) + ρ(yt , y) yt )−F (xt ,yt )+ρ(yt ,˜ yt ) t ,˜ let τt = min C, F (xkΨ(x,y yt )k2 t )−Ψ(x,˜ w ← w + τt (Ψ(x, yt ) − Ψ(x, y˜t )) return waverage  We used a C value of 0.01, and the number of iterations was 6. 2.1.1 Features and Search The feature function Ψsyn is a factored representation, meaning that we compute the score of the complete parse tree by summing the scores of its parts, referred to as factors: X Ψ(x, y) · w = ψ(x, f ) · w f ∈y We used a second-order factorization (McDonald and Pereira, 2006; Carreras, 2007), meaning that the factors are subtrees consisting of four links: the governor–dependent link, its sibling link, and the leftmost and rightmost dependent links of the dependent. This factorization allows us to express useful features, but also forces us to adopt the expensive search procedure by Carreras (2007), which extends Eisner’s span-based dynamic programming algorithm (1996) to allow second-order feature dependencies. This algorithm has a time complexity of O(n4 ), where n is the number of words in the sentence. The search was constrained to disallow multiple root links"
D08-1008,P05-1013,0,0.0281817,"Missing"
D08-1008,P05-1072,0,0.0764465,"hallow syntax (Carreras and Màrquez, 2004) in SRL. In comparison, dependency syntax has received relatively little attention for the SRL task, despite the fact that dependency structures offer a more transparent encoding of predicate–argument relations. Furthermore, the few systems based on dependencies that have been presented have generally performed much worse than their constituent-based 69 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 69–78, c Honolulu, October 2008. 2008 Association for Computational Linguistics counterparts. For instance, Pradhan et al. (2005) reported that a system using a rule-based dependency parser achieved much inferior results compared to a system using a state-of-the-art statistical constituent parser: The F-measure on WSJ section 23 dropped from 78.8 to 47.2, or from 83.7 to 61.7 when using a head-based evaluation. In a similar vein, Swanson and Gordon (2006) reported that parse tree path features extracted from a rule-based dependency parser are much less reliable than those from a modern constituent parser. In contrast, we recently carried out a detailed comparison (Johansson and Nugues, 2008b) between constituent-based a"
D08-1008,J08-2005,0,0.465119,"s in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also been much research on the use of shallow syntax (Carreras and Màrquez, 2004) in SRL. In comparison, dependency syntax has received relatively little attention for the SRL task, despite the fact that dependency structures offer a more transparent encoding of pred"
D08-1008,P03-1002,0,0.0449187,"+Brown test set, or 78.84 if punctuation is treated consistently. Using a dependency-based metric, the F1 figure of our system is 84.29 on the test set from CoNLL-2008. Our system is the first dependency-based semantic role labeler for PropBank that rivals constituent-based systems in terms of performance. 1 Introduction Automatic semantic role labeling (SRL), the task of determining who does what to whom, is a useful intermediate step in NLP applications performing semantic analysis. It has obvious applications for template-filling tasks such as information extraction and question answering (Surdeanu et al., 2003; Moschitti et al., 2003). It has also been used in prototypes of NLP systems that carry out complex reasoning, such as entailment recognition systems (Haghighi et al., 2005; Hickl et al., 2006). In addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and M"
D08-1008,W08-2121,1,0.911861,"also been used in prototypes of NLP systems that carry out complex reasoning, such as entailment recognition systems (Haghighi et al., 2005; Hickl et al., 2006). In addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also b"
D08-1008,P06-2104,0,0.0144666,"ted have generally performed much worse than their constituent-based 69 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 69–78, c Honolulu, October 2008. 2008 Association for Computational Linguistics counterparts. For instance, Pradhan et al. (2005) reported that a system using a rule-based dependency parser achieved much inferior results compared to a system using a state-of-the-art statistical constituent parser: The F-measure on WSJ section 23 dropped from 78.8 to 47.2, or from 83.7 to 61.7 when using a head-based evaluation. In a similar vein, Swanson and Gordon (2006) reported that parse tree path features extracted from a rule-based dependency parser are much less reliable than those from a modern constituent parser. In contrast, we recently carried out a detailed comparison (Johansson and Nugues, 2008b) between constituent-based and dependency-based SRL systems for FrameNet, in which the results of the two types of systems where almost equivalent when using modern statistical dependency parsers. We suggested that the previous lack of progress in dependency-based SRL was due to low parsing accuracy. The experiments showed that the grammatical function inf"
D08-1008,P05-1073,0,0.0343412,"Missing"
D08-1008,W04-3212,0,0.0826378,"n addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also been much research on the use of shallow syntax (Carreras and Màrquez, 2004) in SRL. In comparison, dependency syntax has received relatively little attention for"
D08-1008,W09-4621,1,\N,Missing
D09-1059,W08-2134,0,0.0806608,"3 To be able to compare the scores of the gold-standard and predicted parses, we disabled the automatic classifier for predicate identification and provided the parser with goldstandard predicates in this experiment. 567 Operation w · Φi Queue operations Computation of Φi Fraction 0.64 0.15 0.10 It is thus highly dubious that a joint modeling of syntactic and semantic structure is worth the additional implementational effort. So far, no system using tightly integrated syntactic and semantic processing has been competitive with the best systems, which have been either completely pipelinebased (Che et al., 2008; Ciaramita et al., 2008) or employed only a loose syntactic–semantic coupling (Johansson and Nugues, 2008). It has been conjectured that modeling the semantics of the sentence would also help in syntactic disambiguation; however, it is likely that this is already implicitly taken into account by the lexical features present in virtually all modern parsers. In addition, a problem that our beam search method has in common with the constituent parsing method by Huang (2008) is that highly nonlocal features must be computed late. In our case, this means that if there is a long distance between a"
D09-1059,J07-2003,0,0.0493247,"nded semantic role labels to the phrase tags in a constituent treebank and applied a conventional constituent parser to predict constituent structure and semantic roles. In this paper, we propose a new approximate search method for bistratal dependency analysis. The search method is based on a beam search procedure that extends a k-best version of the standard cubic-time search algorithm for projective dependency parsing. This is similar to the search method for constituent parsing used by Huang (2008), who referred to it as cube pruning, inspired by an idea from machine translation decoding (Chiang, 2007). The cube pruning approach, which is normally used to solve the arg max problem, was also recently extended to summing problems, which is needed in some learning algorithms (Gimpel and Smith, 2009). We apply the algorithm on the CoNLL-2008 Shared Task data, and obtain the same evaluation score as the best previously published system that simultaneously learns syntactic and semantic structure (Titov et al., 2009). In this paper, we assume a different type of constraint: that the graph can be partitioned into two subgraphs that we will refer to as strata or layers, where the first of the layers"
D09-1059,W08-2138,0,0.11488,"Missing"
D09-1059,C04-1026,0,0.0187485,"parsing if we model the interdependency between the two layers. We apply the algorithm to the syntactic– semantic dependency parsing task of the CoNLL-2008 Shared Task, and we obtain a competitive result equal to the highest published for a system that jointly learns syntactic and semantic structure. 1 Introduction Numerous linguistic theories assume a multistratal model of linguistic structure, such as a layer of surface syntax, deep syntax, and shallow semantics. Examples include Meaning–Text Theory (Mel’ˇcuk, 1988), Discontinuous Grammar (BuchKromann, 2006), Extensible Dependency Grammar (Debusmann et al., 2004), and the Functional Generative Description (Sgall et al., 1986) which forms the theoretical foundation of the Prague Dependency Treebank (Hajiˇc, 1998). In the statistical NLP community, the most widely used grammatical resource is the Penn Treebank (Marcus et al., 1993). This is a purely syntactic resource, but we can also include this treebank in the category of multistratal resources 561 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 561–569, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP tiple parses. The system by Henderson et al. (2008) e"
D09-1059,C96-1058,0,0.0568312,"x, y) is a numeric feature representation of the tuple (x, yp , ys ) and w a high-dimensional vector of feature weights. 562 3.1 Review of k-Best Dependency Parsing Based on the structural assumptions made above, we now decompose the feature representation into three parts: The search method commonly used in dependency parsers is a chart-based dynamic programming algorithm that finds the highest-scoring projective dependency tree under an edge-factored scoring function. It runs in cubic time with respect to the sentence length. In a slightly more general formulation, it was first published by Eisner (1996). Starting from McDonald et al. (2005), it has been widely used in recent statistical dependency parsing frameworks. The algorithm works by creating open structures, which consist of a dependency link and the set of links that it spans, and closed structures, consisting of the left or right half of a complete subtree. An open structure is created by a procedure L INK that adds a dependency link to connect a right-pointing and a left-pointing closed structure, and a closed structure by a procedure J OIN that joins an open structure with a closed structure. Figure 2 shows schematic illustrations"
D09-1059,W08-2101,0,0.0442599,"t 2009. 2009 ACL and AFNLP tiple parses. The system by Henderson et al. (2008) extended previous stack-based algorithms for dependency parsing by using two separate stacks to build the syntactic and semantic graphs. Llu´ıs and M`arquez (2008) proposed a model that simultaneously predicts syntactic and semantic links, but since its search algorithm could not take the syntactic–semantic interdependencies into account, a pre-parsing step was still needed. In addition, before the CoNLL-2008 shared task there have been a few attempts to jointly learn syntactic and semantic structure; for instance, Merlo and Musillo (2008) appended semantic role labels to the phrase tags in a constituent treebank and applied a conventional constituent parser to predict constituent structure and semantic roles. In this paper, we propose a new approximate search method for bistratal dependency analysis. The search method is based on a beam search procedure that extends a k-best version of the standard cubic-time search algorithm for projective dependency parsing. This is similar to the search method for constituent parsing used by Huang (2008), who referred to it as cube pruning, inspired by an idea from machine translation decod"
D09-1059,W04-2705,0,0.216464,"Missing"
D09-1059,P05-1013,0,0.0495155,"d, li A PPEND(E, edge) return the top k edges in E a1 a2 p Figure 4: Illustration of the S COPE function for predicate–argument links. If the right substructure contains a predicate, we can find potential arguments in the left substructure. function C REATE - SEC - OPEN P(L,iL , E, I) score ← F ST(L[iL ]) + h,d F ST(E[h, d, IE [h, d]]) return hscore, L, iL , E, IE i While the primary layer is assumed to be projective in Algorithm 3, the syntactic trees in the CoNLL-2008 data have a small number of nonprojective links. We used a pseudo-projective edge label encoding to handle nonprojectivity (Nivre and Nilsson, 2005). To implement the model, we constructed feature representations Φp , Φs , and Φi . The surfacesyntactic representation Φp was a standard firstorder edge factorization using the same features as McDonald et al. (2005). The features in Φs and Φi are shown in Table 1 and are standard features in statistical semantic role classification. function A DVANCE - SEC - OPEN(o, k) where o = hscore, L, iL , E, IE i buf ← ∅ if iL < L ENGTH(L) and IE = [1, . . . , 1] A PPEND(buf, F IRST- SEC - OPEN(L, iL + 1, k)) for h, d if IE [h, d] < L ENGTH(E[h, d]) ′ ← C OPY(IE ) IE ′ ′ IE [h, d] ← IE [h, d] + 1 ′ A P"
D09-1059,J02-3001,0,0.41632,"04) projects have annotated shallow semantic structures on top of it. Dependency-converted versions of the Penn Treebank, PropBank and NomBank were used in the CoNLL-2008 Shared Task (Surdeanu et al., 2008), in which the task of the participants was to produce a bistratal dependency structure consisting of surface syntax and shallow semantics. Producing a consistent multistratal structure is a conceptually and computationally complex task, and most previous methods have employed a purely pipeline-based decomposition of the task. This includes the majority of work on shallow semantic analysis (Gildea and Jurafsky, 2002, inter alia). Nevertheless, since it is obvious that syntax and semantics are highly interdependent, it has repeatedly been suggested that the problems of syntactic and semantic analysis should be carried out simultaneously rather than in a pipeline, and that modeling the interdependency between syntax and semantics would improve the quality of all the substructures. The purpose of the CoNLL-2008 Shared Task was to study the feasibility of a joint analysis of syntax and semantics, and while most participating systems used a pipeline-based approach to the problem, there were a number of contri"
D09-1059,E09-1037,0,0.014795,"propose a new approximate search method for bistratal dependency analysis. The search method is based on a beam search procedure that extends a k-best version of the standard cubic-time search algorithm for projective dependency parsing. This is similar to the search method for constituent parsing used by Huang (2008), who referred to it as cube pruning, inspired by an idea from machine translation decoding (Chiang, 2007). The cube pruning approach, which is normally used to solve the arg max problem, was also recently extended to summing problems, which is needed in some learning algorithms (Gimpel and Smith, 2009). We apply the algorithm on the CoNLL-2008 Shared Task data, and obtain the same evaluation score as the best previously published system that simultaneously learns syntactic and semantic structure (Titov et al., 2009). In this paper, we assume a different type of constraint: that the graph can be partitioned into two subgraphs that we will refer to as strata or layers, where the first of the layers forms a tree. For the second layer, the only assumption we make is that there is at most one link between any two words. However, we believe that for any interesting linguistic structure, the secon"
D09-1059,J05-1004,0,0.306095,"Missing"
D09-1059,C08-1095,0,0.0219263,"pendency representation of sentence structure, starting from Tesni`ere (1959), the linguistic structure of the sentence is represented as a directed graph of relations between words. In most theories, certain constraints are imposed on this graph; the most common constraint on dependency graphs in syntax, for instance, is that the graph should form a tree (i.e. it should be connected, acyclic, and every node should have at most one incoming edge). This assumption underlies almost all dependency parsing, although there are also a few parsers based on slightly more general problem formulations (Sagae and Tsuji, 2008). hˆ yp , yˆs i = arg max F (x, yp , ys ) hyp ,ys i∈Y The learning problem consists of searching in the model space for a scoring function F that minimizes the cost of predictions on unseen examples according to a given cost function ρ. In this work, we consider linear scoring functions of the following form: F (x, yp , ys ) = w · Φ(x, yp , ys ) where Φ(x, y) is a numeric feature representation of the tuple (x, yp , ys ) and w a high-dimensional vector of feature weights. 562 3.1 Review of k-Best Dependency Parsing Based on the structural assumptions made above, we now decompose the feature re"
D09-1059,W08-2122,0,0.103901,"(Debusmann et al., 2004), and the Functional Generative Description (Sgall et al., 1986) which forms the theoretical foundation of the Prague Dependency Treebank (Hajiˇc, 1998). In the statistical NLP community, the most widely used grammatical resource is the Penn Treebank (Marcus et al., 1993). This is a purely syntactic resource, but we can also include this treebank in the category of multistratal resources 561 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 561–569, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP tiple parses. The system by Henderson et al. (2008) extended previous stack-based algorithms for dependency parsing by using two separate stacks to build the syntactic and semantic graphs. Llu´ıs and M`arquez (2008) proposed a model that simultaneously predicts syntactic and semantic links, but since its search algorithm could not take the syntactic–semantic interdependencies into account, a pre-parsing step was still needed. In addition, before the CoNLL-2008 shared task there have been a few attempts to jointly learn syntactic and semantic structure; for instance, Merlo and Musillo (2008) appended semantic role labels to the phrase tags in a"
D09-1059,W05-1506,0,0.0285056,"IM↑OPRD↑OBJ↓. In this work, all interdependency features will be based on paths in the primary layer. s j j+1 e s j e Figure 2: Illustrations of the L INK and J OIN operations. The search algorithm can easily be extended to find the k best parses, not only the best one. In k-best parsing, we maintain a k-best list in every cell in the dynamic programming table. To create the k-best list of derivations for an open structure between the positions s and e, for instance, there are up to |L |· (e − s) · k2 possible combinations to consider if the set of allowed labels is L. The key observation by Huang and Chiang (2005) is to make use of the fact that the lists are sorted. For every position between s and e, we add the best combination to a priority queue, from which we then repeatedly remove the front item. For every item we remove, we add three successors: an item with a next-best left part, an item with a next-best right part, and finally an item with a next-best edge 3 A Bistratal Search Algorithm This section presents an algorithm to approximately solve the arg max problem for prediction of bistratal dependency structures. We present the algorithm in two steps: first, we review a k-best version of the s"
D09-1059,P08-1067,0,0.273351,"w attempts to jointly learn syntactic and semantic structure; for instance, Merlo and Musillo (2008) appended semantic role labels to the phrase tags in a constituent treebank and applied a conventional constituent parser to predict constituent structure and semantic roles. In this paper, we propose a new approximate search method for bistratal dependency analysis. The search method is based on a beam search procedure that extends a k-best version of the standard cubic-time search algorithm for projective dependency parsing. This is similar to the search method for constituent parsing used by Huang (2008), who referred to it as cube pruning, inspired by an idea from machine translation decoding (Chiang, 2007). The cube pruning approach, which is normally used to solve the arg max problem, was also recently extended to summing problems, which is needed in some learning algorithms (Gimpel and Smith, 2009). We apply the algorithm on the CoNLL-2008 Shared Task data, and obtain the same evaluation score as the best previously published system that simultaneously learns syntactic and semantic structure (Titov et al., 2009). In this paper, we assume a different type of constraint: that the graph can"
D09-1059,W08-2123,1,0.911961,"Missing"
D09-1059,W08-2124,0,0.185611,"Missing"
D09-1059,J93-2004,0,0.0365102,"tic and semantic structure. 1 Introduction Numerous linguistic theories assume a multistratal model of linguistic structure, such as a layer of surface syntax, deep syntax, and shallow semantics. Examples include Meaning–Text Theory (Mel’ˇcuk, 1988), Discontinuous Grammar (BuchKromann, 2006), Extensible Dependency Grammar (Debusmann et al., 2004), and the Functional Generative Description (Sgall et al., 1986) which forms the theoretical foundation of the Prague Dependency Treebank (Hajiˇc, 1998). In the statistical NLP community, the most widely used grammatical resource is the Penn Treebank (Marcus et al., 1993). This is a purely syntactic resource, but we can also include this treebank in the category of multistratal resources 561 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 561–569, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP tiple parses. The system by Henderson et al. (2008) extended previous stack-based algorithms for dependency parsing by using two separate stacks to build the syntactic and semantic graphs. Llu´ıs and M`arquez (2008) proposed a model that simultaneously predicts syntactic and semantic links, but since its search algorithm co"
D09-1059,P05-1012,0,0.39118,"presentation of the tuple (x, yp , ys ) and w a high-dimensional vector of feature weights. 562 3.1 Review of k-Best Dependency Parsing Based on the structural assumptions made above, we now decompose the feature representation into three parts: The search method commonly used in dependency parsers is a chart-based dynamic programming algorithm that finds the highest-scoring projective dependency tree under an edge-factored scoring function. It runs in cubic time with respect to the sentence length. In a slightly more general formulation, it was first published by Eisner (1996). Starting from McDonald et al. (2005), it has been widely used in recent statistical dependency parsing frameworks. The algorithm works by creating open structures, which consist of a dependency link and the set of links that it spans, and closed structures, consisting of the left or right half of a complete subtree. An open structure is created by a procedure L INK that adds a dependency link to connect a right-pointing and a left-pointing closed structure, and a closed structure by a procedure J OIN that joins an open structure with a closed structure. Figure 2 shows schematic illustrations: a L INK operation connects the right"
D09-1059,D08-1016,0,0.0182326,"ization problem that we set out to solve is intractable, but we have shown that reasonable performance can be achieved with an inexact, beam search-based search method. This is not obvious: it has previously been shown that using an inexact search procedure when the learning algorithm assumes that the search is exact may lead to slow convergence or even divergence (Kulesza and Pereira, 2008), but this does not seem to be a problem in our case. While we used a beam search method as the method of approximation, other methods are certainly possible. An interesting example is the recent system by Smith and Eisner (2008), which used loopy belief propagation in a dependency parser using highly complex features, while still maintaining cubic-time search complexity. An obvious drawback of our approach compared to traditional pipeline-based semantic role labeling methods is that the speed of the algorithm is highly dependent on the size of the interdependency feature representation Φi . Also, extracting these features is fairly complex, and it is of critical importance to implement the feature extraction procedure efficiently since it is one of the bottlenecks of the algorithm. It is plausible that our performanc"
D09-1059,W08-2121,1,0.92198,"Missing"
D09-1059,P05-1073,0,0.0339218,"Missing"
D09-1059,W04-3212,0,0.404803,"Missing"
E06-1049,W01-1301,1,0.677768,"ntified in the narratives. We finally report the results we obtained. {20 persons}o1 diede2 in a {bus accident}e1 in southern Afghanistan {on Thursday}t1 . In addition, {39 persons}o2 {were injured}e3 in the accidente4 . The buso3 {was on its way}e5 from Kandahar to the capital Kabul when ito4 {drove off}e7 the roado5 while overtakinge6 and {flipped over}e8 , saide9 General Salim Khan, assistant head of police in Kandahar. 1 Extraction of Temporal Information and Scene Visualization Carsim is a program that generates 3D scenes from narratives describing road accidents (Johansson et al., 2005; Dupuy et al., 2001). It considers authentic texts, generally collected from web sites of Swedish newspapers or transcribed from handwritten accounts by victims of accidents. One of Carsim’s key features is that it animates the generated scene to visualize events described in the narrative. The text below, a newspaper article with its translation into English, illustrates the goals and challenges of it. We bracketed the entities, time expressions, and events and we annotated them with identifiers, denoted respectively oi , tj , and ek : The text above, our translation. To create a consistent animation, the progra"
E06-1049,P04-1074,0,0.163577,"cal framework to deal with events in simple past and pluperfect sentences. Hitzeman et al. (1995) proposed a constraintbased approach taking into account tense, aspect, temporal adverbials, and rhetorical structure to analyze a discourse. Recently, groups have used machine learning techniques to determine temporal relations. They trained automatically classifiers on handannotated corpora. Mani et al. (2003) achieved the best results so far by using decision trees to order partially events of successive clauses in English texts. Boguraev and Ando (2005) is another example of it for English and Li et al. (2004) for Chinese. 3 Annotating Texts with Temporal Information Several schemes have been proposed to annotate temporal information in texts, see Setzer and Gaizauskas (2002), inter alia. Many of them were incompatible or incomplete and in an effort to reconcile and unify the field, Ingria and Pustejovsky (2002) introduced the XML-based Time markup language (TimeML). TimeML is a specification language whose goal is to capture most aspects of temporal relations between events in discourses. It is based on Allen’s (1984) relations and a variation of Vendler’s (1967) classification of verbs. It define"
E06-1049,N03-2019,0,0.122594,"(1986) investigated it and formulated a Temporal Discourse Interpretation Principle to interpret the advance of narrative time in a sequence of sentences. Lascarides and Asher (1993) described a complex logical framework to deal with events in simple past and pluperfect sentences. Hitzeman et al. (1995) proposed a constraintbased approach taking into account tense, aspect, temporal adverbials, and rhetorical structure to analyze a discourse. Recently, groups have used machine learning techniques to determine temporal relations. They trained automatically classifiers on handannotated corpora. Mani et al. (2003) achieved the best results so far by using decision trees to order partially events of successive clauses in English texts. Boguraev and Ando (2005) is another example of it for English and Li et al. (2004) for Chinese. 3 Annotating Texts with Temporal Information Several schemes have been proposed to annotate temporal information in texts, see Setzer and Gaizauskas (2002), inter alia. Many of them were incompatible or incomplete and in an effort to reconcile and unify the field, Ingria and Pustejovsky (2002) introduced the XML-based Time markup language (TimeML). TimeML is a specification lan"
E06-1049,W01-1311,0,0.0316368,"sequences of events where it decides the order between two of the events in each sequence. If e1 , ..., en are the events in the sequence they appear in the text, the trees correspond to the following functions: fdt1 (ei , ei+1 ) ⇒ trel (ei , ei+1 ) fdt2 (ei , ei+1 , ei+2 ) ⇒ trel (ei , ei+1 ) fdt3 (ei , ei+1 , ei+2 ) ⇒ trel (ei+1 , ei+2 ) fdt4 (ei , ei+1 , ei+2 ) ⇒ trel (ei , ei+2 ) fdt5 (ei , ei+1 , ei+2 , ei+3 ) ⇒ trel (ei , ei+3 ) The possible output values of the trees are: simultaneous, after, before, is_included, includes, and none. These values correspond to the relations described by Setzer and Gaizauskas (2001). The first decision tree should capture more general relations between two adjacent events without the need of a context. Decision trees dt2 and dt3 extend the context by one event to the left respectively one event to the right. They should capture more specific phenomena. However, they are not always applicable as we never apply a decision 388 tree when there is a time expression between any of the events involved. In effect, time expressions “reanchor” the narrative temporally, and we noticed that the decision trees performed very poorly across time expressions. We complemented the decisio"
E06-1049,E95-1035,0,\N,Missing
E06-2013,J02-3001,0,0.462295,"are, this is the first system that finds this information automatically. 1 Introduction Shallow semantic parsing has been an active area of research during the last few years. Semantic parsers, which are typically based on the FrameNet (Baker et al., 1998) or PropBank formalisms, have proven useful in a number of NLP projects, such as information extraction and question answering. The main reason for their popularity is that they can produce a flat layer of semantic structure with a fair degree of robustness. Building English semantic parsers for the FrameNet standard has been studied widely (Gildea and Jurafsky, 2002; Litkowski, 2004). These systems typically address the task of identifying and classifying Frame Elements (FEs), that is semantic arguments of predicates, for a given target word (predicate). Although the FE layer is arguably the most central, the FrameNet annotation standard defines a number of additional semantic layers, which contain information about support expressions (verbs and prepositions), copulas, null arguments, slotfillers, and aspectual particles. This information can for example be used in a semantic parser to refine the meaning of a predicate, to link predicates in a sentence"
E06-2013,W04-0803,0,0.0307557,"em that finds this information automatically. 1 Introduction Shallow semantic parsing has been an active area of research during the last few years. Semantic parsers, which are typically based on the FrameNet (Baker et al., 1998) or PropBank formalisms, have proven useful in a number of NLP projects, such as information extraction and question answering. The main reason for their popularity is that they can produce a flat layer of semantic structure with a fair degree of robustness. Building English semantic parsers for the FrameNet standard has been studied widely (Gildea and Jurafsky, 2002; Litkowski, 2004). These systems typically address the task of identifying and classifying Frame Elements (FEs), that is semantic arguments of predicates, for a given target word (predicate). Although the FE layer is arguably the most central, the FrameNet annotation standard defines a number of additional semantic layers, which contain information about support expressions (verbs and prepositions), copulas, null arguments, slotfillers, and aspectual particles. This information can for example be used in a semantic parser to refine the meaning of a predicate, to link predicates in a sentence together, or possi"
E06-2013,W04-0814,0,0.12476,"Missing"
E06-2013,P98-1013,0,0.235332,".lth.se Abstract We describe a system for automatic annotation of English text in the FrameNet standard. In addition to the conventional annotation of frame elements and their semantic roles, we annotate additional semantic information such as support verbs and prepositions, aspectual markers, copular verbs, null arguments, and slot fillers. As far as we are aware, this is the first system that finds this information automatically. 1 Introduction Shallow semantic parsing has been an active area of research during the last few years. Semantic parsers, which are typically based on the FrameNet (Baker et al., 1998) or PropBank formalisms, have proven useful in a number of NLP projects, such as information extraction and question answering. The main reason for their popularity is that they can produce a flat layer of semantic structure with a fair degree of robustness. Building English semantic parsers for the FrameNet standard has been studied widely (Gildea and Jurafsky, 2002; Litkowski, 2004). These systems typically address the task of identifying and classifying Frame Elements (FEs), that is semantic arguments of predicates, for a given target word (predicate). Although the FE layer is arguably the"
E06-2013,J03-4003,0,\N,Missing
E06-2013,C98-1013,0,\N,Missing
ghosh-etal-2012-improving,J93-2004,0,\N,Missing
ghosh-etal-2012-improving,W10-2910,1,\N,Missing
ghosh-etal-2012-improving,W05-1506,0,\N,Missing
ghosh-etal-2012-improving,J08-2005,0,\N,Missing
ghosh-etal-2012-improving,prasad-etal-2008-penn,0,\N,Missing
ghosh-etal-2012-improving,P09-2004,0,\N,Missing
ghosh-etal-2012-improving,I08-7009,0,\N,Missing
ghosh-etal-2012-improving,I11-1120,1,\N,Missing
ghosh-etal-2012-improving,I08-7010,0,\N,Missing
ghosh-etal-2012-improving,D07-1010,0,\N,Missing
I11-1120,W05-0305,0,0.333358,"results. However, their task was limited to retrieving the argument heads. In contrast, we integrate discourse segmentation in the parsing pipeline because we believe that spans are necessary when using the discourse arguments as input to applications such as opinion mining, where attributions need to be explicitly marked. Besides, no gold data are available for head-based discourse parsing evaluation and they have to be automatically derived from parse trees with a further processing step. With our approach, instead, we can directly use PDTB argument spans both for training and for testing. Dinesh et al. (2005) extracted complete arguments with boundaries, but only for a restricted class of connectives. The recent work by Prasad et al. (2010) is also limited, since their system only extracts the sentences containing the arguments. In our work, we assume that explicit discourse connectives are given beforehand, either taken directly from a gold standard or automatically identified. The second task based on PDTB was tackled among others by Pitler et al. (2008) and Pitler and Nenkova (2009). In addition to the work on finding explicit connectives and their arguments, there has been recent work on class"
I11-1120,W10-2910,1,0.667362,"out explicit relations and Arg1 extension. combinations are also represented. We used this tool because the output of CRF++ is compatible to CoNLL 2000 chunking shared task, and we view our task as a discourse chunking task. On the other hand, linear-chain CRFs for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Also Sha and Pereira (2003) claim that, as a single model, CRFs outperform other models for shallow parsing. 6.1 Evaluation methodology We present our results using precision, recall and F1 measures. Following Johansson and Moschitti (2010), we use three scoring schemes: exact, intersection (or partial), and overlap scoring. In the exact scoring scheme, a span extracted by the system is counted as correct if its extent exactly coincides with one in the gold standard. However, we also use the two other scoring schemes since exact scoring may be uninformative in some situations where it is enough to have a rough approximation of the argument spans. In the overlap scheme, an expression is counted as correctly detected if it overlaps with a gold standard argument, i.e. if their intersection is nonempty. The intersection scheme assig"
I11-1120,D09-1036,0,0.133389,"a restricted class of connectives. The recent work by Prasad et al. (2010) is also limited, since their system only extracts the sentences containing the arguments. In our work, we assume that explicit discourse connectives are given beforehand, either taken directly from a gold standard or automatically identified. The second task based on PDTB was tackled among others by Pitler et al. (2008) and Pitler and Nenkova (2009). In addition to the work on finding explicit connectives and their arguments, there has been recent work on classification of implicit discourse relations, see for instance Lin et al. (2009). In a similar classification experiment, Pitler et al. (2009) investigated features ranging from low-level word pairs to high-level linguistic cues, and demonstrated that it is useful to model the sequence of discourse relations using a sequence labeler. Although they both outperformed their respective baselines, this task is very difficult and performances are still very low. 3 The Penn Discourse Treebank (PDTB) The Penn Discourse Treebank (Prasad et al., 2008) is a resource including one million words from the Wall Street Journal (Marcus et al., 1993), annotated with discourse relations. Ba"
I11-1120,J93-2004,0,0.045018,"icit discourse relations, see for instance Lin et al. (2009). In a similar classification experiment, Pitler et al. (2009) investigated features ranging from low-level word pairs to high-level linguistic cues, and demonstrated that it is useful to model the sequence of discourse relations using a sequence labeler. Although they both outperformed their respective baselines, this task is very difficult and performances are still very low. 3 The Penn Discourse Treebank (PDTB) The Penn Discourse Treebank (Prasad et al., 2008) is a resource including one million words from the Wall Street Journal (Marcus et al., 1993), annotated with discourse relations. Based on the observation that “no discourse connective has yet been identified in any language that has other than two arguments” (Webber et al. (2010), p. 15), connectives in the PTDB are treated as discourse predicates taking two text spans as arguments, i.e. parts of the text that describe events, propositions, facts, situations. Such two arguments in the PDTB are just called Arg1 and Arg2 and are chosen according to syntactic criteria: Arg2 is the argument syntactically bound to the connective, while Arg1 is the other one. This means that the numbering"
I11-1120,prasad-etal-2008-penn,0,0.873646,"extraction of discourse arguments for given ex1071 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1071–1079, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP plicit discourse connectives – has been attempted a number of times. Soon after the initial release of the PDTB, it was realized that sentence-internal arguments may be located and classified using techniques similar to semantic role detection and classification methods. Wellner and Pustejovsky (2007) were the first to carry out such an experiment on the PDTB, and Elwell and Baldridge (2008) later improved over their results. However, their task was limited to retrieving the argument heads. In contrast, we integrate discourse segmentation in the parsing pipeline because we believe that spans are necessary when using the discourse arguments as input to applications such as opinion mining, where attributions need to be explicitly marked. Besides, no gold data are available for head-based discourse parsing evaluation and they have to be automatically derived from parse trees with a further processing step. With our approach, instead, we can directly use PDTB argument spans both for"
I11-1120,prasad-etal-2010-exploiting,0,0.494984,"empts have already been made in the direction of automatic classification of connectives, while token-level argument segmentation has not been explored. Therefore in this paper we will focus on the segmentation and labeling of discourse arguments (Arg1 and Arg2) with full spans, as defined in the annotation protocol of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). We present a methodology that, given explicit discourse connectives, automatically extracts discourse arguments by identifying Arg1 and Arg2 including the corresponding text spans. We call this approach shallow following Prasad et al. (2010) as opposed to tree-like representations of discourse, as in Rhetorical Structure Theory (Mann and Thompson, 1988). Indeed, we provide a flat chunk classification of discourse relations, building a non-hierarchical representation of the relations in a text. The discourse parser is designed as a cascade of argument-specific CRFs trained on different sets of lexical, syntactic and semantic features. The evaluation is made in terms of exact and partial match of arguments. The partial match condition may be useful in the case of noisy input or for applications that do not require exact alignment."
I11-1120,N03-1028,0,0.452917,"Missing"
I11-1120,D09-1018,0,0.0106177,"e show that the best combination of features includes syntactic and semantic features. The comparative error analysis investigates the performance variability over connective types and argument positions. 1 Introduction Automatic discourse processing is considered one of the most challenging NLP tasks due to its dependency on lexical and syntactic features and on the inter-sentential relations. While automatic discourse processing of structured documents or free text is still in its infancy, a number of applications of this technology in practical NLP systems have been proposed. For instance, Somasundaran et al. (2009) describe the use of discourse structure for opinion analysis. Other applications include conversational analysis and dialog systems (Tonelli et al., 2010). In this work we divide the whole task of discourse parsing into two sub-tasks: connective classification and argument segmentation and classification. Several successful attempts have already been made in the direction of automatic classification of connectives, while token-level argument segmentation has not been explored. Therefore in this paper we will focus on the segmentation and labeling of discourse arguments (Arg1 and Arg2) with fu"
I11-1120,tonelli-etal-2010-annotation,1,0.786179,"ver connective types and argument positions. 1 Introduction Automatic discourse processing is considered one of the most challenging NLP tasks due to its dependency on lexical and syntactic features and on the inter-sentential relations. While automatic discourse processing of structured documents or free text is still in its infancy, a number of applications of this technology in practical NLP systems have been proposed. For instance, Somasundaran et al. (2009) describe the use of discourse structure for opinion analysis. Other applications include conversational analysis and dialog systems (Tonelli et al., 2010). In this work we divide the whole task of discourse parsing into two sub-tasks: connective classification and argument segmentation and classification. Several successful attempts have already been made in the direction of automatic classification of connectives, while token-level argument segmentation has not been explored. Therefore in this paper we will focus on the segmentation and labeling of discourse arguments (Arg1 and Arg2) with full spans, as defined in the annotation protocol of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). We present a methodology that, given explicit"
I11-1120,D07-1010,0,0.697978,". Finally, we draw some conclusions in Section 7. 2 Related Work The task that we address in this paper – automatic extraction of discourse arguments for given ex1071 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1071–1079, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP plicit discourse connectives – has been attempted a number of times. Soon after the initial release of the PDTB, it was realized that sentence-internal arguments may be located and classified using techniques similar to semantic role detection and classification methods. Wellner and Pustejovsky (2007) were the first to carry out such an experiment on the PDTB, and Elwell and Baldridge (2008) later improved over their results. However, their task was limited to retrieving the argument heads. In contrast, we integrate discourse segmentation in the parsing pipeline because we believe that spans are necessary when using the discourse arguments as input to applications such as opinion mining, where attributions need to be explicitly marked. Besides, no gold data are available for head-based discourse parsing evaluation and they have to be automatically derived from parse trees with a further pr"
I11-1120,W03-3023,0,0.0709142,"ation. It includes for example the -ing and -ed suffixes in verb endings as well as the -s to form the plural of nouns. In our 2 We extracted this feature using the Chunklink.pl script made available by Sabine Buchholz at http://ilk.uvt. nl/team/sabine/chunklink/README.html 1074 example sentence, this feature would be for example s for “traders” and “heads”, etc. As for features (F7) and (F8), they rely on information about the main verb of the current sentence. More specifically, feature (F7) is the main verb token (i.e. shook in our example), extracted following the head-finding strategy by Yamada and Matsumoto (2003), while feature (F8) is a boolean feature that indicates for each token if it is the main verb in the sentence or not.3 The previous sentence feature “Prev” (F9) is a connective-surface feature and is used to capture if the following sentence begins with a connective. Our intuition is that it may be relevant to detect Arg1 boundaries in inter-sentential relations. The feature value for each candidate token of a sentence corresponds to the connective token that appears at the beginning of the following sentence, if any. Otherwise, it is equal to 0. We also add gold-standard Arg2 labels (F10) as"
I11-1120,P09-2004,0,0.636093,"rocessing step. With our approach, instead, we can directly use PDTB argument spans both for training and for testing. Dinesh et al. (2005) extracted complete arguments with boundaries, but only for a restricted class of connectives. The recent work by Prasad et al. (2010) is also limited, since their system only extracts the sentences containing the arguments. In our work, we assume that explicit discourse connectives are given beforehand, either taken directly from a gold standard or automatically identified. The second task based on PDTB was tackled among others by Pitler et al. (2008) and Pitler and Nenkova (2009). In addition to the work on finding explicit connectives and their arguments, there has been recent work on classification of implicit discourse relations, see for instance Lin et al. (2009). In a similar classification experiment, Pitler et al. (2009) investigated features ranging from low-level word pairs to high-level linguistic cues, and demonstrated that it is useful to model the sequence of discourse relations using a sequence labeler. Although they both outperformed their respective baselines, this task is very difficult and performances are still very low. 3 The Penn Discourse Treeban"
I11-1120,C08-2022,0,0.0290911,"se trees with a further processing step. With our approach, instead, we can directly use PDTB argument spans both for training and for testing. Dinesh et al. (2005) extracted complete arguments with boundaries, but only for a restricted class of connectives. The recent work by Prasad et al. (2010) is also limited, since their system only extracts the sentences containing the arguments. In our work, we assume that explicit discourse connectives are given beforehand, either taken directly from a gold standard or automatically identified. The second task based on PDTB was tackled among others by Pitler et al. (2008) and Pitler and Nenkova (2009). In addition to the work on finding explicit connectives and their arguments, there has been recent work on classification of implicit discourse relations, see for instance Lin et al. (2009). In a similar classification experiment, Pitler et al. (2009) investigated features ranging from low-level word pairs to high-level linguistic cues, and demonstrated that it is useful to model the sequence of discourse relations using a sequence labeler. Although they both outperformed their respective baselines, this task is very difficult and performances are still very low"
I11-1120,P09-1077,0,0.0761964,"d et al. (2010) is also limited, since their system only extracts the sentences containing the arguments. In our work, we assume that explicit discourse connectives are given beforehand, either taken directly from a gold standard or automatically identified. The second task based on PDTB was tackled among others by Pitler et al. (2008) and Pitler and Nenkova (2009). In addition to the work on finding explicit connectives and their arguments, there has been recent work on classification of implicit discourse relations, see for instance Lin et al. (2009). In a similar classification experiment, Pitler et al. (2009) investigated features ranging from low-level word pairs to high-level linguistic cues, and demonstrated that it is useful to model the sequence of discourse relations using a sequence labeler. Although they both outperformed their respective baselines, this task is very difficult and performances are still very low. 3 The Penn Discourse Treebank (PDTB) The Penn Discourse Treebank (Prasad et al., 2008) is a resource including one million words from the Wall Street Journal (Marcus et al., 1993), annotated with discourse relations. Based on the observation that “no discourse connective has yet b"
I17-1029,P15-1010,0,0.0156358,"word senses. While this is descriptive of the texts in the corpus at hand, it can be problematic for generalization. For instance, word senses that are underrepresented or absent in the training corpus will not be assigned a functional embedding. On the other hand, due to the ability of these models to process large amounts of data, well-represented word senses will acquire meaningful representations. The alternative approach to unsupervised methods is to include data from knowledge resources, usually graph-encoded semantic networks (SN) such as WordNet (Miller, 1995). Chen et al. (2014) and Iacobacci et al. (2015) propose to make use of knowledge resources to produce a sense-annotated corpus, on which known techniques can then be applied to generate word sense embeddings. A usual way of circumventing the lack of sense-annotated corpora is to apply postprocessing techniques onto pre-trained word embeddings as a way of leveraging lexical information to produce word sense embeddings. The following models share this method: Johansson and Nieto-Pi˜na (2015) formulate an optimization problem to derive multiple word sense representations from word embeddings, while Pilehvar and Collier (2016) and one of the m"
I17-1029,P14-1023,0,0.0230604,"ler approaches. Our results show that a corpusbased model balanced with lexicographic data learns better representations and improve their performance in downstream tasks. 1 Introduction Word embeddings, as a tool for representing the meaning of words based on the context in which they appear, have had a considerable impact on many of the traditional Natural Language Processing tasks in recent years. (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Glorot et al., 2011) This form of semantic representation has come to replace in many instances traditional count-based vectors (Baroni et al., 2014), as they yield high-quality semantic representations in a computationally efficient manner, which allows them to leverage information from large corpora. 284 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 284–294, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP to discover different usages of words in corpora, and those that make use of knowledge resources as a way of injecting linguistic knowledge into the models. ing word sense embeddings by leveraging its signal through a regularizer function that is applied on top of a tradition"
I17-1029,N15-1070,0,0.0209642,"ling to account for new meanings of a word. The model that we present in this article tries to preserve desirable characteristics from both approaches. On one side, the model learns word sense embeddings from a corpus using a predictive learning algorithm that is efficient, streamlined, and flexible with respect to being able to discriminate between different usages of a word from running text. This learning algorithm is based on the idea of adding an extra latent variable to the Skip-gram objective function to account for different senses of a word, that has been explored in previous work by Jauhar et al. (2015) and NietoPi˜na and Johansson (2015). On the other side, the learning process is guided by a regularizer function that introduces information from an SN, in an attempt to achieve a clear, complete, and fair division between the different senses of a word. Furthermore, from a technical point of view, the effect of the regularizer function is applied in parallel to the embedding learning process. This eliminates the need for a two-step training process or pretrained word embeddings, and makes it possible to regulate the influence that each source of data (corpus and SN) has on the learning proce"
I17-1029,L16-1482,1,0.838008,"rames, but there is a deterministic mapping from frames to SALDO senses. Two additional datasets are taken from the Senseval-2 Swedish lexical sample task (Kokkinakis et al., 2001). It uses a different sense inventory, which we mapped manually to SALDO senses. The lexical sample originally consisted of instances for 40 lemmas, out of which we removed 7 lemmas because they were unambiguous in SALDO. Since we are using an unsupervised experimental setup, we report results not only on the designated test set but also on the training set. The other datasets come from the Koala annotation project (Johansson et al., 2016). The latest version consists of seven different corpora, each sampled from text in a separate domain: blogs, novels, Wikipedia, European Parliament proceedings, political news, newsletters from a government agency, and government press releases. Unlike the two lexicographical example sets and the Senseval-2 lexical sample, in which the instances have been selected by lexicographers to be prototypical and to have a good coverage of the sense variation, the instances in the Koala corpora are annotated ‘as is’ in running text. Sense-annotated Datasets We evaluated the WSD systems on eleven diffe"
I17-1029,D14-1110,0,0.044739,"Missing"
I17-1029,N15-1164,1,0.880714,"Missing"
I17-1029,S01-1011,0,0.126832,"Missing"
I17-1029,D15-1200,0,0.0214541,"and Mooney (2010) and Huang et al. (2012), who propose to cluster occurrences of words based on their contexts to account for different meanings. With the advent of the Skip-gram model (Mikolov et al., 2013) as an efficient way of training prediction-based word embedding models, much of the research into obtaining word sense representations revolved around it. Neelakantan et al. (2014) and Nieto-Pi˜na and Johansson (2015) make use of context-based word sense disambiguation (WSD) during corpus training to allow on-line learning of multiple senses of a word with modified versions of Skip-gram. Li and Jurafsky (2015) and Bartunov et al. (2016) apply stochastic processes to allow for representations of a variable number of senses per word to be learnt in unsupervised fashion from corpora. The embeddings obtained using this approach tend to be word-usage oriented, rather than represent formally defined word senses. While this is descriptive of the texts in the corpus at hand, it can be problematic for generalization. For instance, word senses that are underrepresented or absent in the training corpus will not be assigned a functional embedding. On the other hand, due to the ability of these models to proces"
I17-1029,heppin-gronostaj-2012-rocky,0,0.0608856,"Missing"
I17-1029,D14-1113,0,0.0217593,"e influence of the lexicographic data on the embeddings by comparing different model parameterizations. We conclude with a discussion of our results in Section 5. 2 Among the earliest efforts in the former group is the work of Reisinger and Mooney (2010) and Huang et al. (2012), who propose to cluster occurrences of words based on their contexts to account for different meanings. With the advent of the Skip-gram model (Mikolov et al., 2013) as an efficient way of training prediction-based word embedding models, much of the research into obtaining word sense representations revolved around it. Neelakantan et al. (2014) and Nieto-Pi˜na and Johansson (2015) make use of context-based word sense disambiguation (WSD) during corpus training to allow on-line learning of multiple senses of a word with modified versions of Skip-gram. Li and Jurafsky (2015) and Bartunov et al. (2016) apply stochastic processes to allow for representations of a variable number of senses per word to be learnt in unsupervised fashion from corpora. The embeddings obtained using this approach tend to be word-usage oriented, rather than represent formally defined word senses. While this is descriptive of the texts in the corpus at hand, it"
I17-1029,P12-1092,0,0.0558975,"in a corpus. We give an overview of related work in Section 2, and our model is described in detail in Section 3. The resulting word sense embeddings are evaluated in Section 4 on two separate automated tasks: word sense disambiguation (WSD) and lexical frame prediction (LFP). The experiments used for evaluation allow us to investigate the influence of the lexicographic data on the embeddings by comparing different model parameterizations. We conclude with a discussion of our results in Section 5. 2 Among the earliest efforts in the former group is the work of Reisinger and Mooney (2010) and Huang et al. (2012), who propose to cluster occurrences of words based on their contexts to account for different meanings. With the advent of the Skip-gram model (Mikolov et al., 2013) as an efficient way of training prediction-based word embedding models, much of the research into obtaining word sense representations revolved around it. Neelakantan et al. (2014) and Nieto-Pi˜na and Johansson (2015) make use of context-based word sense disambiguation (WSD) during corpus training to allow on-line learning of multiple senses of a word with modified versions of Skip-gram. Li and Jurafsky (2015) and Bartunov et al."
I17-1029,D16-1174,0,0.022466,"Missing"
I17-1029,N10-1013,0,0.0642338,"om the data in a corpus. We give an overview of related work in Section 2, and our model is described in detail in Section 3. The resulting word sense embeddings are evaluated in Section 4 on two separate automated tasks: word sense disambiguation (WSD) and lexical frame prediction (LFP). The experiments used for evaluation allow us to investigate the influence of the lexicographic data on the embeddings by comparing different model parameterizations. We conclude with a discussion of our results in Section 5. 2 Among the earliest efforts in the former group is the work of Reisinger and Mooney (2010) and Huang et al. (2012), who propose to cluster occurrences of words based on their contexts to account for different meanings. With the advent of the Skip-gram model (Mikolov et al., 2013) as an efficient way of training prediction-based word embedding models, much of the research into obtaining word sense representations revolved around it. Neelakantan et al. (2014) and Nieto-Pi˜na and Johansson (2015) make use of context-based word sense disambiguation (WSD) during corpus training to allow on-line learning of multiple senses of a word with modified versions of Skip-gram. Li and Jurafsky (2"
I17-1029,P10-1040,0,0.0513817,"strength create in the vector space. Moreover, we evaluate the sense embeddings in two downstream applications: word sense disambiguation and semantic frame prediction, where they outperform simpler approaches. Our results show that a corpusbased model balanced with lexicographic data learns better representations and improve their performance in downstream tasks. 1 Introduction Word embeddings, as a tool for representing the meaning of words based on the context in which they appear, have had a considerable impact on many of the traditional Natural Language Processing tasks in recent years. (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Glorot et al., 2011) This form of semantic representation has come to replace in many instances traditional count-based vectors (Baroni et al., 2014), as they yield high-quality semantic representations in a computationally efficient manner, which allows them to leverage information from large corpora. 284 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 284–294, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP to discover different usages of words in corpora, and those that make use of kno"
I17-1029,E09-1005,0,\N,Missing
J13-3002,P07-1056,0,0.0174502,"Missing"
J13-3002,W06-1651,0,0.808682,"Missing"
J13-3002,D08-1083,0,0.240148,"Missing"
J13-3002,P10-2050,0,0.678975,"the opinion expressions. Ruppenhofer, Somasundaran, and Wiebe (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. Choi, Breck, and Cardie (2006) built a joint model of opinion expression extraction and holder extraction and applied integer linear programming to carry out the optimization step. While the tasks of opinion expression detection and polarity classification of opinion expressions (Wilson, Wiebe, and Hoffmann 2009) have mostly been studied in isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion expressions and assigned them polarity values and this is so far the only published result on joint opinion segmentation and polarity classification. Their experiment, however, lacked the obvious baseline: a standard pipeline consisting of an expression tagger followed by a polarity classifier. In addition, although their model is the first end-to-end system for opinion expression extraction and polarity classification, it is still based on sequence labeling and thus by construction limited in feature expressivity. On a concep"
J13-3002,W02-1001,0,0.0225998,"are unable to handle overlapping opinion expressions, but they are fortunately rare. To exemplify, Figure 1 shows an example of a sentence and how it is processed by the sequence labeler. The ESE defenseless situation is encoded in IOB2 as two tags B-ESE and I-ESE. There are four input columns (words, lemmas, POS tags, subjectivity clues) and one output column (opinion expression tags in IOB2 encoding). The figure also shows the sliding window from which the feature extraction function can extract features when predicting an output tag (at the arrow). We trained the model using the method by Collins (2002), with a Viterbi decoder and the online Passive–Aggressive algorithm (Crammer et al. 2006) to estimate the model weights. The learning algorithm parameters were tuned on a development set. When searching for the best value of the C parameter, we varied it along a log scale from 479 Computational Linguistics HRW has denounced the defenseless situation of these prisoners HRW have denounce the defenseless situation of this prisoner Volume 39, Number 3 NNP VBZ VBN DT JJ NN IN DT NNS − − str/neg − − − − − weak/neg O O B−DSE O B−ESE I−ESE Figure 1 Sequence labeling example. 0.001 to 100, and the bes"
J13-3002,N09-1057,0,0.0109708,"nd Hatzivassiloglou 2003) or low-level grammatical features such as part-of-speech tags and functional words (Wiebe, Bruce, and O’Hara 1999). This is in line with the general consensus in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search (Moschitti and Basili 2004). There are a few exceptions, such as Karlgren et al. (2010), who showed that construction features added to a bag-ofwords representation resulted in improved performance on a number of coarse-grained opinion analysis tasks. Similarly, Greene and Resnik (2009) argued that a speaker’s attitude can be predicted from syntactic features such as the selection of a transitive or intransitive verb frame. In contrast to the early work, recent years have seen a shift towards more detailed problem formulations where the task is not only to find a piece of opinionated text, but also to extract a structured representation of the opinion. For instance, we may determine the person holding the opinion (the holder) and towards which entity or fact it is directed (the topic), whether it is positive or negative (the polarity), and the strength of the opinion (the in"
J13-3002,P08-1067,0,0.0286941,"only simple local features, to generate a relatively small hypothesis set; we then applied a classifier using interaction features to pick the final result. A common objection to reranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate. Investigating inference methods that take a less brute-force approach than plain reranking is thus another possible future direction. Interesting examples of such inference methods include forest reranking (Huang 2008) and loopy belief propagation (Smith and Eisner 2008). Nevertheless, although the development of such algorithms is a fascinating research problem, it will not necessarily result in a more usable system: Rerankers impose very few restrictions on feature expressivity and make it easy to trade accuracy for efficiency. We investigated the effect of machine learning features, as well as other design parameters such as the choice of machine learning method and the size of the hypothesis set. For the features, we analyzed the impact of using syntax and semantics and saw that the best models are thos"
J13-3002,P10-1060,0,0.0531441,"Missing"
J13-3002,W08-2123,1,0.897783,"phrase of the sentence is its holder or not. Separate classifiers were trained to extract holders for DSEs, ESEs, and OSEs. Hereafter, we describe the feature set used by the classifiers. Our walkthrough example is given by the sentence in Figure 1. Some features are derived from the syntactic and shallow semantic analysis of the sentence, shown in Figure 2 (Section 6.1 gives more details on this). S YNTACTIC PATH. Similarly to the path feature widely used in semantic role labeling (SRL), we extract a feature representing the path in the dependency tree between the expression and the holder (Johansson and Nugues 2008). For instance, the path from denounced to HRW in the example is VC↑SBJ↓. S HALLOW- SEMANTIC RELATION. If there is a direct shallow-semantic relation between the expression and the holder, we use a feature representing its semantic role, such as A0 between denounced and HRW. E XPRESSION HEAD WORD , POS, AND LEMMA. denounced, VBD, denounce for denounced; situation, NN, situation for defenseless situation. H EAD WORD AND POS OF HOLDER CANDIDATE. HRW, NNP for HRW. D OMINATING EXPRESSION TYPE. When locating the holder for the ESE defenseless situation, we extract a feature representing the fact th"
J13-3002,P09-2079,0,0.0664067,"Missing"
J13-3002,W06-0301,0,0.597559,"o natural language processing territory; the methods used here have been inspired by information extraction and semantic role labeling, combinatorial optimization, and structured machine learning. For such tasks, deeper representations of linguistic structure have seen more use than in the coarsegrained case. Syntactic and shallow-semantic relations have repeatedly proven useful for subtasks of opinion analysis that are relational in nature, above all for determining the holder or topic of a given opinion, in which case there is considerable similarity to tasks such as semantic role labeling (Kim and Hovy 2006; Ruppenhofer, Somasundaran, and Wiebe 2008). There has been no systematic research, however, on the role played by linguistic structure in the relations between opinions expressed in text, despite the fact that the opinion expressions in a sentence are not independent but organized rhetorically to achieve a communicative effect intended by the speaker. We therefore expect that the interplay between opinion expressions can be exploited to derive information useful for the analysis of opinions expressed in text. In this article, we start from this intuition and propose several novel features de"
J13-3002,D07-1114,0,0.250077,"Missing"
J13-3002,W04-2705,0,0.012106,"fiers); they did not directly use the local features ΦL . We normalized the scores over the k candidates so that their exponentials summed to 1. 6.1 Syntactic and Shallow Semantic Analysis The features used by the rerankers, as well as the opinion holder extractor in Section 4.2, are to a large extent derived from syntactic and semantic role structures. To extract them, we used the syntactic–semantic parser by Johansson and Nugues (2008), which annotates the sentences with dependency syntax (Mel’ˇcuk 1988) and shallow semantics in the PropBank (Palmer, Gildea, and Kingsbury 2005) and NomBank (Meyers et al. 2004) frameworks, using the format of the CoNLL-2008 Shared Task (Surdeanu et al. 2008). The system includes a sense disambiguator that assigns PropBank or NomBank senses to the predicate words. Figure 2 shows an example of the structure of the annotation: The sentence HRW denounced the defenseless situation of these prisoners, where denounced is a DSE and defenseless situation is an ESE, has been annotated with dependency syntax (above the text) and semantic role structure (below the text). The predicate denounced, which is an instance of the PropBank frame denounce.01, has two semantic arguments:"
J13-3002,J05-1004,0,0.00732727,"Missing"
J13-3002,P04-1035,0,0.0774089,"these features leads to statistically significant improvements in all scenarios we evaluated. First, we develop a system for the extraction of evaluations of product attributes from product reviews (Hu and Liu 2004a, 2004b; Popescu and Etzioni 2005; Titov and McDonald 2008), and we show that the features derived from opinion expressions lead to significant improvement. Secondly, we show that fine-grained opinion structural information can even be used to build features that improve a coarse-grained sentiment task: document polarity classification of reviews (Pang, Lee, and Vaithyanathan 2002; Pang and Lee 2004). After the present introduction, Section 2 gives a linguistic motivation and an overview of the related work; Section 3 describes the MPQA opinion corpus and its underlying representation; Section 4 illustrates the baseline systems: a sequence labeler for the extraction of opinion expressions and classifiers for opinion holder extraction and polarity labeling; Section 5 reports on the main contribution: the description of the interaction models and their features; finally, Section 7 presents the experimental results and Section 8 derives the conclusions. 2. Motivation and Related Work Intuiti"
J13-3002,W02-1011,0,0.0153832,"Missing"
J13-3002,H05-1043,0,0.220532,"there have been several publications detailing the extraction of MPQA-style opinion expressions, as far as we are aware there has been no attempt to use them in an application. In contrast, we show that the opinion expressions as defined by the MPQA corpus may be used to derive machine learning features that are useful in two practical opinion mining tasks; the addition of these features leads to statistically significant improvements in all scenarios we evaluated. First, we develop a system for the extraction of evaluations of product attributes from product reviews (Hu and Liu 2004a, 2004b; Popescu and Etzioni 2005; Titov and McDonald 2008), and we show that the features derived from opinion expressions lead to significant improvement. Secondly, we show that fine-grained opinion structural information can even be used to build features that improve a coarse-grained sentiment task: document polarity classification of reviews (Pang, Lee, and Vaithyanathan 2002; Pang and Lee 2004). After the present introduction, Section 2 gives a linguistic motivation and an overview of the related work; Section 3 describes the MPQA opinion corpus and its underlying representation; Section 4 illustrates the baseline syste"
J13-3002,prasad-etal-2008-penn,0,0.015091,"categorization verbs dominating them. (Here, opinions are marked S and holders H.) Example (2) (a) [Domestic observers]H [tended to side with]S the MDC, [denouncing]S the election as [fraud-tainted]S and [unfair]S . (b) [Bush]H [labeled]S North Korea, Iran and Iraq an “[axis of evil]S .” In addition, interaction is important when determining opinion polarity. Here, relations that influence polarity interpretation include coordination, verb–complement, as well as other types of discourse relations. In particular, the presence of a C OMPARISON discourse relation, such as contrast or concession (Prasad et al. 2008), may allow us to infer that opinion expressions have different polarities. In Example (3), we see how contrastive discourse connectives (underlined) are used when there are contrasting polarities in the surrounding opinion expressions. (Positive opinions are tagged ‘+’, negative ‘-’.) Example (3) (a) “[This is no blind violence but rather targeted violence]- ,” Annemie Neyts [said]- . “However, the movement [is more than that]+ .” (b) “[Trade alone will not save the world]- ,” Neyts [said]- , but it constitutes an [important]+ factor for economic development. The problems we focus on in this"
J13-3002,ruppenhofer-etal-2008-finding,0,0.450978,"Missing"
J13-3002,D08-1016,0,0.0154094,"relatively small hypothesis set; we then applied a classifier using interaction features to pick the final result. A common objection to reranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate. Investigating inference methods that take a less brute-force approach than plain reranking is thus another possible future direction. Interesting examples of such inference methods include forest reranking (Huang 2008) and loopy belief propagation (Smith and Eisner 2008). Nevertheless, although the development of such algorithms is a fascinating research problem, it will not necessarily result in a more usable system: Rerankers impose very few restrictions on feature expressivity and make it easy to trade accuracy for efficiency. We investigated the effect of machine learning features, as well as other design parameters such as the choice of machine learning method and the size of the hypothesis set. For the features, we analyzed the impact of using syntax and semantics and saw that the best models are those making use of both. The most effective features we"
J13-3002,D09-1018,0,0.0614763,"s and assigned them polarity values and this is so far the only published result on joint opinion segmentation and polarity classification. Their experiment, however, lacked the obvious baseline: a standard pipeline consisting of an expression tagger followed by a polarity classifier. In addition, although their model is the first end-to-end system for opinion expression extraction and polarity classification, it is still based on sequence labeling and thus by construction limited in feature expressivity. On a conceptual level, discourse-oriented approaches (Asher, Benamara, and Mathieu 2009; Somasundaran et al. 2009; Zirn et al. 2011) applying interaction features for polarity classification are arguably the most related because they are driven by a vision similar to ours: Individual opinion expressions interplay in discourse and thus provide information about each other. On a practical level there are obvious differences, since our features are extracted from syntactic and shallow-semantic linguistic representations, which we argue are reflections of discourse structure, while they extract features directly from a discourse representation. It is doubtful whether automatic discourse representation extrac"
J13-3002,W06-1640,0,0.0164309,"dentifier has been able to generalize from the specific domains. It would still be relevant, however, to apply domain adaptation techniques (Blitzer, Dredze, and Pereira 2007). It could also be interesting to see how domain-specific opinion word lexicons could improve over the generic lexicon we used here; especially if such a lexicon were automatically constructed (Jijkoun, de Rijke, and Weerkamp 2010). There are multiple additional opportunities for future work in this area. An important issue that we have left open is the coreference problem for holder extraction, which has been studied by Stoyanov and Cardie (2006). Similarly, recent work has tried to incorporate complex, high-level linguistic structure such as discourse representations (Asher, Benamara, and Mathieu 2009; Somasundaran et al. 2009; Zirn et al. 2011); it is clear that these structures are very relevant for explaining the way humans organize 505 Computational Linguistics Volume 39, Number 3 their expressions of opinions rhetorically. Theoretical depth does not necessarily guarantee practical applicability, however, and the challenge is as usual to find a middle ground that balances our goals: explanatory power in theory, significant perfor"
J13-3002,stoyanov-cardie-2008-annotating,0,0.00677489,"ntensity feature taking the values L OW, M EDIUM, H IGH, and E XTREME. The two sentences in Example (6) from the MPQA corpus show opinion expressions with polarities. Positive polarity is represented with a ‘+’ and negative with a ‘-’. Example (6) (a) We foresaw electoral [fraud]- but not [daylight robbery]- . (b) Join in this [wonderful]+ event and help Jameson Camp continue to provide the year-round support that gives kids a [chance to create dreams]+ . The corpus does not currently contain annotation of topics (evaluees) of opinions, although there have been efforts to add this separately (Stoyanov and Cardie 2008). 4. Baseline Systems for Fine-Grained Opinion Analysis The assessment of our reranking-based systems requires us to compare against strong baselines. We developed (1) a sequence labeler for opinion expression extraction similar to that by Breck, Choi, and Cardie (2007), (2) a set of classifiers to determine the opinion holder, and (3) a multiclass classifier that assigns polarity to a given opinion expression similar to that described by Wilson, Wiebe, and Hoffmann (2009). These tools were also used to generate the hypothesis sets for the rerankers described in Section 5. 4.1 Sequence Labeler"
J13-3002,W08-2121,1,0.617805,"Missing"
J13-3002,P08-1036,0,0.0485753,"blications detailing the extraction of MPQA-style opinion expressions, as far as we are aware there has been no attempt to use them in an application. In contrast, we show that the opinion expressions as defined by the MPQA corpus may be used to derive machine learning features that are useful in two practical opinion mining tasks; the addition of these features leads to statistically significant improvements in all scenarios we evaluated. First, we develop a system for the extraction of evaluations of product attributes from product reviews (Hu and Liu 2004a, 2004b; Popescu and Etzioni 2005; Titov and McDonald 2008), and we show that the features derived from opinion expressions lead to significant improvement. Secondly, we show that fine-grained opinion structural information can even be used to build features that improve a coarse-grained sentiment task: document polarity classification of reviews (Pang, Lee, and Vaithyanathan 2002; Pang and Lee 2004). After the present introduction, Section 2 gives a linguistic motivation and an overview of the related work; Section 3 describes the MPQA opinion corpus and its underlying representation; Section 4 illustrates the baseline systems: a sequence labeler for"
J13-3002,E99-1023,0,0.0325909,"Missing"
J13-3002,P99-1032,0,0.142381,"Missing"
J13-3002,N10-1121,0,0.38338,"g local features in a small contextual window. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al. 2005; Kobayashi, Inui, and Matsumoto 2007; Joshi and Penstein-Ros´e 476 Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis 2009; Wu et al. 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi, Breck, and Cardie (2006) successfully used a PropBank-based role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for extracting holders, but did not consider their relations to the opinion expressions. Ruppenhofer, Somasundaran, and Wiebe (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. Choi, Breck, and Cardie (2006) built a joint model of opinion expression extraction and holder extraction and applied integer linear programming to carry out the optimization step. While the"
J13-3002,H05-1044,0,0.375829,"Missing"
J13-3002,D09-1159,0,0.127022,"r of publications (Choi, Breck, and Cardie 2006; Breck, Choi, and Cardie 2007). Such systems do not use any features describing the interaction between opinions, and it would not be possible to add interaction features because a Viterbibased sequence labeler by construction is restricted to using local features in a small contextual window. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al. 2005; Kobayashi, Inui, and Matsumoto 2007; Joshi and Penstein-Ros´e 476 Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis 2009; Wu et al. 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi, Breck, and Cardie (2006) successfully used a PropBank-based role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for extracting holders, but did not consider their relations to the opinion expressions. Ruppenhofer, Somasundaran, and Wiebe (2008) argued that semantic role techniques are useful but not comp"
J13-3002,W03-1017,0,0.241498,"cal uses, either as stand-alone applications or supporting other tools such as information retrieval or question answering systems. The research community initially focused on high-level tasks such as retrieving documents or passages expressing opinion, or classifying the polarity of a given text, and these coarse-grained problem formulations naturally led to the application of methods derived from standard retrieval or text categorization techniques. The models underlying these approaches have used very simple feature representations such as purely lexical (Pang, Lee, and Vaithyanathan 2002; Yu and Hatzivassiloglou 2003) or low-level grammatical features such as part-of-speech tags and functional words (Wiebe, Bruce, and O’Hara 1999). This is in line with the general consensus in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search (Moschitti and Basili 2004). There are a few exceptions, such as Karlgren et al. (2010), who showed that construction features added to a bag-ofwords representation resulted in improved performance on a number of coarse-grained opinion analysis tasks. Similarly, Greene and Resnik (2009)"
J13-3002,I11-1038,0,0.450832,"ty values and this is so far the only published result on joint opinion segmentation and polarity classification. Their experiment, however, lacked the obvious baseline: a standard pipeline consisting of an expression tagger followed by a polarity classifier. In addition, although their model is the first end-to-end system for opinion expression extraction and polarity classification, it is still based on sequence labeling and thus by construction limited in feature expressivity. On a conceptual level, discourse-oriented approaches (Asher, Benamara, and Mathieu 2009; Somasundaran et al. 2009; Zirn et al. 2011) applying interaction features for polarity classification are arguably the most related because they are driven by a vision similar to ours: Individual opinion expressions interplay in discourse and thus provide information about each other. On a practical level there are obvious differences, since our features are extracted from syntactic and shallow-semantic linguistic representations, which we argue are reflections of discourse structure, while they extract features directly from a discourse representation. It is doubtful whether automatic discourse representation extraction in text is cur"
J13-3002,H05-2017,0,\N,Missing
J13-3002,P10-1160,0,\N,Missing
J13-3002,J09-3003,0,\N,Missing
johansson-etal-2012-semantic,N10-1138,0,\N,Missing
johansson-etal-2012-semantic,J08-2001,0,\N,Missing
johansson-etal-2012-semantic,S07-1048,1,\N,Missing
johansson-etal-2012-semantic,W04-3212,0,\N,Missing
johansson-etal-2012-semantic,J92-4003,0,\N,Missing
johansson-etal-2012-semantic,P09-1003,0,\N,Missing
johansson-etal-2012-semantic,P08-1068,0,\N,Missing
johansson-etal-2012-semantic,W08-2123,1,\N,Missing
johansson-etal-2012-semantic,P07-1071,0,\N,Missing
johansson-etal-2012-semantic,P06-2057,1,\N,Missing
johansson-etal-2012-semantic,J02-3001,0,\N,Missing
johansson-etal-2012-semantic,P02-1031,0,\N,Missing
johansson-etal-2012-semantic,C08-1050,1,\N,Missing
johansson-nugues-2006-construction,W04-0803,0,\N,Missing
johansson-nugues-2006-construction,W04-2407,0,\N,Missing
johansson-nugues-2006-construction,H01-1035,0,\N,Missing
johansson-nugues-2006-construction,C04-1134,0,\N,Missing
johansson-nugues-2006-construction,J03-4003,0,\N,Missing
johansson-nugues-2006-construction,H05-1108,0,\N,Missing
johansson-nugues-2006-construction,P98-1013,0,\N,Missing
johansson-nugues-2006-construction,C98-1013,0,\N,Missing
johansson-nugues-2006-construction,P02-1050,0,\N,Missing
johansson-nugues-2006-construction,J02-3001,0,\N,Missing
johansson-nugues-2006-construction,J03-1002,0,\N,Missing
johansson-nugues-2006-construction,2005.mtsummit-papers.11,0,\N,Missing
johansson-nugues-2008-comparing,A00-2018,0,\N,Missing
johansson-nugues-2008-comparing,burchardt-etal-2006-salsa,0,\N,Missing
johansson-nugues-2008-comparing,E06-1011,0,\N,Missing
johansson-nugues-2008-comparing,P97-1003,0,\N,Missing
johansson-nugues-2008-comparing,S07-1048,1,\N,Missing
johansson-nugues-2008-comparing,W04-3212,0,\N,Missing
johansson-nugues-2008-comparing,P98-1013,0,\N,Missing
johansson-nugues-2008-comparing,C98-1013,0,\N,Missing
johansson-nugues-2008-comparing,W05-0620,0,\N,Missing
johansson-nugues-2008-comparing,J02-3001,0,\N,Missing
johansson-nugues-2008-comparing,W07-2416,1,\N,Missing
K18-2002,J13-3002,1,0.895149,"negation metric used in the EPE context—counting as true positives only perfectly retrieved full scopes, including an exact match on negated events. and PoS tags. Conversely, the opinion holder extraction and reranking modules make central use of structural information, i.e. paths and topological properties in one or more syntactico-semantic dependency graph(s). In the EPE context, we evaluated how well the participating systems extract the three types of structures mentioned above: expressions, holders, and polarities. In each case, soft-boundary precision and recall measures were computed (Johansson and Moschitti, 2013; Johansson, 2017). Furthermore, for the detailed analysis we evaluated the opinion holder extractor separately, using goldstandard opinion expressions. We refer to this task as in-vitro holder extraction, and this score is used for the overall ranking of submissions when averaging F1 scores across the three EPE downstream applications. The reason for highlighting this score is that it is the one most strongly affected by the design of the dependency representation. Participating Teams Nine teams participated in EPE 2017, in the order of overall rank: Stanford– Paris (Schuster et al., 2017), S"
K18-2002,W09-1401,0,0.358818,"generated to a large degree from syntactic dependency parses. All classification tasks are implemented using the SVMmulticlass classifier (Joachims, 1999). TEES has been developed using corpora from the Biomedical Natural Language Processing (BioNLP) domain, in particular the event corpora from the BioNLP Shared Tasks. These tasks define their own annotation schemes and provide standardized evaluation services. In the context of the EPE challenge we use the BioNLP 2009 GENIA corpus and its associated evaluation program to measure the impact of different parses on event extraction performance (Kim et al., 2009). The metric used for comparing the EPE submissions is the primary ‘approximate span and recursive mode’ metric of the original Shared Task, a micro-averaged F1 score for the nine event classes of the corpus. The specialized domain language presents unique challenges for parsers not specifically optimized for this domain, so using this data set to evaluate open-domain parses may result in overall lower performance than with parsers specifically trained on e.g. the GENIA treebank (Tateisi et al., 2005). When using the EPE parse data, TEES features encompass the type and direction for the depend"
K18-2002,J16-4009,1,0.850677,"ation in the context of EPE 2017 is interpreted as a graph whose nodes are anchored in surface lexical units, and whose edges represent labeled directed relations between two nodes. Each node corresponds to a sub-string of the underlying linguistic signal (input string), identified by character stand-off pointers. Node labels can comprise a non-recursive attribute–value matrix (or ‘feature structure’), for example to encode lemma and part of speech information. Each graph can optionally designate one or more ‘top’ nodes, broadly interpreted as the root-level head or highest-scoping predicate (Kuhlmann and Oepen, 2016). In principle, this notion of dependency representations is broad in that it allows nodes that do not correspond to (full) surface tokens, partial or full overlap of nodes, as well as graphs that transcend fully connected rooted trees. Participating teams in the original EPE 2017 initiative did in fact take advantage of all these degrees of freedom, whereas in connection to the 2018 UD parsing task such variation is excluded by design. Biological Event Extraction The Turku Event Extraction System (TEES) (Björne, 2014) is a program developed for the automated extraction of events, complex rela"
K18-2002,S12-1042,1,0.849881,"on tasks. The first step is entity detection where each token in the sentence is predicted as an entity node or as negative. In the second step of edge detection, argument edges are predicted for all valid, directed pairs of nodes. In the third, unmerging step, overlapping events are ‘pulled apart’ by duplicating trigger nodes. In the optional fourth step of modifier detection, binary modifiers (such as speculation or negation) can be predicted for the detected events. All of the classification steps in the TEES system Negation Resolution The EPE negation resolution system is called Sherlock (Lapponi et al., 2012, 2017) and implements the perspective on negation defined by Morante and Daelemans (2012) through the creation of the Conan Doyle Negation Corpus for the Shared Task of the 2012 Joint Conference on Lexical and Computational Semantics (*SEM 2012). Negation instances are annotated as tri-partite structures: Negation cues can be full tokens (e.g. not), multi-word expressions (by no means), or sub-tokens (un in unfortunate); for each cue, its scope is defined as the possibly discontinuous sequence of (sub-)tokens affected by the negation. Additionally, a subset of in-scope tokens can be marked as"
K18-2002,P10-1052,0,0.0518508,"in unfortunate); for each cue, its scope is defined as the possibly discontinuous sequence of (sub-)tokens affected by the negation. Additionally, a subset of in-scope tokens can be marked as negated events or states, provided that the sentence is factual and the events in question did not take place. In the EPE context, gold-standard negation cues are provided, because this sub-task has been found relatively insensitive to grammatical structure (Velldal et al., 2012). Sherlock approaches negation resolution as a sequence labeling problem, using a Conditional Ran23 dom Field (CRF) classifier (Lavergne et al., 2010). The token-wise negation annotations contain multiple layers of information. Tokens may or may not be negation cues and they can be either in or out of scope for a specific cue; in-scope tokens may or may not be negated events. Moreover, multiple negation instances may be (partially or fully) overlapping. Before presenting the CRF with the annotations, Sherlock ‘flattens’ all negation instances in a sentence, assigning a six-valued extended ‘begin– inside–outside’ labeling scheme. After classification, hierarchical (overlapping) negation structures are reconstructed using a set of post-proces"
K18-2002,de-marneffe-etal-2006-generating,0,0.18263,"Missing"
K18-2002,W12-3602,1,0.835691,"sity of Washington (Peng et al., 2017). These teams submitted 49 distinct runs that encompassed many different families of dependency representations, various approaches to preprocessing and parsing, and variable types and volumes of training data. The dependency representations employed by the participants varied from more syntactically oriented schemes—e.g. Stanford Basic (de Marneffe et al., 2006), CoNLL 2008–style (Surdeanu et al., 2008), and UD—to more semantically oriented representations, such as the Deep Syntactic Structures of Ballesteros et al. (2015), DELPH-IN MRS Dependencies (DM; Ivanova et al., 2012), or Enju Predicate–Argument Structures (PAS; Miyao, 2006). The teams also employed wildly variable volumes of training data, ranging from around 200,000 tokens (the English UD treebanks) to 1,7 million tokens (combining the venerable Wall Street Journal, Brown, and GENIA treebanks). Opinion Analysis The system by Johansson and Moschitti (2013) marks up expressions of opinion and emotion in a pipeline comprised of three separate classification steps, combined with endto-end reranking; it was previously generalized and adapted for the EPE framework by Johansson (2017). The system is based on th"
K18-2002,P13-2017,0,0.136318,"Missing"
K18-2002,S12-1035,0,0.0462694,"of the classifier include different combinations of token-level observations, such as surface forms, part-of-speech tags, lemmas, and dependency labels. In addition, we extract both token and dependency distance to the nearest cue, together with the full shortest dependency path. Standard evaluation measures from the original shared task include scope tokens (ST), scope match (SM), event tokens (ET), and full negation (FN) F1 scores. ST and ET are token-level scores for inscope and negated event tokens, respectively, where a true positive is a correctly retrieved token of the relevant class (Morante and Blanco, 2012). FN is the strictest of these measures and the primary negation metric used in the EPE context—counting as true positives only perfectly retrieved full scopes, including an exact match on negated events. and PoS tags. Conversely, the opinion holder extraction and reranking modules make central use of structural information, i.e. paths and topological properties in one or more syntactico-semantic dependency graph(s). In the EPE context, we evaluated how well the participating systems extract the three types of structures mentioned above: expressions, holders, and polarities. In each case, soft"
K18-2002,I05-2038,0,0.173019,"ted evaluation program to measure the impact of different parses on event extraction performance (Kim et al., 2009). The metric used for comparing the EPE submissions is the primary ‘approximate span and recursive mode’ metric of the original Shared Task, a micro-averaged F1 score for the nine event classes of the corpus. The specialized domain language presents unique challenges for parsers not specifically optimized for this domain, so using this data set to evaluate open-domain parses may result in overall lower performance than with parsers specifically trained on e.g. the GENIA treebank (Tateisi et al., 2005). When using the EPE parse data, TEES features encompass the type and direction for the dependencies combined wit the text span and a single part of speech for the tokens; lemmas are not used. The term (bi-lexical) dependency representation in the context of EPE 2017 is interpreted as a graph whose nodes are anchored in surface lexical units, and whose edges represent labeled directed relations between two nodes. Each node corresponds to a sub-string of the underlying linguistic signal (input string), identified by character stand-off pointers. Node labels can comprise a non-recursive attribut"
K18-2002,J12-2005,1,0.852735,"tances are annotated as tri-partite structures: Negation cues can be full tokens (e.g. not), multi-word expressions (by no means), or sub-tokens (un in unfortunate); for each cue, its scope is defined as the possibly discontinuous sequence of (sub-)tokens affected by the negation. Additionally, a subset of in-scope tokens can be marked as negated events or states, provided that the sentence is factual and the events in question did not take place. In the EPE context, gold-standard negation cues are provided, because this sub-task has been found relatively insensitive to grammatical structure (Velldal et al., 2012). Sherlock approaches negation resolution as a sequence labeling problem, using a Conditional Ran23 dom Field (CRF) classifier (Lavergne et al., 2010). The token-wise negation annotations contain multiple layers of information. Tokens may or may not be negation cues and they can be either in or out of scope for a specific cue; in-scope tokens may or may not be negated events. Moreover, multiple negation instances may be (partially or fully) overlapping. Before presenting the CRF with the annotations, Sherlock ‘flattens’ all negation instances in a sentence, assigning a six-valued extended ‘beg"
K18-2002,P17-1186,0,0.022577,"Missing"
K18-2002,K18-2001,0,0.121439,"Missing"
L16-1430,D15-1304,0,0.0406932,"Missing"
L16-1430,pasha-etal-2014-madamira,0,0.0576435,"Missing"
L16-1430,D14-2007,0,0.112139,"Missing"
L16-1482,J08-4004,0,0.33284,"ally. When selecting the sense for a token, the annotator may select the option ‘none of the above’ if the SALDO lexicon lacks a suitable entry. In some cases, the new sense is very clearly distinct from the existing entries. In other cases, new meanings have been formed from existing senses through productive processes such as metaphor and metonymy, and it is a difficult question to decide exactly when a new sense is important or frequent enough for a new lexicon entry to be created (Cruse, 1986). Ideally, senseannotated corpora should be produced by trained lexicographers (Kilgarriff, 1999; Artstein and Poesio, 2008); this is probably hard to achieve in practice, but it is important that the lexicographers behind the lexicon are available for consultation by the annotators. 4.3. Inter-annotator Agreement Analysis The overlap between the two annotators allows us to measure their inter-annotator agreement, which we measured using the well-known κ coefficient (Cohen, 1960): κ= P (a) − P (e) 1 − P (e) where P (a) is the estimated probability of agreement – that is, the proportion of tokens where the two annotators agree – and P (e) the estimated agreement probability if the two annotators are assumed to be st"
L16-1482,borin-etal-2012-open,0,0.0642655,"Missing"
L16-1482,borin-etal-2012-korp,0,0.0758959,"ctable from the individual words. So for each token, we list a number of possible senses of the token as a whole, of each segment of each possible compund segmentation, and of each multiword unit the token could possibly be a part of. 4.1. Annotation Tool We developed a new annotation tool for this annotation project because we found no existing word sense annotation tool that could easily be adapted for our requirements of being able to annotate senses of compound segments and multiword units. Furthermore, developing our own tool made it easier to communicate with our lexicon infrastructure (Borin et al., 2012a) for explaining the senses to the annotators, and to design the tool for an efficient and ergonomic annotation process with a minimal use of the mouse. Since the SALDO lexicon lacks definitions and glosses, the annotation tool instead explains each available sense for a word by displaying a set of neighbors in the sense network. For instance, the noun fotboll (‘football’) has two senses: one referring to the sport, and another to the ball used in that sport. When annotating an occurrence of this word in a text, the tool shows a list of types of sport and other sportrelated terms when conside"
L16-1482,W15-1811,1,0.815326,"Missing"
L16-1482,E99-1046,0,0.115497,"onsidered sequentially. When selecting the sense for a token, the annotator may select the option ‘none of the above’ if the SALDO lexicon lacks a suitable entry. In some cases, the new sense is very clearly distinct from the existing entries. In other cases, new meanings have been formed from existing senses through productive processes such as metaphor and metonymy, and it is a difficult question to decide exactly when a new sense is important or frequent enough for a new lexicon entry to be created (Cruse, 1986). Ideally, senseannotated corpora should be produced by trained lexicographers (Kilgarriff, 1999; Artstein and Poesio, 2008); this is probably hard to achieve in practice, but it is important that the lexicographers behind the lexicon are available for consultation by the annotators. 4.3. Inter-annotator Agreement Analysis The overlap between the two annotators allows us to measure their inter-annotator agreement, which we measured using the well-known κ coefficient (Cohen, 1960): κ= P (a) − P (e) 1 − P (e) where P (a) is the estimated probability of agreement – that is, the proportion of tokens where the two annotators agree – and P (e) the estimated agreement probability if the two ann"
L16-1482,2005.mtsummit-papers.11,0,0.0120193,"an inter-annotator agreement study. 2. The Eukalyptus Corpus The Eukalyptus corpus consists of around 100,000 tokens of contemporary Swedish text. Since a goal of the project is to measure the domain sensitivity of Swedish NLP tools, we have collected text from five different genres, each of which contains about 20,000 tokens: • Fiction: the first chapters from four novels, • Encyclopedic: full articles from the Swedish Wikipedia, 100 to 3,000 tokens per article, • Social media: blog entries from the SIC corpus ¨ (Ostling, 2013), • Political debates: proceedings from the European parliament (Koehn, 2005), • Professional prose: a mix of different types of information from the goverment, and articles from Arbetaren, a Swedish weekly. For all genres, we have made sure that the text can be distributed freeely, and the corpus and all its annotation will eventually be released under a Creative Commons license. 3. The SALDO Lexicon The word sense annotation described in this paper uses SALDO (Borin et al., 2013) to define its sense inventory. While there are alternative Swedish sense inventories such as the Swedish WordNet (Viberg et al., 2003), SALDO has the advantage of being licensed under a Crea"
L16-1482,W15-1804,1,\N,Missing
N13-1013,P07-1056,0,0.0625985,"is different. However, domain adaptation and cross-treebank training can be seen as instances of the more general problem of multitask learning (Caruana, 1997). Indeed, one of the simplest and most well-known approaches to domain adaptation (Daum´e III, 2007), which will also be considered in this paper, should more correctly be seen as a trick to handle multitask learning with any machine learning algorithm. On the other hand, there is no point in trying to use domain adaptation methods assuming a covariate shift, e.g. instance weighting, or any method in which the target data is unlabeled (Blitzer et al., 2007; Ben-David et al., 2010). 2.1 yˆ = arg max w · f (x, y). y Here w is a weight vector produced by some learning algorithm and f (x, y) a feature representation that maps the sentence x with a parse tree y to a high-dimensional vector; the adaptation methods presented in this work is implemented as modifications of the feature representation function f . Since the search space is too large to be enumerated, the maximization must be handled carefully, and how this is done determines the expressivity of the feature representation f . In the parser by Carreras (2007) the maximization is carried ou"
N13-1013,bosco-etal-2010-comparing,0,0.0285085,"Missing"
N13-1013,W06-2920,0,0.713707,"zation conventions, so for Italian and Swedish we automatically retokenized the treebanks. We also made sure that the two treebanks for one language 130 used the same part-of-speech tag sets, by applying an automatic tagger when necessary. 3.1.1 ¨ German: Tiger and TuBa-D/Z For German, there are two treebanks available: Tiger (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2004). These treebanks are constituent treebanks, but dependency versions are available: T¨uBa-D/Z (version 7.0) includes the dependency version in the distribution, while for Tiger we used the version from CoNLL-X (Buchholz and Marsi, 2006). The constituent annotation styles in the two treebanks are radically different: Tiger uses a very flat structure with a minimal amount of intermediate nodes, while T¨uBa-D/Z uses a more elaborate structure including topological field information. However, the dependency versions are actually quite similar, at least with respect to attachment. The most common systematic difference we observed is in the annotation of coordination. Both treebanks are large: for Tiger, the training set was 31,243 sentences and the test set 7,973 sentences, and for T¨uBa-D/Z 40,000 and 11,428 sentences respective"
N13-1013,D07-1101,0,0.586903,"ch is generally better when one of the treebanks is very small, while the guided parsing approach is better when the treebanks are more similar in size. However, for most training set sizes the combination of the the two methods achieves a higher performance than either of them individually. 2 Methods for Training Parsers on Multiple Treebanks We now describe the two adaptation methods to leverage multiple treebanks for parser training. For clarity of presentation, we assume that there are two treebanks, although we can easily generalize to more. We use a common graph-based parsing technique (Carreras, 2007); the approaches described here could be used in transition-based parsing as well. In a graph-based parser, for a given sentence x the task of finding the top-scoring parse yˆ is stated as an optimization problem of maximizing a linear objective function: programming procedure relying on crucial independence assumptions to break down the search space into tractable parts. The factorization used in this approach allows f to express features extracted not only from single edges, as McDonald et al. (2005), but also from sibling and grandchild edges. To understand the machine learning problem of t"
N13-1013,P07-1033,0,0.454374,"Missing"
N13-1013,W07-2416,1,0.800845,"After preprocessing the data, we created training and test sets. For ISST, the training set was 2,239 and the test set 1,120 sentences, while for TUT the training set was 1,906 and the test set 954 sentences. 3.1.4 English: Two Different Conversions of the Penn Treebank For English, there is no significant dependency treebank so we followed most previous work in using dependency trees automatically derived from constituent trees in the large Penn Treebank WSJ corpus (Marcus et al., 1993). Due to the fact that there is a highly parametrizable constituentto-dependency conversion tool available (Johansson and Nugues, 2007), we could create two dependency treebanks with very different annotation styles. The first training set was created from sections 02–12 of the WSJ corpus. By default, the conversion tool outputs a treebank using the annotation style of the CoNLL-2008 Shared Task (Surdeanu et al., 2008); however we wanted to create a more surfaceoriented style for this treebank, so we turned on options to make wh-words heads of relative clauses, and possessive markers heads of noun phrases. This corpus had 20,706 sentences, and will be referred to as WSJ Part 1 in the experimental section. The second training"
N13-1013,C08-1050,1,0.85033,"ng and similar areas, we use a generalized notion of syntactic relationship that we encode by determining a path between two nodes in a syntactic tree. We defined the function Path(x, y) as a representation describing the steps required to traverse the parse tree from x to y, first the steps up from x to the common ancestor a and then down from a to y. Since we are working with unlabeled trees, the path can be represented as just two integers; to generalize to labeled dependency parsing, we could have used a full path representation as commonly used in dependency-based semantic role labeling (Johansson and Nugues, 2008). We added the following path-based feature templates, assuming we have a potential head h with dependent d, a sibling dependent s and grandchild (dependent-of-dependent) g: • • • • • POS(h)+POS(d)+Path(h, d) POS(h)+POS(s)+Path(h, s) POS(h)+POS(d)+POS(s)+Path(h, s) POS(h)+POS(g)+Path(h, g) POS(h)+POS(d)+POS(g)+Path(h, g) To exemplify, consider again the example la sospensione o l’interruzione shown in Figure 1. Assume that we are parsing according to the ISST representation (drawn above the sentence) and we consider adding an edge with sospensione as head and la as dependent, and another parse"
N13-1013,P12-1071,0,0.120909,"Missing"
N13-1013,J93-2004,0,0.0466482,"plied the TreeTagger POS tagger (Schmid, 1994) to both treebanks, using the pre-trained Italian model with the Baroni tagset3 . After preprocessing the data, we created training and test sets. For ISST, the training set was 2,239 and the test set 1,120 sentences, while for TUT the training set was 1,906 and the test set 954 sentences. 3.1.4 English: Two Different Conversions of the Penn Treebank For English, there is no significant dependency treebank so we followed most previous work in using dependency trees automatically derived from constituent trees in the large Penn Treebank WSJ corpus (Marcus et al., 1993). Due to the fact that there is a highly parametrizable constituentto-dependency conversion tool available (Johansson and Nugues, 2007), we could create two dependency treebanks with very different annotation styles. The first training set was created from sections 02–12 of the WSJ corpus. By default, the conversion tool outputs a treebank using the annotation style of the CoNLL-2008 Shared Task (Surdeanu et al., 2008); however we wanted to create a more surfaceoriented style for this treebank, so we turned on options to make wh-words heads of relative clauses, and possessive markers heads of"
N13-1013,D08-1017,0,0.0931016,"nesses (McDonald and Nivre, 2007), so that combining them may result in a better overall parsing accuracy. There are several ways to combine parsers; one of the simplest and most successful methods of parsing combination uses one parser as a guide for a second parser. This is normally implemented as a pipeline where the second parser extracts features based on the output of the first parser. Nivre and McDonald (2008) used this approach for combining a graph-based and a transition-based parser and achieved excellent results on test sets for several languages, and similar ideas were proposed by Martins et al. (2008). We added guide features to the parser feature representation. However, the features by Nivre and McDonald (2008) are slightly too simple since they only describe whether two words are directly connected or not. That makes sense if the two parsers are trying to predict the same type of representation, but will not help us if there are systematic annotation differences between the two treebanks, for instance in whether to annotate a function word or a lexical word as the head. Instead, following work in semantic role labeling and similar areas, we use a generalized notion of syntactic relation"
N13-1013,D07-1013,0,0.0302341,"ed features that make generalization easier. In particular, using a generalized fs would allow us to use this approach in more complex cases than considered here, for instance if the dependencies would be labeled with two different sets of grammatical function labels, or if one of the treebanks would use constituents rather than dependencies. 2.2 Using One Parser to Guide Another The second method is inspired by work in parser combination, an idea that has been applied successfully several times and relies on the fact that different parsing methods have different strengths and 129 weaknesses (McDonald and Nivre, 2007), so that combining them may result in a better overall parsing accuracy. There are several ways to combine parsers; one of the simplest and most successful methods of parsing combination uses one parser as a guide for a second parser. This is normally implemented as a pipeline where the second parser extracts features based on the output of the first parser. Nivre and McDonald (2008) used this approach for combining a graph-based and a transition-based parser and achieved excellent results on test sets for several languages, and similar ideas were proposed by Martins et al. (2008). We added g"
N13-1013,P05-1012,0,0.113527,"reebanks, although we can easily generalize to more. We use a common graph-based parsing technique (Carreras, 2007); the approaches described here could be used in transition-based parsing as well. In a graph-based parser, for a given sentence x the task of finding the top-scoring parse yˆ is stated as an optimization problem of maximizing a linear objective function: programming procedure relying on crucial independence assumptions to break down the search space into tractable parts. The factorization used in this approach allows f to express features extracted not only from single edges, as McDonald et al. (2005), but also from sibling and grandchild edges. To understand the machine learning problem of training parsers on incompatible treebanks, we compare it to the related problem of domain adaptation: training a system for a target domain, using a large collection of training data from a source domain combined with a small labeled or large unlabeled set from the target domain. Some algorithms for domain adaptation rely on the assumption that the differences between source and target distributions Ps and Pt can be explained in terms of a covariate shift: Ps (y|x) = Pt (y|x) for all x, y, but Ps (x) 6"
N13-1013,P07-1122,0,0.0604073,"Missing"
N13-1013,P09-1006,0,0.148739,"Missing"
N13-1013,P08-1108,0,0.164408,"second method is inspired by work in parser combination, an idea that has been applied successfully several times and relies on the fact that different parsing methods have different strengths and 129 weaknesses (McDonald and Nivre, 2007), so that combining them may result in a better overall parsing accuracy. There are several ways to combine parsers; one of the simplest and most successful methods of parsing combination uses one parser as a guide for a second parser. This is normally implemented as a pipeline where the second parser extracts features based on the output of the first parser. Nivre and McDonald (2008) used this approach for combining a graph-based and a transition-based parser and achieved excellent results on test sets for several languages, and similar ideas were proposed by Martins et al. (2008). We added guide features to the parser feature representation. However, the features by Nivre and McDonald (2008) are slightly too simple since they only describe whether two words are directly connected or not. That makes sense if the two parsers are trying to predict the same type of representation, but will not help us if there are systematic annotation differences between the two treebanks,"
N13-1013,D09-1086,0,0.0444037,"Missing"
N13-1013,W08-2121,1,0.880758,"Missing"
N13-1013,telljohann-etal-2004-tuba,0,0.0345974,". 3.1 Treebanks Used in the Experiments In our experiments, we used four languages: German, Swedish, Italian, and English. For each language, we had two treebanks. Our approaches currently require that the treebanks use the same tokenization conventions, so for Italian and Swedish we automatically retokenized the treebanks. We also made sure that the two treebanks for one language 130 used the same part-of-speech tag sets, by applying an automatic tagger when necessary. 3.1.1 ¨ German: Tiger and TuBa-D/Z For German, there are two treebanks available: Tiger (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2004). These treebanks are constituent treebanks, but dependency versions are available: T¨uBa-D/Z (version 7.0) includes the dependency version in the distribution, while for Tiger we used the version from CoNLL-X (Buchholz and Marsi, 2006). The constituent annotation styles in the two treebanks are radically different: Tiger uses a very flat structure with a minimal amount of intermediate nodes, while T¨uBa-D/Z uses a more elaborate structure including topological field information. However, the dependency versions are actually quite similar, at least with respect to attachment. The most common s"
N13-1013,D07-1096,0,\N,Missing
N15-1164,P14-1023,0,0.00606098,"Missing"
N15-1164,heppin-gronostaj-2012-rocky,0,0.192525,"Missing"
N15-1164,P12-1092,0,0.123726,"e concept. For instance, the word mouse can refer to a rodent or an electronic device. Vector-space representations typically represent surface forms only, which makes it hard to search e.g. for a group of words similar to the rodent sense of mouse or to reliably use the vectors in classifiers that rely on the semantics of the word. There have been several attempts to create vectors representing senses, most of them based on some variant of the idea first proposed by Sch¨utze (1998): that senses can be seen as clusters of similar contexts. Recent examples in this tradition include the work by Huang et al. (2012) and Neelakantan et al. (2014). However, because sense distributions are often highly imbalanced, it is not clear that context clusters can be reliably created for senses that occur rarely. These approaches also lack interpretability: if we are interested in the rodent sense of mouse, which of the vectors should we use? In this work, we instead derive sense vectors by embedding the graph structure of a semantic network in the word space. By combining two complementary sources of information – corpus statistics and network structure – we derive useful vectors also for concepts that occur rarely"
N15-1164,W15-1811,1,0.873635,"Missing"
N15-1164,W14-1618,0,0.0338876,"Missing"
N15-1164,J07-4005,0,0.0237882,"eal-valued vector E(sij ) in the same vector space as the lemma embeddings. The lemma and sense embeddings are related through a mix constraint: PF (li ) is decomposed as a convex combination j pij E(sij ), where the {pij } are picked from the probability simplex. Intuitively, the mix 1429 variables correspond to the occurrence probabilities of the senses, but strictly speaking this is only the case when the vectors are built using simple context counting. Since the mix gives an estimate of which sense is the most frequent in the corpus, we get a strong baseline for word sense disambiguation (McCarthy et al., 2007) as a bonus; see our followup paper (Johansson and Nieto Pi˜na, 2015) for a discussion of this. We can now formalize the intuition above: the weighted sum of distances between each sense and its neighbors is minimized, while satisfying the mix constraint for each lemma. We get the following constrained optimization program: minimize E,p subject to X wijk ∆(E(sij ), E(nijk )) i,j,k X pij E(sij ) = F (li ) ∀i pij = 1 ∀i j X (1) j pij ≥ 0 ∀i, j The mix constraints make sure that the solution is nontrivial. In particular, a very large number of words are monosemous, and the procedure will leave th"
N15-1164,N13-1090,0,0.00881206,"Missing"
N15-1164,D14-1113,0,0.106503,"the word mouse can refer to a rodent or an electronic device. Vector-space representations typically represent surface forms only, which makes it hard to search e.g. for a group of words similar to the rodent sense of mouse or to reliably use the vectors in classifiers that rely on the semantics of the word. There have been several attempts to create vectors representing senses, most of them based on some variant of the idea first proposed by Sch¨utze (1998): that senses can be seen as clusters of similar contexts. Recent examples in this tradition include the work by Huang et al. (2012) and Neelakantan et al. (2014). However, because sense distributions are often highly imbalanced, it is not clear that context clusters can be reliably created for senses that occur rarely. These approaches also lack interpretability: if we are interested in the rodent sense of mouse, which of the vectors should we use? In this work, we instead derive sense vectors by embedding the graph structure of a semantic network in the word space. By combining two complementary sources of information – corpus statistics and network structure – we derive useful vectors also for concepts that occur rarely. The method, which can be app"
N15-1164,C10-2107,0,0.0287538,"Missing"
N15-1164,J98-1004,0,0.655661,"Missing"
P06-2057,C04-1134,0,0.140093,"of FrameNet annotation, there have been a few projects that have studied transfer methods and evaluated the quality of the automatically produced corpus. Johansson and Nugues (2005) applied the wordbased methods of Yarowsky et al. (2001) and obtained promising results. Another recent effort (Padó and Lapata, 2005) demonstrates that deeper linguistic information, such as parse trees in the source and target language, is very beneficial for the process of FrameNet annotation transfer. A rather different method to construct bilingual semantic role annotation is the approach taken by BiFrameNet (Fung and Chen, 2004). In that work, 2.2 Transferring the Annotation We produced a Swedish-language corpus annotated with FrameNet information by applying the SRL system to the English side of Europarl (Koehn, 2005), which is a parallel corpus that is derived from the proceedings of the European Parliament. We projected the bracketing of the target words and the frame elements onto the Swedish side of the corpus by using the Giza++ word aligner (Och and Ney, 2003). Each word on the English side was mapped by the aligner onto a (possibly empty) set of words on the Swedish side. We used the maximal span method to in"
P06-2057,2005.mtsummit-papers.11,0,0.00744961,"ed methods of Yarowsky et al. (2001) and obtained promising results. Another recent effort (Padó and Lapata, 2005) demonstrates that deeper linguistic information, such as parse trees in the source and target language, is very beneficial for the process of FrameNet annotation transfer. A rather different method to construct bilingual semantic role annotation is the approach taken by BiFrameNet (Fung and Chen, 2004). In that work, 2.2 Transferring the Annotation We produced a Swedish-language corpus annotated with FrameNet information by applying the SRL system to the English side of Europarl (Koehn, 2005), which is a parallel corpus that is derived from the proceedings of the European Parliament. We projected the bracketing of the target words and the frame elements onto the Swedish side of the corpus by using the Giza++ word aligner (Och and Ney, 2003). Each word on the English side was mapped by the aligner onto a (possibly empty) set of words on the Swedish side. We used the maximal span method to infer the bracketing on the Swedish side, which means that the span of a projected entity was set to the range from the leftmost projected token to the rightmost. Figure 2 shows an example of this"
P06-2057,W04-0803,0,0.108376,"ments belongs to (“evokes”) the frame S TATEMENT. Two constituents that fill slots of the frame (S PEAKER and T OPIC) are annotated as well. 1 Introduction Semantic role labeling (SRL), the process of automatically identifying arguments of a predicate in a sentence and assigning them semantic roles, has received much attention during the recent years. SRL systems have been used in a number of projects in Information Extraction and Question Answering, and are believed to be applicable in other domains as well. Building SRL systems for English has been studied widely (Gildea and Jurafsky, 2002; Litkowski, 2004), inter alia. However, all these works rely on corpora that have been produced at the cost of a large effort by human annotators. For instance, the current FrameNet corpus (Baker et al., 1998) consists of 130,000 manually annotated sentences. For smaller languages such as Swedish, such corpora are not available. As usual in these cases, [both parties]SPEAKER agreed to make no further statements [on the matter]TOPIC . Figure 1: A sentence from the FrameNet example corpus. 436 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 436–443, c Sydney, July 2006. 2006 Association"
P06-2057,W04-2407,0,0.0249917,"Missing"
P06-2057,J03-1002,0,0.00162339,"rocess of FrameNet annotation transfer. A rather different method to construct bilingual semantic role annotation is the approach taken by BiFrameNet (Fung and Chen, 2004). In that work, 2.2 Transferring the Annotation We produced a Swedish-language corpus annotated with FrameNet information by applying the SRL system to the English side of Europarl (Koehn, 2005), which is a parallel corpus that is derived from the proceedings of the European Parliament. We projected the bracketing of the target words and the frame elements onto the Swedish side of the corpus by using the Giza++ word aligner (Och and Ney, 2003). Each word on the English side was mapped by the aligner onto a (possibly empty) set of words on the Swedish side. We used the maximal span method to infer the bracketing on the Swedish side, which means that the span of a projected entity was set to the range from the leftmost projected token to the rightmost. Figure 2 shows an example of this process. To make the brackets conform to the FrameNet annotation practices, we applied a small set of heuristics. The FrameNet conventions specify that linking words such as prepositions and subordinat437 [We]SPEAKER wanted to express [our perplexity a"
P06-2057,P05-1072,0,0.0776657,"nce we have no such resource to rely on, we are forced to accept that this problem introduces a certain amount of noise into the automatically annotated corpus. SUB PR DET ADV doktorn svarade på ett larmsamtal Figure 3: Example dependency parse tree. [ [ doktorn ] NG_nom ] Clause [ svarade ]VG_fin [ på] PP [ ett larmsamtal ]NG_nom Figure 4: Example shallow parse tree. 3.1 Frame Element Bracketing Methods We created two redundancy-based FE bracketing algorithms based on binary classification of chunks as starting or ending the FE. This is somewhat similar to the chunk-based system described by Pradhan et al. (2005a), which uses a segmentation strategy based on IOB2 bracketing. However, our system still exploits the dependency parse tree during classification. We first tried the conventional approach to the problem of FE bracketing: applying a parser to the sentence, and classifying each node in the parse tree as being an FE or not. We used a dependency parser since there is no constituent-based parser available for Swedish. This proved unsuccessful because the spans of the dependency subtrees frequently were incompatible with the spans defined by the FrameNet annotations. This was especially the case f"
P06-2057,P98-1013,0,0.398611,"g data for the system, we used an annotated corpus that we produced by transferring FrameNet annotation from the English side to the Swedish side in a parallel corpus. In addition, we describe two frame element bracketing algorithms that are suitable when no robust constituent parsers are available. We evaluated the system on a part of the FrameNet example corpus that we translated manually, and obtained an accuracy score of 0.75 on the classification of presegmented frame elements, and precision and recall scores of 0.67 and 0.47 for the complete task. 1.1 FrameNet: an Introduction FrameNet (Baker et al., 1998) is a lexical database that describes English words using Frame Semantics (Fillmore, 1976). In this framework, predicates (or in FrameNet terminology, target words) and their arguments are linked by means of semantic frames. A frame can intuitively be thought of as a template that defines a set of slots, frame elements (FEs), that represent parts of the conceptual structure and typically correspond to prototypical participants or properties. Figure 1 shows an example sentence annotated with FrameNet information. In this example, the target word statements belongs to (“evokes”) the frame S TATE"
P06-2057,H01-1035,0,0.0344077,"ds have been proposed to reduce the need for manual annotation. Many of these have relied on existing resources for English and a transfer method based on word alignment in a parallel corpus to automatically create an annotated corpus in a new language. Although these data are typically quite noisy, they have been used to train automatic systems. For the particular case of transfer of FrameNet annotation, there have been a few projects that have studied transfer methods and evaluated the quality of the automatically produced corpus. Johansson and Nugues (2005) applied the wordbased methods of Yarowsky et al. (2001) and obtained promising results. Another recent effort (Padó and Lapata, 2005) demonstrates that deeper linguistic information, such as parse trees in the source and target language, is very beneficial for the process of FrameNet annotation transfer. A rather different method to construct bilingual semantic role annotation is the approach taken by BiFrameNet (Fung and Chen, 2004). In that work, 2.2 Transferring the Annotation We produced a Swedish-language corpus annotated with FrameNet information by applying the SRL system to the English side of Europarl (Koehn, 2005), which is a parallel co"
P06-2057,J03-4003,0,\N,Missing
P06-2057,H05-1108,0,\N,Missing
P06-2057,C98-1013,0,\N,Missing
P06-2057,J02-3001,0,\N,Missing
P07-3009,W06-2920,0,0.050816,"Missing"
P07-3009,P02-1034,0,0.048334,"lightly higher than margin-based online algorithms. 1 Introduction Natural language consists of complex structures, such as sequences of phonemes, parse trees, and discourse or temporal graphs. Researchers in NLP have started to realize that this complexity should be reflected in their statistical models. This intuition has spurred a growing interest of related research in the machine learning community, which in turn has led to improved results in a wide range of applications in NLP, including sequence labeling (Lafferty et al., 2001; Taskar et al., 2006), constituent and dependency parsing (Collins and Duffy, 2002; McDonald et al., 2005), and logical form extraction (Zettlemoyer and Collins, 2005). Machine learning research for structured problems have generally used margin-based formulations. These include global batch methods such as Max-margin Markov Networks (M3 N) (Taskar et al., 2006) and SVM struct (Tsochantaridis et al., 2005) as well as online methods such as Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003) and the Online Passive-Aggressive Algorithm (OPA) (Crammer et al., 2006). Although the batch methods are formulated very elegantly, they do not seem to scale well to the l"
P07-3009,H05-1066,0,0.115044,"Missing"
P07-3009,W03-3017,0,0.242954,"S, n′ |I, A) → (n′ |n|S, I, A ∪ {(n, n′ )}) (n|S, I, A) → (S, I, A) (S, n|I, A) → (n|S, I, A) Conditions ¬∃n′′ (n′′ , n) ∈ A ¬∃n′′ (n′′ , n′ ) ∈ A ∃n′ (n′ , n) ∈ A Table 1: Nivre’s parser transitions where W is the initial word list; I, the current input word list; A, the graph of dependencies; and S, the stack. (n′ , n) denotes a dependency relations between n′ and n, where n′ is the head and n the dependent. 3 Experiments To compare the logistic online algorithms against other learning algorithms, we performed a set of experiments in incremental dependency parsing using the Nivre algorithm (Nivre, 2003). The algorithm is a variant of the shift–reduce algorithm and creates a projective and acyclic graph. As with the regular shift–reduce, it uses a stack S and a list of input words W , and builds the parse tree incrementally using a set of parsing actions (see Table 1). However, instead of finding constituents, it builds a set of arcs representing the graph of dependencies. It can be shown that every projective dependency graph can be produced by a sequence of parser actions, and that the worst-case number of actions is linear with respect to the number of words in the sentence. 3.1 and MIRA c"
P11-2018,D08-1083,0,0.0176718,"ed to use simple local features. In contrast, in (Johansson and Moschitti, 2010b), we showed that global structure matters: opinions interact to a large extent, and we can learn about their interactions on the opinion level by means of their interactions on the syntactic and semantic levels. It is intuitive that this should also be valid when polarities enter the Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 101–106, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics picture – this was also noted by Choi and Cardie (2008). Evaluative adjectives referring to the same evaluee may cluster together in the same clause or be dominated by a verb of categorization; opinions with opposite polarities may be conjoined through a contrastive discourse connective such as but. In this paper, we first implement two strong baselines consisting of pipelines of opinion expression segmentation and polarity labeling and compare them to the joint opinion extractor and polarity classifier by Choi and Cardie (2010). Secondly, we extend the global structure approach and add features reflecting the polarity structure of the sentence. O"
P11-2018,P10-2050,0,0.571689,"led in isolation. Breck et al. (2007) introduced a sequence model to extract opinions and we took this one step further by adding a reranker on top of the sequence labeler to take the global sentence structure into account in (Johansson and Moschitti, 2010b); later we also added holder extraction (Johansson and Moschitti, 2010a). For the task of classifiying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al., 2009). While the tasks of expression detection and polarity classification have mostly been studied in isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion expressions and assigned polarities. This is so far the only published result on joint opinion segmentation and polarity classification. However, their experiment lacked the obvious baseline: a standard pipeline consisting of an expression identifier followed by a polarity classifier. In addition, while theirs is the first end-to-end system for expression extraction with polarities, it is still a sequence labeler, which, by construction, is restricted to use simple local features. In contrast, in (Johansson and Moschitti, 2010"
P11-2018,W06-1651,0,0.72509,"The polarity takes the values P OSITIVE, N EUTRAL, N EGATIVE, and B OTH; for compatibility with Choi and Cardie (2010), we mapped B OTH to N EUTRAL. 3 The Baselines In order to test our hypothesis against strong baselines, we developed two pipeline systems. The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assigning a polarity to a given opinion expression, similar to that described by Wilson et al. (2009). The first of the two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al. (2007) and Choi et al. (2006). Sequence labeling techniques such as HMMs and CRFs are widely used for segmentation problems such as named entity recognition and noun chunk extraction. We trained a first-order labeler with the discrimi102 native training method by Collins (2002) and used common features: words, POS, lemmas in a sliding window. In addition, we used subjectivity clues extracted from the lexicon by Wilson et al. (2005). For the second baseline, we added our opinion expression reranker (Johansson and Moschitti, 2010b) on top of the expression sequence labeler. Given an expression, we use a classifier to assign"
P11-2018,W02-1001,0,0.0123493,"tems. The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assigning a polarity to a given opinion expression, similar to that described by Wilson et al. (2009). The first of the two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al. (2007) and Choi et al. (2006). Sequence labeling techniques such as HMMs and CRFs are widely used for segmentation problems such as named entity recognition and noun chunk extraction. We trained a first-order labeler with the discrimi102 native training method by Collins (2002) and used common features: words, POS, lemmas in a sliding window. In addition, we used subjectivity clues extracted from the lexicon by Wilson et al. (2005). For the second baseline, we added our opinion expression reranker (Johansson and Moschitti, 2010b) on top of the expression sequence labeler. Given an expression, we use a classifier to assign a polarity value: positive, neutral, or negative. We trained linear support vector machines to carry out this classification. The problem of polarity classification has been studied in detail by Wilson et al. (2009), who used a set of carefully dev"
P11-2018,C10-1059,1,0.852824,". Breck et al. (2007) introduced a sequence model to extract opinions and we took this one step further by adding a reranker on top of the sequence labeler to take the global sentence structure into account in (Johansson and Moschitti, 2010b); later we also added holder extraction (Johansson and Moschitti, 2010a). For the task of classifiying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al., 2009). While the tasks of expression detection and polarity classification have mostly been studied in isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion expressions and assigned polarities. This is so far the only published result on joint opinion segmentation and polarity classification. However, their experiment lacked the obvious baseline: a standard pipeline consisting of an expression identifier followed by a polarity classifier. In addition, while theirs is the first end-to-end system for expression extraction with polarities, it is still a sequence labeler, which, by construction, is restricted to use simple local features. In contrast, in (Johansson and Moschitti, 2010"
P11-2018,W10-2910,1,0.742708,"n and structured machine learning. A crucial step in the automatic analysis of opinion is to mark up the opinion expressions: the pieces of 101 text allowing us to infer that someone has a particular feeling about some topic. Then, opinions can be assigned a polarity describing whether the feeling is positive, neutral or negative. These two tasks have generally been tackled in isolation. Breck et al. (2007) introduced a sequence model to extract opinions and we took this one step further by adding a reranker on top of the sequence labeler to take the global sentence structure into account in (Johansson and Moschitti, 2010b); later we also added holder extraction (Johansson and Moschitti, 2010a). For the task of classifiying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al., 2009). While the tasks of expression detection and polarity classification have mostly been studied in isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion expressions and assigned polarities. This is so far the only published result on joint opinion segmentation and polarity classification. However, their experiment lac"
P11-2018,W04-2705,0,0.0186857,"he [appeasement]− [emboldened]+ the [terrorists]− The [appeasement]0 emboldened the [terrorists]− The [appeasement]− [emboldened]0 the [terrorists]− 4.2 Features of the Joint Model The features used by the joint opinion segmenter and polarity classifier are based on pairs of opinions: basic features extracted from each expression such as polarities and words, and relational features describing their interaction. To extract relations we used the parser by Johansson and Nugues (2008) to annotate sentences with dependencies and shallow semantics in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows the sentence the appeasement emboldened the terrorists, where appeasement and terrorists are opinions with negative polarity, with dependency syntax (above the text) and a predicate– argument structure (below). The predicate emboldened, an instance of the PropBank frame 103 Figure 1: Syntactic and shallow semantic structure. The model used the following novel features that take the polarities of the expressions into account. The examples are given with respect to the two expressions (appeasement and terrorists) in Figure 1. Base polarity classifier score. Sum of the"
P11-2018,E09-1066,1,0.886546,"Missing"
P11-2018,J05-1004,0,0.00785167,"t]− emboldened the [terrorists]− The [appeasement]− [emboldened]+ the [terrorists]− The [appeasement]0 emboldened the [terrorists]− The [appeasement]− [emboldened]0 the [terrorists]− 4.2 Features of the Joint Model The features used by the joint opinion segmenter and polarity classifier are based on pairs of opinions: basic features extracted from each expression such as polarities and words, and relational features describing their interaction. To extract relations we used the parser by Johansson and Nugues (2008) to annotate sentences with dependencies and shallow semantics in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows the sentence the appeasement emboldened the terrorists, where appeasement and terrorists are opinions with negative polarity, with dependency syntax (above the text) and a predicate– argument structure (below). The predicate emboldened, an instance of the PropBank frame 103 Figure 1: Syntactic and shallow semantic structure. The model used the following novel features that take the polarities of the expressions into account. The examples are given with respect to the two expressions (appeasement and terrorists) in Figure 1. Base pol"
P11-2018,D09-1018,0,0.0179178,"ne by 4 points in intersection F-measure and 7 points in recall. The improvements over Choi and Cardie (2010) ranged between 10 and 15 in overlap F-measure and between 17 and 24 in recall. This is not only of practical value but also confirms our linguistic intuitions that surface phenomena such as syntax and semantic roles are used in encoding the rhetorical organization of the sentence, and that we can thus extract useful information from those structures. This would also suggest that we should leave the surface and instead process the discourse structure, and this has indeed been proposed (Somasundaran et al., 2009). However, automatic discourse structure analysis is still in its infancy while syntactic and shallow semantic parsing are relatively mature. Interesting future work should be devoted to address the use of structural kernels for the proposed reranker. This would allow to better exploit syntactic and shallow semantic structures, e.g. as in (Moschitti, 2008), also applying lexical similarity and syntactic kernels (Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Moschitti, 2009). Acknowledgements The research described in this paper has received funding fro"
P11-2018,H05-1044,0,0.0311429,"nion expression, similar to that described by Wilson et al. (2009). The first of the two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al. (2007) and Choi et al. (2006). Sequence labeling techniques such as HMMs and CRFs are widely used for segmentation problems such as named entity recognition and noun chunk extraction. We trained a first-order labeler with the discrimi102 native training method by Collins (2002) and used common features: words, POS, lemmas in a sliding window. In addition, we used subjectivity clues extracted from the lexicon by Wilson et al. (2005). For the second baseline, we added our opinion expression reranker (Johansson and Moschitti, 2010b) on top of the expression sequence labeler. Given an expression, we use a classifier to assign a polarity value: positive, neutral, or negative. We trained linear support vector machines to carry out this classification. The problem of polarity classification has been studied in detail by Wilson et al. (2009), who used a set of carefully devised linguistic features. Our classifier is simpler and is based on fairly shallow features: words, POS, subjectivity clues, and bigrams inside and around th"
P11-2018,J09-3003,0,0.0506316,"be assigned a polarity describing whether the feeling is positive, neutral or negative. These two tasks have generally been tackled in isolation. Breck et al. (2007) introduced a sequence model to extract opinions and we took this one step further by adding a reranker on top of the sequence labeler to take the global sentence structure into account in (Johansson and Moschitti, 2010b); later we also added holder extraction (Johansson and Moschitti, 2010a). For the task of classifiying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al., 2009). While the tasks of expression detection and polarity classification have mostly been studied in isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion expressions and assigned polarities. This is so far the only published result on joint opinion segmentation and polarity classification. However, their experiment lacked the obvious baseline: a standard pipeline consisting of an expression identifier followed by a polarity classifier. In addition, while theirs is the first end-to-end system for expression extraction with polarities, it is still a s"
P11-2018,W08-2123,1,\N,Missing
P12-1080,H05-1091,0,0.0501161,"1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment 766 with only the CCAT subhierarchy of RCV1, which only contains 34 nodes. Other relevant work such as (McCallum et al."
P12-1080,P02-1034,0,0.660364,"we need a representation from which the dependencies between the dif760 1 We used the conversion of margin into probability provided by LIBSVM. P MCAT M11 M13 i=1..l yi αi φ(oi )φ(o) M14 M143 Figure 3: A compact representation of the hypothesis in Fig. 2. ferent nodes of the hierarchy can be learned. Since we do not know in advance which are the important dependencies and not even the scope of the interaction between the different structure subparts, we rely on automatic feature engineering via structural kernels. For this paper, we consider tree-shaped hierarchies so that tree kernels, e.g. (Collins and Duffy, 2002; Moschitti, 2006a), can be applied. In more detail, we focus on the Reuters categorization scheme. For example, Figure 1 shows a subhierarchy of the Markets (MCAT) category and its subcategories: Equity Markets (M11), Bond Markets (M12), Money Markets (M13) and Commodity Markets (M14). These also have subcategories: Interbank Markets (M131), Forex Markets (M132), Soft Commodities (M141), Metals Trading (M142) and Energy Markets (M143). As the input of our reranker, we can simply use a tree representing the hierarchy above, marking the negative assignments of the current hypothesis in the node"
P12-1080,W04-3233,0,0.0703245,"Missing"
P12-1080,P05-1050,0,0.0333768,"Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment 766 with only the CCAT subhierarchy of RCV1, wh"
P12-1080,P03-1004,0,0.0377721,"um number of labels per documents, i.e., 6, times the depth of the hierarchy, i.e., 5 (the positive classification on the leaves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descende"
P12-1080,P05-1024,0,0.0204355,"ing the compact representation the number of nodes is upper-bounded by the maximum number of labels per documents, i.e., 6, times the depth of the hierarchy, i.e., 5 (the positive classification on the leaves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is ("
P12-1080,P10-1052,0,0.0719159,"Missing"
P12-1080,E06-1015,1,0.793324,"from which the dependencies between the dif760 1 We used the conversion of margin into probability provided by LIBSVM. P MCAT M11 M13 i=1..l yi αi φ(oi )φ(o) M14 M143 Figure 3: A compact representation of the hypothesis in Fig. 2. ferent nodes of the hierarchy can be learned. Since we do not know in advance which are the important dependencies and not even the scope of the interaction between the different structure subparts, we rely on automatic feature engineering via structural kernels. For this paper, we consider tree-shaped hierarchies so that tree kernels, e.g. (Collins and Duffy, 2002; Moschitti, 2006a), can be applied. In more detail, we focus on the Reuters categorization scheme. For example, Figure 1 shows a subhierarchy of the Markets (MCAT) category and its subcategories: Equity Markets (M11), Bond Markets (M12), Money Markets (M13) and Commodity Markets (M14). These also have subcategories: Interbank Markets (M131), Forex Markets (M132), Soft Commodities (M141), Metals Trading (M142) and Energy Markets (M143). As the input of our reranker, we can simply use a tree representing the hierarchy above, marking the negative assignments of the current hypothesis in the node labels with “-”,"
P12-1080,W03-1012,0,0.369149,"he average running time again tends to be linear for natural language syntactic trees (Moschitti, 2006a). Given a target T , PTK can generate any subset of connected nodes of T , whose edges are in T . For example, Fig. 5 shows the tree fragments from the hypothesis of Fig. 2. Note that each fragment captures dependencies between different categories. 3.3 Preference reranker When training a reranker model, the task of the machine learning algorithm is to learn to select the best candidate from a given set of hypotheses. To use SVMs for training a reranker, we applied Preference Kernel Method (Shen et al., 2003). The reduction method from ranking tasks to binary classification is an active research area; see for instance (Balcan et al., 2008) and (Ailon and Mohri, 2010). Category C152 GPOL M11 .. C31 E41 GCAT .. E31 M14 G15 Total: 103 Train 837 723 604 .. 313 191 345 .. 11 96 5 10,000 Child-free Train1 Train2 370 467 357 366 309 205 .. .. 163 150 89 95 177 168 .. .. 4 7 49 47 4 1 5,000 5,000 TEST 438 380 311 .. 179 102 173 .. 6 58 0 5,000 Train 837 723 604 .. 531 223 3293 .. 32 1175 290 10,000 Child-full Train1 Train2 370 467 357 366 309 205 .. .. 274 257 121 102 1687 1506 .. .. 21 11 594 581 137 153"
P12-1080,W06-2902,0,0.0170586,"resentation the number of nodes is upper-bounded by the maximum number of labels per documents, i.e., 6, times the depth of the hierarchy, i.e., 5 (the positive classification on the leaves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they"
P12-1080,W04-3222,0,0.0344568,"es of 103 nodes. When using the compact representation the number of nodes is upper-bounded by the maximum number of labels per documents, i.e., 6, times the depth of the hierarchy, i.e., 5 (the positive classification on the leaves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly r"
P12-1080,W02-1010,0,0.0314675,"However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment 766 with only the CCAT subhierarchy of RCV1, which only contains 34 nodes. Other relevant wor"
P12-1080,N06-1037,0,0.0264594,"on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment 766 with only the CCAT subhierarchy of RCV1, which only contains 34 nodes. Other relevant work such as (McCallum et al., 1998) and (Dumais a"
P12-1080,W04-3223,0,\N,Missing
R13-1039,C04-1200,0,0.0150887,"urse relations to achieve their results, conducting a survey on a small amount of data that showed that the contrast relation was the most frequent one. However, the survey presented in Hatzivassiloglou and McKeown (1997) on the WSJ corpus showed that contrast is actually the third most important relation in the corpus. Therefore the hypothesis made by Zirn et al. (2011) may be data specific. The framework of Heerschop et al. (2011) achieved even better results than Zirn et al. (2011). The system uses deep discourse structure as well as SentiWordNet and WordNet in order to disambiguate words. Kim and Hovy (2004) define opinion as a quadruple composed by topic, holder, claim and sentiment. The authors use a Named Entity tagger to identify the potential holder of the opinion. Later Stoyanov and Cardie (2008) argue that in fine grained subjectivity analysis, topic identification is very relevant, and treat the task from the perspective of topic coreference resolution. The authors use named entities beside other topic based features to represent the topical structure of text. Johansson and Moschitti (2013) developed a joint model-based sequence labeler for finegrained opinion expression using relational"
R13-1039,P06-2063,0,0.0223551,"authors try alternative machine learning approaches with combinations of supervised and unsupervised methods for the same task. However, they do not automatically identify discourse relations, but used task-specific manual annotations. 3 Data Resources In order to test our hypothesis we used 80 Wall Street Journal documents Marcus et al. (1993) that are part both of the Penn Discourse TreeBank (PDTB) and of the Multi-Perspective QuestionAnswer (MPQA) bank. Polanyi and Zaenen (2006) investigate the usage of contextual valence shifters and discourse connectives inside a text. In the approach of Kim and Hovy (2006) the system makes use of conjunctions like “and” to infer polarities and applies a specific rule to sentences including the word “but”: if no polarity can be identified for the clause containing “but”, the polarity of the previous phrase is negated. In a more recent system, 3.1 Penn Discourse TreeBank (PDTB) 2.0 The Penn Discourse Treebank (PDTB) is a resource containing one million words from the Wall Street Journal corpus Marcus et al. (1993) annotated with discourse relations. Connectives in the PTDB are treated as discourse predicates taking two text spans as arguments (Arg), i.e. parts of"
R13-1039,P10-2050,0,0.0145931,"ral building block is opinion expression. Opinion expressions belong to two categories: Direct subjective expressions (DSEs) are explicit mentions of opinion, whereas expressive subjective elements (ESEs) signal the attitude of the speaker by the choice of words, other than these there are Objective Speech Events (OSEs). Opinions have two features: polarity and intensity, and most expressions are also associated with a holder, also called source. In this work, we only consider polarities, not intensities or holders. Polarity can be POSITIVE, NEUTRAL, NEGATIVE, and BOTH; for compatibility with Choi and Cardie (2010), we mapped BOTH to NEUTRAL. 4 Experiments Our Approach The goal of our first experiment is to observe the effect of a limited number of gold label features from PDTB. Since no previous work documented the effect of PDTB senses on the task of opinion expression mining using MPQA, we use four PDTB surface senses (described in the Subsection 3.1) as one of the features in this experiment. We then run the second experiment in order to observe 2 304 (http://crfpp.sourceforge.net/) 5.3 Since the dataset is fairly small, we perform a 5-fold cross validation over the dataset to have a rough estimatio"
R13-1039,J93-2004,0,0.0456841,"PQA) scheme Wiebe et al. (2005). We perform two different experiments sets. We first exploit gold features based on shallow discourse structure1 to classify fine-grained opinion expressions. In a second experiment, we use some syntax based features, those are found useful on a shallow discourse structure classification task, along with the named entities. Both of the experiments are found to be useful at different levels of fine-grained opinion expression mining. We use conditional random fields for this entire shallow parsing task. A set of documents from the Wall Street Journal (WSJ) corpus Marcus et al. (1993) annotated both in the Penn Discourse Treebank Prasad et al. (2008) and MPQA corpus is used. We also take advantage of the availability of several robust natural language processing tools pretrained on WSJ data. Opinion analysis deals with public opinions and trends, but subjective language is highly ambiguous. In this paper, we follow a simple data-driven technique to learn fine-grained opinions. We select an intersection set of Wall Street Journal documents that is included both in the Penn Discourse Tree Bank (PDTB) and in the Multi-Perspective Question Answering (MPQA) corpus. This is done"
R13-1039,H05-1045,0,0.0476439,"007), among others. The first approach focuses on conjoined adjectives (i.e. the adjectives which are joined with discourse connectives) within the WSJ corpus. While the second one operates at the sentence level, the third one extracts 1 By shallow discourse structure we mean the explicit discourse connective sense and its two argument spans Ghosh (2012). 302 Proceedings of Recent Advances in Natural Language Processing, pages 302–310, Hissar, Bulgaria, 7-13 September 2013. opinion phrases at the subsentence level for product features. Rich sets of linguistic features are used in the works of Choi et al. (2005), Wilson et al. (2005a), Breck et al. (2007). The first use conditional random models with information extraction patterns; the second is more focused on the classification of opinion phrases using contextual polarity; the third approach improved the performance of Wilson et al. (2005a), using conditional random fields and external knowledge sources. Zirn et al. (2011) incorporated this information using discourse relations. Zirn et al. (2011) studied a fully automatic framework for fine-grained sentiment analysis at sub-sentence level, combining multiple sentiment lexicons and neighbourhood a"
R13-1039,P05-1045,0,0.00972738,"a and the part-of-speech (PoS) tag of the token. The fourth one is the polarity value of the current token taken from a standard subjectivity lexicon maintained by Wilson et al. (2005b). The selection of baseline features is motivated by the work of Breck et al. (2007). The features are listed in the Table 1. Features used to prepare the baseline. BF1. Token (T) BF2. Lemma (L) BF3. PoS tag BF4. Polarity Values (POLV) 3 (http://ilk.uvt.nl/team/sabine/ homepage/software.html) Table 1: Baseline Feature sets opinion expression labeling. 305 Matsumoto4 . We used the Stanford Named Entity tagger by Finkel et al. (2005) to tag the named entities. This tagger is a three-class (viz. PERSON, ORGANISATION, LOCATION) tagger for English. The pre-trained models are trained both on CoNLL 2003 and MUC data, for the intersection of those class sets. NEs are included as a feature following the previous work by Stoyanov and Cardie (2008), where the authors show that information from NEs contribute to the entity relation structure in a discourse. Partial Metric Metrics P J&M 0.547 Baseline 0.628 Discourse based 0.596 NE&Syntax based 0.658 F1 0.497 0.313 0.209 0.339 Table 5: Results for identifying Polarity expressions wi"
R13-1039,I11-1120,1,0.941385,"the top-level classes are the most generic ones and include TEMPORAL, CONTINGENCY, COMPARISON and EXPANSION labels. We used these four surface senses only in our task. We define our discourse structure as shallow since it includes only the discourse connective senses and its two argument spans, excluding other types of hierarchical annotation. 3.2 the effect of named entities with the mentioned feature bundle. This set of features encoding some syntactic-level information may improve the overall classification performance like the same features facilitated a shallow discourse parsing task by Ghosh et al. (2011); in addition to the feature bundle, the named entities might reflect some information about distribution of discourse entities. 5 We perform our experiments at two different stages: (1) we first draw a baseline using basic features from the previous work and a standard sentiment lexicon by Wilson et al. (2005b), then (2) we run further experiments to improve the baseline with additional features. Our goal is to investigate possible improvements using discourse features or some other features that may encode discourse information via shallow parsing. The experiments are entirely run using cond"
R13-1039,prasad-etal-2008-penn,0,0.0136057,"ts sets. We first exploit gold features based on shallow discourse structure1 to classify fine-grained opinion expressions. In a second experiment, we use some syntax based features, those are found useful on a shallow discourse structure classification task, along with the named entities. Both of the experiments are found to be useful at different levels of fine-grained opinion expression mining. We use conditional random fields for this entire shallow parsing task. A set of documents from the Wall Street Journal (WSJ) corpus Marcus et al. (1993) annotated both in the Penn Discourse Treebank Prasad et al. (2008) and MPQA corpus is used. We also take advantage of the availability of several robust natural language processing tools pretrained on WSJ data. Opinion analysis deals with public opinions and trends, but subjective language is highly ambiguous. In this paper, we follow a simple data-driven technique to learn fine-grained opinions. We select an intersection set of Wall Street Journal documents that is included both in the Penn Discourse Tree Bank (PDTB) and in the Multi-Perspective Question Answering (MPQA) corpus. This is done in order to explore the usefulness of discourselevel structure to"
R13-1039,E99-1023,0,0.0602402,"tool because the output of CRF++ is compatible with CoNLL 2000 chunking shared task, and we view our task as an opinion expression chunking task. On the other hand, linearchain CRFs for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Also Sha and Pereira (2003) claim that, as a single model, CRFs outperform other models for shallow parsing. We use conditional random fields to classify subjective (any of direct or expressive) and objective expressions. We encode the opinion expression spans by means of the IOB2 scheme Sang et al. (1999). In order to represent MPQA opinion expressions with IOB2 tags, we remove the expressions where the expression spans are overlapping expressions (i.e. an opinion expression span can be overlapped by another opinion expression span), though overlapping expressions are rare in MPQA [ Johansson and Moschitti (2013)]. Multi-Perspective Question Answering (MPQA) We use the version 2.0 of the MPQA corpus, whose central building block is opinion expression. Opinion expressions belong to two categories: Direct subjective expressions (DSEs) are explicit mentions of opinion, whereas expressive subjecti"
R13-1039,P97-1023,0,0.716719,"ogs, online forums, Facebook, Twitter and other social media channels has given an opportunity of unprecedented reach to publicly sharing thoughts on events, products and services. However, there are some open issues related to this research area, commonly known as Opinion Mining, which can be summarized as follows: (1) Opinions are potentially ambiguous, and (2) Contextual interpretation of polarity is hard to achieve. Subsidiary important problem is the non-availability of large corpora with good annotation quality. Related Work Fine-grained sentiment analysis methods have been developed by Hatzivassiloglou and McKeown (1997), Hu and Liu (2004) and Popescu and Etzioni (2007), among others. The first approach focuses on conjoined adjectives (i.e. the adjectives which are joined with discourse connectives) within the WSJ corpus. While the second one operates at the sentence level, the third one extracts 1 By shallow discourse structure we mean the explicit discourse connective sense and its two argument spans Ghosh (2012). 302 Proceedings of Recent Advances in Natural Language Processing, pages 302–310, Hissar, Bulgaria, 7-13 September 2013. opinion phrases at the subsentence level for product features. Rich sets of"
R13-1039,N03-1028,0,0.100759,"CRF++ tool 2 for sequence labeling classification by Lafferty et al. (2001), with second-order Markov dependency between tags. Beside the individual specification of a feature in the feature description template, the features in various combinations are also represented. We used this tool because the output of CRF++ is compatible with CoNLL 2000 chunking shared task, and we view our task as an opinion expression chunking task. On the other hand, linearchain CRFs for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Also Sha and Pereira (2003) claim that, as a single model, CRFs outperform other models for shallow parsing. We use conditional random fields to classify subjective (any of direct or expressive) and objective expressions. We encode the opinion expression spans by means of the IOB2 scheme Sang et al. (1999). In order to represent MPQA opinion expressions with IOB2 tags, we remove the expressions where the expression spans are overlapping expressions (i.e. an opinion expression span can be overlapped by another opinion expression span), though overlapping expressions are rare in MPQA [ Johansson and Moschitti (2013)]. Mul"
R13-1039,P09-1026,0,0.0159257,"ald (2011) combine fully and partially supervised structured conditional models for a joint classification of the polarity of whole reviews and review sentences. The impact of discourse relations for sentiment analysis is investigated in Asher et al. (2009). The authors conduct a manual study in which they represent opinions in text as shallow semantic feature structures. These are combined with overall opinion using hand-written rules based on manually annotated discourse relations. An interdependent classification scenario to determine polarity as well as discourse relations is presented in Somasundaran and Wiebe (2009). In their approach, text is modeled as opinion graphs including discourse information. In Somasundaran and Wiebe (2009) the authors try alternative machine learning approaches with combinations of supervised and unsupervised methods for the same task. However, they do not automatically identify discourse relations, but used task-specific manual annotations. 3 Data Resources In order to test our hypothesis we used 80 Wall Street Journal documents Marcus et al. (1993) that are part both of the Penn Discourse TreeBank (PDTB) and of the Multi-Perspective QuestionAnswer (MPQA) bank. Polanyi and Za"
R13-1039,C08-1103,0,0.0871378,"vassiloglou and McKeown (1997) on the WSJ corpus showed that contrast is actually the third most important relation in the corpus. Therefore the hypothesis made by Zirn et al. (2011) may be data specific. The framework of Heerschop et al. (2011) achieved even better results than Zirn et al. (2011). The system uses deep discourse structure as well as SentiWordNet and WordNet in order to disambiguate words. Kim and Hovy (2004) define opinion as a quadruple composed by topic, holder, claim and sentiment. The authors use a Named Entity tagger to identify the potential holder of the opinion. Later Stoyanov and Cardie (2008) argue that in fine grained subjectivity analysis, topic identification is very relevant, and treat the task from the perspective of topic coreference resolution. The authors use named entities beside other topic based features to represent the topical structure of text. Johansson and Moschitti (2013) developed a joint model-based sequence labeler for finegrained opinion expression using relational features except discourse-level features, beside a set of classifier to determine opinion holder and also a multi-class classifier that assigns polarity to a given opinion expression. These classifi"
R13-1039,J13-3002,1,0.897822,". The system uses deep discourse structure as well as SentiWordNet and WordNet in order to disambiguate words. Kim and Hovy (2004) define opinion as a quadruple composed by topic, holder, claim and sentiment. The authors use a Named Entity tagger to identify the potential holder of the opinion. Later Stoyanov and Cardie (2008) argue that in fine grained subjectivity analysis, topic identification is very relevant, and treat the task from the perspective of topic coreference resolution. The authors use named entities beside other topic based features to represent the topical structure of text. Johansson and Moschitti (2013) developed a joint model-based sequence labeler for finegrained opinion expression using relational features except discourse-level features, beside a set of classifier to determine opinion holder and also a multi-class classifier that assigns polarity to a given opinion expression. These classifiers were further used to generate the hypothesis sets for a re-ranking system that further improved the performance of the classification. T¨ackstr¨om and McDonald (2011) combine fully and partially supervised structured conditional models for a joint classification of the polarity of whole reviews an"
R13-1039,P11-2100,0,0.0255746,"Missing"
R13-1039,H05-1044,0,0.193979,"The first approach focuses on conjoined adjectives (i.e. the adjectives which are joined with discourse connectives) within the WSJ corpus. While the second one operates at the sentence level, the third one extracts 1 By shallow discourse structure we mean the explicit discourse connective sense and its two argument spans Ghosh (2012). 302 Proceedings of Recent Advances in Natural Language Processing, pages 302–310, Hissar, Bulgaria, 7-13 September 2013. opinion phrases at the subsentence level for product features. Rich sets of linguistic features are used in the works of Choi et al. (2005), Wilson et al. (2005a), Breck et al. (2007). The first use conditional random models with information extraction patterns; the second is more focused on the classification of opinion phrases using contextual polarity; the third approach improved the performance of Wilson et al. (2005a), using conditional random fields and external knowledge sources. Zirn et al. (2011) incorporated this information using discourse relations. Zirn et al. (2011) studied a fully automatic framework for fine-grained sentiment analysis at sub-sentence level, combining multiple sentiment lexicons and neighbourhood as well as discourse r"
R13-1039,I11-1038,0,0.0139238,"gs of Recent Advances in Natural Language Processing, pages 302–310, Hissar, Bulgaria, 7-13 September 2013. opinion phrases at the subsentence level for product features. Rich sets of linguistic features are used in the works of Choi et al. (2005), Wilson et al. (2005a), Breck et al. (2007). The first use conditional random models with information extraction patterns; the second is more focused on the classification of opinion phrases using contextual polarity; the third approach improved the performance of Wilson et al. (2005a), using conditional random fields and external knowledge sources. Zirn et al. (2011) incorporated this information using discourse relations. Zirn et al. (2011) studied a fully automatic framework for fine-grained sentiment analysis at sub-sentence level, combining multiple sentiment lexicons and neighbourhood as well as discourse relations. They used Markov logic to integrate polarity scores from different sentiment lexicons with information about relations between neighbouring segments, and evaluate the approach on product reviews. The authors used only contrast and no contrast discourse relations to achieve their results, conducting a survey on a small amount of data that"
R13-1039,H05-2017,0,\N,Missing
R13-1039,H05-1043,0,\N,Missing
R15-1029,P12-1092,0,0.677659,"e since fairly large corpora are needed to induce data-driven representations of a high quality, while corpora with hand-annotated sense identifiers are small and scarce. Instead, there have been several attempts to use unsupervised methods that create vectors representing the senses of ambiguous words, most of them based on some variant of the idea that was first proposed by Sch¨utze (1998): that the different senses of a word can be discovered by applying a clustering algorithm to the set of contexts where it has appeared. Variations on this idea have turned up in a number of recent papers (Huang et al., 2012; Moen et al., 2013; Neelakantan et al., 2014; K˚ageb¨ack et al., 2015). However, unsupervised Introduction Word meaning representations derived from corpora have recently seen much attention in natural language processing (NLP), most importantly because they can be used very effectively to abstract over the word level in lexicalized NLP systems (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014; Guo et al., 2014; Sienˇcnik, 2015). These representations are derived from corpus statistics, building on the distributional hypothesis that the meaning of a word is refl"
R15-1029,P14-2131,0,0.0171233,"word can be discovered by applying a clustering algorithm to the set of contexts where it has appeared. Variations on this idea have turned up in a number of recent papers (Huang et al., 2012; Moen et al., 2013; Neelakantan et al., 2014; K˚ageb¨ack et al., 2015). However, unsupervised Introduction Word meaning representations derived from corpora have recently seen much attention in natural language processing (NLP), most importantly because they can be used very effectively to abstract over the word level in lexicalized NLP systems (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014; Guo et al., 2014; Sienˇcnik, 2015). These representations are derived from corpus statistics, building on the distributional hypothesis that the meaning of a word is reflected in statistical distributions of the contexts 208 Proceedings of Recent Advances in Natural Language Processing, pages 208–215, Hissar, Bulgaria, Sep 7–9 2015. could be fit to a training corpus by maximizing the likelihood of all the contexts in the corpus, but due to the normalization factors Z(c) – which are computed by summing over the whole vocabulary – this is computationally inefficient, leading to a number of app"
R15-1029,W15-1504,1,0.885567,"Missing"
R15-1029,C12-1089,0,0.0252631,"presentations has generated much interest recently (Lazaridou et al., 2014; Socher et al., 2014). The work in this area that is most similar to ours is that by Hill and Korhonen (2014) and : they extend the context representation of the skip-gram model with features representing the external information like we do, although they do not take word senses into account. Parallel corpora have been used in a number of research projects in order to derive crosslingual word representations; this is different from our goal, which is to use them to help the monolingual model form better sense clusters. Klementiev et al. (2012) presented a neural multi-task learning model that used bilingual cooccurrence data as a way to connect the models in two languages, and Utt and Pad´o (2014) described a syntactically informed context-counting method. Faruqui and Dyer (2014) presented a method that combine two monolingual vector spaces into a multilingual one by Canonical Correlation Analysis. In addition to vector-space models, bilingual and multilingual We evaluated the multi-sense vector models trained with translation-enriched contexts using a number of different benchmarks: word similarity tests, a contextual similarity t"
R15-1029,2005.mtsummit-papers.11,0,0.0949112,"lly grounded in perception and a sample text from a language alone does not carry all information about the concept behind the word (Andrews et al., 2009).2 The perceptual information which has been claimed to improve these models are usually multi-modal data, for instance images as visual context of word usage in a language. In this work, we will instead enrich the training context with another type of supplementary text – the translation of the English text into Swedish – in order to improve the final word sense discrimination model. In our method, we use a parallel corpus such as Europarl (Koehn, 2005), which provides sentenceby-sentence translations. Then by aligning words in each sentence we will add corresponding list of words in enhancing language into the list of words in skip-gram context window. Figure 1 illustrates why we expect this to be useful for forming better word sense clusters. In the figure, the first occurrence of the word plant, meaning an industrial or power plant, is translated by the Swedish word anl¨aggning; the second example means a botanical plant and is translated as planta. This shows clearly that the external context in the form of a translation can be useful fo"
R15-1029,P91-1017,0,0.604066,"n in absence of context we can take most related senses as most obvious choice of sense • δi is the similarity margin between vi ∈ V 212 corpora have been used to derive a number of nongeometric corpus-based representations, such as Brown clusters (T¨ackstr¨om et al., 2012) and topic models (Vuli´c et al., 2015). and the nearest neighbour v1 : δi = sim(v1 , vanalogy ) − sim(vi , vanalogy ) • The score of vanalogy : score = P10 2 i=1 δi 2 δ10 Finally, the use of word translations as a way to distantly supervise word sense disambiguation and discrimination systems is an idea that goes far back (Dagan et al., 1991; Dyvik, 2004) and has reappeared many times. This intuition was behind a number of SemEval cross-lingual word sense disambiguation and lexical substitution tasks (Lefever and Hoste, 2010; Mihalcea et al., 2010). × sim(v1 , vanalogy ) Higher score in this formula indicates that v1 , the most similar vector to vanalogy , has a significant similarity to vanalogy compering to other possible neighbour vectors. By taking the best vanalogy from all possible vanalogy , we automatically pick 3 sense vectors for analogy test. Table 3 shows the results of the evaluation on the Google analogy test set (M"
R15-1029,P08-1068,0,0.0223209,"(1998): that the different senses of a word can be discovered by applying a clustering algorithm to the set of contexts where it has appeared. Variations on this idea have turned up in a number of recent papers (Huang et al., 2012; Moen et al., 2013; Neelakantan et al., 2014; K˚ageb¨ack et al., 2015). However, unsupervised Introduction Word meaning representations derived from corpora have recently seen much attention in natural language processing (NLP), most importantly because they can be used very effectively to abstract over the word level in lexicalized NLP systems (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014; Guo et al., 2014; Sienˇcnik, 2015). These representations are derived from corpus statistics, building on the distributional hypothesis that the meaning of a word is reflected in statistical distributions of the contexts 208 Proceedings of Recent Advances in Natural Language Processing, pages 208–215, Hissar, Bulgaria, Sep 7–9 2015. could be fit to a training corpus by maximizing the likelihood of all the contexts in the corpus, but due to the normalization factors Z(c) – which are computed by summing over the whole vocabulary – this is computational"
R15-1029,N13-1073,0,0.0408088,"In order to facilitate and simplify the training process, we isolated the word alignment process from the rest of the training. In this isolated process in addition to the word alignment process which takes two parallel corpora and suggests one-tomany word alignments per sentence 3 , we produce an enriched corpus by annotating the source corpus with words from the target corpus. In order to get better results from word alignments, we applied a part-of-speech tagger on the Swedish and English words before running the aligner. Then we by taking the union of two word alignments with fast align (Dyer et al., 2013) in both forward and reverse setups, we produced oneto-many mappings. We then read sentences from both corpora in parallel with their word mappings and generated the annotated corpus, which we refer to as the enriched or augmented corpus. The enriched corpus simply is the annotated source corpus which each word has its list of aligned words from target corpus. During the training process, the Enriched MultiSense Skip-Gram Model will parse the annotated tokens, and add the enriched context to the skipgram contexts as we describe in next section. One of the fundamental criticisms against distrib"
R15-1029,P14-1132,0,0.0247086,"t al. (2014), but we imagine that other models (for instance Huang el al., 2012) could be extended in a similar fashion. The model can integrate any kind of languageexternal signal as long as it can be represented as a contextual feature taken from a finite vocabulary. In this work, we enriched the context using word translations taken from the Europarl corpus (Koehn, 2005). Semantic 0.17 0.32 Table 3: Evaluation on the Google analogy test set. 5 Conclusions Related Work The idea of integrating different modalities into corpus-based vector representations has generated much interest recently (Lazaridou et al., 2014; Socher et al., 2014). The work in this area that is most similar to ours is that by Hill and Korhonen (2014) and : they extend the context representation of the skip-gram model with features representing the external information like we do, although they do not take word senses into account. Parallel corpora have been used in a number of research projects in order to derive crosslingual word representations; this is different from our goal, which is to use them to help the monolingual model form better sense clusters. Klementiev et al. (2012) presented a neural multi-task learning model that"
R15-1029,E14-1049,0,0.0242376,"gram model with features representing the external information like we do, although they do not take word senses into account. Parallel corpora have been used in a number of research projects in order to derive crosslingual word representations; this is different from our goal, which is to use them to help the monolingual model form better sense clusters. Klementiev et al. (2012) presented a neural multi-task learning model that used bilingual cooccurrence data as a way to connect the models in two languages, and Utt and Pad´o (2014) described a syntactically informed context-counting method. Faruqui and Dyer (2014) presented a method that combine two monolingual vector spaces into a multilingual one by Canonical Correlation Analysis. In addition to vector-space models, bilingual and multilingual We evaluated the multi-sense vector models trained with translation-enriched contexts using a number of different benchmarks: word similarity tests, a contextual similarity test, and a word analogy test. In every experiment we tried, the enriched model outperformed the non-enriched baseline. It seems straightforward to extend our work to a setting where other types of features are used, and we would like to expl"
R15-1029,S10-1003,0,0.0333019,"nongeometric corpus-based representations, such as Brown clusters (T¨ackstr¨om et al., 2012) and topic models (Vuli´c et al., 2015). and the nearest neighbour v1 : δi = sim(v1 , vanalogy ) − sim(vi , vanalogy ) • The score of vanalogy : score = P10 2 i=1 δi 2 δ10 Finally, the use of word translations as a way to distantly supervise word sense disambiguation and discrimination systems is an idea that goes far back (Dagan et al., 1991; Dyvik, 2004) and has reappeared many times. This intuition was behind a number of SemEval cross-lingual word sense disambiguation and lexical substitution tasks (Lefever and Hoste, 2010; Mihalcea et al., 2010). × sim(v1 , vanalogy ) Higher score in this formula indicates that v1 , the most similar vector to vanalogy , has a significant similarity to vanalogy compering to other possible neighbour vectors. By taking the best vanalogy from all possible vanalogy , we automatically pick 3 sense vectors for analogy test. Table 3 shows the results of the evaluation on the Google analogy test set (Mikolov et al., 2013c). For the third time, the translationenriched model outperforms the MSSG baseline in all tests. Model MSSG EMSSG Total 0.13 0.25 Syntactic 0.04 0.09 6 We have present"
R15-1029,D14-1012,0,0.017315,"d by applying a clustering algorithm to the set of contexts where it has appeared. Variations on this idea have turned up in a number of recent papers (Huang et al., 2012; Moen et al., 2013; Neelakantan et al., 2014; K˚ageb¨ack et al., 2015). However, unsupervised Introduction Word meaning representations derived from corpora have recently seen much attention in natural language processing (NLP), most importantly because they can be used very effectively to abstract over the word level in lexicalized NLP systems (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014; Guo et al., 2014; Sienˇcnik, 2015). These representations are derived from corpus statistics, building on the distributional hypothesis that the meaning of a word is reflected in statistical distributions of the contexts 208 Proceedings of Recent Advances in Natural Language Processing, pages 208–215, Hissar, Bulgaria, Sep 7–9 2015. could be fit to a training corpus by maximizing the likelihood of all the contexts in the corpus, but due to the normalization factors Z(c) – which are computed by summing over the whole vocabulary – this is computationally inefficient, leading to a number of approximations. Mikol"
R15-1029,S10-1002,0,0.02152,"representations, such as Brown clusters (T¨ackstr¨om et al., 2012) and topic models (Vuli´c et al., 2015). and the nearest neighbour v1 : δi = sim(v1 , vanalogy ) − sim(vi , vanalogy ) • The score of vanalogy : score = P10 2 i=1 δi 2 δ10 Finally, the use of word translations as a way to distantly supervise word sense disambiguation and discrimination systems is an idea that goes far back (Dagan et al., 1991; Dyvik, 2004) and has reappeared many times. This intuition was behind a number of SemEval cross-lingual word sense disambiguation and lexical substitution tasks (Lefever and Hoste, 2010; Mihalcea et al., 2010). × sim(v1 , vanalogy ) Higher score in this formula indicates that v1 , the most similar vector to vanalogy , has a significant similarity to vanalogy compering to other possible neighbour vectors. By taking the best vanalogy from all possible vanalogy , we automatically pick 3 sense vectors for analogy test. Table 3 shows the results of the evaluation on the Google analogy test set (Mikolov et al., 2013c). For the third time, the translationenriched model outperforms the MSSG baseline in all tests. Model MSSG EMSSG Total 0.13 0.25 Syntactic 0.04 0.09 6 We have presented a general technique c"
R15-1029,J98-1004,0,0.663774,"Missing"
R15-1029,W15-1830,0,0.0256922,"Missing"
R15-1029,N13-1090,0,0.559836,"enburg mmehdi.g@gmail.com Abstract in which it appears (Harris, 1954). This intuition can be implemented in a number of ways in practice; in this work, we focus on models that represent word meaning as a point in a metric space (Widdows, 2005; Sahlgren, 2006; Turney and Pantel, 2010; Clark, 2015). In particular, one member of this family that has been particularly influential recently is the skip-gram learning algorithm (Mikolov et al., 2013a), which is derived from the log-bilinear language model by Mnih and Hinton (2007). The main reasons for its popularity are its computational efficiency (Mikolov et al., 2013b), its high performance in several evaluations, and the availability of an implementation in the form of the easily usable word2vec package. Vector-space models derived from corpora are an effective way to learn a representation of word meaning directly from data, and these models have many uses in practical applications. A number of unsupervised approaches have been proposed to automatically learn representations of word senses directly from corpora, but since these methods use no information but the words themselves, they sometimes miss distinctions that could be possible to make if more in"
R15-1029,Q14-1017,0,0.0119527,"gine that other models (for instance Huang el al., 2012) could be extended in a similar fashion. The model can integrate any kind of languageexternal signal as long as it can be represented as a contextual feature taken from a finite vocabulary. In this work, we enriched the context using word translations taken from the Europarl corpus (Koehn, 2005). Semantic 0.17 0.32 Table 3: Evaluation on the Google analogy test set. 5 Conclusions Related Work The idea of integrating different modalities into corpus-based vector representations has generated much interest recently (Lazaridou et al., 2014; Socher et al., 2014). The work in this area that is most similar to ours is that by Hill and Korhonen (2014) and : they extend the context representation of the skip-gram model with features representing the external information like we do, although they do not take word senses into account. Parallel corpora have been used in a number of research projects in order to derive crosslingual word representations; this is different from our goal, which is to use them to help the monolingual model form better sense clusters. Klementiev et al. (2012) presented a neural multi-task learning model that used bilingual cooccu"
R15-1029,N04-1043,0,0.0601957,"proposed by Sch¨utze (1998): that the different senses of a word can be discovered by applying a clustering algorithm to the set of contexts where it has appeared. Variations on this idea have turned up in a number of recent papers (Huang et al., 2012; Moen et al., 2013; Neelakantan et al., 2014; K˚ageb¨ack et al., 2015). However, unsupervised Introduction Word meaning representations derived from corpora have recently seen much attention in natural language processing (NLP), most importantly because they can be used very effectively to abstract over the word level in lexicalized NLP systems (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014; Guo et al., 2014; Sienˇcnik, 2015). These representations are derived from corpus statistics, building on the distributional hypothesis that the meaning of a word is reflected in statistical distributions of the contexts 208 Proceedings of Recent Advances in Natural Language Processing, pages 208–215, Hissar, Bulgaria, Sep 7–9 2015. could be fit to a training corpus by maximizing the likelihood of all the contexts in the corpus, but due to the normalization factors Z(c) – which are computed by summing over the whole vocabulary – thi"
R15-1029,N12-1052,0,0.0336972,"Missing"
R15-1029,P09-1113,0,0.0199514,"skipgram model (Neelakantan et al., 2014) by introducing external signals, which are implemented as additional context features during training. In particular, we use a parallel corpus, where the foreign-language words work as a source of external information that helps the algorithm form more distinct clusters. For instance, the fish sense of bass can be clearly distinguished from the musical senses if we have access to a Swedish translation: the fish is called havsabborre, while most of the musical senses would be translated as bas. Our approach can be seen as a form of distant supervision (Mintz et al., 2009), in contrast to the fully unsupervised approaches mentioned above. We evaluated the context-enriched model on a collection of word similarity benchmarks and analogy tests, including the contextual word similarity set used in previous work on learning representations of different senses (Huang et al., 2012), and we saw large improvements when comparing to a baseline without access to the enriched context. 2 Background: the Skip-gram Model and its Multi-sense Extension In the skip-gram model (Mikolov et al., 2013a), a target word w and a context feature c are represented using vectors from two"
R15-1029,P10-1040,0,0.0591661,"ifferent senses of a word can be discovered by applying a clustering algorithm to the set of contexts where it has appeared. Variations on this idea have turned up in a number of recent papers (Huang et al., 2012; Moen et al., 2013; Neelakantan et al., 2014; K˚ageb¨ack et al., 2015). However, unsupervised Introduction Word meaning representations derived from corpora have recently seen much attention in natural language processing (NLP), most importantly because they can be used very effectively to abstract over the word level in lexicalized NLP systems (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014; Guo et al., 2014; Sienˇcnik, 2015). These representations are derived from corpus statistics, building on the distributional hypothesis that the meaning of a word is reflected in statistical distributions of the contexts 208 Proceedings of Recent Advances in Natural Language Processing, pages 208–215, Hissar, Bulgaria, Sep 7–9 2015. could be fit to a training corpus by maximizing the likelihood of all the contexts in the corpus, but due to the normalization factors Z(c) – which are computed by summing over the whole vocabulary – this is computationally inefficient, leadi"
R15-1029,W13-3210,0,0.0423346,"Missing"
R15-1029,Q14-1020,0,0.0474112,"Missing"
R15-1029,D14-1113,0,0.0634782,"to induce data-driven representations of a high quality, while corpora with hand-annotated sense identifiers are small and scarce. Instead, there have been several attempts to use unsupervised methods that create vectors representing the senses of ambiguous words, most of them based on some variant of the idea that was first proposed by Sch¨utze (1998): that the different senses of a word can be discovered by applying a clustering algorithm to the set of contexts where it has appeared. Variations on this idea have turned up in a number of recent papers (Huang et al., 2012; Moen et al., 2013; Neelakantan et al., 2014; K˚ageb¨ack et al., 2015). However, unsupervised Introduction Word meaning representations derived from corpora have recently seen much attention in natural language processing (NLP), most importantly because they can be used very effectively to abstract over the word level in lexicalized NLP systems (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Bansal et al., 2014; Guo et al., 2014; Sienˇcnik, 2015). These representations are derived from corpus statistics, building on the distributional hypothesis that the meaning of a word is reflected in statistical distributions of the con"
R15-1029,N10-1013,0,0.0891553,"clude a comparison for completeness. However, The evaluation procedures for word sense models in all of these test sets are identical: 211 despite the absence of context, human judges estimate their similarity based on their own understanding of senses of those words. Similar to passive sense selection in humans4 , we consider each word as context for the other word to select the best sense. With a twist, instead of using context vectors to predict the sense of the other one, we basically choose the most similar vectors pairs as desired vectors. This is equivalent to what Reisinger and Mooney (2010) term the MaxSim score. To understand why we use this procedure, consider two very different words: in this case, we expect that all of their senses should be very different. Considering two words that the evaluators considered to be similar, it is likely that this does not apply to all of the senses, but only a specific pair. This motivates why we take the highest similarity of senses, and we think that this procedure is more meaningful than the AvgSim score used by (Reisinger and Mooney, 2010). The English-Swedish Europarl’s vocabulary covers 758 of word pairs in SimLex999 and 163 pairs in W"
R15-1029,J15-4004,0,\N,Missing
R15-1029,W09-2412,0,\N,Missing
R15-1029,D14-1032,0,\N,Missing
R15-1061,S13-2049,0,0.112066,"|Vj ∩Li | |Li | The overall F -measure is defined as the weighted average of individual F -measures: F = X i |L | P i max F (Vj , Li ) k |Lk |j • B-cubed F-measure (Bagga and Baldwin, 1998), which computes individual precision and recall measures for every item occurring in one of the lists, and then averaging all precision and recall values. The F -measure is the harmonic mean of the averaged precision and recall. Quantitative Evaluation Most systems that automatically discover word senses have been evaluated either by clustering the instances in an annotated corpus (Manandhar et al., 2010; Jurgens and Klapaftis, 2013), or by measuring the effect of the senses representations in a downstream task such as contextual word similar• V-measure (Rosenberg and Hirschberg, 2007), the harmonic mean of the homogeneity and the completeness, two entropy-based metrics. The homogeneity is defined as the 469 relative reduction of entropy in V when adding the information about L: h(V, L) = 1 − N 10 20 40 80 160 H(V |L) H(V ) Pu-F N-14 ours 9.4 10.7 9.5 10.8 9.0 9.9 7.8 8.9 7.4 8.2 Conversely, the completeness is defined c(V, L) = 1 − N-14 8.9 6.7 5.1 4.3 3.9 V ours 10.6 8.9 7.2 5.6 4.7 (a) Nouns. H(L|V ) . H(L) N 10 20 40"
R15-1061,S10-1011,0,0.110785,"Missing"
R15-1061,D14-1113,0,0.34315,"ences of any given word in a corpus into a fixed number K of clusters which represent different word usages (rather than word senses). Each word’s is thus assigned multiple prototypes or embeddings. Huang et al. (2012) introduced a neural language model that leverages sentence-level and document-level context to generate word embeddings. Using Reisinger and Mooney (2010)’s approach to generate multiple embeddings per word via clusters and training on a corpus whose words have been substituted by its associated cluster’s centroid, the neural model is able to learn multiple embeddings per word. Neelakantan et al. (2014) tried to expand the Skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b) to produce word sense embeddings using the clustering approach of Reisinger and Mooney (2010) and Huang et al. (2012). Notably, Skip-gram’s architecture allows the model to, given a word and its context, select and train a word sense embedding jointly. The authors Introduction Distributed representations of words have helped obtain better language models (Bengio et al., 2003) and improve the performance of many natural language processing applications such as named entity recognition, chunking, paraphrasing, or"
R15-1061,D14-1110,0,0.0760856,"Missing"
R15-1061,N10-1013,0,0.0630461,"d is obtained, not acknowledging the fact that some words have multiple meanings. This has a negative effect on the individual word representations and the language model as a whole. In this paper we present a simple model that enables recent techniques for building word vectors to represent distinct senses of polysemic words. In our assessment of this model we show that it is able to effectively discriminate between words’ senses and to do so in a computationally efficient manner. 1 2 Related Work One of the first steps towards obtaining word sense embeddings was that by Reisinger and Mooney (2010). The authors propose to cluster occurrences of any given word in a corpus into a fixed number K of clusters which represent different word usages (rather than word senses). Each word’s is thus assigned multiple prototypes or embeddings. Huang et al. (2012) introduced a neural language model that leverages sentence-level and document-level context to generate word embeddings. Using Reisinger and Mooney (2010)’s approach to generate multiple embeddings per word via clusters and training on a corpus whose words have been substituted by its associated cluster’s centroid, the neural model is able"
R15-1061,D07-1043,0,0.0874521,"Missing"
R15-1061,P10-1040,0,0.0778104,"ram model (Mikolov et al., 2013a; Mikolov et al., 2013b) to produce word sense embeddings using the clustering approach of Reisinger and Mooney (2010) and Huang et al. (2012). Notably, Skip-gram’s architecture allows the model to, given a word and its context, select and train a word sense embedding jointly. The authors Introduction Distributed representations of words have helped obtain better language models (Bengio et al., 2003) and improve the performance of many natural language processing applications such as named entity recognition, chunking, paraphrasing, or sentiment classification (Turian et al., 2010; Socher et al., 2011; Glorot et al., 2011). Recently, the Skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b) was proposed, which is able to produce high-quality representations from large collections of text in an efficient manner. Despite the achievements of distributed representations, polysemy or homonymy are usually disregarded even when word semantics may have a large influence on the models. This results in several distinct senses of one same word sharing a representation, and possibly influencing the representations of words related to those distinct senses under the premis"
R15-1061,P94-1019,0,0.515031,"s sense-annotated corpora as well as word similarity test sets, so our evaluation is instead based on comparing the discovered word senses to those listed in the SALDO lexicon. We selected the 100 most frequent two-sense nouns, verbs, and adjectives and used them as the test set. To evaluate the senses discovered for a lemma, we generated two sets of word lists: one derived from the lexicon, and one from the vector space. For each sense si listed in the lexicon, we created a list Li by selecting the N senses (for other words) most similar to si according to the graphbased similarity metric by Wu and Palmer (1994). Conversely, for each sense vector vj in our vectorbased model, a list Vj was built by selecting the N vectors most similar to vj , using the cosine similarity. We finally mapped the senses back to their corresponding lemmas, so that the two sets L = {Li } and V = {Vj } of word lists could be compared. These lists were then evaluated using standard clustering evaluation metrics. We used three different metrics: fåne(jerk) idiot dummer(fool) åsna-2 lama(llama) åsna-1 mulåsna(mule) fårskalle(muttonhead) får(sheep) tjur(bull) kamel(camel) Figure 1: 2D projections of the two senses of a˚ sna (’do"
R15-1061,P12-1092,0,\N,Missing
S07-1048,W04-0814,0,0.0492404,"Missing"
S07-1048,W06-2920,0,0.0058074,"d a post-processing step to convert constituent trees into labeled dependency trees that were then used as input to a semantic role labeler. Pradhan et al. (2005) used a rule-based dependency parser, but the results were significantly worse than when using a constituent parser. This paper describes a system for frame-semantic structure extraction that is based on a dependency parser. The next section presents the dependency grammar that we rely on. We then give the details on the frame detection and disambiguation, the The last few years have seen an increasing interest in dependency parsing (Buchholz and Marsi, 2006) with significant improvements of the state of the art, and dependency treebanks are now available for a wide range of languages. The parsing algorithms are comparatively easy to implement and efficient: some of the algorithms parse sentences in linear time (Yamada and Matsumoto, 2003; Nivre et al., 2006). In the semantic structure extraction system, we used the Stanford part-of-speech tagger (Toutanova et al., 2003) to tag the training and test sentences and MaltParser, a statistical dependency parser (Nivre et al., 2006), to parse them. We trained the parser on the Penn Treebank (Marcus et a"
S07-1048,W07-2416,1,0.810098,"ges. The parsing algorithms are comparatively easy to implement and efficient: some of the algorithms parse sentences in linear time (Yamada and Matsumoto, 2003; Nivre et al., 2006). In the semantic structure extraction system, we used the Stanford part-of-speech tagger (Toutanova et al., 2003) to tag the training and test sentences and MaltParser, a statistical dependency parser (Nivre et al., 2006), to parse them. We trained the parser on the Penn Treebank (Marcus et al., 1993). The dependency trees used to train the parser were created from the constituent trees using a conversion program (Johansson and Nugues, 2007)1 . The converter handles most of the secondary edges in the Treebank and encodes those edges as (generally) nonprojective dependency arcs. Such information is available in the Penn Treebank in the form of empty categories and secondary edges, it is however not available in the output of traditional constituent parsers, although there have been some attempts to apply a post-processing step to predict it, see Ahn et al. (2004), inter alia. Figures 1 and 2 show a constituent tree from the Treebank and its corresponding dependency tree. Note that the secondary edge from the wh-trace to Why is con"
S07-1048,P03-1004,0,0.0122447,"motion verbs (which often exhibit complex patterns of polysemy): 137 lemmas could be added to the S ELF _ MOTION frame. Examples of frames with frequent errors are L EADERSHIP, which includes many insects (probably because the most frequent sense of queen in SemCor is the queen bee), and F OOD , which included many chemical substances as well as inedible plants and animals. 3.2 Frame Element Extraction None FE Argument classification Supp Asp Cop Exist Null 3.3 Named Entity Recognition In addition to the frame-semantic information, the SemEval task also scores named entities. We used YamCha (Kudo and Matsumoto, 2003) to detect named entities, and we trained it on the SemEval full-text training sets. Apart from the word and part of speech, we used suffixes up to length 5 as features. We think that results could be improved further by using an external NE tagger. 4 Results Following convention, we divided the FE extraction into two subtasks: argument identification and argument classification. We did not try to assign multiple labels to arguments. Figure 3 shows an overview. In addition to detecing the FEs, the argument identification classifier detects the dependency nodes that should be tagged on the laye"
S07-1048,J93-2004,0,0.0274117,"arsi, 2006) with significant improvements of the state of the art, and dependency treebanks are now available for a wide range of languages. The parsing algorithms are comparatively easy to implement and efficient: some of the algorithms parse sentences in linear time (Yamada and Matsumoto, 2003; Nivre et al., 2006). In the semantic structure extraction system, we used the Stanford part-of-speech tagger (Toutanova et al., 2003) to tag the training and test sentences and MaltParser, a statistical dependency parser (Nivre et al., 2006), to parse them. We trained the parser on the Penn Treebank (Marcus et al., 1993). The dependency trees used to train the parser were created from the constituent trees using a conversion program (Johansson and Nugues, 2007)1 . The converter handles most of the secondary edges in the Treebank and encodes those edges as (generally) nonprojective dependency arcs. Such information is available in the Penn Treebank in the form of empty categories and secondary edges, it is however not available in the output of traditional constituent parsers, although there have been some attempts to apply a post-processing step to predict it, see Ahn et al. (2004), inter alia. Figures 1 and"
S07-1048,nivre-etal-2006-maltparser,0,0.0360609,"frame-semantic structure extraction that is based on a dependency parser. The next section presents the dependency grammar that we rely on. We then give the details on the frame detection and disambiguation, the The last few years have seen an increasing interest in dependency parsing (Buchholz and Marsi, 2006) with significant improvements of the state of the art, and dependency treebanks are now available for a wide range of languages. The parsing algorithms are comparatively easy to implement and efficient: some of the algorithms parse sentences in linear time (Yamada and Matsumoto, 2003; Nivre et al., 2006). In the semantic structure extraction system, we used the Stanford part-of-speech tagger (Toutanova et al., 2003) to tag the training and test sentences and MaltParser, a statistical dependency parser (Nivre et al., 2006), to parse them. We trained the parser on the Penn Treebank (Marcus et al., 1993). The dependency trees used to train the parser were created from the constituent trees using a conversion program (Johansson and Nugues, 2007)1 . The converter handles most of the secondary edges in the Treebank and encodes those edges as (generally) nonprojective dependency arcs. Such informati"
S07-1048,P05-1072,0,0.0090728,"dition, some linguistic phenomena such as wh-movement and discontinuous structures are conveniently described using dependency syntax by allowing nonprojective dependency arcs. It has also been claimed that dependency syntax is easier to understand and to teach to people without a linguistic background. Despite these advantages, dependency syntax has relatively rarely been used in semantic structure extraction, with a few exceptions. Ahn et al. (2004) used a post-processing step to convert constituent trees into labeled dependency trees that were then used as input to a semantic role labeler. Pradhan et al. (2005) used a rule-based dependency parser, but the results were significantly worse than when using a constituent parser. This paper describes a system for frame-semantic structure extraction that is based on a dependency parser. The next section presents the dependency grammar that we rely on. We then give the details on the frame detection and disambiguation, the The last few years have seen an increasing interest in dependency parsing (Buchholz and Marsi, 2006) with significant improvements of the state of the art, and dependency treebanks are now available for a wide range of languages. The par"
S07-1048,N03-1033,0,0.0157634,"dency grammar that we rely on. We then give the details on the frame detection and disambiguation, the The last few years have seen an increasing interest in dependency parsing (Buchholz and Marsi, 2006) with significant improvements of the state of the art, and dependency treebanks are now available for a wide range of languages. The parsing algorithms are comparatively easy to implement and efficient: some of the algorithms parse sentences in linear time (Yamada and Matsumoto, 2003; Nivre et al., 2006). In the semantic structure extraction system, we used the Stanford part-of-speech tagger (Toutanova et al., 2003) to tag the training and test sentences and MaltParser, a statistical dependency parser (Nivre et al., 2006), to parse them. We trained the parser on the Penn Treebank (Marcus et al., 1993). The dependency trees used to train the parser were created from the constituent trees using a conversion program (Johansson and Nugues, 2007)1 . The converter handles most of the secondary edges in the Treebank and encodes those edges as (generally) nonprojective dependency arcs. Such information is available in the Penn Treebank in the form of empty categories and secondary edges, it is however not availa"
S07-1048,P05-1073,0,0.0725678,"Missing"
S07-1048,W04-3212,0,0.0425607,"h 5 as features. We think that results could be improved further by using an external NE tagger. 4 Results Following convention, we divided the FE extraction into two subtasks: argument identification and argument classification. We did not try to assign multiple labels to arguments. Figure 3 shows an overview. In addition to detecing the FEs, the argument identification classifier detects the dependency nodes that should be tagged on the layers other than the frame element layer: S UPP, C OP, N ULL, E XIST, and A SP. The A NT and R EL labels could be inserted using simple rules. Similarly to Xue and Palmer (2004), Argument identification verb targets only), the set of dependencies of the target, part of speech of the target node, path through the dependency tree from the target to the node, position (before, after, or on), word and part of speech for the head, word and part of speech for leftmost and rightmost descendent. In the path feature, we removed steps through verb chains and coordination. For instance, in the sentece I have seen and heard it, the path from heard to I is only SBJ↓ and to it OBJ↓. Path Self_mover etc The system was evaluated on three texts. Table 1 shows the results for frame de"
S07-1048,W03-3023,0,0.0345233,"paper describes a system for frame-semantic structure extraction that is based on a dependency parser. The next section presents the dependency grammar that we rely on. We then give the details on the frame detection and disambiguation, the The last few years have seen an increasing interest in dependency parsing (Buchholz and Marsi, 2006) with significant improvements of the state of the art, and dependency treebanks are now available for a wide range of languages. The parsing algorithms are comparatively easy to implement and efficient: some of the algorithms parse sentences in linear time (Yamada and Matsumoto, 2003; Nivre et al., 2006). In the semantic structure extraction system, we used the Stanford part-of-speech tagger (Toutanova et al., 2003) to tag the training and test sentences and MaltParser, a statistical dependency parser (Nivre et al., 2006), to parse them. We trained the parser on the Penn Treebank (Marcus et al., 1993). The dependency trees used to train the parser were created from the constituent trees using a conversion program (Johansson and Nugues, 2007)1 . The converter handles most of the secondary edges in the Treebank and encodes those edges as (generally) nonprojective dependency"
S12-1016,P98-1013,0,0.0656589,"Missing"
S12-1016,J92-4003,0,0.0342547,"ntic role labeling. We then applied a standard greedy forward feature selection procedure to determine which of them to use. The features containing S ALDO ID refer to the entry identifiers in the SALDO lexicon. Note that the POS tags have coarse and fine variants, such as V ERB and V ERB -F INITE P RESENT-ACTIVE respectively, and we used both of them. Semantic role classifiers rely heavily on lexical features (Johansson and Nugues, 2008b), and this may lead to brittleness; in order to increase robustness, we added features based on hierarchical clusters constructed using the Brown algorithm (Brown et al., 1992). The Brown algorithm clusters word into hierarchies represented as bit strings. Based on tuning on a development set, we found that it was best not to use the full bit string, but only a prefix if the string was longer than 12 bits. F RAME D EPENDENCY RELATION PATH F RAME ELEMENTS P OSITION V OICE A RGUMENT HEAD S ALDO ID A RGUMENT HEAD LEMMA A RGUMENT HEAD POS (F INE ) P REDICATE POS (F INE ) A RGUMENT POS (C OARSE ) A RGUMENT RIGHT CHILD POS (C OARSE ) A RGUMENT WORD P REDICATE WORD CLUSTER A RGUMENT WORD CLUSTER Baseline: A Classifier for Swedish Semantic Roles Following most previous impl"
S12-1016,N10-1138,0,0.0314321,"Missing"
S12-1016,J02-3001,0,0.623785,"Missing"
S12-1016,S07-1048,1,0.89168,"Missing"
S12-1016,W08-2123,1,0.801038,"ly once for a given predicate. We considered a large number of features for the classifier (Table 1). Most of these are commonly used features taken from the standard literature on semantic role labeling. We then applied a standard greedy forward feature selection procedure to determine which of them to use. The features containing S ALDO ID refer to the entry identifiers in the SALDO lexicon. Note that the POS tags have coarse and fine variants, such as V ERB and V ERB -F INITE P RESENT-ACTIVE respectively, and we used both of them. Semantic role classifiers rely heavily on lexical features (Johansson and Nugues, 2008b), and this may lead to brittleness; in order to increase robustness, we added features based on hierarchical clusters constructed using the Brown algorithm (Brown et al., 1992). The Brown algorithm clusters word into hierarchies represented as bit strings. Based on tuning on a development set, we found that it was best not to use the full bit string, but only a prefix if the string was longer than 12 bits. F RAME D EPENDENCY RELATION PATH F RAME ELEMENTS P OSITION V OICE A RGUMENT HEAD S ALDO ID A RGUMENT HEAD LEMMA A RGUMENT HEAD POS (F INE ) P REDICATE POS (F INE ) A RGUMENT POS (C OARSE )"
S12-1016,C08-1050,1,0.840429,"ly once for a given predicate. We considered a large number of features for the classifier (Table 1). Most of these are commonly used features taken from the standard literature on semantic role labeling. We then applied a standard greedy forward feature selection procedure to determine which of them to use. The features containing S ALDO ID refer to the entry identifiers in the SALDO lexicon. Note that the POS tags have coarse and fine variants, such as V ERB and V ERB -F INITE P RESENT-ACTIVE respectively, and we used both of them. Semantic role classifiers rely heavily on lexical features (Johansson and Nugues, 2008b), and this may lead to brittleness; in order to increase robustness, we added features based on hierarchical clusters constructed using the Brown algorithm (Brown et al., 1992). The Brown algorithm clusters word into hierarchies represented as bit strings. Based on tuning on a development set, we found that it was best not to use the full bit string, but only a prefix if the string was longer than 12 bits. F RAME D EPENDENCY RELATION PATH F RAME ELEMENTS P OSITION V OICE A RGUMENT HEAD S ALDO ID A RGUMENT HEAD LEMMA A RGUMENT HEAD POS (F INE ) P REDICATE POS (F INE ) A RGUMENT POS (C OARSE )"
S12-1016,J08-2001,0,0.0278985,"Missing"
S12-1016,P09-1003,0,0.508201,"Missing"
S12-1016,C98-1013,0,\N,Missing
S12-1016,heppin-gronostaj-2012-rocky,0,\N,Missing
S14-2086,S13-2053,0,0.256984,"Evaluation (SemEval) organized a shared task on sentiment analysis in Twitter (Nakov et al., 2013) to enable a better comparison of different approaches for sentiment analysis of microblogs. The shared task consisted of two subtasks: one on recognizing contextual polarity of a given subjective expression (Task A), and one on document-level sentiment classification (Task B). For both tasks, the training sets consisted of manually labeled Twitter messages, while the test sets consisted of a Twitter part and an SMS part in order to test domain sensitivity. Among the best performing systems were Mohammad et al. (2013), G¨unther and Furrer (2013) and Becker et al. (2013), who all train linear models on a variety of task-specific features. In this year the corpus resources were used for a re-run of the shared task (Rosenthal et al., 2014), introducing two new Twitter test sets, as well as LiveJournal data. Abstract This paper describes the enhancements made to our GU-MLT-LT system (G¨unther and Furrer, 2013) for the SemEval-2014 re-run of the SemEval-2013 shared task on sentiment analysis in Twitter. The changes include the usage of a Twitter-specific tokenizer, additional features and sentiment lexica, feat"
S14-2086,S13-2055,0,0.0169899,"ent analysis in Twitter (Nakov et al., 2013) to enable a better comparison of different approaches for sentiment analysis of microblogs. The shared task consisted of two subtasks: one on recognizing contextual polarity of a given subjective expression (Task A), and one on document-level sentiment classification (Task B). For both tasks, the training sets consisted of manually labeled Twitter messages, while the test sets consisted of a Twitter part and an SMS part in order to test domain sensitivity. Among the best performing systems were Mohammad et al. (2013), G¨unther and Furrer (2013) and Becker et al. (2013), who all train linear models on a variety of task-specific features. In this year the corpus resources were used for a re-run of the shared task (Rosenthal et al., 2014), introducing two new Twitter test sets, as well as LiveJournal data. Abstract This paper describes the enhancements made to our GU-MLT-LT system (G¨unther and Furrer, 2013) for the SemEval-2014 re-run of the SemEval-2013 shared task on sentiment analysis in Twitter. The changes include the usage of a Twitter-specific tokenizer, additional features and sentiment lexica, feature weighting and random subspace learning. The impro"
S14-2086,S13-2052,0,0.0664315,"s of their classifier. Bermingham and Smeaton (2010) investigate the impact of the shortness of Tweets on sentiment analysis and Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph. There has also been work on lexicon-based approaches to sentiment analysis of microblogs, such as O’Connor et al. (2010), Thelwall et al. (2010) and Zhang et al. (2011). For a detailed discussion see G¨unther (2013). In 2013, the International Workshop on Semantic Evaluation (SemEval) organized a shared task on sentiment analysis in Twitter (Nakov et al., 2013) to enable a better comparison of different approaches for sentiment analysis of microblogs. The shared task consisted of two subtasks: one on recognizing contextual polarity of a given subjective expression (Task A), and one on document-level sentiment classification (Task B). For both tasks, the training sets consisted of manually labeled Twitter messages, while the test sets consisted of a Twitter part and an SMS part in order to test domain sensitivity. Among the best performing systems were Mohammad et al. (2013), G¨unther and Furrer (2013) and Becker et al. (2013), who all train linear m"
S14-2086,J92-4003,0,0.0642368,"ation (SemEval 2014), pages 497–502, Dublin, Ireland, August 23-24, 2014. 2 System Desciption • The word stems of the normalized tokens, reducing inflected forms of a word to a common form. The stems were computed using the Porter stemmer algorithm (Porter, 1980) This section describes the details of our sentiment analysis system, focusing on the differences to our last year’s implementation. This year we only participated in the subtask on whole message polarity classification (Subtask B). 2.1 • The IDs of the token’s word clusters. The clusters were generated by performing Brown clustering (Brown et al., 1992) on 56,345,753 Tweets by Owoputi et al. (2013) and are available online.2 Preprocessing For tokenization of the messages we use the tokenizer of Owoputi et al. (2013)’s Twitter NLP Tools2 , which include a tokenizer and partof-speech tagger optimized for the usage with Tweets. The tokenizer contains a regular expression grammar for recognizing emoticons, which is an especially valuable property in the context of sentiment analysis due to the high emotional expressiveness of emoticons. It is well known that the way word tokens are represented may have a significant impact on the performance of"
S14-2086,N13-1039,0,0.244873,"Missing"
S14-2086,C10-2028,0,0.104322,"Missing"
S14-2086,pak-paroubek-2010-twitter,0,0.0655357,"Missing"
S14-2086,W02-1011,0,0.0169262,"of the problem, sentiment analysis is framed as a categorization problem over documents, where the set of categories is typically a set of polarity values, such as positive, neutral, and negative. Many approaches to document-level sentiment classification have been proposed. For an overview see e.g. Liu (2012). Text in social media and in particular microblog messages are a challenging text genre for sentiment classification, as they introduce additional problems such as short text length, spelling variation, special tokens, topic variation, language style and multilingual content. Following Pang et al. (2002), most sentiment analysis systems have been based on standard text categorization techniques, e.g. training a classifier using some sort of bag-of-words feature representation. This is also true for sentiment analysis of microblogs. Among This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 1 A popular microblogging service on the internet, its messages are commonly referred to as “Tweets.” 497 Proceedings of the 8th International Works"
S14-2086,S13-2054,1,0.646752,"Missing"
S14-2086,S14-2009,0,0.0597701,"wo subtasks: one on recognizing contextual polarity of a given subjective expression (Task A), and one on document-level sentiment classification (Task B). For both tasks, the training sets consisted of manually labeled Twitter messages, while the test sets consisted of a Twitter part and an SMS part in order to test domain sensitivity. Among the best performing systems were Mohammad et al. (2013), G¨unther and Furrer (2013) and Becker et al. (2013), who all train linear models on a variety of task-specific features. In this year the corpus resources were used for a re-run of the shared task (Rosenthal et al., 2014), introducing two new Twitter test sets, as well as LiveJournal data. Abstract This paper describes the enhancements made to our GU-MLT-LT system (G¨unther and Furrer, 2013) for the SemEval-2014 re-run of the SemEval-2013 shared task on sentiment analysis in Twitter. The changes include the usage of a Twitter-specific tokenizer, additional features and sentiment lexica, feature weighting and random subspace learning. The improvements result in an increase of 4.18 F-measure points on this year’s Twitter test set, ranking 3rd. 1 Introduction Automatic analysis of sentiment expressed in text is a"
S14-2086,C12-2114,0,0.0227336,"d that training on a lot of lexical features can lead to brittle NLP systems, that are easily overfit to particular domains. In social media messages the brittleness is particularly acute due to the wide variation in vocabulary and style. While this problem can be eased by using corpus-induced word representations such as the previously introduced word cluster features, it can also be addressed from a learning point of view. Brittleness can be caused by the problem that very strong features (e.g. emoticons) drown out the effect of other useful features. The method of random subspace learning (Søgaard and Johannsen, 2012) seeks to handle this problem by forcing learning algorithms to produce models with more redundancy. It does this by randomly corrupting training instances during learning, so if some useful feature is correlated with a strong feature, the learning algorithm has a better chance to assign it a nonzero weight. We implemented random subspace learning by training the classifier on a concatenation of 25 corrupted copies of the training set. In a corrupted copy, each feature was randomly disabled with a probability of 0.2. Just as for the classifier, the hyperparameters were optimized empirically. A"
S14-2086,W11-2207,0,0.0173607,"richard.johansson@gu.se the first to work specifically with Twitter1 data were Go et al. (2009), who use emoticons as labels for the messages. Similarly, Davidov et al. (2010), Pak and Paroubek (2010), and Kouloumpis et al. (2011) use this method of distant supervision to overcome the data acquisition barrier. Barbosa and Feng (2010) make use of three different sentiment detection websites to label messages and use mostly non-lexical features to improve the robustness of their classifier. Bermingham and Smeaton (2010) investigate the impact of the shortness of Tweets on sentiment analysis and Speriosu et al. (2011) propagate information from seed labels along a linked structure that includes Twitter’s follower graph. There has also been work on lexicon-based approaches to sentiment analysis of microblogs, such as O’Connor et al. (2010), Thelwall et al. (2010) and Zhang et al. (2011). For a detailed discussion see G¨unther (2013). In 2013, the International Workshop on Semantic Evaluation (SemEval) organized a shared task on sentiment analysis in Twitter (Nakov et al., 2013) to enable a better comparison of different approaches for sentiment analysis of microblogs. The shared task consisted of two subtas"
S14-2086,C10-2005,0,\N,Missing
W04-0908,P84-1106,0,0.396658,"Missing"
W04-0908,P98-1013,0,0.0377922,"Missing"
W04-0908,W01-1301,1,0.676517,"ideo images. The authors found that it could also be useful to reverse the process and generate synthetic video sequences from texts. The logic engine behind the text-to-scene converter (Arens et al., 2002) is based on the Discourse Representation Theory. The system is limited to the visualization of single vehicle maneuvers at an intersection as the one described in this two-sentence narrative: A car came from Kriegstrasse. It turned left at the intersection. The authors give no further details on the text corpus and no precise description of the results. 3 Carsim Carsim (Egges et al., 2001; Dupuy et al., 2001) is a program that analyzes texts describing car accidents and visualizes them in a 3D environment. It has been developed using real-world texts. The Carsim architecture is divided into two parts that communicate using a formal representation of Input Text Linguistic Component Formal Description Visualizer Component Output Animation Figure 1: The Carsim architecture. in i en gran. Passageraren, som var f o¨ dd -84, dog. F¨oraren som var 21 a˚ r gammal v˚ardas p˚a sjukhus med sva˚ ra skador. Polisen misst¨anker att bilen de f¨ardades i, en ny Saab, var stulen i Emmaboda och det ska under dagen"
W04-0908,J01-4004,0,0.00972728,"presented an architecture and a strategy based on information extraction and a symbolic visualization that enable to convert real texts into 3D scenes. We have obtained promising results that validate our approach. They show that the Carsim architecture is applicable to Swedish and other languages. As far as we know, Carsim is the only text-to-scene conversion system working on noninvented narratives. We are currently improving Carsim and we hope in future work to obtain better results in the resolution of coreferences. We are implementing and adapting algorithms such as the one described in (Soon et al., 2001) to handle this. We also intend to improve the visualizer to handle more complex scenes and animations. The current aim of the Carsim project is to visualize the content of a text as accurately as possible, with no external knowledge. In the future, we would like to integrate additional knowledge sources in order to make the visualization more realistic and understandable. Geographical and meteorological information systems are good examples of this, which could be helpful to improve the realism. Another topic, which has been prominent in our discussions with traffic safety experts, is how to"
W04-0908,C98-1013,0,\N,Missing
W04-0908,J02-3001,0,\N,Missing
W05-0624,W04-0814,0,0.02376,"Missing"
W05-0624,W05-0620,0,0.0137414,"hat it is necessary to tune the parameters (typically C and γ). This makes it necessary to train repeatedly using cross-validation to find the best combination of parameter values. • It uses no C parameter, which reduces the need for cross-validation. • The decision function is adapted for probabilistic output. • Arbitrary basis functions can be used. Its significant drawback is that the training procedure relies heavily on dense linear algebra, and is thus difficult to scale up to large training sets and may be prone to numerical difficulties. For a description of the task and the data, see (Carreras and Màrquez, 2005). 2 Sparse Bayesian Learning and the Relevance Vector Machine The Sparse Bayesian method is described in detail in (Tipping, 2001). Like other generalized linear learning methods, the resulting binary classifier has the form m X signf (x) = sign αi fi (x) + b i=1 177 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 177–180, Ann Arbor, June 2005. 2005 Association for Computational Linguistics where the fi are basis functions. Training the model then consists of finding a suitable α = (b, α1 , . . . , αm ) given a data set (X, Y ). Analogous with the"
W05-0624,J02-3001,0,0.157719,"hm does not scale up to large training sets, training on the whole PropBank was infeasible. We instead trained the multiclass classifier on sections 15 – 18, and used an SVM for the soft-pruning classifier, which was then trained on the remaining sections. The excellent LIBSVM (Chang and Lin, 2001) package was used to train the SVM. The features used by the classifiers can be grouped into predicate and node features. Of the node features, we here pay most attention to the parse tree path features. 3.1 Predicate Features We used the following predicate features, all of which first appeared in (Gildea and Jurafsky, 2002). • Predicate lemma. • Subcategorization frame. • Voice. 3.2 Node Features • Head word and head POS. Like most previous work, we used the head rules of Collins to extract this feature. • Position. A binary feature that describes if the node is before or after the predicate token. • Phrase type (PT), that is the label of the constituent. • Named entity. Type of the first contained NE. • Governing category. As in (Gildea and Jurafsky, 2002), this was used to distinguish subjects from objects. For an NP, this is either S or VP. • Path features. (See next subsection.) For prepositional phrases, we"
W06-2930,I05-2044,0,0.0153777,"of actions. We measured the improvement over a best-first strategy incrementing values of N . We observed the largest difference between N = 1 and N = 2, then leveling off and we used the latter value. 4.2 Bidirectionality and Voting Tesnière (1966) classified languages as centrifuge (head to the left) and centripetal (head to the right) in a table (page 33 of his book) that nearly exactly fits corpus evidence from the CONLL data. Nivre’s parser is inherently left-right. This may not fit all the languages. Some dependencies may be easier to capture when proceeding from the reverse direction. Jin et al. (2005) is an example of it for Chinese, where the authors describe an adaptation of Nivre’s parser to bidirectionality. We trained the model and ran the algorithm in both directions (left to right and right to left). We used a voting strategy based on probability scores. Each link was assigned a probability score (simply by using the probability of the la or ra actions for each link). We then summed the probability scores of the links from all four trees. To construct a singlehead, rooted, and cycle-free tree, we finally applied the Chu-Liu/Edmonds optimization algorithm (Chu and Liu, 1965; Edmonds,"
W06-2930,afonso-etal-2002-floresta,0,0.0400617,"erences for the left and right dependencies (Table 7). Distance also degrades results but the slope is not as steep as with Swedish (Table 8). Prepositions are also the main source of errors (Table 9). 5.4 Acknowledgments This work was made possible because of the annotated corpora that were kindly provided to us: Arabic (Hajiˇc et al., 2004), Bulgarian (Simov et al., 2005; Simov and Osenova, 2003), Chinese (Chen et al., 2003), Czech (Böhmová et al., 2003), Danish (Kromann, 2003), Dutch (van der Beek et al., 2002), German (Brants et al., 2002), Japanese (Kawata and Bartels, 2000), Portuguese (Afonso et al., 2002), Slovene (Džeroski et al., 2006), Spanish (Civit Torruella and Martí Antonín, 2002), Swedish (Nilsson et al., 2005), and Turkish (Oflazer et al., 2003; Atalay et al., 2003). 209 Table 4: Precision and recall of binned HEAD direction. Swedish. Dir. Gold Cor. Syst. R P to_root 389 330 400 84.83 82.50 left 2745 2608 2759 95.01 94.53 right 1887 1739 1862 92.16 93.39 Table 5: Precision tance. Swedish. Dist. Gold to_root 389 1 2512 2 1107 3-6 803 7-... 210 and recall of binned HEAD disCor. 330 2262 989 652 141 Syst. 400 2363 1122 867 269 R 84.83 90.05 89.34 81.20 67.14 P 82.50 95.73 88.15 75.20 52."
W06-2930,W03-2405,0,0.0521699,"source of errors (Table 9). 5.4 Acknowledgments This work was made possible because of the annotated corpora that were kindly provided to us: Arabic (Hajiˇc et al., 2004), Bulgarian (Simov et al., 2005; Simov and Osenova, 2003), Chinese (Chen et al., 2003), Czech (Böhmová et al., 2003), Danish (Kromann, 2003), Dutch (van der Beek et al., 2002), German (Brants et al., 2002), Japanese (Kawata and Bartels, 2000), Portuguese (Afonso et al., 2002), Slovene (Džeroski et al., 2006), Spanish (Civit Torruella and Martí Antonín, 2002), Swedish (Nilsson et al., 2005), and Turkish (Oflazer et al., 2003; Atalay et al., 2003). 209 Table 4: Precision and recall of binned HEAD direction. Swedish. Dir. Gold Cor. Syst. R P to_root 389 330 400 84.83 82.50 left 2745 2608 2759 95.01 94.53 right 1887 1739 1862 92.16 93.39 Table 5: Precision tance. Swedish. Dist. Gold to_root 389 1 2512 2 1107 3-6 803 7-... 210 and recall of binned HEAD disCor. 330 2262 989 652 141 Syst. 400 2363 1122 867 269 R 84.83 90.05 89.34 81.20 67.14 P 82.50 95.73 88.15 75.20 52.42 Table 6: Focus words where most of the errors occur. Swedish. Word POS Any Head Dep Both till PR 48 20 45 17 i PR 42 25 34 17 på PR 39 22 32 15 med PR 28 11 25 8 för PR 2"
W06-2930,P05-1013,0,0.126713,"tionality. We trained the model and ran the algorithm in both directions (left to right and right to left). We used a voting strategy based on probability scores. Each link was assigned a probability score (simply by using the probability of the la or ra actions for each link). We then summed the probability scores of the links from all four trees. To construct a singlehead, rooted, and cycle-free tree, we finally applied the Chu-Liu/Edmonds optimization algorithm (Chu and Liu, 1965; Edmonds, 1967). 5 Analysis 5.1 Experimental Settings We trained the models on “projectivized” graphs following Nivre and Nilsson (2005) method. We used the complete annotated data for nine langagues. Due to time limitations, we could not complete the training for three languages, Chinese, Czech, and German. 5.2 Overview of the Results We parsed the 12 languages using exactly the same algorithms and parameters. We obtained an average score of 74.93 for the labeled arcs and of 80.39 for the unlabeled ones (resp. 74.98 and 80.80 for the languages where we could train the model using the complete annotated data sets). Table 3 shows the 208 results per language. As a possible explanation of the differences between languages, the t"
W06-2930,W03-3017,0,0.165768,"nd a dependent. We experimented two main additions to our implementation of Nivre’s parser: N best search and bidirectional parsing. We trained the parser in both left-right and right-left directions and we combined the results. To construct a single-head, rooted, and cycle-free tree, we applied the ChuLiu/Edmonds optimization algorithm. We ran the same algorithm with the same parameters on all the languages. 1. if arc(T OP, F IRST ) ∈ G, then ra; 2. else if arc(F IRST, T OP ) ∈ G, then la; 3. else if ∃k ∈ Stack, arc(F IRST, k) ∈ G or arc(k, F IRST ) ∈ G, then re; 4. else sh. 1 Nivre’s Parser Nivre (2003) proposed a dependency parser that creates a projective and acyclic graph. The parser is an extension to the shift–reduce algorithm. As with the regular shift–reduce, it uses a stack S and a list of input words W . However, instead of finding constituents, it builds a set of arcs G representing the graph of dependencies. Nivre’s parser uses two operations in addition to shift and reduce: left-arc and right-arc. Given a sequence of words, possibly annotated with their part Using the first sentence of the Swedish corpus as input (Table 1), this algorithm produces the sequence of 24 actions: sh,"
W06-2930,dzeroski-etal-2006-towards,0,0.0199432,"dependencies (Table 7). Distance also degrades results but the slope is not as steep as with Swedish (Table 8). Prepositions are also the main source of errors (Table 9). 5.4 Acknowledgments This work was made possible because of the annotated corpora that were kindly provided to us: Arabic (Hajiˇc et al., 2004), Bulgarian (Simov et al., 2005; Simov and Osenova, 2003), Chinese (Chen et al., 2003), Czech (Böhmová et al., 2003), Danish (Kromann, 2003), Dutch (van der Beek et al., 2002), German (Brants et al., 2002), Japanese (Kawata and Bartels, 2000), Portuguese (Afonso et al., 2002), Slovene (Džeroski et al., 2006), Spanish (Civit Torruella and Martí Antonín, 2002), Swedish (Nilsson et al., 2005), and Turkish (Oflazer et al., 2003; Atalay et al., 2003). 209 Table 4: Precision and recall of binned HEAD direction. Swedish. Dir. Gold Cor. Syst. R P to_root 389 330 400 84.83 82.50 left 2745 2608 2759 95.01 94.53 right 1887 1739 1862 92.16 93.39 Table 5: Precision tance. Swedish. Dist. Gold to_root 389 1 2512 2 1107 3-6 803 7-... 210 and recall of binned HEAD disCor. 330 2262 989 652 141 Syst. 400 2363 1122 867 269 R 84.83 90.05 89.34 81.20 67.14 P 82.50 95.73 88.15 75.20 52.42 Table 6: Focus words where mos"
W06-2930,W03-2403,0,0.06087,"Missing"
W07-2416,P98-1013,0,0.0243236,"Missing"
W07-2416,J93-2004,0,0.0457351,"arguably more intuitive than constituent syntax when explaining linking, i.e. the realization of the semantic arguments of predicates as syntactic units. This may also have practical implications for “semantic parsers”, although this still remains to be seen in practice. As statistical parsing is becoming the norm, syntactically annotated data, and hence the annotation style they adopt, plays a central role. For English, no significant dependency treebank exists, although there have been some preliminary efforts to create one (Rambow et al., 2002). Instead, the constituentbased Penn Treebank (Marcus et al., 1993), which is the largest treebank for English and the most common training resource for constituent parsing of this language, has been used to train most of the datadriven dependency parsers reported in the literature. However, since it based on constituent structures, a conversion method must be applied that transforms its constituent trees into dependency graphs. The dependency trees produced by existing conversion methods (Magerman, 1994; Collins, 1999; Yamada and Matsumoto, 2003), which have been used by all recent papers on English dependency parsing, have been somewhat simplistic in view o"
W07-2416,H94-1020,0,0.050243,"Yamada and Matsumoto, 2003), which have been used by all recent papers on English dependency parsing, have been somewhat simplistic in view of original dependency treebanks such as the Danish Dependency Treebank (Trautner Kromann, 2003), in particular with respect to the set of edge labels and the treatment of complex long-distance linguistic relations such as wh-movement, topicalization, it-clefts, expletives, and gapping. However, this information is available in the Penn Treebank from version II when its syntactic representation was extended from bare bracketing to a much richer structure (Marcus et al., 1994), but with a few exceptions this has not yet been reflected by automatic parsers, neither constituent-based nor dependency-based. This article describes a new constituent-todependency conversion procedure that makes better use of the existing information in the Treebank. The Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek and Mare Koit (Eds.) NODALIDA 2007 Conference Proceedings, pp. 105–112 Richard Johansson and Pierre Nugues idea of the new conversion method is to make use of the extended structure of the recent versions of the Penn Treebank to derive a more “semantically useful” representa"
W07-2416,E06-1011,0,0.247839,"To quantify this, we trained and evaluated two statistical dependency 110 parsers on the new treebank. M ALT PARSER (Nivre et al., 2006) is based on a greedy parsing procedure that builds a parse tree incrementally while proceeding through the sentence one token at a time. By using a greedy strategy, a rich history-based feature set for the SVM classifier that selects the actions can be used. The parser produces projective trees only, but can handle nonprojectivity if a preprocessing step is used before training and a postprocessing step after parsing (“pseudoprojective parsing”). MSTPARSER (McDonald and Pereira, 2006) predicts a parse tree by maximizing a scoring function over the space of all possible parse trees. The scoring function is a weighted sum of features of single links or, if the “second-order” feature set is used, pairs of adjacent links. The parser can handle nonprojectivity, although the search then becomes NPhard and has to be approximated. Following convention, we trained the parsers on sections 2–21 of the WSJ part of the treebank. The training step took a few hours for M ALT PARSER using a 64-bit AMD processor running at 2.2 GHz and roughly two days for MSTPARSER using a 32-bit Intel pro"
W07-2416,nivre-etal-2006-maltparser,0,0.194887,"Missing"
W07-2416,rambow-etal-2002-dependency,0,0.0396604,"’ˇcuk, 1988). From a theoretical perspective, dependency syntax is arguably more intuitive than constituent syntax when explaining linking, i.e. the realization of the semantic arguments of predicates as syntactic units. This may also have practical implications for “semantic parsers”, although this still remains to be seen in practice. As statistical parsing is becoming the norm, syntactically annotated data, and hence the annotation style they adopt, plays a central role. For English, no significant dependency treebank exists, although there have been some preliminary efforts to create one (Rambow et al., 2002). Instead, the constituentbased Penn Treebank (Marcus et al., 1993), which is the largest treebank for English and the most common training resource for constituent parsing of this language, has been used to train most of the datadriven dependency parsers reported in the literature. However, since it based on constituent structures, a conversion method must be applied that transforms its constituent trees into dependency graphs. The dependency trees produced by existing conversion methods (Magerman, 1994; Collins, 1999; Yamada and Matsumoto, 2003), which have been used by all recent papers on"
W07-2416,W03-3023,0,0.913212,"ere have been some preliminary efforts to create one (Rambow et al., 2002). Instead, the constituentbased Penn Treebank (Marcus et al., 1993), which is the largest treebank for English and the most common training resource for constituent parsing of this language, has been used to train most of the datadriven dependency parsers reported in the literature. However, since it based on constituent structures, a conversion method must be applied that transforms its constituent trees into dependency graphs. The dependency trees produced by existing conversion methods (Magerman, 1994; Collins, 1999; Yamada and Matsumoto, 2003), which have been used by all recent papers on English dependency parsing, have been somewhat simplistic in view of original dependency treebanks such as the Danish Dependency Treebank (Trautner Kromann, 2003), in particular with respect to the set of edge labels and the treatment of complex long-distance linguistic relations such as wh-movement, topicalization, it-clefts, expletives, and gapping. However, this information is available in the Penn Treebank from version II when its syntactic representation was extended from bare bracketing to a much richer structure (Marcus et al., 1994), but w"
W07-2416,J03-4003,0,\N,Missing
W07-2416,C98-1013,0,\N,Missing
W08-2121,D07-1101,0,0.331336,"sults, but the improvement is not large. These initial efforts indicate at least that the joint modeling of this problem is not a trivial task. The D Arch. and D Inference columns summarize the parsing architectures and the corresponding inference strategies. Similar to last year’s shared task (Nivre et al., 2007), the vast majority of parsing models fall in two classes: transition-based (“trans” in the table) or graph-based (“graph”) models. By and large, transition-based models use a greedy inference strategy, whereas graph-based models used different Maximum Spanning Tree (MST) algorithms: Carreras (2007) – MSTC , Eisner (2000) – MSTE , or Chu-Liu/Edmonds (McDonald et al., 2005; Chu and Liu, 1965; Edmonds, 1967) – MSTCL/E . More interestingly, most of the best systems used some strategy to mitigate parsing errors. In the top three systems in the closed challenge, two (che and ciaramita) used parser combination through voting and/or stacking of different models (see the D Comb. column). Samuelsson et al. (2008) perform a MST inference with the bag of all dependencies output by the individual MALT parser variants. Johansson and Nugues (2008) use a single parsing model, but this model is extended"
W08-2121,W08-2134,0,0.108513,"movements, split clauses, and split noun phrases. 6.3 Normalized SRL Performance Table 6.3 lists the scores for the semantic subtask measured as the ratio of the labeled F1 score and LAS. As previously mentioned, this score estimates the performance of the SRL component independent of the performance of the syntactic parser. This analysis is not a substitute for the actual experiment where the SRL components are evaluated using correct syntactic information but, nevertheless, it indicates several interesting facts. First, the ranking of the top three systems in Table 10 changes: the system of Che et al. (2008) is now ranked first, and the system of Johansson and Nugues (2008) is second. This shows that Che et al. have a relatively stronger SRL component, whereas Johansson and Nugues developed a better parser. Second, several other systems improved their ranking compared to Table 10: e.g., chen from position thirteenth to ninth and choi from sixteenth to eighth. This indicates that these systems were penalized in the official ranking mainly due to the relative poor performance of their parsers. Note that this experiment is relevant only for systems that implemented pipeline architectures, where the"
W08-2121,W08-2139,0,0.0101528,"MaltParser with labels enriched with semantic information; Llu´ıs and M`arquez, who used a modified version of the Eisner algorithm to jointly predict syntactic and semantic dependencies; and finally, Sun et al. (2008), who integrated dependency label classification and argument identification using a maximum-entropy Markov model. Additionally, Johansson and Nugues (2008), who had the highest ranked system in the closed challenge, integrate syntactic and semantic analysis in a final reranking step, which maximizes the joint syntactic-semantic score in the top k solutions. In the same spirit, Chen et al. (2008) search in the top k solutions for the one that maximizes a global measure, in this case the joint probability of the complete problem. These joint learning strategies are summarized in the Joint Learning/Opt. column in the table. The system of Riedel and MezaRuiz (2008) deserves a special mention: even though Riedel and Meza-Ruiz did not implement a syntactic parser, they are the only group that performed the complete SRL subtask – i.e., predicate identification and classification, argument identification and classification – jointly, simultaneously for all the predicates in a sentence. They"
W08-2121,M98-1028,0,0.0358658,"and other phenomena, based on a theoretical perspective similar to that of Government and Binding Theory (Chomsky, 1981). 3.1.2 BBN Pronoun Coreference and Entity Type Corpus BBN’s NE annotation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation, e.g., fearlast and cannot are split in the same ways. Named entity categories include: Person, Organization, Location, GPE, Facility, Money, Percent, Time and Date, based on the definitions of these categories in MUC (Chinchor and Robinson, 1998) and ACE7 tasks. Subcategories are included as well. Note however that from this corpus we only use NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. 3.1.3 Proposition Bank I (PropBank) The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (ARG0, ARG1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall"
W08-2121,W08-2138,1,0.51476,"Missing"
W08-2121,W06-1670,0,0.0107243,"haracters (“ ”) are used in columns 2–5 to ensure the same number of rows for all columns corresponding to one sentence. All syntactic and semantic dependencies are annotated relative to the split word forms (columns 6–8). Table 2 shows the columns available to the systems participating in the open challenge: namedentity labels as in the CoNLL-2003 Shared Task (Tjong Kim San and De Meulder, 2003) and from the BBN Wall Street Journal Entity Corpus,2 WordNet supersense tags, and the output of an offthe-shelf dependency parser (Nivre et al., 2007b). Columns 1–3 were predicted using the tagger of Ciaramita and Altun (2006). Because the BBN corpus shares lexical content with the Penn Treebank, we generated the BBN tags using a 2-fold cross-validation procedure. 2.2 Evaluation Measures We separate the evaluation measures into two groups: (i) official measures, which were used for the ranking of participating systems, and (ii) additional unofficial measures, which provide further insight into the performance of the participating systems. 2.2.1 Official Evaluation Measures The official evaluation measures consist of three different scores: (i) syntactic dependencies are scored using the labeled attachment score (LA"
W08-2121,W05-0620,1,0.81831,"Missing"
W08-2121,gimenez-marquez-2004-svmtool,1,0.278194,"Missing"
W08-2121,C04-1186,0,0.0984296,"formalism, but also extends them significantly: this year’s syntactic dependencies include more information such as named-entity boundaries; the semantic dependencies model roles of both verbal and nominal predicates. In this paper, we define the shared task and describe how the data sets were created. Furthermore, we report and analyze the results and describe the approaches of the participating systems. 1 • SRL is performed and evaluated using a dependency-based representation for both syntactic and semantic dependencies. While SRL on top of a dependency treebank has been addressed before (Hacioglu, 2004), our approach has several novelties: (i) our constituent-to-dependency conversion strategy transforms all annotated semantic arguments in PropBank and NomBank not just a subset; (ii) we address propositions centered around both verbal (PropBank) and nominal (NomBank) predicates. Introduction In 2004 and 2005 the shared tasks of the Conference on Computational Natural Language Learning (CoNLL) were dedicated to semantic role labeling (SRL), in a monolingual setting (English). In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 la"
W08-2121,W08-2122,0,0.366479,"Missing"
W08-2121,W08-2136,0,0.0382094,"Missing"
W08-2121,W08-2123,1,0.339084,"-TMP AM-MNR AM-LOC A3 AM-MOD AM-ADV AM-DIS R-A0 AM-NEG A4 C-A1 R-A1 AM-PNC AM-EXT AM-CAU AM-DIR R-AM-TMP R-A2 R-AM-LOC R-AM-MNR A5 AM-PRD C-A0 C-A2 R-AM-CAU C-A3 R-A3 C-AM-MNR C-AM-ADV AM-REC AA R-AM-PNC C-AM-EXT C-AM-TMP C-A4 Frequency &lt; 10 Frequency 161409 109437 51197 25913 13080 11409 10269 9986 9496 5369 4432 4097 3281 3118 2565 2445 1428 1346 1318 797 307 246 155 91 78 70 65 50 37 29 24 20 16 14 12 11 11 11 70 watanabe). Remarkably, the top-scoring system (johansson) is in a class of its own, with scores 2–3 points higher than the next system. This is most likely caused by the fact that Johansson and Nugues (2008) implemented a thorough system that addressed all facets of the task with state-ofthe-art methods: second-order parsing model, argument identification/classification models separately tuned for PropBank and NomBank, reranking inference for the SRL task, and, finally, joint optimization of the complete task using metalearning (more details in Section 5). Table 11 lists the official results in the open challenge. The results in this challenge are lower than in the closed challenge, but this was somewhat to be expected considering that there were fewer participants in this challenge and none of t"
W08-2121,W08-2135,0,0.0101844,"sks: Henderson et al. (2008), who implemented a generative history-based model (Incremental Sigmoid Belief Networks with vectors of latent variables) where syntactic and semantic structures are separately 170 generated but using a synchronized derivation (sequence of actions); Samuelsson et al. (2008), who, within an ensemble-based architecture, implemented a joint syntactic-semantic model using MaltParser with labels enriched with semantic information; Llu´ıs and M`arquez, who used a modified version of the Eisner algorithm to jointly predict syntactic and semantic dependencies; and finally, Sun et al. (2008), who integrated dependency label classification and argument identification using a maximum-entropy Markov model. Additionally, Johansson and Nugues (2008), who had the highest ranked system in the closed challenge, integrate syntactic and semantic analysis in a final reranking step, which maximizes the joint syntactic-semantic score in the top k solutions. In the same spirit, Chen et al. (2008) search in the top k solutions for the one that maximizes a global measure, in this case the joint probability of the complete problem. These joint learning strategies are summarized in the Joint Learn"
W08-2121,W07-2416,1,0.71052,"oNLL) were dedicated to semantic role labeling (SRL), in a monolingual setting (English). In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 languages. The CoNLL-2008 shared task1 proposes a unified dependency-based c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 http://www.yr-bcn.es/conll2008 159 • Based on the observation that a richer set of syntactic dependencies improves semantic processing (Johansson and Nugues, 2007), the syntactic dependencies modeled are more complex than the ones used in the previous CoNLL shared tasks. For example, we now include apposition links, dependencies derived from named entity (NE) structures, and better modeling of long-distance grammatical relations. • A practical framework is provided for the joint learning of syntactic and semantic dependencies. CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 159–177 Manchester, August 2008 Given the complexity of this shared task, we limited the evaluation to a monolingual, Englishonly set"
W08-2121,W03-0419,0,0.0172595,"Missing"
W08-2121,W08-2124,1,0.554724,"Missing"
W08-2121,W08-2140,0,0.0210024,"a single parsing model, but this model is extended with second-order features. The PA Arch. and PA Inference columns summarize the architectures and inference strategies used for the identification and classification of predicates and arguments. The columns indicate that most systems modeled the SRL problem as a token-by-token classification problem (“class” in the table) with a corresponding greedy inference strategy. Some systems (e.g., yuret, samuelsson, henderson, lluis) incorporate SRL within parsing, in which case we report the corresponding parsing architecture and inference approach. Vickrey and Koller (2008) simplify the sentences to be labeled using a set of hand-crafted rules before deploying a classification model on top of a constituent-based representation. Unlike in the case of parsing, few systems (yuret, samuelssson, and morante) combine several PA models and the combination is limited to simple voting strategies (see the PA Comb. column). Finally, the ML Methods column lists the Machine Learning (ML) methods used. The column indicates that maximum entropy (ME) was the most popular method (12 distinct systems relied on it). Support Vector Machines (SVM) (eight systems) and the Perceptron"
W08-2121,J93-2004,0,0.0476215,"of the constituent-to-dependency conversion process. The section concludes with an overview of the shared task corpora. 3.1 Input Corpora Input to our merging procedures includes the Penn Treebank, BBN’s named entity corpus, PropBank and NomBank. In this section, we will provide brief descriptions of these annotations in terms of both form and content. All annotations are currently being distributed by the Linguistic Data Consortium, with the exception of NomBank, which is freely downloadable.6 6 http://nlp.cs.nyu.edu/meyers/NomBank. html 162 3.1.1 Penn Treebank 3 The Penn Treebank 3 corpus (Marcus et al., 1993) consists of hand-coded parses of the Wall Street Journal (test, development and training) and a small subset of the Brown corpus (W. N. Francis and H. Kuˆcera, 1964) (test only). These hand parses are notated in-line and sometimes involve changing the strings of the input data. For example, in file wsj 0309, the token fearlast in the text corresponds to the two tokens fear and last in the annotated data. In a similar way, cannot is regularly split to can and not. It is significant that the other annotations assume the tokenization of the Penn Treebank, as this makes it easier for us to merge"
W08-2121,W03-3023,0,0.13388,"s(T ) return create-dependency-tree(T ) 3.2 Conversion to Dependencies 3.2.1 Syntactic Dependencies There exists no large-scale dependency treebank for English, and we thus had to construct a dependency-annotated corpus automatically from the Penn Treebank (Marcus et al., 1993). Since dependency syntax represents grammatical structure by means of labeled binary head–dependent relations rather than phrases, the task of the conversion procedure is to identify and label the head–dependent pairs. The idea underpinning constituent-to-dependency conversion algorithms (Magerman, 1994; Collins, 1999; Yamada and Matsumoto, 2003) is that head–dependent pairs are created from constituents by selecting one word in each phrase as the head and setting all other as its dependents. The dependency labels are then inferred from the phrase–subphrase or phrase–word relations. Our conversion procedure (Johansson and Nugues, 2007) differs from this basic approach by exploiting the rich structure of the constituent format used in Penn Treebank 3: procedure import-glarf(T ) Import a GLARF surface dependency graph G for each multi-word name N in G for each token d in N Set the function tag of d to NAME for each dependency link h →L"
W08-2121,H05-1066,0,0.305206,"ate at least that the joint modeling of this problem is not a trivial task. The D Arch. and D Inference columns summarize the parsing architectures and the corresponding inference strategies. Similar to last year’s shared task (Nivre et al., 2007), the vast majority of parsing models fall in two classes: transition-based (“trans” in the table) or graph-based (“graph”) models. By and large, transition-based models use a greedy inference strategy, whereas graph-based models used different Maximum Spanning Tree (MST) algorithms: Carreras (2007) – MSTC , Eisner (2000) – MSTE , or Chu-Liu/Edmonds (McDonald et al., 2005; Chu and Liu, 1965; Edmonds, 1967) – MSTCL/E . More interestingly, most of the best systems used some strategy to mitigate parsing errors. In the top three systems in the closed challenge, two (che and ciaramita) used parser combination through voting and/or stacking of different models (see the D Comb. column). Samuelsson et al. (2008) perform a MST inference with the bag of all dependencies output by the individual MALT parser variants. Johansson and Nugues (2008) use a single parsing model, but this model is extended with second-order features. The PA Arch. and PA Inference columns summari"
W08-2121,W08-2126,0,0.0266466,"mBank, reranking inference for the SRL task, and, finally, joint optimization of the complete task using metalearning (more details in Section 5). Table 11 lists the official results in the open challenge. The results in this challenge are lower than in the closed challenge, but this was somewhat to be expected considering that there were fewer participants in this challenge and none of the top five groups in the closed challenge submitted results in the open challenge. Only one of the systems that participated in both challenges (zhang) improved the results submitted in the closed challenge. Zhang et al. (2008) achieved this by extracting features for their semantic subtask models both from the parser used in the closed challenge and a secondary parser that was trained on a different corpus. The improvements measured were relatively small for the in-domain WSJ corpus (0.2 labeled macro F1 points) but larger for the out-of-domain Brown corpus (approximately 1 labeled macro F1 point). Table 9: Statistics for semantic roles. 4 Submissions and Results Nineteen groups submitted test runs in the closed challenge and five groups participated in the open challenge. Three of the latter groups participated on"
W08-2121,W01-1511,0,0.0379316,"Missing"
W08-2121,W04-2705,1,0.723818,"Missing"
W08-2121,W06-2933,1,0.512001,"Missing"
W08-2121,J05-1004,0,0.584805,"compatible with the Penn Treebank annotation, e.g., fearlast and cannot are split in the same ways. Named entity categories include: Person, Organization, Location, GPE, Facility, Money, Percent, Time and Date, based on the definitions of these categories in MUC (Chinchor and Robinson, 1998) and ACE7 tasks. Subcategories are included as well. Note however that from this corpus we only use NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. 3.1.3 Proposition Bank I (PropBank) The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (ARG0, ARG1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types 7 http://projects.ldc.upenn.edu/ace/ of ARGM (TMP, ADV, etc.).8 Rather than using PropBank directly, we used the version created for the CoNLL-2005 shared task (Carreras and M`arquez, 2005). PropBank’s pointers to subtrees are converted into th"
W08-2121,W08-2125,0,0.116607,"g paper in the proceedings. Results are sorted in descending order of the labeled F1 score for semantic dependencies on the WSJ+Brown corpus. The number in parentheses next to the WSJ+Brown scores indicates the system rank in the corresponding task. 5 Approaches Table 5 summarizes the properties of the systems that participated in the closed the open challenges. The second column of the table highlights the overall architectures. We used + to indicate that the components are sequentially connected. The lack of a + sign indicates that the corresponding tasks are performed jointly. For example, Riedel and Meza-Ruiz (2008) perform predicate and argument identification and classification jointly, whereas Ciaramita et al. (2008) implemented a pipeline architecture of three components. We use the ||to indicate that several different architectures that span multiple subtasks were deployed in parallel. This summary of system architectures indicates that it is common that systems combine several components in the semantic or syntactic subtasks – e.g., nine systems jointly performed predicate/argument identification and classification – but only four systems combined components between the syntactic and semantic subta"
W08-2121,J03-4003,0,\N,Missing
W08-2121,D07-1096,1,\N,Missing
W08-2121,W04-2412,1,\N,Missing
W08-2123,D07-1101,0,0.260049,"frequent trace labels. 3 Semantic Submodel Our semantic model consists of three parts: • A SRL classifier pipeline that generates a list of candidate predicate–argument structures. • A constraint system that filters the candidate list to enforce linguistic restrictions on the global configuration of arguments. • A global classifier that rescores the predicate– argument structures in the filtered candidate list. We used a C value of 0.01, and the number of iterations was 6. 2.1 Features and Search The feature function Ψ is a second-order edgefactored representation (McDonald and Pereira, 2006; Carreras, 2007). The second-order representation allows us to express features not only of head–dependent links, but also of siblings and children of the dependent. This feature set forces us to adopt the expensive search procedure by Carreras (2007), which extends Eisner’s span-based dynamic programming algorithm (1996) to allow second-order feature dependencies. Since the cost function ρ is based on the cost of single links, this procedure can also be used to find the maximizer of F (xi , yij ) + ρ(yi , yij ), which is needed at training time. The search was constrained to disallow multiple root links. 2.2"
W08-2123,C96-1058,0,0.0671199,"Missing"
W08-2123,E06-1011,0,0.0678867,"is task). Algorithm 1 shows pseudocode for the algorithm. Algorithm 1 The Online PA Algorithm input Training set T = {(xt , yt )}Tt=1 Number of iterations N Regularization parameter C Initialize w to zeros repeat N times for (xt , yt ) in T let y˜t = arg max “ y F (xt , y) + ρ(yt , y) ” ˜t )−F (x t ,yt )+ρ(yt ,y ˜t ) t ,y let τt = min C, F (xkΨ(x,y ˜t )k2 t )−Ψ(x,y w ← w + τt (Ψ(x, yt ) − Ψ(x, y˜t )) return waverage parsers that consider features of single links only, the Chu-Liu/Edmonds algorithm can be used instead. However, this algorithm cannot be generalized to the second-order setting – McDonald and Pereira (2006) proved that this problem is NPhard, and described an approximate greedy search algorithm. To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson, 2005), in which nonprojective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the nonprojective links at parse time. The use of trace labels in the pseudo-projective transformation leads to a proliferation of edge label types: from 69 to 234 in the training set, many of which occur only once. Since the running time of our parser depends"
W08-2123,P05-1013,0,0.0648681,"zeros repeat N times for (xt , yt ) in T let y˜t = arg max “ y F (xt , y) + ρ(yt , y) ” ˜t )−F (x t ,yt )+ρ(yt ,y ˜t ) t ,y let τt = min C, F (xkΨ(x,y ˜t )k2 t )−Ψ(x,y w ← w + τt (Ψ(x, yt ) − Ψ(x, y˜t )) return waverage parsers that consider features of single links only, the Chu-Liu/Edmonds algorithm can be used instead. However, this algorithm cannot be generalized to the second-order setting – McDonald and Pereira (2006) proved that this problem is NPhard, and described an approximate greedy search algorithm. To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson, 2005), in which nonprojective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the nonprojective links at parse time. The use of trace labels in the pseudo-projective transformation leads to a proliferation of edge label types: from 69 to 234 in the training set, many of which occur only once. Since the running time of our parser depends on the number of labels, we used only the 20 most frequent trace labels. 3 Semantic Submodel Our semantic model consists of three parts: • A SRL classifier pipeline that generates a list of candid"
W08-2123,W08-2121,1,0.167387,"Missing"
W08-2123,P05-1073,0,0.158705,"stic function (“softmax”). We carried out an initial experiment with a more complex joint feature representation, but failed to improve over the baseline. Time prevented us from exploring this direction conclusively. 5 Results The submitted results on the development and test corpora are presented in the upper part of Table 3. After the submission deadline, we corrected a bug in the predicate identification method. This resulted in improved results shown in the lower part. Corpus Development Test WSJ Test Brown Test WSJ + Brown Development Test WSJ Test Brown Test WSJ + Brown Global SRL Model Toutanova et al. (2005) have showed that a global model that scores the complete predicate– argument structure can lead to substantial performance gains. We therefore created a global SRL classifier using the following global features in addition to the features from the pipeline: C ORE A RGUMENT L ABEL S EQUENCE. The complete sequence of core argument labels. The sequence also includes the predicate and voice, for instance A0+break.01/Active+A1. M ISSING C ORE A RGUMENT L ABELS. The set of core argument labels declared in the PropBank/NomBank frame that are not present in the predicate–argument structure. Similarly"
W09-1201,burchardt-etal-2006-salsa,1,0.483589,"Missing"
W09-1201,D07-1101,0,0.391748,"Missing"
W09-1201,W09-1202,0,0.0278745,"order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the different subtasks of the CoNLL shared task. The idea is to decompose the joint learning problem into four subtasks – syntactic dependency identification, syntactic dependency labeling, semantic dependency identification and semantic dependency labeling. The initial step is to use a pipeline approach to use the input of one subtask as input to the next, in the order specified. The iterative steps then use additional features that are not available in the initial step to improve the accuracy of the overall system. For ex"
W09-1201,W09-1205,0,0.222475,"al token; and (c), the existence of an edge between each pair of tokens. Subsequently, they combine the (possibly conflicting) output of the three classifiers by a ranking approach to determine the most likely structure that meets all well-formedness constraints. (Llu´ıs et al., 2009) present a joint approach based on an extension of Eisner’s parser to accommodate also semantic dependency labels. This architecture is similar to the one presented by the same authors in the past edition, with the extension to a second-order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the differen"
W09-1201,W09-1212,1,0.83271,"Missing"
W09-1201,S07-1008,1,0.697359,"Missing"
W09-1201,H05-1066,1,0.168004,"Missing"
W09-1201,W04-2705,1,0.527791,"Missing"
W09-1201,W09-1219,0,0.0294123,"Missing"
W09-1201,H05-1108,1,0.506508,"y, adding further manual labels where necessary. Then, we used frequency and grammatical realization information to map the remaining roles onto higher-numbered Arg roles. We considerably simplified the annotations provided by SALSA, which use a rather complex annotation scheme. In particular, we removed annotation for multi-word expressions (which may be non-contiguous), annotations involving multiple frames for the same predicate (metaphors, underspecification), and inter-sentence roles. The out-of-domain dataset was taken from a study on the multi-lingual projection of FrameNet annotation (Pado and Lapata, 2005). It is sampled from the EUROPARL corpus and was chosen to maximize the lexical coverage, i.e., it contains of a large number of infrequent predicates. Both syntactic and semantic structure were annotated manually, in the TIGER and SALSA format, respectively. Since it uses a simplified annotation schemes, we did not have to discard any annotation. For both datasets, we converted the syntactic TIGER (Brants et al., 2002) representations into dependencies with a similar set of head-finding rules used for the preparation of the CoNLL-X shared task German dataset. Minor modifications (for the con1"
W09-1201,C08-1085,1,0.175934,"Missing"
W09-1201,J05-1004,0,0.213522,"nnotation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation. For the CoNLL-2008 shared task evaluation, this corpus was extended by the task organizers to cover the subset of the Brown corpus used as a secondary testing dataset. From this corpus we only used NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. • Proposition Bank I (PropBank) – The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (Arg0, Arg1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types of adjuncts (ArgM-TMP, -ADV, etc.). • NomBank – NomBank annotation (Meyers et al., 2004) uses essentially the same framework as PropBank to annotate arguments of nouns. Differences between PropBank and NomBank stem from differences between noun"
W09-1201,E09-1087,1,0.646818,"Missing"
W09-1201,W08-2121,1,0.597132,"Missing"
W09-1201,taule-etal-2008-ancora,0,0.543017,"Missing"
W09-1201,cmejrek-etal-2004-prague,1,0.63993,"Missing"
W09-1201,kawahara-etal-2002-construction,1,\N,Missing
W09-1201,J93-2004,0,\N,Missing
W09-1201,D07-1096,1,\N,Missing
W09-4621,W08-2123,1,0.840351,"typically employing WordNet senses, have been used in text classification, but have not resulted in any conclusive improvements. For a review of previous studies and results, see Mansuy and Hilderman (2006). plan.?? investment.?? Sense disambig. Chrysler plans new investment in Latin America plan.01 investment.01 Argument identification Chrysler plans new investment in Latin America plan.01 investment.01 Argument labeling Chrysler plans new investment in Latin America A0 A2 A0 A1 plan.01 investment.01 Figure 3: Example processed by the semantic pipeline. We used a freely available SRL system (Johansson and Nugues, 2008) to extract the predicate– argument structures1 . The system relies on a syntactic and a semantic subcomponent. The syntactic model is a bottom-up dependency parser and the semantic model uses global inference mechanisms on top of a pipeline of classifiers. The com1 144 Download site: nlp.cs.lth.se. Text Categorization Using Predicate–Argument Structures Semantic pipeline Syntactic dependency parsing Predicate identification Sense disambig. Argument identification Global semantic model Argument labeling Linguistic constraints Pred−arg reranking Syntactic−semantic reranking Figure 2: The archit"
W09-4621,W04-2705,0,0.0290335,"d to unrestricted text, at least business text, with a satisfying level of quality. Chrysler plans new investment in Latin America Predicate identification Chrysler plans new investment in Latin America Role semantics (Fillmore, 1968) is a formalism that abstracts over the bare syntactic representation by means of semantic roles like AGENT and PATIENT rather than grammatical functions such as subject and object. Figure 1 shows the first example sentence in Sect. 2.2 annotated with syntactic dependencies and role-semantic information according to the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) standard. The verb plan is a predicate defined in the PropBank lexicon, which lists its four possible core arguments: A0, planner, A1, the thing planned, A2, grounds for planning, and A3, beneficiary. Similarly, the noun investment is a NomBank predicate whose three possible core arguments are: A0, investor, A1, theme, and A2 purpose. In addition to the core arguments, predicates also accept optional adjuncts such as locations or times. For each predicate, PropBank and NomBank define a number of word senses, such as plan.01 and investment.01 in the example sentence. Features based on word sen"
W09-4621,J07-2002,0,0.0137263,"ate– argument tuples, subject–verb–object triples, and word-sense information – and we extended the document vectors with them. Predicate–argument structures are core constructs in most formalisms dealing with knowledge representation. They are equally prominent in linguistic theories of compositional semantic representation. In the simplest case, predicate– argument tuples can be approximated by subject– verb–object triples or subject–verb pairs and extracted from surface-syntactic dependency trees. SVO representations have been used in vectorspace approaches to a number of tasks (Lin, 1998; Padó and Lapata, 2007). In the widely publicized semantic web initiative, Berners-Lee et al. (2001) advocated their use as a natural way to describe the vast majority of the data processed by machines. They also correspond to binary relations in relation algebra on which we can apply a large number of mathematical properties. Nonetheless, as far as we know, strict SVO representations have never been used in automatic text categorization. Fürnkranz et al. (1998) proposed an approximated SVO representation that could increase the precision of some categorization experiments when combined with a low recall ranging fro"
W09-4621,J05-1004,0,0.0135289,"ic role labelers can now be applied to unrestricted text, at least business text, with a satisfying level of quality. Chrysler plans new investment in Latin America Predicate identification Chrysler plans new investment in Latin America Role semantics (Fillmore, 1968) is a formalism that abstracts over the bare syntactic representation by means of semantic roles like AGENT and PATIENT rather than grammatical functions such as subject and object. Figure 1 shows the first example sentence in Sect. 2.2 annotated with syntactic dependencies and role-semantic information according to the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) standard. The verb plan is a predicate defined in the PropBank lexicon, which lists its four possible core arguments: A0, planner, A1, the thing planned, A2, grounds for planning, and A3, beneficiary. Similarly, the noun investment is a NomBank predicate whose three possible core arguments are: A0, investor, A1, theme, and A2 purpose. In addition to the core arguments, predicates also accept optional adjuncts such as locations or times. For each predicate, PropBank and NomBank define a number of word senses, such as plan.01 and investment.01 in the example se"
W09-4621,W02-1001,0,0.0139503,"erage F1 while a low threshold results in a potentially large number of documents assigned to a wrong category, which has a negative impact on both the micro and the macroaverage F1 . To avoid this, the F1 score is calculated for each category in the training set. If the score is too low, the highest ranking is chosen as the threshold for that category. 4.3 Corpus Tagging and Parsing We annotated the RCV1 corpus with POS tags, dependency relations, and predicate argument structures using the SRL system mentioned in Sect. 3. The POS tagger uses techniques that are similar to those described by Collins (2002). 4.4 Feature Sets We conducted our experiments with three main sets of features. The first feature set is the baseline bag of words. The second one uses the triples consisting of the verb, subject, and object (VSO) for given predicates. The third set corresponds to predicates, their sense, and their most frequent core arguments: A0 and A1. We exemplify the features with the sentence Chrysler plans new investment in Latin America, whose syntactic and semantic graphs are shown in Figure 1. 145 Jacob Persson, Richard Johansson and Pierre Nugues As first feature set, we used the bags of words cor"
W09-4621,J02-3001,0,0.0166025,"ld the car to him. ROOT SBJ OBJ NMOD LOC PMOD NMOD Chrysler plans new investment in Latin America A0 A0 plan.01 A1 A2 investment.01 Figure 1: Example sentence with dependency syntax and role semantics annotation. Upper arrows correspond to the dependency relations and the lower ones to the semantic roles. 3 Automatic Semantic Role Labeling Role-semantic structures can be automatically extracted from free text – this task is referred to as semantic role labeling (SRL). Although early SRL systems (Hirst, 1983) used symbolic rules, modern systems to a large extent rely on statistical techniques (Gildea and Jurafsky, 2002). This has been made possible by the availability of training data, first from FrameNet (Ruppenhofer et al., 2006) and then PropBank and NomBank. Semantic role labelers can now be applied to unrestricted text, at least business text, with a satisfying level of quality. Chrysler plans new investment in Latin America Predicate identification Chrysler plans new investment in Latin America Role semantics (Fillmore, 1968) is a formalism that abstracts over the bare syntactic representation by means of semantic roles like AGENT and PATIENT rather than grammatical functions such as subject and object"
W09-4621,P83-1010,0,0.0429257,"planned by Chrysler, and diathesis alternations such as dative shifts, We sold him the car / We sold the car to him. ROOT SBJ OBJ NMOD LOC PMOD NMOD Chrysler plans new investment in Latin America A0 A0 plan.01 A1 A2 investment.01 Figure 1: Example sentence with dependency syntax and role semantics annotation. Upper arrows correspond to the dependency relations and the lower ones to the semantic roles. 3 Automatic Semantic Role Labeling Role-semantic structures can be automatically extracted from free text – this task is referred to as semantic role labeling (SRL). Although early SRL systems (Hirst, 1983) used symbolic rules, modern systems to a large extent rely on statistical techniques (Gildea and Jurafsky, 2002). This has been made possible by the availability of training data, first from FrameNet (Ruppenhofer et al., 2006) and then PropBank and NomBank. Semantic role labelers can now be applied to unrestricted text, at least business text, with a satisfying level of quality. Chrysler plans new investment in Latin America Predicate identification Chrysler plans new investment in Latin America Role semantics (Fillmore, 1968) is a formalism that abstracts over the bare syntactic representati"
W09-4621,W08-2121,1,0.769372,"Missing"
W09-4621,P98-2127,0,\N,Missing
W09-4621,C98-2122,0,\N,Missing
W10-2910,P98-1013,0,0.0182134,"t al., 2006). However, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy. Therefore, the use of the syntactic/semantic kernels, i.e. (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b), to syntactically contextualize word similarities may improve the reranker accuracy. (ii) The latter can be further boosted by studying complex structural kernels, e.g. (Moschitti, 2008; Nguyen et al., 2009; Dinarelli et al., 2009). (iii) More specific predicate argument structures such those proposed in FrameNet, e.g. (Baker et al., 1998; Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Johansson and Nugues, 2008b) may be useful to characterize the opinion holder and the sentence semantic context. 5 Conclusion We have shown that features derived from grammatical and semantic role structure can be used to improve the detection of opinionated expressions in subjectivity analysis. Most significantly, the recall is drastically increased (10 points) while the precision decreases only slightly (3 points). This result compares favorably with previously published results, which have been biased towards precision and scored l"
W10-2910,D09-1112,1,0.810386,"e use of lexical similarity to reduce data sparseness, e.g. (Basili et al., 2005; Basili et al., 2006; Bloehdorn et al., 2006). However, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy. Therefore, the use of the syntactic/semantic kernels, i.e. (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b), to syntactically contextualize word similarities may improve the reranker accuracy. (ii) The latter can be further boosted by studying complex structural kernels, e.g. (Moschitti, 2008; Nguyen et al., 2009; Dinarelli et al., 2009). (iii) More specific predicate argument structures such those proposed in FrameNet, e.g. (Baker et al., 1998; Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Johansson and Nugues, 2008b) may be useful to characterize the opinion holder and the sentence semantic context. 5 Conclusion We have shown that features derived from grammatical and semantic role structure can be used to improve the detection of opinionated expressions in subjectivity analysis. Most significantly, the recall is drastically increased (10 points) while the precision decreases only slightly (3 points). This resul"
W10-2910,W05-0601,1,0.769097,"r the task of finding opinionated expressions. We note that the performance of our baseline sequence labeler is lower than theirs; this is to be expected since they used a more complex batch learning algorithm (conditional random fields) while we used an online learner, and they spent more effort on feature design. This indicates that we should be able to achieve even higher performance using a stronger base model. The flexible architecture we have presented enables interesting future research: (i) a straightforward improvement is the use of lexical similarity to reduce data sparseness, e.g. (Basili et al., 2005; Basili et al., 2006; Bloehdorn et al., 2006). However, the similarity between subjective words, which have multiple senses against other words may negatively impact the system accuracy. Therefore, the use of the syntactic/semantic kernels, i.e. (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b), to syntactically contextualize word similarities may improve the reranker accuracy. (ii) The latter can be further boosted by studying complex structural kernels, e.g. (Moschitti, 2008; Nguyen et al., 2009; Dinarelli et al., 2009). (iii) More specific predicate argument structures such"
W10-2910,P06-1117,1,0.21832,"jective words, which have multiple senses against other words may negatively impact the system accuracy. Therefore, the use of the syntactic/semantic kernels, i.e. (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b), to syntactically contextualize word similarities may improve the reranker accuracy. (ii) The latter can be further boosted by studying complex structural kernels, e.g. (Moschitti, 2008; Nguyen et al., 2009; Dinarelli et al., 2009). (iii) More specific predicate argument structures such those proposed in FrameNet, e.g. (Baker et al., 1998; Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Johansson and Nugues, 2008b) may be useful to characterize the opinion holder and the sentence semantic context. 5 Conclusion We have shown that features derived from grammatical and semantic role structure can be used to improve the detection of opinionated expressions in subjectivity analysis. Most significantly, the recall is drastically increased (10 points) while the precision decreases only slightly (3 points). This result compares favorably with previously published results, which have been biased towards precision and scored low on recall. The long-distance structural features gives"
W10-2910,P08-1067,0,0.0157514,"e complex approaches requiring advanced search techniques are mainly simplicity and efficiency: this approach is conceptually simple and fairly easy to implement provided that kbest output can be generated efficiently, and features can be arbitrarily complex – we don’t have to think about how the features affect the algorithmic complexity of the inference step. A common objection to reranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). 3.1 This is viewed as the main impediment Figure 2: Sequence labeling example. Syntactic and Semantic Structures The sequence labeler used word, POS tag, and lemma features in a window of size 3. In addition, we used prior polarity and intensity features derived from the lexicon created by Wilson et al. (2005). In the example, viewed is listed as having strong prior subjectivity but no polarity, and impediment has strong prior subjectivity and negative polarity. Note that prior subjectivity does not always imply subjectivity in a particular context; this is why contextual features are essent"
W10-2910,W06-1651,0,0.625063,"p the opinionated expressions, i.e. the text snippets signaling the subjective content of the text. This is necessary for further analysis, such as the determination of opinion holder and the polarity of the opinion. The MPQA corpus (Wiebe et al., 2005), a widely used corpus annotated with subjectivity information, defines two types of subjective expressions: direct subjective expressions (DSEs), which are explicit mentions The task of marking up these expressions has usually been approached using straightforward sequence labeling techniques using simple features in a small contextual window (Choi et al., 2006; Breck et al., 2007). However, due to the simplicity of the feature sets, this approach fails to take into account the fact that the semantic and pragmatic interpretation of sentences is not only determined by words but also by syntactic and shallow-semantic relations. Crucially, taking grammatical relations into account allows us to model how expressions interact in various ways that influence their interpretation as subjective or not. Consider, for instance, the word said in examples (3) and (4) below, where the interpretation as a DSE or an OSE is influenced by the subjective content of th"
W10-2910,W08-2123,1,0.895519,"Missing"
W10-2910,P02-1034,0,0.0607225,"pproach, the learning algorithm thus directly optimizes the measure we are interested in, i.e. the F-measure. 3.5 4 Experiments DT a Structure Learning Approach The Preference Kernel approach reduces the reranking problem to a binary classification task on pairs, after which a standard SVM optimizer is used to train the reranker. A problem with this method is that the optimization problem solved by the SVM – maximizing the classification accuracy on a set of independent pairs – is not directly related to the performance of the reranker. Instead, the method employed by many rerankers following Collins and Duffy (2002) directly learn a scoring function that is trained to maximize performance on the reranking task. We will refer to this approach as the structure learning method. While there are batch learning algorithms that work in this setting (Tsochantaridis et al., 2005), online learning methods have been more popular for efficiency reasons. We investigated two online learning algorithms: the popular structured perceptron Collins and Duffy (2002) and the Passive– Aggressive (PA) algorithm (Crammer et al., 2006). To increase robustness, we averaged the weight vectors seen during training as in the Voted P"
W10-2910,C08-1050,1,0.911231,"as strong prior subjectivity and negative polarity. Note that prior subjectivity does not always imply subjectivity in a particular context; this is why contextual features are essential for this task. This sequence labeler is used to generate the candidate set for the reranker; the Viterbi algorithm is easily modified to give k-best output. To generate training data for the reranker, we carried out a 5-fold cross-validation procedure: We split the training set into 5 pieces, trained a sequence labeler on pieces 1 to 4, applied it to piece 5 and so on. We used the syntactic–semantic parser by Johansson and Nugues (2008a) to annnotate the sentences with dependency syntax (Mel’ˇcuk, 1988) and shallow semantic structures in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows an example of the annotation: The sentence they called him a liar, where called is a DSE and liar is an ESE, has been annotated with dependency syntax (above the text) and PropBank-based semantic role structure (below the text). The predicate called, which is an instance of the PropBank frame call.01, has three semantic arguments: the Agent (A0), the Theme (A1), and the Predicate (A2), which are"
W10-2910,ruppenhofer-etal-2008-finding,0,0.0148568,"s using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for the same task. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, which has recently attracted some attention in the opinion analysis community (Somasundaran et al., 2009). In this paper, we demonstrate how syntactic and semantic structural information can be used to improve opinion detection. While this feature model makes it impossible to use the standard sequence labeling method, we show that with a simple strategy based o"
W10-2910,W03-0402,0,0.108712,"as well. One such linguistic pheonomenon is the discourse structure, which has recently attracted some attention in the opinion analysis community (Somasundaran et al., 2009). In this paper, we demonstrate how syntactic and semantic structural information can be used to improve opinion detection. While this feature model makes it impossible to use the standard sequence labeling method, we show that with a simple strategy based on reranking, incorporating structural features results in a significant improvement. We investigate two different reranking strategies: the Preference Kernel approach (Shen and Joshi, 2003) and an approach based on structure learning (Collins, 2002). In an evaluation on the MPQA corpus, the best system we evaluated, a structure learning-based reranker using the Passive–Aggressive learning algorithm, achieved a 10-point absolute improvement in soft recall, and a 5-point improvement in F-measure, over the baseline sequence labeler . 2 Motivation and Related Work Most approaches to analysing the sentiment of natural-language text have relied fundamentally on purely lexical information (see (Pang et al., 2002; Yu and Hatzivassiloglou, 2003), inter alia) or low-level grammatical info"
W10-2910,P09-2079,0,0.160932,"Missing"
W10-2910,D09-1018,0,0.0280578,"y, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for the same task. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, which has recently attracted some attention in the opinion analysis community (Somasundaran et al., 2009). In this paper, we demonstrate how syntactic and semantic structural information can be used to improve opinion detection. While this feature model makes it impossible to use the standard sequence labeling method, we show that with a simple strategy based on reranking, incorporating structural features results in a significant improvement. We investigate two different reranking strategies: the Preference Kernel approach (Shen and Joshi, 2003) and an approach based on structure learning (Collins, 2002). In an evaluation on the MPQA corpus, the best system we evaluated, a structure learning-bas"
W10-2910,P03-1005,0,0.00900394,"and the argument label. For instance, the ESE liar is connected to the DSE call via an A2 label, and we represent this using a feature DSE:A2:ESE. Apart from the syntactic and semantic features, we also used the score output from the base sequence labeler as a feature. We normalized the scores over the k candidates so that their exponentials summed to 1. 1 70 Available at http://dit.unitn.it/∼moschitt output y and the predicted output yˆ, where Φ is the feature representation function: 1999)2 . It is still an open question how dependency trees should be represented for use with tree kernels (Suzuki et al., 2003; Nguyen et al., 2009); we used the representation shown in Figure 3. Note that we have concatenated the opinion expression labels to the POS tag nodes. We did not use any of the features from Section 3.3 except for the base sequence labeler score. yˆ ← arg maxh w · Φ(x, h) w ← w + Φ(x, y) − Φ(x, yˆ) In the PA algorithm, which is based on the theory of large-margin learning, we instead find the yˆ that violates the margin constraints maximally. The update step length τ is computed based on the margin; this update is bounded by a regularization constant C: TOP ROOT  ρ(y, h) yˆ ← arg max  h w"
W10-2910,W06-0301,0,0.0502376,"the position by Karlgren et al. (2010) – that constructional features signal opinion – originates from a particular theoretical framework and may be controversial, syntactic and shallowsemantic relations have repeatedly proven useful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for the same task. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the disc"
W10-2910,E99-1023,0,0.0195897,"pply over an arbitrarily long distance in the sentence. While it is possible that search algorithms for exact or approximate inference can be constructured for the arg max problem in this model, we sidestepped this issue by using a reranking decomposition of the problem: We first apply a standard Viterbi-based sequence labeler using no structural features and generate a small candidate set of size k. Then, a second and more complex model picks 68 algorithm (Crammer et al., 2006) instead of the perceptron. We encoded the opinionated expression brackets using the IOB2 encoding scheme (Tjong Kim Sang and Veenstra, 1999). Figure 2 shows an example of a sentence with a DSE and an ESE and how they are encoded in the IOB2 encoding. the top candidate from this set without having to search the whole candidate space. The advantages of a reranking approach compared to more complex approaches requiring advanced search techniques are mainly simplicity and efficiency: this approach is conceptually simple and fairly easy to implement provided that kbest output can be generated efficiently, and features can be arbitrarily complex – we don’t have to think about how the features affect the algorithmic complexity of the inf"
W10-2910,D07-1114,0,0.0203528,"y found that the most prominent constructional feature for subjectivity analysis was the Tense Shift construction. While the position by Karlgren et al. (2010) – that constructional features signal opinion – originates from a particular theoretical framework and may be controversial, syntactic and shallowsemantic relations have repeatedly proven useful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for the same task. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic id"
W10-2910,P99-1032,0,0.0126318,"Missing"
W10-2910,W04-2705,0,0.0120623,"sential for this task. This sequence labeler is used to generate the candidate set for the reranker; the Viterbi algorithm is easily modified to give k-best output. To generate training data for the reranker, we carried out a 5-fold cross-validation procedure: We split the training set into 5 pieces, trained a sequence labeler on pieces 1 to 4, applied it to piece 5 and so on. We used the syntactic–semantic parser by Johansson and Nugues (2008a) to annnotate the sentences with dependency syntax (Mel’ˇcuk, 1988) and shallow semantic structures in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows an example of the annotation: The sentence they called him a liar, where called is a DSE and liar is an ESE, has been annotated with dependency syntax (above the text) and PropBank-based semantic role structure (below the text). The predicate called, which is an instance of the PropBank frame call.01, has three semantic arguments: the Agent (A0), the Theme (A1), and the Predicate (A2), which are realized on the surface-syntactic level as a subject, a direct object, and an object predicative complement, respectively. OPRD OBJ SBJ NMOD 3.3 They [called ]DSE him a [lia"
W10-2910,N10-1121,0,0.148812,"eful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for the same task. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena must be studied as well. One such linguistic pheonomenon is the discourse structure, which has recently attracted some attention in the opinion analysis community (Somasundaran et al., 2009). In this paper, we demonstrate how syntactic and semantic structural information can be used to improve opinion"
W10-2910,E06-1015,1,0.816723,"been successful for a number of structure extraction tasks, such as relation extraction (Zhang et al., 2006; Nguyen et al., 2009) and opinion holder extraction (Wiegand and Klakow, 2010). A tree kernel implicitly represents a large space of fragments extracted from trees and could thus reduce the need for manual feature design. Since the paths that we extract manually (Section 3.3) can be expressed as tree fragments, this method could be an interesting alternative to the manually extracted features used with the linear kernel. We therefore implemented a reranker using the Partial Tree Kernel (Moschitti, 2006), and we trained it using the SVMLight-TK software1 , which is a modification of SVMLight (Joachims, C ONNECTING A RGUMENT L ABEL. When a predicate inside some opinion expression is connected to some argument inside another opinion expression, we use a feature consisting of the two expression labels and the argument label. For instance, the ESE liar is connected to the DSE call via an A2 label, and we represent this using a feature DSE:A2:ESE. Apart from the syntactic and semantic features, we also used the score output from the base sequence labeler as a feature. We normalized the scores over"
W10-2910,H05-1044,0,0.0592384,"s affect the algorithmic complexity of the inference step. A common objection to reranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). 3.1 This is viewed as the main impediment Figure 2: Sequence labeling example. Syntactic and Semantic Structures The sequence labeler used word, POS tag, and lemma features in a window of size 3. In addition, we used prior polarity and intensity features derived from the lexicon created by Wilson et al. (2005). In the example, viewed is listed as having strong prior subjectivity but no polarity, and impediment has strong prior subjectivity and negative polarity. Note that prior subjectivity does not always imply subjectivity in a particular context; this is why contextual features are essential for this task. This sequence labeler is used to generate the candidate set for the reranker; the Viterbi algorithm is easily modified to give k-best output. To generate training data for the reranker, we carried out a 5-fold cross-validation procedure: We split the training set into 5 pieces, trained a seque"
W10-2910,D09-1159,0,0.0258029,"or subjectivity analysis was the Tense Shift construction. While the position by Karlgren et al. (2010) – that constructional features signal opinion – originates from a particular theoretical framework and may be controversial, syntactic and shallowsemantic relations have repeatedly proven useful for subtasks of subjectivity analysis that are inherently relational, above all for determining the holder or topic of a given opinion. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al., 2005; Kobayashi et al., 2007; Joshi and Penstein-Ros´e, 2009; Wu et al., 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi et al. (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for the same task. Ruppenhofer et al. (2008) argued that semantic role techniques are useful but not completely sufficient for holder and topic identification, and that other linguistic phenomena"
W10-2910,D09-1143,1,0.574103,"ted with the feature DSE/call.01. 3.4.1 Linear Kernel We created feature vectors extracted from the candidate sequences using the features described in Section 3.3. We then trained linear SVMs using the L IBLINEAR software (Fan et al., 2008), using L1 loss and L2 regularization. P REDICATE AND A RGUMENT L ABEL. For every argument of a predicate inside an opinion expression, we create a feature representing the predicate–argument pair: DSE/call.01:A0. 3.4.2 Tree Kernel Tree kernels have been successful for a number of structure extraction tasks, such as relation extraction (Zhang et al., 2006; Nguyen et al., 2009) and opinion holder extraction (Wiegand and Klakow, 2010). A tree kernel implicitly represents a large space of fragments extracted from trees and could thus reduce the need for manual feature design. Since the paths that we extract manually (Section 3.3) can be expressed as tree fragments, this method could be an interesting alternative to the manually extracted features used with the linear kernel. We therefore implemented a reranker using the Partial Tree Kernel (Moschitti, 2006), and we trained it using the SVMLight-TK software1 , which is a modification of SVMLight (Joachims, C ONNECTING"
W10-2910,W03-1017,0,0.0580957,"ranking strategies: the Preference Kernel approach (Shen and Joshi, 2003) and an approach based on structure learning (Collins, 2002). In an evaluation on the MPQA corpus, the best system we evaluated, a structure learning-based reranker using the Passive–Aggressive learning algorithm, achieved a 10-point absolute improvement in soft recall, and a 5-point improvement in F-measure, over the baseline sequence labeler . 2 Motivation and Related Work Most approaches to analysing the sentiment of natural-language text have relied fundamentally on purely lexical information (see (Pang et al., 2002; Yu and Hatzivassiloglou, 2003), inter alia) or low-level grammatical information such as partof-speech tags and functional words (Wiebe et al., 1999). This is in line with the general consensus in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search (Moschitti and Basili, 2004). However, it has been suggested that subjectivity analysis is inherently more subtle than categorization and that structural linguistic information should therefore be given more attention in this context. For instance, Karlgren et al. (2010) argued from"
W10-2910,J05-1004,0,0.00594556,"is why contextual features are essential for this task. This sequence labeler is used to generate the candidate set for the reranker; the Viterbi algorithm is easily modified to give k-best output. To generate training data for the reranker, we carried out a 5-fold cross-validation procedure: We split the training set into 5 pieces, trained a sequence labeler on pieces 1 to 4, applied it to piece 5 and so on. We used the syntactic–semantic parser by Johansson and Nugues (2008a) to annnotate the sentences with dependency syntax (Mel’ˇcuk, 1988) and shallow semantic structures in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Figure 1 shows an example of the annotation: The sentence they called him a liar, where called is a DSE and liar is an ESE, has been annotated with dependency syntax (above the text) and PropBank-based semantic role structure (below the text). The predicate called, which is an instance of the PropBank frame call.01, has three semantic arguments: the Agent (A0), the Theme (A1), and the Predicate (A2), which are realized on the surface-syntactic level as a subject, a direct object, and an object predicative complement, respectively. OPRD OBJ SBJ NMO"
W10-2910,N06-1037,0,0.0637993,"so a DSE is represented with the feature DSE/call.01. 3.4.1 Linear Kernel We created feature vectors extracted from the candidate sequences using the features described in Section 3.3. We then trained linear SVMs using the L IBLINEAR software (Fan et al., 2008), using L1 loss and L2 regularization. P REDICATE AND A RGUMENT L ABEL. For every argument of a predicate inside an opinion expression, we create a feature representing the predicate–argument pair: DSE/call.01:A0. 3.4.2 Tree Kernel Tree kernels have been successful for a number of structure extraction tasks, such as relation extraction (Zhang et al., 2006; Nguyen et al., 2009) and opinion holder extraction (Wiegand and Klakow, 2010). A tree kernel implicitly represents a large space of fragments extracted from trees and could thus reduce the need for manual feature design. Since the paths that we extract manually (Section 3.3) can be expressed as tree fragments, this method could be an interesting alternative to the manually extracted features used with the linear kernel. We therefore implemented a reranker using the Partial Tree Kernel (Moschitti, 2006), and we trained it using the SVMLight-TK software1 , which is a modification of SVMLight ("
W10-2910,W02-1011,0,0.0155002,"te two different reranking strategies: the Preference Kernel approach (Shen and Joshi, 2003) and an approach based on structure learning (Collins, 2002). In an evaluation on the MPQA corpus, the best system we evaluated, a structure learning-based reranker using the Passive–Aggressive learning algorithm, achieved a 10-point absolute improvement in soft recall, and a 5-point improvement in F-measure, over the baseline sequence labeler . 2 Motivation and Related Work Most approaches to analysing the sentiment of natural-language text have relied fundamentally on purely lexical information (see (Pang et al., 2002; Yu and Hatzivassiloglou, 2003), inter alia) or low-level grammatical information such as partof-speech tags and functional words (Wiebe et al., 1999). This is in line with the general consensus in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search (Moschitti and Basili, 2004). However, it has been suggested that subjectivity analysis is inherently more subtle than categorization and that structural linguistic information should therefore be given more attention in this context. For instance, Kar"
W10-2910,W02-1001,0,\N,Missing
W10-2910,C98-1013,0,\N,Missing
W12-1622,N04-1015,0,0.060516,"Missing"
W12-1622,P05-1018,0,0.0673449,"Missing"
W12-1622,P05-1022,0,0.143449,"Missing"
W12-1622,P02-1034,0,0.101171,"Missing"
W12-1622,W04-3233,0,0.0661238,"Missing"
W12-1622,I11-1120,1,0.87176,"Missing"
W12-1622,ghosh-etal-2012-improving,1,0.891784,"Missing"
W12-1622,J95-2003,0,0.724653,"Missing"
W12-1622,W05-1506,0,0.0972718,"Missing"
W12-1622,W10-2910,1,0.892872,"Missing"
W12-1622,P03-1069,0,0.0879288,"Missing"
W12-1622,J93-2004,0,0.0427885,"Missing"
W12-1622,P09-2004,0,0.294877,"Missing"
W12-1622,prasad-etal-2008-penn,0,0.249655,"Missing"
W12-1622,W03-0402,0,0.0606469,"Missing"
W12-1622,N03-1030,0,0.313686,"Missing"
W12-1622,J08-2002,0,0.0684847,"Missing"
W12-1622,W03-3023,0,0.175855,"Missing"
W12-1622,J05-1003,0,\N,Missing
W12-1622,P06-2103,0,\N,Missing
W12-1622,P02-1062,0,\N,Missing
W12-1902,P06-2057,1,0.828888,"tion has seen some use in monolingual applications: for instance, Burchardt et al. (2005) and Johansson and Nugues (2007) attempted to extend the coverage of FrameNet by making use of WordNet. Padó and Lapata (2005a) used word alignment in sentencealigned parallel corpora to find possible lexical units in new languages. There have been several studies of the feasibility of automatically producing the role-semantic annotation in new languages, although never for languages as structurally dissimilar as Swedish and Finnish. Padó and Lapata (2005b) projected annotation from English to German, and Johansson and Nugues (2006) implemented a complete pipeline for English–Swedish by (1) automatic annotation on the English side; (2) annotation transfer; and (3) training a Swedish semantic role labeler using the automatically produced annotation. 3 Frames from Swedish to Finnish 3.1 Outline of the Experiment We start off by locating such Swedish word senses that are both represented in SweFN and linked to PWN in two Finnish–Swedish parallel corpora. The sentences that include such a word make up the evaluation data set. After this, the Swedish half is enriched with frame labels using the framenet-based semantic role la"
W12-1902,H05-1108,0,0.022207,"were cross-linguistically meaningful, a number of interesting discrepancies were found. Whether the number of discrepancies is higher in a pair of more typologically different languages is an important question. As far as we are aware, there has been no previous attempt in using multilingual WordNets or similar lexicons when deriving lexical units in frames in new languages. The WordNet–FrameNet combination has seen some use in monolingual applications: for instance, Burchardt et al. (2005) and Johansson and Nugues (2007) attempted to extend the coverage of FrameNet by making use of WordNet. Padó and Lapata (2005a) used word alignment in sentencealigned parallel corpora to find possible lexical units in new languages. There have been several studies of the feasibility of automatically producing the role-semantic annotation in new languages, although never for languages as structurally dissimilar as Swedish and Finnish. Padó and Lapata (2005b) projected annotation from English to German, and Johansson and Nugues (2006) implemented a complete pipeline for English–Swedish by (1) automatic annotation on the English side; (2) annotation transfer; and (3) training a Swedish semantic role labeler using the a"
W12-1902,I11-1041,0,\N,Missing
W12-1902,N04-1043,0,\N,Missing
W12-1902,W12-1915,0,\N,Missing
W12-1902,koen-2004-pharaoh,0,\N,Missing
W12-1902,W04-3252,0,\N,Missing
W12-1902,nivre-etal-2006-talbanken05,0,\N,Missing
W12-1902,W12-1912,0,\N,Missing
W12-1902,J93-2004,0,\N,Missing
W12-1902,W10-2922,0,\N,Missing
W12-1902,N12-1052,0,\N,Missing
W12-1902,D10-1120,0,\N,Missing
W12-1902,W08-2112,0,\N,Missing
W12-1902,W07-0604,0,\N,Missing
W12-1902,D08-1016,0,\N,Missing
W12-1902,W03-0105,0,\N,Missing
W12-1902,N09-3017,0,\N,Missing
W12-1902,S07-1005,0,\N,Missing
W12-1902,H94-1048,0,\N,Missing
W12-1902,H01-1035,0,\N,Missing
W12-1902,W06-2920,0,\N,Missing
W12-1902,C96-2141,0,\N,Missing
W12-1902,C04-1192,0,\N,Missing
W12-1902,X96-1049,0,\N,Missing
W12-1902,C08-1042,0,\N,Missing
W12-1902,C10-2052,0,\N,Missing
W12-1902,C08-1113,0,\N,Missing
W12-1902,N06-1041,0,\N,Missing
W12-1902,E99-1010,0,\N,Missing
W12-1902,N09-1012,0,\N,Missing
W12-1902,P11-1048,0,\N,Missing
W12-1902,P11-1087,0,\N,Missing
W12-1902,D10-1056,0,\N,Missing
W12-1902,P02-1033,0,\N,Missing
W12-1902,P10-1131,0,\N,Missing
W12-1902,P08-1085,0,\N,Missing
W12-1902,D10-1117,0,\N,Missing
W12-1902,J94-2001,0,\N,Missing
W12-1902,P08-1086,0,\N,Missing
W12-1902,D11-1109,0,\N,Missing
W12-1902,J92-4003,0,\N,Missing
W12-1902,N07-1030,0,\N,Missing
W12-1902,N09-1010,0,\N,Missing
W12-1902,P05-1048,0,\N,Missing
W12-1902,A00-1031,0,\N,Missing
W12-1902,D07-1007,0,\N,Missing
W12-1902,P07-1005,0,\N,Missing
W12-1902,P06-1055,0,\N,Missing
W12-1902,P09-1057,0,\N,Missing
W12-1902,petrov-etal-2012-universal,0,\N,Missing
W12-1902,W03-0419,0,\N,Missing
W12-1902,P03-1058,0,\N,Missing
W12-1902,P11-2056,0,\N,Missing
W12-1902,P11-1138,0,\N,Missing
W12-1902,D11-1036,0,\N,Missing
W12-1902,D11-1006,0,\N,Missing
W12-1902,W07-2416,1,\N,Missing
W12-1902,P06-1124,0,\N,Missing
W12-1902,P08-1076,0,\N,Missing
W12-1902,P10-1040,0,\N,Missing
W12-1902,W12-1913,0,\N,Missing
W12-1902,D11-1005,0,\N,Missing
W12-1902,D12-1121,0,\N,Missing
W12-1902,W12-1914,0,\N,Missing
W12-1902,P09-1116,0,\N,Missing
W12-1902,P10-2036,0,\N,Missing
W12-1902,P07-1031,0,\N,Missing
W12-1902,P05-1001,0,\N,Missing
W12-1902,2005.mtsummit-papers.11,0,\N,Missing
W12-1902,majlis-zabokrtsky-2012-language,0,\N,Missing
W12-1902,W12-1910,0,\N,Missing
W12-1902,johansson-etal-2012-semantic,1,\N,Missing
W12-1902,U08-1016,0,\N,Missing
W12-1902,sekine-nobata-2004-definition,0,\N,Missing
W12-1902,D07-1096,0,\N,Missing
W12-1902,W99-0612,0,\N,Missing
W12-1902,erjavec-etal-2010-jos,0,\N,Missing
W12-1902,W04-3234,0,\N,Missing
W12-1902,D10-1083,0,\N,Missing
W12-1902,D07-1031,0,\N,Missing
W12-1902,C96-1079,0,\N,Missing
W12-1902,afonso-etal-2002-floresta,0,\N,Missing
W12-1902,W10-2608,0,\N,Missing
W12-1902,P07-1094,0,\N,Missing
W12-1902,P10-2040,0,\N,Missing
W12-1902,P02-1001,0,\N,Missing
W12-1902,P07-1033,0,\N,Missing
W12-1902,W09-3003,0,\N,Missing
W12-1902,P05-1044,0,\N,Missing
W12-1902,D07-1043,0,\N,Missing
W12-1902,W97-0309,0,\N,Missing
W12-1902,P10-2039,0,\N,Missing
W12-1902,W10-1701,0,\N,Missing
W12-1902,D11-1117,0,\N,Missing
W12-1902,N06-1020,0,\N,Missing
W12-1902,N07-1018,0,\N,Missing
W12-1902,D08-1036,0,\N,Missing
W12-1902,D11-1059,0,\N,Missing
W12-3614,P98-1013,0,0.00779948,"return S We omit the description of further implementation details. In particular, the fSUM and fMIN objectives can be computed by incremental updates, which speeds up their evaluation greatly. 3 A Case Study: Diversity and Relevance in a Lexicographic Project We applied the search result diversification method in a new annotation user interface used in the Swedish FrameNet (SweFN) project. This is a lexical resource under development (Borin et al., 2010; Friberg Heppin and Toporowska Gronostaj, 2012) that is based on the English version of FrameNet constructed by the Berkeley research group (Baker et al., 1998). It is found on the SweFN website1 , and is available as a free resource. All lexical resources 1 http://spraakbanken.gu.se/eng/swefn used for constructing SweFN are freely available for downloading. The lexicographers working in this project typically define frames that are fairly close in meaning to their counterparts in the Berkeley FrameNet. When a frame has been defined, lexical units are added. For each lexical unit, a set of example sentences are then selected from KORP, a collection of corpora of different types (Borin et al., 2012). Finally, the lexicographers annotate the frame elem"
W12-3614,W09-4407,0,0.0554959,"Missing"
W12-3614,C98-1013,0,\N,Missing
W12-3614,heppin-gronostaj-2012-rocky,1,\N,Missing
W14-1821,N07-1058,0,0.303068,"Missing"
W14-1821,borin-etal-2012-korp,0,0.0193208,"ts exercise generator module offers tasks both for students of linguistics and learners of L2 Swedish (Figure 1). Additional parts include a corpus editor used for the annotation of the CEFR corpus and the sentence selection module presented in this paper, Hit-Ex3 (Hitta Exempel, “Find Examples” or Hit Examples). The version under development contains also dictation and spelling exercises (Volodina et al., 2013). Resources Our sentence selection module utilizes a number of tools, resources and web services available for Swedish. Korp1 , an infrastructure for accessing and maintaining corpora (Borin et al., 2012), contains a large number of Swedish texts which are equipped with automatic annotations (with some exceptions) for part-of-speech (POS), syntactic (dependency) relations, lemma forms and sense ids. Korp offers, among others, a web service for concordances, which makes a search in corpora based on a query (e.g. a keyword and its POS) and returns hits with a sentence-long context. Moreover, with the corpus pipeline of Korp, tools for automatically annotating corpora are also available. A variety of different modern Swedish corpora from Korp have been used throughout this study including novel,"
W14-1821,N04-1025,0,0.139713,"readability measures for L1 Swedish at the text level include LIX (L¨asbarthetsindex, “Readability index”) (Bj¨ornsson, 1968) and the Nominal Ratio (Hultman and Westman, 1977). In recent years a number of studies, mostly focusing on the L1 context, appeared which take into consideration linguistic features based on a deeper text processing. Morphosyntactic aspects informative for L1 readability include, among others, parse tree depth, subordination features and dependency link depth (length) (Dell’Orletta et al., 2011). Language models have also been commonly used for readability predictions (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005). A recently proposed measure, the Coh-Metrix (Graesser et al., 2011), aims at a multilevel analysis of texts, inspired by psycholinguistic principles. It measures not only linguistic difficulty, but also cohesion in texts. Research on L1 readability for Swedish, using machine learning, is described in Heimann M¨uhlenbock (2013) and Falkenjack et al. (2013). Heimann M¨uhlenbock (2013) examined readability along five dimensions: 2.2 Sentence-level readability Many of the text readability measures mentioned above have shortcomings when used on very short passages co"
W14-1821,W11-2308,0,0.0325955,"Missing"
W14-1821,W13-5608,0,0.0691748,"Missing"
W14-1821,P05-1065,0,0.294165,"at the text level include LIX (L¨asbarthetsindex, “Readability index”) (Bj¨ornsson, 1968) and the Nominal Ratio (Hultman and Westman, 1977). In recent years a number of studies, mostly focusing on the L1 context, appeared which take into consideration linguistic features based on a deeper text processing. Morphosyntactic aspects informative for L1 readability include, among others, parse tree depth, subordination features and dependency link depth (length) (Dell’Orletta et al., 2011). Language models have also been commonly used for readability predictions (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005). A recently proposed measure, the Coh-Metrix (Graesser et al., 2011), aims at a multilevel analysis of texts, inspired by psycholinguistic principles. It measures not only linguistic difficulty, but also cohesion in texts. Research on L1 readability for Swedish, using machine learning, is described in Heimann M¨uhlenbock (2013) and Falkenjack et al. (2013). Heimann M¨uhlenbock (2013) examined readability along five dimensions: 2.2 Sentence-level readability Many of the text readability measures mentioned above have shortcomings when used on very short passages containing 100 words or less (Ki"
W14-1821,D12-1043,0,0.211254,"Missing"
W14-1821,W13-2904,0,0.0290945,"Missing"
W14-1821,W12-2019,0,0.487187,"us research have explored this latter dimension for Swedish before, hence we aim at filling this gap, which can be useful, besides the purposes mentioned above, also in future sentence and text simplification and adaptation tasks. We propose a rule-based as well as a combination of rule-based and machine learning methods for the identification of sentences understandable by L2 learners and suitable as exercise items. During the selection of linguistic indicators, we have taken into consideration previously studied features of readability (Franc¸ois and Fairon, 2012; Heimann M¨uhlenbock, 2013; Vajjala and Meurers, 2012), L2 Swedish curricula (Levy Scherrer and Lindemalm, 2009; Folkuniversitet, 2013) and aspects of Good Dictionary Examples (GDEX) We present approaches for the identification of sentences understandable by second language learners of Swedish, which can be used in automatically generated exercises based on corpora. In this work we merged methods and knowledge from machine learning-based readability research, from rule-based studies of Good Dictionary Examples and from second language learning syllabuses. The proposed selection methods have also been implemented as a module in a free web-based la"
W14-1821,E14-1031,0,0.0184221,"n and underlines the importance of syntactic complexity. Research about ranking Swedish corpus examples is presented in Volodina et al. (2012b). Their first algorithm includes four heuristic rules concerning sentence length, infrequent lexical items, keyword position and the presence of finite verbs, complemented by a sentence similarity measure in the second algorithm. Readability experiments focusing at the sentence level have started to appear recently both for language learning purposes (Pil´an et al., 2013) and for detecting differences between simplified and unsimplified sentence pairs (Vajjala and Meurers, 2014). 3 (b) automatic linguistic annotations obtained with the annotation tools available through Korp. The CEFR corpus at the time of writing included B1 texts from three course books and B2 texts from one course book. The annotation of additional material covering other CEFR levels was ongoing. Not only corpora, but also information from frequency word lists has been used for determining the appropriateness of a sentence. The Kelly list (Volodina and Kokkinakis, 2012) is a frequencybased vocabulary list mostly built on a corpus of web texts from 2010. Besides frequency information, an associated"
W14-1821,volodina-kokkinakis-2012-introducing,1,0.827123,"Missing"
W15-1504,J90-1003,0,0.537104,"ty, see (2), to weight context words by semantic = ψi,c 1 , 1 + e−vcT ui (4) where vc is the context-word embedding for the word c, and ui is the word embedding of target word i. Using ψ semantic in (3) has the effect of assigning bigger importance to context words that have a semantic relation to the target word. Context words that are not useful in characterizing the sense of the target are weighted less. This is in stark contrast to the uniform weighting schema. Levy and Goldberg (2014) discovered an interesting connection between the Skip-gram model and Pointwise Mutual Information (PMI) (Church and Hanks, 1990). Consider the optimizers of the Skipgram objective, word and context-word embeddings, ui , vc , trained using k negative samples. Levy and Goldberg showed that for sufficiently large dimensionality, these vectors satisfy the following relation, ui T vc = P M I(wi , wc ) − log k . Let σ(·) be the logistic function. For vectors satisfying semantic = the conditions stated above, we have ψi,c σ(P M I(wi , wc )−log k) , establishing a connection between the semantic weights applied to Skip-gram embeddings, and PMI, a function frequently used for measuring word similarity (Pantel and Lin, 2002). 3."
W15-1504,P12-1092,0,0.228638,"unimelb. AIKU (Baskaya et al., 2013) uses an approach based on substitute word vectors, inferred using a statistical language model. AI-KU achieved the highest FNMI score of the systems submitted to SemEval-2013. unimelb (Lau et al., 2013), who achieved the highest FBC score at SemEval-2013, is a system based on the topic model Latent Dirichlet Allocation and its non-parametric equivalent, Hierarchical Dirichlet Processes. Word instances are clustered based on the topic distributions inferred by the model. The related problem of training neural embeddings of polysemous words was addressed by Huang et al. (2012) and subsequently by Neelakantan et al. (2014) with the model Multi-sense Skip-gram (MSSG), see Section 2.2. As a second experiment we extend MSSG for WSI. MSSG has not previously been used for WSI, however it produces one word embedding for each word sense, and performs a simple disambiguation procedure during training. MSSG is thus a natural candidate for comparison. We use the standard variant of MSSG, as it achieved the best overall results in the original paper Neelakantan et al. (2014). MSSG disambiguates instances by assigning them to the sense with embedding closest to the average cont"
W15-1504,S13-2049,0,0.218127,", after embedding, we refer to this method as ICE-kmeans. The second method is an extension of the MSSG model (Neelakantan et al., 2014), in which we during training of the model embed word instances using ICE. This improves the disambiguation performed at every iteration of MSSG. As this method performs the clustering in an online fashion, we refer to this method as ICE-online. For this, we have modified the code provided by Jeevan Shankar1 . 5 Evaluation We evaluate our methods for word sense induction on shared task 13 of SemEval-2013, Word Sense Induction for Graded and Non-Graded Senses (Jurgens and Klapaftis, 2013). Henceforth, we let “SemEval-2013” refer to this specific task. We also investigate the influence of our weighting schema on both methods. Further, we study qualitative properties of the word instance embeddings produced by our method. 1 https://bitbucket.org/jeevan shankar/multi-senseskipgram/ 5.1 SemEval-2013, Task 13 The SemEval-2013 (test) data contains 4664 instances, each inflections of one of 50 lemmas (Jurgens and Klapaftis, 2013). The competition included both single-sense instances and instances with a graded mixture of senses. Because the manual annotations were deemed too poor, on"
W15-1504,S13-2051,0,0.0269621,"s sensitive to irrelevant context words. For the MSSG part of ICE-online, we use the parameters reported in Neelakantan et al. (2014). 5.3 Current state-of-the-art We compare the perfomance of our system to that of state-of-the-art systems for WSI. First, we compare to the systems with the current best results on SemEval 2013 task 13 for singlesense word instances, AI-KU and unimelb. AIKU (Baskaya et al., 2013) uses an approach based on substitute word vectors, inferred using a statistical language model. AI-KU achieved the highest FNMI score of the systems submitted to SemEval-2013. unimelb (Lau et al., 2013), who achieved the highest FBC score at SemEval-2013, is a system based on the topic model Latent Dirichlet Allocation and its non-parametric equivalent, Hierarchical Dirichlet Processes. Word instances are clustered based on the topic distributions inferred by the model. The related problem of training neural embeddings of polysemous words was addressed by Huang et al. (2012) and subsequently by Neelakantan et al. (2014) with the model Multi-sense Skip-gram (MSSG), see Section 2.2. As a second experiment we extend MSSG for WSI. MSSG has not previously been used for WSI, however it produces on"
W15-1504,N13-1090,0,0.35013,"mmon approach used in several successful WSI systems is to apply this geometric intuition and represent each context of a polysemous word as a vector, look for coherent clusters in the set of context vectors, and let these define the senses of the word. This approach was pioneered by Sch¨utze (1998) using second order co-occurrences to construct the context representation. It is clear that in order to be useful in a WSI system, a geometric representation of context meaning must be designed in a way that makes clusters distinct. Recently, neural embeddings, such as the popular Skip-gram model (Mikolov et al., 2013a), have proven efficient and accurate in the task of embedding words in vector spaces. As of yet, however, neural embeddings have not been considered for representing contexts in WSI. The systems that seem 25 Proceedings of NAACL-HLT 2015, pages 25–32, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics most relevant in this context are those that train multi-prototype embeddings: more than one embedding per word (Huang et al., 2012). In particular, Neelakantan et al. (2014) described a modified Skipgram algorithm that clusters instances on the fly, effec"
W15-1504,P14-1096,0,0.0354012,"Missing"
W15-1504,D14-1113,0,0.571017,"s clusters distinct. Recently, neural embeddings, such as the popular Skip-gram model (Mikolov et al., 2013a), have proven efficient and accurate in the task of embedding words in vector spaces. As of yet, however, neural embeddings have not been considered for representing contexts in WSI. The systems that seem 25 Proceedings of NAACL-HLT 2015, pages 25–32, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics most relevant in this context are those that train multi-prototype embeddings: more than one embedding per word (Huang et al., 2012). In particular, Neelakantan et al. (2014) described a modified Skipgram algorithm that clusters instances on the fly, effectively training several vectors per word. However, whether this or any other similar approach is useful if considered as a WSI system is still an open question, since they have never been evaluated in that setting. We make the following contributions: (1) We define the Instance-context embedding (ICE), a novel way for representing word instances and their context. ICE combines vectors representing context words using a novel weighting schema consisting of a semantic component, and a temporal component, see Sectio"
W15-1504,J98-1004,0,0.807721,"Missing"
W15-1804,P04-1041,0,0.106389,"Missing"
W15-1804,de-marneffe-etal-2014-universal,0,0.0519921,"Missing"
W15-1804,A97-1014,0,0.28162,"ags, we link words to the large-scale semantic Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 2 lexicon SALDO (Borin et al., 2013), which provides us with a lemma, the inflectional pattern and a sense distinction. We also follow SALDO in assuming that there is a multiword counterpart to each of the parts-of-speech. In the Koala syntax annotation schema, these multiword expressions reside between the lexical and the phrasal levels. 4 4.1 Syntactic annotation Formalism The syntactic structures in the Koala annotation schema follow the format introduced in Skut et al. (1997). It uses rooted trees, the ‘primary graph’, with additional, ‘secondary’, edges. All tokens part of the syntactic structure must occur as leaf nodes in the primary graph. Internal nodes in the primary graph represent phrases or (in our schema) multiword expressions. Unlike traditional phrase structure trees, linear order is not part of the encoding and phrases may be discontinuous. Word order variants therefore need not lead to different trees. Edges, primary as well as secondary, carry grammatical function labels. Secondary edges are used for various kinds of sharing of syntactic material. W"
W15-1804,nivre-etal-2006-talbanken05,0,0.102229,"ndencies. The source material has been taken from public sources, to allow the resulting corpus to be made freely available. 1 Introduction Corpora annotated with part-of-speech tags and syntactic structure are crucial for the development and evaluation of automatic tools for syntactic analysis, as well as for empirical research in syntax. For Swedish, annotated corpora have been available for quite a number of years. The venerable MAMBA treebank (Teleman, 1974) was created in the 1970s. It has formed the basis for a number of Swedish constituency and dependency treebanks such as Talbanken05 (Nivre et al., 2006), the more recent Swedish Treebank, and the Swedish part of the multilingual Universal Dependency Treebank (de Marneffe et al., 2014). The Stockholm–Umeå Corpus (SUC) (Ejerhed et al., 1992) with manually checked part-of-speech tags and base forms for roughly a million tokens, has been a de facto standard for Swedish part-of-speech tagging. The Swedish Treebank uses the SUC part-of-speech tags together with the automatically converted syntactic structures from MAMBA (Nivre et al., 2008). In our project Koala, we develop new annotation tools to be used for the multi-billion token corpora of Korp"
W15-1811,W15-1804,1,0.71819,"Missing"
W15-1811,E09-1005,0,0.464596,"phical perspective. We applied the algorithm to derive vector representations for the senses in SALDO, a Swedish semantic network (Borin et al., 2013), and we used these vectors to build a disambiguation system that can assign a SALDO sense to ambiguous words occurring in free text. To evaluate the system, we created two new benchmark sets by processing publicly available datasets. On these benchmarks, our system outperforms a random baseline by a wide margin, but also a first-sense baseline significantly. It achieves a slightly higher score than UKB, a highly accurate graph-based WSD system (Agirre and Soroa, 2009), but is several orders of magnitude faster. The highest disambiguation accuracy was achieved by combining the probabilities output by the two systems. Furthermore, in a qualitative inspection of the most ambiguous words in SALDO for each word class, we see that the sense distribution estimates provided by the sense embedding algorithm are good for nouns, adjectives, and adverbs, although less so for verbs. 2 Representing the meaning of words and senses In NLP, the idea of representing word meaning geometrically is most closely associated with the distributional approach: the meaning of a word"
W15-1811,P10-2017,0,0.0646423,"Missing"
W15-1811,heppin-gronostaj-2012-rocky,0,0.138296,"Missing"
W15-1811,D15-1292,0,0.0344915,"Missing"
W15-1811,P12-1092,0,0.0344305,"tic representation from a sense-annotated corpus, but this is infeasible since fairly large corpora are needed to induce data-driven representations of a high quality, while sense-annotated corpora are small and scarce. Instead, there have been several attempts to create vectors representing the senses of ambiguous words, most of them based on some variant of the idea first proposed by Sch¨utze (1998): that senses can be seen as clusters of similar contexts. Further examples where this idea has reappeared include the work by Purandare and Pedersen (2004), as well as a number of recent papers (Huang et al., 2012; Moen et al., 2013; Neelakantan et al., 2014; K˚ageb¨ack et al., 2015). However, sense distributions are often highly imbalanced, it is not clear that context clusters can be reliably created for senses that occur rarely. In this work, we build a word sense disambiguation system by combining the two approaches to representing meaning. The crucial stepping stone is the recently developed algorithm by Johansson and Nieto Pi˜na (2015), which derives vector-space representations of word senses by embedding the graph structure of a semantic network in the word vector space. A scoring function for"
W15-1811,N15-1164,1,0.874954,"Missing"
W15-1811,W15-1504,1,0.876226,"Missing"
W15-1811,W14-1618,0,0.0379168,"at they typically have a wide vocabulary coverage makes it attractive to integrate them in NLP systems for additional robustness (Turian et al., 2010). However, there are many reasons to study how these two very dissimilar approaches can complement each other. Mikolov et al. (2013c) showed that vector spaces represent more structure than previously thought: they implicitly encode a wide range of syntactic and semantic relations, which can be recovered using simple linear algebra operations. For instance, the geometric relation between Rome and Italy is similar to that between Cairo and Egypt. Levy and Goldberg (2014) further analyzed how this property can be explained. One aspect where symbolic representations seem to have an advantage is in describing word sense ambiguity: the fact that one surface form Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 69 may correspond to more than one underlying concept. For instance, the word mouse can refer to a rodent or an electronic device. Except for scenarios where a small number of senses are used, lexical-semantic resources such as WordNet (Fellbaum, 1998) for English and SALDO (Borin et al., 2013) for Swedish are crucial i"
W15-1811,J07-4005,0,0.684794,"function used to construct the original word-based vector space. This approach to WSD is attractive because it can leverage corpus statistics similar to a supervised method trained on an annotated corpus, but also use the lexical1 According to Gyllensten and Sahlgren (2015), this problem can be remedied by making better use of the topology of the neighborhood around the search term. semantic resource for generalization. Moreover, the sense representation algorithm also estimates how common the different senses are; finding the predominant sense of a word also gives a strong baseline for WSD (McCarthy et al., 2007), and is of course also interesting from a lexicographical perspective. We applied the algorithm to derive vector representations for the senses in SALDO, a Swedish semantic network (Borin et al., 2013), and we used these vectors to build a disambiguation system that can assign a SALDO sense to ambiguous words occurring in free text. To evaluate the system, we created two new benchmark sets by processing publicly available datasets. On these benchmarks, our system outperforms a random baseline by a wide margin, but also a first-sense baseline significantly. It achieves a slightly higher score"
W15-1811,N13-1090,0,0.713529,"geometry of the vector space, e.g. by defining a distance metric. These two broad frameworks obviously have very different advantages: while the symbolic representations contain explicit and very detailed relational information, the data-driven representations handle the notion of graded similarity in a very natural way, and the fact that they typically have a wide vocabulary coverage makes it attractive to integrate them in NLP systems for additional robustness (Turian et al., 2010). However, there are many reasons to study how these two very dissimilar approaches can complement each other. Mikolov et al. (2013c) showed that vector spaces represent more structure than previously thought: they implicitly encode a wide range of syntactic and semantic relations, which can be recovered using simple linear algebra operations. For instance, the geometric relation between Rome and Italy is similar to that between Cairo and Egypt. Levy and Goldberg (2014) further analyzed how this property can be explained. One aspect where symbolic representations seem to have an advantage is in describing word sense ambiguity: the fact that one surface form Proceedings of the 20th Nordic Conference of Computational Lingui"
W15-1811,W13-3210,0,0.0420982,"Missing"
W15-1811,J07-2002,0,0.0159751,"Missing"
W15-1811,W04-2406,0,0.050357,". One straightforward solution could be to build a vector-space semantic representation from a sense-annotated corpus, but this is infeasible since fairly large corpora are needed to induce data-driven representations of a high quality, while sense-annotated corpora are small and scarce. Instead, there have been several attempts to create vectors representing the senses of ambiguous words, most of them based on some variant of the idea first proposed by Sch¨utze (1998): that senses can be seen as clusters of similar contexts. Further examples where this idea has reappeared include the work by Purandare and Pedersen (2004), as well as a number of recent papers (Huang et al., 2012; Moen et al., 2013; Neelakantan et al., 2014; K˚ageb¨ack et al., 2015). However, sense distributions are often highly imbalanced, it is not clear that context clusters can be reliably created for senses that occur rarely. In this work, we build a word sense disambiguation system by combining the two approaches to representing meaning. The crucial stepping stone is the recently developed algorithm by Johansson and Nieto Pi˜na (2015), which derives vector-space representations of word senses by embedding the graph structure of a semantic"
W15-1811,J98-1004,0,0.724961,"Missing"
W15-1811,P10-1040,0,0.28044,"s typically similarity: a mouse is something quite similar to a rat. Similarity of meaning is often operationalized in terms of the geometry of the vector space, e.g. by defining a distance metric. These two broad frameworks obviously have very different advantages: while the symbolic representations contain explicit and very detailed relational information, the data-driven representations handle the notion of graded similarity in a very natural way, and the fact that they typically have a wide vocabulary coverage makes it attractive to integrate them in NLP systems for additional robustness (Turian et al., 2010). However, there are many reasons to study how these two very dissimilar approaches can complement each other. Mikolov et al. (2013c) showed that vector spaces represent more structure than previously thought: they implicitly encode a wide range of syntactic and semantic relations, which can be recovered using simple linear algebra operations. For instance, the geometric relation between Rome and Italy is similar to that between Cairo and Egypt. Levy and Goldberg (2014) further analyzed how this property can be explained. One aspect where symbolic representations seem to have an advantage is i"
W15-2001,borin-etal-2014-bring,1,0.84875,"Missing"
W15-2001,O95-1004,0,0.371098,"ting the production of a Swedish counterpart of Roget with a large and upto-date vocabulary coverage. This is not to be done by translation, as in previous work by de Melo and Weikum (2008) and Borin et al. (2014). Instead, an existing but largely outdated Roget-style thesaurus will provide the scaffolding, where new word senses can be inserted with the help of two different kinds of semantic relatedness measures: as the PWN, the digital version of Roget offers a valuable complement to PWN (Jarmasz and Szpakowicz, 2004), which has seen a fair amount of use in NLP (e.g., Morris and Hirst 1991; Jobbins and Evett 1995; Jobbins and Evett 1998; Wilks 1998; Kennedy and Szpakowicz 2008). It has been proposed in the literature that Roget-style thesauruses could provide an alternative source of lexical-semantic information, which can be used both to attack other kinds of NLP tasks than a wordnet, and even work better for some of the same tasks, e.g., lexical cohesion, synonym identification, pseudo-word-sense disambiguation, and analogy problems (Morris and Hirst, 1991; Jarmasz and Szpakowicz, 2004; Kennedy and Szpakowicz, 2008; Kennedy and Szpakowicz, 2014). An obstacle to the wider use of Roget in NLP applicat"
W15-2001,P98-1100,0,0.289133,"Swedish counterpart of Roget with a large and upto-date vocabulary coverage. This is not to be done by translation, as in previous work by de Melo and Weikum (2008) and Borin et al. (2014). Instead, an existing but largely outdated Roget-style thesaurus will provide the scaffolding, where new word senses can be inserted with the help of two different kinds of semantic relatedness measures: as the PWN, the digital version of Roget offers a valuable complement to PWN (Jarmasz and Szpakowicz, 2004), which has seen a fair amount of use in NLP (e.g., Morris and Hirst 1991; Jobbins and Evett 1995; Jobbins and Evett 1998; Wilks 1998; Kennedy and Szpakowicz 2008). It has been proposed in the literature that Roget-style thesauruses could provide an alternative source of lexical-semantic information, which can be used both to attack other kinds of NLP tasks than a wordnet, and even work better for some of the same tasks, e.g., lexical cohesion, synonym identification, pseudo-word-sense disambiguation, and analogy problems (Morris and Hirst, 1991; Jarmasz and Szpakowicz, 2004; Kennedy and Szpakowicz, 2008; Kennedy and Szpakowicz, 2014). An obstacle to the wider use of Roget in NLP applications is its limited avai"
W15-2001,N15-1164,1,0.849372,"Missing"
W15-2001,de-melo-weikum-2008-mapping,0,0.229925,"Missing"
W15-2001,W10-2803,0,0.0155292,"f having been assigned only one SALDO word sense – Bring lemmaPOS combinations that appear in multiple Bring classes. Second, during the practical disambiguation work conducted in order to prepare the evaluation dataset for the experiments described below, the typical case was not – as would have been expected if the above assumption were correct – that ambiguous items occurring in several Bring classes would receive different word sense assignments. On the contrary, this turned out to be very much a minor phenomenon. A “word sense” is not a well-defined notion (Kilgarriff, 1997; Hanks, 2000; Erk, 2010; Hanks, 2013), and it may well be simply that this is what we are seeing here. Specifically, the Swedish lexicographical tradition to which SALDO belongs reflects a “lumping” view on word sense discrimination. If we aspire to link resources such as Roget, Bring, SALDO, etc. between languages, issues such as this need to be resolved one way or another, so there is clearly need for more research here. broms-2 ‘horsefly’, but it is only the second sense that should be listed in this Bring class. In this work we consider the task of selecting a SALDO sense for a Bring entry, but we imagine that t"
W15-2001,W98-0710,0,0.112152,"of terms related to animals. SALDO defines two senses for this word: broms-1 ‘brake’ and Proceedings of the workshop on Semantic resources and semantic annotation for Natural Language Processing and the Digital Humanities at NODALIDA 2015 4 as well as phrases and clauses. This in turn determines the granularity – the degree of polysemy – posited for lexical entries. One thing that seems to be assumed about Roget – and which if true consequently ought to hold for Bring as well – is that multiple occurrences of the same lemma (with the same part of speech) represent different word senses (e.g., Kwong 1998; Nastase and Szpakowicz 2001). This is consistent with a “splitting” approach to polysemy, similar to that exhibited by PWN and more generally by an Anglo-Saxon lexicographical tradition. However, this is not borne out by the Bring– SALDO linking. First, there are many unambiguous – in the sense of having been assigned only one SALDO word sense – Bring lemmaPOS combinations that appear in multiple Bring classes. Second, during the practical disambiguation work conducted in order to prepare the evaluation dataset for the experiments described below, the typical case was not – as would have bee"
W15-2001,W15-2000,0,0.271331,"Missing"
W15-2001,P94-1019,0,0.254815,"t φ (fil-4) = { fil-4 ‘(computer) file (n)’, datorminne-1 ‘computer memory (n)’, datalagring-1 ‘data storage (n)’, lagring-1 ‘storage (n)’, lagra-1 ‘store (v)’, lager-2 ‘stock/store (n)’, f¨orr˚ad-1 ‘store (n)’, f¨orvara-1 ‘store/keep (v)’, ha-1 ‘have (v)’ }. Computationally, these sets are implemented as high-dimensional sparse vectors, which we normalize to unit length. Although in this work we do not explicitly use the notion of similarity functions, we note that the cosine similarity applied to this representation gives rise to a network-based measure similar in spirit to that proposed by Wu and Palmer (1994): sim(s1 , s2 ) = p 3.2 Disambiguating by comparing to a prototype The fact that corpus-based representations for SALDO senses are located in a real-valued vector space allows us to generate a prototype for a certain Bring conceptual class by means of averaging the sense vectors belonging to a that class in Bring. This prototype is in the same vector space that the sense representations, so we are able to measure distances between sense vectors and prototypes and determine which sense is closer to the concept embodied in the class prototype. Thus, our first method for disambiguating links betw"
W15-2001,J91-1002,0,\N,Missing
W15-2001,P08-1048,0,\N,Missing
W15-2001,C98-1097,0,\N,Missing
W16-1401,E09-1005,0,0.747231,"trained on annotated corpora (Taghipour and Ng, 2015), but because of the difficulty of the sense annotation task (Artstein and Poesio, 2008), the luxury of supervised training is available for a few languages only. An approach that circumvents the lack of annotated corpora is to take advantage of the information available in lexical knowledge bases (LKBs) like WordNet (Miller, 1995; Fellbaum, 1998). This kind of resource encodes word sense lexicons as graphs connecting lexically and semantically related concepts. Several methods are available that use LKBs for WSD (Navigli and Lapata, 2007; Agirre and Soroa, 2009). These approaches usually apply a relatively complex analysis of the underlying graph based on the context of a target word to disambiguate it; e.g., Agirre and Soroa (2009) use the Personalized PageRank algorithm to perform walks on the graph. However, these methods are computationally very costly, which makes them practically useless for large corpora. In this paper, we investigate a more time-efficient approach to graph-based WSD. We represent the concepts in the LKB by training vector space models on synthetic datasets created using random walks on the LKB’s graph. These synthetic dataset"
W16-1401,J08-4004,0,0.0762279,"ver the sense graph. We used this method to build a WSD system for Swedish using the SALDO lexicon, and evaluated it on six different annotated test sets. In all cases, our system was several orders of magnitude faster than a state-of-the-art PageRank-based system, while outperforming a random baseline soundly. 1 Introduction Word sense disambiguation (WSD) is a difficult task for automatic systems (Navigli, 2009). The most accurate WSD systems build on supervised learning models trained on annotated corpora (Taghipour and Ng, 2015), but because of the difficulty of the sense annotation task (Artstein and Poesio, 2008), the luxury of supervised training is available for a few languages only. An approach that circumvents the lack of annotated corpora is to take advantage of the information available in lexical knowledge bases (LKBs) like WordNet (Miller, 1995; Fellbaum, 1998). This kind of resource encodes word sense lexicons as graphs connecting lexically and semantically related concepts. Several methods are available that use LKBs for WSD (Navigli and Lapata, 2007; Agirre and Soroa, 2009). These approaches usually apply a relatively complex analysis of the underlying graph based on the context of a target"
W16-1401,D14-1110,0,0.0545012,"Missing"
W16-1401,N15-1165,0,0.118456,"Missing"
W16-1401,W15-1811,1,0.806231,"Missing"
W16-1401,N15-1164,1,0.891117,"Missing"
W16-1401,L16-1482,1,0.828086,"ecf. 3.2 Evaluation Corpora For development and evaluation, we used six different collections of sense-annotated examples. The first two, the SALDO examples (SALDO-ex) and Swedish FrameNet examples (SweFN-ex) consist of sentences selected by lexicographers to exemplify the senses (Johansson and Nieto Pi˜na, 2015a). The former is dominated by the most frequent verbs, while the latter has a more even distribution. In our experiments, these two collections were used as a development set to tune the system’s parameters. The additional four collections are taken from an ongoing annotation project (Johansson et al., 2016); each collection corresponds to a domain: blogs, novels, Wikipedia, and Europarl (Koehn, 2005). Unlike the two collections mentioned above, in which the instances have been selected by lexicographers to be prototypical and to have a good coverage of the sense variation, the instances in these four collections are sampled uniformly from running text. Corpus SALDO-ex SweFN-ex Blogs Europarl Novels Wikipedia Size 1055 1309 1014 1282 1204 1311 n ¯s 3.1 2.9 2.9 2.7 3.0 2.7 Table 1: Evaluation corpus statistics. We preprocessed the examples in the six collections to tokenize, compound-split, and le"
W16-1401,2005.mtsummit-papers.11,0,0.00677388,"otated examples. The first two, the SALDO examples (SALDO-ex) and Swedish FrameNet examples (SweFN-ex) consist of sentences selected by lexicographers to exemplify the senses (Johansson and Nieto Pi˜na, 2015a). The former is dominated by the most frequent verbs, while the latter has a more even distribution. In our experiments, these two collections were used as a development set to tune the system’s parameters. The additional four collections are taken from an ongoing annotation project (Johansson et al., 2016); each collection corresponds to a domain: blogs, novels, Wikipedia, and Europarl (Koehn, 2005). Unlike the two collections mentioned above, in which the instances have been selected by lexicographers to be prototypical and to have a good coverage of the sense variation, the instances in these four collections are sampled uniformly from running text. Corpus SALDO-ex SweFN-ex Blogs Europarl Novels Wikipedia Size 1055 1309 1014 1282 1204 1311 n ¯s 3.1 2.9 2.9 2.7 3.0 2.7 Table 1: Evaluation corpus statistics. We preprocessed the examples in the six collections to tokenize, compound-split, and lemmatize the texts, and to determine the set of possible senses in a given context. We used cont"
W16-1401,P14-2050,0,0.0482482,"architecture, which has also been extended to handle word sense representation (Neelakantan et al., 2014; Chen et al., 2014; Johansson and Nieto Pi˜na, 2015b; Nieto Pi˜na and Johansson, 2015). Our aim in this paper is to build graph-based word sense embeddings and apply them to the task of WSD as follows: Given a sentence with an ambiguous word, we can then compare the representation of its context words with each of the ambiguous word’s sense representations to decide which of them fits the context better. For this purpose we use a modified version of the original Skip-gram implementation by Levy and Goldberg (2014), word2vecf, which specifies separate target and context vocabularies, making it possible to represent word senses as targets while keeping the context vocabulary restricted to word forms. 2.2 Random walks as contexts Given a node in a graph G, a random walk generates a random sequence of interconnected nodes by selecting randomly from the edges of the current node at each step. The length of the random walk is controlled by a stop probability ps . I.e., at each node visited in the walk, the probability of stopping is ps ; if the walk does not stop, one of the node’s edges is followed to inclu"
W16-1401,D14-1113,0,0.10374,"geometrical distance measure. These representations are trained to, given a word, predict its context; the training algorithm, thus, works with two separate vector spaces in which context and target words are represented. 1 Proceedings of the 2016 Workshop on Graph-based Methods for Natural Language Processing, NAACL-HLT 2016, pages 1–5, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics Skip-gram introduced a highly efficient approach to language modeling using a shallow neural architecture, which has also been extended to handle word sense representation (Neelakantan et al., 2014; Chen et al., 2014; Johansson and Nieto Pi˜na, 2015b; Nieto Pi˜na and Johansson, 2015). Our aim in this paper is to build graph-based word sense embeddings and apply them to the task of WSD as follows: Given a sentence with an ambiguous word, we can then compare the representation of its context words with each of the ambiguous word’s sense representations to decide which of them fits the context better. For this purpose we use a modified version of the original Skip-gram implementation by Levy and Goldberg (2014), word2vecf, which specifies separate target and context vocabularies, making it"
W16-1401,R15-1061,1,0.870155,"Missing"
W16-1401,K15-1037,0,0.0311104,"context embeddings are constructed by applying the Skip-gram method to random walks over the sense graph. We used this method to build a WSD system for Swedish using the SALDO lexicon, and evaluated it on six different annotated test sets. In all cases, our system was several orders of magnitude faster than a state-of-the-art PageRank-based system, while outperforming a random baseline soundly. 1 Introduction Word sense disambiguation (WSD) is a difficult task for automatic systems (Navigli, 2009). The most accurate WSD systems build on supervised learning models trained on annotated corpora (Taghipour and Ng, 2015), but because of the difficulty of the sense annotation task (Artstein and Poesio, 2008), the luxury of supervised training is available for a few languages only. An approach that circumvents the lack of annotated corpora is to take advantage of the information available in lexical knowledge bases (LKBs) like WordNet (Miller, 1995; Fellbaum, 1998). This kind of resource encodes word sense lexicons as graphs connecting lexically and semantically related concepts. Several methods are available that use LKBs for WSD (Navigli and Lapata, 2007; Agirre and Soroa, 2009). These approaches usually appl"
W16-4807,W14-5316,0,0.0649194,"Missing"
W16-4807,P98-1002,0,0.0156317,"a set, we removed 500 documents (around 6,000 words) for each language to be used in training and evaluating our system. We used the rest of the data to compile lexicons, for each language, by extracting the unique vocabulary using a script. We also used external lexicon for RB. 10 A standard metric used to evaluate the quality of a set of annotations in classification tasks. This term refers to the use of more than one language in a single interaction. The classic code-switching framework does not always apply to Arabic for many complex reasons which are out of our scope. Researchers like D. Sankoff (1998) suggested to classify the use of mixed languages in Arabic as a separate phenomenon and not code-switching. Others like Davies et al. (2013) called it ’mixed Arabic’. We will use ’language mixing’ to refer to both code-switching and borrowing. 12 Documents containing vocabulary of different languages. In our case, Arabic written in Latin script plus Berber, English, French, German, Spanish and Swedish words. 13 Including Algerian, Egyptian, Gulf, Kuwaiti/Iraqi, Levantine, Moroccan and Tunisian Arabic. 14 Being familiar with north African Arabic dialects, we have noticed that Maltese is much c"
W16-4807,W14-3629,0,0.0673557,"Missing"
W16-4807,P12-3005,0,0.096452,"Missing"
W16-4807,P11-2007,0,0.0807122,"Missing"
W16-4807,W14-3901,0,0.266654,"Missing"
W16-4809,L16-1284,0,0.0241158,"Missing"
W16-4809,W14-5904,0,0.111254,"Missing"
W16-4809,P13-2081,0,0.201674,"Missing"
W16-4809,bouamor-etal-2014-multidialectal,0,0.0991641,"Missing"
W16-4809,J96-2004,0,0.0914454,"Missing"
W16-4809,J14-1006,0,0.116132,"Missing"
W16-4809,W16-4801,0,0.0496792,"Missing"
W16-4809,W15-5410,1,0.828429,"Missing"
W16-4821,P13-2081,0,0.0320962,"ALI have been applied to the DSL and DLV tasks for some languages. Goutte et al. (2016) give a comprehensive bibliography of the recently published papers dealing with these tasks. Discriminating between Arabic varieties is also an active research area although limited work has been done, so far, to distinguish between written Arabic varieties. The main reason is the lack of annotated data (Benajiba and Diab, 2010). Zaidan (2012) in his PhD distinguished between four Arabic varieties (Modern Standard Arabic (MSA), Egyptian, Gulf and Levantine dialects) using character and word n-gram models. Elfardy and Diab (2013) identified MSA from Egyptian at a sentence level, Tillmann et al. (2014) proposed an approach to improve classifying Egyptian and MSA at a sentence level, and Saˆadane (2015) in her PhD distinguished between Maghrebi Arabic (Algerian, Moroccan and Tunisian dialects) using morpho-syntactic information. Furthermore, Malmasi et al. (2015) used a parallel corpus to distinguish between six Arabic varieties, namely MSA, Egyptian, Tunisian, Syrian, Jordanian and Palestinian. Distinguishing between spoken Arabic varieties is also an active research area as there are sufficient phone and TV program re"
W16-4821,W15-5413,0,0.297764,"Missing"
W16-4821,L16-1284,0,0.139644,"Missing"
W16-4821,W15-5407,0,0.0680657,"a exists in the form of phone conversations and television program recordings, but, in general, dialectal Arabic data sets are hard to come by” (Zaidan and Callison-Burch, 2014). Akbacak et al. (2009), Akbacak et al. (2011), Lei and Hansen (2011), Boril et al. (2012), and Zhang et al. (2013) are some examples of work done to distinguish between spoken Arabic varieties. Similarly to Goutte and L´eger (2015), we experimented with both character-based and word-based ngrams as features. However, we used only one prediction step instead of two for both sub-tasks. Compared to the system proposed by Malmasi and Dras (2015), we used the same set of features with only one SVM classifier instead of an ensemble of SVM classifiers. 3 Methodology and Data We used a supervised machine learning approach where we trained a linear SVM classifier (LinearSVC) as implemented in the Scikit-learn package1 . In sub-task 1, we submitted two runs (run1 and run2). We experimented with different character-based and word-based n-grams and different combinations as features, and we reported only the best scoring features for each run. In run1: we used character-based 4-grams as features using TF-IDF weighting scheme. In run2: we use"
W16-4821,W16-4801,0,0.0403558,"Missing"
W16-4821,C12-1160,0,0.0717191,"Missing"
W16-4821,W14-5313,0,0.123659,"et al. (2016) give a comprehensive bibliography of the recently published papers dealing with these tasks. Discriminating between Arabic varieties is also an active research area although limited work has been done, so far, to distinguish between written Arabic varieties. The main reason is the lack of annotated data (Benajiba and Diab, 2010). Zaidan (2012) in his PhD distinguished between four Arabic varieties (Modern Standard Arabic (MSA), Egyptian, Gulf and Levantine dialects) using character and word n-gram models. Elfardy and Diab (2013) identified MSA from Egyptian at a sentence level, Tillmann et al. (2014) proposed an approach to improve classifying Egyptian and MSA at a sentence level, and Saˆadane (2015) in her PhD distinguished between Maghrebi Arabic (Algerian, Moroccan and Tunisian dialects) using morpho-syntactic information. Furthermore, Malmasi et al. (2015) used a parallel corpus to distinguish between six Arabic varieties, namely MSA, Egyptian, Tunisian, Syrian, Jordanian and Palestinian. Distinguishing between spoken Arabic varieties is also an active research area as there are sufficient phone and TV program recordings which are easy to transcribed. “The problem is somewhat mitigate"
W16-4821,J14-1006,0,0.0223516,"lects) using morpho-syntactic information. Furthermore, Malmasi et al. (2015) used a parallel corpus to distinguish between six Arabic varieties, namely MSA, Egyptian, Tunisian, Syrian, Jordanian and Palestinian. Distinguishing between spoken Arabic varieties is also an active research area as there are sufficient phone and TV program recordings which are easy to transcribed. “The problem is somewhat mitigated in the speech domain, since dialectal data exists in the form of phone conversations and television program recordings, but, in general, dialectal Arabic data sets are hard to come by” (Zaidan and Callison-Burch, 2014). Akbacak et al. (2009), Akbacak et al. (2011), Lei and Hansen (2011), Boril et al. (2012), and Zhang et al. (2013) are some examples of work done to distinguish between spoken Arabic varieties. Similarly to Goutte and L´eger (2015), we experimented with both character-based and word-based ngrams as features. However, we used only one prediction step instead of two for both sub-tasks. Compared to the system proposed by Malmasi and Dras (2015), we used the same set of features with only one SVM classifier instead of an ensemble of SVM classifiers. 3 Methodology and Data We used a supervised mac"
W17-4108,P16-1100,0,0.0604564,"Missing"
W17-4108,W14-4012,0,0.0183483,"Missing"
W17-4108,D15-1167,0,0.116139,"Missing"
W17-4108,1983.tc-1.13,0,0.106307,"Missing"
W17-4108,W16-2010,0,0.0416049,"Missing"
W18-4003,J81-4005,0,0.723584,"Missing"
W18-4003,N15-1184,0,0.0774046,"Missing"
W18-4003,P16-1089,0,0.0535206,"Missing"
W18-4003,E12-1060,0,0.0301263,"ace model receives support from a linguistic resource. Furthermore, by analyzing the disagreements between the lexicon and the embedding model, we can acquire insight into the shortcomings of their respective coverage. For instance, unlinked lexicon entries evidence those instances that the vector model is unable to learn, while unlinked word sense embeddings may suggest new usages of words found in the corpora. Being able to locate these cases opens the way towards improving lexica and embedding models. Automatic discovery of novel senses has been shown as a feasible and productive endeavor (Lau et al., 2012). In our evaluation we provide some insight into these situations: we use a mapping to calculate the probability that a word in a sentence from a corpus is an instance of an unlisted sense (i.e., not present in the lexicon.) This mapping process, and some of its potential applications, are explained in detail in the following sections. Section 2 contains a description of the mapping mechanism Section 3 goes on to evaluate the performance of this mapping on finding instances of unlisted senses. We present our conclusions in Section 4. This work is licensed under a Creative Commons Attribution 4"
W18-4003,D15-1200,0,0.0240587,"ding an effective way of representing the meaning of words, embeddings facilitate computations in models and pipelines that need to analyze semantic aspects of language. Based on their success, an effort has been concentrated in improving embedding models, from devising more computationally effective models to extending them to cover other semantic units beyond words, such as multi-word expressions (Yu and Dredze, 2015) or word senses (Neelakantan et al., 2014). Being able to represent word senses solves the problem of conflating several meanings of one polysemic word into a single embedding (Li and Jurafsky, 2015). Furthermore, having complete and accurate word sense representations brings embedding models closer to a range of existing, expert-curated resources such as lexica. Bridging the gap between these two worlds arguably opens a road to new methods that could benefit well-established, widely used resources (Faruqui et al., 2015; Speer et al., 2017). This is the focus of the work we present in this article. We propose an automatic way of creating a mapping between entries in a lexicon and word sense representations learned from a corpus. By having an identification between a manual inventory of wo"
W18-4003,D14-1113,0,0.104388,"ord embeddings have boosted performance in many Natural Language Processing applications in recent years (Collobert et al., 2011; Socher et al., 2011). By providing an effective way of representing the meaning of words, embeddings facilitate computations in models and pipelines that need to analyze semantic aspects of language. Based on their success, an effort has been concentrated in improving embedding models, from devising more computationally effective models to extending them to cover other semantic units beyond words, such as multi-word expressions (Yu and Dredze, 2015) or word senses (Neelakantan et al., 2014). Being able to represent word senses solves the problem of conflating several meanings of one polysemic word into a single embedding (Li and Jurafsky, 2015). Furthermore, having complete and accurate word sense representations brings embedding models closer to a range of existing, expert-curated resources such as lexica. Bridging the gap between these two worlds arguably opens a road to new methods that could benefit well-established, widely used resources (Faruqui et al., 2015; Speer et al., 2017). This is the focus of the work we present in this article. We propose an automatic way of creat"
W18-4003,Q15-1017,0,0.0308805,"sent in the lexicon. 1 Introduction Word embeddings have boosted performance in many Natural Language Processing applications in recent years (Collobert et al., 2011; Socher et al., 2011). By providing an effective way of representing the meaning of words, embeddings facilitate computations in models and pipelines that need to analyze semantic aspects of language. Based on their success, an effort has been concentrated in improving embedding models, from devising more computationally effective models to extending them to cover other semantic units beyond words, such as multi-word expressions (Yu and Dredze, 2015) or word senses (Neelakantan et al., 2014). Being able to represent word senses solves the problem of conflating several meanings of one polysemic word into a single embedding (Li and Jurafsky, 2015). Furthermore, having complete and accurate word sense representations brings embedding models closer to a range of existing, expert-curated resources such as lexica. Bridging the gap between these two worlds arguably opens a road to new methods that could benefit well-established, widely used resources (Faruqui et al., 2015; Speer et al., 2017). This is the focus of the work we present in this art"
W19-6134,N19-1423,0,0.0057219,"’s sentences. We also extended this basic approach to two different multiclass scenarios, where the policy conditions are subdivided in different ways. We implemented the classifiers using the scikitlearn library (Pedregosa et al., 2011). The classifiers use a tfidf-weighted feature representation based on n-grams of size one and two, without any feature selection. The classifier is a linear support vector machine with L2 regularization and a regularization term of 1.0. A one-versus-rest approach was used for multiclass classification. Preliminary experiments using a classifier based on BERT (Devlin et al., 2019) were less successful. 4 Experiments We carried out a number of experiments to see how well the classifier retrieves policy conditions from the IMF loan agreements, and how well different types of policy areas and policy types can be distinguished. 4.1 Finding Policy Conditions In the first experiment, we investigated the model’s capability of finding mentions of the policy conditions in the documents. This task was framed as a binary classification task where annotated text pieces were treated as a positive set while non-annotated pieces constituted a negative set. The negative examples were"
